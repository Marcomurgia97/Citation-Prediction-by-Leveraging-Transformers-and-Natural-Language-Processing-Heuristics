text
"on srm structure, integrated design and the optimization of srm components are main methods to reduce srm vibration, such as multiobjective optimization design method [cit], new stator tooth [cit], new rotor tooth [cit], and new poles ratio [cit] . on srm control strategies, varieties of methods have been proposed, such as torque distribution controller [cit], multilevel systematic design method [cit], and new motor torque controller [cit] . the above studies do reduce the srm vibration; however, these studies focus only on srm itself; the negative influence of srm on vehicle dynamic performance has not been fully considered. as the main component that flexibly connects the wheel and the vehicle body, suspension transmits the force acting between the wheel and the vehicle body. as a kind of suspension system, electromagnetic active suspension can produce controllable active force, which enables the suspension system to dynamically adjust the supporting force according to the vibration of wheel and vehicle body. in the previous work, we analyzed the influence of the srm unbalanced radial force on comfort and stability of iwm-ev. the conclusion [cit] shows srm vertical force is highly coupled with road excitation and srm airgap eccentricity, and this coupling yields srm vertical vibration that does harm to comfort and stability. this study used electromagnetic active suspension and linear quadratic gaussian (lqg) controller to reduce the srm vertical vibration and the negative influence on vehicle dynamic performance."
"in this paper, several performance indexes of iwm-ev are considered by linear quadratic gaussian (lqg) controller, such us vehicle body acceleration, airgap eccentricity, srm stator acceleration, suspension dynamic deflections, and tyre deformation. it is easy to use the methodology of analytic hierarchy process (ahp) to select the weighted coefficients of performance indexes [cit] . this lqg controller is based on optimal control theory and the controller can propose different objective functions according to design requirements and improve the vehicle performance by comprehensively considering various performance indexes."
"the proposed ristretto sequence was implemented on a philips 3 t systems (philips healthcare, best, the netherlands). experimental data were acquired in a gel phantom (cirs, norfolk, va, usa) using the proposed ristretto scheme, as well as conventional gre-mre and where applicable the expresso sequence. 30 in vivo data were collected in the breast and liver in one healthy volunteer upon informed consent and according to institutional guidelines using both the ristretto and expresso schemes. fractional encoding 29 with bipolar motion encoding gradients (megs) was used for motion sensitization and the external actuator was phase-locked to the sequence using a transistor-transistor logic (ttl) trigger."
"depending on the number of slices and wave-phase offsets acquired, this amounts to up to 5% of total scan duration. in order to successfully employ expresso, sequence parameters such as the wave frequency, number of slices, echo time, spoiling area, readout bandwidth, and encoding fraction have to be tuned such that they fit into the scheme. while an optimal set of sequence parameters can be found, the process of finding these parameters can be cumbersome and is not straightforward."
two models were constructed for studying the effects of electromagnetic active suspension. one is srm vertical force model that reflects the srm vertical force exerting on the wheel; another is full iwm-ev model that reflects the influence of electromagnetic active suspension force and unbalanced vertical force on vehicle.
"in this work it has been demonstrated that ristretto mre allows for the fine-tuning of imaging shot duration and hence acquisition time in conventional mre. since the expresso timing can be reproduced using the ristretto approach, the expresso scheme can never outperform ristretto."
"we demonstrated in phantom experiments as well as in in vivo scans that the change in sequence timing and wave-phase ordering does not substantially influence overall mre data quality and leads to good agreement in both displacement fields and recovered shear velocities. for the in vivo comparison, the expresso scheme was first optimized to make best use of its reduced timing flexibility. starting from there, ristretto was used to further reduce acquisition duration. hence, overall scan time reduction in the breast scan was limited to only 5%. the phantom study was also acquired with optimal conditions for the expresso sequence (number of slices dividable by three and 36 hz wave frequency). however, the slice study shows that changes to the fov can lead to significant scan time increases when expresso mre is used. allows an optimal mre protocol to be found matching the exact fov requirements for any given case."
"where 0 is the overlap angle of stator and rotor, f t is tyre tangential force, g m is airgap length of srm, δg is the airgap eccentricity, and is the angle between stator and wheel longitudinal axis. each pair of opposite stator with eccentricity will yield unbalanced radial force, for the convenience of presentation, only stators 1 and 4 are picked out to investigate the influence of srm vertical force on srm vibration and vehicle dynamic performance. when the is 90 ∘, srm unbalanced radial force is equal to srm vertical force."
"for this simulation, the road excitations at four tyres are assumed to be the same, only the front left in-wheel srm response is presented in this paper. in order to study the effects of electromagnetic active suspension under urban condition and high-speed condition, vehicle responses are simulated at 60 km/h vehicle speed on class c road and 100 km/h vehicle speed on class c road."
"(1) making judgment matrix . h ij (no unit) is the relative importance ratio of the index i and j. table 3 is a comparison table of the relative importance of each index. according to table 3, judgment matrix h shown in (23) can be constructed."
"ri is the random consistency index. when n is 14, ri is 1. 59. if cr is less than 1, h passes the consistency checking. if cr is more than 1, h needs to be revised [cit] ."
"while the same rule applies for n d as with multi-slice ristretto. a phase correction is not necessary as all k-lines are acquired with the same phase offset, ensuring data consistency."
"sequence timing options for eight wave-phase offsets acquired with conventional mre (black square), expresso mre (black cross) and ristretto mre (gray dot) for 36, 60, and 100 hz. imaging shot duration (a), and acquisition duration in units of the wave period (b), versus the number of acquired slices. cut-off values for the imaging shot duration are plotted for the first four water/fat in-phase conditions including a 1.4 ms overhead for the rf pulse, readout, and spoiling, providing a lower limit for the imaging shot duration. the graphs can be used as a look-up table for finding optimal imaging parameters for a given wave frequency figure 4 1), comparison of the real parts of the complex displacement field, scan time, and acceleration of conventional gre-mre (a), expresso mre (b), and the three generalization steps to ristretto mre after phase correction (c-e). in all cases 14 slices were acquired at 36 hz wave frequency and using equal imaging parameters. displacement fields are in excellent agreement for all six mre sequences. 2), through-plane view of the same acquisitions without per-slice phase correction showing the effect of the accelerated acquisition on the acquired wave phase; 3), respective through-plane displacement fields after phase correction in figure 5, correlation plots of the real parts of the complex displacement field vector measured with conventional and ristretto mre are shown for 25, 36, 40, and 60 hz wave frequencies. a linear regression analysis was performed to determine the correlation coefficient, its standard deviation, and the standard error of the fit. results are given alongside the correlation plots. correlation coefficients are very close to one, ranging from 0.9924 to 1.0228. the standard error of the fit for 40 and 60 hz was lower than 0.6 μm, which is less than 2% of the peak displacement amplitude of 30 μm. for 25 and 36 hz, the standard error of the fit was slightly elevated, measuring 0.92 and 1.16 μm. the relative standard errors of the fit are within the same domain as in a previously published correlation study utilizing gre-mre. 28 in figure 6, the scan durations for the slice (a) and frequency (b) studies are compared between the three acquisition techniques. and is comparable between all techniques. in all cases, ristretto mre outperforms expresso mre in terms of scan duration."
"1.39 ± 0.09 m/s; ristretto, 1.37 ± 0.09 m/s) are in good agreement. an additional through-plane view of the real parts of the displacement fields before and after phase correction is shown in the rightmost column of figure 8 . again, good agreement between the two techniques could be found."
"since no further delay objects are introduced and all imaging shots are of the same duration, it must be assured that all phase offsets are acquired. this is fulfilled only by delays n d that are co-primes of n p, i.e."
"the root mean square (rms) of dynamic characteristics of the passive suspension vehicle can be produced by simulating the passive suspension vehicle model under the corresponding conditions. according to the literature, [cit], scale factors of vehicle dynamic characteristics can be described by the following equations:"
"we have demonstrated that the multi-shot expresso concept can be generalized to allow for flexible tuning of imaging shot durations. in typical applications, the resulting ristretto mre scheme permits reduction of total scan durations by up to 450% relative to conventional mre depending on the number of slices and wave frequency."
"(2) this electromagnetic active suspension could effectively reduce the srm vertical vibration. srm airgap eccentricity and stator acceleration were both reduced. specifically, when the frequencies of the srm airgap eccentricity and stator acceleration are near the stator's resonance frequency of 107.4 hz, the srm vertical vibration shows a most obvious drop."
"vehicle responses are simulated at 60 km/h vehicle speed on class c road as shown in figure 4 . responses of vehicle body acceleration, airgap eccentricity, srm stator acceleration, suspension dynamic deflections, and tyre deformation at 60 km/h are shown in figures 5-9 it can be seen that the electromagnetic active suspension can reduce the power spectral density (psd) values of vehicle body acceleration, airgap eccentricity, srm stator acceleration, suspension dynamic deflections, and tyre deformation at their resonance frequency. specifically, figure 5 shows that the resonance frequency of vehicle body acceleration is 1.221 hz, and the peak value of vehicle body acceleration decreases from 0.5395 (m/s reduced by 19.5%. this indicates that the electromagnetic active suspension can reduce the force acting between the wheel and the vehicle body, and ride comfort improved. figure 6 shows that the resonance frequency of airgap eccentricity is 98.57 hz, and the peak value of airgap eccentricity decreases from 1.307e −10 m 2 /hz to 8.339e −11 m 2 /hz and reduced by 36.2%. figure 7 shows that the resonance frequency of stator acceleration is 98.57 hz, and the peak value of stator acceleration decreases from 8 figure 9 shows that the resonance frequency of tyre deformation is 6.714 hz, and the peak value of tyre deformation decreases from 1.154e −5 m 2 /hz to 1.140e −5 m 2 /hz and reduced by 1.21%. because driving stability is related to suspension dynamic deflections and tyre deformation, decrease of them implies that electromagnetic active suspension can improve vehicle safety. therefore, compared with passive suspension, electromagnetic active suspension can availably reduce values of body acceleration, airgap eccentricity, srm stator acceleration, and so on. ride comfort and safety under urban condition improved."
"the interleaved acquisition of wave-phase offsets is key to increasing sequence timing flexibility. if the number of phases is a power of two, any uneven number can be chosen, which allows for imaging shot adjustments of"
"ahp is a decision-making method. it decomposes elements that are always related to decisionmaking into objectives, criteria, and programs and then makes qualitative and quantitative analysis on this basis. it was used to select weighted coefficients of performance indexes for lqg controller [cit] ."
"the effects of electromagnetic active suspension under high-speed condition are weaker than effects under urban condition, because the weighted coefficients of lqg controller are more suitable for urban condition. all the vehicle responses at both 60 km/h and 100 km/h were shown in table 4 ."
"switched reluctance motor (srm) has achieved good performance due to its remarkable advantages: high starting torque, wide operating speed range, and high efficiency. these advantages endow srm with great potential on in-wheel motor electric vehicles (iwm-ev). however, asymmetrical magnetic pull caused by airgap eccentricity yields unbalanced radial force; this unbalanced radial force is recognized as one of the main reasons for the srm vibration [cit] . to reduce the vibration, scholars have focused on exploring the structure and control strategies of srm [cit] ."
"ristretto permits the imaging shot duration to be increased beyond the one wave period limit. this allows gre-mre for high frequency mre acquisitions, where slice excitation, motion sensitization, readout, and spoiling combined exceed the wave period. thus, in contrast to expresso, ristretto enables pre-clinical gre-mre studies on full-body systems with limited slew and gradient strength capabilities that necessitate the use of multiple meg repetitions."
"in this work, we propose a generalization of the expresso scheme termed ristretto that enhances sequence timing flexibility by eliminating the sequence dead time through incorporating the delay into each imaging shot and additionally allowing for the interleaved acquisition of wave phases. thereby, accelerated multi-slice mre acquisitions become possible independent of the number of slices acquired, and sequence parameter optimization is simplified. the gain in flexibility is demonstrated for typical use cases of gre-mre, and the sequence is validated in a phantom study and in vivo feasibility is demonstrated."
"comparison of phantom scan durations and rmses of the real part of the complex displacement field for different numbers of acquired slices (a), and change in wave frequency (b). the slice study was conducted at 36 hz, while the frequency study was conducted measuring 15 slices, while all other imaging parameters were held constant. the data labels above the expresso and ristretto bars denote accelerations compared with the respective conventional mre acquisition. for 13 and 17 slices, as well as 40 and 60 hz, expresso mre cannot be used. ristretto mre (red) can substantially accelerate acquisition in all cases (1.92-to 4.48-fold acceleration compared with conventional mre) and always shows lower acquisition time than expresso mre. for all scans, rmse is below 5% of the peak amplitude (30 μm)"
"the delay object, which shifts the sequence with respect to the external wave to acquire the wave-phase offsets, can be omitted by distributing its duration evenly over all imaging shots of one multi-slice train ( figure 1d ). thereby, the imaging shot duration is slightly increased, while the per-slice repetition time is kept constant. the imaging shot duration is then given by"
"vehicle responses are simulated at 100 km/h vehicle speed on class c road as shown in figure 10 . vehicle body acceleration, airgap eccentricity, srm stator acceleration, tyre deformation, and suspension dynamic deflections responses at 100 km/h are shown in figures 11-15 ."
"for successful recovery of the displacement field, the slice reference phase needs to be accounted for before wave inversion. the difference in slice reference phase between successive imaging shots is given by"
"in order to conduct this study, a full iwm-ev model, srm vertical force model, and control diagram for the electromagnetic active suspension are built in matlab/ [cit] b. then the weighted coefficients are calculated and lqg controller is completed. finally, the frequency responses 2 journal of control science and engineering on stochastic roads and time responses on representative roads are analyzed. conclusion shows that the electromagnetic active suspension can effectively reduce srm vibration and the negative influence on vehicle dynamic performance."
"figure 1 timing diagram comparison of the proposed ristretto multi-shot sequence (c-e), with conventional (a), and expresso multi-slice (b), gre-mre. each square denotes a fractional gre-mre imaging shot (f), acquiring one k-line for one encoding direction, slice, and wave-phase offset. the full mre dataset is acquired in four loops, their ordering being from innermost to outermost: slice, wave phase, k-line, and encoding direction. red squares above the diagrams denote the shift in wave phase. red markers within the sequence diagram denote delay objects, which are introduced into the sequence to shift the multi-slice acquisition with respect to the trigger of the wave. a ttl is sent for each k-line and encoding direction after all wave-phase offsets and slices are acquired"
"where x is the state variables, y is the output variables, u is the control vector, and a is the system matrix. b is control matrix. g is disturbance matrix. c is the output matrix. d is transfer matrix."
"change. in order to study the effects of electromagnetic active suspension on handling stability, vehicle responses are simulated under single lane change condition. the steering angle model is shown in figure 22 . airgap eccentricity, roll angle, sideslip angle, and yaw rate responses at 100 km/h on class c road are shown in figures 23-26 . figure 23 shows that the airgap eccentricity response does not have much relevance to steering angle, but the electromagnetic active suspension efficiently reduces the airgap eccentricity. figure 24 shows that the vehicle starts to roll at 0.5s. the roll angle reaches peak value at 1.8s and decreases by 3.66%. figures 25 and 26 show that the trends of sideslip angle and yaw rate are similar to steering angle. although the contributions of the electromagnetic active suspension to sideslip angle and yaw rate under this condition are not very significant, the electromagnetic active suspension system does have positive influences on the lateral dynamic performance."
"in the expresso scheme, different eddy current contributions can be accumulated per slice, as the sequence delay disturbs the otherwise equidistant slice acquisition. here, the ristretto scheme overcomes this issue by including the sequence offset in the imaging shot duration. hence, an eddy current steady state can be established that is independent of the slice ordering and the time-point at which the slice is acquired. since encoding directions are sequentially acquired, eddy currents and mechanical vibrations arising from the different meg directions can potentially bias the encoding. future work should thus encompass further adaptation of the ristretto sequence to allow for additional interleaved encoding directions."
"in figure 3, imaging shot duration (a) and the duration for the acquisition of all slices and phases per k-line and encoding direction (b) are"
"given that the number of acquired wave phases is a prime number, e.g. 3, 5, 7, or 11, the delay can be any number smaller than n p, allowing for maximal timing flexibility of"
"successful acquisition of an mre dataset requires wave-phase consistency for each acquired slice, wave-phase offset, and encoding direction separately. hence, the expresso concept can be first generalized by acknowledging, that the total duration of all imaging shots for the same phase offset, k-line, and encoding direction can be chosen as an integer multiple of the wave period. thus, an arbitrary number of slices n s can be acquired within an arbitrary number of wave periods n w ( figure 1c ). the number of slices per wave period n spp is then given by a rational number"
"in order to reduce srm vertical vibration and improve iwm-ev dynamic performance, this paper used electromagnetic active suspension and lqg controller to suppress the srm unbalanced radial force. the controller based on optimal control theory can efficiently match electromagnetic active suspension. in theory, compared with the passive suspension, this electromagnetic active suspension can reduce srm vertical vibration and provide better dynamic performance to the iwm-ve. based on the analyzed simulation results, the following 3 conclusions can be summarize:"
"in figure figure 7 shows the through-plane displacement fields for both scans before and after correction of the acquisition phase offset δφ for one representive position in the breast, demonstrating successful recovery of the displacement fields."
"where is the slip ratio and b x, c x, d x, e x, b y, c y, d y, and e y are empirical parameters. f xij, f yij are the longitudinal force and lateral force."
"the present work has demonstrated the feasibility of ristretto mre in the liver and breast in healthy volunteers. future work will be directed toward assessing the performance of liver ristretto mre in patients, where the reduction in bh duration is expected to improve overall data quality, as well as investigating the use of the ristretto scheme for brain, renal, prostate, or cardiac mre."
"where q 1 is the weighted coefficient of body vertical acceleration; q 2 is the weighted coefficient of pitch angle; q 3, q 4, q 5, and q 6 are the weighted coefficients of airgap eccentricity; q 7, q 8, q 9, and q 10 are the weighted coefficients of tyre deformation; q 11, q 12, q 13, and q 14 are the weighted coefficients of suspension dynamic deflections; q is the weight matrix of the state; u is the control vector of fc; and r is the weighted coefficient of u. rewrite (30) as a standard form:"
"in figure 8, a comparison of expresso and ristretto mre is given for the in vivo liver. again, the protocol was first optimized for the use of expresso, leading to a bh duration of 14.5 s. switching to ristretto mre, the bh duration could be reduced to 12.3 s, which is equivalent to a 15% reduction in scan duration. both the real parts of the displacement fields and the shear velocity maps and roi-averaged values (expresso,"
"subjective weighted coefficients of pitch angle, airgap eccentricity, tyre deformation, and suspension dynamic deflections can be calculated as follows. figure 3 . vehicle body acceleration and pitch angle are chosen to be the performance indexes for improving ride comfort and handling stability. airgap eccentricity, tyre deformation, and suspension dynamic deflections of each wheel are chosen to be the performance indexes for reducing the vertical vibration of srm. the performance function is defined as follows:"
"acquisition train with respect to the wave period to allow for the successive acquisition of the wave-phase offsets. while the concept allows suitable sequence parameter combinations, which minimize the scan duration, to be found, the requirement of an integer number of slices per wave period leads to substantial timing constraints. in addition, the delay objects can be seen as dead time, where acquisition is not possible."
"and the constraint that the slices per wave period n spp must be a divisor of the total number of slices n s, i.e."
"where ba, pa, aeij, tdij, sddij are rms of vehicle body vertical acceleration (bva), pitch angle (pa), airgap eccentricity (ae), tyre deformation (td), and suspension dynamic deflections (sdd). pa, aeij, tdij, sddij are pitch angle scale factor, airgap eccentricity scale factor, tyre deformation scale factor, and suspension dynamic deflections scale factor; vehicle body vertical acceleration (bva) scale factor is 1."
"while efficient expresso accelerations can be found for most frequencies below about 60 hz, the process of finding a suitable combination of number of slices, slices per wave period, meg duration, echo time, readout bandwidth, and total acquisition time can be cumbersome. with the use of ristretto, the imaging shot duration can be independently minimized for every sequence parameter combination, retaining most of the imaging flexibility known from conventional mri."
"srm vibration and iwm-ev body movement influence each other, so it is difficult to improve the dynamic performance of the vehicle considering only the srm structure and controller design. therefore, this study used electromagnetic active suspension, connecting body and wheels, to reduce the srm vibration and the negative influence on vehicle body."
"(3) this electromagnetic active suspension could improve the safety. under both urban condition and high-speed condition, suspension dynamic deflection and tyre deformation were reduced. on representative roads, electromagnetic active suspension reduced roll angle, sideslip angle, and yaw rate. it indicated that the electromagnetic active suspension can improve lateral dynamic performance and reduce srm vertical vibration at the same time. in summary, this paper used electromagnetic active suspension and lqg controller to effectively improve the iwm-ev ride comfort and safety and also reduce the srm vertical vibration. due to limited research resource, this paper does not construct an experimental platform to verify the simulation results. but we are preparing an experimental platform and will give introduction in the next work."
"hence, the ristretto scheme can equally be applied to multi-frequency mre, where each frequency component has to be corrected with the same slice reference time offset δφ."
"under high-speed condition, the power spectral density (psd) values of vehicle body acceleration, airgap eccentricity, srm stator acceleration, suspension dynamic deflections, and tyre deformation can be reduced at their resonance frequency. figure 11 shows that the peak value of vehicle body acceleration decreases from 0.9104 (m/s 2 ) 2 /hz to 0.8158 (m/s 2 ) 2 /hz and reduced by 10. 4%. this means that, even under high-speed conditions, the electromagnetic active suspension can still significantly reduce the vibration of the vehicle body, and the ride comfort improved obviously. as can be seen from figures 12 and 13 figure 14 shows that the peak value of suspension dynamic deflections decreases from 2.24e −4 m 2 /hz to 2.02e −4 m 2 /hz and reduced by 9.8%. figure 15 shows that the peak value of tyre deformation decreases from 1.902e −5 m 2 /hz to 1.894e −5 m 2 /hz and reduced by 0.42%. therefore, electromagnetic active suspension can improve ride comfort and reduce srm vibration under high-speed condition. because decreases of suspension dynamic deflections and tyre deformation can greatly enhance the driving stability under high-speed condition, the vehicle safety improved."
"(1) this electromagnetic active suspension could effectively improve the ride comfort. vehicle body acceleration was reduced on urban condition, high-speed condition, and representative roads. for urban condition, the vehicle body acceleration reduced by 19.5% near the vehicle body's resonance frequency 1.22 hz. for high-speed condition, the vehicle body acceleration reduced by 10.4% near the vehicle body's resonance frequency 1.22 hz."
"the second aspect we consider when generating lightweight, responsive models for use at runtime is the depth of the decision tree used, which directly corresponds to model complexity. using the five most important features for every model, figure 10 shows the accuracy of each model at a range of decision tree depths (1 through 25). we can see that using the top five features and a tree depth of 15 produces models whose accuracy for lulesh and ares is within 0.1% of the models using all available features, and for cleverleaf, is within 8% of the model using all available features."
"b) cleverleaf: a shock hydrodynamics proxy application with adaptive mesh refinement developed at the university of warwick and the uk atomic weapons establishment. cleverleaf uses the samrai library from lawrence livermore national laboratory to add an amr capability to a parent shock hydrodynamics mini-application [cit], working on dynamically sized patches of data that represent areas of interest in the physical domain with different levels of resolution. there are 88 forall kernels in cleverleaf, the majority of which are over all the elements of the current amr patch. the other kernels in cleverleaf iterate over the boundary regions of these patches in strips that are 2 elements wide in order to apply the physical boundary conditions to the hydrodynamics problem. the main input-dependence in cleverleaf thus comes from the shape and size of the patches. c) ares: an arbitrary lagrangian-eulerian (ale) radiation hydrodynamics code capable of running small serial applications to large, massively parallel applications [cit] on millions of processors. ares comprises several million of lines of code, and it is used primarily in munitions modeling and inertial confinement fusion applications. ares also has an amr capability, with patches created dynamically as the simulation evolves. one of the physics packages in ares has been ported to use 536 raja kernels that iterate over a number of different aspects of the problem domain. ares contains a mixed material capability; however, the number of material regions is not fixed and can change dynamically during the course of a simulation as materials mix together."
"apollo is a framework for tuning templated kernel execution parameters at runtime. figure 3 shows the apollo workflow. first, we run the application to generate training data samples. the samples are the input to apollo's model-generation framework which trains a per-kernel decision model that can predict the fastest parameter values for each kernel execution. these models can be linked into the application dynamically, without recompilation. we designed apollo's decision models to execute with very low overhead, and we trade the costly on-line search used by existing auto-tuners for off-line training."
"we use a lightweight annotation system, caliper [cit], to measure the runtime of the kernels, and to store arbitrary additional attribute-value pairs representing additional features. caliper provides a simple interface that allows application developers to add semantic annotations of interest to each kernel. for example, a developer could provide the current time step or the dimensions of a patch in an amr mesh. table i lists the features we collect. while this is a small sample of all the runtime features we could collect in a multi-physics code, it allows us to explore the efficacy of our technique for improving application performance. new features, such as additional problem-specific information from an application input file, can be easily added to characterize kernels and their input data more completely. however, relying on too many parameters can perturb execution in real runs. in practice, we use feature importance analysis to find small sets of important features and to reduce the size of our models; we discuss feature importance analysis in more detail in section iv-b. to generate a complete set of training data, a given input problem must be run multiple times, once for each value of the parameter we are modeling. execution policy is a static parameter. to allow us to vary the execution policy dynamically, we developed a raja extension which reads the execution policy from an environment variable. using the new generic lambda feature added to c++14, we use the auto keyword to determine the type of the execution policy passed into the raja forall method"
"to train a classifier, we start with the feature vectors mentioned in section iii-a. for each training run, we collect a set of feature vectors mapped to their runtimes. because there are multiple training runs with different execution models, the same feature vector may map to many possible runtimes. we label each unique feature vector with the execution model that resulted in the fastest runtime. we feed these labeled feature vectors to a learning algorithm, which trains a classifier to select a label given a feature vector."
"the performance of modern scientific codes depends not only on the target architecture, but also on input-dependent aspects of the code. for example, in an adaptive mesh refinement (amr) simulation, a kernel may handle a wide range of patch sizes. a small patch may not contain enough computational work to amortize the cost of spawning a parallel openmp region, so we might choose to execute it sequentially. moreover, such behaviors evolve over the course of a run. these dynamic applications require on-line tuning to achieve the best performance, as tuning parameters may need to be set based on the state of the code each time the kernel executes."
"our work is implemented in apollo, an extension of the raja performance-portability library. this paper makes the following contributions: 1) a novel tuning technique using simple decision tree classifier models instead of costly search strategies; 2) an interface for collecting arbitrary training features from kernel executions in multi-physics codes; 3) a method for dynamically selecting statically optimized code paths at runtime; and 4) apollo, a production-ready framework that implements these techniques and has been tested on a real multi-physics code."
"apollo uses a decision tree to generate code for run-time auto-tuning. this approach differs from existing auto-tuning frameworks in several respects. first, most existing frameworks select the fastest version of each kernel by executing it many times with different tuning parameters. this precludes any input-dependent optimizations, as it selects a kernel version before run-time, when the kernel's inputs are known. second, frameworks that can adapt at runtime search the tuning parameter space at run-time, which incurs high overhead and does not allow kernels to adapt quickly to changing inputs. the tuning parameter space can be very large, and even sophisticated search methods can take minutes to converge."
"whilst figures 12a and 12b show consistent speedups values between 4-5x and 3-4x, the sedov problem in figure 12c shows speedups from 1.29x on 16 cores to 2.3x on 256 cores. that is, apollo gets more improvement out of this problem as we strong scale. in strong scaling, the adaptively refined mesh is subdivided into increasingly smaller and smaller subdomains, and apollo is able to speed up execution for more patches at the strong scaling limit. the same trend is visible in figure 13 . using apollo, ares runs from 8% faster on 16 cores up to 15% faster on 256 cores. note that ares is a production multi-physics code, and the hotspot deck exercises many different physics modules. our speedups with apollo are measured in wall clock time for the entire ares run, but only a single physics component has been modified to work with raja (and therefore apollo)."
the kernels in the above applications display significant performance variability and the fastest execution policy can be 1-3 orders of magnitude faster than the slowest (figure 1 ). there is a great potential for performance improvement by dynamically selecting the best policy each time a kernel executes.
"while we only train our models on single-node runs, we can use our tuning models in larger, multi-process runs. this is important, since cleverleaf and ares use adaptive mesh refinement, and while the overall refinement of the mesh is deterministic for our runs, the sizes of refined patches created in a distributed run are not the same as those created in a sequential run. each node in an mpi-parallel run may have a very different workload. this is the kind of input-dependent behavior our models are designed to optimize. figure 12 shows the runtimes and speedups of dynamically tuning cleverleaf for all three input problems. we use a larger initial problem size in each case and we strong scale up to 256 cores. in all cases, tuning with apollo is significantly faster than using the default raja execution policy. figure 12 also shows a visualization of the mesh configuration and density field for a subset of the problem space at the final time of the simulation. since the best parameter value is largely dependent on the subdomain size, these visualizations explains some of the speedups provided when dynamically tuning with apollo's models. for example, the curved shock in the sedov problem generates many small subdomains which apollo correctly runs serially, beating the default execution policies by up to 2.3x."
"we have used our framework in two proxy applications, and with several different input problems in a production multi-physics code, where apollo achieves speedups from 1.2x to 4.8x."
"supercomputing architectures are increasingly diverse, and applications must be adapted to each architecture to achieve the best performance. developing and maintaining multiple implementations of complex scientific applications with thousands of lines of code is infeasible, so having a single-source code at the application kernel level is essential. raja allows the applications to maintain a single source code, while allowing performance portability to different architectures. raja decouples the kernels in the application from the execution parameters, enabling static tuning of the policy used to perform the loop iterations. raja uses lightweight syntax and standard c++ features for portability and ease of integration into existing production applications."
"auto-tuning is a broad field, and there has been much existing work on tuning the performance of codes. extensive work has been done in the area of compile-time auto-tuning [cit] and run-time adaptation [cit] . existing approaches typically perform a large, guided search of the performance parameter space, running many instances of the kernel to determine the best assignment of tuning parameters. despite a host of techniques that prune the search space and reduce the required number of trials, the fastest searches today still take minutes to run, meaning existing tuning approaches work only if the code's behavior changes slowly."
"raja enforces decoupling of the kernel body from its execution model (how the iterations are scheduled to hardware) with c++11 lambda functions. using a lambda function, the kernel body and its surrounding scope is captured and passed to raja's forall execution method. the execution method allows a single-source kernel body to be dispatched to multiple programming model backends, determined by the exec policy. the following listing contains an example raja kernel:"
"we use the data set collected by our measurement framework to build runtime auto-tuners, which select the fastest execution model for each kernel when it is executed. we model the problem of selecting a code variant (in this case a template instantiation) as a classification problem. in machine learning, the classification problem asks us to identify the category to which a sample belongs. our samples are the feature vectors we collected for each kernel invocation, and we categorize samples into groups by their fastest execution model. using this classifier at runtime allows us to select the best kernel implementation for its input data."
"apollo's tuners trade costly on-line search for off-line training. training can be expensive, but it is performed off-line before the program runs. decision trees encode the knowledge required to make a tuning decision quickly at runtime, and they can use measured features to make decisions at runtime. instead of exploring the tuning parameter space, apollo can select a kernel variant directly based on the run-time values of features. this allows kernels to quickly adapt to their input data with only a few conditional evaluations. for example, apollo kernels can use differently optimized code variants for each differently-sized patch of data in an amr simulation."
"to showcase how our technique can speed up the execution of real kernels, we built tuning models for all three applications, and tested them on all input problem configurations. in this section we present the speedups we achieved by dynamically selecting statically optimized kernel variants. the decision models are evaluated every time a raja kernel is invoked, and the selected parameter will be set to the value predicted by the model. in the previous section we determined a lightweight model configuration with fewer features (5) and reduced tree depth (15), and we use that configuration to generate the models we use here. note that for each application, the same model is re-used across input decks, and across mpi ranks for the parallel runs of cleverleaf and ares."
"apollo generates a tuning model from a decision tree using a simple code generation algorithm. we traverse the decision tree generated by our training algorithm and generate nested conditional statements that test the values of features corresponding to each node. internal nodes in the tree become if statements, and the leaves become assignments that select a kernel variant or parameter value to be used at runtime."
"when tuning an application to the hardware, an application developer might select a single parameter value for the entire run. however, due to the input-dependence, choosing a single value will often result in significantly slower runtime when compared to a dynamic selection of the parameters. figure 2 shows the potential runtime if we were able to pick the best execution policy for each unique kernel launch in cleverleaf, rather than statically selecting openmp."
"to tune applications, we must have a way of both expressing and selecting the possible values for a given tuning parameter. in this paper, we focus on tuning applications that use the raja performance portability framework. in the following sections we describe raja, as well as the parameters we tune and the applications to which we apply this tuning."
"when a kernel is executed, the begin call of the apollo interface uses the predicted parameter values to tune the raja execution policies. a simple model generated to tune execution policies looks like this: again, apollo uses the models learned offline to dynamically tune application parameters. learning offline allows us to amortize the cost of modeling over a number of runs, and remove the cost of constructing the model from the runtime overhead of the framework. in section iv, we show how these models can speed up all three of our target applications."
"performance portability is extremely important for modern simulation codes. algorithms must run efficiently on an increasing number of target architectures, but choosing the best threading model, block size, scheduling policy, and other parameters for each architecture is a daunting task. large codes contain thousands of independent kernels, and developing and maintaining multiple versions of each one is infeasible. portability frameworks such as raja [cit] and kokkos [cit] have emerged to fill this gap, allowing developers to separate the concerns of tuning and correctness. developers write one, single-source, version of each kernel, and set architectural tuning parameters at compile time using c++ template parameters."
"we developed a simple interface that enables apollo to tune raja-exposed kernel execution parameters without further modifications to the application ( figure 5 ). in raja, we have added apollo::begin() and apollo::end() hooks before and after each raja loop template. through this interface, raja interfaces with one of two apollo components: 1) recorder: collects kernel features and measures the execution item to use as training data; 2) tuner: selects kernel execution parameters at runtime. the apollo calls pass the indexset object and the kernel body (as a lambda function) to the loaded apollo component. the recorder simply stores observed feature values in a file to be used as training inputs. the tuner contains a function with the generated conditional code from our decision tree. it uses runtime feature information to tune the execution parameters. with this design, the decision model is not tightly coupled with the code, and we can re-train our decision model on new recording data without recompiling the application. this allows us to generate new, finely tuned decision models over time as the inputs to our application codes change."
"in this paper we focus on input-dependent parameters, where their best value depends on information known only at application runtime. the most obvious tuning parameter exposed by raja is the execution policy, which determines whether a kernel is executed sequentially or in parallel using openmp threads. by default, raja requires the execution policy to be selected at compile time, allowing for static tuning. however, the best execution policy may depend on dynamic parameters, like the number of iterations required."
"during a training run, we collect data about each kernel execution. this information takes the form of a tuple, or, in machine learning parlance, a feature vector. the vector serves as the input to our decision model. each element of the tuple is the value of a particular feature of the kernel. for each feature vector, we also record the kernel's runtime. the features we collect are designed to characterize each kernel, as well as to capture application-specific information that might affect kernel performance. we collect the following distinct categories of information about the kernel invocation: 1) kernel features, collected from parameters passed to the raja forall method; 2) instruction features, gathered from the lambda function that corresponds to the kernel body; 3) measurements of the kernel runtime; 4) application features, optionally specified by the application developer."
"from these results we can see that in many cases, apollo's models are applicable across both applications and input decks. of particular interest to us is the fact that the models learned from lulesh are effective when applied to cleverleaf and ares. lulesh is a mini-app containing less than 40 unique kernels, but the range of training data collected appears to cover the performance space containing the kernels from both cleverleaf and ares. conversely, models produced with cleverleaf and ares do not perform well for lulesh. we attribute this drop in accuracy to the narrower range of iteration counts observed in these applications, since each problem configuration was run with fewer global problem sizes."
"statically choosing an execution policy for a kernel prevents us from making the best choice based on run-time adaption of the code, and the static choice will not be fastest in all cases. to determine the potential improvement of a fast, dynamic tuning approach, we examined kernels from several applications of interest to the u.s. department of energy. a) lulesh: a proxy application that models shock hydrodynamics developed to aid the department of energy's co-design effort [cit] . lulesh has two main categories of kernels using raja's forall construct. the first category iterates over domain elements, with problem size-dependent iteration counts. the second category iterates over material regions; these kernels have lower iteration counts dependent solely on the number of material regions in the problem (the default case has 11 iterations)."
"we also expose an additional openmp parameter that can be tuned dynamically. openmp's static schedule can take an integer parameter to control how blocks of loop iterations are shared between threads. specifically, the parameter controls the number of consecutive iterations that get assigned to each thread. for example, a value of 1 would interleave the iterations across threads. the default value is n/t where n is the number of iterations, and t is the number of openmp threads."
"we implemented our data processing and model generation as a python package, that is a complement to the runtime c++ components in the apollo framework. we read training data samples into pandas dataframes [cit], process them, then convert them to numpy [cit] arrays for use with the scikit-learn package [cit] ."
"our tuning models so far have been built and applied to single applications independently. an important aspect of our work is the ability to generate tuning models that can be used to dynamically tune other applications. in table iii we show a first step in this direction, where we evaluate each model on a test set taken from a single application and input problem combination."
"since evaluating the model at runtime adds overhead, we perform a comparison between a static parameter choice and the model choice for a range of problem sizes. in lulesh and cleverleaf, the default execution policy is openmp everywhere, as chosen by the developers of the raja versions of these applications. ares is a more complex code, and the developers have manually assigned kernels in the code as being more appropriate for serial or openmp execution (regardless of input data), and the default version of ares uses these policy selections. figure 11 shows the speedup with apollo in all three applications running on a single node. for cleverleaf, our dynamic tuning is the most successful, speeding up execution by up to 4.8x. apollo achieves a speedup of 3.36x for lulesh and 1.15x for ares. the speedup comes mainly from avoiding the overhead of spawning openmp regions for very small patches, and our models are able to determine from the patch size and other features when it is faster to run a loop sequentially than to pay for the overhead of using threads."
"since a template parameter is used to specialize the forall method for specific execution backends, the code can be both inlined and optimized by the compiler. note that the forall method is also templated on the type of the lambda function. since each c++ lambda function has a unique type, a unique instance of this forall function will be created for every application kernel, meaning that the compiler can optimize each code path."
"apollo can use a large number of features to make run-time tuning decisions, but this requires us to measure each input feature, and doing so can be costly. decision tree models can be reduced to focus on the most important features by simply removing deeper levels of the tree. this reduces the accuracy of the model, but it also decreases the measurement cost of each decision. in this section we analyze the impact of decision trees constructed using a reduced subset of features, and a reduced decision tree depth. figure 8 show the normalized importance of the top 5 features for each application. the number of indices and timestep are important in all applications. the problem name is also an effective feature in cleverleaf and ares, highlighting the input-dependent nature of the decisions. we also see instruction count features appearing. the movsd feature is a scalar load, showing the impact of a kernels memory demands on the execution policy choice. to examine the impact of the features used in the models on their accuracy, we build and evaluate models using subsets of up to ten of the most important features identified in figure 8 . we can see in figure 9 that accuracy is stabilized when using 4 features, with accuracy scores nearing those achieved using all features."
"for our first experiments, we evaluate the accuracy of our decision tree models for tuning two parameters: execution policy and openmp chunk size, measuring how often the models pick the fastest parameter value. we build a single model that takes as input all available features for each application and parameter combination. the model is trained to make decisions regardless of the input problem -that is, we did not use any features specific to a particular input deck. we use 10-fold cross validation, where the input cases are divided into 10 equal sets, with 10 models created using rotating groups of 9 sets as training data, and testing our classifier on the tenth set. reported scores are the mean accuracy of the ten models. we can also look at the runtime of the predicted parameter values compared to the best possible choice, helping us quantify the case where the model might make an incorrect choice, but pick the second best value. in this case, the accuracy of the model would be penalized, although the overall runtime of the set of parameter values selected might remain close to the best. figure 6 contains the relative speedups for each kernel when dynamically tuning the execution policy values using the model, compared to the best possible choices, and the default static selection of openmp everywhere. figure 7 shows the relative speedups for each kernel when tuning the openmp chunk size, compared to a default static selection of 128."
"the runtimes of the predicted policies are close to best, matching the statistical accuracy of the model. for all three applications, using the predicted policies results in a mean speedup when considering all kernels. for both lulesh and cleverleaf, the predicted policies always beat the default configuration for the 8 most variable kernels in both applications. in ares, one kernel shows a slowdown when using the predicted policy, but the remaining 7 most variable kernels all show a significant speedup. the runtimes of the predicted chunk sizes are close to the best for lulesh and cleverleaf, despite the model only picking the fastest size 36% and 22% of the time respectively. this result shows that even in the case of an incorrect decision, the models often pick parameter values that perform well. in the case of ares, the predicted chunk sizes are far worse that the default or best in most cases. we present this result to show how our approach can be applied to multiple parameters, but will focus the remainder of the paper exclusively on the more accurate models for tuning execution policy."
"the exec policy is the execution policy, a template parameter that determines how the kernel execution will be scheduled onto the hardware. this code is translated to a specific, specialized version of the generic forall method based on the template parameter.the iteration pattern and range of indices can be defined by an indexset object, the iset parameter."
"one way to dynamically select execution policies at runtime would be to use more generic execution methods. however, we found that using a general abstract execution function prevented the c++ compiler from performing some of the static optimizations it could otherwise perform. we developed a version of lulesh where all kernels shared a common openmp execution function meaning that the raja forall method is no longer specialized for every single kernel, and instead only once. the result of this more general implementation was a 30% slowdown that the template-specialized implementation, which is not acceptable for our target scientific applications. we need a framework that allows dynamic policy selection without limiting static code optimization."
"in ares, the sedov blastwave problem is simulated with a full mixed material capability. this adds an additional layer of input-dependent behavior to a code since additional logic is required to correctly simulate this mixed cell. the jet problem is a simple shaped charge deck, and the hotspot problem simulates the ignition of an inertial confinement fusion capsule. both these problems use multiple materials and additional physics simulation capability enabled. this additional physics capability requires a different layout of simulation data in memory and presents more complex patterns of mixed cells throughout the simulation domain."
"there are many types of classifiers in the literature. for apollo, we chose to use a decision tree classifier, which uses a binary tree-like model of decisions to label samples [cit] . to make a prediction for an unseen sample, the tree is evaluated from the root to a leaf, and at each node a comparison of one feature value determines which child node to visit. we use decision trees for two reasons. first, they are comparatively simple, and it is very easy to convert a decision tree model into a set of conditional statements that can be executed at runtime. second, it is easy to prune decision trees to create smaller, less complex models: we simply cut the tree off at a low level and evaluate only the first few levels of conditions. this allows us to consider fewer input features if we need to reduce model evaluation cost. as we add a larger number of tuning parameters to apollo, we may need to consider more complex classifiers."
"the kernel features describe the type of the kernel (e.g., forall), the constraints on the number of iterations, and the index type. instruction features capture the frequency with which specific instruction mnemonics occur within each lambda function, and provide an insight into the character of the kernel. we measure function size by recording the number of instructions in the lambda function that represents the kernel body. we must have instruction counts prior to making a prediction, so we collect them from the application binary using the dyninst library [cit] ."
"apollo leverages raja's separation of concerns. programmers continue to write single-source kernels, but they no longer need to select an optimal execution policy. apollo selects the fastest known template variant of the kernel at runtime, and generates the code to perform the dynamic selection. this provides the performance of statically optimized kernels and selects the best kernel version for the input data. the remainder of this section describes our implementation in detail."
"to cope with this complex tuning problem, we use machine-learning in an off-line training step to generate classifiers that can rapidly predict the fastest parameter values for a kernel at runtime. using data collected from training runs, we train a decision tree classifier, and generate light-weight code for the classifier that can be used on-line to dynamically select tuning parameters for each kernel. to the best of our knowledge, this is the first technique to approach tuning at this granularity with this level of generality and responsiveness."
"to evaluate apollo's tuning capabilities, we apply it to the three applications described in section ii-c: lulesh, cleverleaf, and ares. all three of these applications exhibit input-dependent performance characteristics, and both cleverleaf and ares use adaptive mesh refinement (amr)."
"polysome profile model. after performing the same grid search as for the model trained to predict the mrl of a sequence, the best hyperparameters for the polysome profile cnn were as follows. k-mer linear model. utr sequences were represented as k-mers at each position of the utr. these position-specific k-mers were used as features for training a model via linear regression. 1-mers to 6-mers were tested. training involved regularization to limit overfitting and fivefold cross-validation. the same training and test sets used in building the cnn were used."
"to validate fmri tasks that can be used confidently in clinical practice, we assessed the reliability of activations maps using dice coefficients. a typical issue related to reliability metrics is to establish what a good reliability value is. here, we defined a protocol as valid when it showed a strong withinsubject reliability that is, a higher within-subject map overlap than between-subjects map overlap. whilst in previous research, the group maps were tested, our approach statistically tests how reliable a task is at the subject level. our approach is particularly relevant in the clinical context since overall the same areas are expected to be activated across healthy participants, although in patients nonclassical areas could take over various functions because of a slow growing tumor. assuming that true activations are those that are reliable, then this approach allows deciding if a task is suitable for clinical purposes or not."
"in medicine, magnetic resonance imaging (mri) is typically used to image the structure of organs. mri is however also used to obtain information about perfusion, diffusion, vascularization and physico-chemical state of tissues. functional mri (fmri) is a technique that measures hemodynamic changes after enhanced neural activity [cit], allowing to image non-invasively and with relatively high spatiotemporal resolution, the entire network of brain areas engaged when subjects undertake particular tasks [cit] . soon after its inception, fmri has been used for clinical cases [cit] . nowadays, clinical research using fmri encompasses many areas of neurology, from developmental, psychiatric, and dementia related disorders to strokes and brain tumors [cit] . despite the popularity of fmri in cognitive and clinical research and its proven utility for surgical planning [cit], it is not used extensively in day to day clinical practice. there are four main reasons for this: (i) fmri requires special equipment, (ii) dedicated protocols must be in place, (iii) collected data have to be post-processed to obtain a final image, and (iv) results from analyses must be made available to the clinicians in a usable format."
"the images produced by fmri software and the images used in the clinical environment have different format. this might seem trivial but it is still a major problem. data coming out of the mri scanner are in the dicom format (http://dicom.nema.org/) but researchers using fmri typically convert them to nifti format (neuroimaging informatics technology initiative http://nifti.nimh.nih.gov/) because it has many advantages for research use, and in particular it facilitates interoperability among software. in addition to changing format, data are often deidentified, making the conversion back to dicom and its use on clinical pacs and other tools like neuro-navigation difficult."
"in this fast changing world, different stakeholders will have to work together to develop new educational models to cater for new generations of learners who will be using mobile technologies that do not exist as yet. educators need to re-conceptualize education and make the shift from education at certain ages to lifelong learning [cit] . the current educational model is outdated because it was developed before the advent of information and communication technologies. the current model, based on classroom-based face-to-face delivery, is geared towards educating a certain segment of the population. also, teachers are being trained for the current model of education, and will therefore continue using the model when they become teachers. teacher training must be re-invented to prepare teachers for the technology-enhanced educational system. the current generation of students use 'always-on' technology such as mobile devices, where they need information and feedback 'now' rather than 'later' . because of the flexibility of using mobile devices in learning, students prefer them to personal computers, despite the time needed to get started using mobile devices [cit] ."
"many higher education organizations are implementing mobile learning to provide flexibility in learning [cit] . using mobile technology to reach students will benefit higher education by increasing enrolment and having a broader student population, since students in different age groups will be able to access course materials anywhere and anytime [cit] ."
"as more individuals around the world are using mobile technology to learn and to do everyday tasks, the question is \"what is the future of mobile learning in education?\" in the future, mobile devices will look completely different from today's; hence, higher education must plan to deliver education to meet the demands of new generations of students. we are in the first generation of mobile learning, since it is in its early stage of development. nevertheless, there are billions of mobile devices being used around the world [cit] . the next generation of mobile learning will be more ubiquitous; there will be smart systems everywhere that learners can learn from, and learners themselves will be mobile. learners will learn from multiple sources rather than using one device. also, the next generation of mobile technology will be virtual, with virtual input and output capabilities."
"results indicate that the motor task, the covert word repetition and verb generation are suitable. the overt verb generation showed some degree of reliability, although not for broca area. this task was the same duration as the covert verb generation task, and thus lacked power since sparse sampling was used to allow participants to answer. it is likely that increasing the number of trials would increase its reliability, but total scanning time should also be considered. when working with clinical populations it is essential to have short scanning sessions (each tasks here last ~5 min to make them compatible with busy clinical departments, whilst some tasks in cognitive neuroscience can last up to an hour or more). the landmark task showed a strong activation around the right inferior parietal gyrus at the group level, an area also known to cause hemi-neglect if injured [cit] . yet, no clear activations were observed in this region at the subject level, thus exhibiting poor reliability. this may be explained by a high within-subject scanner noise relative to the present hemodynamic changes as well as the cognitive complexity of the task. activation of the right ipc are observed at the group level because the within-subject variance is discarded. this last result also demonstrates that tasks which have been successfully used in cognitive neuroscience to study some brain regions (at the group level) can be useless for clinical application in individuals. this in turn highlights the need to within-subject reliability analyses to validate protocols."
gg c a ac a t cc t g gg g c ac a a gc t g ga g t ac a a ct a c aa c a gc c a ca ac gt ct at at ca tg gc cg ac aa gc ag aa ga a c gg ca tc aa gg tg aa ct tc aa ga tc cgccacaacatcgaggacggcagcgtgcagctcgccgaccactaccagcag aacacccccatcggcgacggccccgtgctgctgcccgacaaccactacctgagcacccagtccaagctgagcaaag accccaacgagaagcgcgatcacatggtcctgctggagttcgtgaccgccgccgggatcactctcggcatggacga gctgtacaagttcgaataaagctagcgcctcgactgtgccttctagttgccagccatctgttgtttg mcherry library sequence. the sequence has the same defined 5′ end and truncated bgh poly(a) signal sequences as the egfp library.
future mobile learning will shrink the global virtual space. mobile technology can be used to connect students from different parts of the world to create and share information with each other.
"selection of human utr sequences. all human 5′ utr transcripts from the human genome, as annotated by ensembl, were retrieved using biomart 50 . the first 50 nucleotides upstream of the annotated tiss were selected for synthesis, totaling 35,212 sequences."
"design and synthesis of 5′ utr sequences of varying length. random and human 5′ utr sequences of varying length (25 to 100 nucleotides) were synthesized by agilent technologies. all human 5′ utr transcripts from the human genome, as annotated by ensembl, were retrieved using biomart 50 . the first 100 nucleotides upstream of the annotated tiss were selected for synthesis, totaling 17,586 sequences in the length range from 25 to 100 nucleotides. fragments were pcr amplified and cloned into the pet 28 egfp vector as described above."
"in many university hospitals, research centers with fmri equipment are present on site (and even sometimes in or next to the clinical department), and therefore patients can be scanned without the need for transportation to a different location. to ensure good clinical practice, established fmri protocols must however be in place. these protocols must allow the mapping of given brain areas with high specificity. because, there are many possible tasks to map the same brain area [cit] and these have also been developed for group studies, there are no 'off-the-shelf' protocols that can be used to elicit reliable activations at the single subject level. it is therefore mandatory to establish standards to define 'good clinical fmri protocols' and create such protocols to establish fmri as a clinical tool. figure 1 . illustration of hardware setting for fmri: (i) on the left side is shown epi data acquired by an mri scanner with a repetition time of 2.5 sec; (2) in the middle a dedicated computer with specific hardware monitors the scanner data acquisition while presenting stimuli at specified times (3) on the right is a series of stimuli showed inside the scanner using mr compatible goggles, corresponding to our verb generation task. the synchronization between the mr images acquired (left) and the stimuli presented (right) is mandatory to contrast brain images acquired while the patient was seeing words vs. seeing noise stimuli."
reanalysis of library utrs. five thousand egfp library sequences were selected over a range of mrls. these were synthesized and tested via polysome profiling with the rest of the designed sequences.
"superase-in (thermo fisher scientific) and 100 µg ml −1 cycloheximide). wash buffer and lysis buffer were chilled throughout the protocol. after 12 h of growth at 37 °c, cells were placed on ice and medium was aspirated. translating ribosomes were halted by adding 5 ml of wash buffer and cells were then kept at 37 °c for 5 min followed by aspiration on ice. cells were washed by adding 5 ml of wash buffer and aspirating thoroughly. cells were then lysed with 300 µl of ice-cold lysis buffer and scraped from the plates, cell clumps were disrupted by pipetting approximately five times and cells were placed into a prechilled microcentrifuge tube. cells in lysis solution were incubated for 10 min on ice and then triturated by passing through a 25-gauge needle ten times ) and placed on ice for 30 min. lysate was then stored at −80 °c or used directly for polysome profiling."
"of the 12,000 total utrs evolved for targeted expression in the first set, the median mrl for mrl targets of three through eight followed the expected trend from low to high, with low variability within each group. for the utrs in the second set that were evolved stepwise, predicted mrls closely matched the trend of the observed mrls along the trajectory. while we created sequences with high ribosome loading ( supplementary fig. 12e ), in both sets, the prediction from the model and the observed mrl eventually diverged as the model produced utrs with very high predicted mrls. we suspected that the divergence between predictions and measurements at very high mrl values might reflect the unusual sequence composition of the maximally evolved utrs, which often contained multiple long poly(u) sequences that were rarely seen in the random library. we corrected the model by training it (fig. 3d) for four additional iterations with 6,082 utrs from the target mrl sublibrary, which had a much higher frequency of homopolymers, and 2,695 previously unseen random utrs. re-evaluation of held-out sequences from the 'target mrl' library showed a dramatic improvement as compared to the original model (r 2 improved from 0.386 to 0.772) ( fig. 3e and supplementary fig. 13a ), as did the predicted loading of the sequences that evolved stepwise ( fig. 3c and supplementary fig. 12a-d ). using this expanded dataset, we retrained the optimus 5-prime model from fig. 2, which showed increased accuracy with all sublibraries and unchanged performance with random library sequences ( supplementary fig. 13b ). this improvement led us to use the retrained version of optimus 5-prime from this point on."
"for all patients analyzed, the workflow processed the data without difficulties returning reports, lateralization indices for language tasks (when performed), and activation maps. a total of 21 activations maps were analyzed, showing a good correspondence between fmri and des. five patients underwent des and fmri for motor region mapping with 10 sites tested. five patients (four different patients and one also tested for the motor task) underwent des and fmri to map broca area with also 10 sites tested. finally, only 1 patient (one also tested for the motor task) underwent des and fmri to map wernicke area."
"filter activation by utr position. for a given filter, the filter's activation at each utr position was assessed (only the top 100,000 utrs in terms of total read counts were analyzed). genetic algorithm for designing new 5′ utr sequences. the 5′ utr model used for evolving new sequences was trained with a different architecture than the main model that is described above and used throughout the manuscript. this was because sometime after training this first model, we determined that adding a third convolution layer and additional filters to each layer resulted in improved performance."
"polysome fraction processing and next-generation sequencing. fractions of 500 µl corresponding to ribosome peaks including the 40s and 60s peaks were individually collected and processed. five-hundred microliters of trizol (thermo fisher scientific) was added to each fraction and the fractions were vortexed. after incubating at room temperature for 5 min, 100 µl of chloroform was added and the mixture was vortexed and then incubated for 5 min at room temperature. fractions were spun at 13,000 r.p.m. for 10 min and the rna from the supernatant was purified following the protocol for rna clean & concentrator (zymo research). elution was performed with 15 µl of rnase-free water. the purified rna was reverse transcribed using superscript iv (thermo fisher scientific) and gene-specific primers (primer 289 for egfp libraries and primer 220 for mcherry libraries). both reverse transcription primers have unique molecular indices (umis). the products were then amplified with overhangs for illumina-based sequencing; the reverse transcription primers contain barcodes that indicate the polysome fraction from which the reverse transcription product was derived. a custom forward primer for read 1 anneals to the defined 5′ end of the 5′ utr; the mcherry library and the egfp library have the same 5′ end sequence. products were sequenced with the illumina nextseq platform using nextseq 500/550 v2 high output 75 cycle kits."
"to produce the mcherry library the same process as above was performed, with some modifications. the same defined 5′ utr that lies upstream of the randomized 50-nucleotide utr in the egfp construct was used (primer 252). klenow extension with primer 253 created the double-stranded insert that was assembled with the agei-linearized backbone by gibson reaction. the mcherry cds, however, did not have intentionally-placed stop codons. egfp library sequence. bold indicates the defined 5′ end of the 5′ utr. the 50-nucleotide oligomer random utr (non-random utr in the case of the designed library) immediately follows. the underlined sequence corresponds to a truncated bgh poly(a) signal. during in vitro transcription, a 70-nucleotide-long poly(a) tail is added at the 3′ end."
"filter visualization. for each filter, 2,000 8-mers from the egfp 5′ utr library that showed the highest activation were selected. from these, pwms were calculated and used to visualize the sequence compositions that strongly activated each filter. visualization of the second convolution layer involved a wider sequence window (15 bases) and pwms were calculated with fewer k-mers (maximum 200)."
"to validate our approach, we asked whether it captured known aspects of translation regulation. translation initiation is largely dependent on start codons and their context and position relative to a cds 12, 17 . our data clearly showed the expected decrease in ribosome loading for sequences with either out-of-frame upstream start codons (uaugs) (fig. 1c) or upstream open reading frames (uorfs) 18, 19 ( supplementary fig. 2b ). on average, we observed considerably lower use of cug and gug as alternative start codons as compared to aug (fig. 1c and supplementary figs. 3 and 4), in contrast to other reports that have shown widespread usage of non-aug start sites 15, 20, 21 . this difference is possibly due to these alternative start codons being used more often under stress conditions 22 . however, we found that cug and gug start codons could"
"evolution for targeted mrls. we evolved three distinct sets of sequences for targeted expression: sequences without uaugs and upstream stop codons, sequences where uaugs and upstream stop codons were allowed and sequences where uaugs were not allowed but upstream stop codons were. each set evolved initially random sequences to hit mrls of 3, 4, 5, 6, 7, 8 and 9 as well as a maximum value. from these sequences, 200 were selected for mrls 3-7 and 1,000 sequences were selected for mrls 8 and 9 and the maximum mrl value. in total, including the three sequence conditions, 12,000 sequences were synthesized and tested via polysome profiling. in fig. 2b, the predicted values are scaled to the observed values within the data. this creates a discrepancy between the categorical names (x-axis markers) and the predicted mrls, which are the values that should be used for the comparison between observed and predicted data."
"this special section of rusc. universities and knowledge society journal contains five articles. selected after a blind peer-review process, they report on specific mobile learning applications in higher education. brief introductions to the selected articles are given below."
"students can use the mobile telecommunication system to show where they are so that students from other parts of the world can learn about those locations. [cit] conducted a study that examined global learning with students from different cultures using mobile technology. they found that the process of creating, sharing and negotiating provided an opportunity for students to foster relationships and to contextualize their lives to develop shared understandings. the process used to create and share information with different cultures resulted in the development of intercultural competencies and skills to communicate between cultures."
"because of the availability of mobile technology globally, this is the first time in history that educators have had the opportunity to allow individuals from around the world to access educational resources to enable education for all. this is facilitated by many initiatives that are making educational resources available as open educational resources. the increasing availability of open educational resources for mobile technology is making access to learning more affordable for anyone who wants to learn."
"think back 15 years ago. it was hard to imagine that today people would be using mobile technology to learn, to socialize, and to conduct everyday business. many sectors of society have adapted to use mobile technology to deliver services to customers. in the financial sector, customers now have access to banking services using mobile technology -\"in the pocket banking\" [cit] ."
"mobile learning can transform pedagogy to cater for new generations of learners because it offers the opportunity to use active learning strategies and for learners to learn in their own context, which will result in higher-level learning [cit] . with mobile technology, a group of learners can access content from electronic repositories or create their own content, validate the content, and help each other regardless of location. learner-generated content can then be used by other learners [cit] . mobile learning benefits learners because they can use mobile devices to learn in their own learning community, where situated learning, authentic learning, context-aware learning, contingent learning, augmented reality mobile learning and personalized learning are encouraged [cit] . learning will move more and more outside of the classroom and into the learners' environments, both real and virtual, thus becoming more situated, personal, collaborative and lifelong [cit] . mobile technology allows learners from different cultures to express themselves more readily compared to face-to-face situations [cit] ."
"polysome profiling. sucrose gradient buffers contained either 20% or 55% (wt/vol) sucrose as well as 100 mm kcl, 20 mm hepes ph 7.2 and 10 mm mgcl 2 . sucrose (20%, 5.4 ml) was gently layered over 5.4 ml of 55% sucrose in an ultracentrifuge tube. the tube was then sealed with parafilm, placed on its side and left overnight at 4 °c. approximately 2 h before use, the gradient was returned to an upright position. once prepared, cell lysate was layered over the gradient and centrifuged for 3 h at 151,000g using a beckman sw-41 ti rotor. only for mcherry library, a slightly different protocol was applied. specifically, sucrose gradient buffers contained either 7% or 47% (wt/vol) sucrose as well as 150 mm nacl, 20 mm tris-hcl ph 7.2, 5 mm mgcl2 and 1 mm dithiothreitol. sucrose (7%, 5.4 ml) was gently layered over 5.4 ml of 47% sucrose in an ultracentrifuge tube. the tube was centrifuged for 1 h and 45 min at 39,000 r.p.m."
"also, learners can use the technology to develop communities of learners, where learners can tutor and help each other in the learning process, thus resulting in high-level learning."
"the ability to predict the impact of cis-regulatory sequences on gene expression would facilitate discovery in fundamental and applied biology. here we combine polysome profiling of a library of 280,000 randomized 5′ untranslated regions (utrs) with deep learning to build a predictive model that relates human 5′ utr sequence to translation. together with a genetic algorithm, we use the model to engineer new 5′ utrs that accurately direct specified levels of ribosome loading, providing the ability to tune sequences for optimal protein expression. we show that the same approach can be extended to chemically modified rna, an important feature for applications in mrna therapeutics and synthetic biology. we test 35,212 truncated human 5′ utrs and 3,577 naturally occurring variants and show that the model predicts ribosome loading of these sequences. finally, we provide evidence of 45 single-nucleotide variants (snvs) associated with human diseases that substantially change ribosome loading and thus may represent a molecular basis for disease. impact ribosome loading, especially when surrounded by strong sequence context as detailed in supplementary figs. 3 and 4 . the region surrounding the start codon, known as the translation initiation site (tis) or the kozak sequence, is a primary determinant of whether a ribosome will begin translation. we scored the repressive strength of all out-of-frame tiss by finding the mean mrl of sequences with all permutations of nnnaugnn (except where nnn was aug) (fig. 1d) . using the 20 most repressive and 20 least repressive sequences, we calculated nucleotide frequencies for the strongest and weakest tiss. this analysis recapitulated the importance of a purine (a or g) at position −3 relative to the aug and a g at position +4 (refs. 10, 23, 24 ) (fig. 1e) . ultimately, these data suggest that each tis sequence can uniquely tune translation initiation to a fine degree. translation initiation and elongation are also affected by rna secondary structure that forms within 5′ utrs and cdss, with the strongest structures (i.e., lowest free energy) showing the most negative effect on translation 17, 25 . by calculating utr minimum free energy (mfe) 26 and comparing it to utr mrl, we captured and quantified this repressive effect of secondary structure on ribosome load 17, 25 ( supplementary fig. 2c )."
"in vitro transcription. a template for in vitro transcription was produced via pcr of the library plasmid with primers 254 and 255 and kapa hi-fi polymerase (kapa biosystems). the double-stranded dna product has a t7 promoter at the 5′ end and a truncated bgh poly(a) signal sequence followed by a 70-nucleotide poly(a) sequence (introduced with primer 254) at the 3′ end. the ivt reaction used the hiscribe t7 high-yield rna synthesis kit (neb) and 3′-o-me-m 7 g(5′) ppp(5′)g rna cap (neb) was used as the cap structure analog. the dna template was digested with dnase i (neb) and the ivt mrna was purified using rna clean & concentrator (zymo research). this protocol was used to produce the unmodified egfp ivt mrna and mcherry ivt mrna for transfection. for synthesis of individual mrnas for assessment of expression, linear dna templates were assembled containing a t7 polymerase promoter, 5′ utr, coding sequence, 3′ utr and template-encoded poly(a) tail. mrna transcription and purification were carried out as described previously 46 . for mrna libraries containing alternatives to uridine, utp was replaced with pseudouridine-5′-triphosphate or n 1 -methylpseudouridine-5′-triphosphate during transcription. the final mrnas utilized cap1 to increase mrna translation efficiency. after purification, the mrna was diluted in citrate buffer to the desired concentration."
"please select the one below that is the best fit for your research. if you are not sure, read the appropriate sections before making your selection."
"to build a model capable of predicting ribosome loading of human 5′ utr variants and designing new 5′ utrs for targeted expression (fig. 1a), we first created a library with 280,000 gene sequences consisting of a random 5′ utr and a constant region containing the cds for enhanced green fluorescent protein (egfp) and a 3′ utr (fig. 1b) . specifically, the 5′ utr of each construct began with 25 nucleotides of defined sequence used for pcr amplification, followed by 50 nucleotides of fully random sequence before the egfp cds. hek293t cells were transfected with ivt library mrna, cells were collected after 12 h and polysome fractions were then collected and sequenced ( supplementary fig. 1 ). for a given utr, the relative counts per fraction were multiplied by the number of ribosomes associated with each fraction and the resulting values were summed to obtain a measured mean ribosome load (mrl; supplementary note 1). below, we refer to the entire workflow required to measure the mrl of all 5′ utrs in a library, that is, library transfection, polysome profiling, highthroughput sequencing and mrl analysis, as a 'polysome profiling experiment' . we initially focused on the first 50 bases upstream of the cds to specifically investigate the regulatory signals that mediate the initiation of translation beyond ribosomal recruitment to the 5′ cap. intriguingly, variants within the 50-nucleotide window directly adjacent to the start codon are under stronger negative selection than those further upstream 16, providing another motivation to focus on this window."
"from a data handling and format perspective, the pipeline can be split into three parts: (i) obtain raw dicom image, anonymize and convert to nifti; (ii) do the actual data analysis; (iii) transform images back to dicom re-attaching patient's information. from the scanner, identifiable dicom data are stored on a secure server. the data are then de-identified using dicom confidential [cit] and converted to nifti using dcm2nii from the mricron suite [cit] and transferred onto a dataprocessing server. dicom confidential is a policy-driven dicom de-identification toolkit developed in java. it provides great flexibility regarding both what data should be protected and how to do it. although the library comes with a number of ready-made classes that cover the most usual requirements, users can develop their own java classes to accommodate their needs. using dicomconfidential allows us to strip identifiers from the dicom headers and simultaneously generate a mapping file where the patient identification number is linked to a randomly generated pseudoidentifier that substitutes it in the header of mri files. once transferred, the de-identified and converted data (now in nifti format) are processed via a matlab based script that calls dedicated pipelines (one for anatomical images and one per fmri task). each pipeline relies on the psom engine (20) which allows 1to avoid stopping the workflow by capturing errors if any and 2returning reports of the data analysis steps performed. each pipeline relies on spm to analyze the fmri data [cit] and updates a report file initiated in a script. at the end of each pipeline, the result images are 'printed' as series of slice pictures in jpeg format which are then converted to dicom, re-attaching the patients' chi. this last conversion is performed using the jpg2dcm java utility [cit] . all the results are then copied onto a shared nhs/university server."
"for the attentional task, subjects had to decide which of two lines are bigger or smaller. this so-called landmark task is an fmri substitute for the line bisection task used in clinical context. this task tests for the presence of unilateral spatial neglect by asking to mark with a pencil the centre of a series of horizontal lines. here, no significant difference between within-subjects' overlaps and betweensubjects' overlaps were observed (table 4 ). although 10 out of 10 subjects show significant activations in both sessions, locations were variables and show minimal overlaps."
"finally, to aid interpretation of the model we applied visualization techniques developed in computer vision and recently popularized in computational biology 4, 8, 28 . visualization of the filters in the first and second convolution layer revealed recognizable motifs including strong tis sequences (for example, accaug), stop codons (uaa, uga and uag), uorfs, non-canonical start codons (cug and gug) and sequences composed of repeated cg or au elements that are likely involved in the formation of secondary structure ( fig. 2f and supplementary fig. 10 ). of note, several filters did not fall into either of these categories and also did not match previously described position-weight matrices (pwms) for rna-binding proteins (tomtom 29 and the homo sapiens rbp database 30 ), suggesting the possibility for previously undescribed regulatory interactions."
"stepwise evolution of sequences. as a sequence evolved using our algorithm, a new sequence was created if its score is improved relative to its previous state. we recorded the sequences for these steps and tested their performance relative to the model prediction. four distinct conditions were used and 20 utrs for each were evolved, totaling 80 examples of utr stepwise evolution. utrs in the first two conditions were evolved to the highest mrl over 800 iterations; one condition allowed for uaugs and the other did not. the third condition evolved sequences to the lowest mrl over 800 iterations and then changed to select for the highest mrl over 800 iterations while allowing uaugs. the fourth condition was the same as the third except that uaugs were not permitted. in total, beginning with 20 sequences for each condition, 7,526 utrs were generated for analysis."
"all sequence evolutions began with randomized sequences. over a set number of iterations, a single randomly selected base or two bases with a 50% probability, were introduced and the fitness was evaluated using the model. if the new sequence scored higher or closer to the target mrl, then it was accepted; otherwise, the unchanged sequence was selected."
"predicting the effect of human 5′ utr variants on ribosome loading. we next set out to establish whether a model trained only on synthetic sequences can predict how human 5′ utr sequences control translation. assessing model performance on endogenous transcripts is challenging owing to the confounding contributions of 3′ utrs and cdss. as an alternative approach, we synthesized and tested via polysome profiling a 5′ utr library consisting of the first 50 nucleotides preceding the start codon of 35,212 common human transcripts as well as 5′ utr fragments carrying 3,577 variant sequences from the clinvar database 37 that occur within these regions. the same egfp context as the randomized library was used in this alternative approach. the top 25,000 sequences by read coverage, which includes 22,747 common and 2,253 variant sequences, were used for downstream analysis. using the retrained model, we were able to explain 82% of the observed variation in mrl for the common and snv-containing 5′ utr sequences (fig. 4a) . despite being trained on random sequences, the model was able to learn the cis-regulatory rules of human 5′ utr sequences that lie directly upstream of a cds."
translation validation. ten 5′ utr sequences with a wide range of mrls were selected from the egfp library and individually cloned into the same vector as the randomized library. ivt mrna was synthesized and hek293t [cit] and then monitored for egfp fluorescence using an incucyte s3 live-cell analysis system. expression was reported as the maximum egfp fluorescence over a 20.5-h time window.
g g g ac a t cg t a ga g a gt c g ta c t ta ( n5 0) at gc ct cc cg ag aa ga ag at ca ag ag cg tg ag ca ag gg cg ag ga gg at aa ca tg gc ca tc at ca ag ga gt tc at gc gc tt ca ag gt gc ac at gg ag gg ct cc gt ga ac gg cc ac ga gt tc ga ga tc ga gg gc ga gg gc ga gg gc cg cc cc ta cg ag gg ca cc ca ga cc gc ca ag ct ga a g gt ga cc aa gg gt gg cc cc ct gc cc tt cg cc tg gg ac at cc tg tc cc ct ca gt tc at gt ac gg ct cc aa gg cc ta cg tg aa gc ac cc cg cc ga ca tc cc cg ac ta ct tg aa gc tg tc ct tc cc cg ag gg ct tc aa gt gg ga gc gc gt ga tg aa ct tc ga gg ac gg cg gc gt gg tg ac cg tg ac cc ag ga ct cc tc cc tg ca gg ac gg cg ag tt ca tc ta ca ag gt ga ag ct gc gc gg ca cc aa ct tc cc ct cc ga cg gc cc cg ta at gc ag aa ga ag ac ca tggg ct gg ga gg cc tc ct cc ga gc gg at gt ac cc cg ag ga cg gc gc cc tg aagggcgagatcaagcagaggctgaagctgaaggacggcggccactacg acgctgaggtcaagaccacctacaaggccaagaagcccgtgcagctgcccggcgcctacaacgtcaa catcaagttggacatcacctcccacaacgaggactacaccatcgtggaacagtacgaacgcgcc gagggccgccactccaccggcggcatggacgagctgtacaagtcttaacgcctcgactgtgccttctag ttgccagccatctgttgtttg
"mobile technologies are becoming more personal with the introduction of gesture-based interaction and affective computing. devices can interpret gestures made by learners and respond appropriately based on the gesture. when a learner holds a mobile device, the device will read the physiological state of the learning to detect the learner's emotions. based on the emotion of the learner, the device will decide on what the learner should do next. because of the computing power and multimedia capabilities of mobile technologies, educational resources must be more game-like to motivate learners to learn."
"we would like to thank a. rosenberg and j. linder for helpful discussions on data analysis and modeling. we would also like to thank m. moore, a. hsieh and y. lim for constructive comments on the manuscript. we are grateful to c. wang for providing fluorescence data 27 . this work was supported by a sponsored research agreement by moderna and national institutes of health grant r01hg009892 to g.s."
"a description of all covariates tested a description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons a full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) and variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)"
"fmri delineates areas of the brain involved in motor or cognitive functions (so called eloquent areas) by asking patients to perform different tasks whilst image time-series are acquired. for motor related areas, a simple finger tapping (mapping the primary motor cortex) or more complex finger sequences (mapping the premotor cortex) may, for instance, be performed. for language areas, visual or auditory stimuli are presented whilst scanning, and patients perform different tasks such as reading, listening, repeating, etc. all patients must perform several trials, and crucially these trials must be synchronized with image acquisition. the tight coupling between stimulus presentation, task and image acquisition is mandatory for the statistical analysis, contrasting task periods versus rest periods, or contrasting different tasks periods against each other ( figure 1 ). this implies that mri compatible equipment is available to interface between the scanner and the software used to design the tasks. the fmri hardware, is also used to deliver instructions to the patient via mri compatible headphones, screen, goggles, etc. and possibly also to record behavioral responses (via e.g. microphone, response pads), all of this in phase with the image acquisition. typically, such equipment is available in research centers but not hospitals, constituting an obstacle to day-to-day application of fmri."
"mobile learning facilitates equal opportunities for all by allowing learning to be accessible across time zones, thus making location and distance irrelevant to the learner. wireless mobile devices are small enough to be portable, which allow learners to use them anywhere and anytime to interact with other learners everywhere to share information and expertise, complete a task or work collaboratively on a project. workers in organizations can use mobile devices to learn on the job so that they can transfer what they learn in the school system to the job. one example is the use of mobile devices to train workers to improve their communication skills in the workplace so that they can be productive on the job [cit] experts in the field of their studies. situated learning, which is the application of knowledge and skills to specific contexts, is facilitated, since learners can complete courses while working on the job or in their own space, and apply what they learn at the same time."
"the method developed here, which combines polysome profiling of a randomized 5′ utr library with deep learning, has provided a wealth of information detailing the relationship between the 5′ utr sequence preceding a cds and regulation of translation. the data and model enabled quantitative assessment of secondary structure, uaugs and uorfs, kozak sequences and other cis-regulatory sequence elements in the context of unmodified mrna and ψ-and m 1 ψ-modified mrna. optimus 5-prime, the cnn trained on the data, has excellent performance, explaining up to 93% of mrl variation in the test set and up to 82% of variation for truncated human utrs. in future work, this approach could be further generalized to include the impact of the mrna 5′ terminus including the 5′ cap structure, and even 3′ utr sequence on ribosome loading. our model also proved capable of predicting the effect of disease-relevant 5′ utr variants on translation, even suggesting mechanisms of action. of note, predictions are not limited to common variants or even to those that have been previously described; instead, the model can be used to screen every possible snv, insertion or deletion in the 100 bases upstream of a start codon, of which there are millions in the human genome, and select for further study those that have the strongest impact on ribosome loading and thus have the highest likelihood of being pathogenic. finally, using optimus 5-prime and a genetic algorithm, we were able to engineer new 5′ utr sequences for targeted ribosome loading, enabling applications in synthetic biology and precision medicine that are even more forward looking."
"t he sequence of the 5′ utr is a primary determinant of translation efficiency 1, 2 . while many cis-regulatory elements within human 5′ utrs have been characterized individually, the field still lacks a means to accurately predict protein expression from 5′ utr sequence alone, limiting the ability to estimate the effects of genome-encoded variants and the ability to engineer 5′ utrs for precise translation control. massively parallel reporter assays (mpras; methods that assess thousands to millions of sequence variants in a single experiment) coupled with machine learning have proven useful in addressing similar challenges by producing quantitative biological insight that would be difficult to obtain through traditional approaches [cit] . earlier mpras designed to learn aspects of 5′ utr cis regulation relied on fluorescence-activated cell sorting 10, 11 or growth selection 12 to stratify libraries by activity. these techniques require the expression of a single library variant per cell, which must be transcribed within the cell from a dna template, making it difficult to distinguish between the effects of transcriptional and translational control. polysome profiling 13 overcomes this limitation by enabling single cells to translate tens to hundreds of in vitro-transcribed (ivt) and transfected mrna variants. polysome profiling has been used extensively to measure translation of native rna isoforms 14, 15, but isolating the role of 5′ utr regulation has been difficult owing to differences in the size and sequence of the concomitant coding sequences (cdss) and 3′ utrs."
"any methods, additional references, nature research reporting summaries, source data, statements of code and data availability and associated accession codes are available at https://doi.org/10.1038/ s41587-019-0164-5."
"sequence processing. raw sequence files, separated by their fraction-associated barcodes, were processed with cutadapt 47, outputting the 50-nucleotide utr and 9-15 nucleotides corrsponding to the n-terminal region of the cds. utrs were clustered and umis were counted using bartender 48 . the egfp library contained approximately 750,000 unique sequences and the mcherry library contained approximately 500,000 sequences. utrs were removed if the cds sequence did not match the intended sequence. because many of the remaining sequences had very few reads, we took the top 280,000 sequences for the egfp library and the top 200,000 sequences for the mcherry library. no sequences in the egfp and mcherry libraries matched. to normalize differences in total read counts between fractions, relative reads were calculated within each fraction. using these values, the relative distribution of reads for each utr across the fractions was determined. mrl was calculated by multiplying each fraction's relative distribution of reads by the number of ribosomes associated with each fraction and these values were summed (supplementary note 1)."
"a fully automated analysis workflow ( figure 2 ) was created to analyze tasks showing high withinsubject reliability: (1) motor task showing activations for the hand, foot and lip areas, (2) auditory language task showing activations of the auditory cortex and wernicke area, (3) the verb generation task showing activations of the supplementary motor area and broca area. the workflow relies on multiple open source software packages as well as matlab®."
synthesis of designed 50-nucleotide sequences. all designed and human 5′ utr sequences were synthesized by customarray. fragments were pcr amplified and cloned into the pet 28 egfp vector as described above.
"in \"a comparative study of computer and mobile phone-mediated collaboration: the case of university students in japan\", gibran alejandro garcia compared how two types of media influence the participation, interaction and collaboration of students. it inquired into the students' collaboration experiences, opinions, and difficulties they encountered during the online discussions. then it explored the impact that these two types of media had on the students' final outcome. the study concluded that mobile phones had great potential to enhance interaction in online collaboration."
"to determine whether optimus 5-prime would generalize to other coding sequences, we built a separate degenerate 5′ utr mrna library with an mcherry cds replacing the cds for egfp. following the polysome profiling and modeling procedure described above, we found that the model, although only trained on the egfp library, still performed well, explaining 77% and 78% of the variation in mrl from two independent polysome profiling experiments performed with this new reporter library ( supplementary fig. 5 ). the decrease in accuracy is explained in part by differences between the protocols for egfp and mcherry polysome profiling (methods)."
"in the future, more research should be conducted to transform education using mobile learning. [cit] suggested that there is a need for more rigorous research on the use of mobile technology in learning to enhance the use of mobile learning in education. also, there is a need for more extensive quantitative and qualitative research studies on mobile learning to advance the implementation of mobile learning in the 21st century [cit]"
"the use of a dedicated analysis workflow in the clinical context demonstrates the feasibility of clinical fmri with all patients successfully, automatically analyzed. in most cases, valuable information was gained from thresholded maps offering computer-aided medical decision. in some instances, looking at an unthresholded map may be useful to find additional areas, and such cases point to the need to still have an fmri expert to go over the report and check activation maps, in liaison with the radiologist and neurosurgeon."
"having previously developed a set of tasks suitable for patients [cit], we present here (i) a validation of those protocols, showing higher within than between-subjects reliability and (ii) an automated analysis pipeline from data transfer to reporting, fitting with the busy day-to-day clinical practice. pipeline analysis and optimization can take many different forms, but this is out of the scope of this article. we focus here on the implementation of such pipeline using open source software and the clinical validity of the results obtained."
"libraries are being digitized and information formatted for access using mobile technology -\"a what is the future of mobile learning in education? library in everyone's pocket\" [cit] . the healthcare system is also employing mobile technologies to deliver training to healthcare professionals and services to patients [cit] . with communication technology, learners can use mobile technology anywhere and anytime to access educational resources [cit] ."
"randomized 50-nucleotide oligomer 5′ utr library. a vector (pet 28) encoding a t7 promoter followed by 25 nucleotides of a defined 5′ utr (gggacatcgtagagagtcgtactta) and the egfp cds was linearized with agei to allow for insertion of the 5′ utr library between the defined sequence and the cds. the defined 25-nucleotide sequence allows for pcr amplification after reverse transcription. two nucleotides were changed at positions +11 (c to a) and +14 (c to t) in the egfp cds to introduce stop codons (taa) in frame −1 and −2 relative to the atg. the oligonucleotide (supplementary table 5, primer 282) that was used for library insertion contains the defined 5′ utr, followed by 50 nucleotides of randomized bases and 21 nucleotides that overlap the egfp cds (including the atg start site) (idt). a reverse primer (primer 283) complementary to the 21-nucleotide egfp overlap was used to produce a double-stranded product via klenow extension with klenow polymerase i (neb). the vector and insert were assembled by gibson reaction (neb) and the product was electroporated into 5-alpha electrocompetent escherichia coli (neb). a small portion of the electroporated bacteria was plated and resulted in ~750,000 colonyforming units, and the rest was grown in liquid culture overnight (all bacteria were grown under kanamycin selection). the isolated plasmid is the egfp library."
"there was a good correspondence between fmri and des (only 3/21 discrepancies), which suggests that it might be possible in the future to substitute fmri for des. indeed, despite being the gold standard, des does not allow the surgeons to draw unequivocal conclusions about the role of stimulated areas' [cit], just like fmri. fmri has however the advantage of being non-invasive, and to possibly reveal cases where surgery is too risky (eloquent areas around the tumor or even inside the tumor) thus providing invaluable information to surgeons and to patients alike."
"for null hypothesis testing, the test statistic (e.g. f, t, r) with confidence intervals, effect sizes, degrees of freedom and p value noted for manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers. we strongly encourage code deposition in a community repository (e.g. github). see the nature research guidelines for submitting code & software for further information."
"the use of mobile technologies is changing the way we live and how we access education. one clear development is a blurring of our social, business, learning and educational lives as the pattern of our communication and interaction across time and space changes [cit] . countries around the world are starting to see that internet access anywhere and anytime is a human right for citizens and have set goals to establish the infrastructure to allow access by all, which will facilitate the use of mobile technology in education [cit] . there is great potential for mobile learning in developing countries, but careful planning is required for mobile learning to be successful [cit] ."
here we report the development of an mpra that measures the translation of hundreds of thousands of randomized 5′ utrs via polysome profiling and rna sequencing. we then use the data to train a convolutional neural network (cnn) that can predict ribosome loading from sequence alone.
"the use of mobile technology allows for cloud teaching where access to people, resources and information will float freely regardless of location [cit] . learners in different time zones and locations will be able to access tutors when needed. according to a futurelab report [cit], digital technology will be embedded and distributed in most objects. personal artefacts such as keys, clothes, shoes, notebooks and newspapers will have devices embedded within them, which can communicate with each other [cit] . this will make learning more ubiquitous and pervasive."
"convolution neural network for mrl prediction. all code was written in python 2.7 and all neural network development was done using the keras (https://keras. io) and tensorflow backends 49 . for hyperparameter selection, the top 50,000 sequences, in terms of total read counts per utr, were used. we performed a tenfold cross-validation grid search to exhaustively test hyperparameter combinations of convolution layers (2, 3), convolution filter lengths (8, 10, 12), number of convolution filters (40, 80, 120), number of nodes in the dense layer (40, 80, 120) and dropout probability between all layers (0, 0.2, 0.4). the best hyperparameter combinations were as follows."
"assuming that such protocols are in place, and can be run by trained radiographers, the data must be processed before reporting because, in contrast to standard structural imaging (e.g. t1 or t2 weighted images), there is no direct output from the scanner. although scanner manufacturers offer fmri acquisition mode, which in theory allow obtaining results after a scanning session, the schemas are extremely rigid and do not fit modern complex protocols. off-line analyses must therefore take place, and this can take from half an hour to several hours depending on the length of the processing pipeline, number of tasks performed, the complexity of the analyses, and the hardware used. such analysis also requires expert knowledge. together, these constitute another strong deterrent to clinical fmri. we believe that this complexity in data analysis can be overcome by creating automated analysis workflows that (i) allow checking data quality and analysis and, (ii) output 'ready-to-use' reports and images."
"-accession codes, unique identifiers, or web links for publicly available datasets -a list of figures that have associated raw data -a description of any restrictions on data availability"
"mobile learning is not about the technology, it is about the learner. the learner is mobile and is at the centre of the learning, and the technology allows the learner to learn in any context. [cit] state that mobile learning is a social rather than technical phenomenon of people on the move, constructing spontaneous learning contexts and advancing through everyday life by negotiating knowledge and meanings through interactions with settings, people and technology."
"the histograms for all periods share the same range [l, u], where l and u are the lowest and highest ghg emissions (or travel times) observed in ri. thus, ⌈ ti α ⌉ isomorphic histograms are obtained for each edge."
"the semantics of fta is given in terms of featured transition systems (ftss) [cit] . an fts extends labelled transition systems with a set of features f, a feature model fm, and a total function that assigns fe to transitions."
"when translating ifta to fta with committed states, the complexity of the model grows quickly. for example, the ifta of a simple replicator with 3 output ports consists of a location and 8 transitions, while its corresponding fta consists of 23 locations and 38 transitions. without any support for composing variable connectors, modelling all possible cases is error prone and it quickly becomes unmanageable. this simplicity in design achieved through multi-action transitions leads to a more e cient approach to translate ifta to uppaal ta, in particular by using the composition of ifta. the ifta resulting from composing a network of ifta, can be simply converted to an fta by flattening the set of actions in to a single action, and later into an uppaal ta."
"we now introduce two operations: product and synchronisation, which are used to define the composition of ifta. the product operation for ifta, unlike the classical product of timed automata, is defined over ifta with disjoint sets of actions and clocks, performing their transitions in an interleaving fashion."
"notation: we write a.l, a.l 0, a.a, etc., to denote the locations, initial locations, actions, etc., of an fta a, respectively. we write`1 cc,a,c !`2 to denote that (`1, cc, a, c,`2) 2 a.e, whenever automaton a is clear from the context."
"rather than estimating a single value for a traversal of a route, e.g., the expected ghg emissions, we estimate the ghg emissions distribution using histograms. this yields much more detailed information than a single value and is useful in many applications, e.g., stochastic route planing [cit] and probabilistic thresholdbased routing [cit] ."
"for each edge, we build time dependent eco-weight histograms. if an edge is not covered by at least 1, 000 gps records, a ghg emissions value ev is derived based on the length and the speed limit of the edge, which can be obtained from openstreetmap. thus, the edge is associated with a single histogram with only one bucket, indicating an ev with probability 1."
"assess -analyzes requests. when a request is ready to be assessed (assess), a clock is reseted to track the processing elapsed time (tp). authorities have 90 days to make a decision of weather accept it (accept) or reject it (reject)."
"the paper makes four contributions. first, we propose an eco road network as a foundation for enabling eco-routing. second, compact, time-dependent histograms are proposed to represent the time-dependent, uncertain eco-weights for all edges in a compact manner. third, methods that are able to estimate ghg emissions of routes in the eco road network are proposed. fourth, experiments are conducted to provide insight into the efficiency and accuracy of the paper's proposals."
"we further study the efficiency of aggregating histograms. we use a set of 1, 012 routes that each is traversed by at least 1, 000 trajectories. on average, a route covers 16 edges. figure 6(a) shows the performance of our histogram aggregation method for routes with different numbers of edges."
"reo is a channel-based exogenous coordination language where complex coordinators, called connectors, are compositionally built out of simpler ones, called channels [cit] . exogenous coordination facilitates anonymous communication of components. each connector has a set of input and output ports, and a formal semantics of how data flows from the inputs to the outputs. we abstract from the notion of data and rather concentrate on how execution of actions associated to input ports enables execution of actions associated to output ports. table 1 shows examples of basic reo connectors and their corresponding ifta. for example, merger (i 1, i 2, o) synchronises each input port, separately, with the output port, and fifo1 (i, o) introduces the notion of delay by executing its input while transitions to a state where time can pass, enabling the execution of its output without time restrictions."
"we use a set of reo connectors to integrate these ifta. the final integrated model can be seen in figure 5 . for simplicity, we omit the feature expressions associated to ports and the resulting feature model. broadly, we can identify two new components in this figure: payment -(right of app) orchestrates payment requests based on the presence of payment methods. it is composed by componets cc, pp, and a set of connectors. a router synchronises payment requests (payapp) with payment by cc or paypal (paypp or paycc). a merger synchronises the successful response (paidpp or paidcc), while other merger synchronises the cancellation response (cancelpp or cancelcc) from either cc or pp. compositionality and modularity of spls. an extension to petri nets, feature nets (fns) [cit] enables specifying the behavior of an spl in a single model, and supports composition of fns by applying deltas fns to core fns. an extension to ccs process calculus consisting on a modular approach to modelling and verifying variability of spls based on deltaccs [cit] . resulting in models of features that can be reused easily. a compositional approach for verification of software product lines [cit] where new features and variability may be added incrementally, specified as finite state machines with variability information."
"to address this issue, this paper proposes interface fta (ifta), a mechanism enriching fta with (1) interfaces that restrict the way multiple automata interact, and (2) transitions labelled with multiple actions that simplify the design. interfaces are synchronisation actions that can be linked with interfaces from other automata when composing automata in parallel. ifta can be composed by combining their feature models and linking interfaces, imposing new restrictions over them. the resulting ifta can be exported to the uppaal real-time model checker to verify temporal properties, using either a network of parallel automata in uppaal, or by flattening the composed automata into a single one. the latter is better suited for ifta with many multiple actions."
", where hi.c is the total number of cost values that are used to derive hi, which is equivalent to the number of traversal records in the i-th period. the probability value for the k-th bucket in h is given by equation 2."
"the sum of squared error (sse ) is employed to measure the discrepancy between the original histogram h and the histogram after bucket reduction h. since the error is only introduced by the buckets we merge and we merge only two adjacent buckets bi and bi+1, error introduced by this merge operation is given by equation 3."
"appeal -handles appeal requests. when an appeal is received (appeal), a clock is reseted to track the appeal submission elapsed time (tas). authorities have 20 days (appeal .inv (`1 )) to start assessing the request (assessapl)."
"a few recent studies consider how to derive weights using gps data. one study [cit] covers the estimation of single-valued ecoweights for edges with infrequent or no gps records. however, it is unable to capture time-dependence and uncertainty. orthogonal to this study, we assume a setting where edges have considerable amounts of gps records, and we focus on capturing detailed ghg emissions distributions during different intervals for the edges. another recent study [cit] concerns the update of near-future (e.g., the next 15-min or 30-min) eco-weights based on incoming real-time gps data. in contrast, we capture time-dependent ghg emissions for longer periods, e.g., a day, a week, or a month, based on historical gps data. as a result, our work is complementary to that study."
"together, the product and the synchronisation can be used to obtain in a compositional way, a complex ifta modelling spls built out of primitive ifta."
"eco-routing is an easy-to-employ and effective approach to reducing ghg emissions from transportation. given a source-destination pair, eco-routing returns the most environmentally friendly route, i.e., the route that produces the least ghg emissions [cit] . the literature reports that eco-routing can yield some 8-20% reductions in ghg emissions from road transportation [cit] . * supported by the reduction project that is funded by the european commission as fp7 [cit] -7 strep project no. 288254."
"assuming α is set to 1 hour, figure 2 (a) shows two isomorphic histograms during periods [8 a.m., 9 a.m.) and [9 a.m., 10 a.m.) for an edge in north jutland, denmark. the high similarity of the two histograms motivates us to compress them into one histogram with little loss of information. we proceed to show how to compress the histograms with good accuracy using histogram merging and bucket reduction."
"cc and pp -handle payments through credit cards and paypal, respectively. if a user requests to pay by credit card (paycc) or paypal (paypp), a clock is reset to track payment elapsed time (tocc and topp). the user has 1 day (cc .inv (`1 ) and pp .inv (`1 )) to proceed with the payment which can result in success (paidcc and paidpp) or cancellation (cancelcc and cancelpp)."
"definition 2 (feature expressions (fe) and satisfaction). a feature expression ' over a set of features f, written ' 2 f e(f ), is defined as follows"
"we use a large gps tracking data set containing more than 200 million gps records collected from 150 [cit] . the sampling frequency is 1 hz, i.e., one gps record per second. we use the road network of denmark from openstreetmap 1 . to get the best map-matching we extract edges from openstreetmap data with the finest granularity, with 2,358k vertices and 2,520k edges. we apply an existing map-matching tool [cit] to match the gps records to the road network, from which we get a set of trajectories tr. we merge the adjacent edges into longer ones if they can be viewed as conceptually a single edge. the resulting road work that we proceed to use has around 414k vertices and 1, 628k edges. figure 3 shows the distribution of the lengths of the edges in the road network and the routes derived from our gps data."
"merge the buckets pair in minpairbuckets; 10: return tdh; algorithm 1 works iteratively. for each iteration, it enumerates all adjacent buckets pairs and finds the pair that achieves the smallest sse to merge (lines 2-9). note that to identify two buckets that need to be merged, every histogram in the given tdh has to be checked. this process terminates when the total number of buckets for the edge is below the reduction threshold t red ."
"preassess -checks if a request contains all required documents. when a request is received (submit), a clock is reseted to track the submission elapsed time (ts). authorities have 20 days (preasses.inv (`1 )) to check the completeness of the documents and notify whether it is incomplete (incomplete) or ready to assessed (assessapp)."
"definition 11 (timed bisimulation). given two ftss f 1 and f 2, we say r ✓ f 1 .s⇥f 2 .s is a bisimulation, if and only if, for all possible feature selections"
"to study the approximation accuracy of our histograms, we measure the distance between the original data distribution and the derived histogram representations, including initial equi-width histograms, histograms after merging, and histograms after bucket reduction."
"the greenhouse effect is due to the concentration of greenhouse gases (ghg) in the earth's atmosphere, which prevents heat from escaping into space. the combustion of fossil fuels results in ghg emissions, and transportation is a prominent fossil fuels burning sector. thus, reducing the ghg emissions from transportation is crucial in combating global warming."
"definition 1 (clock constraints (cc), valuation, and satisfaction). a clock constraint over a set of clocks c, written g 2 cc(c) is defined as follows"
"multiple ftas can be composed and executed in parallel, using synchronising actions to synchronise edges from di↵erent parallel automata. this section introduces interfaces to fta that: (1) makes this implicit notion of communication more explicit, and (2) allows multiple actions to be executed atomically in a transition. synchronisation actions are lifted to so-called ports, which correspond to actions that can be linked with actions from other automata. hence composition of ifta is made by linking ports and by combining their variability models."
"as future work, it is of interest to study the correlations between the ghg emissions from adjacent edges. modeling such correlation may increase the accuracy when estimating ghg emissions distributions for routes. next, it is also of interest to explore advanced routing algorithms that can fully utilize the time-dependent, uncertain eco-weights, e.g., to compute probabilistic eco-routes. finally, using an inverted approach that assigns a time dependent histogram based eco-weight to a group of edges that have similar travel cost distributions may result in further storage space reductions."
"this paper introduced ifta, a formalism for modelling spl in a modular and compositional manner, which extends fta with variable interfaces to restrict the way automata can be composed, and with multi-action transitions that simplify the design. a set of reo connectors were modeled as ifta and used to orchestrate the way various automata connect. we discussed a prototype tool to specify and manipulate ifta, which takes advantage of ifta composition to translate them into ta that can be verified using the uppaal model checker."
"app -models licenses requests. an applicant must submit the required documents (subdocs), and pay a fee (payapp) if pa is present, before submitting (submit). if the request is accepted (accept) or considered incomplete (incomplete), the request is closed. if it is rejected (reject) and it is not possible to appeal (¬apl ), the request is closed, otherwise a clock (tapl ) is reseted to track the appeal window time. the applicant has 31 days to appeal (app.inv (`5 )), otherwise the request is canceled (cancelapp) and closed. if an appeal is submitted (appeal), it can be rejected or accepted, and the request is closed."
"software product lines (spls) enable the definition of families of systems where all members share a high percentage of common features while they di↵er in others. among several formalisms developed to support spls, featured timed automata (fta) [cit] model families of real-time systems in a single model. this enables the verification of the entire spl instead of product-by-product. however, fta still need more modular and compositional techniques well suited to spl-based development."
"next, we consider the 1, 012 trajectories from section 6.3 to evaluate the accuracy of histogram aggregation. we split the trajectories into two sets: training trajectories (from the first 18 months) and testing trajectories (from the last 6 months). for each route, we aggregate the histograms from the ern to estimate the ghg emissions histogram. we then generate ground-truth ghg emissions histograms for the route in each time interval without any data compression using the testing trajectories. the accuracy of histogram aggregation is defined as the histogram similarity (hsimilarity) between the estimated histogram and the ground-truth histogram, using equation 1. figure 6(b) shows the accuracy of our histogram aggregation method with varying the number of edges in a route."
"the rest of this paper is structured as follows. section 2 presents some background on fta. section 3 introduces ifta. section 4 presents a set of reo connectors modeled as ifta. section 5 discusses a prototype tool to specify and manipulate ifta. section 6 presents the case study. section 7 discusses related work, and section 8 concludes."
"for the continent-wide road network of north america with 52 m edges [cit], table 2 also shows the estimated storage usage of the different methods when there is sufficient gps data. this suggests that our compact representation of time-dependent histograms reduces the storage substantially while maintaining good accuracy."
"the remainder of this paper is organized as follows. section 2 reviews related work, and section 3 covers preliminaries and formalizes the problem. in sections 4 and 5, the methods for building and using the eco road network with histogram-based eco-weights are proposed. section 6 reports on the experimental results, and section 7 concludes."
"according to a recent benchmark on vehicular environmental impact models [cit], environmental costs of traversing edges can be derived from gps data using vehicular environmental impact models. in this paper, we study how to obtain time-dependent, uncertain eco-weights for edges from a large gps tracking data set."
"given two distributions, several techniques exist to measure their similarity, such as cosine similarity, the k-s test, and the chi-square test. the simplicity and efficiency of computing cosine similarity makes it appropriate for evaluating the similarity of two histograms."
"the semantics of ifta is given in terms of ftss, similarly to the semantics of fta with the di↵erence that transitions are now labelled with sets of actions. we formalize this as follows."
"two states s 1 2 s 1 and s 2 2 s 2 are bisimilar, written s 1 ⇠ s 2, if there exists a bisimulation relation containing the pair (s 1, s 2 ). given two ifta a 1 and a 2, we say they are bisimilar, written a 1 ⇠ a 2, if there exists a bisimulation relation containing the initial states of their corresponding ftss."
"given an initial time-dependent histogram tdh for an edge, a corresponding merged time-dependent histogram tdh is computed iteratively. in each iteration, a pair of adjacent histograms with the highest histogram similarity is identified. if the similarity exceeds a user-defined threshold tmerge, the two histograms are merged according to equation 2, and the union of the two argument histograms' periods becomes the period of the new histogram. the iteration terminates when tmerge exceeds the highest histogram similarity. for example, the two histograms for adjacent periods shown in figure 2"
"a single-valued edge weight typically cannot fully capture the environmental cost of traversing an edge. for instance, while traversing an edge, aggressive drivers may generate more ghg emissions than average drivers. thus, emissions vary across drivers, and an uncertain eco-weight that records the distribution of the cost of traversing an edge captures reality better. further, eco-weights are generally time dependent, due to the temporal variation in traffic. for instance, during peak hours, traversing an edge normally produces more ghg emissions than during off-peak hours."
"a nifta can be converted into a nfta with committed states, which in turn can be converted into a network of uppaal ta, through a stepwise conversion, as follows. nifta to nfta. informally, this is achieved by converting each transition with set of actions into to a set of transitions with single actions. all transitions in this set must execute atomically (committed states between them) and support all combinations of execution of the actions. nfta to up-paal nta. first, the nfta obtained in the previous step is translated into a network of uppaal ta, where features are encoded as boolean variables, and transition's feature expressions as logical guards over boolean variables. second, the feature model of the network is solved using a sat solver to find the set of valid feature selections. this set is encoded as a ta with an initial committed location and outgoing transitions to new locations for each element in the set. each transition initializes the set of variables of a valid feature selection. the initial committed state ensures a feature selection is made before any other transition is taken."
"we illustrate the applicability of ifta with a case study from the electronic government (e-government) domain, in particular, a family of licensing services. this services are present in most local governments, who are responsible for assessing requests and issuing licenses of various types. e.g., for providing public transport services, driving, construction, etc. such services comprise a number of common functionality while they di↵er in a number of features, mostly due to specific local regulations."
"after histogram merging and bucket reduction, we obtain an ern where each edge is associated with a compact time dependent histogram. here, we study how to use the obtained ern to estimate the ghg emissions of traversing a route."
"the synchronisation operation over an ifta a connects and synchronises two actions a and b from a.a. the resulting automaton has transitions without neither a and b, nor both a and b. the latter become internal transitions."
"we evaluate the storage savings that can be obtained by using histogram merging and bucket reduction. a bucket in a histogram requires two integer values (i.e., 8 bytes) to indicate the lower and upper bound of the bucket and a double (i.e., 8 bytes) for the bucket probability."
"lets consider any action transition h(`1,`0), ⌘i s ! h(`0 1,`0), ⌘ 0 i 2 f 1 .e. if it comes from a transition in (1), then 9`1 g,s,r !`0 1 2 a.e s.t. a 6 2 s,"
"given a merged time dependent histogram tdh of an edge and a reduction threshold t red indicating the total number of buckets available for the edge, algorithm 1 describes how to obtain a time dependent histogram that meets the storage budget while maintaining good accuracy. note that for different edges, the reduction threshold t red, i.e., the bucket budget, may be different. a simple heuristic is to assign higher bucket quotas to edges that have many merged histograms in their time dependent histograms after histogram merging. an edge with more histograms requires more buckets to retain its distribution information, so more buckets should be assigned to it."
"is the invariant, a partial function that assigns ccs to locations, fm 2 fe (f ) is a feature model defined as a boolean formula over features in f, and : e ! fe (f ) is a total function that assigns feature expressions to edges."
"the finest temporal granularity α is set to 15 minutes as the default, and the merge threshold tmerge and the reduction threshold t red used are shown in table 1 . the experiments focus on the values shown in bold. the algorithms for histogram merging and bucket reduction are implemented in python, which is suitable for scientific and numeric computations. a machine with a 16-core intel xeon 2ghz cpu, 32gb main memory, and 4tb external memory is used."
"definition 10 (composition of ifta). given two disjoint ifta, a 1 and a 2, with possible shared features, and a set of bindings to study properties of ifta operations, we define the notion of ifta equivalence in terms of bisimulation over their underlying ftss. we formally introduce the notion of timed bisimulation adapted to ftss."
future work includes studying further properties of ifta and the impact of integrating this notion of composition and variability with well known specification and refinement theories for i/o automata.
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. vehicle routing generally relies on a weighted-graph representation of a road network. the vertices and edges represent road intersections and road segments, respectively. the key to enabling effective eco-routing is to assign eco-weights to the edges that accurately capture the environmental costs (i.e., ghg emissions or fuel consumption) of traversing the edges. based on the resulting weighted graph and the types of weights, e.g., single-value weights, time-dependent weights, or uncertain weights, existing routing algorithms [cit] can be applied to enable eco-routing."
"figure 4(a) shows the running time to build equi-width histograms and to merge the histograms for an edge. the time increases as the number of gps records associated with an edge increases. thus, if an edge is covered by more gps records, it takes longer to build the initial histograms. figure 5(a) show the running time of bucket reduction using 6, 000 traversal records, which is the maximum number of records that an edge has in our dataset."
"delegating coordination aspects to connectors enables separation of concerns. each automata can be designed to be modular and cohesive, facilitating the maintenance, adaptability, and extension of an spl. in particular, facilitates the replacement of components and enables changes in the way they interact without a↵ecting their internal definition. using bare fta for designing variable connectors, can be error prone and it quickly becomes unmanageable. ifta simplifies this design by enabling the modeling of automata in isolation and composing them by explicitly linking interfaces and combining their feature models."
"vt-micro [cit], a robust model that can evaluate vehicular environmental impact [cit], is applied to compute the ghg emissions of the trajectories. the time interval of interest ti is set to a day: [0:00, 24:00). the bucket widths are set to 10 ml for ghg emissions."
"although much work has been conducted to enable time-dependent [cit] and stochastic [cit] routing services in various application scenarios, existing proposals do not provide a detailed description of how to obtain the time-dependent and uncertain weights, and most studies rely on randomly generated synthetic weights."
"in this paper, we propose an identity-based broadcast signcryption scheme, and formally prove its security (confidentiality and unforgeability) under the strongest existing security models for broadcast signcryption (ind-cca2 and euf-cma respectively). compared with other broadcast signcryption schemes regarding the security and computation overheads, we believe that our scheme is more efficient and more suitable for broadcast system devices with low computational capabilities. our scheme has the following merits: (1) one subscriber can securely register and unregister from a broadcaster without affecting the other subscribers; (2) the scheme satisfies the forward secrecy attribute."
"with α is the global smoothing constant which is between 0 and 1, c is the edge enhancing smoothing constant and k is a parameter. the amount of smoothing is determined by the number of iterations set by the user. in this work, frangi's vesselness measure needs to be modified and used to construct the new diffusion tensor. frangi's vesselness filter is defined as:"
"the model of identity-based broadcast signcryption consists of the following four algorithms: setup: on input of a security parameter k the pkg uses this algorithm to produce its master public/private key pair (p pub, s). it also outputs some params which are the global public parameters for the system."
"in brief, user interaction with the si should supply information about how users make use of domain knowledge and user interactions with vigor should provide us with data about users' use of more complicated tools and how users structure their searches."
"in conclusion, we have presented a new study of the behaviour of different user types while searching for video. the findings of this evaluation have illustrated some potentially new and exciting findings for the evaluation and design of multimedia retrieval systems and will hopefully lead the way in the consideration for expertise in future interactive video retrieval evaluations."
"this article advocates the use of measurement redundancy for energy saving improvement. indeed, we argue, in this paper, in favor of a grouping technique where the nodes having redundant measurements should be grouped. the measurement redundancy is the fact that several sensors in a same geographical proximity, report a same measurement value. thus, the core idea of our approach is to determine and to group redundant nodes in order to have only one representing node by group which sends its data. taking into account the measurement redundancy in clustered wsns, our approach is based on a three-level hierarchical scheme as in leach [cit] ."
"while we are aware that this definition may be somewhat narrow, it is a well-defined definition which has been widely used for the evaluation of state-of-the-art video systems at the trecvid workshop [cit] . if the evaluation were on a different collection, e.g., youtube or television archives in an industrial setting, then perhaps a different definition of an expert may be more appropriate."
"to evaluate the performance of mr-leach, we simulated it using the ns2 simulation tool which provides an opportunity to integrate the leach protocol. firstly, we integrated the implementation of leach. then, to fig. 3 . sensor network after geographical proximity grouping integrate our grouping technique by geographical proximity, we modified leach according to the hierarchical structure in the association commands of the mac layer and the method of filling the redundancy table at the physical layer."
a second goal of this research is to examine which particular characteristic of being an expert user results in more successful searches. is it greater knowledge of tools and the search system? or is it indeed greater domain knowledge? the implications of this discovery could affect the design of video retrieval tools to help users. these tools should aspire to close the gap between the particular aspects of user searches that make the searches representative of a novice or expert.
"query by text is the most popular method of searching for video. it is used in many large-scale video retrieval systems such as youtube, blinkx, etc., and is also the most popular query method at trecvid [cit] . query by text is simple and users are familiar with this paradigm from text-based searches. in addition to this, query by text does not require a representation of concepts or features associated with a video, but does require that meaningful textual descriptions of the video and its content are available. textual descriptions in some cases may be extracted from closed captions or through automatic speech recognition; however, a study of a number of state-of-the-art video retrieval systems [cit] concludes that the availability of these additional resources varies for different systems. where these resources are available, they may not always be reliable due to limitations in automatic speech recognition or language differences for example. more recent state-of-the-art online systems rely on using annotations provided by users to provide descriptions of videos. however, as was stated previously, this further complicates the retrieval process, either because of misconception surrounding annotations [cit] or users' reluctance to provide annotations [cit] ."
as part of the post task questionnaire the users were asked about the tasks and the search that they had carried out. the following semantic differentials were used on a 5-point scale:
"when it comes to using the additional search functionality available in the grouping interface, i.e. using groups to launch a query using a low-level feature, it was found that experts use these features more often than any other user group. what is even more surprising is that the novice+ users actually use this feature less often than when they were members of the novice user group. this might be part of an overall trend for the novice+ users, given that the novice+ users also carry out fewer searches with image examples as well. nevertheless, it should be noted that despite the fact that novice+ users use the additional search functionality available in the groups less than when novice users, they do use less query images than novice users; thus their behaviour mirrors expert users in this respect."
"however, leach-c greatly increases the network overhead since all the sensors send their location information to the base station during each phase of chs election. moreover, the delay had not been improved. several studies presented in the literature [cit] showed that such centralized architecture does not support the scalability. thus it is particularly suitable for small networks."
"where 13 ] are the eigenvectors of the structure tensor. in weickert's work [cit] the eigenvalues are computed by a from 2d to 3d extended equation. in the literature, there are mainly two possibilities for the extension from 2d to 3d. the first type is edge enhancing diffusion (eed) whose diagonal elements are defined as:"
"in this paper an examination of the search behaviour and the search performance of users with varying levels of expertise while searching for video has been presented. the evaluation which was conducted put the behaviour of novice and expert users side by side and in particular looked at the influence that a gain in background knowledge has on the performance and behaviour of novice users. there are a number of interesting and important findings that have been made as a consequence of this study. it was discovered that the expert users have the best performance in terms of the number of videos found, the number of relevant videos that were found and map for videos that were found. in contrast to this novice users had the worst performance overall, and when the novice users returned as novice+ users their performance improved. however, what is most interesting is that when these novice users returned as novice+ users the difference in task performance is no longer statistically significant in comparison with the expert users as it had been for some performance measures. this is an indication that with some adequate training the performance of novice users' can be comparable to that of expert users. this finding could be very important for future evaluations of multimedia information retrieval systems and also the development of multimedia search interfaces."
"vigor (see fig. 3 ) comprises of a search panel (a), results display area (b) and workspace (c). these facilities enable the user to both search and organise results effectively. the search panel (a) and the results panel (b) (both on the left-hand side of the interface) are where users can view the formulated queries and search results, these search facilities can also be found on the si. the main component of vigor which differentiates it from the si is the provision of a workspace (c). the workspace serves as an organisational ground where the user can construct groupings of images. groups can be created by clicking on the create group button; the users then add a textual label for the group (g). users can potentially add an infinite number of annotations to the group, but each group must have at least one annotation. drag-anddrop techniques allow the user to drag videos into a group (d) or reposition the group on the workspace. it should be noted that any video can belong to multiple groups simultaneously. each group can be used as a starting point for further search queries. users can select particular videos and can choose to view similar videos (f) based on one or all of a set of feature categories [colour, texture, shape or text, respectively (e)]. the workspace is designed as a potentially infinite space to accommodate a large number of groups. vigor also contains the same play and tooltip functionality as the si."
"mr-leach uses a grouping technique based on geographic proximity. depending on the geographic location, our technique allows the grouping of the non-cluster-head nodes having the same measurement values. thus, only the group representing nodes send their packets. this way, the intra-cluster communications are reduced and the energy consumption of the nodes is optimized."
"while all of these methods outlined above have problems, they have been used together in a number of systems, such as informedia [cit] and mediamill [cit] . these systems have been amongst the most successful systems at recent trecvid interactive search evaluations. however, these top results are for \"expert\" users, who, as discussed earlier, establish an idealistic performance threshold for users [cit] . also, a combination of these approaches requires a vast amount of metadata to be extracted and stored for each individual video clip. as we have previously outlined, there are a number of different ways in which a user can query a video retrieval system; these include query by text, query by example and query by concept. each of these approaches have had limited success on video retrieval, and to date none of these approaches has provided an adequate solution to providing the tools to facilitate video search [cit] . thus, in an attempt to overcome these limitations some innovative interfaces have been proposed for providing different types of interaction for multimedia search."
"3) the manuscript includes detailed studies of diffusion tensor construction for anisotropic diffusion equation. 4) a detailed theoretical support of the new improved vesselness filter is given. figure 1 depicts the flowchart of the proposed vesselness enhancement filter. first, original medical images are processed as input. second, the hessian matrix of each voxel in the 3d data is computed, and the enhanced frangi's vesselness measure is calculated. next, vesselness enhancement diffusion is performed for original medical images. the enhanced frangi's vesselness measure is used to construct the diffusion coefficient. finally, the hessian matrix is again computed. given the new eigenvalues, the proposed vessel enhancing filter can be designed."
the two basic security properties that are desired out of any ibbsc scheme are message confidentiality and unforgeability. we formally extend the existing strongest security notions for encryption and digital signature (ind-cca2 and euf-cma respectively) to ibbsc below.
"there exists a non-trivial adversary a who can defeat the signcryption by learning something about the encrypted message, that means there exists an algorithm to solve the bidhp with non-negligible advantage. since this is not possible, no adversary can defeat the signcryption this way. hence, our proposed scheme is secure against any ind-ibbsc-cca2 adversary a attack."
this process is repeated until the end of the round which corresponds to clusters lifetime as shows in the figure 2. there will be again clusters formation and application of grouping technique by geographical proximity.
"in figure 8 we present a visual comparison of the segmentation results in axial view obtained by using the mfat method [cit] and the proposed enhancement filter with the ground truth regions, for dataset 01 lad, dataset 01 lcx and dataset 02 lcx. the first column (blue) shows the ground truth regions labelled by the professional cardiologist. the second column (yellow) gives the segmentation results by the mfat method. the segmentation results obtained by using the proposed vesselness filter are presented in the last column (red). the experimental results demonstrate that the proposed vascular enhancement approach is capable of preserving more coronary artery features and reducing pseudo vessel structures. as a result, the segmentation results are more precise and reliable for clinical diagnosis."
"as depicted by fig. 2, at the end of the initialization phase, mr-leach works by periodic superframes. the superframes are divided into a set of time slots which are allocated to the representing nodes to send their data to the ch. other time slots are assigned to the chs to send the data to sink. after each superframe, the role of the representing node rotates between the nodes of the group. a set of superframes forms a round and mr-leach ensures that in each round all nodes send their readings at least once in turn."
"interactive video retrieval refers to the process of users formulating and carrying out video searches and subsequently reformulating queries and getting new search results based on the previously retrieved results. as video is extremely rich content there are a number of different ways that users can query video retrieval systems. the use of the low-level features that are available in images and videos, such as colour, texture and shape to retrieve results, is one common approach. this approach is often used for query by example, where users provide sample images or video clips as examples to retrieve similar images or video clips. while this approach seems reasonable it also presents a number of problems. it requires a representation and extraction of all of the features required from all of the videos, presenting issues of efficiency. also the difference between the low-level data representation of videos and the higher level concepts users associate with video, commonly known as the semantic gap [cit], provides difficulties. in an attempt to bridge this semantic gap, a great deal of interest in the multimedia search community has been invested in search by concept. the idea is that semantic concepts such as \"vehicle\" or \"person\" can be used to aid retrieval; an example of this is the large-scale ontology for multimedia (lscom) [cit] . however, query by concept also has a number of issues that hinder its use; it requires a large number of concepts to be represented and to date has not been deployed on a large-scale for general usage."
"there is a growing necessity for effective tools and techniques to assist users in the difficult task of searching for video. current cutting edge video retrieval systems rely on various techniques to bridge the semantic gap, including using annotations provided by users, methods that use the low level features available in the videos or an existing representation of concepts associated with the retrieval task. none of these methods are sufficient to overcome the problems associated with video search (see sect. 2 for a full discussion). these problems are magnified by the lack of understanding of user behaviour while searching for video clips. in order to address this issue, in this paper we introduce an experimental study to investigate the differences between expert and novice users when searching for video. understanding the differences between the behaviour of these two user types could have a number of positive outcomes that will influence the development of tools and techniques for video search. e.g. improved search interfaces, improved query methodologies, a more diverse set of tools that can be used by all user types for search, etc., all of these close the gap between novice and expert users. additionally, further understanding of user behaviour could influence the assessment of tools and techniques, which in turn could lead to more realistic and robust evaluations of these tools and techniques. these potential beneficial outcomes and the success of our evaluation are based on the assumption that more expert users should be more successful in finding relevant video than more novice users. for our evaluations we use the trecvid collection, tasks and definition of expertise, and for a number of years in trecvid expert users have outperformed novice users. this presents us with a unique opportunity to compare different user types (in terms of expertise) performance and approach for video retrieval and as such answer some key questions regarding the role of expertise in video retrieval."
"an additional fascinating trend appears when the additional search features available for groups in vigor are examined, in particular which features are used most often by users for the different tasks. as has been seen thus far, the expert users are the most successful users in our evaluation; given this fact and their background knowledge, the assumption could be made that the expert users would be most likely to be able to determine the most useful additional search feature available for each task. table 3 shows the search feature used most often for each user type for each task. it can be seen in table 3 that the expert users and novice users used the same feature most often on two occasions; in contrast with this, expert and novice+ users use the same feature most often for five of the eight tasks. novice and novice+ users are in agreement three out of eight times. overall, all user groups utilise the same feature most often for only one of the tasks; this is task eight where users must retrieve gray-scale images, so the fact that they pick the same feature (colour) in this instance is not surprising. the results in table 3 once again demonstrate a change in the behaviour of novice+ users, i.e. when the novice users return and are in the novice+ group where they have more background knowledge, their behaviour becomes more like that of an expert user."
"in this section, the proposed vessel filter is implemented on five real patient ccta datasets, which were obtained from the national heart center singapore, and each ground truth artery, i.e., the left anterior descending artery (lad), the left circumflex artery (lcx) and the right coronary artery (rca), was labelled by the experienced cardiologist. the dataset size and processing time by using different vessel enhancing approaches of each group of data is summarized in table 4 . the proposed enhancing approach is proven to be efficient, with an average processing time of about 9.8 minutes. this enables the proposed diffusion scheme to be further applied as a real-time medical images preprocessing tool in clinical practice."
"we now discuss the impact of different network sizes on the average end-to-end delay. fig. 7 plots the results of the average end-to-end delay evolution. as first result, we can see that the average delay grows with increasing of the network size. however, this increasing is less significant for mr-leach due to measurement redundancy consideration. in fact, the ch in mr-leach expects a number of packets k n before the aggregation, where n is the size of the cluster."
"in the future, we will consider the influences of the number of privileged users on the size of the signcryption and the computation overheads on the broadcaster side. another aspect of future work is further reducing the number of pairing computations during designcryption on the subscriber side. besides, it is unrealistic to assume that a single trusted authority will be responsible for issuing secret keys to members of a large-scale network. therefore, we will consider multiple domains environment where a subscriber can register to another domain broadcaster."
"the initialization of mr-leach protocol comprises two main stages: in the first stage, the chs collect and observe the cluster readings, while in the second they determine the measurement redundancy and group the redundant nodes. we give more details hereafter."
in post search task questionnaires subjects opinions were solicited about the videos that were returned by the system. the following likert 5-point scales and semantic differentials were used. some of the questions used are contradictions and some of the scales were inverted to reduce bias. the scales and differentials were
"the filter is not proportional to any eigenvalue and, at the same time, is robust to low-magnitudes of the eigenvalues. compared with vesselness filters of other forms, this type of vesselness filter has two main advantages. first, it returns a response that is close-to-uniform. second, it enables that the response is invariant to object contrast. these two advantages play an utmost role in balanced and accurate vesselness enhancement."
"while the actions of the novice+ users alter to be closer to the actions of an expert user, it was found that the attitude of the novice+ users towards the tasks and systems was more positive than that of both the novice and expert users. the expert users repeatedly gave the most negative and critical feedback of all aspects of the search, tasks and interface. in contrast when returning to be novice+ users the novice+ users gave the most positive responses. in fact a number of these differences in attitudes were statistically significant based on the user type. again this finding should be considered in the future design of evaluations of multimedia information retrieval systems and indeed also for future definitions of what are expert and novice users for these systems."
"the average user responses are shown in table 5, with the most positive response for each user group shown in bold. the figures in table 5 illustrate that for the bulk of differentials the novice+ users gave the most positive answers. the general tendency, with the exception with how familiar the users were with the task, was that the expert users give the least positive response and the novice+ users give the most positive result. in the case of all of the differentials, with the exception of the clear criteria, the novice+ users give a more positive response than the novice users. it could be interpreted that the expert users are more critical of the tasks and searches, as they have a more in depth knowledge. in contrast to the expert users the novice users are new to the system and are unfamiliar and confused by the tasks and searches; however, when the novice users come back as novice+ users they have already seen the collection and interfaces. thus all of the user groups have different perceptions and opinions on the interfaces and search process, meaning that they offer different perspectives."
"we can see that the challenger c has the same advantage in solving the inv-cdhp as the adversary a has in forging a valid signcryption. so, if there exists an adversary who can forge a valid signcryption with nonnegligible advantage, that means there exists an algorithm to solve the inv-cdhp with non-negligible advantage. since this is not possible, no adversary can forge a valid signcryption with non-negligible advantage. hence, our proposed scheme is secure against any euf-ibbsc-cma adversary a attack."
"the si (see fig. 2 and the search and results panel on the left of fig. 3 ) consists of a search panel (a) and a results display area (b). the search panel is where users carry out their searches. users enter a text-based query in the search panel to begin their search (a). the users can add videos from the results as examples or enter text to reformulate their queries to continue the search process (b). the result panel is where users can view the search results (c). users can drag shots from this panel and add them as example shots to reformulate their query (b). users can also drag shots from this panel and add them as relevant images for the task (h). in all panels additional information about each video shot can be retrieved. hovering the mouse cursor over a video keyframe will result in that keyframe being highlighted, along with neighbouring keyframes and any text associated with the highlighted keyframe (henceforth referred to as tooltip). if a user clicks on the play button a popup panel appears to play the highlighted video shot. as a video is playing it is possible to view the current keyframe for that shot, any text associated with that keyframe and the neighbouring keyframes. users can play, pause, stop and navigate through the video as they can on a normal media player."
"ü y and n denote that the property holds and does not hold in the scheme respectively. as we all know, a bilinear pairing operation is very time-consuming than other operations [cit] . table 1 summarizes the performance result of the proposed scheme in terms of the computational costs for the registration phase, sigcryption phase and unsigcryption phase, respectively."
"when the network size becomes large, its management raises difficulties. the structuring of a sensor network is one of the main mechanisms to save energy in each node, which allows to extend the system lifetime. one of the best known structures is the hierarchy. the hierarchization technique enables to partition the network into subsets in order to facilitate its management. in such a technique, network view becomes local; special nodes may have additional roles. the literature includes several contributions on hierarchization techniques of sensor networks, that is briefly presented in the following section. we distinguish two types of node groups: the zone and the cluster. a cluster is defined as a set of nodes with cluster head (ch). the ch behaves as a relay between the sensor nodes and the base station directly or through other chs. ch usually has higher energy resources than other nodes in the network. ch is elected according to various criteria and information about the network: the level of energy, links with other sensor nodes, geographical position etc. a zone is defined as a set of nodes but does not have dedicated cluster head. thus, a cluster is a subclass of a zone. formation of groups (zones or clusters) is based on network information, thus requiring its instrumentation. these measures are taken in certain circumstances that may be static (such as node position in a fixed system) or dynamic (such as energy level of the nodes)."
"as has been demonstrated thus far, the search behaviour of the novice users changes as they gain more background knowledge of the collection and the search tools. the analysis of user behaviour continues by looking at the navigation behaviour of different user types and also their use of the available search tools. table 4 shows the average number of times that each user type plays a video, navigates through a video (using the next or previous function) to view a neighbouring shot and uses the tooltip functionality or creates a video group. once again it can be seen, based on the figures in table 4 that the behaviour of the novice users changes when they return as a novice+ user to become closer to the behaviour of an expert user. the expert users use the navigational tools more often than the novice users; they play and navigate through more videos and also use the tooltip function more often (to gain information about the video shots). the expert users also create the most video groups; in addition, the expert users use these as points to launch searches more often than novice users (see table 2 ). the novice users perform all of these actions the least often of all of the user types, with the novice+ user's behaviour changing to be closer to that of the expert users with additional knowledge of the tools and the collection. this change in the behaviour of the novice users with respect to the search system could indicate that the users are more aware of the contents of the collection. the si requires users to utilise their knowledge of the video collection to devise effective search queries to search the collection. the change in behaviour of the novice users with respect to vigor is an indication that the novice+ users are more aware of the benefits of tools available and are also confident enough to use the tools provided. vigor includes more tools which allow users to express themselves and to delve deeper into the collection, while at the same time making it easier for users to query the collection. the results in these last two sections indicate that when the novice users return for a second session as novice+ users they become more aware of the collection and more confident with the tools, with increased knowledge of both. the change in behaviour and difference in performance for both of vigor and si system indicate that perhaps the novice+ users' change in performance and behaviour cannot be accredited to a simple gain in knowledge about the tools or about the collection, but perhaps in both. these findings potentially have a number of implications for the design and evaluation of multimedia information retrieval systems for all user types. in order to explore these findings in more detail we turn to the user questionnaires."
"where l(x, y, z) is the original image, t the diffusion time and d the diffusion tensor. the diffusion tensor d usually is defined as:"
the main contributions of this work are summarized in the following aspects: 1) a detailed review of related work about vascular structures enhancement in medical images is presented.
"to investigate the behavior of mr-leach we used a network scenario consisting of 100 to 1000 nodes randomly deployed over a 250 * 250 m 2 area. the coordinates of the sink node are (135, 155). the nodes know their position and can communicate it to their respective ch. under all these simulation settings and for each given network size, we collected and averaged the results of 10 runs."
"2) the enhanced frangi's vesselness measure is proposed by multiplying an exponential term, which ensures that it is smooth at the origin and can be substituted into the diffusion equation. the theoretical support is also presented."
"the remainder of this paper is organized as follows. the preliminaries for bilinear pairings and security definitions are given in the next section. the formal models of identity-based broadcast signcryption are described in section 3. section 4 describes a concrete identity-based broadcast signcryption scheme. the security analysis and discussions of the proposed scheme are presented in section 5. in section 6, the performance comparison among the proposed scheme and the recently proposed schemes is presented. section 7 gives our conclusion and the future work of this research."
"proof. let p be the generator of g 1 .we assume the distinguisher c receives a random instance (p, ap) of inverse computational diffie-hellman problem. his goal is to compute a"
"since the expert users have the best performance for our evaluation, the assumption was made that the expert users form the benchmark when it comes to user search behaviour, in comparison with the other user groups in the evaluation. with this assumption in mind an analysis of the search behaviour of the different user types was conducted. overall, the observation was made that the change in task performance between novice and novice+ participants coincided with a change in search behaviour when the novice users returned as novice+ users. indeed, the search behaviour of the users in the novice+ group became more similar to that of expert users. novice+ users had shorter query terms, submitted fewer queries with examples and used fewer examples when submitting those queries than the novice users. this trend held true for both video retrieval systems evaluated. improved search behaviour and performance when using the si would indicate that the novice+ users are using their greater knowledge of the collection in comparison with the novice users to improve their performance. the si encourages users to utilise their familiarity with the video collection, as the si consists of the bare minimum in terms of query tools to allow users to search the video collection. on the other hand, a change in user behaviour when using vigor is also observed between novice and novice+ users. at the same time as novice+ users are using less group searches in comparison with novice and expert users, the behaviour of novice+ users is closer to expert users in other ways. novice+ users create more groups, use less image examples and begin to utilise the same search tools as expert users. also across both systems it was observed that the novice+ users' search behaviour becomes more similar to expert users' and less like that of novice users with respect to navigation, novice+ users play videos and navigate more often than novice users and also use the tooltip functionality to gain information about shots more often. whilst expert users set the standard for the application of these tools and systems, these new findings indicate that novice users can adjust their search behaviour quite quickly in relation to both the content of the video collection and also the search tools that the system provides. once again this is an extremely significant finding with respect to the design and evaluation of multimedia search tools, as it demonstrates the ability of novice users to acclimatise and discover how to use a new video search system quickly."
"once original medical images are pre-processed by utilizing the proposed vessel enhancing technique, the hessian matrix at each voxel location is again computed, and the eigensystem is explored. the purpose of this section is to design an vesselness filter function in the form of the ratio of eigenvalues. the filter is not proportional to any eigenvalue and, at the same time, is robust to low-magnitudes of the eigenvalues. compared with vesselness filters of other forms, this type of vesselness filter has two main advantages. first, it returns a response is close-to-uniform. second, it enables that the response is invariant to object contrast. these two advantages play an utmost role in balanced and accurate vesselness enhancement [cit] ."
"to explore the effectiveness and accuracy of the proposed vascular structure enhancement approach, two publicly available retinal image datasets: drive [cit] and stare [cit], are used to present a more rigorous form of quantitative validation. particularly, the proposed approach is evaluated against the state-of-the-art techniques. first, a visual inspection of the mean roc curve, that represents some qualitative information, can be conducted. in addition, quantitative information, i.e., the mean of auc between the ground truth vessels and the filtered results, are also presented. figure 2 presents the detected retinal vascular structures by using different enhancement methods, compared with the specialist labelled ground truth vessel structures. it can be observed that jerman's method can extract only main branches, a lot of thin vessel branches are missing. besides, large region of artifacts is introduced in the high intensity area (figure 2(d) ). in figure 2 (e), the mfat method is able to return more thin vessel branches, and remove the large region of artifacts. however, many thin branches are isolated from the main branches. the proposed method is shown to detect most vessel branches, thin vessels are connected to the main branches, and no artifact is introduced by our approach. figure 3 furthermore, the mean auc values between the ground truth vessels and the filtered results by frangi's filter, jerman's method, mfat method and the proposed method, are also presented in table 1 . generally, higher values of auc represent better vascular structure enhancement results. if the enhancement result is found to be identical to the ground truth, it will return the auc value of 1. given the qualitative and quantitative results, the proposed vessel enhancing method slightly outperforms the state-of-the-art methods."
"the expert group consisted of eight males, all of whom were members of our research group. the novice group consisted of five males and three females; the novice users were mainly students in the computing science department but also included people from other professions, e.g., a musician and a marketing executive. the average age of the members of the expert group was 27 with the average of the members of the novice group being 26. all of the expert users had extensive experience of dealing with multimedia search tools in both work and leisure. similarly, the novices also had experience of dealing with multimedia search tools; many of the users in both groups cited flickr 3 and youtube as being services that they use very often."
"in the section, we present the generic model and security model of identity-based broadcast signcryption. in our models, there are three types of entities: broadcaster; users who want to subscribe or register to the broadcaster; trusted authority called the private key generator (pkg)."
"the performance metrics in our study are as follows: 1) energy consumption: to evaluate the energy consumption, we recovered the energy levels at the start and the current energy levels of sensor nodes from trace files generated by simulations. 2) network lifetime: the network lifetime is defined as the estimated time before the network becomes nonfunctional. to evaluate the network lifetime, we recovered from the trace files generated by the simulations, the time after which the energy level of the nodes is zero. energy levels are recovered after each round. 3) end-to-end delay: to evaluate the end-to-end delay, we recovered the time of collection and receiving packets from the trace files generated by simulations. results obtained for metrics of interest are shown in figures 4, 5, 6 and 7 (with 5% of the nodes being cluster-heads). fig. 4 shows the energy consumption of the clusterheads versus the number of nodes in the network. from the figure we can see that when the network size increases the energy consumption of the chs in mr-leach decreases, in contrast of leach. indeed, with high density the measurement redundancy increases. thus, the chs receive less data. this is because, the number of groups increases which reduces the number of transmissions in the clusters. furthermore, the curves of fig. 4 also show that the performance gap between mr-leach and leach becomes more significant when the network size increases. fig. 5 compares the average energy consumption of the nodes. we run the simulations during 20 rounds, each round corresponds to 25 seconds. the reason behind the 20 rounds is to allow all nodes to behave as a ch at least once. we note that in mr-leach, energy consumption of nodes decreases when the number of nodes increases, contrary to leach. indeed, with a high density, a node communicates less with its ch. thus, the chs receive less packets due to the measurement redundancy management. so, when the number of nodes is being higher, they are grouped and only the group representing nodes send their packets in the clusters. fig. 6 shows how the network lifetime is affected over time using mr-leach versus leach. in this set of experiments, we consider a network size of 1000 sensor nodes. figure 6 shows that mr-leach has a greater network lifetime compared with leach. we found that mr-leach is more effective to extending the network lifetime. indeed, compared to leach, it takes approximately 1.5 times longer until the first node dies and approximately 1.4 times longer until the last node dies."
"in the random oracle model, our identity-based broadcast signcryption scheme is secure against any euf-ibbsc-cma adversary a if inv-cdhp is hard in g 1 ."
"a final objective is to examine if increased domain and/or tool knowledge can lead to improved novice performance and also in addition if it is possible for a novice user to perform as well as an expert user. once again the implications of this finding could be beneficial as a guide for designing video search tools. moreover, it is hoped that as well as influencing what types of tools will aid user search, the findings of this paper will aid the design and evaluation of multimedia search tools for a wide variety of users."
"in modern medical imaging, manual extraction and annotation of vascular regions is time-consuming and skill-demanding. on the other hand, the performances of the existing vascular enhancing approaches vary easily due to artifacts and surrounding noises. furthermore, it is a grand challenge to identify the vascular directions at junctions. as a result, accurate and automatic vessel enhancement approaches with minimal user interaction are in great demand. we are committed to improving the quality of medical images by filtering noises and enhancing edges, and using the smoothness suitable for the underlying image structure to maintain edges. a three-dimensional nonlinear anisotropic diffusion filtering method is introduced. the diffusion tensor proposed in this work is controlled by the image structure tensor. as a result, the main diffusion takes place along the vessel direction of the underlying image. a novel diffusion tensor, which is based on and extended from frangi's vesselness measure, is developed to control the vesselness enhancement diffusion for ct images. besides, the theoretical supports of the enhanced frangi's vesselness measure and the diffusion tensor are mainly described. the superiority of the proposed 3d improved vesselness filter based on the ratio of eigenvalues is also studied in this paper."
"with the aim of evaluating the effect of domain and tool knowledge on user search behaviour and performance, two interfaces for video search were utilised for the evaluation. the first interface is called the search interface (si). the si consists of basic tools for video search, which include query by text and query by example. this interface puts the emphasis on the user to use their own knowledge to perform search tasks; this permits an assessment of the impact of collection knowledge on user actions and interactions. the second interface is the vigor system [cit], which was outlined in the related work section. vigor allows users to create groups of video shots to assist with completing their search and also allows users to use these groups as a starting point for further searching to achieve their search goal. previous evaluations have shown that vigor results in increased user performance and increased user satisfaction [cit] . the purpose of vigor in this evaluation is to provide a system which can supply information about how different users conceptualise different searches. also as vigor is a slightly more complex video search system, the user interactions can potentially provide further information about how different user types utilise more complicated tools when they are available. in addition, the different focus of both interfaces and indeed the use of two interfaces provide a simulation of a typical video retrieval experimental setup, where more often than not, more than one interface or system is being investigated in order to examine an effect."
"based on the results presented in table 1 it is apparent that the expert users outperform the novice and novice+ users in all reported aspects of task performance. the expert users find more video shots that they believe to be relevant, find more relevant video shots and result sets found by the expert users also have the highest map score, which is an indication of the best overall performance. on the other hand, for some of the tasks it was found that novice and novice+ users had higher precision for the retrieved video shots; however, this gives a false impression in relation to task performance as this was normally for low numbers of videos."
"in this work, we develop a hybrid technique for fast and accurate vascular enhancement filter, which contains two main steps: vesselness diffusion and improved vesselness filter based on the eigenvalues ratio. the diffusion tensor proposed in this work is controlled by the image structure tensor. as a result, the main diffusion takes place along the vessel direction of the underlying image. for ideal tubular structures, the diffusion process guarantees that the diffusion figure 6. maximum intensity displays (mip) of three 3d [cit] vascusynth sample: group 2 (data3), group 3 (data4) and group 4 (data5), by using different vessel enhancing methods, frangi's (first column), jerman's [cit] (second column), mfat [cit] (third column) and the proposed method (last column). strength is maximal and hence thin vessels can be preserved. for non-vessel structures, the diffusion strength is isotropic and high and noises can be removed. the proposed vesselness enhancement diffusion algorithm is designed for medical images including vascular structures, which can be viewed as a pre-processing technique before vessel segmentation and centerline extraction. the frangi's filter is modified by multiplying a term, in order to make the derivatives of the vesselness measure smooth at the origin and allow it to be used in the diffusion equation. however, the term needs to be carefully selected. moreover, the parameters of the diffusion procedure can also be further explored to produce better diffusion results in the further work. once original images is filtered by the proposed diffusion algorithm, the hessian matrix at each voxel location is computed, and the eigensystem is further explored. an improved vesselness filter function can be designed in the form of the ratio of eigenvalues."
"in recent years, as a result of the improving capabilities and the falling costs of many hardware systems, there are greater than ever resources to store and manipulate videos in a digital format. additionally, with greater than ever broadband capabilities it is now practical to watch videos almost anywhere as simply as text-based pages. people can now create their own personal digital libraries from multimedia produced through digital cameras, mobile phones and camcorders etc., and use a variety of systems to place this material on the web, as well as gather these materials in their own personal multimedia collections. this has resulted in the emergence of numerous online video search and sharing sites, among them youtube 1 and blinkx. 2 despite this rapid growth in the availability and the ubiquity of video, the systems that currently exist to organise and retrieve these videos are not sufficient to deal with such large and rapidly increasing volumes of video. in an effort to assist the development of tools and techniques to support video search, the trecvid workshops provide a common test collection to enable large-scale evaluation of research approaches. in terms of interactive video search trecvid participants have had a relatively great deal of success. however, a lot of this success is for expert users who form the upper bound for model users of interactive video search systems [cit] . also in recent years some of the video search systems have concentrated more on users categorising search results rather than actually aiding users in searching for video [cit] . in order to examine the reasons behind the success of expert users and the difficulties involved with interactive video search, we will discuss interactive video search in more detail."
"in this sub-section we outline some innovative and interesting interfaces for interactive multimedia retrieval; we first discuss some image retrieval interfaces before discussing video retrieval. image retrieval and video retrieval are similar in that in both cases there is a combination of visual and often textual information; however, there are a number of important differences that make video search much more difficult than image search. the first main difference is the multimodal nature of video, encompassing images, text, audio and a temporal factor, etc. while text and visual features may be used to aid or hamper image search, these are only two of the many modalities involved in video search. second, video is a much more interactive medium in comparison with still images. interactive video retrieval systems have to make an additional effort to aid the user in deciding whether the selected videos are relevant or not for their tasks, whereas for image retrieval systems the user can easily and quickly discern relevant and irrelevant results. the result of this is that interaction and usage information from interactive video retrieval systems are far noisier than the usage information on image retrieval systems. for instance, on average, 75% of the user results that a user interacted with on the image retrieval system developed by craswell & szummer were relevant [cit], whereas only 7-9% of search results that the user interacted with were relevant for a similar interactive video retrieval system [cit] . as a result of this, the goals of many interactive video retrieval systems are to lower the effort for the user to explore the complex information space and also to assist the user in deciding if a result is relevant to their information need. despite these differences it is still important to consider the myriad of different image retrieval systems available while considering video retrieval, as both image and video retrieval systems attempt to bridge the semantic gap."
"we compared our design with leach protocol. for this, we performed simulations with the same parameters and metrics for the two approaches. we are interested primarily in the energy consumption of nodes, which represents an important criterion for determining the lifetime of a sensor network. we evaluated the end-to-end delay as it is a performance criteria for many monitoring applications over wsn."
"in this section, we compare the efficiency of our scheme with other schemes appearing in the literature in table 1 regarding the security and computation overheads not including precomputation overheads required by different phases including registration phase, signcryption phase and unsigncryption phase."
"this paper addressed the energy efficiency in the clustered wireless sensor networks. our solution consists in taking benefit from the measurement redundancy in such networks. in this context, there is need of approaches which can manage these wsns in a better way. we proposed mr-leach (measurement redundancy aware leach) protocol, which is an extension to the wellknown leach protocol to improve energy consumption in cluster-based wsns. mr-leach has several strengths. it combines the geographical proximity of the nodes with the measurement redundancy in order to improve the energy efficiency and provides better end-to-end communication delays. the performance of our solution has been investigated through intensive simulations. furthermore, we assessed how it outperforms leach protocol by extending the network lifetime and bounding the end-to-end communication delay. in our future work, we would like to extend our approach to consider event-triggering wsn and initial wsn deployment including explicit redundancy specification."
"geographical proximity is only checked between sensor nodes which report the same measure during the time interval intm esure. it is therefore checked between the sensor nodes which are likely to be redundant. figure 3 shows the architecture of network after applying our grouping technique. a description of our technique is given below. during the observation phase of measurements (intm easure), each ch creates a table of values (v alt able) where it will store the reported measurement values by the sensor nodes members of the cluster. this table will be used thereafter to calculate the redundancy matrix (regroup) as detailed in algorithm 1. each ch then verifies the geographical proximity of all sensor nodes that reported the same measurement value (with some precision α) during intm easure. each ch then places in the same row of the matrix regroup, the identifiers of all sensor nodes which are in the same geographical proximity. after this geographical proximity grouping, each ch asks some sensor nodes to fall asleep for the next super-frame so that only one node in each group at a time (or at the same time), during the next superframe."
"in the broadcaster recordation phase, broadcaster b follows the steps below. a. basic security theorem 1. in the random oracle model, our identitybased broadcast signcryption scheme is secure against any ind-ibbsc-cca2 adversary a if bidhp is hard."
"in this study, we developed a hybrid vesselness filter for vascular structures enhancement from noisy medical images, which takes the advantages of vesselness enhancement diffusion, and integrates the improved frangi's filter based on the ratio of eigenvalues of the hessian matrix. we have first modified the original frangi's vesselness filter by multiplying an exponential term to make it smooth at the origin. based on this, a novel diffusion tensor can be constructed, and original medical images can be processed, in order to enhance vascular structures and inhibit surrounding noises. second, the hessian matrix was computed and the novel vessel enhancing filter was then developed based on the eigenvalues ratio. this novel approach has been validated over the public 2d retinal datasets and 3d synthetic vascular structure models. it showed that the the novel method can detect more thin retinal vessel branches by a visual comparison. besides, the auc value of the roc curve of the proposed method was further compared with that of each state-of-the-art approach, and the new method gave the highest auc value. [cit] vascusynth sample with low level of noise. experimental results demonstrate that the proposed filter outperforms other existing approaches for curvilinear structure enhancement from noisy images. moreover, the novel approach is further evaluated on real patient coronary computed tomography angiography (ccta) datasets with ground truth regions labelled by professional cardiologist. our method is proven to produce more accurate coronary artery segmentation results. given the accuracy and efficiency, the proposed vesselness filter should have more routine clinical applicability as a real-time vascular structures enhancement tool before vessel segmentation and quantification. yanning zhang is currently a professor and the assistant president of the northwest university of technology. she is also a special professor of ''changjiang scholar,'' the chief of the national defense 973 project, and the first batch of ''ten thousand people plan'' scientific and technological innovation leaders of the central group department. her research interests include image processing, pattern recognition, computer vision, and intelligent information processing research, and with space, aviation, and other national major needs. she was awarded the national second prize for teaching achievements, the provincial and ministerial science and technology progress award, the national red flag-bearer of march 8, and the advanced individual of 863 science and technology tackling key problems."
2) principe of our approach: mr-leach combines geographical proximity of the nodes and their readings redundancy in order to reduce the number of the intracluster transmissions.
"vessel enhancement technique has been widely used as a common prerequisite in biomedical imaging before vessel segmentation and centerline detection, which provides important pathological information. among the reviewed vessel enhancing approaches, the frangi's filter is widely used, since it is easy to implement, and returns very high response uniformity on objects with uniform intensities. besides, some improved vesselness filters are developed. they are usually modified from frangi's function, like yang's method [cit], and can obtain only minor improvement on vascular structures detection. non-linear anisotropic diffusion filtering can take place in certain structure direction since it uses the information of the structure tensor orientation. hence, they are able to preserve important features during the filtering process. however, diffusion based methods are not capable of detecting vessels within a wide range of scales, since they act at a fixed size. moreover, diffusion tensors, like the fractional anisotropic tensor (fat), have been widely used to advance the detection of vascular structures. fat has great potential in medical images processing, like diffusion tensor regularization. however, the method introduces too many parameters and threshold values, which makes it difficult to be used for other medical datasets."
"the rest of this paper is organized as follows. related work in the field of hierarchical power management in wsns is summarized in section ii. section iii describes mr-leach protocol and details the geographical proximity based grouping technique. in section iv, we evaluate the performance of our approach. finally, we conclude and present some research perspectives in section v."
"in order to investigate if there was any difference in user behaviour for query formulation a number of aspects of user queries were investigated, including query length, the number of times images were used in a query, the average number of example images used, the number of times group of images were used to launch a search and the average number of examples used. this analysis (see table 2 ) revealed that expert users have the shortest textual queries typically, but subsequent to having used the search tools the novice+ users shorten their queries. also, expert users are less likely to add an example image to their query and when expert users do add examples images they add less example images in comparison with novice users. once again it can be seen that after using the system novice+ users query creation behaviour changes; the novice+ users begin to carry out less searches with a query examples and also with less examples than when they were novice users, in a way their behaviour is becoming more like that of an expert user."
in order to understand the improved performance of novice+ and expert users in comparison with novice users an analysis of the approach and behaviour of different users while carrying out their search tasks was conducted
"the proposed vesselness filter is described in section 2, which includes two main parts: 1) frangi's filter based vesselness enhancement diffusion. 2) improved enhancement function based on the eigenvalues ratio of the hessian matrix. section 3 presents the experimental results on public 2d retinal datasets, 3d synthetic vascular structure models and 3d real patient coronary computed tomography angiography (ccta) datasets. in section 4 we give a brief discussion about the proposed enhancement approach. finally, conclusions are given in section 5."
"a 2-searcher-by-2-task latin square design was adopted for this evaluation. each participant carried out two tasks using the si, and two tasks using vigor. in order to avoid any order effect associated with the tasks or with the systems the order of system usage was varied as was the order of the tasks. with the goal of determining the effect of domain and tool knowledge, each novice user returned to carry out four different tasks on the day following their original evaluation; this produced a third group of distinct users which will henceforth be referred to as novice+. using this experimental model it is possible to evaluate the potential effect of additional knowledge of the collection and/or the tools provided on user search behaviour and interactions. [cit] collection allowed us to conduct analyses that may not have been possible with other collections."
"to explore the performance of the proposed vessel enhancing scheme, five other vascular enhancement methods (frangi's, yang's [cit], ved, jerman's and mfat) were also performed into five ctca datasets. figure 7 depicts the comparison of the roc curves for all the approaches. frangi's filter achieves the lowest mean auc value. this may be explained by the fact that frangi's filter is sensitive to noise. yang's improved filter achieves low value of auc. ved is found to outperform the two latter methods. this is can be explained by the fact that ved utilizes the vesselness measure to construct the diffusion tensor. jerman's method and the mfat method can achieve higher auc values. the proposed method is found to outperform the five existing state-of-the-art vessel enhancement methods, with the highest auc value of 0.897."
"broadcast encryption is useful for distributing digital contents to internet users over a broadcast channel. it allows a center to deliver the encrypted data to a large set of users so that only a particular subset of privileged users can decrypt it. however, we find almost all ibbsc schemes that have been proposed until now do not satisfy register secrecy and forward secrecy. following this, we have proposed a fixed version of the scheme to achieve register secrecy and forward secrecy attributes, also we have proven its ind-cca2 and euf-cma security formally in the random oracle model. these are the strongest security notions for message confidentiality and authentication respectively. while we have proposed a secure ibbsc scheme, we have not compromised the performance. in fact, the proposed scheme only requires two pairing operations for end user devices with limited computing capability."
"on the whole our objective is to provide answers to a number of questions regarding novice and expert users for video search. to begin with we wish to discriminate between the actions of novice and expert users. by analysing the ways in which both sets of users carry out video search tasks, we hope to identify the good practices that lead to successful searches by expert users and the actions that lead to unsuccessful searches by novice users. there are a number of ways that multimedia search behaviour may be influenced by expertise. first, background knowledge could affect the quality, quantity and type of queries used. this topic has been investigated extensively for the web through log file analysis [cit], in electronic databases [cit] and briefly but not to a great extent for video search [cit] . however, searching for multimedia provides a larger number of methods to query search systems when compared with other search paradigms (see the sect. 2 for a full discussion). second, background knowledge could affect the videos that users select to click on. in most retrieval systems video shots or entire videos are normally represented by one keyframe; a keyframe is an image that represents an entire video shot. due to the volume of information that may be contained in even short video clips it may be the case that these selected image keyframes may not be representative of the video content. however, based on these keyframes users must decide about whether to view a video or not. models of language comprehension have shown the importance of background knowledge when making judgements within text [cit] . indeed in the online domain, links followed during navigation have been investigated thoroughly [cit] . however, in the visual domain where an image represents a retrieved video, this issue of background knowledge has not been as thoroughly investigated. finally, with respect to the decision to follow a particular search direction, domain knowledge may affect the decision of a searcher to leave a particular search path, which in the case of video search would be ceasing to watch a particular video or set of video clips."
"1. the expert has been active within the multimedia ir community for at least a year, having a better sense of the accuracy of the various automated video processing techniques 2. the expert has used similar video retrieval systems prior to the timed runs on the trecvid data and therefore is more familiar with the system operations than study participants who first see it during the evaluation 3. the expert knows about trecvid evaluation, e.g. the emphasis on shot-based retrieval and the use of mean average precision as a key metric, etc."
"in this section, the proposed method is both quantitatively and qualitatively evaluated against three different categories of datasets: public 2d retinal dataset, 3d synthetic vascular models and 3d real patient ccta datasets. the accuracy of the new method is compared with that of the state-of-theart approaches. the receiver operating characteristic (roc) curve is used to perform the visual comparison. also, the area under the curve (auc) of the roc curve is explored as the evaluation metric to compare the performances between the proposed vessel enhancing filter and the state-of-the-art techniques. our hybrid vesselness filter for vascular structures enhancement takes the advantages of vesselness enhancement diffusion, and integrates the improved frangi's filter based on the ratio of eigenvalues of the hessian matrix. the proposed method is mainly compared with frangi's filter [cit], jerman's method [cit] and multiscale fractional anisotropy tensor (mfat) method [cit] . the main reasons are summarized in the following aspects: 1) frangi's filter is widely used, since it is easy to implement, and returns very high response uniformity on objects with uniform intensities. in our work, we have first modified the original frangi's vesselness filter by multiplying an exponential term to make it smooth at the origin. then it can be used in the diffusion equation. 2) jerman's method represents the category of enhancement filters that are based on the ratio of multiscale hessian eigenvalues, and it achieves the state-of-the-art performance for this category of enhancement filters."
"we proposed the integrated eit approach consisting of the customized eit system and new visualization platform. it displays five functional images and six eit measures together which was suggested by the trend consensus for describing the lung status of a mechanically ventilated patient. we presented at the two operating modes. in the real-time screen view, we can monitor pulmonary function based on the air volume changes over time. the analysis screen mainly views tissue characteristics for overdistension/collapse for finding optimal ventilator settings. we validated the eit measures and functional eit images in the integrated method with experimental data obtained from normal and disease animal model."
"query 3: find dna copy number high-level amplifications in pdac samples that also contain genes differentially expressed in pdac versus chronic pancreatitis (cp) and display copy number information, gene information and differential expression experimental details (figure 1 ). the query above shows a simple way to integrate multi-dimensional data by combining data on copy number variations with results from differential expression. this will give an overview of genomic or transcriptomic changes of any chromosome(s) or chromosomal region(s) in the selected sample(s) as determined by a variety of platforms stored in ped. by overlaying the information on transcriptional deregulation from expression arrays and gene content with copy number variations from genomic arrays, one can quickly highlight the commonly affected regions in the studied patient population and the impact of the copy number aberrations on gene expression patterns. for example, by looking for high-level amplifications and genes up-regulated in pdac when compared to chronic pancreatitis, it is possible to highlight potential oncogenes."
"when applying peep titration which decreases the peep level from high to low pressure, we can use the analysis screen view (fig. 10) for finding optimal mechanical ventilation settings [cit] . after decremental peep titration, the program shows tissue-classified results (overdistension/collapse) as image and percentile values for each peep. also, cv and gi parameters display together because peep titration can be applied with different posture. from the pulmonary function analysis, we could find the optimal peep setting."
compliance ( c ) is the change in volume that occurs per unit change in the pressure [cit] . it can be defined as follows for a given lung volume change and pressure change.
a duke breast cancer data set is chosen for experiments. training data set with few records is as shown in the following table 1 . testing data set with few records is as shown in the following table 2 . a liver disorder data set is chosen for experiments. training data set with few records is as shown in the following table 3 . testing data set with four records is as shown in the following table 4 . a heart data set is chosen for experiments. training data set with few records is as shown in the following table 5 . testing data set with four records is as shown in the following table 6 .
"three functional images of tv, △eelv, and compliance in the real-time screen view were validated when applying the peep titration protocol for normal pigs. refer to the 0 cmh 2 o, we increased peep level from 5 to 25 cmh 2 o with 5 cmh 2 o step. after that, we apply a decremental peep in the opposite direction when recording the eit and ventilator data in the integrated eit system continuously. here, we excluded the ventilation delay image because we could not watch any change in the normal pigs. from the ventilator, we could get the global compliance value when dividing the pressure data by volume data. we could compare it to the sum of pixels in the compliance image for validating the process of compliance images. we used the lung-collapsed animal model to verify tv, △eelv, ventilation delay image, and air distribution parameters ( a/p ratio, cov x, cov y, gi, cv). here, we used ventilation image which added tv and △eelv images. for five pigs, regional lung collapse was induced using the saline lavage method to wash out surfactant from parts of the lungs. the presence of regional lung collapse was examined by x-ray ct images and the arterial blood gas analysis. switched to the pressure control mode, the peep titration was applied to find the optimal peep level for each case. we watched the region changes for overdistended and collapsed lung when applying the peep titration in the analysis screen view. optimal peep was chosen at the peep level when it could minimize both overdistension and collapse values."
"during volume-controlled mechanical ventilation for 10 normal pigs, tv was adjusted at 6 different levels from 100 to 600 ml at 0 cmh 2 o peep. fig. 1a shows typical tv images calibrated with the applied air volumes from the mechanical ventilator. the correlation analysis in fig. 1b showed r 2 value of 0.98 between tv values from the reconstructed images and the supplied air volumes from the mechanical ventilator. the repeatability was assessed by the bland-altman analysis as shown in fig. 1c with less than 6% mean error between these two measurements. the bland-altman plot indicates that the calibrated tv image implemented in the integrated eit system can be used as a reliable measure of the true tidal volume."
"the anterior-to-posterior ventilation ratio ( a/p ratio) is a measure of general distribution characteristics and calculated using the sums of the pixel values in the ventral and dorsal regions [cit] . the a/p ratio expresses any changes in the regional ventilation along the vertical direction. the center of ventilation ( cov ) is defined following the concept of the center of gravity in mechanics. the cov x and cov y in (3) are the ratios of the x-and y-weighted pixel sums to the global pixel sum, respectively, where x and y are coordinates of the pixel: where x j and y j are the horizontal position (x-axis) and vertical position (y-axis) of pixel j. the cov x and cov y increase from 0% at the top-left corner and approach 100% at the bottom-right corner [cit] ."
"support vector machine is one of the established machine learning techniques. support vector machine was first introduced, by boser, guyon, and vapnik in colt-92 [cit] . support vector machines are used for classification and regression are with a set of related supervised learning methods [cit] . these machines belong to a generalized family of linear classifiers. support vector machine is mostly used classification and regression prediction tool, that implements machine learning concepts to maximize predictive accuracy, which avoids over fit to the data. a better learning technique must always avoid over fit of the data."
"where v is the volume change and p is the pressure change. assuming that the pressure is homogeneous throughout the entire lungs, the compliance image can be produced by dividing each pixel of the tv image by the pressure difference obtained from the ventilator when acquiring tv image data [cit] ."
"a breast cancer, liver disorders and heart disease data sets are chosen for experiments. data set is classified with support vector machine and random forest. the results are analyzed with a comparison of prediction performances with both the techniques. the results with different parameters are tuned and parameters selections for optimal classification results are chosen. implementation is also done with rf tool on mat lab with microsoft vc++ compiler installed over it. same breast cancer, liver disorders and heart disease data sets are used to carry out experiments with both the techniques."
"ped allows the user to combine complex queries to ask detailed disease-specific questions and combine results with public annotation sources. there are options available for genomics, proteomics, transcriptomics and mirna profiling, allowing these data types to be queried in isolation or combined (figures 1 and 2) . the interface incorporates the full functionality and data from ensembl and biomart for cross-linking different data sets."
"machine learning is a concept under artificial intelligence and it is concerned with the development of techniques, methods and algorithms which enable the system to learn from the available data. this means the development of algorithms which enable the machine to learn from available data and perform tasks and activities of modeling with sample data and testing the new data. machine learning works closely with statistics in many different ways. there are different techniques and methodologies developed for machine learning tasks [cit] . neural network approaches are much in use but do have limitations with generalization, producing models that normally get over fit with the data. this is a result of the optimization algorithms used for statistical method and parameter selection to select the best model possible. other learning techniques like decision trees, ripper classification, expert systems and ai were used to predict. this problem of prediction and prognosis can be better solved with machine learning and classification support vector machine technique which implements classification."
"the animal experiments using ten normal pigs (age: 6 months, weight: 29.5 ± 4.4 kg, chest circumferences: 64 ± 9 cm) were conducted in accordance with all regulations of the institutional animal care and use committee [cit] 035). the animal was intravenously anesthetized using a syringe pump (0.5 ml/kg iv injection of 4:1 ketamine and xylazine mixture) and connected to a mechanical ventilator (hamilton-g5, hamilton medical, switzerland) by tracheal intubation. the status of the animal was continuously monitored using a patient monitor (intellivue mp50, philips, the netherlands). the chest hair was removed and 16 ag/agcl electrodes were attached at the fifth intercostal space for eit imaging (fig. 11) . to obtain eit images with proper anatomical structural information, the chest shape and electrode positions were measured using a 3d scanner (sense, 3d systems, u.s.). eit data were collected by injecting 1 ma pp currents at 11.25 khz using a 16-channel khu mark 2.5 eit device with a temporal resolution of 50 frames/s [cit] . adopting the adjacent current-injection and voltage-measurement protocol, each eit image was reconstructed using 208 boundary voltage data excluding the voltages measured between any pairs of electrodes used for current injections. the collected eit data were synchronized with the airway volume and pressure data from the mechanical ventilator. for eit image reconstructions, we used the fidelity-embedded regularization (fer) method with a subspace-based motion artifact removal algorithm [cit] . to validate the operation of integrated eit system for monitoring of lung functions using 11 functional images and measures, the estimated air volume from the timeseries of tv images was evaluated with the tidal volume controlled in mechanical ventilator at the first experiment. the tv images were produced from the eit image while increasing the tidal volume from 100 to 600 ml. we performed a linear analysis and a bland-altman analysis using the pixel sum in the tv images and the measured global tv data from the ventilator."
"we proposed the integrated eit system for functional lung ventilation imaging to enhance its clinical usefulness. the system presented in this paper can be readily used for human subjects in future clinical studies. applying the proposed integrated approach to different eit devices, their performance analyses could be conducted. from these future studies, it will be very helpful to produce clinical lung eit standards in terms of the hardware, software, and clinical practice in bedside real-time image-based monitoring of mechanically ventilated patients. the integrated approach will apply not only lung protective ventilation through optimal patient-specific ventilator setting but also spontaneous breathing monitoring for ventilator weaning. we need to verify the usefulness of monitoring of functional images and eit measures provided by the integrated approach for ventilator weaning during the spontaneous breathing test [cit] . this will further facilitate the clinical acceptance of the technique, which can reduce the risk of mechanical ventilation and also the overall healthcare cost of icu patients."
"support vector machines are systems that use hypothesis space of a linear function in a bigger dimensional feature space. these systems are trained with a learning algorithm with optimization theory that uses a learning bias taken from the theory of statistical learning. support vector machine was famous with other parallel learning techniques but now it is playing a major role in machine learning research. this technique also being used in many other critical domains like image processing, patterns recognition and medical diagnosis tec. svm becomes more important while using pixel maps as input, the accuracy of svm is comparable with other well known modeling techniques like neural networks with extended features in a handwriting recognition task [cit] . svm is also being used for many applications, such as face analysis, hand writing analysis, engineering, business, management and many more areas. svms are also being used for pattern classification and regression based applications in different domains."
query 4: find mirnas differentially expressed in pdac versus cp whose expression has been confirmed by rt-pcr techniques and display mirna attributes and study information (figure 2 the successful development and implementation of ped fills the urgent requirement of the pancreatic cancer community for resources capable of integrating the overflowing influx of data generated by novel high-throughput technologies.
"it is well-established and widely accepted that pancreatic adenocarcinoma (pdac) progresses from noninvasive pancreatic lesions-pancreatic intraepithelial neoplasia (panins) (15) . based on the degree of cellular and nuclear atypia, these precursor neoplasia are delineated as panin-1a, panin-1b, panin-2 and panin-3."
results are observed to be much better with radial basis function with svm and with certain set of parameters and these are comparable with random forest technique.
"in the real-time screen view, four functional images of lung ventilation are suggested by the trend consensus including tv, △eelv, compliance, and ventilation delay. the tv"
"eit imaging cannot compete against x-ray ct, for example, in terms of the spatial resolution. the relatively poor spatial resolution of eit is due to the fundamental limitations of its ill-posed inverse problem. we should, therefore, take full advantage of its high temporal resolution such as 50 frames/s in its clinical applications. time-difference eit imaging augmented by integrated functional imaging techniques described in this paper could be a clinically useful bedside image-based monitoring tool especially for lung ventilation imaging during mechanical ventilation. during mechanical ventilation, the pressure from the mechanical ventilator is adjusted to recruit collapsed parts. very careful pressure controls are necessary not to produce acute lung injury from overdistending other parts of the lungs. it is, therefore, important to continuously monitor the actual amount of air volume supplied to the lungs and its distributions in the lungs in real time. though eit has high potential for providing these image-based monitoring functions, its clinical acceptance is relatively slow. one of the reasons stems from the lack of an integrated approach providing all available information in a quickly readable way. in this paper, we proposed such an integrated approach by implementing eleven functional imaging and note that the optimal peep value is higher than the value of the normal case in fig. 10 to recruit the collapsed parts due to the injury 18:83 measures proposed in the most recent consensus from the interdisciplinary group called translational eit development study (trend) [cit] . from the real-time screen view of integrated eit system, the functional image of tv could be successfully obtained to quantify the air volume supplied to the lungs. the aeration change could be estimated from the △eelv obtained at two different peep levels. functional eit images of compliance and overdistension and collapse clearly change after inducing the lung collapse model. these functional images together with time delay images could clearly localize overdistended and collapsed parts of the lungs. diagnostic parameters of cov, a/p ratio, gi, and cv provided useful feedback information during the peep titration to recruit collapsed parts of the lungs. this change can also be used for optimal peep selection. within the peep titration for finding optimal peep, both methods of ventilation homogeneity and tissue characterization can be tried through the analysis screen view of the integrated eit system. after applying optimal peep in clinical trials, validation should be verified by monitoring the change of ventilation characteristic in the real-time screen view. also, we need to consider the issues of interpreting for the integrated eit system results. we measured the macroscopic phenomena due to the microscopic changes in the alveoli and airway. therefore, careful insight is required based on understanding the"
"to detect overdistended and atelectatic lung regions, a joint analysis of several eit images is performed since those regions can be characterized by different values of compliance and tv. to distinguish between these two regions, the following key observation is employed: an increase in pressure decreases the compliance of an overdistended region whereas it increases the compliance of an atelectatic region. while increasing peep from 0 to 25 cmh 2 o in a step of 5 cmh 2 o, six compliance images are produced for each pixel, a plot of six compliance values versus peep is obtained, where this pixel compliance curve is concave for most pixels. we set the peep value when the maximum compliance c max occurred as peep c max . while the peep value was smaller or larger than peep c max, the pixel belonged to an atelectatic or overdistended region, respectively. for a pixel at a certain peep value, the difference between c max and its compliance is computed as c, which quantifies the degree of atelectasis or overdistension. the tissue classification image is derived by applying this analysis to all pixels and can guide the selection of an optimal peep value to recruit collapsed lungs with a minimal amount of overdistension [cit] ."
"where t 40% j is the time point at which the impedance of j pixel reaches 40% of its maximum value and t max − t min is the time difference between start-inspiration ( t min ) and end-inspiration ( t max ). the average opening time is calculated in the same way using the global impedance signal that is the sum of all pixel values. the ventilation delay image is formed from the difference image between each pixel for opening time and the average opening time [cit] . the unit of ventilation delay was converted from a percentage to the time unit to facilitate comparison with clinical outcomes. in the real-time screen view, we additionally display the standard deviation of ventilation delay calculated from the ventilation delay image as a measure."
"results obtained for cancer and liver disease data sets with svm and random forest using different kernel functions like linear, polynomial, radial basis and sigmoid are tabulated. it is noticed that there is a varying accuracy of classification with different probabilistic estimate with different kernel function."
"in cancer research, advances in technology have resulted in the generation of vast quantities of complex data. barriers to the effective use of this data include heterogeneity and lack of interoperability between isolated resources (1) . we attempted to overcome these obstacles to cancer research by using biomart (2) to design a generic model for a comprehensive cancer resource. initially, focusing our efforts on pancreatic cancer, we established the pancreatic expression database (ped) (3) (4) (5) ."
"to follow the trend consensus, time-difference eit dynamic images are expressed as images of resistivity changes between each time and a chosen reference time. the clinical significance of an eit image is based on the theoretically and experimentally validated correlation between its pixel value and the air volume at the corresponding location inside the lungs [cit] . different functional images and parameters can be derived from a time-series of eit images based on this correlation. table 1 shows the list of functional images and measures adopted in the integrated eit system among the parameters of the lung eit proposed by trend. these are classified into five categories including three different groups of functional images and two groups of parameters."
"plans for the database include expanding to include reanalysed differential expression data and methods to enable users to assess the quality of the information added to the database. there are also plans to improve the graphical data view, especially for genomic information."
the global inhomogeneity index ( gi ) in (4) expresses the degree of inhomogeneous air distributions in the lungs [cit] . the gi value is computed by normalizing the total deviation from the median value by the sum of pixel values:
the use of biomart as a framework facilitates interoperability with other cancer resources and enables users to cross-query data from a number of relevant resources rather than being limited to a single database. the international cancer genome consortium uses biomart technology to share data and make it publicly available. data from ped is automatically cross-queried from the icgc (see icgc paper in this issue) and can be queried with cosmic data (19) via the biomart framework. this allows direct cross-comparison of experimental findings generated from the two icgc pancreatic cancer projects (australia and canada) with literature-derived information from ped.
"the support vector machines svm have been developed by vapnik [cit] and are yielding good results due to many challenging features and better empirical performance. svm basically uses the structural risk minimization (srm) principle; this is superior [cit] to traditional empirical risk minimization (erm) principle, being used by conventional neural networks. erm technique tries to minimize the error on the training data but srm tries to minimize an upper bound on the expected risk with maximizing separating planes. this difference makes svm to work with a better ability to generalize the training data and make predictions. this is always the goal of statistical learning. svms were developed basically to solve the classification problem, but currently they are also being used to solve regression problems [cit] . fig. 1 shows an over fitting classifier with data overlapping with training data. fig. 2 shows a better classifier with almost no overlapping"
"electrical impedance tomography (eit) is a noninvasive imaging method to produce cross-sectional images of electrical conductivity distributions inside the human body [cit] . among numerous clinical applications, time-difference eit has been used for real-time bedside imaging and monitoring of a mechanically ventilated patient in intensive care units (icu) [cit] . during mechanical ventilation, it is important to recruit collapsed parts without causing overdistension of other parts of the lungs. the so-called lung protective ventilation (lpv) requires continuous bedside imaging of regional lung ventilation in response to a ventilator setting and/or a posture of the patient. proper interpretation of functional lung ventilation images is essential to search for an optimal patient-specific ventilator setting including respiration rate (rr), airway pressure, tidal volume (tv), positive end-expiratory pressure (peep), end-expiratory lung volume (eelv), global and regional lung compliances, posture, and other respiratory mechanics parameters [cit] ."
the eit data stores the measurement time information and the measured voltage data set in the binary format every 512 frames. airway volume and pressure data measured from the ventilator are also saved as text format with pc time information.
"the ped system comprises of tools capable of querying data content either by using simple queries based on an individual premise such as gene expression, or by combining information across multiple data types such as conducting a query addressing both gene expression and copy number data (figures 1 and 2) . by providing the capacity to refine any biological data query according to various criteria, ped provides a resource that allows the pancreatic cancer community to explore and find new relationships among the factors that contribute to the pathogenesis of this disease. the resulting information can be used to elucidate the changes associated with tumourigenesis and the development of resistance to treatment and aid in the development of novel molecular diagnostic tools for the prevention and diagnosis of pancreatic cancer."
"different peep values were sequentially applied to normal pigs at 0, 5, 10, 15, 20, 25, 20, 15, 10, 5, and 0 cmh 2 o . the chest x-ray ct images of the pig in fig. 2a were obtained after setting each peep value to confirm its effects. note that the lungs were progressively more and less inflated as the peep value was increased and decreased, respectively. figure 2b -d shows the reconstructed functional images of tv, △eelv, and compliance, respectively, during the peep titration. ventilation delay image was confirmed in the collapse model where the delay difference was clearly shown. as the peep value increased, △eelv increased as expected. since increased △eelv reduced the lung compliance, the tidal volume decreased at higher values of peep. the correlation between the compliance image and the global lung compliance estimated using the pressure/volume data from the mechanical ventilator was high with the r 2 value of 0.96. these results showed that eit images could be used to quantitatively measure air distribution in the lungs in real time."
"to demonstrate the utility of ped, we present several biologically relevant queries that can be performed using the current system. predictive biomarkers are vital to cancer research and have been shown to enhance long-term survival for many solid tumours. this data are vital in the identification of asymptomatic, early-stage disease biomarkers. this is especially true for pancreatic cancer, one of the most lethal of solid tumours in which patients tend to be diagnosed in advanced stages of the disease (14) ."
"the ventilator-induced lung injury (vili) should be avoided when an increased peep level is used to recruit any collapsed parts of the lungs. to select an optimal peep level, the regions of collapse and overdistension should be identified and the effects of varying the peep level on those regions should be observed. figure 7 shows the optimal peep analysis through the analysis screen view of integrated eit system in lung collapse model. during the decreasing phase of the peep titration from 25 to 0 cmh 2 o in 5 cmh 2 o step, the reconstructed compliance images were analyzed to obtain the maximum compliance at each pixel during the peep titration. the airway pressure from the mechanical ventilator at the time of maximum compliance is denoted as p max . at each peep value, every pixel is classified as normal, collapse, or overdistension using the compliance difference c between c max and its compliance at the peep value. overdistension and collapse images show how the amounts of the overdistended and collapsed parts of the lungs changed as the peep level decreased. overdistension and collapse can also be provided as measures according to peep titration. the peep level of 7.9 cmh 2 o was chosen as the optimal level since it could minimize both overdistension and collapse values. optimal peep selection using air distribution parameters was also provide in the analysis screen view. it shows the change of gi and cv according to peep level, and it guides peep selection through air distribution inhomogeneity. from these results, gi and cv were significantly reduced from over 10 cmh 2 o peep."
"disease diagnosis and prediction involves multiple physicians from different specializations in case of cancer, liver disorders and heart disease. this requires multiple biomedical markers and multiple clinical factors like the age, general health of the patient, its location, type of disease, the grade and size of the disorder. for reasonable prediction information like cell based, patient based and population based all must be carefully considered by the attending medical practitioner. it is challenging even for the most skilled technician to do. both physicians and patients need to face same challenges when it comes to the matter of disease prevention and disease prediction. family history, age, diet, weight, habits like smoking, heavy drinking, and exposure ultra violet radiations, radon, asbestos plays a major role in predicting an individual's risk for developing the said diseases. sometimes these conventional clinical, behavioral parameters and environment may not be sufficient to make better predictions. in most of the critical cases to predict the disease we need some specific molecular details about either the infected part or the patient's genetic status. with the speedy development of the proteomic, genomic and imaging technologies, this molecular scale information about patients is now can be readily acquired."
"the initial form of support vector machine is to maximize the distance separating the elements of two different classes. when the classes to which the elements belong to are known a priori, the problem is called classification. the set of data used to calculate the boundary limit between the classes is called the training set, while the data set used to test the efficacy of the method, is called validation set."
"air distributions in porcine lungs with collapsed parts during peep titration figure 5 graphically presents the estimated center of ventilation ( cov ) and anteriorto-posterior ventilation ratio ( a/p ratio ) values of the porcine lungs with collapsed parts during the peep titration. right after inducing the lung injury, the a/p ratio was significantly increased by 51.1% because the lung collapse occurred more in the dorsal (posterior) region than in the ventral (anterior) region as confirmed by the x-ray ct images. similarly, the cov was shifted toward the ventral region. as the peep level increased, the a/p ratio was restored to 91% of the normal value. figure 6 shows how the a/p ratio and cov changed during the peep titration. right after inducing the lung injury, the cov y decreased to 45.3% from its normal value of 50% and then increased back to 48.3% as the peep level increased to 20 cmh 2 o . on the other hand, the cov x changed less than 1%. both global inhomogeneity index ( gi ) and coefficient of variation ( cv ) indices increased immediately after the lung injury and then decreased back to their normal values at 20 cmh 2 o peep."
the architectural flexibility of ped biomart-based schema means that it can be easily extended to encompass additional malignant and non-malignant diseases and has been used as a prototype for other malignant diseases such as breast cancer (http://bioinformatics.breastcancertissuebank.org).
"the collapse model was used to confirm a significant change in ventilation and ventilation delay. after producing the collapsed part through the saline lavage method, we increased the peep from 5 to 20 cmh 2 o . figure 3 shows the x-ray ct images in (a) and two different eit functional images in (b) and (c) before and after producing the collapsed parts. when inducing the lung collapse model using saline lavage in the supine posture, the collapse of the alveoli was severely induced in the dorsal region due to gravity. the x-ray ct images show that the lung collapse occurred in the dorsal region clearly. the eit ventilation images in (b), expressed as a sum of tv and △eelv images, showed decreased pixel values after the injury. as peep increased, the collapsed parts were gradually recruited. in the ventilation delay images in fig. 3c, positive pixel values (blue region) indicate that the corresponding pixels inflated earlier than other pixels. the inhomogeneity of the ventilation delay image after inducing the injury was sharply increased. the air was supplied to the dependent region relatively late compared to the other regions. when the peep level was increased, the difference in pixel values within the ventilation delay image was reduced. the degree of homogeneity in regional ventilation delay images has a possibility to be used as an index to evaluate tidal recruitment [cit] . from the plots in fig. 4a, we can see that the lung volume increased rapidly at 15 cmh 2 o peep and was recovered to 87.1% of the normal value at 20 cmh 2 o peep. in fig. 4b, the inhomogeneity of ventilation delay was increased significantly after inducing the lung injury and decreased as increasing the peep level [cit] ."
"the real-time screen view (fig. 9) shows the pulmonary function change during mechanical ventilation. dynamic image and global impedance signal were directly calculated from two successive data for the time-series of reconstructed eit images within 8 ms. the computation time was faster than other processing time for eit data acquisition and reconstructing tomographic images. these are updated at same speed as the imaging frame rate of the eit system. in the real-time screen view, four functional images of tidal volume (tv), end-expiratory lung volume change ( △eelv), compliance, and ventilation delay were computed within 20 ms. these functional images can be updated at every breath. additionally, we presented cov and a/p ratio as figures for easier indication. four parameters of a/p ratio, gi, cv and ventilation delay were displayed with the format of the time chart. especially, ventilation delay originally presented in image format, but it was better to display along with time chart as the standard deviation of ventilation delay because it can provide more intuitive interpretation."
"the database describes the association of over 6000 dna copy number alterations; 8000 genes and their 30 000 transcripts and 22 000 proteins; and 279 mirnas in pancreatic cancer as well as the observed levels of deregulation among a broad range of specimen and experimental types (table 1) . these include healthy/patient tissues and body fluids specimens, cell lines and murine models as well as the provision of information pertaining to any treatments/drugs administered to the samples during the study."
"the idea of empirical data modeling is applicable to many engineering applications. empirical data modeling uses an induction process to build up a model of the system, from which it can deduce responses of the system which are to be tested or observed. the observational available data obtained is finite and taken as a sample. this sampling is non-uniform and due to the high dimensional nature of the problem data, the input space will be in a sparse distribution. as a result the problem more often is wrongly presented."
our thanks to the experts who have contributed towards development of the concepts and feed back on this paper. the authors would also like to thank the anonymous reviewers for their extensive and helpful feedback.
"examination-specific measure overdistension/collapse image is computed as the difference of two eit images obtained at the end-inspiration and end-expiration times. the end-inspiration and end-expiration times can be detected by searching for peaks and valleys in the time-varying signal derived by summing all pixel values. the sum of all pixel values is called the global impedance signal since it is proportional to the sum of all trans-impedance values across the chest. one tv image can be produced for each breathing cycle and a time-series of tv images expresses ventilation changes over multiple breathing cycles. the eelv image represents the amount of residual air remaining in the lungs at the end of expiration. the total eelv can be measured using a spirometer with the nitrogen washout or helium dilution techniques. in lung eit, the aeration image ( △eelv) is calculated as the difference of two eit images measured at two end-expiratory times. the △eelv images, therefore, represent eelv changes between two end-expiratory times especially during a peep titration."
abbreviations a/p ratio: anterior-to-posterior ventilation ratio; cov: center of ventilation; crs: respiratory system compliance; cv: coefficient of variation; eelv: end-expiratory lung volume; eit: electrical impedance tomography; fer: fidelity-embedded regularization; gi: inhomogeneity index; icu: intensive care unit; lpv: lung protective ventilation; peep: positive endexpiratory pressure; rr: respiration rate; tv: tidal volume.
"the results retrieved in response to query 1 display genes that have been expressed in all the panin collection included in the database. the results table will also provide links to the original studies. in this instance, the deregulation of the s100p gene is highlighted as an early event in the development of pancreatic cancer. this is in accordance with previous published findings (16, 17 determination of reliable non-invasive biomarkers, such as those present in serum, are important when attempting to avoid surgical intervention and limit any physiological and psychological stress on the patient. ped not only allows users to query profiles derived from tissues but also those from media such as serum, plasma and urine. in addition, linking of the ped resource to reactome (18) enables both the identification of potential genes specific to pancreatic cancer for biomarker discovery and visualization of the affected pathways."
"we will first describe the summary of animal experimental results using an eit device whose performance was fully validated [cit] and the integrated platform with 11 functional imaging methods and associated parameters briefly. then, we will discuss the pros and cons of the integrated eit system and suggest future clinical study designs."
"considering that eit could be a potential solution to this clinical problem, numerous attempts have been made to produce and interpret functional eit images of lung ventilation using custom-designed or commercial eit devices [cit] . to find an optimal ventilator setting, various criteria have been proposed and tested in experimental studies of animal and human subjects [cit] . though new clinical findings have been progressively utilized for lpv of patients in icu and also to treat patients with pulmonary dysfunction [cit], there was no standard and/or clear definitions of parameters needed to accelerate clinical acceptance. following the significant efforts of several concerted actions [19, [cit], the interdisciplinary group called translational eit development study (trend) was recently formed and built a consensus for clinical applications of lung eit by comparing and analyzing related studies over the past 30 years [cit] ."
"ped is a major resource for the integration and mining of pancreatic cancer literature data. with its data content constantly growing, ped contains the largest collection of pancreatic cancer molecular profiling data sets. currently, the database includes over 60 000 measurements obtained using a range of omics technologies; including transcriptomics, proteomics, genomics and mirna. the expansion in data content improved query capabilities and enhanced interoperability have facilitated the systematic study of pancreatic cancer. although a number of expression data repositories and literature databases exist, ped provides a unique way to integrate and mine data specifically related to pancreatic cancer and cross-query data from other biomart databases and, therefore, allows the community to perform highly detailed queries on facets of pancreatic cancer."
"after applying the optimal peep, the real-time screen view can be used to monitor the recovery of the real-time pulmonary function. figure 8 shows the results after inducing acute lung injury. the effects of lung injury are shown in the dynamic image and other functional images. note that the values of cv, gi, sd delay and a/p ratio are increased when compared with those values shown in fig. 9 before the injury. the optimal peep effect can be expected to restore the values to the before injury state."
integrated eit software platform was implemented in pc with the eclipse environment (java se 8). the pc was communicated with a 16-channel khu mark 2.5 eit device with adjacent current-injection and voltage-measurement protocol and receiving measured data through usb 2.0 [cit] . rs232 serial port was used for storing air volume and pressure data measured from a mechanical ventilator. functional images and measures were generated through data post-processing as described in the following sections.
the coefficient of variation ( cv ) is a statistical measure defined as the ratio of the standard deviation over the mean. the cv value is used to express the heterogeneity of the air distributions in the lungs for obstructive pulmonary diseases [cit] .
"where ν is small constant, preventing g x and g y to take the value 0 and ζ is a normalizing factor. tv is desirable in 5 in a digital image, both domains d and ω are discretized into a lattice, and dt is fixed. therefore, spatial and temporal derivative operators are approximated, typically, by first-order differences. we use the formal notation"
"the asymmetric unit of the title compound is shown in fig. 1 . the thiazolidine (s1/n1/c8-c10) ring is essentially planar, with a maximum deviation of 0.071 (2) å for atom c10. the central thiazolidine (s1/n1/c8-c10) ring makes dihedral angles of 88.01 (8) and 87.21 (8)° with the terminal phenyl (c1-c6) and (c11-c16) rings, respectively. the dihedral angle between the phenyl (c1-c6) and (c11-c16) rings is 49.45 (5)°."
"where t denotes the input (e.g. ∆e, l * ) value, l is the number of distinct gray levels, ω 1, ω 2 are percentages of pixels that belong to the background and defects, respectively, and µ 1 and µ 2 are the average values of background and defect pixels."
"the traditional way of print quality diagnosis relies on manual examination of a printed page, which is specially designed for the testing purpose. the assessment work that is usually conducted by well-trained experts, includes marking exact areas with local defects and rating the overall page. each test page can be rated as \"a\" \"b\" \"c\" or \"d\" four ranks, in which \"a\" and \"b\" mean the page passes the print quality assessment, while \"c\" and \"d\" mean the page fails the assessment. however, given the large number of pages to be evaluated, manually examining all pages is too costly and time-consuming. to solve this problem, a print defect detection algorithm is highly desired for building a smart print quality diagnosis system."
"we emphasize that our approach does not assume a rigid scene, or a single moving object. it also does not assume that the occluded region is simply connected. instead, our model is general under the assumptions (a)-(b) as we show in appendix a, and allows arbitrary (piece-wise diffeomorphic) domain deformations, corresponding to an arbitrary number of moving or deforming objects, and an arbitrary number of simply connected occluded regions (jointly represented by a multiply-connected domain ω)."
"to a well ground intimate mixture of triphenyl phosphine (0.43 g, 1.6 mmol) and benzaldehyde, (0.15 g, 1.5 mmol) in a microwave vial (10 ml) equipped with a magnetic stirring bar, benzylazide, (0.2 g, 1.5 mmol) was added in drop with stirring. stirring was continued until liberation of nitrogen ceased and then mercaptoacetic acid, (0.15 g, 1.6 mmol) was added to the above mixture and the reaction vessel was sealed with a septum. it was then placed into the cavity of a focused monomode microwave reactor (cem discover, benchmate) and operated at 150°c (temperature monitored by a built-in ir sensor), power 80w for 10 minutes. the reaction temperature was maintained by modulating the power level of the reactor."
"after descreening the half-toned pages, we need to transfer the pixels from the rgb color space to the cie l*a*b* color space, where l * is the lightness and a * and b * are the greenred and blue-yellow color components, respectively. compared with the rgb color space, l*a*b* is designed to be perceptually uniform according to human color vision [cit] . thus, the l*a*b* color space is widely used in color comparisons."
"each block sample contains the following three types of metadata: 1) glocal features, including filename, block index, and color, which are related to the whole page; 2) blockwise features, including the block's x and y coordinate ranges, average l *, a *, b * values, dde, mdl, ddl, and the ground truth of local defects. these features can help us locate the block among the raw pages, and are only related to the block itself; 3) local defect features: light/dark, defect size, equivalent eclipse's major and minor axis length, and severity. only the 5,043 blocks that passed initial selection have the third type of features."
"taking the final output blocks of the decision tree, we can get the final detection output as shown in figure 12 . by aggregating their information we can generate a feature vector for the whole test page. there are several features that we care about: number of all types of defects on the test page, number of each type of defects, local defects' average size, and maximal and minimal size, and standard deviation of the size, average severity, maximal and minimal severity. in addition, we also want to know the average location of all local defects to determine whether or not their distribution is random. the output feature vector for a test page is listed in table 1 ."
"in this paper, we develop a blockwise algorithm to detect and characterize local defects. this method involves a coarse-tofine strategy in defect areas detection: first select possible regions by simple thresholding, and then apply a decision tree to exclude false alarms. in addition, our algorithm can classify different local defects according to their perceptual attributes including size, brightness, and other aspects."
"in this manuscript we (i) show that, starting from the standard assumptions (a)-(b), the problem of detecting (multiply-connected) occlusion regions can be formulated as a variational optimization problem (sect. 2). we then (ii) show how the functional to be minimized can be relaxed into a sequence of convex functionals and minimized using re-weighted 1 optimization (appendix a and (16)). at each iteration, the functional to be minimized is related to those used for optical flow estimation, but the minimization is with respect to the indicator function of the occluded region, not just the (dense) optical flow field. we then bring to bear two different approaches to optimize these functionals, one is (iii) an optimal first-order method, due to nesterov (sect. 3), and the other is (iv) an alternating minimization technique, known as split-bregman method (sect. 4). we evaluate our approach empirically in sects. 5 and 1.2, and discuss its strengths and limitations of in sect. 6."
"in the crystal structure, (fig. 2), the molecules are linked by intermolecular weak c-h···o hydrogen bonds forming supramolecular chains along the b-axis. furthermore, the crystal packing is stabilized by weak c-h···π interactions involving the c11-c16 ring."
"where β indicates the amount of relaxation. to solve (23), we divide the optimization problem into three subproblems and solve them iteratively. the first subproblem iŝ"
"occlusions arise when a portion of the scene is visible in one image, but not another. in da vinci steropsis, portions of the scene that are visible from the left eye are not visible from the right eye, and vice-versa. in a video-stream, occlusions typically occur at depth discontinuities. we are interested in determining the occluded regions, that is the subset of an image domain that back-projects onto portions of the scene that are not co-visible from a temporally adjacent image. 1 the occluded region is, in general, multiply connected, and can be quite complex, as the example of a barren tree illustrates."
"the overall workflow of our method is shown in figure 2 . the detection of local defects can be roughly divided into two stages: finding candidate areas, and verify the defect features inside each candidate block to give the final results. the two-stage detection pipeline can greatly reduce the runtime while ensuring the required miss rate."
"several algorithms have been proposed to perform occlusion detection. many define occlusions as the regions where forward-and backward-motion are inconsistent [cit] . this is problematic as the motion in the occluded region is not just inconsistent, it is undefined, as there is no \"motion\" (domain deformation) that takes one image onto another. other approaches [cit] formulate occlusion directly as a classification problem, and perform motion estimation in a discrete setting, where it is an np-hard problem. this can then be approximated with combinatorial optimization. others [cit] also exploit motion symmetry to detect occlusions and weight the inconsistencies with a monotonically decreasing function."
"laser electrophotographic (ep) printers have been widely used in past decades. as one of the most important criteria in evaluating the performance of a printer, print quality is not only of concern to customers, but also designers of printers. for the reason that print quality can reflect the current working status and reveal hidden mechanical problems inside of a printer, the assessment of print quality has continued to be an important topic in printer-related research."
"the additional term p(t) is the percentage of pixels at level t. it serves as a weighting term, so that the lower the percentage is, the higher the weight is."
"another set of algorithms infer occlusion boundaries [cit] and occluded regions [cit] ) training a learning based detector using appearance, motion and depth features. the accuracy of these methods largely depends on the performance of the underlying feature detectors."
"portions of the scene that are co-visible can be mapped onto one another by a domain deformation [cit] ), called optical flow. it is, in general, different from the motion field, that is the projection onto the image plane of the spatial velocity of the scene [cit], unless three conditions are satisfied: (a) lambertian reflection, (b) constant illumination, and (c) constant visibility properties of the scene. most surfaces with benign reflectance properties (diffuse/specular) can be approximated as lambertian almost everywhere under sparse illuminants (e.g., the sun). in any case, widespread violation of lambertian reflection does not enable correspondence, so most optical flow methods embrace (a), either implicitly or explicitly. similarly, constant illumination (b) is a reasonable assumption for ego-motion (the scene is not moving relative to the light source), and even for objects moving (slowly) relative to the light source. assumption (c) is needed in order to have a dense flow field: if an image contains portions of its domain that are not visible in another image, these can patently not be mapped onto it by optical flow; (c) is often assumed because optical flow is defined in the limit where two images are sampled infinitesimally close in time, in which case there are no occluded regions, and one can focus solely on discontinuities 2 of the motion field. thus, the great majority of variational motion estimation approaches provide an estimate of a dense flow field, defined at each location on the image domain, including occluded regions. in their defense, it can be argued that for small parallax (slowenough motion, or far-enough objects, or fast-enough temporal sampling) occluded areas are small. however, small does not mean absent, nor unimportant, as occlusions are critical to perception [cit] ) and a key for developing representations for recognition. for this reason, we focus on occlusion detection in video streams."
"and note that, because of (i) e 1 is large but sparse, while because of (ii) e 2 is small but dense. 4 we will use this as an inference criterion for w, seeking to optimize a data fidelity term that minimizes the number of nonzero elements of e 1 (a proxy of the area of ω), and the negative log-likelihood of n."
"by inspecting each roi according to the process shown in figure 9, we can get the attributes above. along with dde, mdl, ddl, these features will be used in the following steps to exclude false positives (invisible defects). to better visualize the detected visible defects, we draw a white bounding box around the block with gray spots, and a black box around the block with dark spots. we use the connected components algorithm to combine bounding boxes of adjacent blocks with the same type defect into one bigger box. if we mark blocks with visible local defects as 1, and the rest as 0, we can plot their distribution versus each feature as shown in figure 10 . according to these plots, we can tell that mdl, ddl, and dde are all good metrics for choosing candidate blocks in the initial step. the severity can exclude abnormal cases (e.g. infinite values). with defect size, we can exclude cases that are too small to see. similarly, the major and minor axis lengths can help us exclude thin lines that are imperceptible and non-local. although all these features obviously correlate with the existence of visible local defects, there is no single threshold that divides blocks with/without defects."
"we have presented an algorithm to detect occlusions and establish correspondence between two images. it leverages on a formulation that, starting from standard assumptions (lambertian reflection, constant illumination), arrives at a variational optimization problem. we have shown how this problem can be relaxed into a sequence of convex optimization schemes, each having a globally optimal solution, and presented two efficient numerical schemes for solving it."
"finally, we have evaluated the performance of our algorithm on the middlebury evaluation dataset, table 4 . nesterov's algorithm is used to estimate the optical flow on the evaluation set. in this experiment, we constructed the image pyramid with 11 levels with scale factor 0.75 and applied 10 warpings at each level. the flow fields estimated before the reweighting stage are ranked 10th and 12th in terms of aepe and aae respectively while the estimates after reweighting step are ranked 13th in terms of both error measures as of june 7, 2011. our method also detects the occluded regions accurately on most of the sequences at evaluation set, fig. 7 ."
"one of the main objectives of organic and medicinal chemistry is the design, synthesis and production of molecules having value as human therapeutic agents. during the past decade, combinatorial chemistry has provided access to chemical libraries based on privileged structures with heterocyclic moiety receiving special attention as they belong to a class of compounds with proven utility in medicinal chemistry. there are numerous biologically active molecules with five-membered rings, containing two hetero atoms. among them, thiazolidin-4-ones are the most extensively investigated class of compounds, which have many interesting activity profiles namely bactericidal [cit], antifungal [cit], anticonvulsant [cit], anti-hiv [cit], antituberculotic [cit], cox-1 inhibitors [cit], inhibitors of the bacterial enzyme murb [cit], non-nucleoside inhibitors of hiv-rt [cit] and anti-histaminic agents [cit] ."
"optical flow estimation is a mature area of computer vision, and benchmark datasets have been developed, the best known example being the middlebury [cit] ). unfortunately, the middlebury benchmark does not provide ground truth for occlusions on the entire dataset or a scoring mechanism to evaluate the performance of occlusion detection algorithms. unfortunately, this also biases the motion estimation scoring mechanism as ground truth motion is provided on the entire image domain, including occluded regions, where it can be extrapolated using the priors/regularizers, but not validated from the data."
"where v 1 and v 2 are the first and second components of the optical flow v, μ is a multiplier factor to weight the strength of the regularizer and the weighted isotropic tv norm is defined by"
"to overcome this gap, we have produced a new benchmark by taking a subset of the training data in the middlebury dataset, and hand-labeling occluded regions. we then use the same evaluation method of the middlebury for the (ground truth) regions that are co-visible in at least two images. this provides a motion estimation score. then, we provide a separate score for occlusion detection, in terms of precision-recall curves. this dataset (that at the moment is limited by our ability to annotate occluded regions to a subset of the full middlebury, but that we will continue to expand over time), as well as the implementation of our algorithm in source format has been released publicly (http://vision.ucla.edu/optical-flow)."
"we create a blockwise dataset that includes several types of local defects including gray spots, pinholes, etc. for further refinement models. this dataset is from 15 test pages with 66 uniform color regions including 12 colors. these test pages are in a4 size, scanned with 600 dpi. taking out margins and barcode areas, 67,465 blocks are sampled from all test pages, among which 1,502 blocks are marked as blocks with local defects. after initial computation, 5,043 blocks are selected as rois by our algorithm."
"the test pages are letter-size pages with at least one constanttint area that is printed with one solid color (cyan, magenta, red, etc). these test pages are scanned at 600 dpi and stored in portable network graphs (png format) that includes an alpha channel as a mask, which is a binary channel where 0 (black) stands for the masked part while 1 (white) is for the content. the mask channel is used to tell our algorithm which part of the test page should be processed. because besides those constant-tone areas that our defect detection algorithm focuses on, each test page also has some other contents such as fiducial marks, and a barcode that records metadata about the master file and the original printer. also, the unprinted areas, e.g., white edge, should also be excluded from our area of interest. figure 3 shows an original image, and the masked image of one test page."
"the reaction mixture was allowed to stand at room temperature. then the residue was purified by column chromatography on silica (petrolium ether-ethyl acetate, 94:6) to afford the 3-benzyl-2-phenylthiazolidin-4-one. yield: 0.38g (95%); m.p."
"repeat the calculations above until we go over all blocks. higher dde is related to a block with more fluctuations, thus it is more likely to have local defects, we take the dde as the metric to get the candidate roi. as shown in figure 7, first we remove the baseline of dde to make our algorithm less sensitive to local noise. then we need to filter out the blocks with less fluctuation by thresholding. the peaks remaining in the last graph identify the blocks that comprise the roi."
"occlusion detection would be easy if the motion field was known. vice-versa, optical flow estimation would be easy if the occluded domain was known. as often in vision problems, one knows neither, so in the process of inferring the object of inference (the occluded domain) we will estimate optical flow, the \"nuisance variable,\" as a byproduct."
"in figure 8, we compare the results of applying the valleyemphasis algorithm followed by thresholding, to the descreened image expressed in either ∆e, l * units. figure 8 shows the results of different inputs after thresholding. the left-most image is the descreened input block, which includes a gray spot on the upperleft corner and some dispersed dark agglomerates. the result of l * seems to be more focused on the gray spot region, while the ∆e result marks out both the gray spot the and dark agglomerates. for comparison, we also show the result that is directly acquired from the fwhm (full width at half maximum) of ∆l, which is more close to the ∆e results. the reason for this difference lies in the operation to get the \"∆ values\", which is computing the absolute difference from average. in this manner, both dark and light regions strongly deviate from average, so that they are both marked out by the valley-emphasis algorithm. according to more comparisons on our test pages, the l * inputs tend to give more gray spot results while fewer dark spots. depending on our actual needs, we can choose either input to maximize accuracy. after we get the mask of defects within a block, we can conduct the analysis for its attributes:"
"our algorithm can not only be applied on print quality assessment, but also on detecting the scratches and contamination in the manufacturing of glass touchpads. figure 13 shows that our method is robust to background noise that is generated from the matte surface and uneven lighting."
"in this paper, we develop a coarse-to-fine method to automatically detect local defects, including the initial detection by thresholding, and the secondary refinement by a trained model. different from previous works, we propose block-wise features to describe attributes of visible defects in the candidate area, which can help us determine the exact defect type. with these proposed features, we build a block-wise dataset of local defects for future training. a decision tree model is applied to produce more accurate results for visible local defects. finally, we agglomerate block-wise results to generate a feature vector for each test page, which can be used for further assignment of page rank."
"since the local defects are randomly located on our test page, it is very likely that a local defect falls on the boundary or even a vertex of the grid. if such a case happens, these local defects might be hard to detect. so it is necessary to search a second time for the missing defects. in the second detection, we move the grid by 35 pixels in both x and y directions from its initial location. figure 6 shows the difference between the two grids. the local defects that cannot be detected in the first time would fall in the middle of the block. so we just need to combine the two detection results to get all local defects. however, there can be overlaps between two detection results. the connected components algorithm is applied to count the local defects accurately. the combined output is our initial estimation of areas with local defects, or the region of interest (roi)."
"denote average values within the block j, and l * i j, a * i j, b * i j stand for the pixel values. after finishing the calculations all the pixels, the mean ∆e (mde) for a block j can be computed:"
"the local defect is one of the print defects of most critical concern. typical local defects include gray spots and solid spots. the gray spot (also called carrier spot) is a phenomenon of low density around the agglomerates, which usually happens when the toner transfer from the organic photoconductor (opc) to the intermediate transfer belt (itb) is blocked by some developed car-* this work was supported by hp inc, boise, id 83714, usa riers or toner agglomerates on the opc. thus, a poor transfer of toners occurs around the agglomerates. an obvious visual feature of gray spots is that their color is lighter than that of the surrounding content (as shown in figure 1 )."
"in order to implement this scheme, we need to address the nonsmooth nature of 1 . [cit], that has already been used profitably for noise reduction, inpainting and deblurring [cit], and incorporated in software libraries for sparse recovery [cit] ). in our case, we write ψ(v 1, v 2, e 1 ) as a summation of terms"
"x y z u iso */u eq s1 0.04954 ( (10) 0.0561 (9) 0.0005 (7) 0.0099 (7) 0.0130 (7) (7) 0.0020 (7) 0.0074 (7) c8 0.0561 (9) 0.0309 (7) 0.0462 (8) −0.0036 (6) 0.0185 (7) 0.0023 (6) c9 0.0496 (9) 0.0456 (9) 0.0662 (10) 0.0059 (7) 0.0173 (8) −0.0016 (8) c10 0.0372 (7) 0.0346 (7) 0.0369 (7) −0.0029 (5) 0.0117 (5) −0.0001 (5) c11 0.0339 (7) 0.0319 (6) 0.0382 (7) −0.0008 (5) 0.0069 (5) −0.0012 (5) c12 0.0450 (8) 0.0365 (7) 0.0472 (8) −0.0005 (6) 0.0165 (6) 0.0010 (6) c13 0.0495 (9) 0.0448 (9) 0.0555 (9) −0.0074 (7) 0.0130 (7) 0.0118 (7) c14 0.0547 (10) 0.0337 (8) 0.0667 (11) −0.0038 (7) −0.0020 (8) 0.0079 (7) c15 0.0611 (11) 0.0368 (8) 0.0649 (11) 0.0126 (7) 0.0050 (9) −0.0067 (7) c16 0.0471 (8) 0.0443 (8) 0.0451 (8) 0.0074 (7) 0.0111 (7) −0.0034 (6) geometric parameters (å, °) s1-c9 1.7967 (17) c7-h7a 0.9700 s1-c10"
"as an illustration, the waiting time τ (a, t) is shown for our sample ornstein-uhlenbeck process on fig. 1b . then the return time r(a) for the threshold a is defined as"
"a key issue with rare event algorithms is to understand if they are actually useful to compute rare events and their probabilities for actual complex dynamical systems. the ams algorithm has shown to be very efficient for partial differential equations with noise [cit] . in this section, we give a brief illustration that more complex dynamics can be studied. we illustrate the computation of return times using rare event algorithms for a turbulent flow. the possible limitations of rare event algorithms are further discussed in the conclusion."
"shows the return time plot obtained using 50 repetitions instead of 10 in fig. 10 . figure 11 : illustration of the computation of return times for the averaged drag over the square obstacle pictured in fig. 9, using 50 repetitions of the gktl algorithm. the parameters are the same as in fig. 10 . this figure illustrates the reduction in the occurrence of plateaus for the return time curve obtained using the gktl algorithm. the dashed black line represents the reference return times. the solid blue line represents the return times obtained using the gktl algorithm."
"in order to provide a proof-of-concept for such rare events approaches for turbulent flows, we compute the return time for extreme values of the drag in a simple academic flow. the setup we consider, illustrated in fig. 9, is that of a two-dimensional channel flow, with a square obstacle immersed in the middle of the domain. turbulence is generated upstream by means of a grid. this flow is simple enough so that long time series can be obtained in a reasonable figure 9 : snapshot of a typical vorticity field of the flow under study. a steady parabolic velocity profile is imposed at the inlet. turbulence is then generated by a grid. we used the gktl algorithm the compute the return times of the average drag over the square here marked by the grey area."
"from a modelling perspective, it is natural to assume that successive occurrences of a rare event are independent from one another [cit] . then, the average number of events occurring in a time interval is proportional to the length of that interval. this is the definition of a poisson process. in this case, all the statistics are encoded in a single parameter, the rate of the poisson process. in the following, we will assume that we are dealing with the simple case of a well identified process that can be described by a single return time or rate. this is often a sufficient framework; indeed the long time behaviour of many systems can be described phenomenologically, or exactly in some limits, as markov processes described by a set of transition rates describing independent processes (see for instance [cit] for systems driven by a weak noise). we note however that many other physical systems are not amenable to such a simple effective markov processes, for instance structural glasses or amorphous media."
the validity of the mean field approximation and the fact that the typical relative error due to this approximation is of order 1/ √ n has been proven [cit] to be true for a family of rare event algorithms including the one adopted in this paper.
"in section iii a, we proposed two choices for the number of iterations in the algorithm. first, we described the algorithm with a fixed number of iterations j. alternatively, as is often seen in the ams literature, one may decide to iterate the algorithm until all trajectories reach set b. then j is a random number. in that case, the threshold a which defines the set b becomes the control parameter for the stopping criterion. under those circumstances, the estimatorq m can be expressed asq"
"for rare events, plotting return times using (6), as is classically done, proves itself much more convenient and efficient than the naive sampling using (3). it is important to note however, that the use of (6) is valid only after computing maxima over an interval of duration ∆t much larger than τ c, a remark that not been considered in many previous publications. moreover, the generalisation (7) we propose in this paper is much more accurate for events with a return time of order of ∆t . this procedure to compute return time plots can also be generalised in combination with the use of rare event algorithms, as we shall see in the next section."
"members requested specific help in the spreadsheet only some of the time. in group a, the median person asked for specific help only 20% of the time (5 weeks out of 26), and the most frequent asked for help 70% of the time (18 weeks out of 26). the system assigns partners even without a specific help request, so group a members regarded it as optional. group b had far more specific help requests, because the spreadsheet was filled out synchronously in meetings to encourage participation. the median group b member made a specific help request 70% of the time (5 weeks out of 7). figure 4 shows the kinds of help requested, which varied between the groups. the most common group a request was programming and debugging help (40%), while the most common request in b was for writing (33%). both groups showed significant requests for user study design and testing (20% for a, 15% for b). we also saw instances where the specific-help field of the spreadsheet was used to make an offer of help instead of a request. one group a member was responsible for all 18 of these offers, suggesting that he preferred to give help than get it."
"pair research is intended to increase informal workplace learning, or over-the shoulder learning (otsl) [cit] . one of the benefits of otsl over more formal learning approaches is shared context. the helper understands the task the learner is trying to do and the learner's goals for doing it. twidale also argues that encouraging otsl may require not only tool support but also organizational changes that assign positive value to giving help to colleagues. pair research as a practice may help to induce these changes."
"prior to deployment, it may be important to understand how pair research integrates in the organizational context. group b may have stopped after 7 weeks in part because the team's reward structure may have recognized and reinforced individual excellence as opposed to team collaboration and learning [cit] . group b's membership may have been unclear [cit] or not stable enough to learn and adopt a new collaborative practice [cit] . moreover, group b's members could have relied on outside resources or support or felt that they were too similar to each other to support each other. member personality types, such as openness and extraversion [cit], and ability to communicate may have also influenced adoption. assessing the organizational context prior is one way to address this concern."
"we consider a continuous time markov model able to generate trajectories. it can be either a stochastic process, for instance a diffusion, or a chaotic deterministic dynamical system. let us now describe the algorithmic procedure."
"in this section, we describe the connection between the trajectory-adaptive multilevel sampling (tams) algorithm and the classical ams algorithm. the aim is to deduce the mathematical properties of the tams algorithm from the known ones for the ams algorithm. for instance we will conclude that the optimal score function is the committor function (17) . this section can be skipped by the reader interested in the algorithm only, without trying to understand the mathematical aspects."
"who should specify the activities? an activity might be proposed by the consumer, e.g. \"i need help with r\", or by the provider, like \"i can help anybody with design sketching.\" we chose to focus on consumer-specified activities in order to make productivity a primary goal. a project to-do item then becomes a natural activity. if we had wanted to prioritize learning instead, we might use provider-specified activities, as does skillshare.com, or for that matter, traditional classroom education. although consumer-specified help requests were the norm, we also saw provider-specified activities, mainly by one group member who preferred not to ask for help but offered it instead."
"also related is apprentice learning [cit] . berlin and jeffries followed several graduate student interns and their mentors in a computer science research laboratory for several months, a similar setting and duration as the results reported in this paper. one finding was that incidental learning, triggered by specific incidents in problem-solving, was a major benefit of interactions between apprentices and mentors. but apprentices also had to use strategies to minimize their use of the mentor's time. by matching people who can best help one another and rotating pairs frequently, pair research seeks to provide some of the benefits of apprenticeship learning without reliance on a particular mentor."
"how long should a pair work? we want each person to walk away from pair research with the feeling that they have accomplished something in the time they worked together. but we also want to keep the time commitment low to encourage participation and to avoid fatigue. based on the pomodoro technique [cit], our normal minimum is 30 minutes for each partner. this includes a few minutes to talk informally and off-topic, which helps establish the trust and personal connection critical for learning. with each partner taking a turn, the normal result is a one-hour session. in practice, the duration is negotiated by each pair based on the goals they set for themselves, so some pairs meet for longer than an hour."
", and error bars are computed with an onthe-fly estimation of the variance (see section iii d). moreover, repeating the experience with different choices of score functions is a way to validate the results, checking the overlap of confidence intervals."
"the classical ams algorithm is based on the evolution of an ensemble of trajectories, based on selection-mutation rules, in order to compute rare event probabilities, and more generally committor functions. return times can not be estimated directly from a committor function and require the estimation of trajectory statistics. the method we propose to compute return times involves the estimation of probabilities of trajectories with a fixed duration t a . in order to deal with this, we propose a specific modification of the classical ams algorithm, called trajectory adaptive multilevel splitting."
"a key issue with rare event algorithm is to understand if they are actually useful to compute rare events and their probabilities for actual complex dynamical systems. many of the proposed approaches fail to pass such a test, either because the algorithm is too complex to be used for complex dynamical systems, or the algorithm is restricted to specific systems (equilibrium or reversible dynamics, diffusions with small noises), or the algorithm simply fails. a key issue with many potentially successful rare event algorithms, for instance the ams algorithm and the gktl algorithm among others, is that their success depends much on the quality of the rule used for selecting trajectories. for instance the ams or the tams algorithm rely on a score function, and the gktl use as a selection rule the increment of a the time average which one aims at computing. whenever one uses a good score function, those algorithms are extremely useful and show tremendous sampling improvements [cit] . for the ams algorithm, the choice of a good score function often relies on a good rough qualitative understanding by the user of the effective dynamics that leads to the rare events. then the ams algorithm leads to excellent quantitative results, even with complex dynamical systems (see for instance [cit] ). several examples have illustrated than those algorithm may fail to lead to improvement in other cases, see for instance [cit] . faced with such difficulties, one may either use an empirical approach, or try to improve the algorithms in order to cure potential problems, as we explain now."
"lastly, fig. 7 also shows the return times estimated forx t using the ams algorithm, for a comparable computational cost. the agreement between the two estimates illustrates that the method proposed in ii b may be applied to any rare event algorithm suitable for the type of observable under study. here, while the ams algorithm allows for computing return times for both the instantaneous and time-averaged observables, the gktl algorithm is not suited for instantaneous observables."
"the goals of the pair research system were to improve productivity, informal learning, and collaboration. for productivity, the results suggest that members feel motivated by the process, and do succeed in making progress on their work. for collaboration, members interact with far more people than they normally would, which increases opportunities for further collaboration. for learning, we saw some mentions of specific tools, skills, and work practices, and also learning about other group members and their work and their abilities, which again may lay the groundwork for future collaboration."
"in designing the pair research process, we faced a number of design decisions. this section discusses these decisions and some alternatives. some decisions were based on prior work, and others learned from experience of early design iterations."
"how often should pairs change? a secondary objective to matching people with those who are best able to help is to spread knowledge and skills. to promote this objective, we bias the matching to encourage new collaborations and discourage repeating recent pairings."
"the structure of this paper is as follows: in section ii, we introduce the method to compute return times from a timeseries and from rare event algorithms. we define the trajectory-adaptive multilevel sampling (tams) algorithm in section iii. we apply the method to compute return times for the instantaneous and time-averaged observables for an ornstein-uhlenbeck process, respectively, in section iii (using the tams algorithm) and iv (using both the tams and the giardina-kurchan-tailleur-lecomte (gktl) algorithms). finally, we introduce the application to complex dynamical systems in section v, before presenting our conclusions in section vi. we discuss in the conclusions the range of applicability of these algorithms."
"we stress that the method described here does not depend on the observable of interest, or on the details of the algorithm itself. in the remainder of the paper, we provide a proof-of-concept for this method, by considering two kinds of observables, sampled by two different algorithms: first, we study the return times for instantaneous observables using the adaptive multilevel splitting (ams) algorithm (section iii), then we turn to time-averaged observables using both the ams and the giardina-kurchan-tailleur-lecomte (gktl) algorithm (section iv). we show that the method allows to compute accurately return times at a much smaller computational cost than direct simulation. in both cases, we apply the technique to the simple case of an ornstein-uhlenbeck process, for which the results are easily compared with direct simulation and theoretical predictions, before illustrating the potential of the method for applications in complex systems (section v)."
"while the classical ams algorithm requires to specify only a real-valued score function ξ -also called a reaction coordinate in many works, due to connections with molecular dynamics simulations, see [cit], and also [53, section 4.3] -the trajectory adaptive multilevel splitting requires in general a time dependent score function, see section iii c for the optimal choice."
"how many people should work together? we use pairs by extension of pair programming, and for simplicity. we briefly experimented with 3-person teams as a way to resolve the problem of an odd-numbered pool. the small teams were harder to schedule, took longer to work together, and could not always manage a fair three-way exchange of effort. this finding is consistent with decades of small group research [cit] . the current process simply leaves an odd person unpaired for the week. larger groups have occasionally been suggested. for example, one week a member noticed several people asking for help with learning node.js, so a study group formed independently of the pair research process."
"turning to the survey data, which collected the outcomes of a sample of pairings in group a, the mean time spent on pair work was 1.3 hours per week. but pair work failed to happen 16% of the time (9 instances out of 55 pairings in the survey) because of scheduling difficulties between the partners, suggesting that the practice might benefit from a regular scheduled time, at least for co-located groups. when pairs succeeded in meeting, group a members rated the usefulness of the experience as a mean 4.5 on a scale of 1 to 5, though these ratings may be biased by awareness that other group members might see them."
"this paper presented pair research, a new socio-technical system that pairs members of a work group to work together on each other's projects each week. in two deployments, we found that the system motivates participants, helps them learn about their colleagues, and makes opportunities for further collaboration."
"in this paper, we have considered the question of estimating the return time of rare events in dynamical systems. we have compared several estimators, using both usual timeseries (generated with direct numerical simulations) and rare event algorithms, by generalising the approach relating the return times to the extrema over trajectory blocks. this approach relies on the fact that rare events behave, to a good approximation, like a poisson process: this allows for the derivation of a simple formula (see (6) ) for estimating the return times based on block maxima. we slightly improved this formula (see (7)), and further showed that it was possible, provided only minor modifications, to evaluate it with data produced by rare event algorithms. indeed, while the traditional block maximum method consists in dividing a given trajectory in blocks with arbitrary length (larger than the correlation time of the system, and smaller than the return time one seeks to estimate), there is a class of rare event algorithms which yields precisely an ensemble of trajectories exhibiting the rare event more often than direct simulation, together with the probability of observing each member of the ensemble. hence, we have generalised the block maximum formula to non-equiprobable trajectory blocks; this allowed us to use directly rare event algorithms, such as the ams and the gktl algorithm, to estimate return times for rare events. using the ornstein-uhlenbeck process as an illustration, we showed that the method is easy to use and accurately computes return times in a computationally efficient manner. indeed, compared to direct sampling, combining the generalised block maximum approach to rare event algorithms allowed for computing return times many orders of magnitude larger, at fixed computational cost. this method does not depend on the dynamics of the system or on the type of observable, as long as a suitable rare event algorithm is selected. as an illustration, we computed return time plots for both instantaneous and time-average observables for the ornstein-uhlenbeck process, using the ams and the gktl algorithms. this approach paves the way to numerical computation of return times in complex dynamical systems. to showcase the potential of the method, we discussed briefly an application of practical interest: extreme values of the drag force on an object immersed in turbulent flows."
"comments in both the survey and interviews expressed the value of having a fresh perspective, different from the person's usual set of collaborators: \"it brings together people who are interested in each others' work but wouldn't otherwise act on that interest.\""
"academic research is a hard endeavor. the path to discovery has many hurdles that can dampen enthusiasm for making progress. while collaboration and help-seeking can boost productivity and produce better research, few mechanisms in academic research directly promote and facilitate it. this is particularly troubling for young researchers, who may only receive feedback on their work in group meetings, work with only a few collaborators (one of whom is their advisor), and be more easily frustrated by setbacks, real or imagined."
"we collected and analyzed three kinds of data about the deployments. first, to learn about the pair assignment process, we collected the spreadsheets containing help requests, preference ratings, and the resulting pairs for each week. the help requests were coded by one researcher using the categories shown in figure 4 ."
"in pair programming, pairs typically spend a full iteration of the software project working together, which may last for weeks. belshee has argued for promiscuous pair programming instead [cit] . experimenting with pair-switching at various intervals from every 30 minutes to every 3 days, he found the highest task completion rate for 90-minute switching intervals. at this rate, pairs typically consisted of one person already familiar with the task, and one person who was still learning. frequent switching seemed to propagate knowledge quickly around the development team. by switching partners every week, pair research similarly seeks to spread knowledge and skills quickly within a group."
"in many physical systems, the mean state and the typical fluctuations about this state, usually studied in statistical physics, are not the only quantities of interest. indeed, fluctuations far away from the mean state, although they are usually very rare, can play a crucial part in the macroscopic behaviour of the system. for instance, they can drive the system to a new metastable state, possibly with radically different properties [cit] . such transitions arise in a wide variety of situations, such as josephson junctions [cit], quantum oscillators [cit], turbulent flows [cit], magnetohydrodynamics dynamos [cit], diffusion-controlled chemical reactions [cit], protein folding [cit], climate dynamics [cit] . even if the system returns to its original state after undergoing the large fluctuation, the impact of this event may be so large that it is worth being studied on its own. one may think for instance about heat waves [cit] and tropical cyclones, rogue waves in the ocean [cit], strong dissipative events in turbulent flows [cit], shocks in financial markets [cit] . here, we are concerned with the study of such atypical fluctuations starting from the equations (deterministic or stochastic) which govern the dynamics of the system. this approach is different from and complementary to the purely statistical methods which try to extract the best possible information about the distribution of rare events from an existing timeseries, such as, for instance, extreme value statistics [cit] ."
"the empirical approach consists in identifying a priori the conditions for success of the algorithms and identify relevant dynamical phenomena that fulfil these conditions. for the ams algorithm this amounts at understanding sufficiently well the dynamics, in order to be able to define a macroscopic variable that will describe well the dynamics leading to the extremes, and to propose a related score function. the algorithm may also be used to test some hypothesis on such macroscopic variables, and learn about the dynamics. the gktl algorithm is usually successful in conditions when the sampling of time averages is dominated by a persistent macroscopic state."
"many members said the process helped spur them to action. \"it helped me do something that i was ambivalent about.\" \"it is a 'forcing function' ... to work on things i have been putting off.\" \"i [said] 'this week i need help with this and committed to do it.\""
"pair research was frequently cited as a way to learn about other group members and their work. a new member of group a expressed this very strongly: 'our group's paired research is the best thing that happened to me since i came -[getting] to know individual's work one by one, face-toface.\" members of group b found this value in the spreadsheet alone, perhaps because they filled it out more than group a: \"even though we're in the same lab, i don't know what specific tasks they're working on.\" \"it's a good way to know who's doing what.\" some members learned new tools or practices from working closely with their partners. \"i switched to sublime because of pair research.\" \"when i worked with [my partner] she decided to drop what she was planning to do for paired research and learn about bootstrap (we had talked about it when i showed her my experiment during paired research).\""
"finally, a number of comments showed that the process provided emotional and social support. \"i was able to help [him] on his thesis when he was stressed.\" \"i felt energized and excited about my work after every pair research.\""
"we illustrate the method by computing return times, first for an instantaneous observable (one-point statistics) using the adaptive multilevel splitting (ams) algorithm [cit], and second for a time-averaged observable, using both the ams algorithm and the del moral-garnier algorithm [cit] (or equivalently the giardina-kurchan algorithm [cit] in a non-stationary context). the computation of return times with the ams algorithm leads us to define a generalisation called the trajectory-adaptive multilevel sampling (tams) algorithm. this generalisation has several practical advantages: it computes directly return times r(a) for a full range of return level a rather than a single one, and it avoids the tricky estimation of time scale on an auxiliary ensemble, and the sampling from this auxiliary ensemble. as a test, we first carry out these computations for a simple stochastic process, the ornstein-uhlenbeck (ou) process, for which analytical results are available and the accuracy and efficiency of the algorithm can be tested thoroughly. then, to demonstrate the usefulness of the method in realistic applications, we briefly showcase a problem involving a complex dynamical system: extreme values of the drag on an object immersed in a turbulent flow."
"in section ii a, we defined the return time for a time-homogeneous stochastic process and explained how to efficiently compute it for rare events from a timeseries. however, a major difficulty remains as we still have to generate numerically the rare events in the timeseries, which comes at a large computational cost. in the present section, we explain how to apply the above method to the data produced by algorithms designed to sample efficiently rare events instead of direct simulations."
"an activity is the particular work to be done or kind of help needed. the people are consumers who need help and providers who give help. considering other systems in the people-matching design space, online dating sites typically focus on people, but some (e.g. howaboutwe.com) allow members to propose dating activities and have other members rate that activity. in contrast, marketplaces typically focus on the products or services for sale, but some (e.g. angieslist) rate the service providers. our system focuses on activities rather than people, in order to make pair research primarily about work progress rather than social interaction. members nevertheless used preferences in complex ways, to specify interest in both the activity and the other person."
it follows from (26) that an estimator of q(a) can be computed from the n generated trajectories as : (25) . in the following we consider the time averaged position
"pair research is inspired in part by pair programming [cit], in which two programmers work at the same workstation, with one writing code while the other watches and reviews the code as it is typed. pair programming has been shown in some studies to be more efficient than solo programming, producing higher-quality code. the effect of pair programming on learning is less clear. one controlled study of introductory computer science showed no benefit for exam scores, but an improvement in attitude toward the course [cit] . the two partners in pair programming are typically working on the same development project, sharing a common overall goal. pair research differs in that its two partners are usually working on different projects."
"how public should the matching process be? labor markets and dating sites usually keep preferences and matchings private. we chose instead to make them visible to all members of the system, in order to make it easier to learn about the skills and interests of others."
"the preference ratings are used to bias the pair assignment for the week, so that people with high mutual preference are very likely to be paired, people with negative mutual preference are never paired, and people with neutral preference receive a random partner. to generate pairings, the system uses collected preferences to construct a weighted graph. nodes in the graph represent members in the pool, and the graph contains an edge between two members if and only if both members have nonnegative preferences. the weight of an edge between two members is the average of their mutual preferences, plus a bonus if the pair has not been matched recently. a small random perturbation is added to break ties, and to cause random assignment for members who did not provide any preferences. the system then finds a maximum weighted matching on the constructed graph. matching problems like this one are well-studied in the literature [cit], and have applications in matching students to school [cit], residents to hospitals [cit], and donors to patients [cit] ."
"the fair exchange of the process was important to several participants. \"[i like] that we both got something out of it.\" \"she helped me, but i feel a little bit guilty because i couldn't help (but it was her fault because she wasn't ready).\" \"i wish we had not spent as much time on my stuff and had spent more equal time on his.\""
"based on the initial outcomes, we will continue to deploy the system to better understand the social (i.e. size, gender) and technical (i.e. matching algorithm, capturing shared content) factors in academic research as well other domains such as public policy research in government and curriculum development in education. in addition, we understand that pair research likely does not come without costs. we will investigate whether confusion about the ownership of ideas occurs after meetings, whether the method prevents self-directed learning or novices from gaining sufficient expertise to advise others, or whether the method encourages social comparison."
"where the x n are the n backward reconstructed trajectories. empirical estimators of quantities related to rare (for p 0 ) events of the kind of (26) (thus using data distributed according to p k ) have a dramatically lower statistical error, due to the larger number of relevant rare events present in the effective ensemble. in particular, one can use the reconstructed trajectories to compute return times using the method described in section ii b."
"note that we could mutate more replicas at each step by selecting an arbitrary number of levels q (j) n, instead of just the minimum q j as described above. the particular case described above is sometimes referred to as the last particle method [cit] ."
"the occurrence of plateaus in fig. 10 is due to the increasing multiplicity of trajectories as the amplitude a increases. indeed, because of the selection procedure involved in the gktl algorithm, a subset of trajectories can share the same ancestor. henceforth, they are likely to differ only by a small time-interval at the end of their whole duration. in such cases, it is common that the maximum over the trajectory is attained in earlier times. as a consequence, this subset of trajectories will contribute the same value to the set of maxima from which return times are computed. this effect is accentuated in the present case of a deterministic system, as it takes some time for copies to separate after being perturbed at a branching point. a straightforward way of mitigating the occurrence of such plateaus is to increase the number of trajectories or/and the number of repetitions of the algorithm. as an illustration, fig. 11 figure 10: illustration of the computation of return times for the averaged drag over the square obstacle pictured in fig. 9 . the averaging window is 5 correlation times. the dashed black line represents the reference return times computed from a timeseries spanning 10 6 correlation times, using (7) . the solid blue line represents the return times obtained using the gktl algorithm."
"the standard way of analysing the efficiency of an estimatorθ n (or rather, a family of estimators indexed by a parameter n, e.g. a sample size) of a quantity θ is to consider the mean-square error:"
"what pool of people should be involved? a large number of members in the pool provides a more diverse set of expertise, but may be uneven in quality. our two deployments were centered on existing work groups, and varied in size between 10 and 30 members. both deployments also included members with a range of seniority, from undergraduates to faculty."
"it is instructive to look at the differences between the group a and group b deployments, because each had different successes and failures that shed light on how the system could be improved. group b used the spreadsheet far more effectively than group a because they didn't just rely on email reminders over saturday and sunday, but filled out the spreadsheet together at a friday group meeting. another contrast was the length of deployment. group a continued using pair research throughout the spring semester, but group b stopped partly because the group leader is a bottleneck in the current system, having to approve the pairing before emailing it out to the group. on one occasion, the pairing wasn't announced until thursday, leaving only two days left in the week to actually work. we have since changed the system to send the final pairing automatically."
"should pair meetings be fixed-time or independentlyscheduled? we initially prototyped the process at a regularlyscheduled group meeting time, but members requested more flexibility, so pairs now decide on their own when to meet. this flexibility also allowed people from other time zones to join the pool without having to negotiate a common meeting time for the whole pool. in practice, however, flexibility can lead to scheduling failures, discussed more later in this paper."
"one co-author's research group, inspired by pair research but not using the spreadsheet matching system, combined group meetings and one-on-one meetings into a single synchronous time block. the group meeting starts with brief updates and requests for ad-hoc meetings with other group members (which are recorded on a whiteboard), and then breaks up into those ad-hoc meetings, which continue until everybody's meeting needs are satisfied. conducting pair research synchronously may be the right solution for a colocated group, though as groups grow and become geographically distributed, like group a, it becomes more challenging to bring the entire group together at the same time."
"how should people be matched? initial experiments used randomized matching, which often led to people tailoring the pair research session to their partner's expertise rather than their needs. to support people receiving help from those who are most able to help, our current prototype solicits preferences from members and computes an optimal pairing given those preferences."
"even the pair work session may need to happen more automatically. both group a and group b struggled with fitting pair research into already-busy schedules. group a experienced 16% failures due to scheduling, and group b stopped after 7 weeks in part because the entire group got busy, not just the group leader. one way to address this would make the entire process happen in one synchronous session, in which members fill out the spreadsheet, immediately determine the pairing, and then immediately break up to work in pairs."
"from now on, we shall assume that the statistics of rare events is poissonian. this is a reasonable approximation for many dynamical systems as long as there is a well-defined mixing time after which the initial conditions are forgotten. of course, it would not hold for systems with long-term memory. in the next paragraph, we use this assumption to derive new expressions that allow accurate and efficient sampling of the return times."
"in this section, we illustrate the computation of return times using the method described in section ii b for a time-averaged observable. while it could be done using the tams algorithm presented in section iii b, we instead illustrate the use of a different rare-event algorithm, specifically designed to compute large deviations of time-averaged dynamical observables: the giardina-kurchan-tailleur-lecomte (gktl) algorithm [cit] ."
"the system sends weekly email notifications to remind members to submit their needs and preferences. members enter their needs in the spreadsheet on saturday, then return to the spreadsheet again on sunday to look at what others need and fill out the preference matrix. the pairs are assigned on sunday night and emailed automatically. figure 3 shows an example of a typical matching. figure 4 . results of two deployments of pair research. roles are (u)ndergraduate, (g)rad student, (p)ostdoc, and (f)aculty."
"second, in order to learn about how pairs actually worked together, we sent a short weekly survey to group a for the last 9 weeks of its deployment. group b did not receive the survey, because its deployment had already ended. the survey asked for how long pairs spent working together, what they actually worked on, the usefulness of the pairing session on a scale from 1 to 5, and open-ended comments on positive and negative aspects of the week's experience. the survey produced responses for 55 of the 244 group a pairings, from 18 distinct members."
"several authors have proposed new algorithms to cure some of the problems. a class of algorithms seek at changing the dynamics such that the computation will be more efficient (see for instance [cit] for diffusions with small noise, or [cit] in relation with the gktl algorithm and references therein). those methods are limited to diffusions, as they require to relate the statistics of paths for different dynamics, for instance through the girsanov formula. they can involve recursive learning of an optimal dynamics and be very successful for dynamics with a few degrees of freedom [cit] . another class of algorithms, milestoning (see [cit] ), is aimed at computing a reduced description of the original dynamics, that can afterwards permits to efficiently compute dynamical quantities, for instance first passage times (see [cit] and references therein)."
"the average time between two occurrences of the same event, referred to as its return time (or return period), is a useful statistical concept for practical applications. for instance insurances or public agency may be interested by the return time of a 10 m flood of the seine river in paris. however, due to their scarcity, reliably estimating return times for rare events is very difficult using either observational data or direct numerical simulations. for rare events, an estimator for return times can be built from the extrema of the observable on trajectory blocks. here, we show that this estimator can be improved to remain accurate for return times of the order of the block size. more importantly, we show that this approach can be generalised to estimate return times from numerical algorithms specifically designed to sample rare events. so far those algorithms often compute probabilities, rather than return times. the approach we propose provides a computationally extremely efficient way to estimate numerically the return times of rare events for a dynamical system, gaining several orders of magnitude of computational costs. we illustrate the method on two kinds of observables, instantaneous and time-averaged, using two different rare event algorithms, for a simple stochastic process, the ornstein-uhlenbeck process. as an example of realistic applications to complex systems, we finally discuss extreme values of the drag on an object in a turbulent flow."
"the pair research spreadsheet is a low-fidelity prototype [cit] of what may become a full-fledged web application. in general, we have found that collaborative spreadsheets and documents are excellent for this purpose, similar to paper prototyping [cit] but for collaborative sociotechnical systems that might otherwise require programming to experiment with. even though our spreadsheet now has a programmed backend that matches people and sends emails automatically, we could have run these parts by hand using a simple but nonoptimal greedy matching algorithm executed by a human wizard. low-fidelity spreadsheets and docs permit early experimentation with potentially-risky social parts of a sociotechnical system, without investing development effort in the technical parts before it can be justified."
"as explained in section iii a, the algorithm generates an ensemble of m trajectories x m (t) with associated probability p m . it follows directly from (13) that an estimator of q(a) is:"
where the average in this computation is taken with respect to the poisson process interval pdf p (τ ) made explicit. one may be tempted to use the estimatorr
"third, to gather some experience from group b in the absence of survey data, we conducted in-person or video interviews with two members of group b and a remote member of group a, covering the same questions as the survey and also asking about collaboration outside of pair research and about opportunities for informal learning."
"pair research is a new socio-technical system consisting of both a social process, meeting to work as a pair, and a technical system that supports the process, collecting people's preferences and automatically matching them to help each other do work. we developed a spreadsheet prototype to manage pair research. each week, the system automatically reminds group members to submit what they want help on and what they can help with, makes an optimal pairing given that information, and notifies members of their partners. this paper makes the following contributions: (1) the design space for pair research, drawing from literature, existing systems, and early prototyping experience; (2) a pair research system which collects needs and preferences and matches members automatically; and (3) the results of two deployments, one with roughly 10 people over 2 months, the other with almost 30 people over 6 months, still ongoing. the de-ployments show that members used pair research in a wide variety of ways, and that it helped them start and complete tasks, share expertise, and learn about other group members."
"pair research has been deployed in two university research groups, a and b, each run by one of the authors. group a initially had 15 members at a single school, and the pool grew to include 29 people, including collaborators from five other schools. group b had 10 members at a single school. figure 4 shows statistics about the two deployments, including demographics of group members."
"for many practical applications, the most useful information about a rare event is the return time: it is the typical time between two occurrences of the same event. this is how hydrologists measure the amplitude of floods for instance [cit] . as a matter of fact, one of the motivations of gumbel, a founding father of extreme value theory, was exactly this problem [cit] . other natural hazards, such as earthquakes [cit] and landslides [cit], are also ranked according to their return time. similarly, climatologists seek to determine how the frequency of given heat waves [cit] or cold spells [cit] evolves in a changing climate [cit] . public policies rely heavily on a correct estimate of return times: [cit] as areas vulnerable to events with a 100-year return time. such definitions are then used to determine insurance policies for home owners. in the industry as well, return times are the metric used by engineers to design systems withstanding a given class of events. just like the extreme values of any observable, the return time of a rare event is very difficult to estimate directly from observational or numerical data, because extremely long timeseries are necessary."
"pairing was promiscuous. group a members had a mean 16.8 pairings over 26 weeks, with a mean 12.0 different partners, so 71% of the time they were working with a partner they had never had before. for contrast, most of group a's members would normally work closely with only 1-2 other group members, counting the adviser. pair research increased their number of close contacts by an order of magnitude."
"realisations, then the estimator for each realisation would have been biased with a bias of order 1/n (see appendix b), and the final estimator after k realisations would still be biased with a bias of order 1/n ."
"others suggested that more preparation would have been useful before the pair work session. \"this could have been improved had we specifically written down our goals prior to the start.\" the time gap between filling out the spreadsheet and meeting to work together also came up as a problem: \"i often find it restricting to 'commit' to working on a certain subject ...up to a week before i actually work on it.\" \"lately i have not known [what i want to do] far enough in advance.\""
"we developed a prototype user interface to manage the process, implemented as a collaborative spreadsheet (figure 2 ). the heart of the user interface is a preference matrix for person matching. each week, group members specify the help they need that week (e.g. \"debugging django,\" \"feedback on my writing,\" \"need a pilot user for my prototype\"). other members then fill out the preference matrix according to how able and interested they are to provide that help, where 1 is maximum interest, -1 is maximum disinterest, and 0 or blank is neutral. the preference matrix may be sparse, because some members may not fill it out. members are matched with others even if they do not fill out the matrix."
"we have been experimenting with a new kind of interaction within a research group that we call pair research, as a generalization of pair programming. each week, group members pair up, guided by a matching algorithm. each pair meets for a one-or two-hour session, of which half the time is spent working together on one person's project, and the other half working on the other person's project. the work might be any activity involved in computer science research, including pair programming, user testing and design critique, data collection and analysis, brainstorming and research discussion, and mentoring and advising. the following week, different pairs are formed, and the process repeats."
"for each trajectory x n, a random number of copies of the trajectory are generated, on average proportional to the weight w i n and such that the total number of trajectories produced at each event is equal to n . the parameter k is chosen by the user in order to control the strength of the selection and thus to target a class of extreme events of interest. the larger the value of k, the more trajectories with large values of the time average observable will survive the selection."
"of course, we could construct similarly an estimator generalising (6), but as we have seen in the previous section, the estimator (7) yields better performance."
"new columns were added to the spreadsheet as the process evolved. the ready column allows a member to temporarily drop out of the pool, for example while traveling or going away for holidays, but requires a date on which they will return, so that the reminder emails can automatically resume. the group and can pair with columns were added when the pool broadened to multiple research groups, so that a member can keep their pairing local if they choose. undergraduates, in particular, expressed a reluctance to engage with members of remote research groups, perhaps because they do not (yet) feel ready to make a long-term investment in getting to know people in the wider research field."
"there is thus a need to develop rare event algorithms specifically designed for computing return times, valid also when large deviation estimates are not relevant. this is the aim of this paper. the approach developed in this work relies on the combination of two observations. first, if one assumes that rare events are described by a poisson process, then return times can be related to the probability of observing extrema over pieces of trajectories, which are of duration much larger than the correlation time of the system, but typically much smaller than the computed return times. second, several classes of rare event algorithms can be easily generalised to compute the probability of extrema over pieces of trajectories, rather than to compute single point statistics. we show that combining these two remarks allows us to build a powerful tool to compute return times in an elementary way with simple and robust algorithms. as a side remark, we also discuss a new way to construct return time plots from a timeseries, which provides an important improvement for return times moderately larger than the sampling time, even when we are not using a rare event algorithm."
"formula (23) is valid only for times t a that are integer multiples of the resampling time τ . the killed trajectories have to be discarded from the statistics. starting from the final n trajectories at time t a, one goes backwards in time through the selection events attaching to each piece of trajectory its ancestor. in this way one obtains an effective ensemble of n trajectories from time 0 to time t a, distributed according to p k . all trajectories reconstructed in this way are real solutions of the model: we have not modified the dynamics, but only sampled trajectories according to the distribution p k rather than according to the distribution p 0 ."
"to design and develop pair research, we deployed the system in two technical labs within universities characterized by high research activity. while the diversity and size of the sample was limited, like other systems designers [cit] we found that small deployments are useful for developing a proof of concept and identifying social and technical factors to consider in february 15-19, 2014, baltimore, md, usa future designs. further, we conducted observations throughout the deployment. the advantage to this research approach is the ability to collect in situ data, not just reflective data; the disadvantage is that bias is introduced through participant observation [cit] ."
"while we have been motivated by the academic research setting, the pairing system may find application to groups in other domains, particularly other kinds of hard knowledge work that involves diverse expertise. in particular, we find pair research to be effective as a form of transient, lowcommitment collaboration, especially to overcome hurdles and promote progress on hard tasks by drawing on other people's expertise, effort, and social pressure. we look forward to expanding pair research to other research groups and experimenting in other work settings in future work."
"under the paraxial approximation, which is to say that the propagation is dominantly along the z-axis and the dispersion in the transverse plane is negligible, we have"
"differing from the instrumentation-based solutions, the transport-of-intensity equation (tie) serves as a purely computational approach [cit] . originating from the parabolic wave equation, tie links the phase image to the variations in the intensity induced by wave propagation. the essence of this method is to measure the intensity along the propagation direction at multiple (three in our case) positions. in mathematical terms, the relationship between these measurements and the phase image is expressed via an inhomogeneous second-order differential equation whose solution is unique (up to an additive constant) [cit] . an advantage of tie is that the resulting phase does not need to be unwrapped, as is common for many other interferometric methods [cit] . allowing phase images to be obtained by using numerical methods-rather than implementing specialised hardware modifications-tie-based imaging is a viable tool for electron microscopy [cit] and x-ray imaging [cit] . in addition, tie can be applied to a partially coherent source, even though it has been initially derived for coherent illumination [cit] . this makes it applicable to a practical differential interference contrast (dic) microscope [cit] ."
the variational formulation of the phase reconstruction problem by utilising tie for dic microscopy. the proposed method incorporates total variation (tv) regularisation which allows for preservation of abrupt phase transitions.
"we are now interested in combining the tie formalism with differential interference contrast (dic) images. fundamentally, dic microscope relies on polarising optics to produce two orthogonally polarised beams. these beams traverse the specimen small distance apart from each other. hence, they perceive the same phase profile with a very small displacement. then, the beams are optically interfered such that the intensity image-related to the derivative of the phase-has increased contrast [cit] ."
"reconstructing the phase of a complex field given its intensity is a fundamental problem in bioimaging. most cells and soft tissues are highly transparent. thus, without staining or tagging, they generate very low contrast intensity images. this implies that they are barely visible under a standard brightlight microscope. knowing that such specimens change the phase of the light wave (i.e. shape and structure of the specimen is encoded in the phase image) has led to the development of many well-established imaging modalities [cit] ."
"practically speaking, the axial derivative of the intensity is approximated by finite differences. one possible approximation is carried out by using two equispaced defocused measurements:"
"in microscopic imaging applications, the defocused measurements are recorded via moving the stage of the microscope (see figure 1), which is equivalent to displacing the detector along the optical axis. therefore, by solving (3), one retrieves the phase at the detector plane which, assuming a perfect imaging system, corresponds to a magnified version of u 0 ."
"where ∇ ⊥ is the transverse gradient operator and · denotes the dot product. the remarkable aspect of (2) is that it establishes a direct mathematical link between two physical quantities, namely the spatial phase and the axial derivative of the intensity of the field. for bioimaging applications, one usually assumes that u 0 represents the field leaving a thin, phase-only object-this is reasonable for most biological samples such as cells and microorganisms-that is illuminated uniformly. this means that i(x, z) is constant over x so that (2) is rewritten as"
"we have introduced a variational framework for phase reconstruction using tie and the dic microscopy. our model uses tv regularisation and is solved iteratively via admm. finally, we have demonstrated our phase reconstruction method with experimentally-recorded dic images."
"the remainder of the paper is organised as follows: in section 2, we start by explaining the underlying mathematical scheme that constitutes our forward model. in section 3, we state an inverse problem formulation and explain our iterative algorithm. we conduct experiments where we reconstruct phase images from a series of dic images of hela cells in section 4."
let us now focus on the sub-minimisation problems. we note that the minimisation over u is separable and is computed by 1 note that the phase image is thought to be lexicographically reordered.
"assuming periodic boundary conditions, this is directly solved by using the fast fourier transform [cit] . the final step of the admm is a trivial update."
"in bioimaging applications, it is desirable that the specimens be well-isolated from the background. this necessitates that the reconstruction algorithm preserves the discontinuities across the boundaries. thus, for regularising the solution, we choose to use the total variation (tv) functional that is defined as"
"by looking at the results given in figure 2, we first observe that the phase images obtained via tie render some intracellular structures in a more revealing way than dic images. for instance, the cell nucleolus (indicating concentrated dna) is more visible in tie, especially so with our reconstruction method (see the region inside the circles in figure 2 .(e)). also, our method produces an image where the nucleus is better resolved compared to the other reconstructions (indicated by arrows in figure 2 .(e)). finally, we note that the background is reconstructed with improved homogeneity and the halos around the cell membrane (see the circles in figure 2 .(f)) are significantly reduced. the latter can be of importance if further morphological analysis of such cells is considered for which a proper segmentation is needed."
"where i dic is the intensity image recorded by the dic microscope. in (5), c is a constant depending on the bias value introduced by the microscope and is known to the practitioner. by looking at (5), we see that the tie technique can be readily extended to dic imaging. for solving (5), [cit] have applied the inverse operator"
"the common practice for resolving tie has been the direct inversion of the model (with appropriate boundary conditions) [cit] . although this approach is non-iterative, it is extremely sensitive to the measurement noise since the said inversion is characterised by an integral operator. consequently, the reconstructed phase images suffer from noise amplification. this observation strongly motivates the development of reconstruction algorithms that are resilient to noise. also, the reconstruction performance can be further improved by imposing suitable characteristics on the solution."
"the training sample for rf consists of several hundreds of angular backscatter values derived from five ground-truth locations (figures 4a,b, table 1 ). we fitted models using 50, 100, 200, 500, and 1000 trees and found that all of them produced a zero out-of-bag error rate (oob). this is an indication that the training sample is particularly effective and produces consistently correct results regardless of the number of trees used to fit the model. additionally, it was found that, regarding variable importance, some differences occurred when fitting was done using a low (50) and high (500) number of trees. when the model was fitted with 50 trees, almost half of the angular layers scored much higher importance values for predicting all classes. on the other hand, when 500 trees were used for model fitting, the importance scores were much lower for the majority of angular layers regarding prediction of all classes. the individual class importance scores varied for each class and for the number of trees as well. in general, for the model fitted with 50 trees, the importance scores varied individually for each class, while angular layers of 50° and 60° incidence (table 1) . for the artificial neural network (ann) map, all 18 ground-truth locations were needed for training."
"1. imu and camera frames are aligned to the system body frame. the standard deviations of the three-axis position error are (0.13, 0.15, 0.11) mm and (0.12, 0.10, 0.13) mm for the mimu and camera, respectively. meanwhile, the standard deviations of the three-axis euler angle error are (0.02°, 0.02°, 0.02°) and (0.03°, 0.04°, 0.02°) for the mimu and camera, respectively. compare the difference between the method with corner correction and without corner correction. we find the camera extrinsic parameters' accuracy of former is higher than the latter (tables 9 and 10), and the camera measurement residuals of former is lower than the latter ( figure 6 ). it indicates the corner correction described in section 2.3 is effective. the reasons why the difference of imu calibration parameters before and after modification is not significant have been briefly discussed. 2. there are three errors affecting the calibration accuracy. firstly, the calibration errors of imu and camera intrinsic parameters, which affect the measurements' accuracy. at present, a reasonable choice of camera calibration method ensures that imaging accuracy reaches the subpixel level, and imu is factory calibrated. secondly, the time delay between the imu and camera data acquisition, which affects the calibration accuracy and stability of filter. we align the data by the time label (both imu and camera data are marked on the time label, respectively), and don't evaluate the time delay exactly. thirdly, we ignore the non-orthogonality of the turntable axes, and the turntable has been factory calibrated. further research on a solution without orthogonal axes could be performed. 3. we can observe the convergence of each parameter in the ekf process. the experiment results"
"thirdly, \"blur correction\", the motion blur exits in the checkerboard image. the checkerboard corners are detected with the aid of imu to achieve higher accuracy."
"in this study, we exploit the complementary nature of imu and camera to improve the calibration accuracy. the camera measurements can suppress the inertial propagation drift, whereas the motion parameters evaluated by imu can accurately extract the feature points under a smearing effect. we exploit the complementary nature of these two components to improve the calibration accuracy."
"when the cross angle between the velocity of the checkerboard is expressed in the camera frame and the x axis of the camera frame is θ, the covariance matrix of psf can be written as"
"the pattern recognition approach used here (sad) is described in reference [cit], where it was initially implemented. the method compares the two-dimensional (2d) shape of known angular signatures with unknown ones. a similarity assessment is used for comparison of both signatures which are treated as 2d vectors. the level of similarity with each of the reference signatures determines the acoustic class membership of the unknown angular signatures. an adaptation of the sad algorithm for 2d data was used for testing the similarity of angular signatures."
"although the db values of our data are not absolute (mbes not calibrated), they comply with the standards of reference [cit] for repetitive seafloor mapping, meaning that they are stable (repeatable over the same type of seafloor) and linear (changing according to changes in seafloor type in the same way). this fact allows for suggesting a theoretical framework, according to which our empirically derived angular responses can be compared to those derived by the applied physics laboratory (apl) model [cit] . initially, it was observed that the empirical angular responses (figure 4c ) obey the rule of angular dependence of seafloor backscatter. this means that, for responses resulting from rough seafloor with coarse sediment grain sizes, there is not a large db difference between the incidence angles close to nadir and the middle part of the swath. accordingly, responses resulting from a smooth seafloor with fine sediment grain sizes show a more pronounced difference in db values between the nadir area and the middle part of the swath (due to the specular effect). similarly, there is a clear decrease of the db values for the outer swath incidence angles for all seafloor types. this behavior is in agreement with the pattern observed in the theoretical angular response curves produced by the apl model. moreover, both the apl curves and hac responses show a decreasing offset (of db values) for sediments with decreasing grain sizes (figure 4 ). considering that both the apl and hac angular responses show similar geometrical characteristics, it can be suggested that the hac responses rely on a geophysical base, and, by applying a suitable transformation (e.g., simple offset or a linear transformation), this may assist in calibrating the output db values of the hac layers. calibrating the hac layers according to absolute db values will offer the possibility of applying geophysical models (such as the apl) for inferring seafloor properties per cell. such a development is going to offer the possibility for the hac concept to be adopted by software developers in an effort to improve the traditional ara approach. however, it has to be noted that geophysical models such as the apl provide only a limited range of angular curves, thus covering only the most common sediment types, since these models are based mainly on experimental data; thus, it is not practical to examine the full range of seafloor types in the laboratory. in reality, the seafloor includes numerous combinations of the most common sediment types along with a great variety of substrates (e.g., corals, macroalgae). the advantage of the empirically derived angular responses is that they are representative of the seafloor heterogeneity from which they are derived from. in addition, a comprehensive set of angular responses can be produced for each seafloor type identified from ground-truth information. this can occur only when the shape of the angular responses is sufficiently different between similar seafloor types, and when the mbes outputs stable and linear db measurements. concluding, it is suggested that future mbes backscatter software may incorporate the utility of the shac allowing the user to produce a stack of angle-normalized mosaics and to identify particular seafloor types on cells of these mosaics based on ground-truth data locations. in this way, the user can construct a set of empirically derived angular responses that will then be applied for classifying the entire dataset."
"finally, \"estimator\", we estimate the extrinsic parameters based on ekf. the main idea of the proposed calibration method is shown in figure 1, which can help understand the part of the calibration. there are four step of this calibration. firstly, \"preparation\", the following preparations should be performed before the system calibration:"
"there are many methods that can solve the equations that provide the extrinsic parameters, such as ekf [cit], genetic algorithm [cit], and so on. here, we utilize an ekf for calibrating the extrinsic parameters. the ekf algorithm is briefly introduced in table 2 the image process algorithm is summarized as table 1 . the corner extraction accuracy can be significantly enhanced by using the above checkerboard corner detection algorithm, especially when a smearing effect exists."
the checkerboard should be fixed in an appropriate location that can be observed by the cameras while the system rotates along with the turntable; the intrinsic parameters of the camera and imu should be calibrated; the visual/inertial system should be fixed on the turntable.
"thirdly, \"blur correction\", the motion blur exits in the checkerboard image. the checkerboard corners are detected with the aid of imu to achieve higher accuracy."
"we can observe the convergence of each parameter in the ekf process. the experiment results show that the method is valid and is not restricted in the kalman filter. some optimal algorithms, such as the particle filter and levenberg-marquardt algorithm can also be used. the calibration parameters are obtained, and the complete visual/inertial integrated system is established. future research may focus on the calibration in the navigation process, and the proposed method may be seen as a standard calibration step in factory production and user operation."
"during the working process of a digital camera, the shutter needs to open for a moment to project light onto the photographic material. this brief moment is called the exposure time. under highly dynamic conditions, the relative pose between the camera and the object changes evidently during the exposure time, thereby blurring or stretching the generated image [cit] . the calibration of the extrinsic parameters requires a rotation, especially for imu. therefore, for the camera measurements, we must extract the checkerboard corners during rotation, but a motion blur may be generated in the process. conventional checkerboard corner detection methods compute the local optimum value [cit] with static images. therefore, when noise and motion blur are present in an image, the errors in the extraction results evidently increase."
"for camera measurements, the rotation angle is computed based on all the checkerboard corners' pixel coordinates. the effects of motion blur are eliminate by involve all corners into computation process."
"given that u v t ω t y ∆t − v v t ω t x ∆t + t x ω t y ∆t − t y ω t x ∆t /z v t t 1 in this study, we have:"
"the hac concept is an objective way of deriving homogeneous angular responses in contrast to applying backscatter mosaic segmentation. mosaic segmentation requires expert knowledge which can be subjective, and, by varying the segmentation parameters, different angular responses may be obtained. considering the results of this study, we suggest that future backscatter studies should either rely on dense backscatter soundings (for using the ihac) or on mbes surveys with conventional overlap, but producing several backscatter mosaics normalized for different incidence angles each time, to generate the shac. in addition, a small amount of highly overlapping lines may/should be collected over known seafloor for validation purposes."
"where f i 0 is the accelerometer measurement in the initial pose. therefore, b a must be initially evaluated to determine g i 0 . the gyro measurement is formulated as"
"the sad algorithm uses a set of reference angular signatures representing a known seafloor type (class) (figure 4c ) for comparison with unknown signatures extracted from each grid cell of the hac. matching is based on criteria set by the user (standard deviation offset, majority) and each cell is assigned with the class represented by the successfully matched reference signature. in this study, the offset value was selected according to the standard deviation of db values from each angular signature; thus, different offsets were applied for each class (figure 3 ). the majority threshold was set to 90% of the total number of incidence angles. some cells could be matched with up to two adjacent angular signatures and receive double class assignment. in this regard, class overlap was quantified and it was found to affect class boundaries of neighboring classes ( figure 5 ). cells without continuous angular signatures or cells that did not conform to the majority criterion remained unclassified."
"finally, an ann approach was examined in this study. anns mimic the functionality of biological neurons and they are extensively used in speech/face recognition, and also in landscape classification studies [cit] . they possess a number of advantages that distinguish them from common algorithms, whereas they also hold some disadvantages. regarding the merits of anns, these include not making any assumptions about the data (e.g., a normal distribution of training data is not a prerequisite for data analysis), being able to identify non-linear relationships in high-dimensional data, and, most importantly, acquiring learning without the input of physical models that describe data variability. however, anns are considered as black boxes regarding input-output of data, and they require several trial-and-error iterations for optimal tuning of algorithm settings. in this study, two training sets were used to support the ann classification. the main concept of a supervised ann is that an input of variables is examined in a combinatorial way regarding the production of certain outputs. the combination of variables leading to acceptable outputs (according to the training set) is determined by weight factors that strengthen the relationship between the input and output. learning occurs when the ann modifies the weight factors via a back-propagation technique in order to minimize a user-defined error rate value. the trained ann then applies its \"knowledge\" to the rest of the dataset for classification of unknown elements. the ann was applied to backscatter angular responses initially by reference [cit], producing promising results. however, they used seafloor patches generated from a half-swath sliding window to extract the angular responses. this fact had an effect on the homogeneity of the angular responses and hindered class separability in their study. the ann used in this study was implemented in the opencv library provided by saga."
"considering that extracting angular responses from the lowest level of acoustic elements (i.e., the grid cells of a backscatter layer/mosaic) is the way forward in improving the performance of ara, and that the hac provides this possibility in a solid way, we focused the scope of this paper on utilizing an hac from mbes data with high overlap and, alternatively, from mbes data with standard overlap. we present a method to construct the hac from mbes data with standard overlap in order to overcome the practical limitations of backscatter-dedicated surveys, thus minimizing the time and cost of the fieldwork. this method was recently considered in the literature by references [cit] . in addition, we examined and compared the performance of four supervised machine learning classifiers in terms of class separation and how to deal with a restricted amount of training data along with the high-dimensional hac dataset. building a high-resolution hac from mbes data of standard overlap and exploiting a small amount of ground-truth samples for classification will have an impact on future backscatter studies by decreasing the cost of data collection, while, at the same time, maximizing the usability and quality of acoustic and ground-truth datasets."
"the calibration process can be a standard calibration step for factory production and user operation. the turntable is the standard equipment for imu calibration, and the checkerboard is the standard device for camera calibration. so, the cost is low, and the calibration process is simple and convenient. our experiment results indicate that our proposed method is valid and achieves a fair level of accuracy. our method can also align the camera frame to the imu frame. the rest of this paper is organized as follows. section 2 introduces the mathematical model and the proposed calibration method. section 3 presents the simulation and the real-world experiment. section 4 concludes the paper."
"the measurement model for imu is formulated as follows (the superscripts and subscripts f, b, i, and v represent the frame of checkerboard, system body, imu, and camera, respectively, while f 0, b 0, i 0, and v 0 represent the frame fixed on the earth and coincident to the frame of checkerboard, system body, imu, and camera in its initial pose, respectively)."
"1. for camera measurements, the rotation angle is computed based on all the checkerboard corners' pixel coordinates. the effects of motion blur are eliminate by involve all corners into computation process. 2. the calibration process is not long, so the effect of accumulation error is not significant. 3. there are system errors exits, such as the non-orthogonal of rotation axes, and the time delay of data acquisition, which also influence the error level. table 10 and figure 7 summarize the experiment results. the origin of the mimu frame is discussed in the sbg-imu user manual, while that of the camera is the optical center of the lens. therefore, the translation vector between the mimu and camera can be roughly evaluated. the results that are evaluated based on the mechanical structure coincide with those that are evaluated by using ekf. the experiment results, the existing problems and possible reasons, the strategies for improving the results, and some directions for future work are presented below:"
"the main idea of the proposed calibration method is shown in figure 1, which can help understand the part of the calibration. there are four step of this calibration. firstly, \"preparation\", the following preparations should be performed before the system calibration:"
"there are many methods that can solve the equations that provide the extrinsic parameters, such as ekf [cit], genetic algorithm [cit], and so on. here, we utilize an ekf for calibrating the extrinsic parameters. the ekf algorithm is briefly introduced in table 2 x and ω i y ) measurements as well as the extracted checkerboard corners (u x,v x, u y, and v y ). table 2 . ekf updating process."
"in this study, the concept of the hac matrix was considered for improving the application of ara using multiple angular backscatter layers with minimal ground-truth information. there are two ways of constructing the hac layers, one of which is based on interpolation of actual soundings (ihac) from single incidence angles (i.e., from backscatter-dedicated surveys with large overlap). the other is to produce backscatter mosaics using a different normalization angle each time (shac). both hac types yield comparable angular responses per cell, which is the finest element of the matrix layers. this guarantees that angular responses come from naturally homogenous areas of the seafloor, thus providing trustworthy comparisons with the ground-truth data and eliminating the need for backscatter mosaic segmentation."
"seafloor acoustic mapping with multibeam echosounders (mbes) faces the emergence of new data acquisition styles, which also trigger the development of new data processing and interpretation approaches. acquiring mbes datasets using multiple frequencies [cit] or acquiring backscatter-dedicated mbes surveys [cit] enormously increased the volume of data per seafloor"
"all classifiers produced very similar classification results ( figure 5, table 2 ) and, thus, showed great potential in automated supervised seafloor classification using high-dimensional mbes backscatter datasets with a limited amount of ground-truth data for training. only the ann classifier required a greater amount of training data (threefold) for producing classification results comparable to the other classifiers. the final classification maps expressed an agreement in the areal percentage per class, and it could also be seen that all classes covered comparable geographical areas among each of the maps. table 2 shows results of each supervised classification map when compared to the classification map of reference [cit] . this map is considered to represent sediment variability in a more objective way, since it was produced by an unsupervised method that performs cluster validation. the comparison was done with the map comparison kit implemented in reference [cit] (http://mck.riks.nl) and agreement was quantified using kappa, kappa histogram (k hist), and kappa location (k loc) coefficients. the kappa coefficient is a standard measure of agreement between two categorical maps, while the kappa histogram expresses the agreement regarding the amount of cells belonging to each class, and the kappa location expresses the agreement of class assignment by location (i.e., high kappa location score means that all areas were classified similarly between the two maps). all scores suggested strong agreement between each classification map and the bayesian classification map of reference [cit] with minor fluctuations. the scores of the classification maps produced using the shac appeared slightly lower than the scores of the classification maps produced using the interpolated hac, but this difference is not statistically significant (i.e., both ihac and shac scores in table 2 belong to the highest ranking of agreement). this difference occurred because the backscatter mosaics of the shac contained some noise in the nadir and outer ranges, which was not possible to remove from the data, leading to misclassification of a small number of cells. table 2 . agreement scores between classification maps produced with the interpolated hyper-angular cube (ihac), the synthetic hyper-angular cube (shac) data and the bayesian classification map from reference [cit], calculated using the map comparison kit. sad-sum of absolute differences; rf-random forest; svm-support vector machine; ann-artificial neural network. until now, the main approach to overcome the problem of acoustic class homogeneity in ara studies was by segmenting the backscatter mosaic into objects (groups of cells) using algorithms requiring several user-defined parameters. then, angular responses were extracted from soundings falling within each object, assuming that they better discriminate the corresponding seafloor types. the concept examined in this study takes ara one step forward by introducing the hac matrix."
"the hac matrix approach presented here allows for extraction of angular signatures from the lowest level of spatial constituents (i.e., the grid cells of the hac layers), eliminating the need for backscatter mosaic segmentation. angular signatures derived from fine-scale homogeneous seafloor areas (grid cells similar to the average mbes footprint size) are relevant to the scale of ground-truth data, supporting the highest possible separation of seafloor types. in this way, along swath seafloor variations can be resolved and acoustic classes can be assigned for each grid cell and not coarse seafloor patches. these two facts are expected to advance seafloor mapping by offering more detailed acoustic classification results that are consistent with seafloor acoustic properties (i.e., angular dependence of backscatter). highly detailed seafloor classification maps can effectively depict fine-scale variations of sediments on the seafloor and provide valuable input to benthic habitat and other studies that rely on effective separation of seafloor sediments."
"where subscripts x and y indicate that the turntable rotates around the x and y axes, respectively, u i, v i is the pixel coordinate of each checkerboard corner, and n is the number of checkerboard corners. the value of u i, v i can be computed as"
"show that the method is valid and is not restricted in the kalman filter. some optimal algorithms, such as the particle filter and levenberg-marquardt algorithm can also be used. the calibration parameters are obtained, and the complete visual/inertial integrated system is established. future research may focus on the calibration in the navigation process, and the proposed method may be seen as a standard calibration step in factory production and user operation. the experiment results, the existing problems and possible reasons, the strategies for improving the results, and some directions for future work are presented below:"
"visual/inertial integrated systems have been used across many contexts, including indoor [cit], underwater [cit], space environments [cit], and taking some measurement tasks [cit] . to achieve accurate navigation and measurement, camera and inertial sensor frames should be aligned to the carrier frame in a process often called extrinsic parameter calibration or alignment. in this process, a coordinate transforming relationship is established between the sensor frame and the system body frame. without calibrating the extrinsic parameters, the navigation errors will be coupled with the misalignment errors [cit] ."
"there are three errors affecting the calibration accuracy. firstly, the calibration errors of imu and camera intrinsic parameters, which affect the measurements' accuracy. at present, a reasonable choice of camera calibration method ensures that imaging accuracy reaches the sub-pixel level, and imu is factory calibrated. secondly, the time delay between the imu and camera data acquisition, which affects the calibration accuracy and stability of filter. we align the data by the time label (both imu and camera data are marked on the time label, respectively), and don't evaluate the time delay exactly. thirdly, we ignore the non-orthogonality of the turntable axes, and the turntable has been factory calibrated. further research on a solution without orthogonal axes could be performed."
"there are system errors exits, such as the non-orthogonal of rotation axes, and the time delay of data acquisition, which also influence the error level. table 10 and figure 7 summarize the experiment results. the origin of the mimu frame is discussed in the sbg-imu user manual, while that of the camera is the optical center of the lens. therefore, the translation vector between the mimu and camera can be roughly evaluated. the results that are evaluated based on the mechanical structure coincide with those that are evaluated by using ekf. the difference of imu calibration parameters before and after modification is not significant. the camera measurements can suppress the angle integral error due to imu's drift, and increase the accuracy of imu extrinsic parameters calibration results. thus, more accuracy camera measurements will lead to more accuracy imu extrinsic parameters calibration results. but the accuracy of imu extrinsic parameters calibration results is closed before and after checkerboard corner modification, through compare the results in tables 9 and 10 . there may be three reasons explain it."
"is the tangential acceleration, g i is the gravitational acceleration expressed in the imu frame, and c i b is the translation matrix from the system body frame to the imu frame, which can be calculated by the euler angle ψ i b"
"the svm is a machine learning algorithm which is particularly used for supervised classification of terrestrial multispectral data and, recently, for seafloor acoustic mapping [cit] . in a study by reference [cit], they applied the svm on backscatter mosaic objects, whereas, in this study, we applied the algorithm on cells of the hac layers. the svm maximizes the separation of classes in a high-dimensional dataset by fitting a hyper-dimensional plane (or just hyperplane) to the data feature space. the hyperplane is produced using a kernel function when the data cannot be linearly separated in feature space. the selection of a particular type of kernel function depends on the data separation pattern (in feature space) and will have a strong effect on classification. the svm has the ability to identify non-linear relationships within the data, and does not require normal distribution of the training backscatter values from within each acoustic class. the svm used in this study was implemented in the opencv library provided by the system for automated geoscientific analyses (saga)."
"testing the performance of the four applied machine learning classifiers (sad, rf, svm, and ann) revealed valuable information about new capabilities for automated mbes backscatter classification. a particular aspect in this study is the application of these algorithms on the hac matrix. this type of data structure allows for exploiting the angular dependence of backscatter within a new processing concept, while preserving the high spatial resolution of angular backscatter layers. one important finding is that the majority of the above classifiers produce valid classification results using only a small but representative amount of ground-truth training data. thus, careful selection of ground-truth information is crucial for the performance of machine learning classifiers. it is suggested that each seafloor type should be represented by at least one training area. this implies that even the minimum amount of ground-truth data (one sample per seafloor type) is enough for classification, as long as the data come from acoustically and geologically distinct areas."
"soundings from within matrix cells corresponding to ground-truth locations shown in table 1 were utilized as training data in the following supervised classifiers: a pattern recognition approach (sum of absolute differences, sad), random forest (rf), support vector machine (svm), and an artificial neural network (ann). each algorithm was applied on both types of hac matrices, i.e., that made of interpolated backscatter layers (ihac) and that made from angle-normalized mosaics (shac). these algorithms were selected since traditional supervised classification with the maximum likelihood classification (mlc) method has some limitations regarding the quality and quantity of training samples and the dimensionality of the backscatter data [cit] . in supervised classification, the training data provide the \"knowledge\" regarding each defined class that assist the algorithm in classifying the rest of the (unknown) data. the \"knowledge\" consists of a set of input data including the angular responses of each incidence angle and the corresponding seafloor type value (categorical). in this study, we use the term \"descriptors\" to refer to the angular layers of the ihac or the shac from which the angular responses were obtained."
"finally, \"estimator\", we estimate the extrinsic parameters based on ekf. the main idea of this calibration method is that establish the equations that provide the extrinsic parameters and solve these equations. to better understand the method, the basic measurement models of the imu and camera are described in the section 2.1. because the calibration operation process is swinging motion, the detailed measurement equations of imu and camera under swinging motion are described in section 2.2. we exploit the complementary nature of imu and camera to improve the calibration accuracy, the method of inertial aided checkerboard corner extraction under motion blur is described in section 2.3. and the extrinsic parameters are evaluated based on ekf, the main idea of this calibration method is that establish the equations that provide the extrinsic parameters and solve these equations. to better understand the method, the basic measurement models of the imu and camera are described in the section 2.1. because the calibration operation process is swinging motion, the detailed measurement equations of imu and camera under swinging motion are described in section 2.2. we exploit the complementary nature of imu and camera to improve the calibration accuracy, the method of inertial aided checkerboard corner extraction under motion blur is described in section 2.3. and the extrinsic parameters are evaluated based on ekf, which is shown in section 2.4. the rotation angle of turntable is evaluated by camera measurement to suppress the imu's drift."
"as a result, careful planning of ground-truth sampling prior to classification should take place, and it should consider variations in backscatter data as well. in the case that the training set consists of polygon cells (e.g., for svm and ann), the cell size should be set at the same size as the grid cell size of the hac layers. this affects the quality of angular responses considered and will have an impact on the class separation when the sediment variability is high within a few meters. it is implied that very large cell sizes will result in more mixed angular signatures and may produce inconsistent classification results. the same holds for the case when the training data are points (e.g., for sad and rf). angular responses of reference seafloor types should be continuous and sufficiently separated from each other."
soundings forming the angular signature of known seafloor types hold particular standard deviations for each seafloor type (figure 3 ). this information helps in defining tolerance boundaries for the mean backscatter values of each seafloor type. the standard deviation offset can be used for table 1 .
"at time t, camera and imu start collecting data simultaneously. the corners extracted have some delay due to the motion blur in the image, with real data at time t + τ/2. τ is the exposure time of camera. we can modify the corner coordinates with the inertial data. the details are presented as follows."
"this step is the same as that in the conventional method. the corners in the image, including the complete checkerboard, are examined. i x and i y denote the x (horizontal) and y (vertical) components of the 2d numerical gradient, and the first and second derivatives are computed as the gradient direction is evaluated by"
"the shac provides the possibility of collecting mbes data with standard overlap and obtaining angular responses per cell as efficiently as with using an ihac. thus, the shac minimizes the costs of backscatter-dedicated surveys and seems as a promising alternative in ara classification. the high dimensionality of the hac matrices makes them ideal inputs to machine learning algorithms for advanced supervised seafloor classification by incorporating the angular backscatter information. machine learning algorithms require the input of ground-truth data for training. practical limitations are usually responsible for collecting small amounts of ground-truth data, and this presents another challenge in seafloor mapping. in this study, we tested the performance of four machine learning algorithms using the hac layers and only one sample per seafloor type for classification. all four algorithms produced very comparable results that were in agreement with the classification results from an unsupervised method used as reference. the hac approach described here shows potential in future ara studies by maximizing acoustic class homogeneity and providing improved class separability. the use of machine learning algorithms in conjunction with high-dimensional angular backscatter layers provides a reliable tool for fast, automated seafloor classification that integrates angular backscatter information in an effective way."
"the translation matrices c v b and c i b, which occur between the system body frame and the camera frame as well as between the system body frame and the imu frame, respectively., which are derived from the camera and imu principal points to the origin of the system body frame, respectively."
"considering that extracting angular responses from the lowest level of acoustic elements (i.e., the grid cells of a backscatter layer/mosaic) is the way forward in improving the performance of ara, and that the hac provides this possibility in a solid way, we focused the scope of this paper on utilizing an hac from mbes data with high overlap and, alternatively, from mbes data with standard overlap. we present a method to construct the hac from mbes data with standard overlap in order to overcome the practical limitations of backscatter-dedicated surveys, thus minimizing the time and cost of the fieldwork. this method was recently considered in the literature by references [cit] . in addition, we examined and compared the performance of four supervised machine learning classifiers in terms of class separation and how to deal with a restricted amount of training data along with the high-dimensional hac dataset. building a high-resolution hac from mbes data of standard overlap and exploiting a small amount of ground-truth samples for classification will have an impact on future backscatter studies by decreasing the cost of data collection, while, at the same time, maximizing the usability and quality of acoustic and ground-truth datasets."
"the sad algorithm was initially developed for video compression and it is widely used for object recognition tasks [cit] . in our implementation, the algorithm calculated the difference between reference and unknown backscatter values of corresponding incidence angles. then, similarity criteria were applied to the sum of differences for each class assignment. it is implied that the lower the values of the sum of absolute differences are, the higher is the similarity between the 2d objects (angular signatures) under examination will be. the user has to set only two criteria that will influence the similarity assessment and, hence, the final classification results. these criteria include the standard deviation offset and the majority criterion. the standard deviation offset criterion controls the maximum acceptable absolute differences between known and unknown angular signatures. it is obtained by means of data exploration of soundings falling within the ground-truth cells (figure 3) ."
"a calibration experiment is conducted to confirm the validity of the proposed method and to evaluate the accuracy of the system. figure 5 shows the experiment architecture, whereas table 7 presents the main devices. the system body frame coincides with the turntable frame for the sake of simplicity because the mechanical interface of the turntable frame is clearly defined."
"where a i true is the true specific force excluding gravity, ω i true is the true angular velocity, g i is the gravity expressed in the imu frame, b a and b g are the bias errors of the accelerometer and gyro, respectively, and n a and n g are the process white gaussian noises of the accelerometer and gyro, respectively. both b a and b g show minimal changes within a short period."
"given that imu can measure angular velocity and acceleration vectors during rotation, the constraint equations of inertial vectors and the extrinsic parameters of imu can be established. when the turntable rotates around by one axis, the position vector and euler angle along the rotation axis cannot be computed. however, when the turntable rotates by around two orthogonal axes, extrinsic parameters of imu can be computed. fortunately, imu calibration is generally performed by using a turntable with two or three orthogonal axes."
"a calibration experiment is conducted to confirm the validity of the proposed method and to evaluate the accuracy of the system. figure 5 shows the experiment architecture, whereas table 7 presents the main devices. the system body frame coincides with the turntable frame for the sake of simplicity because the mechanical interface of the turntable frame is clearly defined."
"an extrinsic parameter calibration method for a visual/inertial integrated system is developed based on a swinging motion. a checkerboard corner detection algorithm is then utilized to detect checkerboard corners with a smearing effect. the extrinsic parameter calibration method is developed based on the imaging model and dynamic equation. this method is validated by performing a simulation and a real-world experiment, which results highlight the effectiveness of the proposed method. this method can also be seen as a standard calibration step and used for visual/inertial systems, especially for visual and inertial navigation integrated systems."
"we utilize inertial data to eliminate the smearing effect and then extract the checkerboard corners with the aid of the imu data. the corner detection algorithm is developed as follows. the lucy-richardson method is used to rectify the blurred image in step a, and the conventional corner detection method is used to find the corners' pixel coordinates in step b. given that checkerboard corners are constrained on a few lines, we add a linear constraint to refine the checkerboard corner location. through the linear constraint, the corners' pixel coordinates are refined in step c. finally, imu measurement is used to correct the position of corner detected for movement in step d."
"camera calibration generally involves the use of a checkerboard. in the calibration process, both the intrinsic parameters and the translation matrix and position vector between the camera and checkerboard frames can be computed. by turning the turntable, the constraint equation can be established based on the relationship between the extrinsic parameters of camera and the change in the camera position and attitude. the constraint equations of the visual/inertial system are described as follows."
"the method for imu aided checkerboard corner correction under motion blur is introduced, which is the tip step to improve accuracy of camera calibration. we use the imu to evaluate the motion parameters, and use these motion parameters to eliminate the effects of motion blur in image. it is described in section 2.3. the extrinsic parameters are evaluated based on ekf, and the rotation angle is evaluated using camera measurement to suppress imu's drift. 3 ."
"secondly, \"data acquisition\", the turntable is controlled manually to rotate around two orthogonal axes, such as the x and y axes, for the checkerboard image acquisition by camera and the inertial data acquisition by imu, respectively."
"where β x is the rotation angle of the turntable when rotating around the x axis. however, when rotating around the y axis, we have"
"secondly, \"data acquisition\", the turntable is controlled manually to rotate around two orthogonal axes, such as the x and y axes, for the checkerboard image acquisition by camera and the inertial data acquisition by imu, respectively."
"author contributions: c.o. proposed the idea of the calibration method and carried out the theoretical derivation; c.o., k.z. and s.s. performed the experiments and analyzed the data; s.s. and z.y. conceived and supervised the experiments; c.o. wrote the paper."
"the layers building the hac matrix may result either from interpolation of spatially dense backscatter values of each incidence angle (ihac) or by producing normalized backscatter mosaics using different incidence angles for reference each time (shac). the ihac requires backscatter-dedicated mbes surveys with more than 100% overlap for obtaining dense soundings per seafloor area. since backscatter-dedicated mbes surveys are time-consuming (thus, costly), the shac is a useful alternative. the shape of angular responses yielded from the ihac may appear less smooth than those derived from the shac. this occurs because the cell values of the ihac reflect the effects of the natural seafloor variability from actual soundings values, while the shac cell values represent angular variability based on calculation of average local backscatter intensity. thus, it is essential to select an appropriate number of pings for averaging during the angle-normalization process. averaging over a relatively large number of pings (i.e., including backscatter values from many seafloor types) will produce an shac, the angular signatures of which might be very smooth and possibly overlapping/non-distinguishable. it is suggested that the optimal number of pings may be estimated using the pings within a certain radius from the ground-truth locations. pings from within a small radius should form well-shaped angular signatures which gradually (with increasing radius) produce more mixed signatures."
"the sad algorithm uses a set of reference angular signatures representing a known seafloor type (class) (figure 4c ) for comparison with unknown signatures extracted from each grid cell of the hac. matching is based on criteria set by the user (standard deviation offset, majority) and each cell is assigned with the class represented by the successfully matched reference signature. in this study, the offset value was selected according to the standard deviation of db values from each angular signature; thus, different offsets were applied for each class (figure 3 ). the majority threshold was set to 90% of the total number of incidence angles. some cells could be matched with up to two adjacent angular signatures and receive double class assignment. in this regard, class overlap was quantified and it was found to affect class boundaries of neighboring classes ( figure 5 ). cells without continuous angular signatures or cells that did not conform to the majority criterion remained unclassified."
"backscatter values that are encompassed by either training areas or points are not required to have normal distribution when the described classifiers are used. performing accurate seafloor classification using high-dimensional acoustic data, even with a small training set, is a very common situation in the field of seafloor mapping. the above algorithms in conjunction with the hac structure will decrease the time and cost for ground-truth data collection, by effectively using a smaller but representative amount of seafloor samples."
"for expression quantification, bowtie2 42 is used to produce alignments of the reads against the assembled transcripts. alignments are then properly formatted using samtools 43 and picard (http://broadinstitute.github. io/picard/). 43 using these alignments, express 44 can be used to quantify the expression of all isoforms. additionally, the script \"run_rsem_align_n_estimate\" of the trinity package implemented in trufa uses bowtie 45 and rsem 46 to provide an alternative procedure for expression quantification output. trufa generates a large amount of output information from the different programs used in the customized pipeline. briefly, a user should be able to download fastqc html reports, fastq files with cleaned reads (without duplicated reads and/or trimmed), trinity-assembled transcripts (fasta), read alignments against the transcripts (bam files), go annotations (.txt and.dat files which can be imported into the blast2go java application), and read counts (text files providing read counts and tpm). various statistics are computed at each step and are reported in text files, such as the percentage of duplicated/trimmed reads, cegma completeness report, assembly sequence composition, percentage of mapped reads, and read count distributions."
"it can be observed, in table 2, that groups formed, both online as offline are well defined because dd is higher, although in online processing the ss index is lower than ds, it means that within the groups are assigned patterns from a different one. this may be due to two situations, the first, the influence of ș parameter on the threshold to formed groups, then the greater the value of ș the greater the radius of groups and consequently the number of resulting groups can be different to real number, as in this case, which causes that patterns are assigned to one group that is not their own, according to the real data."
"on the other hand, sensorlock deals with the problem of security in sensor networks from a broader perspective. it considers the need for secure key storage in sensor nodes quite important since they are great in number and their content is subject to capturing and tampering attempts. thus, besides security in key distribution it is also intended, as explained in section 3, to provide a larger resiliency and robustness to the system in such a way that the sensors' capturing and tampering will not result in any key discovery."
"flash memory through the application of sensorlock, mac and conf are stored in a encrypted way in sensor memory. all encryption and decryption operation of such keys is accomplished within cm, avoiding plain text keys exposure in sensor memory. in summary, sensorlock shields the keys used in tinysec architecture avoiding that attackers can read plain text keys from sensor memory, impersonating legitimate nodes or listen to the network communications."
"for the sensorlock applied to tinysec case, we argue that the access to the stored keys within sensors, sink included, simultaneously does not aggregate information to system keys discovery. that is, as each sensor encrypts keys with its own secret, the key discovery likelihood remains 1/2 corresponding to brute-force attack."
"(3) sensorlock over tinysec. (sl/tinysec) in this scenario, sensorlock is applied to tinysec. it was coded as a function that plays the role of cryptographic module where takes place encryption, decryption and secret storage. in this architecture, the predistributed key used to compute macs is encrypted with and stored within the sensor memory. whenever the sensor sends a certain message, such a key is decrypted, with, and used to encrypt the message. in section 3.2, it has shown more details about how to apply sensorlock to tinysec architecture. both are transmitted intermittently. since the sensors recover pairwise keys, the communications take place. we point out that the communication takes place between node son and node father; that is, such nodes must share keys so that there is communication between them. before forwarding a message, the node son verifies if there is a shared key with its father. in case it is true, the message is sent. the keys must be shared between sons and fathers due to the ctp son-father communication flows, which is used in our implementation. it is sufficient that just a pair of sensors in the end to end path do not have shared keys so that the message does not arrive to sink node. in such case, the message is discharged before to reach the sink. ncd architecture is discussed in section 3.4."
"trufa platform, user manual, example data sets and tutorial videos are accessible at the web page https://trufa.ifca.es/web. accession numbers to the read files used in this study are provided in the results and discussion section and can be obtained from http://www.ncbi.nlm.nih.gov/sra/."
"jaccard coefficient doesn't take into account data pair that doesn't match (dd), and as a result of this, reflects the proportion of data that has been assigned, so, according to this index, the similarity between partitions is very low. while fowlkes and millows index calculus indicates de probability that the elements are assigned the same way in the group and in the real data, thus the obtained value indicates that probability is 50%."
"additionally, the working of ctp protocol also can influence obtained result. in ctp, the relation son-father between nodes can change during the experiment (simulation run) that result in changing in relation son-father. eventually, after such changing, the son may not share keys with its new father and all message forwarded to such son node will not be forwarded toward sink node. hence, as the bigger the end to end delay in terms of number of hops, the bigger the probability for a message to be dropped. on the other hand, those nodes placed near sink have major probability of delivery of their messages what contribute to the reduction of end to end delay in ncd and sl/ncd architectures. for ncd codification, it is used as the current implementation of ctp available in tinyos 2.1.1 where the beacon default initial value is 128 ms. the beacon is sent periodically and its value is incremented. it is used by ctp to link quality estimation. from that, a node can be associated with other father nodes in the new network tree established by ctp protocol. figure 6 shows the average end to end delay for a 100-node network. this average is obtained from 100 simulation runs. the delays oscillate between 684.828 ms and 1666.495 ms to wc, tinysec, sl/tinysec, spins, and sl/spins architectures. the results indicate a similar behavior to 49-node network where wc presents minor average delays in relation to spins and tinysec architectures; spins and sl/spins delay bars slightly overcome wc. sl/spins presents negligible overhead over spins architecture. however, tinysec and sl/tinysec bars result in higher delay than wc, spins, and sl/spins. one of the reasons is the major processing overhead in macs computation. it is observed as a similar behavior in ncd and sl/ncd bars when compared to the 49-node networks. their delays are remarkably small in relation to other architectures, due to the aforementioned reasons in the 49-node network."
"it is observed that the architectures wc, tinysec, sl/tinysec, spins, and sl/spins present high absolute values of message sending, since the sending of messages is not conditioned to the existing of a shared key. that is, wc does not use any keys, whereas the keys are previously known by the sender and the receiver nodes, considering the remaining architectures aforementioned."
"for estimation of network power consumption, the following events are considered: the sending, the forwarding, and the receiving of messages. we deem that processing consumption is less significant compared to power consumption caused by radio data transmission and receiving. the power consumption in radio transmission and receiving modes was obtained from micaz architecture specification because our simulation experiments are carried out over such sensor architecture [cit] . the network overall consumption is presented in joule (j), as figure 12 . it is realized that energy consumption decreases as the network scalability increases for each architecture. a similar behavior is also observed in transmitted and received messages, in figure 10, that decrease as network scalability increases. such similar behaviors are expected since energy consumption is calculated in function of transmitted and received messages, which decrease due to major medium competition in more dense scenarios."
"as sensorlock random number generator, an option of use is tinyrng proposed by francillon and castelluccia [cit] . it consists in a deterministic algorithm that generates pseudorandom numbers for nodes in wsns. the authors proposed the use of bit errors in messages received by sensor as source of entropy. tinyrng uses encryption algorithms, as cbc-mac presented in section 4.1, also used in sensorlock what facilitates its implementation as kg within the proposed mechanism. besides, a public implementation of tinyrng is available which may contribute to reducing sensorlock production costs."
"however, such an algorithm requires solving an np-complete problem in every time slot, and is very difficult to implement in a distributed manner. in the next subsection we introduce the recently developed throughput optimal distributed csma scheduling algorithm."
"international journal of distributed sensor networks 9 for this evaluation, we consider the following metrics: end to end delay, power consumption estimation, number of sent/received packets, number of hops, access medium time, and scalability. table 2 shows simulation parameters. each simulation stop when the number of events generated described in table 2 is reached. as examples of events, we can cite the transmission and receiving of messages, access to sensor's memory, sensing, and so on [cit] ."
"the energy consumption is similar among architectures with same number of nodes. for example, scenarios as tiny-sec 49 and sl/tinysec 49, spins 49, and sl spins 49 reveal a similar energy consumption demanded by sensorlock in relation to other approaches. though ncd 49 and sl/ncd have absolute values too smaller than previous scenarios, their difference in terms of energy consumption is also similar. figure 13 depicts the average consumption by node. such consumption is given by the reason between the network overall consumption and the network number of node, for each evaluated architecture. figure 14 shows the average time demanded by medium access control mechanism based on csma/ca. it is important to point out that such time is part of end to end delay in each message sent, in average. in the case of wc, tinysec, sl/tinysec, spins, and sl/spins architectures, it is noticed as a increasing behavior of time demanded by medium access control mechanism as the network scalability (49, 100, 144, 225, and 400 nodes) increases. it is because the medium competition and eventual collisions increase with the increasing of number of network nodes. such scenario causes the increasing of waiting time for access medium."
"(a) furthermore, in the datasets which only knows the number of groups, the calinski-harabazs and davies-boulin index with the real number of groups can be used to determine which the largest quality clustering, thus is finding the corresponding ș value for that clustering. from both index have a growing trend, the clustering analysis based on both allows finding more reliable clustering of better quality. as for the datasets that lack of any information describing the actual grouping, both indexes are used again to define the quality."
"when a reference genome is available, which is normally the case for model system species, a reference-guided assembly is preferable to a de novo assembly. however, an increasing number of rna-seq studies are performed on non-model organisms with no available reference genome for read mapping (particularly those studies focused on comparative transcriptomics above the species level), and thus require a de novo assembly approach. moreover, when a reference genome is available, combining both de novo and reference-based approaches can lead to better assemblies. 15, 16 analysis pipelines encompassing de novo assemblies are varied, and generally include steps such as cleaning and assembly of the reads, annotation of transcripts, and gene expression quantification. 16 a variety of software programs have been developed to perform different steps of the rna-seq analysis, [cit] but most of them are computationally intensive. the vast majority of these programs run solely with command lines. processing the data to connect one step to the next in rna-seq pipelines can be cumbersome in many instances, mainly due to the variety of output formats produced and the postprocessing needed to accept them further as input. 20 the platform is highly parallelized both at the pipeline and program level. it can access up to 256 cores per execution instance for certain components of the pipeline. on top of allowing the user to obtain results in a relatively short time thanks to hpc (high-performance computing) resources, trufa is an integrative and graphical web tool for performing the main and most computationally demanding steps of a de novo rna-seq analysis."
on the other hand xuan [cit] analyzes some of the most diffused measurements on information theory and proposes an adjustment based on the forecast proposed by hubert and arabie from mutual information and information variation distance (vi) from which its normalization takes place based on normalization index. this adjustments help to maintain the index close to zero.
"from the counting-pairs results were calculated the index in table 3, which shows the result of the index used to assess the quality of the clustering. to begin, generally can be seen in the table 3 that results of offline and online processing are quite similar, the difference among them is about one hundredth, besides having the same behavior. this indicates that online processing of the dataset according to ram available generate a clustering very similar to the offline clustering."
"differently from previous proposals, the system of oliveira and barros [cit], hereinafter referred to as network coding distribution (ncd), is based on key distribution, key predistribution, and mobility. in such scheme, the task of key distribution, which is carried out using a network coding technique [cit], is accomplished by a mobile node (mn). this system, however, has a limitation: it allows an attacker to discover all keys used in the system when accessing the memory of any sensor along with the mobile node memory access."
applied to ncd. this study case is based on the oliveira and barros [cit] scheme referred in this paper as network coding distribution (ncd). a previous
"moreover, sensorlock allows an additional level of security in relation to the approach wss, which makes the system more robust, even if the attacker has access to the memories of the mobile node and of any sensor from the network, simultaneously. the resiliency against such event is demonstrated as follows. theorem 1. the knowledge of memory content of a sensor and the mobile node, simultaneously, does not increase the information concerning any key used by the system."
"proof: due to space limits, we only describe the intuition behind the proof. from the complementary slackness property, from the lagrangian of mov we have"
"one of the most popular architectures to sensor networks security is tinysec [cit] . the authors argue that end to end security is unsuitable to wsns due to data aggregation because intermediate nodes eventually may need to access, modify, and eliminate such information. as an alternative, they defend that security services such as authentication, integrity, and confidentiality should be provided between neighboring nodes, proposing their implementation at the link layer. the authors explain their architecture may support the development of a key distribution mechanism; however they do not define such mechanism. the tinysec architecture enables the authenticated message exchange among sensors through message authentication codes (mac). thus, the proposal uses a shared master key predistributed among all network nodes. for this reason, if an attacker captures the keys of just one node he has access to all network communications."
"on the other side, f-measure calculates in a more accurate way how much clusters are similar between each and the original cluster, obtaining a balanced average of the patterns assigned correctly respect to the total of them and those who should have been assigned, thus the result obtained in table 3 indicates the accuracy of the clustering is good, being the patterns assigned in a correct way in its most."
"the block 1 is used to key initialization. for that, it receives a key from sensor's memory and secret stored within cm. the output of this block is the encrypted key which is stored in sensor's memory. moreover, 1 block and exclusive or block ⊕ cooperate for receiving keys. that is, the ⊕ block receives the distribution message and recovers a key that belongs to the other sensor, represented as ( 2) in figure 1 . soon after that, such key is input of 1 block, which encrypts it within the cm and stores it within the sensor's memory."
"for simulations, we used tinyos simulator (tossim) [cit] . tossim is a discrete event simulator and works by translating hardware interrupts into discrete events. tossim is a library of tinyos. it is important to highlight that the tinyos simulator supports only the micaz architecture in its compilation process. therefore, our experiments are performed over such architecture."
"in this work, we focus on large-scale experiments relying on 49-node, 100-node, 144-node, 225-node, and 400-node networks. in this sense, we present an evaluation of sen-sorlock through simulation using the tinyos platform. we implement key distribution architectures using broadly known protocols and algorithms, as collection tree protocol and advanced encryption algorithm."
"the clustering is a data mining task designed for identifying similar features between the data and put similar patterns together in the same group, while among the groups exist a difference between the features of their patterns. this task starts from an unlabeled data set, that is to said, there is no specific information indicating the classification of the patterns and it is expected that real data does not have prior information [cit] ."
"we presented trufa, a bioinformatics platform offering a web interface for de novo rna-seq analysis. it is intended for scientists analyzing transcriptome data who do not have either bioinformatics skills or access to fast computing services (or both). trufa is essentially a wrapper of various widely used rna-seq analysis tools, allowing the generation of rna-seq outputs in an efficient, consistent, and user-friendly manner, based on a pipeline approach."
"on the other hand, the ncd and sl/ncd architectures obtained low delays compared to remaining approaches. several factors contribute to such behavior, as the ncd architectures peculiarities and the ctp protocol operation."
"based on the previous analysis of the results is possible to see that no all the validation index quantify the clustering quality in the same way, and this quantity sometimes does not completely describe the clustering quality. due to mention before, taking into account that the results for d31 dataset are known, could be determined that in the most of the experiments, with all the ș values for this dataset, the fmeasure is closer to the right description of the cluster quality, as it was mentioned it considers the number of assigned patterns correctly to respect all these and the ones that really should be assigned."
"key management in wsns certainly drives many issues. indeed, it is frequently claimed that standard security mechanisms are prohibitive in wsns because they demand extensive use of scarce resources, such as processing power, battery capacity, limited memory, and low bandwidth. however, since sensors are typically unattended, it is a challenge to protect sensitive data stored in sensors, as well as cryptographic keys. key storage is a challenging issue in sensor networks which still demands suitable solutions [cit] ."
"in this work, for large-scale simulations, the following tools and platforms are used: tinyos, nesc, tossim, ctp, and security mechanisms. in the next paragraphs, we describe each of them."
"regarding the received messages, message loss is due to interferences and collisions present in wireless medium. specially, it is observed that the number of received messages using ncd and sl/ncd is smaller than using other approaches, since ncd and sl/ncd demand hop by hop key sharing for a message to be successfully forwarded to sink. particularly, one aspect that can contribute to the small amount of received packets in tinysec and sl/tinysec consists in eventual packet drops due to macs verification by intermediate sensors in end to end path. such hop by hop verification does not take place in spins, for example, that verifies macs only in end systems. in general, the scenarios present a delivery ratio with variation between 16% and 76%."
"further, under mild assumptions about the consensus algorithm, it can be shown that the two schedules in (17) and (21) are the same with high probability. finally, it can be shown that the updates in (18) and (19) correspond to the standard primaldual algorithms for solving convex optimization problems. thus, the variables will converge to the optimal. we next elaborate on the change in step 1, by showing the equivalence between the csma phase in dis-sch and the simplex search phase in sim. we have the following proposition."
"the future lines for this issue are oriented to the proposal to implement different validation index and analyze their behavior, as well as work with other datasets to parse through the index an optimal ș value, and compare if is the same as in this paper."
"implementation details and ncd peculiarities help to explain the obtained results regarding delay. in ncd approach, it is enough that just a pair of sensors (node son and node father) in end to end path does not have shared keys so that the message does not reach sink. hence, messages from sensors placed near to sink, in terms of number of hops, tend to have more success probability in delivery of messages. thus, on average, messages with 1hop distance nodes from sink are frequently delivered in ncd and sl/ncd architectures. this explains lower end to end delay and the abrupt reduction of number of delivered messages to sink node, when compared to other approaches. we highlight that during simulations node message delivery was observed with, for example, 7 and 8 hops far from sink. however, the number of 1-hop delivered messages observed was too big that result in an average of 1.84 hops for ncd 49 architecture and 1.85 hops for sl/ncd 49 architecture. such data is shown in table 3 ."
"pipeline. several programs can be called during the cleaning step (table 1) . the program fastqc (http://www. bioinformatics.bbsrc.ac.uk/projects/fastqc) has been implemented to assess the quality of raw reads and give the statistics necessary to tune cleaning parameters (fig. 1) . after the quality of the data is determined, cutadapt 32 and prinseq 33 allow, among other functionalities, the removal of adapters as well as low quality bases/reads. in particular, prinseq has been chosen for its ability to treat both single and paired-end reads and to perform read quality trimming as well as duplicate removal. using the blat fast similarity search tool, reads can be compared against databases of potential contaminants such as, eg, univec (which allows identifying sequences of vector origin; http://www.ncbi.nlm. nih.gov/vecscreen/univec.html) or user-specified databases. trufa's scripts will automatically remove those reads, giving hits with such queried databases."
"the cm's generic design supports free choice of symmetrical algorithm, to encrypt and decrypt the secret, used within cm blocks allowing its customization. we can cite as symmetric ciphers the rc5 and aes [cit] ."
"the average number of transmitted (by sensors) and received messages (by sink) in all architectures is shown in figure 10 . this result reveals a decreasing behavior where the amount of delivered messages decreases according to the size of the network increases. it is important to notice that each experiment performs an amount of 10 million events. thus, as the number of events is the same for all simulations, nodes in sparse network scenarios send more messages than nodes in dense network scenarios."
"this article is divided into five sections: the first one explains the clustering and validation methods. next, introduce a review of previous work on this subject; after that, in section three the proposal is described, and in section four and five are shown the experiments and the results, respectively. finally, conclusions and study open lines are exposed in section six."
"to perform the cluster validation on data sets that have information on their actual grouping, or reference set, raises the using of the counting-pairs index as well as the fmeasure. for the data sets lacking of information or reference set, were used the davies-boulin and kalinskyharabasz validation index, also the precedent obtained on the data sets that has actual grouping information."
"the block 3 receives as input the key ( ) and a message . the kind of processing that the block 3 does over such message is defined through control information from the application. thus, the block 3 can process the message in different ways: encryption ( ) ( ), decryption ( ) ( ), and computation of message authentication codes mac ( ) ( ). moreover, the application selects the suitable key from sensor's memory according to the security service demanded (confidentiality, authenticity, or integrity) and sends it to sensorlock. as example of possible outputs from block"
"this algorithm takes as a first center any of the objects and takes as a second center the furthest from the first, from these two centers are calculated the groups threshold and select again the furthest object from the two centers, its distance is compared with the nearest center. if the threshold is exceeded a new center is created, if not, then the object is assigned to the nearest center, this is done until there are no more centers. finally, the assignation of objects is verified with its nearest centers. the main pseudo-code is shown below [cit] : while tĸø"
"following will be described the results for the datasets joensuu and mopi. for their validation were implemented the calinski-harabasz and davies-boulin index, from the results of which is determined what is ș more convenient parameter value for processing the data. it is important to point, as it has been mentioned, that in all the results there is a growing tendency to the ș value, in both index, so that is taken as main reference the d-b index score, for which the lower value the better clustering result."
"tinysec is depicted in figure 2 (a). in steps (1), (2), and (3), we can verify that the keys mac and conf are distributed before the network operation. in step (4), the keys are used to encrypt a message. if confidentiality, integrity, and authentication are important, first conf is used to encrypt the packet payload field. later on, the computation of the message mac is done. beyond the key mac, such computation receives as input whole packet. finally, the mac is attached to the packet. in step (5) the decryption of the message is carried out."
"however, the ncd and sl/ncd architectures present opposite behavior. that is, the time demanded by medium access control mechanism tends to decrease as the number of network nodes increases. this is due to the smaller access medium competition than other architectures. as aforementioned, ncd and sl/ncd have the peculiarity of sending messages only when two nodes share keys that decreases the access medium competition and, consequently, the waiting time to obtain such access."
"there are two principal approaches of clustering and different kinds of algorithms for doing this task. murty [cit] expose a categorization of these types, the first one corresponds to soft clustering which handles the cluster overlapping through fuzzy and genetic techniques and hard clustering where is not permitted the group overlapping. this last approach is divided into hierarchical and partition algorithms."
"(sl/ncd) in this scenario, sensorlock is applied to ncd. in sl/ncd, the preloaded keys into the sensors are encrypted with . before the sensor sends a message, the key must be decrypted so as to encrypt the message. when the mn distributes keys, the sensors receive these keys and soon after they encrypt them with their secrets . such new architecture, sensorlock applied to ncd, is described in section 3.4."
"when applicable, reads corresponding to each end of a pairended reaction were concatenated separately into two files, and all files were compressed with gzip before uploading to the platform. each of the compressed read files was uploaded to trufa in less than a day (typical uploading times from a personal computer anywhere ranging from 30 seconds to 12 hours for files ranging from 200 mb to 12 gb, ie, between 0.25 and 25 gbp)."
"in such work, it was presented a mathematical analysis and small-scale experiments in order to measure what is the processing overhead due to the application of sensorlock in communications between sensors. on the other hand, in this work, we evaluate such scheme in large-scale wsns. the scheme in figure 4 (a) depicts pairwise keys distribution in wsns. all system keys are stored as encrypted text in mn that accomplishes xor operation between previously stored keys, one from sensor and other from sensor . these sensors want to communicate with each other (1) . through such operation, the key is canceled out. in (2) it is possible to verify the pairwise key distribution encrypted broadcast transmission. in steps (3) and (4), the keys are received and stored as plain text in sensors and . in this approach, if mn and a network regular sensor are captured [cit], it is easy to derive the information which is used to encrypt all system keys, compromising its security. on the other hand, we point out that sensorlock can be applied to encrypt several keys as demanded by ncd architecture analyzed in this study."
"(1) all keys stored in memory must be encrypted with a sensor private secret from sensor . (2) the secret must be stored in a cryptographic module (cm) with tamper proof technology. (3) the cm must contain hardware modules to encrypt and decrypt keys and messages without even exposing the secret or the keys used to encrypt and decrypt messages. based on such fundamentals, sensorlock cm architecture is depicted in figure 1 . the secrets and are stored in nonvolatile memory. the secret is used to encrypt and decrypt all keys stored in sensor memory. on the other hand, is generated and used during the key renewal process. such keys are stored in tamper-responsive device that assures the destruction of them when a tampering attempt to cm is detected. a key generation (kg) block is used for generating and renewing of secret."
"considering the entire pipeline, each testing dataset was analyzed by trufa in less than a week (table 3), confirming a good time efficiency of the platform. according to macmanes 13 on the effect of read trimming for rna-seq analysis, optimizing trimming parameters leads to better assembly results. this optimization should take no longer than 3 days of computation for datasets such as the ones used here and can be easily done with trufa by producing in parallel various assemblies and their quality statistics with different sets of trimming parameters and parameter values."
"in this paper, sensorlock is applied to three symmetric key distribution architectures: tinysec, spins, and ncd. tinysec and spins are well-known security architectures for wsns. they are based on key predistribution where a reduced number of keys are stored within sensors. differently, ncd is based on predistribution and distribution. moreover, it supports the storage of a huge number of keys in each sensor. it is important to highlight that sensorlock is applicable to all architectures aforementioned, as detailed in the next sections."
"an important component available in tinyos is the collection tree protocol (ctp). it consists in a protocol responsible for routing and forwarding messages from regular network nodes to sink node. ctp organizes the network in tree structure with father and son nodes. the protocol builds routes of minimal cost to sink using the expected transmission count (etx) metric. it is important to highlight that all implemented architectures in this work use such routing protocol. ctp was tested in different testbeds and was integrated with tinyos 2.1.1, which supported our decision in its adoption. however, any wsn routing protocol could be considered for it."
"according to hu and sharma [cit], there are two classes of tamper proof mechanisms: active and passive. the former involves tamper proof hardware circuits within the sensor nodes that enable an immediate response to tampering attempts, what makes them tamper-responsive. the latter includes mechanisms that do not require active circuits, such as protective coatings and tamper seals. in general, such material is tamper-evident, but some of them may damage the protected area if removed and, because of that, they are also called tamper-resistant mechanisms."
"the web server without the need to download large quantities of output. we also plan to complete the platform by providing features for read mapping against a reference genome (such as, eg, star, 50 tophat, and cufflinks). a cloud version of trufa, which would increase considerably its global capabilities, is also envisioned to be run in the egi.eu federated cloud (see https://www.egi.eu/infrastructure/cloud/) in the near future."
"as seen, there are different kinds of clustering algorithms, but the one analyzed and implemented here is the batchelor and wilkins algorithm [cit] . for applying the clustering algorithm the number of groups to obtain does not need to be previously known and it just needs one free parameter to calculate dynamically the radius of the groups."
"another ncd architecture peculiarity is the fact that there are two kinds of messages: distribution messages and sending data messages. both are transmitted intermittently during all simulation time. it is believed that such aspect contributes to collision increment and, as a consequence, message loss increment, decreasing chances of message delivery of nodes far, number of hops, from sink node."
"on the other hand, the ncd and sl/ncd architectures present smaller absolute values of sent messages. this is because in such architectures the sender node may not share keys with the receiver node. in this case, the message is not sent that results in reduction of effectively transmitted messages compared to other architectures. as aforementioned, the sender (node son) must share a key with receiver (node father) so that the message can be sent hop by hop safely in such architectures."
"it is important to observe that sensorlock is implemented over other architectures; the number of sent messages is very similar to the same scenario without its use (tinysec, spins, and ncd). therefore, our mechanism does not introduce overhead in sent messages as well as significant variations in message delivery ratio. the delivery ratio is given by the ratio of the number of received messages and the number of transmitted messages. figure 11 illustrates the message delivery ratio for all studied architectures."
"simulation results reveal that sensorlock is lightweight introducing low end to end communication delay and an estimated power consumption quite similar to existing approaches. we believe that sensorlock can be produced with low cost. public components may be used to reach a feasible cost, as tinyrng. besides, sensorlock blocks operate with symmetric encryption, which is fast and implies in low computational cost-and, consequently, less power consumption-compared to asymmetric encryption. the mechanism assures the protection of data stored in sensors, using a tamper-protected secret stored in a tamperresponsive device. finally, the mechanism was applied to three key distribution architectures with different characteristics."
"the other relevant issue in sensor networks is power consumption. thus, measurements are done in order to estimate power consumption by nodes in several studied architectures. figure 12 shows an average power consumption estimative for all network."
"the main contribution of this work is to extend the application and supply a large-scale evaluation of sensorlock to three distinct key distribution architectures: tinysec [cit], spins [cit], and network coding distribution (ncd) [cit] . in this sense, we analyze sensorlock through simulations using the tinyos platform considering large-scale networks with 49, 100, 144, 225, and 400 nodes. for experimentation, we use the collection tree protocol (ctp) as routing protocol and the advanced encryption standard (aes) algorithm. simulation results reveal that sensorlock scheme introduces an average end to end communication delay overhead varying from 0.5% to 4.4%. moreover, the results indicate a quite similar estimated power consumption compared to existing approaches. hence, this work demonstrates the feasibility of implementing such scheme in different key distribution architectures and large-scale sensor networks."
"the hierarchical algorithms are divided into divisive and agglomerative algorithms. the first one use a top-down strategy to divide the data, and the second does the opposite, taking each point as a group and putting together the similar ones to make a big group; both using a tree known as dendrogram. the partition algorithms are based in the square error and mainly in prototypes; this prototypes serve as a reference on how a pattern should look like to belong to a cluster. in this final classification is where the clustering algorithm used here belongs."
"in figure 4 (b), sensorlock is applied. in the first place, it takes place the network initialization. in mn, all system keys are generated and stored in a encrypted way with the key (1). in sensors, the initialization consists in encrypting predistributed keys to them, as shown in steps (2) and (3). after this procedure, just the encrypted versions of such keys remain in flash memory, and the other version, in plain text, is removed."
"there are different tasks in data mining that can be performed; one of them is clustering, it refers to data processing that does not have any type of information. for this task is implemented algorithms that can group data according to their similitude, either by determining a radius distance or through the density found among the instances [cit] . clustering is a task that requires a way of measuring grouping quality, if it does not have previous information it has to guarantee that the result is optimum. for this, it has been developed different validation techniques that allow to measure from several points of view the quality and structure of the groups obtained as a result of the clustering task [cit] . it is important to analyze the behavior of these techniques in an on-line environment and with large size data sets."
"traditionally, such modules are prepared to operate with asymmetric encryption algorithms, secure storage area, and true random number generator. however, public key cryptography is not suitable to sensor networks due to its processing costs and the need of an infrastructure that is difficult to provide in ad hoc networks, specially wsns. on the other hand, symmetric encryption is computationally cheaper demanding low complexity infrastructure to such networks. thus, sensor networks cms could be simpler and should be customized to operate in this kind of network."
"data mining and its tools are in charge of processing data for obtaining knowledge that provides a guideline for decision making. these tools are algorithms capable of finding data patterns, either through a training process by which it gets learn to recognize common features among data as from a training sample (offline learning) or by a continuous processing, which data are unknown a priori (online learning) [cit] ."
"then it is essentially the same max-weight solution of (11) . further, note that the csma based sampling phase converges slowly (exponential time, in general), whereas solving mov are relatively fast. thus, we can assume that the parameters θ(t) have converged to the optimal, and finally, the claim follows from the steady state distribution in (6), and the well known approximation that"
"the trimming and assembly steps are guided by the integration of widely used quality control programs toward the optimization of the assembly process. moreover, the implementation of hmmer, blast+, and blast2go to the platform for de novo annotation is, to our knowledge, a feature not available in other rna-seq analysis web servers such as gigagalaxy or oqtans. this step is the most computationally demanding among all rna-seq analysis steps (including snps calling and differential expression), and trufa uses highly parallelized steps to obtain annotations in a relatively short time frame. although annotations can be performed in other platforms such as fastannotator, having all these steps from cleaning to annotations and expression quantification in the same pipeline reduces unnecessary transfer of large outputs and provides an advantage to the nonexpert user."
"hence, we demonstrate the system's security increases by the using of sensorlock since the capturing of sensors and mn simultaneously does not increase the information concerning any key used by the system."
"the remainder of this work is organized as follows. section 2 provides a study regarding key storage and distribution in wsns. in section 3, we present a brief description of sensorlock mechanism and propose its application to three relevant key distribution architectures for wsns. section 4 presents large-scale simulation results and a discussion about them. finally, in section 5, the conclusions and future work are presented."
"even though our implementation does not fully model the original scheme in which the information is protected within a cm area, we claim that such hardware implementation would make the encryption and decryption of keys even faster."
"the parameter used for the clustering algorithm, ș, influences the calculation of the threshold to determine when there are new groups, therefore it is important to choose a value that is as optimal as possible to get a quality clustering, for which ș was assigned values of 0.1, 0.2, 0.3, 0.5, 0.7 y 0.9. for cluster validation of artificial dataset d31 was implemented the counting-pairs through contingency matrix to calculate the index obtained from it, which are: ri, ari, jaccard index, fowlkes & millows index and the f-measure. likewise shows the results obtained from index c-h and d-h."
"in that work, its architecture is proposed and a first study case is presented where sensorlock is applied. besides, the evaluation was carried out so as to (i) show mathematically that the whole system and its keys are protected even if sensors are tampered with and (ii) determine the mechanism processing overhead using small-scale experiments. however, that work does not demonstrate the feasibility of using sensorlock in different key distribution architectures and large-scale wsns."
"applied to tinysec. to solve the stored keys exposed problem observed in tinysec architecture and increase its robustness, we propose the application of sensorlock in its context. tinysec provides security through shared master keys, called tinysec keys, among all network nodes: conf is used for confidentiality and the other, mac, is used to supply authentication and integrity of messages. therefore, it is important to highlight that the capture of just one node by an attacker may lead to the impersonation of legitimate sensors and the access to network communications."
"the generic design of cm ensures freedom to choose the symmetric algorithm, as rc5 [cit] and aes [cit], for example, used by cryptographic functions running within cm. additionally, we present a mathematical analysis to demonstrate sensorlock's security against capturing and tampering attacks."
"to start with, we present the average end to end delay for a 49-node network in figure 5 . this average is obtained from 100 simulation runs. it is observed as a very low time difference among studied systems. the end to end delay difference obtained in sl/tinysec approach over tinysec is 2.057 ms. on the other hand, the results indicate an end to end delay difference from sl/spins approach over spins of 8.877 ms. also, the end to end delay difference observed in sl/ncd over ncd is 13.580 ms. hence, it is fair to state that sensorlock introduces an average overhead of 8.171 ms (1,3%) and, on the other hand, it offers the benefit of ensuring confidentiality, authenticity, and integrity in network communications."
"it is important to realize that even though the main motivation to use sensorlock lies on the protection of keys stored in sensor's memory, its design allows encrypting all data collected by sensor."
"in figure 2 (b), sensorlock is applied to tinysec. initially, the keys are generated and predistributed among the sink and sensors. in the sensors, and sink, such keys are initialized, as shown in steps (1), (2), and (3). that is, mac and conf are encrypted with the key from each sensor. afterwards, the plain text versions of the keys are removed from the sensor memory, remaining only their encrypted version."
"(4) where, n ij corresponds to the number of instances in common between the clusters obtained with the algorithm and the original partitions. the following index can be obtained:"
"initialization: the above simplex algorithm solves the problem rel by moving along adjacent vertices of the feasible region in a cost reducing direction. in order to understand its behavior, we first need to identify a vertex. according to the equality constraints in rel, a vertex (or a basic solution in the simplex algorithm terminology) (x b, γ) can be uniquely determined by a basis matrix as follows:"
"for this study case, we highlight that the access to the stored keys within sensors, sink included, simultaneously does not aggregate information to system keys discovery. that is, as each sensor encrypts keys with its own secret, the key discovery likelihood remains 1/2 corresponding to bruteforce attack."
"cleaned reads, after passing an optional second quality control with fastqc to verify the overall efficiency of the first cleaning step, are ready for assembly. trufa implements the software trinity, 34 which is an extensively used de novo assembler and has been shown to perform better than other single k-mer assemblers. 35 after the assembly, an in-house script provides basic statistics describing transcripts lengths distribution, total bases incorporated in the assembly, n50, and gc content. in addition, to evaluate the completeness of the assembly, a blast+ 36 similarity search is performed against the uniprotkb/swiss-prot database, and a trinity script evaluates whether those assembled transcripts with hits are full-length or nearly full-length. the cegma software can also provide a measure of the completeness of the assembly by comparing the transcripts to a set of 248 core eukaryotic genes, which are conserved in highly divergent eukaryotic taxa. 37 both the number of recovered genes from the total of 248 and their completeness have been used for de novo assembly quality assessments. 38, 39 the newly assembled transcripts can be used as query for similarity searches with blat 40 or blast+ against the ncbi nr and uniref90 databases. in parallel, hmmer 41 searches can be performed applying hidden markov models (hmm) against the pfam-a database. both analysis can be run as well with user-specified databases or models respectively. further annotation and assignation of gene ontology (go) terms can be obtained with blast2go 28 for the transcripts with blast hits against the nr database."
"tinyos is a component-based operating system specifically developed for wsns. tinyos is written in nesc (network embedded systems c) language and it is free and open source. such language is an event-driven programming language used to create sensor applications to run on tinyos [cit] . in our experiments, version 2.1.1 was used. tinyos is operating system based in components. there are two kinds of components: module that contains the application implemented and configurations, responsible for connecting components."
"simulated scenarios figure 6 : average end to end delay from 100 simulation runs for a 100-node network. figures 7, 8, and 9 present simulation results obtained from 100 simulation runs for 144-node, 225-node, and 400node networks, respectively. for 144, 225, and 400 nodes, sensorlock applications reveal 2.3%, 0.5%, and 4.4%, respectively, of overhead in terms of average end to end delay when compared to architectures that do not use such mechanism (tinysec, spins, and ncd)."
"in this section we propose optimal distributed scheduling, which is based on the simplex algorithm. for a detailed description of the simplex algorithm for general lp problems, please see, for example, [cit] . since the simplex algorithm requires a feasible starting point, we next transform the problem sch into a relaxed problem rel, where a feasible initial vertex is easy to obtain:"
"to complete the rna-seq analysis pipeline available in trufa, we plan to expand the platform by incorporating programs for differential expression analysis and snp calling. other programs, especially for assembly (eg, soapdenovotrans, velvet-oases) and visualization (eg, gbrowse) of the data, are planned to be also included in the future. in addition, integrating go terms for each annotated transcripts would permit the user to browse sequences of interest directly from figure 3 . measures of completeness and read usage for the assemblies produced with trufa. cegma results represent the percentage of completely and partially recovered genes in the assemblies for a subset of 248 highly conserved core eukaryotic genes. overall alignment rate and concordant pairs (providing at least one alignment) were computed with bowtie2. ); # annotation, number of transcripts with at least one annotation after blast2go analysis; # hmmer hits, number of transcripts with at least one hit against the pfam a database (e-value,10 -6 ); user time, time needed to perform the complete pipeline (cleaning, assembly, annotation, and expression quantification)."
"the trufa platform has been designed to be interactive, user-friendly, and to cover a large part of a rna-seq analysis pipeline. users can launch the pipeline from raw or cleaned illumina reads as well as from already assembled transcripts. each of the implemented programs (table 1) can be easily integrated into the analysis and tuned depending on the needs of the user. trufa provides a comprehensive output, including read quality reports, cleaned read files, assembled transcript files, assembly quality statistics, blast, blat, and hmmer search results, read alignment files (bam files), and expression quantification files (including values of read counts, expected counts, and tpm, ie, transcripts per million 30 ). some outputs can be directly visualized from the web server, and all outputs can be downloaded in order to locally perform further analyses such as single nucleotide polymorphisms (snps) calling and differential expression quantification. the platform is mainly written in javascript, python, and bash. the source code is available at github (https://github.com/trufa-rnaseq). the long-term availability of the trufa web server (and further developed versions) is ensured given that it is currently installed in the altamira supercomputer, a facility integrated in the spanish supercomputing network (res). the number of users is currently not limited and accounts are freely provided upon request."
"first, we present sensorlock and its fundamentals in section 3.1. after, in sections 3.2 and 3.3, we propose how to apply sensorlock to two classical wsn key distribution schemes. both are based on key predistribution. in section 3.4, we propose the application of sensorlock to a system based on key distribution that uses multiple keys. thus, we demonstrate that sensorlock is applicable to different key distribution schemes."
"in general, the values oscillate between 453.482 ms and 1152.735 ms for wc, tinysec, sl/tinysec, spins, and sl/ spins architectures. we observe that wc architecture presents a lower average delay in comparison to spins and tinysec architectures. on the other hand, the average delay in spins and sl/spins slightly overcomes wc architecture. differently from wc system, spins accomplishes cryptographic procedures as key derivation and mac computation by a regular sensor node. additionally, sink node derives the key and computes mac again so as to verify message authenticity and integrity. the average value of sl/spins denotes a negligible overhead over spins, as observed. however, the bar that represents the tinysec architecture overcomes the other approaches exposed untill here. it is relevant to remember that in tinysec implementation a mac is computed per hop in end to end path, causing major processing overhead and, consequently, increasing the end to end delay when compared to wc and spins approaches. as indicated through our experiments, sl/tinysec introduces a negligible overhead in terms of end to end delay increasing compared to tinysec."
"the results of a first run performing only a fastqc analysis were used to set the parameters (see supplementary table 1) for the cleaning process, except for the yeast dataset, which was assembled without preprocessing. read cleaning, assembly, mapping, and annotation statistics are shown in tables 2 and 3 . the yeast dataset showed highly similar results to the original analysis, validating the trufa assembly. the difference observed in the number of transcripts is most likely due to the not fully deterministic nature of the trinity algorithm. 47 however, the percentage of reads mapped back to the transcripts was slightly higher in the original study. 47 for the other three datasets, trufa showed globally comparable results. except for the mean transcript length for the c. sinensis assembly, all other statistics for both c. sinensis and d. melanogaster assemblies were higher in the present analyses with respect to the original ones (table 2) . remarkably, the percentage of reads mapping back to the transcripts was significantly higher for the green tea dataset using trufa. this could be due to a more efficient read-cleaning step or to differences between bowtie2 (used in trufa) [cit] mappings. cegma analysis showed that more than 80% (range 85.5%-98.39%) of the core eukaryotic genes are fully recovered and more than 98% (range 98.8%-100%) are partially recovered in all dataset assemblies (fig. 3) . this indicates an overall high completeness of the assemblies performed herein with trufa. in addition to the assembly and the mapping of the reads, trufa was able to annotate de novo 25%-42% of the transcripts using the blat, blast+, and blast2go pipeline with an e-value of,10 -6 (table 3) . hmmer searches identified 17%-60% of the transcripts with at least one hit with an e-value,10 -6 . the expression of each transcript was quantified using rsem and express, although no data were available for comparison with the original studies."
"the proposal is to calculate the result of each validation index and taking into account the suggestions for their analysis, comparing these values with each other and with the number of groups known beforehand to determine if the clustering quality is good as well as knowing the most appropriated value of the clustering algorithm parameter to obtain a grouping with quality close to the original. in the situation of those sets that have their actual grouping. table 1 shows the datasets used for testing. these sets were taken from the repositories of the sipu (speech and image processing unit, http://cs.joensuu.fi/sipu/datasets/)."
"sensorlock is a secure key storage mechanism which can be applied to different key distribution architectures for wsns. essentially, sensorlock mechanism addresses the key exposure problem by defining the following fundamentals:"
"nowadays, the data, collected from different areas, represents one of the biggest and more interesting information sources, so their processing deliver hidden knowledge to professionals as patterns form, this allows to use it in crucial processes like medicine, financial operations and purchase trends analysis [cit] ."
"realizing the limitations of these distributed csma scheduling algorithms, we try to improve the delay performance by avoiding the random-walk based transitions between schedules. instead of allocating positive probabilities to all the schedules, as required by mcmc, we require that the scheduler choose the transmitting schedules among a sparse set of explicitly enumerated \"modes\" (basic schedules), so that the schedule transitions are faster, since there is no need for the scheduler to enter the sub-optimal \"transition modes\". further, since the number of basic schedules is small, optimal scheduling, from among these schedules, can be achieved efficiently in a distributed fashion. this is done by solving a simple max-weight independent set problem, which can be achieved by a certain consensus algorithm."
"wireless sensor networks (wsns) are a particular case of mobile ad hoc networks (manets). sensor networks are composed of tiny nodes that collect data from the environment in which they are deployed, sending data through multiple hops toward sink nodes. these networks are applied with several purposes from weather forecasting to assisted living, contributing to the pervasive computing [cit] ."
"overall dis-sch is very similar to sim, with the following two important changes: 1) the simplex search phase in sim is replaced with the csma phase, and that 2) the centralized moving procedure mov is replaced with a simple max-weight scheduling in (17), which is then followed by parameter updates and average consensus algorithm. it may appear that (17) is similar to the max-weight scheduler in (2), with the difference that the queue lengths q(t) are replaced by the \"virtual queue lengths\" θ(t). however, the max-weight independent set in (17) is chosen from n + 1 independent sets, instead of the family of all independent sets, which has exponential size. thus, (17) can be solved efficiently (in linear time), whereas the max-weight scheduler in (2) is np-complete, in general. in the following we elaborate on the change in step 2, by showing that the scheduling and consensus phases in dis-sch is equivalent to solving the problem mov."
"differently from the ncd's proposal, where the access to the sensors memory allows an attacker to discover all the keys of the system, sensorlock's design assumes, by definition, that an attacker can exploit two attack points: the mobile node and the sensors. we specifically consider the following attacker model composed of threats caused by an attacker:"
"before introducing the distributed csma scheduling algorithm, we need to have an optimization interpretation of the scheduling problem. we can formulate the scheduling problem as the following feasibility problem:"
"the centralized simplex algorithm sim is shown in algorithm 2, which is essentially an application of the general simplex algorithm to the specific scheduling problem rel."
"the organization of the rest of the paper is as follows: in section ii we introduce the system model, and in section iii we propose and analyze the scheduling algorithm. section iv shows the simulation results, and finally section v concludes this paper."
"additionally, some integrated circuit manufacturers offer affordable cryptographic modules (cms), also known as trusted platform module (tpm). table 1 shows cms and their features, applications, and prices. as example of an off-theshelf cm is at97sc3204-u1m90 atmel crypto controller-tpm [cit] . such cm costs usd 5.61 that corresponds to just 4% of the cost of a sensor telosb or micaz. therefore, we argue that it is an acceptable cost to aggregate security in sensor's operation. in general, such cms use active tamper proof mechanisms in order to protect sensitive data."
"therefore, as the majority results of d-b, the last three ș values increase significantly with respect to the first three, these last are discarded, so most of the graphics included here are shown just the most relevant area for the analysis, but at the same time shows the behavior of the clustering through the index value. consequently the optimal values of ș are determined, from which is selected the best based on the c-h result and the number of generated groups with the corresponding ș value."
"proposition 4: if there is no change in the matrix b, the parameters θ updated in (18) will converge to the optimal dual variable for mov, γ will converge to the optimal cost γ ⋆, and the average time fractions (x b, x new ) will converge to the optimal primal variables (x ⋆ b, x ⋆ new ) for mov. proof: due to space limits, we only describe the intuition behind the proof. we first form the lagrangian of mov"
". it is based on the withingroup and between-group ratio to evaluate a particular data partition, that is to say, quantifies the proportion of dispersion [cit] ."
"input. currently, the input data accepted by trufa includes illumina read files and/or reads already assembled into contigs. read files should be in fastq format and can be uploaded as gzip compressed files (reducing uploading times). reads from the ncbi sra databases can be used but should be first formatted into fastq format using, eg, the sra toolkit. 31 already assembled contigs should be uploaded as fasta files. other fasta files and hmm profiles can be uploaded as well for custom blast-like and protein profile-based transcript annotation steps, respectively. thus far, no data size limitation is set."
"in general, it is observed as a low end to end delay difference among systems. the difference of tinysec over sl/ tinysec is 6.163 ms. the difference of sl/spins over spins is 24.531 ms. finally, the difference of ncd over sl/ncd is 4.858 ms. in case of sl/tinysec, it is possible to notice in figure 6 that the bar is slightly overcome by tinysec. however, it is not possible to state that an architecture is more efficient than others, because the error bars are statistically equivalent. besides, we believe that simulation random component contributes to such result. thus, we argue that sensorlock mechanism introduces an average processing overhead of 7.742 ms (1.0%)."
"the first step of a de novo rna-seq analysis consists in assessing data quality and cleaning raw reads. the output of a next-generation sequencing (ngs) reaction contains traces of polymerase chain reaction (pcr) primers and sequencing adapters as well as poor-quality bases/reads. hence, it is advised to perform read trimming, which has been shown to have a positive effect on the rest of the rna-seq analysis, 21 although parameter values for such trimming have to be optimized. 13 once reads have been cleaned, they are assembled into transcripts, which are subsequently categorized into functional classes in order to understand their biological meaning. finally, it is possible to perform expression quantification analyses by estimating the amount of reads sequenced per assembled transcript and taking into account that the number of reads sequenced theoretically correlates with the number of copies of the corresponding mrna in vivo. 6 all the above-mentioned steps in the rna-seq analysis pipeline are included in trufa and correspond to distinct sections in the web-based user interface (see figs. 1 and 2 ). for each step, the options available are those that are either critical to the analysis or, to our knowledge, the most widely used in the literature. there are several online platforms already available to perform different parts of a rna-seq analysis. for example, galaxy (https://usegalaxy.org/) 22 allows analyzing rna-seq data with a reference genome (using tophat 23 and cufflinks 24 ), whereas gigagalaxy (http://galaxy.cbiit.cuhk.edu.hk/) can produce de novo assemblies using soapdenovo. 25 another transcriptome analysis package integrated in galaxy, oqtans, 26 provides numerous features including de novo assembly with trinity, read mapping, and differential expression. nonetheless, to our knowledge, gigagalaxy or oqtans do not perform de novo annotations. conversely, fastannotator 27 is a platform specialized in transcript annotations using blast2go, 28 priam, 29 and domain identification pipelines, but does not perform other steps of the rna-seq analysis."
"percentage variation criterion (vrc). it evaluates the quality of the clusters through the use of variance of the patterns within the cluster and between the clusters [cit] . this distance uses the centers [cit] . its performance is obtained comparing the resulting calculation of the index by varying the algorithm parameters and selecting as best grouping that has the highest value, which can be seen as a peak in the resulting graph. even if the results have a linear trend, up or down then there is no reason to prefer one solution over another."
"we next compare the performance of the distributed simplex scheduling algorithm dis-sch with the distributed csma scheduling algorithm in algorithm 1 using matlab simulation. during the simulation, we assume that the packet arrivals are i.i.d, with 95% of the maximum uniform arrival rate. the average consensus algorithm for the distributed simplex scheduling is implemented as follows: we assume that the communication graph for the average consensus is the same as the interference graph. in each time slot, a random maximal matching is first formed, and each matched pair update their variables by taking the average of their local copies. we also allow a certain time period for the average consensus algorithm to converge, before it is used for distributed scheduling."
"in this section, we present the experiments in order to evaluate sensorlock mechanism. first, we present the methodology used. second, the implemented architectures are described. finally, we reveal our findings."
"in this subsection we propose the distributed scheduling problem, and prove its throughput optimality, by showing that it is a distributed implementation of the centralized simplex scheduling algorithm in the last subsection. we first illustrate the full scheduling algorithm dis-sch in algorithm 3."
"the block 2 performs key decryption. its input is an encrypted key from the sensor's memory and the secret . the outputs are the key ( ) in plain text from sensor . such key may be input of exclusive or block, as aforementioned, or input of the block 3 ."
"the main contribution of this work is to apply sensorlock to three different key distribution architectures, tinysec, spins, and network coding distribution, and evaluate its performance. sensorlock is a secure key storage mechanism which can be applied to different key distribution architectures tailored for wsns. its main goal is to solve the stored keys exposure problem. additionally, sensorlock may protect all data stored in sensor's memory."
"as discussed in section i, however, the above scheduling algorithm suffers from long delay, due to the random walk behavior associated with the time-reversible markov chain. in the following section we try to solve this problem by introducing the simplex scheduling algorithm."
"so, data showed next, takes as a reference the value of ș and 0.2 is one of the values that shows a close group, in number as the original, in the case of d31 data base. another reason for choosing it is that it also has as a result, in the task of classification, a high precision without getting far from the original group. in table 2 shows the values obtained in the counting-pairs, online and offline. figure. in an intuitive way, the ss and dd high values indicates that exists similarities between divisions obtained with clustering algorithm, which is quite similar to the original, although some objects were integrated in different groups which were assigned in the partition, this is because the number of groups determined by the algorithm is lower than the original division, as it is explained ahead."
"in fig. 1a-b, scatter plots for hv and hh backscatter show a similar relationship with biomass but with a larger dynamic range for hh. for remningstorp, hv and hh increases with biomass for the full biomass range. [cit] data behaves differently, i.e. [cit] . the main reason for this difference is likely due to the more pronounced topography in krycklan. in fig. 1c, a scatter plot for vv shows little dependence between biomass and backscatter. vv backscatter for remningstorp is also generally higher than for krycklan for a given biomass, again likely caused by topographic effects. figs. 1a-c show that backscatter from remningstorp tends to decrease with time, i.e. [cit], which likely is caused by soil moisture conditions changing from wet to dry."
"for simplicity, for the remainder of this article, we will consider only one-sided tests of h 0 . a formal sensitivity analysis uses the likelihood for the full table 1, including the missing responses."
"we have several additional comments. first, we have considered the simplest possible case of binary responses. for responses with more than two levels, missingness models rapidly grow quite complex and regions of plausible deviations from mar quickly become high dimensional. in these cases, parsimonious families are required to capture enough of the space of deviations to provide robust results. it is likely that, for other data types, the findings shown here will only become more extreme, but additional work will be required to demonstrate this."
"much of the research concerning erp effectiveness has tried to analyze the relationship between erp implementation activities and organizations. however, implementation policies such as customization policies have not been considered important factors. as for japanese companies, the package software customization issue is important, and implementation policies with software functionality are defined as items for analysis. the research items for policies and functions are defined as follows: customization policy; master data utilization; it-focused implementation; business-focused implementation; functional level of enterprise performance management (epm) and business intelligence (bi) systems; functional level of marketing and sales systems; functional level of finance and accounting systems; functional level of global supply chain management (scm) systems. customization policy was scaled with the degree of standardization focus and narrow down the customized function by degree of criticality of the business processes."
"a collection of texts in machine-readable format is called a corpus. the creation of a corpus is often motivated by interest in linguistic phenomena. therefore, the design and creation of a corpus is always linked to purpose of usage. thousands of corpora have been created and many are freely available. these corpora vary in size, type, format, usage, and purpose of creation. they are usually annotated with morphological, syntactic, semantic, discoursal, or prosodic information. individual texts in a corpus often have meta-data in the header that give information about such attributes as genre of the text, author, source, date and country of publication, etc. [cit] ."
"finally, when it is expected from the design stage that sensitivity analyses will be used with a specified plausible region and expected missingness rates, sample size increases relative to those required in the absence of missingness are required to maintain adequate power. furthermore, we have shown that, unless expected missingness rates are relatively low or plausible regions relatively small, the required sample size adjustments may be quite dramatic or, in the extreme, achieving the desired power may be impossible regardless of sample size."
"the web provides a massive collection of texts which is growing rapidly. constructing corpora by harvesting web pages is usually referred to as web-crawling. the web is an excellent information source with large amounts of data which one can select, organize, and compile into corpora of all types [cit] s, arabic corpora have been constructed. however, not many of them are freely available as open-source. most are for written modern standard arabic (msa). morphosyntactically annotated arabic corpora are very rare and not freely available to researchers. this paper reports on the construction and annotation of a comprehensive 100-million-word corpus of contemporary arabic. the purpose is to provide an open-source corpus of contemporary arabic which is balanced, representative of the language, and comparable to the internationally recognized british national corpus. the text of the corpus was selected from a wide range of genres, domains, and types. it consists of 83% written language and 17% spoken language. the texts of the corpus were collected primarily from text materials available online but also from the transcripts of purpose-made recordings (see section 3). the corpus was automatically annotated both morphologically and syntactically. a sample of one million words was manually and semimanually verified; it was additionally annotated for sentiment and glossed in english. to accomplish this annotation, we used diwan [cit] but had to specifically develop for it morphological and syntactic annotation schemes on the basis of the long-established arabic linguistic tradition (see section 5). we also added new features to the diwan annotation tool to facilitate our semi-manual annotation process (see section 6)."
"for the purpose of this article, we will consider the primary efficacy goal of a randomized controlled trial to be to establish whether there is a causal association between assigned treatment and the response of interest. we do this using a frequentist hypothesis testing approach, although other approaches should lead to similar conclusions. we begin with the framework of the rubin causal model 6 as described by holland 7 ."
"the king saud university corpus of classical arabic (ksucca) consists of around 50 million words [cit] . the corpus includes texts of six genres, namely religion, linguistics, literature, science, sociology, and biography. the artenten corpus used web crawlers to automatically harvest 5.8 billion words from arabic websites [cit] . its purpose was linguistic and lexicographic in nature. it was automatically annotated using madamira and it is available on sketch engine."
"despite our best efforts, missing outcomes are common in randomized controlled clinical trials (rcts). numerous authors have commented on specific problems resulting from missing responses. 1, 2 [cit], the national research council's committee on national statistics issued a panel report titled the prevention and treatment of missing data in clinical trials. 3, 4 among other things, this report stressed the importance of sensitivity analyses as an essential tool to assess the reliability of primary analyses. the report also lists a number of areas in need of further research, two of which are the following: the effect of missing data on the power of clinical trials and how to set useful target rates and acceptable rates of missing data in clinical trials. o'neill and temple 5 also recommend new research into, among other things, \"sample size calculations in the presence of missing data.\""
"in this article, we discuss the impact of missing responses on the causal conclusions that can be drawn from a randomized controlled trial. unless we can be confident that responses are missing at random, a complete case analysis does not have a valid interpretation as an assessment of the causal effect of treatment on the underlying risk of interest. we have shown that causal inference from the complete case analysis breaks down not only as missingness rates increase but, because with larger sample sizes, we can detect smaller differences between groups, as sample sizes increase such analyses are less robust to even small amounts of missingness. because the missing data mechanism will always be uncertain, sensitivity analyses should be used to formally assess robustness to plausible missing data mechanisms. here, we have chosen to consider sensitivity analyses using likelihood-based inference and fixed plausible regions for the missingness mechanism. other approaches are possible, but the general behavior should be independent of the approach. for example, one might consider multiple imputation, where the imputations are generated from a range of mnar models. however, because our likelihood-based models implicitly impute the missing responses, inference using similar models coupled with explicit imputation should yield virtually identical results. furthermore, as noted earlier, by weighting pairs, (r c, r t ), using a probability distribution over a region , one could conduct sensitivity analyses with a more bayesian flavor. one can also, in principle, incorporate baseline covariates into these analysis, but we have not investigated that approach."
"be the set of estimated probabilities for the frames f (out of f p ) belonging to patient clip p. then, the estimated probability of abnormality for p is computed as an average of frame probabilities:"
"two specialized arabic corpora use the quran as a source of their textual content; hence, each consists of the same number of words in the quran, 77430 words. the quranic arabic corpus is morphologically and syntactically annotated. its annotation was done automatically and verified collaboratively by the wider community [cit] . the second corpus is the boundary annotated quran corpus. it is annotated with prosodic information and phrase boundaries . it took advantage of boundary markups that flag starts and stops in the quran [cit] . interest in dialectal arabic corpora has recently surged. an example of such corpora is the curras palestinian arabic corpus, a corpus of more than 56k tokens, which are annotated with morphological and lexical features [cit] this brief review, which is based on a more extensive survey of the literature, points to the absence of resources that make the claim that they represent in a comprehensive manner the arabic of today as written and spoken by contemporary native speakers. there is a great need for a corpus of modern arabic as used by present-day native speakers of the language. the corpus must be truly representative of the language that the current inhabitants of the arab world use, regardless of whether it is of the high or low variety. it must also be balanced in its representation of the written and spoken language, and of the various discourse genres. it must truly depict the language of the curricula and academia."
"the dominant tree species for both sites are norway spruce (picea abies), scots pine (pinus sylvestris) and birch (betula spp.). the sar acquisitions were complemented by field measurements of forest parameters as well as highdensity helicopter lidar data."
"our ipcl dataset comprises a total of 114 patients (45 normal, 69 abnormal). every patient has a me-nbi video (30fps) recorded following protocol in \"correlating imaged areas with histology\" section. raw videos can present some parts where nbi is active. in this dataset, only me subsequences are considered. all frames are extracted and assigned to the class normal or abnormal depending on the histopathology of the patient. they are quality controlled one-by-one (running twice over all the frames) by a senior clinician with experience in the endoscopic imaging of oesophageal cancer. frames that are highly degraded due to lighting artifacts (e.g. blur, flares and reflections) up to the point where it is not possible (for the senior clinician) to make a visual judgement of whether they are normal or abnormal are marked as uninformative and not used. this curation process results in a dataset of 67742 annotated frames (28,078 normal, 39,662 abnormal) with an average of 593 frames per patient. for each fold, patients (not frames) are randomly split into 80% training, 10% validation (used for hyperparameter tuning), and 10% testing (used for evaluation). the statistics of each individual fold are presented in the supplementary material."
"the corpus is designed to have detailed metadata about each article. this is valuable knowledge that can be used to guide the search within the corpus. it can also be used in text classification and text data mining. moreover, the corpus and its metadata constitute an excellent dataset for training machine learning algorithms on such tasks as genre identification. the metadata include infor- figure 1 shows a sample article in xml with the text, title and metadata clearly specified."
"there are features in diwan that indicate the proclitics and enclitics of words. the clitics are assigned slots: prc3, prc2, prc1, and prc0 for proclitics, and enc0; enc1, and enc2 for enclitics. a lower index indicates closer proximity to the stem. additionally, there are features that mark the part of speech (pos), functional number and gender of nouns, and aspect of verbs. functional number and gender refer to the function of a word, rather than its form. for example qadp 'leaders' is functionally masculine and plural, even though it ends in, which is the marker of feminine singular nouns."
"consistent with previous recommendations, we recommend that 1. missingness rates should be kept as low as possible, even 10%, may be too high, especially in large trials; 2. robust sensitivity analyses are essential for assessing the potential impact of missingness; 3. if robust sensitivity analyses are to be performed and trials required to stand up to scrutiny, much larger sample sizes will be required. the usual adjustments are likely far to small; 4. deliberately introducing missingness through \"per-protocol\" or \"adherers-only\" analyses not only does not properly address the concerns raised by incomplete adherence but may easily render analyses completely uninterpretable."
"these results suggest that the rejection probabilities could be controlled by recalibrating the complete data critical value. however, this approach requires that we know the underlying values of t and c . a more principled alternative, requiring only specification of , is the formal sensitivity analysis discussed in the next section."
"twenty million words of the category of world affairs were selected from newspapers published in 20 arab countries where around one million words were collected for each country from one or two newspapers published in that country. more than seven million words were collected from online sources to fill the subcategory of commerce and finance. these articles belong to a variety of topics within the commerce and finance genre. they include accounting; taxes; investment; finance; financial legal issues; inventory; currency, etc. the subcategory of imaginative language consists of 16 million words. the texts were collected from written sources that include; stories; novels; poetry; plays; translations of international stories and novels. the subcategory of leisure consists of 12 million words which include articles on topics such as animals; cars; technology; health; women; tourism; cooking recipes; how to; arabian cities; jordanian stories and traditions; and fitness. the subcategory of arts was collected from web sources and comprises around seven million words. the texts of this category contain articles on arts; digital photography; film and video production; printing; area planning and landscaping; sculpture; ceramics and metals; computer graphic arts; entertainment and performance; cinema and theater; photography; music; architecture; fine arts; decorative arts; international arts; arabic calligraphy, etc. around seven million words were collected from books and web resources for the category of applied sciences. the topics included in this category are medicine; engineering; information technology; energy, etc. finally, the natural and pure sciences subcorpus consists of around four million words that come from mathematics, physics, chemistry, biology, etc."
"in fig. 1d, a scatter plot for vv/hh ratio is shown. contrary to hv, hh and vv, no clear difference between remningstorp and krycklan is observed and no systematic temporal change is observed showing that the vv/hh ratio is less affected by soil moisture and topography."
"with these recommendations in mind, we note that clinical trials in humans are not conducted primarily for the benefit of the investigators or study sponsors, but rather for the benefit of the broader scientific community and, most important, the target population of potential recipients of the interventions under study. as such, design and analysis decisions should be made so that the trial results are convincing, not just to sponsors or investigators, but to an informed, reasonable skeptic from this community-someone who, without convincing evidence, will not accept the conclusion. for example, sponsors should invest adequate resources into minimizing the extent of missing data and not simply accept it as inevitable. modest improvements in missingness rates may dramatically improve the credibility of findings and the return on investment might be the difference between a failed inconclusive trial and a convincing result. conversely, the broader community needs to establish criteria by which results can be considered robust, providing sponsors and investigators a starting point. however, as noted, these criteria cannot be based on prior data because, by definition, the required data does not, and never will, exist."
"although the accuracy of resnet-18-cam is comparable to the baseline network (resnet-18), resnet-18-cam conveniently computes a heatmap per class as part of the network processing. however, the explainability in the context of our classification problem remains very challenging due to the low resolution of the heatmaps produced."
"tends to be more liberal and more prone to change, the written variety more coded and more conservative. arabic has three major varieties, two written and one spoken: classical arabic, the language of scholarship until the end of the eighteenth century; modern standard arabic, the language of ed- ucation and formal written communication from the arab renaissance in the nineteenth century onward; and the dialects, the colloquial regional varieties that are spoken in everyday communication."
"in this chapter, the authors describe the result of an analysis of the erp implementation satisfaction structure. satisfaction items consisted of operational satisfaction and standardization policy. since the authors have mentioned that japanese firms want to improve their operational efficiency, it would be useful to determine the structure of the satisfaction structure, including operational satisfaction and implementation activities. figure 2 is the conceptual model for analysis. since \"erp can be used to help firms create value [cit],\" companies want to achieve value [cit] . it is easy to understand that overall satisfaction will increase if operational satisfaction meets companies' expectations. the research items for operational satisfaction are defined as follows: understanding business performance rapidly at company level; prompt implementation of accounting settlements; managing the enterprise information system at a global level; cost reduction; agile it implementation forcing bpr and corresponding to international financial reporting standards (ifrs)."
"example plausible regions shown on log r c and log r t scales where the outcomes are \"dead\" (failure) and \"alive\" (success). the lightly shaded ellipse shows the \"skeptical region\" whose boundary lies between the points (1/5, 1/5) and (5, 5) . the darker shaded ellipse shows the \"optimistic region\" whose boundary lies between the points (1/2, 1/2) and (2, 2) . the point (1, 1) corresponds to missing at random"
"we have shown that for fixed levels of statistical significance and fixed plausible regions, trials with larger sample sizes are less robust than trials with small sample sizes. in small trials, large observed differences are required to achieve high levels of statistical significance and these large differences are less likely to be a result of missingness. for large trials, high levels of statistical significance can be attained with relatively small observed differences; however, these small differences are more easily contaminated by data that are missing not at random."
"where e (r c,r t ) is expectation when (r c, r t ) is true. for sample size n * and a specified alternative hypothesis and missingness probabilities, we have that the inflation factor is approximately"
"as for the erp issues, there is some work related to the advanced technologies such as mobile solution in erp [cit] . these solutions are related to defining new business processes rather than business process re-engineering. the authors will focus on bpr in this paper."
"for a small and representative sample of 158 frames (ipcl types a, b1, b2, b3), we ask 12 senior clinicians to label them as normal/abnormal and report the inter-rater agreement as krippendorff's α coefficient [cit], achieving 76.6%. we also 1 https://github.com/luiscarlosgph/ipcl. draw a comparison between raters and our gold standard histopathology results, achieving an average accuracy across raters of 94.7%."
"note further that the pair (r c, r t ) that yields the minimum in equation (7) is a random variable and computing the exact distribution of the min (r c,r t ) z(r c, r t ) is analytically intractable, most easily done by computationally intensive simulation. thus, we will use the following approximation that can be shown by simulation to be quite accurate. by exchanging the order of expectation and minimization in (7), define  (r c, r t ) by"
"morphological annotation of the whole corpus was automatically performed using madamira [cit] . we isolated a one-million word snapshot of the corpus for manual verification. twenty-five b.a. students of arabic at the university of jordan carried out the manual verification and two professors of linguistics supervised their work and vetted their annotation. the annotators used diwan [cit] to review and verify madamira's analysis. the morphological annotation required (1) development of a new tag-set with detailed morphological description. fourteen new noun-tags were added to madamira. these new tags fall into three groups: i) derived nouns: active participle, passive participle, exaggeration, qualificative adjective, noun of time/place, noun of instrument, and elative noun; ii) underived nouns: concrete noun and abstract noun; and iii) gerunds: original gerund, gerund with initial miim, gerund of instance, gerund of state, and gerund of profession. (2) providing the roots of the nouns and verbs, since such a root conveys the core lexical meaning of a word. it normally consists of three consonants, and less frequently of two or four consonants. the majority of arabic words (nouns and verbs) are derived from triliteral roots, uncommonly from biliteral or quadriliteral roots. for instance, the consonantal root d.r.s has the basic lexical meaning of studying, from which these words are derived:"
"building a balanced and representative corpus remains an ideal goal for corpus creators. a balanced corpus includes a wide range of texts from the different genres and domains that the corpus claims to depict. sometimes, this type of corpus is referred to as a reference, general, or core corpus. similarly, a corpus is claimed to be representative if it contains the major linguistic variation in the concerned language. although it is not an easy task to achieve balanceness and representiveness in a corpus, it can be done with a level of approximation and scalability [cit] ."
"explaining network predictions is of particular interest to draw a comparison between image features that clinicians employ in their clinical practice and those that might exist but be unknown to them. conversely, adding attention to those image features that are known to be relevant but are not used by the network could potentially improve its performance. in the context of escn detection, this leads to investigate whether the network is actually looking at deep submucosal vessels and ipcl patterns to predict abnormality. the answer to this question typically comes in the form of a heatmap, with those parts relevant to the classification being highlighted."
"as for the erp issues, there is some work related to the advanced technologies such as mobile solution in erp [cit] . these solutions are related to defining new business processes rather than business process re-engineering. the authors will focus on bpr in this paper."
"the reasons why many companies in japan are vigorously accepting erp packaged software are: firstly, the project organization structure and human resources for erp are not mature, and secondly, the effects are not easy to grasp [cit] . however, there are some successful cases of erp implementation in japan. table 2 shows one of the effective cases of erp implementation, which realized a reduction of 120 million yen in running costs per year [cit] . it might be useful for japanese firms to not only consider how to adjust their management style and implementation, but also to consider how to manage and develop their global business [cit] . the authors developed a model to find out how to achieve effectiveness of erp implementation in japan, considering implementation policy and operational effectiveness. details of the results are given in the next chapter."
"the first two algorithms are based on using hv backscatter only and have been derived from a number of data sets representing tropical, temperate and boreal forests [cit] . the first model has only one regression coefficient, whereas the second model has two coefficients."
"darosn 'lesson', mudar∼is 'teacher ', diraasap 'sutdying', madorasap 'school', daaris 'student'. in all these derived words, the consonants d-r-s constitute their root [cit] . (3) providing the morphological pattern of each noun and verb. this pattern constitutes a canonical template that consists of a series of discontinuous consonants including those of the root, a series of discontinuous vowels, and a templatic pattern. it carries a schematic meaning and grammatical information together including the word's part of speech. for instance, the morphological pattern c1vvc2vc3 together with the vowel melody -a -i -represents the active participle of form i verbs [cit], 2001 ratcliffe, robert, 1998; [cit], 2005 ."
"the primary product of biomass will be 100-m gridded maps of above-ground dry biomass covering the main forest biomes. development of algorithms for estimation of forest biomass from p-band sar is one of the most critical elements being addressed in phase a. the two main biomes considered are tropical and boreal forests. the algorithm development is being supported by airborne sar data collections, i.e. biosar-1 [cit], biosar-2 [cit], and biosar-3 [cit] for tropical forests. e-sar from dlr was used for biosar, whereas the sethi sar system from onera was used for tropisar. l-band data were also collected but not used in the present study."
"as indicated by (6), prior to generating a class prediction, a cam at resolution t is produced. this heatmap contains both positive and negative contributions from the input image towards class c. however, for the sake of heatmap clarity, we consider only the positive contributions towards each class when generating our cams. that is, we want to see what part of the image contributes to normality/abnormality, as opposed to what part of the image does not contribute to normality/abnormality. thus, our cams are generated as followŝ y (c)"
"computer-aided endoscopic detection and diagnosis could offer an adjunct in the endoscopic assessment of escn lesions; there has been a high level of interest in recent years in developing clinically interpretable models. the use of cnns has shown potential across several medical specialties. in gastroenterology, considerable efforts have been devoted to the detection of malignant colorectal polyps [cit] and upper gastrointestinal cancer [cit] . however, its utility in endoscopic diagnosis of early oesophageal neoplasia remains in its infancy [cit] ."
"the texts of the written subcorpus were primarily selected from sources available online. to get around copyrights, we followed eckart's example by 'scrambling' the texts such that the original structure of a document would be destroyed. \"this inhibits the reconstruction of the original documents. with respect to german copyright legislation this approach is considered safe\" [cit] . we assume this is satisfactory to copyright laws in most countries around the world."
"in our opinion, our findings are sobering. while the problem of missing data has historically been a major concern, the consequences may be more severe than previously understood. missingness rates well in excess of the maximum of 15% that we have considered in this article are commonplace. it is likely that, were robust sensitivity analyses were performed, the findings of a great many trials would be called into question. clearly, more research into the treatment, and maybe more important, the prevention of missing data is required."
"languages often have a low variety that is used in everyday communication and a high variety that is used in formal settings. the spoken language table 3 : annotated sentences of jcca corpus. in this table, the abbreviation bw represents buckwalter transliteration, gloss the english meaning, lex the lexical entry, pfx the prefix, stm the stem, sfx the suffix, gen the gender, root the consonantal roots,sntmnt the sentiment designation, and ptrn the morphological pattern."
"to ensure that this corpus of modern arabic is representative, balanced, comprehensive, and for general purposes, we followed the model of the british national corpus (bnc) 5 . that is why this corpus contains slightly more than 100-million words of the same text types, domains, and genres. the corpus contains 87% of texts from written sources and 13% of transcribed spoken language. the written part includes texts from applied sciences, arts, belief and thought, commerce and finance, imaginative works, leisure, natural and pure sciences, social sciences, and world affairs. the spoken subcorpus includes transcripts of spontaneous conversations (4.2%) and context-governed spoken language (6.2%) from the categories of educational/informative, business, public/institutional, and leisure. tables 1 and 2 show the text categories of the corpus of the written and spoken subcorpora respectively."
"1. if observations are deliberately omitted from the analysis for subjects who are nonadherent to their assigned treatment thereby inducing missingness, except under the strong untestable (and likely implausible) assumption that subjects are nonadherent at random, the resulting analysis does not assess the causal effect of treatment. specifically, a so-called per-protocol analysis cannot correct for nonadherence to assigned treatment. 2. as we show numerically in section 4, if h 0 is true (assigned treatment has no causal effect on failure), buth 0 is false, as the sample size increases, power for rejectingh 0 increases. specifically, the adverse impact of missing data on statistical inference increases with sample size. consequently, in large trials, even low rates of missingness can have a dramatic adverse effect on the credibility of the conclusions."
"much of the research concerning erp effectiveness has tried to analyze the relationship between erp implementation activities and organizations. however, implementation policies such as customization policies have not been considered important factors. as for japanese companies, the package software customization issue is important, and implementation policies with software functionality are defined as items for analysis. the research items for policies and functions are defined as follows: customization policy; master data utilization; it-focused implementation; business-focused implementation; functional level of enterprise performance management (epm) and business intelligence (bi) systems; functional level of marketing and sales systems; functional level of finance and accounting systems; functional level of global supply chain management (scm) systems. customization policy was scaled with the degree of standardization focus and narrow down the customized function by degree of criticality of the business processes."
"is randomly assigned and, therefore, the mean of y(u) over subjects assigned treatment is an unbiased estimate of p . hence, if we have no missing responses, we can conduct valid inference regarding the average causal effect of t relative to c. we make two comments regarding equation (2). (2) is the sole reason for using randomization. assuming that we have complete ascertainment of all responses, no other conditions are necessary to ensure valid causal conclusions. if treatments are not randomly assigned, it is unclear that equation (2) holds. 2. equation (2) does not imply that treatment groups need to be balanced with respect to important baseline factors. in fact, it is clear from this equation that approximate balance is merely a side effect of randomization and not necessary for valid causal inference."
"the fifth algorithm is a modified version of an algorithm developed based on data from alaska [cit] . the latter contains additional terms, i.e. a second order term of the vv/hh ratio as well as first and second order terms of hh backscatter, which were dropped to make a comparison with the sixth algorithm easier. [cit] data. it includes the vv/hh ratio besides hv backscatter, as well as ground slope dependence. the total number of coefficients is six. the algorithm reduces to the fifth algorithm by excluding the three terms with ground slope dependency. as will be seen, the inclusion of vv/hh ratio and ground slope is important when applying the retrieval algorithms across multiple data sets."
". ‡ for the binary case that we consider in this article, this is strictly true. for nonbinary responses, it is possible for equality to hold even if mar fails."
"*note that other summary measures can also be used-odds ratios, risk ratios, etc.-but the discussion is more complicated, and our overall findings will be unchanged. † it is impossible to determine from the data whether (2) holds. we require additional external information, eg, that treatment assignment is randomized, to determine this."
"since erp systems are aimed at realizing effectiveness targeted at a wide range, which means since \"erp can be used to help firms create value [cit],\" companies want to achieve value [cit] . it is easy to understand that overall satisfaction will increase if operational satisfaction meets companies' expectations. the research items for operational satisfaction are defined as follows: understanding business performance rapidly at company level; prompt implementation of accounting settlements; managing the enterprise information system at a global level; cost reduction; agile it implementation forcing bpr and corresponding to international financial reporting standards (ifrs)."
"initially, a macroscopic assessment was made of the suspected lesion in an overview, with the borders of the lesion delineated by the endoscopist. the endoscopist then identified areas within the borders of the lesion on which to undertake magnification endoscopy. the ipcl patterns were interrogated using magnification endoscopy in combination with narrow-band imaging (me-nbi). magnification endoscopy was performed on areas of interest at 80 − 100x magnification. using the jes ipcl classification system, the ipcl patterns were classified by the consensus of three expert endoscopists (ww, hpw, rjh) as type a, b1, b2, b3, in order to give a prediction of the worst-case histology for the whole lesion. the entire lesion was then resected by either endoscopic mucosal resection (emr) or endoscopic submucosal dissection (esd). resected specimens were formalin-fixed and assessed by an expert gastrointestinal histopathologist. as is the gold standard the worst-case histology was reported for the lesion as a whole, based on pathological changes seen within the resected specimen. similarly to abnormal lesion areas, type a recordings (normal, healthy patients) were obtained by visual identification of healthy areas, magnification endoscopy, visual confirmation of normal vasculature and ipcl patterns, and biopsy to confirm the assessment."
"in addition to the network generating cams at every resolution prior to generating the scores as part of the prediction pipeline, the combined loss l proposed allows for the validation of the accuracy at each resolution depth of the network. we refer to the architecture that implements the model in (5) with embedded cams at different resolutions following (6) and loss (9) as resnet-18-cam-ds (see fig. 2 )."
sar data have been calibrated to o and o using projection cosines determined from the imaging geometry and ground slope angles [cit] . the latter were computed by least-squares fitting of a 50 m x 50 m plane centered on each pixel based on a 50-m grid dem. table 2 [cit] .
"we focus on the problem of classifying video frames as normal/abnormal. these frames are extracted from the magnification endoscopy (me) recording of a patient. to the best of our knowledge, we introduce the first ipcl normal/abnormal open dataset 1 containing me video sequences correlated with histopathology. our dataset contains 68k video frames from 114 patients."
"our proposed method resnet-18-cam-ds achieves slightly higher average accuracy (91.7%) across folds than our baseline resnet-18 (84.3%). although the automated classification accuracy (91.7%) is still below the average achieved by the clinicians (94.7%), it performs better than some of them (their ci low value is 83.9%). it is also encouraging to see that accuracy did not decrease at the expense of an improved explainability. more data and further methodological refinements will most likely lead to improved accuracy. qualitative results in fig. 3 seem to indicate that the network is looking at ipcl patterns to assess abnormality, which aligns with the clinical practice. however, we have not observed high activations over the large green submucosal vessels in the heatmaps for the normal class. this suggests that they may not be used by the network as an aid to solving the classification problem. future work could concentrate on adding an attention mechanism to the network in order to consider such vessels as a feature of normal images. open access this article is licensed under a creative commons attribution 4.0 international license, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the creative commons licence, and indicate if changes were made. the images or other third party material in this article are included in the article's creative commons licence, unless indicated otherwise in a credit line to the material. if material is not included in the article's creative commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. to view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/."
"there are many categories of research targeted at erp, such as the critical success factors (csf) of implementing erp, project management, and so on. successful implementation of erp can be considered as a critical solution for improving the effectiveness of it investment for some japanese firms, since it might help to achieve cost-effectiveness, and achieve value from new business processes [cit] . therefore, seeking the critical success factors of erp implementation is quite worthwhile. major csf pointed out from the research results are, bpr [cit], top management support or commitment [cit], and so on."
"we propose a novel convolutional network (cnn) architecture to solve the binary classification task with a particular focus on the explainability of predictions. our proposed method achieved an average accuracy of 91.7%. in addition to a global classification estimation, our novel design produces activation maps and class scores at every resolution of the convolutional pyramid. the network has to explain where it is looking at prior to the generation of a class prediction. looking at the activation maps for the abnormal class, we have observed that the network is looking at ipcl patterns when predicting abnormality. no conclusive evidence has been found that it is paying attention to large deep submucosal vessels to detect normal tissue. we believe that this baseline method may serve as a reference for future benchmarks on both video frame classification and explainability in the context of escn detection."
this dataset will be made publicly available online upon publication and can thus serve as a benchmark for future work on detection of escn based on magnification endoscopy images.
"given (5), let t x,t be the output tensor produced by f θ t . then, similarly to (4), we propose to generate a class score prediction at each resolution t as followŝ"
and therefore the mean of y(u) over subjects assigned with nonmissing data is not an unbiased estimate of e[y (u)] and inference based solely on nonmissing responses does not have a direct causal interpretation. this is the fundamental problem with missing data in randomized trials.
"the historical arabic corpus (hac) has 45 million words that were organized into primary and secondary resources, seven genres, and 100-year eras in the gregorian calendar. its intended purpose is historical semantics and etymological lexicography [cit] ."
"since the corpus constructed here is comprehensive and since it claims to be representative of contemporary arabic, it has to exclude classical arabic, but include modern standard arabic, and the regional dialects. we define contemporary arabic as the language both written and spoken by living native speakers of arabic; therefore, the dialects need to be represented. we are not alone in this view, check out a frequency dictionary of arabic [cit] and the oxford arabic dictionary [cit] . the major spoken varieties are, therefore, represented in the corpus: north africa is represented by the moroccan dialect; the nile region by egyptian; the arabian peninsula by taizi, sanaani, and najdi; greater syria by shami, jordanian, and palestinian. the data in the form of contextualized sentences were collected from (1) personal communication in facebook and whatsapp family groups; (2) jokes, songs, videoclips, movie scripts, and tv interviews in the local dialects; and (3) personal interviews of old speakers, especially those with minimal education. the data were collected by students who came from these regions. like any other language, arabic has differences between the dialects and the standard variety, between the spoken and written varieties. there is variation in the pronunciation of some consonants and vowels (e.g., q, d, z, v, *, a); suppression of word final inflections; fixed wordorder (i.e., subject-verb-object (svo)); contracted forms (e.g., mazal∼i$ for ma zal∼a $ay'n 'nothing remains'); use of high frequency lexical items(e.g., qaeid rather than jalis 'sitting'); use of some lexical items that are archaic in msa (e.g.,"
"since erp systems are aimed at realizing effectiveness targeted at a wide range, which means not only for one division, cooperational effectiveness or satisfaction might be quite important. the research items for cooperational satisfaction are defined as follows: information utilization; inventory reduction by supply chain improvement; and improvement of customer satisfaction."
"a corpus-representative snapshot of one million words are designated as the corpus gold standard. this is a sample of words semi-manually annotated and verified. each word is morphologically decomposed into its prefixes, stem, suffixes, proclitics, and enclitics. then, each morpheme is annotated with a morphological tag or possibly tags. the stem is labeled by one morphological tag, and its root and morphological pattern are specified. other morphological attributes, such as the number and gender of a noun, are indicated as well. the tag set we used here was informed by traditional arabic grammar (see section 6). moreover, each word was annotated for sentiment designation (i.e., positive, negative, or neutral sentiment). the annotation process was done using a specialized program, diwan [cit] . twenty annotators with expertise in arabic linguistics were trained on the tag set and on the annotation tool and they were supervised by three linguists who ensured the accuracy of annotation and verification."
"arabic corpora vary in size, type, purpose, design, text type, etc. [cit] surveyed freely available arabic corpora and classified 66 of them into six main categories, namely (i) raw text corpora, (ii) annotated corpora, (iii) lexicons, (iv) speech corpora, (v) handwriting recognition corpora and (vi) miscellaneous corpora."
"this paper outlined the methodology for the design, construction, and annotation of the jordan comprehensive contemporary arabic corpus (jcca). the corpus is balanced, comprehensive, and representative of contemporary arabic as written and spoken in arab countries today. it consists of 100 million words that reflect current usage of the language. the corpus consists of 87% written and 13% spoken language. the text of the corpus was selected such that it would be representative of a wide range of geographical regions, genres, subject matters, modes, and media. di- wan was upgraded and used to annotate and manually verify the annotation of a one-million-word snapshot of the corpus, making it a gold standard of superior quality that can serve as a resource against which automatic annotation may be compared. jcca construction made these additional contributions: (i) development of a new and elaborate tag-set that is based on the morphology of traditional arabic grammar; (ii) addition of the roots and morphological patterns of nouns and verbs; (iii) coverage of the major spoken varieties of arabic: north africa; the nile; the arabian peninsula; and levant. future work is to make this corpus a monitor corpus where new texts are added proportionally every year. this will facilitate tracking language change and will render the corpus more amiable to lexicography."
"biomass is one of three candidate missions for esa's earth explorer 7 [cit] . it consists of a p-band synthetic aperture radar (sar) that will systematically acquire polarimetric and interferometric data over all major forested areas of the earth [cit] . forest biomass is one of the most important parameters in the earth's carbon cycle and, hence, for projections of future climate change. the present climate change is mainly due to fossil fuel burning and accumulation of co2 in the atmosphere. however, it is also known that deforestation contributes with 10% to 30% of the total anthropogenic co2 source. the large uncertainty is due to shortcomings in current observational techniques which is a strong motivation for implementing the biomass mission."
"the third and fourth algorithms have been developed and evaluated based on data from yellowstone [cit] . the third algorithm includes both linear and quadratic terms in o, i.e. in total seven coefficients. the fourth algorithm includes ground slope corrections, splits biomass into crown (w c ) and stem (w s ) biomass, and has fourteen coefficients."
"intrapapillary capillary loops (ipcl) are a clinical microvascular feature recognised as an endoscopic marker for escn [cit] . they have been classified by the japanese endoscopic society (jes) in a simplified system aimed at improving the easy recognition of escn by endoscopists [cit] . the type of ipcls present also facilitates the accurate prediction of the lesion histology; type a ipcls (see fig. 1 ) correlate with normal tissue. type b1, b2, b3 ipcls (see fig. 1 ) demonstrate progressive morphologic abnormalities and correlate with the invasion of early neoplasia in the muscularis mucosa and submucosal tissue. [cit] demonstrate that the jes classification offers high diagnostic accuracy compared to other classifications for the prediction of dysplastic tissue-with the overall accuracy for histology prediction 90.5% across type b1-3. a computer-assisted detection (cade) system that can classify still images or video frames as normal or abnormal with high diagnostic accuracy could provide a useful adjunct to both expert and inexpert endoscopists."
"a location-based service (lbs) query targets a location/region and has a speci c interest; the lbs server responds with the most upto-date relevant information, e.g., the latest menu of a restaurant, movies at a cinema, or remaining parking slots at a shopping mall. during this process, users' current or future whereabouts and interests are disclosed to the lbs server through their queries. access to all submi ed information is deemed necessary to best serve users, and the lbs server is entrusted with rich information. however, many studies reveal service providers can be honest-but-curious, aggressively collecting information to pro le users, identifying home or working places or inferring interests towards commercial purposes."
"considering the characteristics of the service types mentioned above, the service registry stores specific information for each type of service. independent of the service type a link to the interface description is available. for provisioned services the endpoint is already known and therefore stored. for not provisioned services the service registry contains a link to the corresponding service package in the service package repository and if the service is dedicated or shared. in addition, the number of currently running instances is also stored."
"we distinguish between two kinds of services. the first kind of service is provided by a service provider, who also manages the service. the scientist can use this service, but he has no knowledge about the implementation and the underlying middleware and infrastructure. we call this kind of service a provisioned service. for the second kind of service all artifacts needed to provision the service and the underlying middleware and infrastructure can be accessed by the scientist. this kind of service we call a not provisioned service. provisioned services follow the always on semantic, they are running and ready to use. not provisioned services have to be explicitly provisioned before they can be used. in our previous work we worked out an extended classification for service binding strategies [cit] . typical strategies for static and dynamic service binding rely on provisioned services. to enable the on-demand provisioning and de-provisioning of services including their underlying middleware and infrastructure we defined a new service binding strategy which we call dynamic binding with software stack provisioning. this service binding strategy is based on not provisioned services."
"service calls are initiated by the workflow engine (fig. 5, step1) . a service call contains the actual payload as well as different metadata (step 2). the functional requirements (fr) describe the required interface, the nonfunctional requirements (nfr) describe requirements concerning the quality of a service, for example cost or security. whereas the functional and nonfunctional requirements correspond to traditional soc concepts, the provisioning requirements (pr) are specific for our on-demand provisioning approach. they describe requirements specific for the provisioning process, for example allowed cloud providers or the region where resources have to be provisioned."
"the architecture components used during the modeling phase are the modeling and monitoring tool [cit] and the bootware running locally on the user's machine, and the service package repository, the service registry and the user registry running in a cloud environment. these components are active during all life cycle phases. in the modeling phase, the modeling and monitoring tool is used to model workflows. the service registry and the service package repository provide all services that can be used by the workflows. the bootware is utilized by the modeling and monitoring tool to start the next life cycle phase, the middleware runtime phase."
"an alternative collaborative privacy protection approach is to pass/share lbs-obtained information among users, to decrease exposure to the lbs server [cit] . is is orthogonal to location obfuscation, both in terms of location and query privacy; the two could complement each other. e sharing approach requires nodes to cache information received from the lbs and pass it to neighbors when requested."
"adversary model: we assume lbs servers are honest-butcurious: they follow the protocols, responding faithfully to queries, but they may trace the nodes (linking their queries) or even deanonymize them and infer sensitive data (e.g., home and work sites)."
"en, the session key is encrypted with the public key in the serving node's pseudonym. e whole message (including the encrypted session key, and the encrypted pseudonym and query) is signed by the querying node. e serving node decrypts the session key (with its private key) to decrypt the pseudonym and the query. once the message is veri ed, the serving node generates the response and encrypts it with the same session key. e encrypted response is then signed by the serving node and sent back to the querier."
"if no serving node is discovered until timeout (i.e., no beacon from serving nodes in the same region is received within t wait ), the node queries the lbs server directly. e lbs server is also issued an ltc and uses it to authenticate itself to the nodes. e lbs server and the nodes are registered with the same pki architecture, so that the lbs server can authenticate the nodes. moreover, we assume the responses to the lbs-submi ed queries are signed by the lbs server. however, the responses form the serving nodes need not carry the lbs signatures."
"a not provisioned service can furthermore be a dedicated or a shared service. a dedicated service can or may only process one service call at the same time. if several service calls are sent to the same dedicated service, for every service call we have to provision a new instance of the service including its underlying middleware and infrastructure. an example for such a dedicated service can be a simulation service needing a lot of compute resources without having any elasticity capabilities. a shared service can in contrast process several service calls at the same time."
"ese problems are exactly addressed in this paper: we propose a security architecture for decentralized/collaborative privacy protection for lbss. we propose new components that are orthogonal to the lbs servers. we leverage pseudonymous authentication to provide privacy-enhancing message authentication and integrity for communication with other users (nodes) and with the infrastructure entities. we leverage proactive caching of poi data by a small fraction of users that serve others, sharing the cached poi data. is ensures peer responses can provide the same quality as lbs responses do. e burden is balanced among users through a periodical randomized role assignment by the infrastructure. while users bene t from the information sharing system, we minimize their exposure to the lbs server and curious peers, and thwart security threats (from active malicious peers). our evaluation shows both e ective exposure reduction and highly successful valid poi provision in a realistic intelligent transportation se ing even with a huge (in practice) fraction (e.g., 20 %) of malicious nodes. e rest of the paper is organized as follows: we explain the system and adversarial models, and requirements in sec. 2. en, we present the proposed scheme in sec. 3. sec. 4 begins with a qualitative analysis on security and privacy, followed by a quantitative, simulation-based evaluation before we conclude (sec. 5)."
"the main building blocks in service oriented architectures (soa) are services, loosely coupled components providing functionality over a unified interface. to realize more complex functionalities, different services can be reused and combined. this concept is called service composition. usually, service compositions are modeled (modeling time), then deployed on a suitable execution environment (deployment time) and finally executed by this environment (run time). in service oriented computing (soc) [cit] the concept of publish-find-bind aims to decouple service providers and service consumers. a service provider registers all services it offers in a service registry (publish). a service consumer, which needs a specific service, then searches the service registry for a suitable service (find). afterwards, he can start sending requests to the selected service (bind). the step of choosing the right service is called service selection. the search for a suitable service and the binding to this service can be performed at modeling time, at deployment time, or at run time of a service composition. when the service selection is done at modeling time or at deployment time, this binding strategy is called static binding. nevertheless, if the service selection is done at run time, this binding strategy is called dynamic binding. using the binding strategy dynamic binding, the needed services are at modeling time described with functional and non-functional requirements. at run time, for each service call the corresponding requirements are forwarded to a middleware component, the so-called enterprise service bus (esb) [cit] . this component carries out the service selection, based on the passed functional and non-functional requirements, and finally binds the service call to a suitable service."
"in traditional soc usually the service selection step returns exactly one service offer i.e. exactly one service endpoint [cit] . in our approach this only applies when the set of suitable service offers contains at least one provisioned service. however if the set of suitable service offers contains only not provisioned services, the service selection step returns service package references for all suitable service offers. the esb then forwards these service package references to the provisioning manager. the provisioning manager encapsulates all provisioning related functionality and therefore has all information available to decide which service package he is actually able to provision. in addition we also delegate the evaluation of the provisioning requirements to the provisioning manager. as a result our architecture shows a clear separation of traditional service selection and routing capabilities (esb and service registry) on the one hand and provisioning related components (provisioning manager and service package repository) on the other hand."
"eries sent to the lbs servers expose user locations and interests, and can be used to infer sensitive data. we maintain the honestbut-curious assumption for any trusted third party, including the ones we introduce in our scheme (sec. 3)."
"in our previous work we have developed the architecture of a system supporting our approach for on-demand provisioning and de-provisioning of workflow execution middleware and services needed for the execution of (simulation) workflows [cit] . we present the architecture in fig. 1, where we distinguish between components run locally on the user's machine and the components run on a cloud. we also show which components are used during which life cycle phases of the involved applications (i.e. simulation workflows, execution middleware, services). the life cycle phases we consider here are the modeling of simulation workflows, the middleware runtime/execution phase and the service runtime phase."
"the bootware is the basic piece of software needed to provision the workflow execution middleware (in a cloud environment). instead of provisioning the whole workflow execution middleware in one step, we follow a two-step bootstrapping process. in the first step (fig. 1, step 1 ) the bootware provisions the provisioning engine and its underlying middleware and infrastructure to a cloud environment. this reduces the complexity of the bootware component by limiting its capabilities to the provisioning of one special componentthe provisioning engine. the provisioning engine itself is a generic component able to provision any kind of service and is a rather complex system [cit] . in the second step, the bootware calls the provisioning engine that provisions the workflow execution middleware in a cloud environment."
"accountability: upon detection of misbehavior reported to the ra, the ra can reveal actual, long-term real identity of the misbehaving node through pseudonym resolution [cit] and possibly evict the node from the system."
"our decentralized privacy protection scheme design is driven by privacy, resilience and e ciency considerations. our approach signi cantly extends p2p lbs privacy schemes [cit] with the following main ideas: (1) each node is equipped with short-term anonymous credentials, used to authenticate all node-to-node (and node-to-lbs) interactions. (2) peer-provided poi can be drawn from a larger volume of poi data, proactively distributed by the lbs server to a small fraction of randomly chosen nodes, termed serving nodes. (3) nodes submit queries to serving nodes, which periodically announce their presence and available pois (i.e., poi for their regions). table 1 summarizes the used notation."
"after the workflow engine has finished the execution of all running workflows, the bootware initializes the deprovisioning of the workflow execution middleware. in the first step the provisioning engine de-provisions all other middleware components. in the next step the bootware deprovisions the provisioning engine."
in the following we will show how the service and service package selection process is realized in our architecture. in fig. 5 we present the part of our architecture realizing the service binding. the workflow engine is responsible for the execution of the workflows. the enterprise service bus coordinates the processing of the service calls. the service registry is a global directory containing information about all services. it offers information about functional and nonfunctional properties of a service. for each not provisioned service the service package repository contains the corresponding service package together with provisioning metadata. the provisioning manager is capable to provision service packages using a suitable provisioning engine.
"nodes can be also honest-but-curious or outright compromised. e p2p interactions allow nodes in the system to aggressively collect all the peer queries and responses. such transcripts from multiple honest-but-curious nodes could be merged and used by the adversary. furthermore, nodes can deviate from the collaborative protocol functionality and policies, and a ack the system, notably their peer nodes. ey can forge or tamper with responses, and masquerade other nodes. is could, in turn, a ect quality of service and force honest nodes to expose themselves to the lbs server(s)."
"indicates the exposure degree under a single identity id i . to derive the exposure degree of a node, the exposure degrees under each id i are weighted by a time parameter"
"the architecture of the provisioning manager is modular, as the provisioning manager can be extended by plugins. a plugin connects a provisioning engine to the provisioning manager. the plugin declares to the provisioning manager which service package format and which target cloud environment is supported by the corresponding provisioning engine."
"anonymity/pseudonymity and unlinkability -node actual (longterm) identities should not be linked to their messages. anonymity should be conditional, allowing the system to identify a misbehaving node and evict it. ideally, it should be impossible for any observer to link any two messages (e.g., queries) by the same node. however, for practical reasons, messages can be linkable at most over a protocol selectable period, τ ."
"system model: fig. 1 shows the considered system architecture. mobile devices (termed nodes in the rest of the paper), e.g., smartphones and vehicular on-board units (obus), are equipped with various communication interfaces, e.g., wi-fi and cellular. ey can access lbss, submi ing queries regarding their current locations/regions. ey also communicate in a p2p manner over a wireless ad hoc (e.g., ieee 802.11p) or cellular (e.g., lte direct) network. nodes can share poi information and choose to query the lbs server only when no response is received from their peers. nodes are registered with an identity and credential management system (i.e., a public-key infrastructure (pki)). certi cation authorities (cas) (see sec. 3 for more details) issue credentials to the registered nodes and the service providers (sps) (i.e., lbs servers here), so that sps and nodes can interact securely."
"is role is explicitly visible at each issued pseudonym through an a ribute set accordingly. to balance the workload among the nodes, a randomly assigned serving node would only have to serve at most for a protocol selectable period t ser e, which can coincide with the pseudonym request interval γ (the period covered by the lifetimes of the set of obtained pseudonyms)."
"there exist several approaches for the on-demand provisioning of services [cit] . however, all these approaches do not tackle possible implications on service selection imposed by the concept of on-demand provisioning."
"node exposure: we re ne the measurement of node exposure to curious lbs servers and curious nodes, de ning the exposure degree of a node as:"
"sybil-resilience: e ltca and the pca issue tickets and pseudonyms with non-overlapping lifetimes [cit], ensuring a node is equipped with only one valid pseudonym at any point in time."
"nonetheless, opening up the system functionality is a doubleedged sword: it reduces user exposure to the curious provider (lbs or anonymizer) but it also exposes her to possibly faulty or misbehaving peers. in fact, p2p systems [cit] show that insecure decentralized schemes face serious problems. for example, sensitive information could be exposed or malicious nodes could share bogus data. signed lbs server responses can be self-veri able when passed to peers [cit] . however, queries and cached information from di erent users could be diverse, making it necessary to share multiple complete lbs responses (each with a signature a ached), even though only a subset of each lbs response might be needed by the querying peer. moreover, it is hard to decide whether the peer responses cover the same information that could be received directly from the lbs server, thus, guaranteeing the quality of service. last but not least, peer queries, openly submi ed to a node's neighborhood, could expose users to other nodes and passive eavesdroppers."
for dedicated services the esb then calls the provisioning engine to de-provision the service. for shared services the esb first checks if the service is still processing other service calls. only if the service is idle it will be de-provisioned.
"requirements: we require that peer-provided information be veri able and the nodes be accountable for their messages. e nodes should be able to e ciently obtain poi data from their peers with the same quality as that obtained directly from the lbs server. while the nodes bene t from p2p poi sharing, node exposure to neighboring assisting peers (and de nitely, to the lbs server) should be minimized. towards this, the following security and privacy requirements need to be met:"
"in our previous work we introduced and realized the concept of on-demand provisioning and de-provisioning of workflow execution middleware and services for simulation workflows. besides its advantages like optimized resource allocation and a user friendly way of managing complex systems, this approach has some implications on the traditional service selection known from soc. in this paper we developed a solution approach for this challenge. we introduced an extended architecture for on-demand provisioning supporting service selection as well as service package selection. as part of this architecture we also provided a metamodel for the service registry as foundation for the selection process. finally we gave a detailed description and discussion of the service and service package selection process. as a result our extended architecture is able to transparently handle service selection for provisioned as well as not provisioned services."
the rest of the paper is organized as follows. in section ii we present our previous architecture for the on-demand provisioning and de-provisioning of workflow execution middleware and services for simulation workflows. in section iii we extend this architecture to enable service selection also for not provisioned services. in section iv we first introduce our metamodel for the service registry and then we define our service and service package selection process. some aspects of this new selection process are discussed in detail in section v. an overview about related work is given in section vi and we finish the paper with a summary and outlook in section vii.
"our approach extends the recent p2p lbs privacy protection approach, addressing a number of practical open issues. more importantly, it ensures resilience to misbehaving peers and low exposure to curious peers and lbs servers even if they collude with the curious identity management facility. we show that the exposure to curious nodes is low even if 20 % of nodes are compromised and collude, while the same ratio of active malicious nodes could only a ect 1 % of the peer-responded lbs queries."
"when the service selection component determines the set of compliant service offers, i.e. all service offers fulfilling the functional requirements (i.e. the interface) and the nonfunctional requirements (i.e. qos) of the service call, this result set can consist of provisioned services as well as not provisioned services. in this case our system per default always returns the endpoint of a provisioned service. however, this behavior is configurable. the user can define his preferred service type: he can choose between provisioned service and not provisioned service. an advantage of provisioned services is that they are always available, whereas not provisioned services have to be initially provisioned before they can process a service call. on the other hand, it is very possible that the scientists prefer not provisioned services. they may rather trust a not provisioned service available as service package than a provisioned service that simply provides an endpoint. a service package contains details about the implementation and the structure of the service, an often very important aspect in the context of traceability, reproducibility and linked experiments [cit] ."
"one important aspect in the just described process is that for not provisioned services the service selection component returns all compliant service offers. afterwards the provisioning manager can select a service offer containing a service package which is suitable for provisioning. if the service selection component would return always only one service offer, like known from traditional soc, this can lead to situations where a service request cannot be processed although a suitable service offer exists. considering the example of fig. 6 discussed before, if the service selection component would for example return only service offer s1 (in step 2), then the service package selection (step 4) will result in an empty set and the service request cannot be processed. the service package of service offer s1 does not fulfill the provisioning requirements and the service packages of service offers s4 and s8 have been already discarded in the previous service selection step (step 2)."
"we mandate proactive caching of poi information by a small fraction of selected nodes, termed serving nodes. a serving node is responsible for requesting poi data from the lbs server, store this information locally and serve neighboring nodes' (peer) queries. e role (i.e., serving or non-serving node) assignment is done by the pca at the time of pseudonym acquisition: the pca assigns a pseudonym requester as a serving node with probability pr ser e ."
"by means of the service selection decision tree depicted in fig. 7 it is described, which results the service selection component returns, depending on the user's configuration. in the left subtree the configuration option \"prefer provisioned service\" is shown. this configuration corresponds to the default configuration of our system. when the set of compliant service offers contains at least one provisioned service, the service selection component returns an endpoint of exactly one provisioned service. afterwards, the esb forwards the initial service call to this endpoint (see also fig. 5, step 7a) . however, if the set of service offers does not contain a provisioned service but at least one running shared service, the service selection component returns an endpoint of exactly one running shared service. then the esb forwards the service call to this selected endpoint (fig. 5, step 7a) . if the set of service offers does contain neither a provisioned service nor a running shared service, the service selection component returns for each service offer a service package reference. afterwards the esb forwards these service package references and the provisioning requirements to the provisioning manager (fig. 5, step 7b )."
"eries are encrypted and bound to a speci c serving node, thus they cannot be meaningfully replayed and any other serving node would fast reject them. more important, the query identi er can trivially allow the (given) same serving node to reject, and not serve replayed a acks, at the expense of modest local memory for overheard recent queries."
"using the binding strategy \"dynamic binding with software stack provisioning\" changes the service selection process. on the one hand the selection process has to consider both, provisioned services as well as not provisioned services. on the other hand for not provisioned services an additional service package selection is needed. before introducing the service and service package selection process, we will present the metamodel for the service registry used in our approach. the esb interacts with the service registry based on this metamodel."
"con dentiality: communication among the nodes and the infrastructure entities (ltca, pca and lbs) is kept con dential by using public key cryptography and (symmetric) session keys."
"a basic assumption in soc is that services are always on and available. in traditional soa scenarios from the business domain, services are typically used continuously. from a service provider point of view, it is therefore absolutely appropriate to make the service constantly available. however, there exist domains where services are used rarely and not regularly, e.g. simulation workflows. in such cases, it is not suitable for a service provider to make his services constantly available as this means wasting resources. in our work, we consider the escience domain, especially scientific experiments modeled as simulation workflows [cit] . simulation workflows are typically executed irregular and rarely. when a simulation workflow is executed, the used services however require significant resources. for the time the simulation workflow is not running, the services are not needed, but the corresponding allocated resources are furthermore blocked. altogether, this leads to a bad utilization of services and the corresponding resources."
"the metamodel of the service registry is depicted in fig. 4 as entity relationship diagram (in chen notation). in this section we will only present the parts of the metamodel that are relevant in context of this paper. the service registry provides a set of service configurations. a service configuration describes the combination of a service interface, i.e. the functional properties of a service, and a set of nonfunctional properties, the so-called quality of services (qos) . qos are modeled as simple name-value pairs. although we consider for a service configuration there can exist multiple service offers. a service offer is offered by exactly one service provider. a service configuration therefore can be offered by multiple service providers and a service provider can offer multiple service configurations. we distinguish two types of service offers. a provisioned service represents a traditional service as known from soc, i.e. it is always on and available. for such a service an endpoint is provided in the service registry. a provisioned service is a functionality provided at an endpoint with certain nonfunctional properties, everything else is transparent. in contrast, a not provisioned service at first has to be explicitly provisioned before it can process service calls."
the service package repository contains service packages. services that are available in the service package repository are always registered in the service registry. the service registry is a central data store containing information about all available services and enabling their discovery. the information provided includes functional and nonfunctional properties of a service and a reference to the corresponding service package in the service package repository. the information in the service registry is not only about services stored in the service package repository but also about services that are already available (and provided by a third party).
"the workflow middleware runtime phase is supported by the components of the simulation workflow execution middleware provisioned at the end of the modeling phase. in our example these are the simtech swfms [cit], the esb and the provisioning engine. these components interact with the components used already in the modeling phase. the esb receives service calls from the workflow engine to invoke services on behalf of workflow activities. for provisioned services the esb selects the endpoint of a service from the service registry and forwards the service call. for not provisioned services the esb interacts with the service registry and the provisioning engine. first it gets all information needed to provision the service, like a reference to the service package repository or if the service is dedicated or shared, from the service registry. then the esb calls the provisioning engine to provision the service (which starts the runtime phase of the service life cycle). the provisioning engine gets all needed artifacts like the implementation of the service from the service repository and uses these artifacts to provision the service including its underlying middleware and infrastructure (step 3 in fig. 1 ). after the service provisioning is done, the esb forwards the service call to the newly provisioned service."
"to solve the problem of service selection for on-demand provisioning and de-provisioning of services, in this paper we contribute (1) an extension of our existing architecture to enable a sophisticated selection of not provisioned services with different types of service packages, (2) a metamodel for a service registry supporting the discussed scenario, and (3) the definition of a service and service package selection process for the on-demand provisioning and de-provisioning of services."
"lbs privacy has been widely studied. location k-anonymity [cit] ensures that at least k − 1 other users are involved in an obfuscated region, r, used as the querier's location."
"we assume the whole area (e.g., a city under the same ltca) is divided into (equally sized) regions. a serving node is responsible for requesting all poi data for the region it is located in. whenever it enters a new region, it has to request the poi data for that new region. moreover, we assume poi information is refreshed every t po i period. erefore, whenever a poi refresh point is reached, serving nodes have to request the updated poi data."
"con dentiality and reduced exposure -poi data should be accessible only by legitimate participants. sensitive information (e.g., node queries) should be accessible only by authorized entities, and the amount of information revealed to peers and the lbs server should be minimal."
"in the right subtree of the service selection decision tree depicted in fig. 7 is shown, which results are returned by the service selection component for the configuration option \"prefer not provisioned service\". if the set of compliant service offers contains at least one running shared service, the service selection component returns the endpoint of exactly one running shared service. then the esb forwards the initial service call to this endpoint (see also fig. 5, step 7a) . however, if the set of compliant service offers contains at least one not provisioned service but no running shared service, the service selection component returns a service package reference for afterwards the esb forwards these service package references together with the provisioning requirements to the provisioning manager (fig. 5, step 7b) . if the set of compliant service offers does not contain any not provisioned service, the service selection component returns an endpoint of exactly one provisioned service. the esb then forwards the initial service call to this endpoint (fig. 5, step 7a )."
"during the service runtime phase the services are executing the functionality they are implementing. at the beginning of this phase all components of our architecture shown in fig. 1 are provisioned and running. as soon as a service has finished its computation the result is returned to the esb, which in turn sends it back to the workflow engine."
"consequently for a not provisioned service instead of an endpoint a service package reference is provided which points to a service package repository. in the service package repository all data and metadata needed to provision a not provisioned service is stored. our metamodel allows that a service configuration can be provided by multiple not provisioned services, i.e. for one service configuration there can exist multiple service packages. as one service package can be provisioned multiple times, for a not provisioned service there can exist multiple not provisioned service instances which are also managed in the service registry. in addition for not provisioned services we distinguish between shared services and dedicated services and consequently between shared service instances and dedicated service instances. for shared service instances the service registry stores the number of currently processed service calls. this information is necessary to determine if a shared service instance is still needed or if it can be safely de-provisioned. each instance of a not provisioned service is assigned to a user. this user initiated the provisioning of the service and only this user is allowed to call this service. every instance of a not provisioned service is again available over an endpoint."
"authentication and integrity -node messages should allow their receivers to corroborate the legitimacy of their senders and verify they were not modi ed or replayed. accountability -message senders cannot deny having sent a message (non-repudiation). any node can be tied to its actions, and, if need arises, be held accountable and possibly evicted."
"eries by one node, v, to a certain serving node, s, can be linked while v uses the same pseudonym. moreover, v could choose among several s i nodes even within a given region for successive queries. mobility of all nodes (serving or not), short-lived pseudonyms, and rotating assignment of serving nodes minimize exposure to any curious serving node acting alone; also thanks to encrypted queries that prevent eavesdropping of other p2p exchanges. however, colluding serving nodes could merge the queries they received, a empting to link them the same way the curious lbs server would do. moreover, collusion with the cas could further linking queries by linking pseudonyms of the same node."
"decentralized k-anonymity [cit] eliminate the need of an anonymizer and protects user privacy in a collaborative manner: e.g., an obfuscated area is formed by k users within each other's communication range [cit] . however, if such k users are too close, e.g., in a church, a shopping mall or a cinema, such symbolic \"addresses\" can still be disclosed. us, it is hard to de ne how large k should be to ensure an appropriate level of protection."
"so that a service can be automatically provisioned in our architecture, all artifacts needed for the provisioning have to be available as a service package. such an artifact is for example the topology of the service, i.e. a description of which applications, middleware and infrastructure are required to operate a service and how these are connected. other artifacts are the implementations of each component respectively the references to these implementations. a service package can be available in different established formats such as chef 1, puppet 2 or tosca [cit] . for each of these formats there exist provisioning engines which can handle the corresponding format. for example, a service package in chef format can be read and automatically provisioned by a chef provisioning engine. however, to provision a service package in tosca format, a special provisioning engine for tosca is needed. in our previous realization of the architecture described in section ii we use tosca for the description of the service packages, opentosca [cit] as provisioning engine and amazon aws 3 as cloud environment. we could have also realized our architecture using a different service package format and a different provisioning engine, for example puppet and a puppet provisioning engine. our architecture is designed to be generic, there is no dictation about a concrete provisioning format and a concrete provisioning engine. only upon realization the decision for a concrete format has to be made."
"when receiving a service call, the esb executes a service discovery (step 3). in this step all service configurations which are compliant with the functional requirements of the service call are determined by the service registry (step 4). afterwards a service selection is carried out (step 5). in this step all service offers fulfilling also the nonfunctional requirements are determined (step 6). if the result set contains at least one provisioned service, the service selection component returns exactly one endpoint (of a provisioned service). in this case the esb forwards the service call to the selected endpoint (step 7a). if the result set contains no provisioned services but at least one running shared service, the service selection component returns exactly one endpoint (of a running shared service) and the esb forwards the service call to this endpoint (step 7a). if the result set however contains no provisioned services and no running shared services, the service selection component returns a service package reference for each service offer in the result set. the esb forwards these service package references together with the provisioning requirements to the provisioning manager (step 7b). afterwards the provisioning manager dissolves the references by querying the service package repository for the metadata of the referenced service packages (step 8, 9). then the provisioning manager carries out a service package selection. he selects exactly one service package which on the one hand fulfills the provisioning requirements of the service request and which on the other hand can be processed by one of the available provisioning engines (step 10). after that the selected service package is provisioned by a suitable provisioning engine (step 11). in the last step the esb forwards the service request to the service provisioned before (step 12)."
"a serving node broadcasts beacons every t beacon . a beacon, signed and with the pseudonym a ached, includes the identi er of the region of the serving node and the expiration time of the corresponding poi data. an interested node listens to beacons in the network for t wait at maximum. if a beacon from a serving node in the same region is received, it sends a p2p query to the serving node: the node generates a session key and it encrypts the pseudonym and the query with the session key."
"in our previous work we addressed this deficit using cloud technologies. we developed an approach and architecture for the on-demand provisioning and de-provisioning of workflow execution middleware and services for simulation workflows [cit] . in this approach, services including their underlying middleware and infrastructure are provisioned not until they are needed, and de-provisioned when they are not needed anymore. as simulation workflows are typically long running, the additional provisioning time is not expected to affect the execution time noticeably. in such an on-demand provisioning scenario, the traditional service selection process from soc can no longer be applied. there are two main reasons for this. first, in our approach we use two fundamentally different service types, traditional services with always on semantic (provisioned services) and services which are provisioned on demand (not provisioned services). second, not provisioned services are provided as service packages. a service package contains all artifacts needed to provision a service automatically. therefore, the service selection process has to be extended with an additional service package selection step determining a suitable service package."
"is indicates a modest pr ser e (e.g., 0.06 or 0.08) is enough to hide a signi cant amount of queries from the lbs server."
"authentication and integrity: entity and message authentication, and message integrity are achieved with message signature verication. message (e.g., beacon) timestamps prevent replays over signi cant periods."
"besides the ongoing realization of the whole system we plan to extensively evaluate our system using a real world use case from the domain of simulation workflows. although we already achieved some promising results regarding some single aspects of our approach [cit], an evaluation of an overall end to end scenario is still missing."
"in fig. 6 we show an example that further illustrates the service selection for not provisioned services. as a starting point a set of all provided service offers is depicted on the left. this set contains 12 service offers with different functional and nonfunctional properties. the functional properties are depicted by the shape of the service offer icon, the nonfunctional properties are depicted by the hatching of the service offer icon. after a service call arrives, in the first step a service discovery is performed i.e. all service offers providing a certain interface are selected. in our example the wanted interface is symbolized by a square. the service offers s1, s2, s4, s5, s6, and s8 provide the wanted interface and are therefore candidates for the service request. in the second step a service selection is performed on this candidate set i.e. all service offers fulfilling the nonfunctional requirements are selected. in our example the nonfunctional requirements are symbolized by a diagonal hatching. the service offers s1, s4, and s8 fulfill these nonfunctional requirements and are therefore still candidates for the service request. in the third step the service package discovery is performed, i.e. for each service offer the corresponding service package is determined. the following service package selection consists of two steps. first the provisioning manager matches the provisioning requirements with the provisioning capabilities of the service packages (step 4). in our example the provisioning requirements states that the service has to be provisioned in the amazon cloud infrastructure (aws). these requirements are fulfilled by service offer s4 and s8. second the provisioning manager matches the formats of the service packages with capabilities of the available provisioning engine plugins. in our example the service package of s4 has the format \"chef\" and the service package of s8 has the format \"tosca\". the provisioning manager has two plugins available both supporting chef but for different cloud infrastructures. as a result the service package of s4 is selected (step 5). in our example this service offer fulfills all requirements ˗ functional, nonfunctional and provisioning requirements ˗ and it can be provisioned by one of the available plugins of the provisioning manager."
"it was necessary to synthesize the waveform in such a way so as to allow the gf to resolve the multiple frequency components in greater detail rather than to treat the waveform itself as a wide-band signal. consequently, the synthesis of the frequency coefficient from each channel only had a single pulse rather than a complex pulse. therefore, upon the correlation of the waveform from each channel at a specific time, no multiple similarities were found, and this helped to eliminate the phenomenon of multiple local maxima prior to the detection stage. however, this strategy will not work if the mf method, which leads to poor output performance, is used."
"given a value of x true there is a certain distribution on x with some parameters h. note that we have not used a subscript for g and c to emphasize that the same distribution is used for all genes. we give three examples here, but our framework works with general distri- the primary input for uncurl is the highly sampled single cell sequenced data and optionally any prior information that is known about the specific dataset. uncurl then converts the observed sampled data to an estimated version of the true data using a novel sampling model aware matrix factorization. this can then be used in downstream unsupervised learning tasks. (b) the convex mixture of cell states assume that all cell states lie in the convex hull spanned by a few extreme cell types many common sampling distributions, including the ones mentioned above. we thus utilize an alternating maximization algorithm to estimate these model parameters as follows: [cit] utilized a different definition of smoothness and derived a generalized algorithm (nolips) that is capable of achieving a sublinear rate of convergence for a class of non-lipschitz continuous functions including the poisson log-likelihood. here we use a custom alternating minimization approach using the nolips algorithm to optimize the poisson log-likelihood with additional modifications to allow for faster computation for sparse matrices and ability to parallelize the computation (see supplementary methods)."
"the detection process was continued with the use of the correlation technique. each channel, i from the gfri and gfti, was correlated separately, resulting in an array of correlations, ai as given in (4). this correlation output was needed for the next phase of the detection processing. the correlations of each channel were then summed up to give a single correlation output, which provided information regarding the status of the detection, denoted as d, as described in (5) . the output of d that fulfilled the threshold limit was known as the true pair correlation and led to accurate detection."
"the trend for alternative signal processing approaches for future man-made sensor systems has placed demands on researchers to explore new study perspectives. thus, the knowledge that has been discussed so far could be beneficial for new developments in radar and sonar system applications. next, the advantage of having multiple frequency components is that it allows more details regarding targeted features to be exploited from the waveform rather than by performing a demanding signal compression method in order to achieve such a high resolution. finally, the preliminary studies that have been discussed in this paper could result in the emergence of a new bio-inspired concept which can be useful for developments in radar and sonar system applications in the near future."
"additionally, uncurl allows for the integration of prior information which leads to large improvements in accuracy. to enable semi-supervised learning, uncurl's toolbox contains a method (qualitative normalization, or qualnorm) for standardizing any prior biological information including bulk rna-seq data, microarray data or even information about individual marker gene expression to a form compatible with scrna-seq data. we demonstrate that initialization using prior knowledge in an appropriately standardized manner dramatically improves performance compared to unsupervised learning."
"both the txl and ec were passed through the gf in order to obtain the synthesized coefficients known as the frequency response coefficients, which were denoted as gfri and gfti.these comprised all the parameters, as given in (2). the gfri and gftiwere arranged in rows (channels), i of the bandpass filters (bpf), respectively. next, these coefficients became the input prior to the testing phase, which involved the correlation of the individual channels from gfri and gfti."
"to demonstrate the utility of prior biological knowledge, we look at a dataset with available qualitative information in the form of bulk rna-seq data obtained from different experimental conditions, and consider the effect of semi-supervision on visualization with tsne. [cit], comprised of five non-pyramidal cell types: oligodendrocytes, astrocytes, interneurons, microglia and endothelial cells. an upper bound on the performance with semi-supervision information is obtained when we feed the aggregate means of the true clusters (the means of all cells with each of the ground-truth labels) as the initialization. we compare this with semi-supervision using the output of qualnorm, bulk means and unsupervised preprocessing. in order to test the validity of our qualnorm framework, we compare the performance with aggregate-mean initialization to the performance obtained when we process these aggregate means through the qualnorm framework. in figure 4b, the four initialization methods are compared, and it is seen that while semi-supervision with aggregate means and qualnorm means lead to a clear separation of cell types and an improved over unsupervised preprocessing, initialization with bulk means leads to worse visualization for this dataset. a similar experiment ( supplementary fig. s1 ) with the 10â pooled dataset also leads to a qualitative improvement in tsne/pca based visualizations and improvement in clustering purity."
"since it is not possible to obtain ground truth ordering of cells, we first study the effect of uncurl preprocessing on simulated datasets. we created two separate synthetic lineages (a linear and a branched) using a method described in the supplementary methods, and sampled using a poisson distribution. for both datasets, we preprocessed the datasets using uncurl and magic and tested three commonly used lineage inference tools (monocle, monocle2 and slicer) on both preprocessed and unprocessed datasets ( supplementary figs s2-s4 ). for the linear dataset, a good measure of lineage accuracy is the rank correlation of the true ordering of the cells with the pseudotime (an arbitrary metric that is commonly used to measure progress along a trajectory). we find that uncurl improved the rank correlation of all three methods compared to both the unprocessed data and magic, whereas preprocessing using magic lead to worse performance for both monocle2 and slicer. for the branched data, there is not a similarly simple way to quantify lineage estimates so we looked at the branch purity (a measure of how well cells have been ordered into the right branches). even in this case we found that the branch purity obtained in monocle2 after preprocessing with uncurl was higher than both unprocessed and preprocessing using magic."
"this paper investigated and analysed the detection performance of human echo locator clicks by gf processing and mf processing. the detection technique proposed in this paper involved training and testing phases prior to the final decision stage using both transmitted and echo signals. in the training phase, these signals had to be synthesized by passing themthrough a gf in order to obtain a set of frequency component coefficients, which were arranged into individual channels."
"most commonly used computational tools for cell type identification [cit], lineage estimation [cit] and similar applications rely on an initial dimensionality reduction step using methods such as pca [cit], lle [cit] or tsne [cit] . however, these algorithms assume that the underlying data is drawn from a gaussian or a t-distribution, an assumption that does not always hold for scrna-seq data [cit] . the discrepancy between the assumed and actual distribution fundamentally limits the accuracy of the resulting predictions. in addition to such general purpose preprocessing methods, several tools were developed to specifically deal with scrna-seq data [cit] . however, these approaches do not scale well with"
"in the following sections we develop an explicit framework to utilize prior biological information to initialize the matrix factorization. this relies on clustering each gene into clusters of high and low expression, which is done using k-means clustering for gaussian and log-normal (after log-transformation) distributions. here we outline a similar procedure for the poisson distribution that allows our approach to be consistent across different distributions along with an approach to initialize the clustering."
"in this manuscript, we introduced a preprocessing framework for scrna-seq data. our framework, uncurl, uses the estimated sampling distribution of scrna-seq data together with a convex mixture model assumption to estimate a true state matrix from observed scrna-seq data. uncurl further includes a computational framework, qualnorm, which can be used to incorporate prior biological knowledge into an improved estimate of the true state matrix."
"thus, this paper studied the detection performance of the human echolocator waveform using bio-inspired processing (proposed method) and a mf. the bio-inspired method incorporated a gf [cit] . for the bio-inspired method, both the transmitted and echo signals were synthesized using gf processing in order to obtain the frequency response components. these frequency component coefficients were aligned into individual rows to construct a set of filterbanks containing synthesized information of the waveform. this concept is believed to imitate the theory of bm in synthesizing frequency components upon the entry of sound signalsinto the human ear. the process continued with the cross-correlation of each row from the set of filterbanks, followed by the obtaining of a single correlation output prior to the detection stage. last, but not least, the information discussed in this paper can be disseminated and be of benefit to other researchers with regard to emerging concepts for new developments in radar and sonar system applications in the near future."
"as seen in figure 2b, distribution selector correctly predicts the dominant distribution for each of the synthetic datasets. on the real datasets, we do not know the underlying distribution, however, we can check which distribution performs best on the downstream task of clustering. we check whether the predicted distribution leads to the highest accuracy. the predicted clusters are identified by assigning each cell to the highest weight class in the cell type fraction matrix, w. the cluster purity is then measured using normalized mutual information (nmi) between the predicted clustering and the true cell types in the data and is seen to be highest for the distributions that are predicted to be dominant distribution according to distribution selector as seen in figure 2c . in general, the poisson distribution is seen to be a better fit for count or umi data, while the log-normal distribution is a better fit for normalized (fpkm, rpkm, tpm) data."
"2.1.1 procedure an implicit assumption shared by many scrna-seq data analysis tools is that any biological sample contains a limited number of cell types and that any individual cell can be considered a mixture of these cells. here, we make this convex mixture model explicit, which leads to a model similar to nmf. nmf is classically used when the entries have gaussian noise [cit] ) and has been found beneficial in analyzing gene expression data gleaned from microarrays [cit] . in scrna-seq, the sequencing process can produce noise following several different distributions such as gaussian, log-normal, poisson and negative binomial, potentially with zero-inflation [cit] . while nmf can be directly applied to the scrna-seq data (shao and hö [cit] ), utilizing the sampling distribution becomes critical especially when the number of reads per cell is small. while the sampling distribution is carefully modeled in differential expression studies [cit], the most commonly used algorithms for visualization, cell-type identification as well as lineage estimation do not account for this model. thus, while factoring the matrix, we need to account for the sampling distribution in order to estimate the true cell-state matrix and mixing coefficients accurately from the observed gene expression matrix."
"in our program we have a set of possible distributions to choose from. in order to select the distribution to use automatically, we implemented a method using fit error ( fig. 2a) . first, we fit the different distributions for each gene using maximum likelihood. then, we compute the distance between the empirical distribution and each of the fitted distributions. this test is similar to the rootmean-square statistic for goodness of fit [cit] . the distribution with the minimum such distance is considered the best fit. finally, we output the best estimation fraction vector which captures the fraction of genes for which each distribution is the best-fit distribution. this is meant to be a guideline for the selection of the sampling distribution during the matrix factorization process."
"by comparing against several benchmarking datasets, we demonstrated that preprocessing using uncurl leads to superior separation of cell types in reduced dimensions as well as higher cluster purity for clustering tasks compared to prior tools. we further showed that semi-supervision using different types of prior information can lead to further improvement in accuracy of the learning tasks. furthermore, we demonstrate that semisupervised preprocessing using uncurl allows the incorporation of prior information in even lineage estimation tasks. uncurl scales to large datasets and typically runs faster than prior methods, particularly on large and sparse datasets. the run time for uncurl scales linearly with the number of cell types in the dataset, but it may be possible to further reduce run-time using a hierarchical strategy. (e) average expression of the top cluster specific genes overlaid on the uncurl processed tsne plot. the expression for each cluster is colored to correspond the coloring used in a. it can be seen that the average expression of the top genes are very cluster specific, indicating that they identified distinct sub-populations uncurl is an efficient preprocessing framework for several unsupervised and semi-supervised learning tasks, but it still has some limitations. tools that already specifically account for the sampling effect of single cell sequencing may not benefit from uncurl preprocessing. another potential shortcoming is the need to know or at least have a good estimate of the true number of cell types. this is a limitation common to many approaches for single cell data analysis, and we are currently working on incorporating biological prior information into selecting the optimal number of cell types in a dataset. another limitation is that the semi-supervision framework converts all prior information into a binary format, although it can be easily generalized to multi-level quantizations. while this constraint is reasonable for truly cell type specific genes, it still limits the amount of prior information that can be used. for instance, many genes that are not strictly cell type specific might also contain useful information that cannot currently be utilized. future work will thus be aimed at expanding the semi-supervision framework to incorporate more diverse qualitative information."
"our algorithm exploits the low-dimensional nature of the true biological state matrix, i.e. it assumes that each cell is in a convex combination of a few archetypal cell-states. under this assumption, the true state matrix can be expressed as a product of an archetypal main state matrix, m, comprising of gene-expression in the archetypal states, and a matrix of mixing coefficients, w, a cluster-by-cell matrix for which each column sums to 1. we demonstrate that working with the estimated (and factorized) true state matrix considerably improves performance of state-of-the-art methods as compared to directly operating on the sequencing data."
"here, s i is the set of cell indices for which the log-likelihood is highest for the ith mean. the m step creates a new estimate of the means, which are then used to redo the e step. this procedure is repeated till convergence or until a maximum number of iterations."
"a recent analysis has demonstrated that the detection of human echo locator clicks is possible [cit] by adopting bioinspired processing. the proposed detection method, which incorporates a gammatone filter (gf), imitates the properties of the basilar membrane (bm) in the synthesis of sound signals. studies have revealed that the waveforms are individually unique and different compared to bats and dolphins, which also use echolocation on a daily basis. a look at these mammals reveal that their signal properties follow the characteristics of linear frequency modulation.hence, the conventional method of using amatched filter (mf) for detection purposes will work [cit] . therefore, these findings suggest that the strategy for echo location employed by humans is different from that of bats and dolphins."
"clustering and dimensionality reduction are common downstream tasks for scrna-seq data. these are both unsupervised tasks: clustering involves dividing the cells into different cell types based on similarity of gene expression patterns, while dimensionality reduction involves creating a low-dimensional view of the data for visualization or clustering. both tasks are useful in identifying cell subpopulations in an unlabelled dataset."
"this work is supported in part by nsf grant ccf-1651236 and nih grant 5r01hg008164-02 awarded to s.k., and nih award 1-r01 [cit] 29-01a1 to g.s."
"the computational performance of uncurl on various datasets compares favorably to that of other methods as seen in figure 6 . the runtimes of uncurl is usually less than the other comparable methods for clustering tasks on most datasets as seen in figure 6 (a more comprehensive comparison is in supplementary table s3 ). the memory usage was lower than that of comparable methods such as simlr and magic. uncurl's performance is best on sparser datasets, where more entries are zero, since the nolips update function only uses nonzero values of the data matrix. runtime comparisons with magic and zifa are limited because magic's memory usage is quadratic in the number of cells and zifa is slow compared to other algorithms, making it impractical to run on the largest datasets."
"the erb space is responsible for the uniform spacing (logarithm factor) of the ee waveform based on (1). then, the erb filter bank is responsible for constructing the rows (channels), i of the band-pass filters (bpf). each row, gfi is known as a frequency coefficient component, which consists of all the parameters given in (2) . now, the output filter bank from the proposed method contains the synthesized frequency response components extending from 20hz to 20khz (assuming the hearing range of humans). as a result, the gf processing should be able to resolve the frequency component from the sound signals based on the psychoacoustic theory."
"next, in the testing phase, each channel from the transmitted-echo signal pair needed to be correlated prior to the detection stage. to complete the processing, the correlation products from the individual channels were summed up to obtain a single correlation product for decision making. the detection performance using the gf technique demonstrated that an ideal output was achieved by fulfilling the basic requirement for detection. meanwhile, the output from the mf method revealed a poor performance by showing a high cross-correlation level, which can cause the system to deliver a false detection."
"then, to test the effect of having information about only a subset of the genes, we chose different number of top cell type specific genes (measured by one-vs-all differential expression) as initialization points for uncurl and tested their effect on the purity of the clusters. furthermore, we also tested the effectiveness of three different semi-supervision strategies namely, i) true cluster centers (generated by taking aggregate means with the known true labels), ii) bulk means and iii) qualnorm means. it is seen in figure 4d, that while the true cluster centers lead to almost perfect estimation of cluster membership, the qualnorm means lead to better accuracy than using the bulk data for most subset sizes, which we attribute to biases inherent to different sequencing methods. this effect becomes more pronounced as the number of genes being considered increases. an interesting observation here is that information about a few cell type specific genes is enough to very high nmi values, even when the information is qualitative."
"human echolocation is a technique that is usually used by blind people to identify their surroundings by analysing the returning echoes which are actively generated from the person. [cit] . the experiments revealed that blindfolded participants were able to sense the presence of obstacles by the noise created by scuffing their heels on the floor. [cit], an experiment on human echo location was conducted by rice, and it was discovered that hissing sounds and punctuated tongue clicks were the main sources of echo location among participants [cit] . more researches in recent years have explored the cognitive and psychophysical factors involved in human echolocation [cit] . this fascinating phenomenon has drawn the attention of scholars to analyse human echolocator waveform clicks, and it has been discovered that they are individually unique with the existence of two frequency component regions [cit] ."
"a detection mechanism was proposed to mimic the closest possible strategy to employ with the human echo location process based on the author's knowledge of the human hearing theory. during an actual echolocation event, the human echo locator is likely to listen to both the transmitted leaked signal (txl) and theecho signal (ec) for them to be synthesized for further action. therefore, both the txl and ec that had been extracted from the raw data earlier were required as the input for the purpose of detection in this paper. basically, two phases of processing were involved in the proposed processing scheme,namely, i) the training phase and ii) the testing phase. the summary of the proposed detection mechanism is illustrated in fig. 4 . fig. 4 . bio-inspired detection mechanism for human echo locator click signal"
"the runtime of uncurl is oðn p kþ, where n p is the number of nonzero elements in the input matrix x, and k is the number of cell types. this means that uncurl scales linearly in the number of cells in the dataset. to deal with the dependence on k, one possibility is to use uncurl hierarchically: first run it with a small k on the entire dataset, then partition the dataset based on the assigned clusters, and run uncurl on the subsets."
"to verify the accuracy of the distribution selection methodology, we first generated three synthetic datasets using different distributions (poisson, gaussian and log-normal). each gene in the synthetic dataset has a mean that was randomly chosen between 0 and 1, with a constant variance for all genes for the gaussian and log-normally distributed datasets."
"one biologically important downstream task post dimensionality reduction is lineage estimation, which is often used to study the dynamics of various genes during cell differentiation or development. lineage inference aims at identifying smooth continuous manifolds in which cells lie in order to study the gradual change in gene expression during various developmental processes. while there exist several common tools that exist for this purpose [cit], most tools operate directly on the sampled observed data. additionally, most lineage estimation tools do not allow for the incorporation of any prior biological data beyond selecting the subset of genes to use for lineage inference. here we demonstrate that the use of uncurl preprocessing can lead to the estimate of cleaner lineages as well as allow for the incorporation of qualitative prior information into the lineage inference process."
"an important point to note here is that in each of these datasets, the true number of cell types (k) was known and used during state estimation. however, this may not always be the case, so robustness to varying values of k is desirable. we tested the robustness of clustering nmi to k on various datasets ( supplementary fig. s7 ) and found that uncurl is quite robust to overestimation of k (to within a factor of 2), but that underestimation can lead to worse results."
"the human hearing theory has demonstrated that the unique construction of the bm is meant to serve its purpose in synthesizing the frequency components in detail and acts as a band-pass filter. as a result, only a distinct region along the bm corresponds to specific frequency components. these feedback responses along the bm create a non-linear characteristic upon the synthesis of sound signals. the equivalent rectangular bandwidth (erb) is a mathematical approximation that is used to represent the measured psychoacoustic width of the filter in the human auditory system, as given in (1),where cfi is the estimation of the centre frequency index along the bm (assuming a human hearing range of between 20hz to 20khz) [cit] . hence, the artificial measurement of the filter width using a gf is specified based on the erb theory. thus, the strategy of the bm in synthesizing the waveform can be imitated by implementing the gf processing. the realization of the synthesis of the human echolocator waveform in this paper used two subprocesses, defined as the erb space and the erb filter bank. the details of the gf construction can be retrieved here [cit] ."
"here, we introduce uncurl, a preprocessing framework for scrna-seq data that addresses these shortcomings by estimating the true transcriptomic state of the cells prior to the sampling effect of rna-seq. the preprocessed data from uncurl can then be directly used as input by most major unsupervised learning algorithms commonly used in the context of scrna-seq data. an overview of the algorithmic workflow of uncurl can be seen in figure 1a . the main technical contribution of uncurl is a generalized non-negative matrix factorization (nmf) that explicitly accounts for the most likely sampling distribution of the dataset. furthermore, uncurl incorporates an accelerated optimization method tailored for sparse input data, so as to handle datasets with millions of cells efficiently."
"the possible explanation on the ideal detection from the gf processing was the unique approach of the gf processing itself.each channel with a specific filter width was tuned to a different centre frequency by a given erb space. this enabled only a specific frequency to pass through the channel and prevented other impulses (frequencies) from getting through. now, each channel consisted of a single sinusoidal signal rather than a complex waveform. during the individual channel correlation, the output resembled the correlation of a sinusoidal waveform, consisting of multiple local maxima peaks, which were true of the left signal, as shown in fig. 5, and the right signal, as shown in fig. 6 . prior to the detection phase, the correlation products from individual channels, ai were summed up to obtain a single correlation result. as a result, these multiple maxima peaks were gradually decreased as the local maxima was getting dominant and continued to rise to become a peak, while the weak side lobe got suppressed during the summation phase, as shown in fig. 7 . next, the detection performance of the transmitted-echo signal pair from the individual processing schemes was observed. the detection results showed that the performance of the proposed method, which incorporated the gf,surpassed the performance of the mf, and this was true at both the left and right signals, as shown in fig. 8 . in daily life, binaural listening helps humans during the localization of sound by providing information regarding the status of the sound signal in a 3d perspective with respect to the human head. therefore, in this paper, it was important to test both the left and right signals in order to validate the detection performance. perhaps, the analysis could be extended in future work for a binaural detection strategy."
"the construction of the human hearing system is complex and can be divided into three main regions known as the i) outer ear, ii) middle ear, and iii) inner ear. the system is responsible for the processing and synthesis of sound signals upon entry into the human auditory system. each region is assigned a different purpose in the processing and synthesis of sound signals. multiple curved shapes of the pinna (outer cone shape of the human ear) serve their purpose in modifying the sound signals, especially the high frequency components, and are useful during the localization of sounds [cit] . impedance matching occurs in the middle ear, where the sound signals need to be amplified as they travel from one medium to another (air to liquid). therefore, an adequate amount of energy needs to be relayed to the stapes in order to oscillate the bm in the inner ear at an optimum rate [cit] . the unique construction of the width and stiffness along the bm varies from base to apex, being wider and softer at the apex and narrower and stiffer on approaching the base. as a result, only a specific region along the bm will oscillate upon receiving the sound signals. this illustrates that human hearing has non-linear properties, and serves as a filterbank, where each channel has a specific bandwidth range. this strategy allows the human auditory system to resolve in detail multiple frequency components from sound signals."
"the tasic (fig. 3c ) dataset is comprised of cells from mouse cortex tissue, with 49 cell types, including neuronal and non-neuronal cells. using uncurl along with tsne, we were able to identify 49 distinct clusters that corresponded very strongly to the cell types identified previously. for the same dataset, the performance of tsne without preprocessing and magic preprocessing was seen to be significantly worse, with many cell types being grouped together."
"we now look at real biological data with some amount of ground truth data available. [cit] which contains cell types comprised of four stages of olfactory neurogenesis along a linear differentiation path. the cell types were identified using markers specific to different stages of development. while this information itself is of the form of qualitative prior information, using the marker genes used to label the cells would make the problem trivial for uncurl. hence, we instead simulate a bulk dataset by using the true labels of the dataset, which is then binarized to generate qualitative marker information for all sufficiently expressed genes (same as used in the original paper). we then use this to use both the qualnorm initialization as well as unsupervised initialization to preprocess the dataset using uncurl. we then perform lineage inference using monocle2, monocle and slicer on the unprocessed data, uncurl preprocessed data and magic preprocessed data. as seen in figure 5a -c, qualnorm initialized uncurl leads to consistent improvements of the lineage inference algorithms when measured by the amount of overlap between the known cell types in the lineage graphs. in addition, preprocessing using uncurl without semi-supervision leads to qualitatively similar lineages as the unprocessed datasets with slight overlap between consecutive cell types ( supplementary fig. s5 ). on the other hand, while preprocessing using magic seems to lead to qualitatively similar lineages for monocle2 and slicer, it also leads to the estimation of a major non-existent branch in the case of monocle (fig. 5b) ."
"since state estimation is a non-convex problem, its result depends greatly on the initialization. two commonly used methods for nmf initialization are based on k-means and svd (singular value decomposition), respectively [cit] . in uncurl, we have two ways to initialize the state estimation: (i) distribution-specific k-means initialization and (ii) truncated svd þ k-means based initialization. we describe our distribution specific k-means in the rest of the section, particularly for the poisson case."
"the scalability of uncurl allows it to run on very large data matrices, including the 1.3 million-cell dataset from 10xgenomics (2017). this dataset is composed of unsorted brain cells from 18-day mouse embryos. we tested uncurl on both the full dataset and a 20 000 cell subset. since this dataset does not have any ground truth labels, we used various exploratory methods to characterize the different cell types present. we empirically chose the number of main cell types to be 10 after experimenting with various values of k. our selection of k was based on the distinctness of genetic signature of the identified clusters. figure 7a show the result of uncurl's clustering and visualization on the 20 000 cell subset while 7b shows the unprocessed tsne visualization followed by k-means clustering. to test the concurrency of the two approaches, we generated a confusion matrix (fig. 7d ) and noticed decent overlap between the clusters resulting from the two distinct methods (with an nmi of 0.45)."
"in many scenarios where scrna-seq is carried out, there is a wealth of prior knowledge. for example, there may be fish images or bulk gene expression data measured through microarray or rna-seq. alternatively, marker genes may be known for a subset of cell-types. two major issues in using such information are the incompatibility between different data types (e.g. fish images or microarray data with rnaseq data), and variability between experiments using the same technique (e.g. bulk rna-seq batch effects). we develop a method to specifically account for such variations in order to leverage this prior information. a key benefit with our method is that since the prior information is leveraged in uncurl preprocessing, it can boost the performance of downstream methods not designed to utilize such prior information. a basic problem that we need to solve is in deciding how to incorporate such differing type of information into our framework. our hypothesis is that even though the particular quantitative measurements may not transfer well to scrna-seq, the qualitative information inherent in such a dataset can be exploited. to do that, we first convert the available prior information into a binary matrix, that can then be imported into our method. the binarization is motivated by the observation that cell type specific genes often have bimodal expression patterns: they are high in certain cell types and low in most others [cit] . in some cases, the prior biological knowledge available may be marker information which is already in a binary format. in other cases, where prior biological knowledge is available as bulk gene expression or other realvalued measurements, we first binarize the gene expression by thresholding around the central value for each differentially expressed gene. differentially expressed genes are identified using deseq2 [cit] on the bulk expression data using onevs-all different expression analysis. in our experiments, we have used the poisson version of t-test [cit] to identify genes that are composed of two separate distributions and have limited the input to only include up to 25 'on' genes per cell type with the highest p-values. details of this process are described in the supplementary methods."
"this evidence indicated that the proposed method is probably an ideal method for the detection of human echo locator signals. by understanding the strategies on how the detection process is conducted by human echo locators, perhaps a similar technique can be adopted and realized in the development of a man-made sensor system. this could give rise to the need for a concept in bio-inspired processing which can be beneficial for new developments in radar and sonar systems in the near future."
"the proposed method demonstrated an ideal detection performance, where: i) there was a low cross-correlation, which exceeded 10db, thereby indicating that the features of the main lobes were well-preserved, and ii) the multiple local maxima no longer existed compared to the mf output. meanwhile, the mf detection output revealed poor features, where: i) the threshold level between the main lobes and side lobes, which was less than -3db, masked the actual features from the main lobes, and ii) the existence of multiple local maxima demonstrated multiple signal similarities found during the correlation, which led to false detection and poor performance."
"while the results in these two cases of missing information highlight the flexibility of the qualnorm framework to handle different amounts of provided prior information, they also demonstrate how having even a little additional information is enough to improve unsupervised learning results significantly."
"uncurl is capable of running on larger datasets comprising of up to millions of cells. uncurl uses a fast public nmf package [cit] for the log-normal and gaussian distributions, while using spnolips (sparse-parallel-nolips, described in the supplementary methods), which is a custom implementation based on the nolips algorithm [cit] for the poisson distribution. both implementations are capable of using sparse matrices as input for memory and runtime advantages, and are parallelizable."
"we demonstrate the utility of using uncurl as a preprocessing tool by comparing the clustering performance of various methods with and without preprocessing with uncurl, along with clustering after dimensionality reduction with zifa and simlr. additionally we compare with another preprocessing tool, magic [cit] . as seen in figure 3a, uncurl improves the performance of k-means and pca on all four datasets (a more comprehensive set of results can be seen in supplementary table s2), and it improves the performance of tsne on most datasets. moreover, in all four datasets the top performing approach (showed with the red dotted line) is an uncurl preprocessed approach."
"this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. for commercial re-use, please contact journals.permissions@oup.com bioinformatics, 34, 2018, i124-i132 doi: 10.1093 /bioinformatics/bty293 [cit] increasing cell number. finally, all existing methods rely almost exclusively on unsupervised learning and do not incorporate useful and commonly available prior information such as bulk gene expression data or cell type specific marker genes to guide the analysis process."
"first, we simulate the scenario where we have information available about a subset of cell types by using different number of known means (generated by binarizing the public bulk data [cit] and passing it through the qualnorm) and generating the other means using our version of the distribution informed k-meansþþ algorithm. we then calculate nmi between predicted and true clusters (using arg-max of w) to quantitatively measure the performance of state estimation. as seen in figure 4c, we observe that increasing the number of known means leads to improvement in accuracy. moreover, we also see that prior information about even a subset of cell types is usually enough to improve the performance over the completely unsupervised case."
"to evaluate the clusters generated by uncurl, we identified the genes most highly expressed in each cluster compared to other clusters (see supplementary methods for description). we then created a subset of the expression matrix, where the rows are grouped by cluster-specfic genes and columns are grouped by cluster-specific cells. visualizing the expression heatmaps before and after uncurl (fig. 7c) shows that the top cluster-specific genes are distinctly expressed only in their individual clusters. moreover, this expression pattern is amplified in the uncurl processed data compared to the unprocessed data. this pattern is not seen when the data is preprocessed with magic ( supplementary fig. s8 ). to further validate our findings, we overlay the average expression of the top cluster-specific genes for each cluster on the tsne visualization (fig. 7e )."
"having identified the sampling distribution for a dataset, the state estimation procedure factorizes the data into two matrices, m and w using the distribution, as described in section 2.1.1. the product of these factorized matrices then can be treated as an estimate of the true state matrix and used for all subsequent downstream learning tasks."
"based on the detection performance using the proposed method, some conclusions can be made: i) the gf processing is likely an ideal strategy to synthesize the human echo locator waveform by estimating the width of the filter in the human auditory system according to the psychoacoustic theory, ii) the logarithm properties of the gf processing have the potential to better resolve the multiple frequency component from the human echo locator waveform, and iii) the correlation of the individual channels in between the transmitted-echo signal prior to the detection stage greatly suppresses the multiple local maxima phenomenon during detection,thereby leading to an ideal detection output."
"high-throughput scrna-seq technologies [cit] can provide biological insights such as revealing cell type composition [cit], cell lineage relationships [cit] or even spatial relationships [cit] between cells in heterogeneous multi-cellular systems. enabling such insights are two key advantages of single cell transcriptomic datasets. first, having information about individual cells helps avoid aggregation and conflation of traits from disjoint groups of cells within a mixed sample [cit] . second, scrna-seq can generate a very highdimensional dataset, both in terms of the number of cells and genes that can be assayed, compared to other methods with single-cell resolutions. however, advanced computational methods are required to extract latent biological information from the raw read-counts, which provide only a heavily sampled version of the full cellular transcriptome [cit] ."
"visualization of the snr roi matrices revealed a quite homogeneous distribution of snr throughout the brain using the 32ch coil, with the highest snr in posterior cortical areas of the brain. snr of the 8ch coil was highest in the frontal lobe, but dropped in central and medial areas (figure 1 )."
"in neurosurgery, it is of key importance to have a thorough, accurate, and detailed knowledge of the anatomical structure of the surgical target because the surgical outcomes are directly connected to patient's neurological functions. compared with 2d methods, 3d computer simulation enables more accurate, realistic, and intuitive diagnosis and surgical analysis [cit] . with rapid advances in technology, virtual reality, augmented reality, and wearable devices have been adopted for various uses in medicine, ranging from simply facilitating fitness to more complex devices used in surgery [cit] . in medicine, wearables can be broadly divided into body sensors and head-mounted displays (hmd). during operative procedures, hmds can provide surgeons with 3d radiographic information while maintaining sterility."
"the quadratic regression analysis of operative time and number of cases performed was used to determine the learning curve for a f i g u r e 1 schematic view of the overall configuration of the multimodal head-mounted display system for neuroendoscopic surgery. dvi, digital video interactive; dicom, digital imaging and communications in medicine t a b l e 1 hardware components of the multimodal head-mounted display system f i g u r e 2 patient's mri image and reconstructed virtual endoscopy image of cerebral ventricles. (a and b) t1 and t2 images showed lateral ventricles, and the third ventricle had enlarged, and the surrounding brain tissue was oedematous; (c) 3d-space image showed that the cerebral aqueduct seemed obstructed; (d) 3d reconstruction of ventricular system with software 3d slicer; (e) virtual endoscopy image demonstrated that the interventricular foramen (red line) was enlarged; (f) the third ventricle failed to connect to the fourth ventricle because the cerebral aqueduct was obstructed"
our work investigates how we can separate individuals with high and low socio-economic status using mobile phone call detail records (cdr). finding good proxies for income in mobile phone data could lead to better poverty predictionwhich could ultimately lead to more efficient policies for addressing extreme poverty in hardest hit regions.
"the hmz-t1 processor was a small set-top box with only one input and one output. but there were three different image sources (the endoscope image, the 3d ve image, and real-time video of surroundings) to display. to overcome this defect, both hdmi switcher (ugreen, china) and hdmi splitter (ugreen, china) were used at the same time. through the switcher, neuroendoscope, laptop computer, and action camera were connected to the hmd all together. with the splitter, collected images were displayed on both the hmd and the monitor of neuroendoscope simultaneously so that the assistants and nurses could get the same view with the operator. combining all these instruments together, the neurosurgeon could freely switch among different image sources using a sterilized remote controller of the switcher without moving cables around."
"next, we feed the rf and gbm models with the same representation as fed into the dl algorithm -achieving rf performance of auc 64% and gbm performance of auc 61% . the performance of these models greatly suffers as the number of input features increase without increasing the training size. we conclude that given a fixed training sample, the traditional models are not able to learn a complicated function that represents higher level extractions, but perform better when using manual domain-specific feature engineering."
"this study proposed an innovative wearable multimodal display system for easy switching between endoscopic, surrounding, and computerized image modalities in neuroendoscopic surgery. wearable technology is an emerging industry which has made possible the integration of technology into healthcare and surgery as well as daily life. the trend has been supported by the miniaturization of the components necessary for the collection, storage, processing, sharing, and presentation of data [cit] . several hmd devices have been identified, which are currently utilized in surgery, imaging, simulation, education, and as a navigation tool ( f i g u r e 4 learning curve of the multimodal head-mounted display system. quadratic regression analysis of the operative time and number of cases experienced"
"3) the top-up pattern: interestingly, the recharge amount per transaction is more predictive than the total recharge amount. we observe that individuals from the lower income quantiles usually top-up with lower amounts when they first fill up their account. in order to predict household income based on mobile phone communication and mobility patterns, we implemented a multi-layer feedforward deep learning architecture. our approach introduces a novel data representation for learning neural networks on real cdr data."
"multimodality medical image fusion and processing have developed rapidly in recent years. multimodality medical image and postprocessing have proved to be clinically useful for accurate diagnosis and effective surgical treatment of brain disease (giordano, wrede, stieglitz, samii, & lüdemann, 2007; [cit] ) . reconstructed images can be adjusted and rotated freely, not only providing a better representation of spatial relationships between lesions and surrounding structures but also predicting the patterns and variants that will be encountered on the trajectory to lesions [cit] . moreover, reconstructed images allow the surgeon to depict the actual operative window from angles imitating surgical approaches and to understand the neurovascular complex by viewing from angles that are impossible to obtain intraoperatively [cit] . 3d visualization can provide the surgeon with individualized, objective anatomic information. in this study, the multimodal hmd system realized free switching between different modal images. the reconstructed ve images provide subtle visual cues such as shadow effects, depth perception, and highlighting results, which allow for an interactive evaluation by zooming, panning, and rotating [cit] . by comparing ve image with actual endoscope image, the neurosurgeon can get a better comprehension of the positional relation between lesions and normal brain tissues, enabling more accurate, realistic, and intuitive surgical analysis."
"demographic and clinical characteristics of included patients are summarized in table 2 . neuroendoscopic surgery was safely and successfully implemented in all cases and system-related complications occurred in no patients. after surgery, one patient suffered from transient mild oculomotor paralysis, which completely recovered 2 weeks later. none of the patients had intracranial infection. clinical improvement was observed in all patients after surgery. after an average follow-up of 14.7 months (range, 5-30 months), no patients needed a reoperation."
the study was approved by the institutional review board of chines pla general hospital and was conducted in conformity to the declaration of helsinki. all patients gave their written informed consent prior to participation in this study.
"our deep neural network is trained to separate individuals with high and low socio-economic status. we evaluate our models' performance with auc [cit] achieving 77% and 74% auc on test set when the classifier is predicting above/below median household income and above/below poverty level respectively (fig 2, table 3 ). the corresponding auc on the train sets are respectively 80% and 77%, showing that our model does not overfit significantly. our results indicate that the dl model achieves a higher performance than those of multi-source rf and gbm models, which vary between 71-72% and 68-69% for above/below median household income and poverty level respectively."
"with this in mind, we perform a large-scale countryrepresentative survey in a low hdi asian country, where household income is collected from approximately 80 000 individuals. the individual records are de-identified and coupled with raw mobile phone data that span over 3 months. this dataset allow us to build a deep learning model, as well as a benchmarking model using custom feature engineering and traditional data mining algorithms."
"before image preprocessing and analysis, we checked the scans thoroughly for image quality and the presence of artifacts. data processing and statistical analyses were carried out using functional magnetic resonance imaging of the brain software library (fsl) version 5.0.8. [cit] ."
"recent advances in deep learning [cit] have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [cit] and speech recognition [cit] . it seems natural to ask whether similar techniques could also be beneficial for useful prediction tasks on mobile phone data, where classic machine learning algorithms are often under-utilized due to time-consuming country and domain-specific feature engineering [cit] ."
"the system mainly contained three components: a hmd, an action camera, and a laptop computer, all of which were commendably connected together. concrete connection pattern and detailed specification information of instruments were presented in figure 1 and to realize real-time visualization of surrounding environment on the premise not taking off the hmd repeatedly, an action camera (hdr-as20, sony, japan) was bounded to the hmz-t1 viewer. with a 170° ultrawide lens, the hdr-as20 could record video of 1,080p with intense contrast and sharpness. fastened to side arm of hmz-t1, the action camera was like a pair of eyes observing the surrounding environment. with the action camera, the neurosurgeon could perceive the surroundings whenever needed by just switching the input sources rather than putting on and taking off the hmd repeatedly."
"however, everything has its limitations. for one thing, neuroendoscope generally offers two-dimensional (2d) image and has no depth of field compared with microscope. for another, the learning curve of neuroendoscopic surgery is long and shallow, which means a longer learning time [cit] . furthermore, the monitor of a neuroendoscope is usually connected together with xenon light source and digital video recording system, which is large and ponderous, making it inconvenient to adjust the location freely during operation. using a standard monitor sometimes requires surgeons to move into unpleasant postures."
"to overcome the deficiencies mentioned above, we developed a low-cost, easy to use multimodal hmd system on the strength of wearable devices and computer image processing technology."
"classification results of the traditional learning algorithms are inherently limited in performance by the quality of the extracted features [cit] . deep learning can instead reproduce complicated functions that represent higher level extractions, and replace manual domain-specific feature engineering."
"our approach suggests that multi-layer feedforward models are an effective tool for predicting economic indicators based on mobile communication patterns. while capturing the complex dependencies between different dimensions of the data, deep learning algorithms do not overfit the training data as seen by our test performance. furthermore, our deep learning model, using only a single dimension of the data in its raw form, achieves a 7% better performance compared to the best traditional data mining approach based on custom engineered features from multiple data dimensions. even though such an automated approach is time-saving, many of the classic machine learning approaches have the advantage of being interpretable. however, since a large portion of a data mining process is data preparation, there is a big demand to automate this initial step."
"we demonstrated that for dti, measured fa was higher for nearly the entire white matter when comparing the 32ch coil with the 8ch coil, except for parts of the left temporal and parietal lobe. for md, the results were hemispherical dependent, i.e., the 32ch coil showed higher md compared to the 8ch coil in the white matter of the left hemisphere but lower mds in the right hemisphere. the asymmetry in especially md metrics was unexpected and inexplicable. previous research demonstrated that md may have more variance and be less reproducible than fa, even within sites [cit] . interestingly, the unexpected pattern in diffusion metrics was different from the snr profiles of both coils and volumetric results. therefore, we emphasize that snr profiles and pattern of changes that occur in t1-weighted data cannot be translated to dti data. our results underline previous studies that already demonstrated the possible pitfalls of pooling diffusion data [cit] . harmonization methods have been investigated in a number of studies, in attempt to overcome the problems with pooled dti data, but sufficient harmonization has proven to be difficult [cit] . note that we slightly increased our fov to allow coverage of the cerebellum for the 32ch coil data, which also increased tr, and could have influenced our results. this is, however, expected to be a very minor effect, since both trs are significantly longer than five times the t1 of the white matter."
"all image data processing was performed with 3d slicer (https:// www.slicer.org, version 4.5.0), an open source software platform for medical image informatics, image processing, and 3d visualization. to acquire appropriate mri data for processing, all patients routinely underwent both conventional and 3d sampling perfection with application optimized contrast (3d-space) using different flip angle evolution mri examination performed on the same 1.5-t mri scanner (siemens espree, germany) [cit] ."
"earlier studies have shown a good correlation between location/human mobility and socio-economic levels [cit] . using this as a motivation we build a simple vector whose length corresponds to the number of mobile towers (8100 dimensions), and the vector elements correspond to the mobile phone activity at the given tower -shown in table 2 ."
"in this study, we demonstrated that quantitative results of standard processing pipelines for t1-weighted mri, diffusion tensor imaging and resting state fmri will be severely affected by the use of different head coils. paired-wise group analyses between coils revealed different patterns of gray and white matter volume, white matter connectivity and functional connectivity. voxel-based morphometric analysis of t1-weighted imaging data, revealed smaller apparent gray matter volume for the 32ch coil in the outer frontal, temporal and parietal cortex, the inner cerebellum, the precuneus, and posterior cingulate cortex compared to the 8ch coil. gray matter volume was larger using the 32ch coil in the occipital lobe, the central layer of the frontal, temporal, parietal cortex, the peripheral layer of the cerebellum, and subcortical areas. a less extensive but similar pattern of gray matter volume differences was previously found using two identical scanners with, respectively, an 8-channel and a 12-channel head coil [cit] . compared to gray matter vbm, we found an opposite pattern of head coil differences for the white matter vbm, meaning that in areas where gray matter volume was larger, white matter volume was smaller and vice versa. the visual overlap of gray and white matter differences may be a result of the spatial smoothing (7 mm). when the contrast between gray and white matter is unclear, a higher level of smoothing is necessary to account for the uncertainties in partial volume estimation. our results indicate that differences in gray/white matter contrast of the images leads to differences in tissue classification during segmentation [cit] . three factors could have led to altered image contrast: (1) despite using inhomogeneity correction prior to segmentation, the snr of the 8ch and 32ch coil has affected the probabilities of gray and white matter [cit] . for example, in our case, higher snr in posterior cortical areas for the 32ch coil increased the probability of a voxel being white matter, and the same principle may explain the results in the frontal areas of the 8ch coil, (2) both coils have differences in signal distribution due to the coil geometry [cit] and the clear algorithm does not fully correct for these [cit], or (3) there has been a difference in the effective b1 distribution [cit] ). as we demonstrate here, differences in image contrast in structural images could pose serious problems for studies combining mri hardware elements and need to be equalized before tissue segmentation and partial volume estimation to prevent methodological errors."
"we build a structured dataset consisting of 150 features from 7 different feature families, see table 1 . the features are custom-made and range from basic phone usage, top-up pattern, social network and mobility to handset usage. the features include various parameters of the corresponding distributions such as weekly or monthly median, mean and variance."
"results are described in detail in the supplementary material. for t1-weighted imaging, after applying the scaling factor, vbm analyses showed almost complete removal of the head coil differences throughout the brain for both gm and wm (supplementary figure s3) . validation of the dti scaling factors showed a successful harmonization of the fa images, removing all significant coil differences. for md, head coil differences were reduced, except for some small areas at the forceps major and right thalamus (supplementary figure s4) . for resting state analyses, validation of the scaling factors showed extensive reduction of coil differences in the medial and lateral visual networks, and complete removal of all significant differences in the default mode network, executive control network (salience network), and the left and right dorsal visual stream networks (supplementary figure s5) ."
from the household income we derive two binary classifiers (1) below or above median household income and (2) below or above upper poverty level. the income threshold for poverty level is derived from official statistics and based on average national household income and household size. our survey classifies participants into 13 household income binswhere bin 1 and 2 correspond to below upper poverty level.
"2) the handset brand: in the country of our study, minimal and more affordable handset brands are very popular among the lower income quantiles, while expensive smartphones are considered as a huge status symbol."
"as future work, we would like to investigate the performance implications of including temporal aspects of raw cdrs in our models and the data representation. in addition, we will work on finding a general representation of telecom data that can be used for various prediction tasks."
"we use a standard multi-layer feedforward architecture where the weighted combination of the n input signals is aggregated, and an output signal f() is transmitted by the connected neuron. the function f used for the nonlinear activation is rectifier f() ≈ log(1+e  ) . to minimize the loss function we apply a standard stochastic gradient descent with the gradient computed via back-propagation. we use dropout [cit] as a regularization technique to prevent over-fitting. for the input layer we use the value of 0.1 and 0.2 for the hidden layers. dropout secures that each training example is used in a different model which all share the same global parameters. this allows the models to be averaged as ensembles, improving generalization and preventing overfitting. the split between train and test set, as well as the introduced misclassification cost for poverty level, are similar to the benchmark models."
"the study has been carried out in accordance with the declaration of helsinki and has been approved by the medical and ethical review committees of the erasmus mc university medical center, rotterdam, netherlands and the leiden university medical center, leiden, netherlands. written informed consent has been obtained from all participants."
"eye fatigue occurred in three patients whose operation time exceeded 80 min at the early stage of this study. no complaints of blurred vision, dizziness, or other forms of simulation sickness arose."
"1) random forest, where we build several independent classifiers and average their predictions, thus reducing the variance. we choose grid search to optimize the tree size."
where the user spends most of his time is a good signal of his income. this indicates that our models have detected regions of low economic development status.
"in this study, we present a low-cost multimodal hmd system for neuroendoscopic surgery. as shown by the study, the system is feasible, practical, helpful, and relatively cost efficient. through free switching among different image modalities, the system enables a more relaxed position, a better comprehension of lesion's 3d positional relations, and a better view of the operative field. with advances in technology and instruments, wearable technology would play a more significant role in neuroendoscopic surgery and other surgeries in the near future."
"major strengths of this study are the large sample size and the within-subject study design of two protocols within one scanning session using similar acquisition parameters. despite our best efforts, some confounding factors are essentially inevitable, such as scanner drift, re-positioning of the participants heads inside the different coils and use of head cushions [cit] . other factors that could have influenced the results in our study may be habituation of the subject and scanner warm-up, especially for the resting state fmri, and the anisotropic voxel size of the t1 weighted sequence. we are aware that extrapolating the results from our study to other head coils or other vendors may be difficult. instead, we emphasize that our study may be appreciated as increasing awareness for the variability and possible bias in quantitative mri metrics in data originating from different hardware elements. this is especially important for clinical research, where data acquired with different hardware elements is increasingly pooled."
screen resolution of the hmd was high enough for the neurosurgeon to judge lesions and peripheral structures and to operate surgical equipment accurately. the range and depth of vision of the system seemed comparable to or even better than those of the video monitor.
"during operation, the neurosurgeon could adjust or rotate the reconstructed 3d ve image whenever necessary with the assistance of a leap motion controller connected to the computer ( figure 3c ). the ve view during operation helped the surgeon to get a better understanding of spatial relationships between lesions and surrounding anatomic structures. the ability of freely switching among endoscope images, ve images, and surrounding images was a key feature of this system."
"for both the 8ch coil and 32ch coil, we isolated an average signal image and one noise-only image for each orientation plane. we subdivided the signal and noise images into nonoverlapping regions of interest (roi) of 16 by 16 voxels. next, we calculated the mean signal of the roi using the averaged signal image, and the standard deviation of the noise of the roi using the noise-only image. since the noise images were amplitudereconstructed, the measured standard deviation was corrected for the rician noise distribution [cit] . ultimately, for each roi, snr was calculated according to:"
"when switched to endoscope modality, the endoscopic image was always in the direct line of sight, allowing the neurosurgeon to work in a relaxed position. when the neurosurgeon would like to look over patient's medical images, all he needed is to switch the input source to computer using a sterilized remote controller."
a posteriori inspection of the top features selected by our traditional models leads to some interesting qualitative insights. we observe a similar pattern in the top features selected by the rf and gbm model. figure 3 shows the top features in our rf model. we notice the importance of three feature families:
"jlp designed the study, collected and analyzed the data, and was involved in data interpretation and writing of the manuscript. yt, mo, jg, and jmpa contributed to data analyses, interpretation and writing of the manuscript. ee, jmpo, lj, lm, ed, and js were involved in data collection and writing of the manuscript. mb and sr contributed to data interpretation and writing of the manuscript. ah was involved in study design, analyses and interpretation of the data, and writing of the manuscript."
"it took just several minutes to assemble the multimodal hmd system in a surgery. the neurosurgeon first employed the action camera view to accomplish surgical procedures that did not need the neuroendoscope, such as scalp incision and trepanning. once the neuroendoscope was placed into the ventricle, endoscopic image would be displayed on the hmd. when the neurosurgeon wanted to look through patient's preoperative mri image or reconstructed 3d ve image to better understand the positional relation between lesions and surrounding normal tissues, all he needed to do was to switch the input source to laptop computer. details on how the system worked in a neuroendoscopic surgery are shown in figure 3 ."
2) gradient boosting machines (gbm) where the base classifiers are built sequentially. the algorithm combines the new classifier with ones from previous iterations in an attempt to reduce the overall error rate. the main motivation is to combine several weak models to produce a powerful ensemble.
the datasets for this manuscript are not publicly available yet as they are part of the ongoing longitudinal ftd-risc study and the international genfi cohort. requests to access the raw mri datasets should be directed to js (j.c.vanswieten@erasmusmc.nl). voxel-specific scaling factors from this study are available upon reasonable request to ah (a.hafkemeijer@lumc.nl).
"interestingly, five patients with suprasellar arachnoid cyst had been previously misdiagnosed with other hydrocephalus-causing diseases, and four had been previously treated with ventriculoperitoneal shunt with no symptom relief. all mri scans and medical image processing were performed prior to surgery. it took 12 min on average to reconstruct a 3d ve image of one patient while assembly of the system cost 5-10 min."
"in our set-up, each model is trained and tested using a 75/25 split. for the response variable related to poverty level we introduce misclassfication cost. since few people are below the poverty level (minority class), a naive model will predict everyone above poverty line."
"the rest of this paper is organized as follows: in section 2 we describe the features and models used for benchmarking our deep learning approach. section 3 describes the deep learning approach itself. in section 4 we compare the results of the two approaches. finally, we draw our conclusions in section 5."
"this section describes the features and the standard machine learning algorithms used as our benchmark. the data preparation phase in the data mining process is a tedious task, that requires specific knowledge about both the local market and the various data channels as potential sources for input features. typically the data warehouse architecture and the data format vary between the operators and third-party software packages, making it hard to create generalizable feature-sets."
"and accommodated glasses very well. to our experience, it was pleasant to wear that weight on the head for 1.5 hr or a shorter time, after that, it might make the host discomfort or annoyed. the weight of hmd required improvement. luckily, most ventricular neuroendoscopic surgeries could be finished within 2 hr. compared with most commercially available surgical assistant systems, the cost of our system is much less (total cost, exclusive of the computer, approximately $1,200). moreover, the system had a steep learning curve, which meant that it was easy and quick to learn and master such a multimodal hmd system."
"for the present study, we included 77 cognitively healthy participants who underwent mri of the brain on a 3tesla philips achieva scanner (philips medical systems, best, netherlands) at the leiden university medical center, leiden, netherlands. the mri protocol contained t1 weighted, dti and resting state functional mr images, acquired with both an 8 channel sense head coil (8ch) and an 32 channel sense head coil (32ch) within one mri session (coil geometry is displayed in supplementary figure s1 ). during acquisition, optimal image quality was obtained by using the incorporated \"constant level appearance\" (clear) inhomogeneity correction algorithm on the scanner. we display one raw dataset for all sequences from both coils from a representative healthy participant in supplementary figure s2 ."
"an important application of this work is the prediction of regional and individual poverty levels in low hdi countries. since our approach only requires de-identified customer and tower ids, we find this method more privacy preserving compared to traditional data mining approaches where the input features may reveal sensitive information about the customers."
"cognitively healthy participants were included in the context of the prospective longitudinal frontotemporal dementia risk cohort (ftd-risc) in which families with autosomal dominant inherited ftd gene mutations are followed using standardized assessment protocols including an mri of the brain every year, as described previously [cit] . to confirm cognitively healthy status of all participants, mini mental state examination [mmse [cit] ] and the frontal assessment battery [fab [cit] ] are reported as cognitive screening measures and the neuropsychiatric inventory [npi-q [cit] ] and frontotemporal dementia rating scale [frs [cit] ] are reported as behavioral screening questionnaires."
ensemble methods have been proven as powerful algorithms when applied to large-scale telecom datasets [cit] . they combine predictions of several base classifiers built with a given learning algorithm in order to improve robustness over a single classifier. we investigate two different state-of-the-art ensemble methods:
"there are some limitations that should be acknowledged. the application of wearable devices in neurosurgery is relatively new, and the true usefulness of such a multimodal hmd system needs to be better validated and assessed with prospective controlled study."
the study was partly funded by the national natural science foundation of china (81771481). the authors would like to express their sincere appreciation to all participating patients.
"in conclusion, this study provides evidence that the results of standard analysis models are severely compromised when data from different head coils is combined, or when head coils are changed during longitudinal clinical studies, even though acquisition protocols are completely harmonized. studies combining neuroimaging mri data with multiple head coils or other mri hardware elements should be aware that measurements of gray and white matter volume, white matter connectivity and functional connectivity will differ between head coils and should handle these confounding factors with caution."
"for preprocessing of the resting state fmri scans, we applied the fmri expert analysis tool (feat) as implemented in fsl, consisting of motion correction with mcflirt and spatial smoothing with a kernel of 6 mm fwhm. the datadriven independent component analysis (ica) based automatic removal of motion artifacts (ica-aroma) approach was used to identify and remove noise components from the resting state fmri data [cit] . after denoising, high pass temporal filtering was performed with a cut-off frequency of 0.01 hz. the functional resting state images were registered to the corresponding t1 weighted images using boundarybased registration and were subsequently registered to the 2 mm isotropic mni standard space using non-linear registration with a warp resolution of 10 mm. voxel-based functional connectivity was studied in a standardized manner using the eight standard beckmann resting-state functional networks of interest [cit], i.e., the medial and lateral visual system network, the primary auditory network -also known as the salience network -, the sensory motor network, the default mode network, the executive control network and the left and right dorsal visual processing stream networks. to further account for noise, white matter and csf templates were included in the analyses as regressors. functional connectivity of each network of interest was calculated using dual regression, as previously described [cit] . in short, the eight standard resting state networks [cit] were used as a reference. voxel-based resting state functional connectivity was determined in terms of similarity of the bold fluctuations in the brain in relation to characteristic fluctuations in the standard resting state networks. with dual regression, individual time series were first extracted for each template, using the resting state networks, and the two additional white matter and cerebrospinal fluid maps, in a spatial regression against the individual fmri data set (regression 1). the resulting matrices described temporal dynamics for each template and individual. next, the temporal regressors were used to fit a linear model to the individual fmri data set (regression 2), to estimate the spatial maps for each individual. this results in 3d images for each individual, with voxel-wise z-scores representing the functional connectivity to each of the predefined standard networks."
"to assess the influence of the head coil on gray and white matter volume measurements, we applied the standard voxelbased morphometry (vbm) pipeline as implemented in fsl. preprocessing of the t1 weighted images included brain extraction followed by radiofrequency (rf) inhomogeneity correction, tissue segmentation and realignment to montreal neurological institute (mni) standard space using non-linear registration. we performed quality control to ensure good brain extraction, that was not different between both head coils. next, fmrib's automated segmentation tool (fast) was used for correction for spatial intensity variations, also known as bias field or rf inhomogeneity, and segmentation of the t1 weighted images [cit] . the corrected, segmented gray matter images were re-registered non-linearly to a study-specific template with a balanced set of 8ch and 32ch coil images. the registered partial volume images were divided by the jacobian of the warp field to correct for any local expansion or contraction. an isotropic gaussian kernel with a sigma of 3 mm, which corresponds to a full width at half maximum kernel (fwhm) of approximately 7 mm, was used to smooth the gray matter segmentations. we also applied the vbm processing pipeline to the white matter segmentations, resulting in registered, corrected and smoothed white matter images for voxel-wise analyses."
"neuroendoscopic surgery was successfully performed under the assistance of the multimodal hmd display system in all patients, with no system-related technical problem happened. the neurosurgeon (chen, x. l.) experienced this multimodal hmd as a helpful, comfortable, lightweighted, and easily adjustable device. switching the display to the action camera modality, the neurosurgeon could perform surgical procedures and make contact with the environment easily as usual."
"our treatment of linear algebra is organized in two levels. on the abstract level, a hierarchy of structures (see section 3.4) provides interfaces, notations and shared theories for vectors, f -algebras and their morphisms. on the concrete level, these structures are instantiated by particular models, centered on matrices [cit] . the central ingredient to this latter formalization is an extended gaussian elimination procedure similar to lup decomposition. this formalization of matrix algebra itself contains proofs of nontrivial results, covering determinants, laplace expansion for cofactors, and properties of direct sums."
"the formalization described in the present article, however, is of a different nature. the proof of the odd order theorem does not rely on mechanical computation, and the arguments were meant to be read and understood in their entirety. what makes the formalization difficult -and interesting -is the combination of theories involved. working with these theories formally required developing a methodology that makes it possible to switch, efficiently, between the various views a mathematical text can superimpose on the same mathematical object. another important task has been to formalize common patterns of mathematical reasoning. when it comes to formal verification of software, interaction with a proof assistant is commonly based on case analysis and structural induction. in contrast, the proof of the odd order theorem relies on a variety of argument patterns that require new kinds of support."
"in title compound, c 12 h 11 br 2 no 2, the coumarin ring system is almost planar, the two rings being inclined to one another by 1.40 (15) . there are two short intramolecular interactions (n-há á ábr and c-há á ábr) involving the br atoms. in the crystal, molecules stack along the a-axis direction viainteractions; the centroid-centroid distances vary from 3.6484 (19) to 3.7942 (19) å ."
"to compare orientation selectivity of the two models better, we also plot o/p ratios as a function of c e in fig. 7 . as indicated by these ratios, when presented with 50%-contrast stimulus, v1 cells with poisson afferents are less selective across all c e tested. the difference, however, gets smaller as c e increases. at large c e, say, 0.25, a single lgn spike can trigger a large enough g lgn (whose maximum is 56) to drive its downstream cortical neuron to fire even when the initial v1 membrane potential is low. the 'abilities' of the orthogonal and preferred stimuli to drive a v1 cell are thus not as different as in the case of smaller c e ."
"τ is the time constant of the leak (sec); i(t) is the stimulus applied (sec −1 ). n(t) is comprised of poissondistributed noise shots (sec −1 ) of steady rate (1 khz), uniform size, and random polarity, and is added at every time step. this additive, stimulus-independent noise represents background synaptic activity. a spike is fired when the membrane potential, v, reaches the threshold, v thres [cit] ."
"to help decipher the mechanism behind the difference observed in v1 orientation selectivity between the nlif and poisson inputs, we plot in fig. 9 a few properties of the summed lgn synaptic input (mean and standard deviation of g lgn across time and trials, first harmonic of the trial-averaged g lgn ) with respect to stimulus orientation. as a side note, these"
"the proof of theorem 1 proceeds by induction, showing that no minimal counterexample g exists. at the outset g is only known to be simple, nonabelian of odd order, but all proper subgroups of g should be solvable. the first half of the proof exploits these meager facts to derive a detailed description of the maximal proper subgroups of g, reducing the general structure of g to five cases. the second half of the proof uses character norm inequalities to rule out four of these, and extract some algebraic identities in a finite field from the last one. galois theory is then used to refute these, completing the proof."
"to compare with the nlif model, we also create lgn spike trains using inhomogeneous poisson processes. poisson rates as a function of time are determined from peri-stimulus histograms (psths) of the corresponding nlif model."
"a group g consists of a set, usually also named g, together with an associative binary law *, usually denoted by juxtaposition, and an identity element 1, such that each element g of g has an inverse g −1, satisfying gg"
"another common measure for orientation selectivity is the bandwidth of orientation tuning, and it can be estimated by fitting individual orientation tuning curves to gaussian distributions,"
"previous studies have shown that stimulus onset reduces membrane-potential and firing-rate variability in v1 [cit] . in fig. 11 we see that poisson-driven v1 cells have ff ∼ 1.1 (calculated using 250 ms windows, i.e., one stimulus cycle) across all stimulus orientations and contrasts. in addition, variability of the background rate is the same as the stimulus-driven case. v1 neurons with lgn afferents modeled by the nlif equation, on the other hand, show a reduction in ff in response to a drifting-grating stimulus in general. unsurprisingly, the decline in variability is most apparent when the corresponding firing rate is the highest, i.e., response to the preferred stimulus at 50% contrast. variability of response to the orthogonal stimulus at 20% contrast is similar to the variability in spontaneous activity."
"this group, denoted g/h, is called the quotient group of g and h because it identifies elements of g that differ by an element of h. if g 1 and g 2 are groups, g 1 and g 2 are both normal in the group"
"the noisiness of the lgn input has an influence also on stimulus detectability based on the spike trains of model v1 neurons. receiver-operating characteristic (roc) curves indicate how likely it is that a neuron can correctly distinguish a signal trial (visually driven) from a blank trial (spontaneous activity), based on its firing rate. integrating under the roc curve gives us the detection probability. (0.5 is the chance level, 1 indicates perfect detectability.) we first note in fig. 12(a) that for both nlif-and poisson-driven cortical neurons, as stimulus strength decreases from 50% to 20%, the separation between the noise and signal+noise pulsenumber-distributions (pnds) narrows. this in turn shifts the roc curve at lower contrast closer to the diagonal 'chance' line. [cit], we see an increase in detection probability as the stimulus contrast increases (fig. 12(b) ). figure 12 (b) also shows clearly that cortical cells with poisson afferents have lower detection probability than cells with nlif afferents across all contrasts tested. as an example, the detection probabilities at 20% contrast are 0.64 and 0.72 for poissonand nlif-driven cortical neurons, respectively. this observation implies that nlif-driven cortical neurons are better at telling the 'signal' (stimulus-driven cases) apart from the 'noise' (spontaneous activities). poissondriven v1 neurons have larger contrast-dependent response variability than their nlif counterparts, resulting in a greater overlap between their 'noise' and 'signal+noise' distributions."
"manifestations of this automatic proof search machinery are ubiquitous. for example, we can prove (1 \\in f @* (g :&: h)) by applying the group1 lemma, which states that a set contains the unit element 1 if it happens to be a group. the canonical structure mechanism infers this group structure automatically for f @* (g :&: h): if g and h have a group structure, then so does their intersection, as well as the image of that intersection under a group morphism f."
"a group g is simple when its only proper normal subgroup is the trivial group 1, i.e., if its only proper normal series is 1 ⊳ g. a normal series whose factors are all simple groups is called a composition series. the jordan-hölder theorem states that the (simple) factors of a composition series play a role analogous to the prime factors of a number: two composition series of the same group have the same factors up to permutation and isomorphism. unlike natural numbers, however, non-isomorphic groups may have composition series with isomorphic factors. the class of solvable groups is characterized by the elementary structure of their factors: definition 3 (solvable group). a group g is solvable if it has a normal series whose factors are all abelian."
"in short, the strength of the lgn-v1 synaptic coupling also has a large influence on cortical selectivity. it is therefore an important parameter in constructing a realistic neuronal network. we are guided by the cortical firing rate to choose a value of c e of 0.20."
"in this equation, the cortical cell's membrane capacitance is assumed constant and absorbed into the conductance, so that the unit of conductance is sec −1 . the leakage conductance, g l, the resting potential, v l, and the reversal potential of the excitatory synaptic current, v e, are set to the values of 50, 0, and 14/3, respectively. a refractory period is not implemented."
"variability of cortical responses is characterized by the fano factor. for fig. 10, we count the number of spikes in a sliding 50 ms window, and the fano factor in fig. 11 is calculated using a bin size of 250 ms."
"local analysis provides us both with a precise description of the characters of a maximal subgroup m, and an isometry mapping certain virtual characters of m (differences of characters) to virtual characters of g. this dade isometry is only defined on functions that vanish on 1, so in order to extract usable information on g one needs coherence theorems extending it to a set of proper characters. the first, due to sibley, covers frobenius and type v maximal subgroups, and the second type ii-iv subgroups."
"to summarize, the nlif model provides a good description of the temporal response properties of lgn cells. however, the model's absolute response versus stimulus contrast does not include the contrast nonlinearity observed experimentally [cit] . we do not aim to reproduce the contrast response function, as our goal here is to assess how the response variability of the thalamic input affects the orientation selectivity of v1 neurons. consequently, we focus on the response regime corresponding to stimulus contrast in the range of 10-50% in our analysis of cortical selectivity."
"given the substantial amount of group theory that needed to be formalized, we relied on two important observations to optimize the data structures used to represent finite groups. first, in many proofs, one can take most or all of the groups involved to be subgroups of a larger ambient group, sharing the same group operation and identity. moreover, local notions like the normalizer of h inside of g, denoted n g (h), are often used \"globally,\" as in n(h), a practice which implicitly assumes that the normalizer is to be computed within an ambient container group. second, and more importantly, many theorems of group theory are equally effective when stated in less generality, in terms of subgroups of such an ambient group. for example, given a theorem having to do with two unrelated groups, it does not hurt to assume that the two groups are subgroups of a larger group; this can always be made to hold by viewing them as subgroups of their direct product."
"the canonical structures inference mechanism essentially provides a prolog like resolution engine that is used throughout the libraries to write statements that are more readable and easier to use. programming this resolution engine can be quite tricky, and a more technical explanation would go beyond the scope of this paper [cit] ."
"very often, the verification of small, uninteresting details is left implicit in an ordinary mathematical text, and it is assumed that a competent reader can fill these in. canonical structures can be programmed to play a similar role. in particular, structures can package data with proofs, as in section 3.2, in which case searching for a particular structure that contains a certain value can amount to looking for a proof that some property holds for that value."
"it is interesting to note that standard galois theory is usually carried out on normal extensions rather than on splitting fields. while the two notions are constructively equivalent, splitting fields are much easier to construct in practice."
"the instances of the mathematical structures of our hierarchy (see section 3.4) are required to have boolean operators for the comparison and, possibly, the ordering of their inhabitants. we provide instances for all these structures, and all the instances needed for this formalization are in fact either finite types or countable types. we therefore benefit from other classical properties otherwise not available in the constructive logic of coq. indeed, countable types satisfy the functional choice axiom for boolean predicates (markov's principle); functions on finite types can be represented by their graphs, which are extensional: the graphs of any two functions that are pointwise equal are in fact equal [cit] . boolean reflection extends to any first-order theory that has a decision procedure. in particular, the algebraic hierarchy mentioned in section 3.4 has an interface for fields with a decidable first-order theory. the specification of this property uses a deep embedding of first-order formulas together with a boolean satisfiability predicate. finite fields are of course instances of this interface, as are algebraically closed fields, which enjoy quantifier elimination [cit] . decidable fields are used in the formalization of representation theory, both in dealing with modular representations, which are based on finite fields, and complex representations, which are based on algebraic complex numbers."
"in addition to the hierarchy of algebraic structures, we also provide a hierarchy for numeric fields [cit], which are fields equipped with a boolean order relation, possibly partial. the purpose of this small hierarchy is to capture the operations and the theory of the complex number field and its subfields (cf section 5.2)."
"in this paper, we first show that with appropriate parameters, the nlif model can provide a reasonable functional description of lgn neurons. the parameters are constrained by experimental data on lgn neurons of anesthetized cats. with a physiologically realistic lgn model in hand, we then build the simplest feedforward model of a cortical neuron whose sole input is the sum of two lgn spike trains. we show that model v1 neurons whose lgn afferents are modeled by the nlif formalism exhibit higher orientation selectivity than neurons with poisson lgn input. therefore, capturing the spike-firing statistics of the lgn input to the cortex is important for understanding the causes of cortical orientation selectivity. more realistic largescale models of the cortex can be built using nlif inputs from the lgn."
"to illustrate how variability in the thalamic input could influence its downstream cortical cell's responses, we model the simplest case where a cortical neuron receives purely feedforward inputs from one on-and one off-center lgn neurons (fig. 1) . in what follows we compare v1 responses for the two cases in which the dynamics of each lgn neuron is described either by an nlif model or by an inhomogeneous poisson process."
"galois theory establishes a link between field extensions and groups of automorphisms. a field extension is built by extending a base field with roots of polynomials that are irreducible on this base field. the vector space structure of such an extension plays an important role. the remarks and methods described in section 3.3 apply in this situation: instead of assigning a type to each new extension, field extensions are formalized as intermediate fields between a fixed base field f and a fixed ambient splitting field extension l. a splitting field extension of f is a field extension of f generated by an explicit finite list of all the roots of a given polynomial."
"the advantage to using dependent type theory is that types can express complex specifications. for example, in our formalization, if g is an object of type fingrouptype, then g is a record type which packages the type representing the elements of g, the binary group operation, the identity, and the inverse, as well as proof that these items satisfy the group axioms. in addition, coq's type inference algorithm can eliminate the need to provide information that can be reconstructed from type constraints, just as implicit information is reconstructed by an experienced reader of a page of mathematics. for example, if g and h are elements of the carrier type of g an object of type fingrouptype, then when presented with the expression g * h, coq can infer that * denotes the binary operation of g, as well as the fact that that operation is associative, and so on. thus, type inference can be used to discover not only types, but also data and useful facts [cit] . working with such an elaborate type system in a proof assistant can be delicate, however, and issues like the decidability of type checking impose severe restrictions on the nature of the dependent types one can work with in practice."
"the molecular structure of the title molecule, with the atom-numbering scheme. displacement ellipsoids are drawn at the 50% probability level. the short br···h interactions are shown as dashed lines."
"in order to formalize this kind of proof efficiently, the ssreflect library uses notation to pair an inequality with the condition under which equality holds. for example, consider the following lemma:"
"the success of the present formalization relies on a heavy use of the inductive types [cit] provided by coq and on various flavors of reflection techniques. a crucial ingredient was the transfer of the methodology of \"generic programming\" to formal proofs, using the type inference mechanisms of the coq system."
"in section 2 we outline the statement and proof of the odd order theorem. section 3 provides some examples of the design choices we adopted to represent mathematical concepts in the type theory underlying the coq system. in section 4 we review some examples of the techniques we used to represent efficiently different kinds of proof patterns encountered in this proof. in section 5 we provide three examples of advanced mathematical theories whose formalization require a robust combination of several areas of formalized mathematics, before scaling to the main proof. section 6 concludes the paper with comments and some quantitative facts about this work."
"in other cases first-order decidability fails, notably for the rationals and for number fields. as a result, we elected not to rely on this interface for some basic results in the theory of group modules that cannot be proved constructively. instead, we proved their double negation, expressed using the classically monadic operator [cit] : note the implicit use in this statement of the coercion mentioned in section 3.2. the statement (classically p) is logically equivalent to (~~p), but this formulation is more useful in practice, because when using a hypothesis of the form (classically p) in the proof of a statement expressed as a boolean (on which excluded middle holds), one can constructively assume that p itself holds."
"records are just a generalization of the dependent pair construction described in section 3.2, and, in the same way, can be used to package together types, data, operations and properties. they can thus play the role of abstract \"interfaces.\" such interfaces are very natural in abstract algebra, but are also useful in developing a theory of iterated operations [cit], a theory of morphisms, a theory of algebraic structures [cit] and so on. for an extensive list of interfaces used in the ssreflect library, the reader can refer to section 11.3 of the ssreflect documentation [cit] ."
"as we have tried to make clear in this paper, when it comes to formalizing this amount of mathematics, there is no silver bullet. but the combined success of the many techniques we have developed shows that we are now ready for theorem proving in the large. the outcome is not only a proof of the odd order theorem, but also, more importantly, a substantial library of mathematical components, and a tried and tested methodology that will support future formalization efforts."
"in short, the success of such a large-scale formalization demands a careful choice of representations that are left implicit in the paper description. taking advantage of coq's type mechanisms and computational behavior allows us to organize the code in successive layers and interfaces. the lower-level libraries implement constructions of basic objects, constrained by the specifics of the constructive framework. presented with these interfaces, the users of the higherlevel libraries can then ignore these constructions, and they should hopefully be able to describe the proofs to be checked by coq with the same level of comfort as when writing a detailed page in l a t e x. the goal of this section is to describe some of the design choices that were made in that respect."
"we then refactored that construction to eliminate the use of the real algebraics. we construct the algebraics directly as a countable algebraic closure, then construct conjugation by selecting a maximal real subfield. because we are within a closure we can use galois theory and adapt the usual proof of the fta to show that conjugation is total."
"the status of computation in coq's formalism also plays a central role in the present formalization. every term or type has a computational interpretation, and the ability to unfold definitions and normalize expressions is built in to the underlying logic. type inference and type checking can take advantage of this computational behavior, as can proof checking, which is just an instance of type checking. the price to pay for this powerful feature is that coq's logic is constructive. in coq many classical principles, such as the law of the excluded middle, the existence of choice functions, and extensionality are not available at the level of the logic. these principles can be recovered when they are provably valid, in the constructive sense, for specific objects like finite domains, or they can be postulated as axioms if needed. the present formalization, however, does not rely on any such axiom. although it was not the primary motivation for this work, we eventually managed to obtain a completely constructive version of the proof and of the theories it requires."
"a slight difficulty is that canonical structures are designed to guide unification, which is used by type inference to process types (like int), while here we need to process values (like the intersection of two sets). the crucial observation is that coq's logic features dependent types, which means that values can be injected into types, even artificially, to make them available to unification, and hence to canonical structure resolution. we call the mechanism for doing this a phantom type. the use is similar to the use of phantom types in functional programming [cit], where one enriches a type with a dummy (phantom) annotation to trick the type system into enforcing additional invariants."
"the classically operator is used only in the file formalizing the theory of group modules for representations. note that although we use the classically operator to weaken the statement of some theorems we formalized, we did not need to alter the statement of odd order theorem to describe its proof completely within the calculus of inductive constructions."
"in the crystal, the molecules stack along the a axis direction (fig. 2 ). there are a number of π-π interactions present: cg1···cg1 i 3.7580 (19) å; cg2···cg1 i 3.6484(19 å; cg2···cg2 ii 3.7942 (19) å [where cg1 is the centroid of ring (o2,c1-c5); cg2 is the centroid of ring (c4-c9); symmetry codes: (i) -x+1, -y+1, -z+1; (ii) -x+2, -y+1, -z+1]."
"such constructions are ubiquitous. forming subtypes as described in section 3.2 in each case would be unwieldy and would make the application of lemmas mentioning partial constructions particularly cumbersome. the general approach has been to make each of these constructions total, either by modifying the input when it is invalid, or returning a default value. for example, the direct product construction returns the empty set (which is not a group) if the input groups have a nontrivial intersection; applying a morphism to a set automatically shrinks the input set by intersecting it with the domain of the morphism. the downside of this approach is that lemmas involving partial constructions often, but not always, depend on side conditions that need to be checked when the lemma is applied."
"lgn synaptic inputs provide feedforward inputs to model v1 cells whose tuning properties are depicted in fig. 6 . as expected, the mean lgn input conductance is constant irrespective of the orientation, exhibiting no orientation selectivity at all. the first harmonic of trial-averaged g lgn, on the other hand, shows high orientation selectivity, peaking at the preferred orientation and dropping to zero at the orthogonal-to-preferred orientation. this, however, is unlikely to be the main factor that causes the difference in selectivity between nlif-and poisson-driven cortical cells because the first-harmonic tuning curves of the nlif and poisson models are similar in both the shape and amplitude. the standard deviations of both poisson and nlif inputs are tuned to stimulus orientation (fig. 9, middle panels), and the difference between the orientation tuning curves of standard deviation of the lgn inputs closely resembles the tuning curves of mean v1 firing rates in fig. 6 . what is significant is that the variability of the poisson input is higher than that of the nlif input across all orientations at both contrast levels."
"subgroups and factors of solvable groups are solvable, so by the structure theorem for abelian groups, a finite group is solvable if and only if all the factors of its composition series are cyclic of prime order. we are now able to state the odd order theorem."
"we start by focusing on a model v1 cell in which the lgn-v1 synaptic coupling coefficient, c e, is 0.20. the v1 cell's responses to stimuli of different orientations at 50% and 20% contrasts are plotted in fig. 6 . first, v1 cells receiving poisson input have higher mean firing rates than their nlif counterparts. second, neuronal responses of a v1 cell depend on the stimulus orientation, especially at high contrast. third, poissondriven cortical neurons have larger responses to the orthogonal-to-preferred orientation relative to their peak responses than nlif-driven neurons."
the differences between the outcomes with these two lgn models are more readily perceived if we plot the ratio between cortical responses (mean firing rate with background rate subtracted) to a specific orientation and firing in response to the preferred orientation (set to 0
"the difference between orientation selectivity of v1 cells with poisson and nlif afferents is unlikely to be caused by the average response properties of lgn neurons. mean firing rates of lgn neurons modeled by the nlif and poisson formalisms in response to drifting gratings at 50% and 20% contrasts are 20 (19 for poisson) and 16 (15 for poisson) ips, respectively. similarly, their psths and trial-averaged g lgn as a function of time match almost perfectly."
"all the constructions of this formalized galois theory hence apply to extensions of a field f that are subfields of a field l. if k and e are intermediate extensions between f and l, the galois group type (see section 3.3) of e is the type of automorphisms of e. then the galois group gal(e/k) of a field extension e/k is the set of automorphisms of the galois group type that fix k. partiality issues are dealt with in a manner similar to their treatment in finite group theory (see section 3.3): the definitions take as arguments subspaces of the ambient field, but the theory is available for those vector spaces that are fields, a fact that can generally be inferred via a canonical structure. for example, gal(e/k) is a set when e and k are vector spaces, but is equipped with a group structure as soon as e is a field."
"a standard pattern of reasoning seems to conclude out of blue that some assertion holds, from a proof that a chain of nonstrict inequalities in which the first and last terms are the same. the implicit content is a three-step proof: the circularity of the chain forces each inequality to be an equality; for each inequality, the equality case is characterized by a certain condition; hence the conjunction of these conditions holds and the desired statement follows from these. typical examples of such inequalities come from the properties of convex functions, e.g., the inequality between the arithmetic and geometric means is related to the strict convexity of the exponential function. the equality conditions can, however, be more elaborate; for example, the rank of a sum of finite dimensional vector spaces is smaller than the sum of the ranks of the summed vector spaces and equality holds if and only if the sum is direct."
"to study how variability in the thalamic input affects cortical selectivity, we investigate the simplest scenario: a v1 neuron receiving feedforward inputs from two lgn neurons with a phase difference that depends on the stimulus orientation. the thalamic input is the sum of the responses of two lgn neurons filtered through the synapse (eq. (3))."
"in our formalization, we represent such ambient groups as fingrouptypes, and then represent the groups of interest as subsets of that type that contain the identity and are closed under the group operation. this is much simpler than having to maintain a plurality of types and morphisms between them. we form a new fingrouptype only when strictly necessary, for example when forming a quotient group, which requires a new group operation [cit] ."
"at 50% contrast, cortical cells with poisson afferents have a moderately high o/p ratio of 0.5; whereas cells receiving the nlif input exhibit better orientation selectivity with a o/p ratio of approximately 0.3 (see fig. 6 ). unlike its responses to high-contrast stimuli, responses of a v1 cell to low-contrast drifting gratings of non-preferred orientations are very close to its spontaneous activity, bringing the o/p ratio down to almost zero. the o/p ratios at low contrast are 0 and 0.15 for cortical cells with nlif and poisson afferents, respectively. again, a more regular thalamic input (i.e., nlif) gives rise to a more selective cortical cell, albeit to a lesser degree."
"the odd order theorem asserts that every finite group of odd order is solvable. [cit], with a proof that filled an entire issue of the pacific journal of mathematics. the result is a milestone in the classification of finite simple groups, and was one of the longest proof to have appeared in the mathematical literature to that point. subsequent work in the group theory community aimed at simplifying and clarifying the argument resulted in a more streamlined version of the proof, described in two volumes [cit] . the first of these, by h. bender and g. glauberman, deals with the \"local analysis\", a term coined by thompson in his dissertation, which involves studying the structure of a group by focusing on certain subgroups. the second of these, by t. peterfalvi, invokes character theory, a more \"global\" approach that involves studying a group in terms of the ways it can be represented as a group of matrices."
"finding out how the lgn processes visual information and determining what is the nature of the lgn's intricate circuitry have garnered great interest for decades [cit], among others) . yet it remains a challenge to characterize by computational modeling what is important for visual function in the lgn input to v1. to describe all known ion channels and synaptic inputs of an lgn neuron is computationally demanding; the accompanying dynamics may prove too difficult to simulate and/or interpret. consequently, researchers have spent considerable efforts to find a minimal model of lgn cells (and the network that drives them) that can realize details of their dynamics."
"first, the mean rates of both maintained and visuallydriven discharges strongly correlate with v thres and the noise shot size. by either reducing v thres or increasing the noise shot size, neurons fire more frequently. second, response variability increases as the noise shot size or v thres is increased. the fano factor of the visuallydriven discharge, in particular, is strongly influenced by the noise shot size. the model neuron fires most regularly when v thres and the noise shot size are both small. with a low spike-firing threshold, the constant input alone is strong enough to drive neurons to fire. at high v thres, firing events tend to be fluctuation-driven, and the stimulus-independent noise (controlled by the noise shot size parameter) plays an important role in both firing rate and response variability."
one should also note that it is generally believed that many v1 neurons in the input layer receive inputs from more than two lgn cells [cit] ). the effect we observe here will become gradually smaller as the number of lgn afferents per cortical cell increases. the dependence on the amount of afferent convergence needs further study.
"despite the fact that the nlif model is more realistic than a simple poisson point process, it is still only a simplified description of all known mechanisms underlying the neuronal responses of lgn cells. as alluded to previously, the nlif model fails to account for burst spiking activities. one remedy is to employ a more sophisticated lgn model that explicitly includes the 'burst' mode in the dynamics, e.g., the integrate-andfire-or-burst neuron [cit] . one should note that there are significantly fewer bursts in awake animals compared to the anesthetized. [cit], for example, report that anesthesia increases the percentage of bursts by roughly 3 times in macaque monkeys. as a result, the nlif model without bursts may be an adequate model for the lgn in awake animals. another improvement on the nlif model adopted in this paper would be to include a mechanism for the contrast-dependent non-linearity of visual responses. this non-linearity of lgn cells may be mostly a reflection of the retinal contrast gain control and can be modeled in the steady state as a functional of the spatio-temporal contrast [cit] ). last but not least, lgn neurons are more than just passive relays. anatomical data have shown that only 10-20% of the input to the lgn are comprised of retinal afferents [cit], for example) . neuronal responses of lgn cells are thus driven and modulated by a complicated feedforward (retina), feedback (from cortex), and recurrent (within the lgn itself) circuitry. further improvements on the nlif model of lgn cells could include more of this complex circuitry."
"our development includes more than 150,000 lines of proof scripts, including roughly 4,000 definitions and 13,000 theorems. the roughly 250 pages of mathematics in our two main sources [cit] translate to about 40,000 lines of formal proof, which amounts to 4-5 lines of ssreflect code per line of informal text. during the formalization, we had to correct or rephrase a few arguments in the texts we were following, but the most time-consuming part of the project involved getting the base and intermediate libraries right. this required systematic consolidation phases performed after the production of new material. the corpus of mathematical theories preliminary to the actual proof of the odd order theorem represents the main reusable part of this work, and contributes to almost 80 percent of the total length. of course, the success of such a large formalization, involving several people at different locations, required a very strict discipline, with uniform naming conventions, synchronization of parallel developments, refactoring, and benchmarking for synchronization with coq."
"decidable equality plays another important role in facilitating the use of subtypes. if a is a type, a subset of a can be represented by a predicate b on a, that is, a map b from a to the type prop. an element of this subset can be represented by an element a of a, and a \"proof\" p that b holds for a. the dependent pair a, p is an element of the dependent sum, or sigma type, σ x:a bx. as b takes values in prop, σ x:a bx is also called a subtype of a, for the reasons just described."
"as explained in section 4.2, data is sensitive to both the environment and the specific sensors. hence, we believe that if the sensor issues are properly addressed, the accuracy of the data could be significantly improved. therefore, our data processing is mainly focused on compensating sensor issues, such as sensors calibration and anomaly detection [cit] . in this section, we share a list of important findings encountered during the processing of the data."
"in the stage of data acquisition, data is only checked visually. gaps or certain patterns in data can indicate whether the sensors or network is working properly at some degree. however, it is difficult to compensate the data or to evaluate the data uncertainty when no ground truth is available. moreover, in the section 5, we will illustrate that processing methods can have different impacts on data uncertainties. therefore, processes and techniques are needed to understand the uncertainties so that some of them can be reduced."
"in this paper, we discuss a list of data issues that we encountered during our study and inference their possible causes. we further explain why some of the data issues are significant and cannot be compensated for by certain methods. the rest of this paper is organised as follows. we first introduce our deployments and the use of sensors in section 2. then, we discuss a list of data issues that occurred in different stages of sensing in an order of 1) the deployment of sensors; 2) the obtaining of sensor data; and 3) the processing of the data from the section 2 to 5. finally, we conclude this paper in section 7 with some of the open challenges for the environmental monitoring using low-cost sensors."
"the remaining nine sensors show a partial loss of the data, which have a similar pattern to figure 1 . figure 1 shows the completeness of data received in a week time from these nine elm sensors at wacl. the colour is associated with a percentage of data that have been received in an hour. light yellow indicates the data has been completely received (100%) and dark blue shows a complete data loss (0%). in the figure only a small percentage of data are completely received whereas the data loss can be observed frequently in different levels. such data pattern can be widely observed across all of our deployments despite the system having a mechanism to avoid the data loss as mentioned in section 2."
"micro-calibration, on the contrary, relies on reference sensors. however, this would require a reference sensor to be co-located with every low-cost sensor. since the requirement can be practically difficult, solutions that utilise a fresh calibrated sensors to propagate the calibration has been widely used [cit] ."
"another issue is the sliding window for the hourly average. even though a different starting point of a sliding window will not significantly affect the result, the result will still be affected by the use of different samples. hence, it is believed that if we know how the reference is produced, a better sliding window could be determined and the uncertainty can be minimised."
"macro-calibration utilises the consistency of nearby environment and maximises the similarity of measurements from the neighbouring sensors. this calibration requires the ground truth to be estimated from corresponding sensors. as mentioned in section 5.1, the estimated ground truth may have large uncertainties, which can further affect the result of calibration."
"as mentioned in section 4, it is important to have a ground truth for data processing. however, unlike in a lab environment, conditions in real environments do not have a control, which results in obtaining a ground truth in the environment of deployments difficult. in most practices, data from high-quality sensors is often considered as the ground truth and is referred to as the reference. since no sensor can obtain an absolute ground truth, such practice potentially biases the result of the process. however, to the best of our knowledge, there is no better solution. another issue of using high-quality sensors as references is the temporal resolution. due to the design of the sensor, the reference data is often provided on an hourly basis. consequently data obtained in a higher temporal resolution, like a minute time band, will require an aggregation process before it can be compared to the reference. as a result, the temporal scale of low-cost sensor can be affected."
"apart from the examples above, there are still many open issues that prevent low-cost sensors to be widely adapted for the urban environmental monitoring. we identify some of the core issues as:"
"sensors deployed at the fishergate failed to report data back to the server after the deployment. we identified that it was due to the effect of the metal cage. the cage that protects the sensors blocks all signals of the communication. for the deployment in wacl, 11 out of 20 sensors stopped getting measurements after 2 months of the deployment. it was due to the gsm service providers. those issues can results in a permanent loss of data and a physical inspection may allow the cause to be corrected."
"a deployment of sensors often has a purpose. in this work, we introduce three of our deployments in york, uk. [cit] when we first had the elm sensor. for this deployment, the aim was to understand the performance of elm sensors in an uncontrolled environment as we only have a datasheet describing how it behaves in a lab. we wanted to know if sensors can report accurate or consistent data in respect to high-quality sensors without or with a simple calibration. hence, twenty elm sensors were co-located with a reference sensor for more than two months at the wolfson atmospheric chemistry laboratories (wacl). the reference sensor is a high-quality sensor that is carefully maintained by wacl. this location is on the west campus of the university of york. since it is outside of city centre and away from major roads and junctions, the environment is believed to be consistent and considered as mild."
"as explained in section 4.2, the anomalies can potentially increase data uncertainty and hard to be compensated. as a result, detection and removal anomalies is another important process to ensure data quality."
"temporal dependency and spatial dependency are mostly used contextual information in the literature [cit] . however, they may not be ideal for our application. figure 6 shows an auto-correlation of no 2 between time t and time t − 1. it suggests that in our application the temporal dependency can be weak, which suggests that utilising temporal dependency can be insufficient for anomaly detection in our application. figure 7 shows the data of no 2 from wacl and fishergate respectively, at each location an elm sensor is co-located with a reference. comparing the sensors at wacl with those in fishergate, it is believed that utilising spatial dependency for the detection of anomalies is also inappropriate as the spatial variation can be significant for certain parameters."
"in the second deployment, the magnitude of measurement is constantly and significantly higher than the reference, especially at the a high temporal resolution. apart from low-cost sensors having a lower accuracy, we believe the difference in measurement is also associated with the dynamic of the environment, such as emissions from vehicles. since low-cost sensors sample the environment more frequently than high-quality sensors, every 20s, low-cost sensors are more affected by the dynamics of the urban environment."
"the detection of anomalies can be difficult for the environmental data. for example, we can observe a few spikes in the figure 2 . however, we would not confidently classify them as anomalies since those spikes may also be introduced by events at various levels or time-bands [cit], such as spikes caused by a bus idling near a sensor (the minutes time band), york race days (the day time band), and roadworks (the week time band). as a result, knowing environmental conditions can be important for any further data processing."
"the authors in [9, [cit] suggest utilising the cross-sensitive parameters, temperature and humidity in the calibration as the response of sensor can be closely associated with those variables. their result shows the calibration errors are significantly reduced. however, it is noted that no two sensors are identical, hence, it is necessary to calibrate every sensor in the network. furthermore, locations would affect the calibration as the environmental conditions can vary in different places. as a result, having the ground truth for the calibration would be an issue."
"detection of anomalies has been an active research in many domains. since anomalies in our dataset may not have an unique pattern as explained in 4.2, many well-known methods or techniques that rely on data pattern to distinguish anomalies are not applicable. for example, supervised learning methods may face issues when the anomalies are not correctly labelled in the training phase; or threshold based methods may be difficult to accurately classify the anomaly as anomalies do not have an unique data pattern."
"it is noted that elm sensors were commercially available. hence, the reliability of the system in terms of data communication should be better than many open-source systems. however, data gaps can be observed across all sensors. it suggests that data communication for wsn in urban environment may still be an issue."
"existing studies that utilise macro-calibration often require a ultra dense network [cit] . however, considering the issues discussed in section 2, it can be difficult to deploy a network to fit the requirement of calibration in cities."
"furthermore, the propagation of calibration will also require similarity or consistency of the environment. as a result, it may face to a similar problem that discussed in section 5.3.1. some suggest using the calibration function obtained during the night time as the air pollutant is more spatially consistent than during the day time. however, the pattern of pollutant in a day and night time can be different as shown in figure 4 . the figure 4 shows the measurements of o 3 in the day and the night respectively. the measurements were averaged into hourly basis using median value. it suggests that the measurements are generally lower during the night than the day. as a result, the calibration function determined using the night time data may not be optimal for the data from day time [cit] . ideally, every sensor will undertake a series of lab tests. in the lab, the environment is encapsulated and sensors will not be influenced by other gases or conditions. however, since sensors in real practice will be exposed in an environment that contains multiple gases and some of them can react each other under certain environmental conditions, calibration of sensors in a real environment can be different from the lab."
"a conventional way to calibrate a sensor in real environments is to determine a predictive model between an uncalibrated sensor and its reference. however, due to cross-sensitivity and other unknown reasons, the correlation between the data from low-cost sensor and the reference is often weak [cit] . as a result, the calibration can be difficult and the calibration errors are often large. figure 5 shows an example of a low correlation between an elm sensor and a reference. it suggests that a more comprehensive calibration method may be required. scatter plot between a reference sensor and a low-cost sensor"
"real data suggests that existing contextual information may not be applicable for our application. since there is a lack of studies in understanding anomalies in environmental data, it is still not clear what contextual information is relevant or should be utilised for the detection of anomalies of a particular parameter."
"for all three deployments, like most of end-users, we do not have a direct access to the hardware during and after the deployments. the data is obtained through the service providers and downloaded from their server directly via the api [cit] ."
"we believe that having a transferable calibration model can be significantly useful for the calibration of networks. ideally, the transferable model is able to obtain an calibration function in one location, e.g. in a place where the reference is available. then, the calibration function is still working when sensors are moved to a different location. as the sensor response is closely associated with different environmental conditions (e.g. different concentration and combination of gases) and various environmental variables (e.g. vehicles, wind, sunlight), if we can identify how the sensor response is affected by those variables and conditions, the transferable model can be obtained by automatically adjusting the differences."
"furthermore, the detection of anomalies in the data can be another issue. currently, we often do not know why and how an anomaly is caused. hence, distinguishing anomalies from real events faces a high false positive rate, which can have a significant impact on data analysis. for example, if data caused by an event is wrongly classified as anomalies, a decision may be taken differently. therefore, a better understanding of anomalies and their causes is also important."
"for some applications, the ground truth is also suggested to be obtained using statistical estimation, like macro-calibration that will be discussed in section 5.3. however, comparing the ground truth that obtained from high quality sensors, such approach introduces more uncertainty and has less accuracy. hence, using an estimated reference is not suggested if a high-quality sensor is available."
"the place of the deployment is often determined in advance according to the purposes, however, real deployments are often constrained by practical limitations which can be unforeseen in a planning phase. in this section, we share a list of issues that we encountered during the deployments."
"in this section, we discuss issues that we encountered during the data acquisition. the process of data acquisition collects data from the deployed sensors for the further processes. at this stage only the data pattern is checked by visual inspection. the data pattern is classified as the normal and abnormal based only on domain knowledge. an abnormal data pattern is often associated with the issues in networks or sensors, such as data gaps caused by the communication issues and constant values caused by malfunction of sensors [cit] . the abnormal data pattern often suggests that a physical inspection of sensors or networks may be needed. the normal data pattern indicates that the monitoring networks is working properly in the system level, e.g. the communications. however, it would require further processes to evaluate the accuracy of the data."
"for the detection of anomalies, understanding the pattern, magnitude and percentage of anomalies can be important. as our sensors are deployed in real environments which means that the ground truth of anomalies may be unavailable, it can be difficult to accurately label or deeply understand anomalies. as a result, most of quantitative analysis for anomaly detections are evaluated on an artificial data. however, since the understanding of anomalies from environmental data is not comprehensive, the artificial data may not fully simulate the reality, which leads to an issue of detecting anomalies in real applications."
"data uncertainty describes how data vary from the ground truth. since data reflects the variations from the sensors and the environments, we believe a higher variation of a sensor will lead to a higher data uncertainty. low-cost sensors are widely reported to have a larger variation (e.g. due to low sensitivity and selectivity) and the response of the sensor is easier to be affected by the environment (e.g. due to cross-sensitivity). hence, data from low-cost sensors often have a larger uncertainty."
"for the techniques of the data aggregation, arithmetic mean and median are often used. the arithmetic mean is the most often used technique to average the data. it is the sum of received values divided by the number of received values. however, it is noted that the arithmetic mean is sensitive to the sample size (the number of received values). considering the number of samples in a window of an hour can be non-unique due to the data gaps, using arithmetic mean can result in the confidence interval of the mean value from each hour being different. moreover, the mean is also sensitive to extreme values. for example, in figure 2, if all observed spikes were anomalies, then the mean value could be largely influenced by the anomalies. however, it does not mean using median value is always a better option than the mean. as median value is a single value, median value will not represent for other samples. if the spikes do have an meaning, taking the median value may not be appropriate as it ignores all information from the spikes. moreover, if a percentage of anomalies is more than 50% on an hourly samples, the median value is then biased."
"the processing of data aims to improve data quality to a standard. the data quality is often defined by end-users indicating whether the accuracy of the data is sufficient for their processes. therefore, ideally we would know the uses of the data from end-users and optimise the techniques accordingly. as we do not know the uses, our aim of the data processing becomes to improve the data quality by maximising the accuracy of the data with respect to the reference."
"the sensors are designed to have a life time for about 18 months as some of the sensors provide their data via chemicals that degrade. data, by default, is uploaded to a server using gsm. however, when the gsm service is not available, data is temporally stored (within the limits of available resources) in an on-board data logger and uploaded again when the gsm communication recovers."
"finally, depending on the type of the network, such as flat and hierarchy network, the strategy of data aggregation can be different. it is also worth considering how to archive the data and how to balance the trade-offs, such as trade-off between the communication cost and amount of data to be transmitted."
"the first issue is the location of deployments in terms of spatial locations. elm sensors are powered by mains electricity instead of battery, which is believed to address the power limitations that many low-cost sensors have. however, our deployment is then constrained by the power supplies. we determine the best practice for a consistent ac supply across a city is to utilise lamp-posts. however, locations of the sensor deployment are then constrained by the availability of lamp-posts in the city. lamp-posts in york are managed by third parties, hence, deploying sensors on lamp-posts requires their cooperation. furthermore, getting permissions from the local council can also be a time consuming process. therefore, sensor deployments in urban environment can be very expensive in terms of both labour costs and arrangement."
"in the first deployment, temperature and humidity show a consistent measurement over the number of sensors. but the measurements of no 2 and o 3 have a large variation among the different sensors. considering the distance between sensors are within 5 meters range and the environment condition is relatively consistent, we suspect the inconsistency of data was caused by sensors."
"from those deployments, we observe various data issues and some of them can easily be compensated. however, others are more difficult to address. for example, the calibration of the networks can be one of them. as explained in section 5.3, the propagation of calibration function in a network relies on the consistency of the environment. however, an urban environment is dynamic, which would result in large calibration errors in the result. on the other hand, it is also practically impossible to have reference sensors co-located with every low-cost sensor in the network. therefore, the calibration of networks is still an open challenge."
"the main purpose of data aggregation in this work is to average data with a higher temporal resolution into the same resolution as the reference (hourly) for the evaluation. data aggregation can be generally classified as an on-line or off-line process. an online process stands for the data is aggregated on the sensor before it is transmitted to a server. an advantage of this process is it significantly reduces the amount of data that needs to be transmitted and saves the costs of communication. however, such a process is not reversible, which means that if the aggregation process makes inappropriate transformations they cannot be recovered. on the contrary, an off-line process transmits everything back to a server. as a result, raw data can be securely stored and can be recovered at any time when it is necessary. since our sensors are powered by ac, the power is not an issue. hence, we utilise an off-line process in our study. it is noted though the off-line process can make the errors due to communications worse as discussed in section 4.1."
"sensor ageing and material degradation can occur in all sensors at different stages, and they are believed to influence the response of a sensor and further affect data uncertainty, like a drift of measurements. figure 2 shows a week long measurement of no 2 from a elm sensor at the fishergate. in the figure, a large variation of data can be observed. since it is impossible to separate the variations that caused by the sensor and the environment, it would not be possible to identify the drift of sensors or to know the drift rate. it shows an importance of having a ground truth during the deployment. the ground truth will indicates the variation of the environment. hence, the variation of the sensor can be subtracted from the data."
"the biggest issue in using calibrated sensors to propagate the calibration is the calibration errors as calibration errors of individual sensor will propagate through the calibration path. as a result, the error of micro-calibration will be closely associated with the calibration of errors of individual sensor and the size of the network."
"the third deployment was in cooperation with arup to evaluate how green infrastructures in an urban environment would impact the micro-environment. eleven elm sensors were firstly placed at the wacl for a month co-located with a reference sensor (similar to the fisrt deployment) so that some calibration data was available and then moved to scarcroft road, york. in this deployment, sensors were spread around the area, some sensors were on the road side while others were placed in a green park. no reference sensors were available on site. [cit] for more than 8 months."
"sensors may have non-unique responses in different conditions of environments [cit] . hence, the aim of the second deployment was to understand how elm sensors would perform in a typical urban environment and to determine how the response of sensors would differ from in the mild environment. this deployment was at the fishergate, which is in the centre of york next to a busy junction. this environment is therefore considered as harsh. at the fishergate, two elm sensors were co-located with a high-quality reference sensor for more than 8 [cit] . the reference sensor is managed by the city of york council and it is a part of automatic rural and urban networks (arun)."
"since ideal locations of deployment can often be constrained by the practical limitations, obtaining of data in an desired location can be an issue. furthermore, some methods of data compensation require the network or sensors in a certain topology or in a certain spatial and temporal range [cit] . as a result, those methods may be difficult to apply in such circumstance."
"current air pollution mitigation strategies require the air to be monitored in an appropriate spatial and temporal scale [cit] . high-quality sensors installed for regulatory monitoring purposes often have a high market price and demand frequent manual handling such as re-calibration. these constraints make current sensors prohibitively expensive to deploy, especially at a higher spatial resolution. therefore, an alternative solution is urgently required. as permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. low-cost sensors are defined as electronic sensors that cost several orders less than high-quality sensors. as a result, low-cost sensors are able to construct a higher density network with a relatively low cost, which further enables data to be obtained at the sufficient spatial and temporal resolution and addresses the issues that the current monitoring networks are having [cit] . however, there are many challenges that make it difficult to obtain useful information from these low-cost sensors. it is noted that in this paper information is considered different from data; with data being what is sensed and information being resulting models after processing upon which decisions may be taken."
"an interesting observation during the deployment is the data gaps in wacl occur more frequently during the university term time than the off-term. this relationship suggests that some of the data loss can be associated with the volume of communication traffic. since during a term time more gsm users will share a limited bandwidth, the data collision is likely to occur. however, as we don't have the direct access to the hardware, we haven't had sufficient evidence to confirm this."
"another issue is the location of deployments in terms of height. according to the regulatory, the air-intake of a high-quality sensor, which is used as a reference in this work, should be placed at 1.60 meters above the ground. this height is to determine pollution exposures for adults. other studies suggest to place sensors lower as children being more vulnerable than adults are far below that height. however, our deployment failed to meet either of the requirements. since we do not know how local community would react to the deployment and we have to prevent the sensors being physically damaged, sensors on lamp-posts are placed at three meters above the ground and sensors on top of high-quality sensors are locked in a metal cage. we are aware that our deployment indirectly ignores the variation of the height. however, considering the effect of the height is not our main interest and to the best of our knowledge no studies have shown it is significant, we did not investigate it further."
"since the sensors are designed to provide data in a consistent time interval, any gaps in the data are considered as abnormal. we have an omission failure that causes a complete loss of the data. then, long terms and short terms data gaps can also be observed across all deployments."
"in the third deployment, the calibration functions determined at wacl was soon found to be invalid in the deployment. it is because some of parameters change with time and space. for example, the temperature is different between summer and winter; the no 2 concentration is inconsistent between mild and harsh environments."
"furthermore, environmental interference or other unknown issues can cause sensors to misbehave temporally, which results in abnormal data patterns (e.g. spikes) and affects data uncertainty. the random data patterns caused by sensor issues are referred to as anomalies in this work. for example, an unexpected spike caused by communication interference in the continuous data is considered as an anomaly. since anomalies in the data can have random patterns, compensation of anomalies can be difficult in practice. therefore, identifying anomalies and removing them can be important for reducing data uncertainty."
"new smartphones such as galaxy s3, galaxy nexus, iphone 5 and later models have an integrated barometer sensor in the phone which passively measures atmospheric pressure. slight variations in atmospheric pressures can be detected by these algorithms to detect work done against gravity, hence improving the results. another motivation behind our work is to develop a practical framework for eee estimation. existing accelerometry equations require heavy computations or require high sampling frequency, either of which will drain the battery of smartphones quickly."
"henry and ramirez-marquez 26 use three system states -stable original state, disrupted state and stable recovered state -to quantifying system resilience. the advantage of this metric is that both the disruption and recovery phases are considered. similar time-dependent metrics have been adopted in other studies. [cit] however, these time-dependent metrics perform poorly in representing the global level of system resilience intuitionally in comparison to metrics presented as a single numerical value."
the rest of this article is organized as follows. section 'development of improved integrated metric' describes the development of the proposed integrated resilience metric. section 'experimental comparison' presents the comparison of the proposed metric with previously reported ones using three scenarios. section 'case study on information exchange in networked system' presents a case study on information exchange in a networked system to demonstrate the application of the proposed metric. section 'conclusion' concludes this article.
"however, the quantification of the time factor in a relative scale may incur problems because many realistic systems are only concerned with the absolute time scale. figure 3 illustrates the system performance in two cases with different time periods. in case 1, the dynamic performance is carried out in the same manner as that in case 2; however, the time taken in case 1 is only half of that taken in case 2. by fixing the other conditions for both cases, it is natural to conclude that the system performs better in case 1 in terms of system resilience because the resilience is rewarded when the period is shorter in the transition process. however, for metrics using the relative time scale, a conclusion that the two cases have the same resilience would be obtained because the time difference between the cases cannot be reflected. consequently, metrics that adopt relative time scale factors are not appropriate in such a scenario."
"where, ω i,j denotes weight between link i and j; all the inputs to a node are summed and passed through transfer function γ. input layer neurons uses tansig (tan-sigmoid) transfer function."
"the number of input layers is determined by the modality of x i.e. the number of feature vectors extracted from accelerometer and barometer data. we use one hidden layer, composed of simple elements (called neurons) and each neuron uses a non-linear transfer function to map inputs into outputs [cit] . the connections between neurons largely determines the network function. one can train a neural network to perform a particular function by adjusting the values of the connections (weights) between elements. the final layer produces the ann's output. the output of a feed-forward neural network with one hidden layer and one output neural network is given by"
all the three metrics are derived from the same notional performance data shown in figure 1 . the absorptive and restorative capacities are considered as measures of system resilience and are quantified using performance data. all the factors are integrated into a single metric.
the system performance y(t) is measured as the total number of messages received in a network at each time step in a simulation. the system performance data are then used to calculate the system resilience using the proposed resilience metric.
"the minimum performance y min is commonly used to evaluate the absorption ability. 13, 30 hence, the disruption consequence factor s d is obtained by normalizing y min with the desired performance level y o, which is used as a benchmark. noticeably, y min is incorporated in both the disruption process factor and disruption consequence factor, but the multiple of two factors will not increase the effect of y min to the system. because the 'disruption process factor' is determined by the summation of performance data during the whole disruption process. though y min is incorporated in the summation of performance data, it is just a point data and has minor effect on the value of 'disruption process factor'. therefore, the effect of y min is mainly presented on the 'disruption consequence factor'."
"node removals and recovery actions are implemented within quantitative time steps. for instance, a disruption event lasting from time t d to t r may include several instances of node removals. the occurrence frequency of node removals, time point of occurrence and number of nodes removed for each occurrence can be adjusted."
"this article presents an improved metric for the quantitative assessment of system resilience based on the system absorptive capacity and system restorative capacity. each capacity is evaluated from the perspective of state transition and quantified by the time, process and consequence factors. the proposed metric has three advantages. first, a new time factor is proposed and incorporated into the resilience metric to quantify the effect of time on system performance. second, system resilience is classified into two capacities based on the classification of the disruption and restoration phases. two weight coefficients are assigned to the two capacities, which enhance the flexibility of the proposed metric when the stakeholder has different requirements for the absorptive and restorative capacities. third, the numerical values of resilience under the proposed metric lie in a proper range and can be compared conveniently across different engineering systems. three scenarios are used to validate the performance of the proposed metric, wherein the metric is compared with two previously reported metrics. in addition, an example of information exchange networks is used to demonstrate the application of the proposed metric."
"node removals are considered as disruption events. nodes can be removed uniformly at random, that is, random node failures, or in a targeted manner, that is, intentional network attacks. targeted node removal is based on the node degree; nodes with the highest node degrees are removed each time. node degrees are recalculated following any changes to the network topology."
"based on the absolute time scale, a new time factor is proposed and incorporated into the resilience metric to quantify the effect of time on system performance. the absorptive and restorative capacities are used to quantify the proposed resilience metric in the form of a summation. two weight coefficients are assigned to the two capacities to enhance the flexibility of the proposed metric according to various system requirements of stakeholders. each capacity is quantified from the perspective of state transition because of the dynamic behaviour during the disruption and recovery phases. three dimensions -transition process, transition time and transition consequence -are used to describe the resilience capacities. the proposed metric is compared and discussed using experimental scenarios. a case study on information exchange in a networked engineering system is conducted to validate the proposed metric."
"in this section, we briefly introduce the two regression models we use in this work for eee using accelerometer and barometer data. the former is linear while the other is a non-linear model."
"the recovery actions considered in this study focus on link rewiring, wherein nodes affected by a disruption rewire any links disconnected by the disruption. two recovery strategies are considered: random rewiring and preferential attachment. in random rewiring, disconnected nodes randomly decide whom they rewire to. in preferential attachment, disconnected nodes decide whom they rewire based on the probability equation given by equation (12), which gives preference to highly connected nodes."
"researchers have used a sampling frequency of 10-800 hz [cit] for activity detection. however, studies have shown that 0.1-20 hz is decent range for most human activities [cit] . in this study, however, we restrict our measurements to the default smartphone sampling rate of 2hz which corresponds to low battery consumption and processing overhead. both accelerometer and barometer sensors are sampled at 2hz (corresponding to 2 samples per second)."
"where, researchers have reported 60-95% correlation using equation 1 for ambulatory activities such as walking or running. however, the performance degrades when used for activities involving change of altitude. we use this as fv(35) in our algorithm."
"extracting each feature vector from raw sensor inputs can be time consuming. particularly, on an embedded device like a smartphone, such operations may drain the battery."
"the total performance factor s is calculated using the entire performance data for both the disruption and recovery phases. the recovery factor r and absorption factor d correspond to our recovery consequence factor and absorption factor, respectively. as noted above, the recovery time factor t is calculated in a relative scale and is different from the absolute time factor proposed in this study. according to the author's description, the volatility factor z is adopted to measure the ability of a system to provide a smooth transition from a degraded state to a recovered state. for convenience, the volatility factor is not considered in this study."
our primary aim was to build an application capable of accurately providing eee without leveraging significant computational resources on the smartphones. low computational and power requirements will make such an algorithm more usable and attractive to consumers.
"fitbit is a highly popular commercial device which uses accelerometer and altimeter sensors to capture personal activity, a significant improvement over traditional pedometers. however, some experiments have demonstrated that fitbit is not very accurate as it lacks activity-classification algorithms [cit] . nike+ fuelband has the same limitations. exiting body sensor related energy expenditure estimation mostly employs a body-worn accelerometer and performs signal analysis to estimate calories expended in real-time using regression formulas. however, using a single sensor on the body is not enough to provide accurate measurement for body movement. instead, multiple sensors are needed to improve the activity estimation performance [cit] ."
"the performance data of the six scenarios during the disruption phase (0-50 time steps) in case 1 are generated with the same equation (10) . as a result, the performance lines with various markers are plotted coincidently, which resulted in such 'hexagon star' presentation. this phenomenon also occurs in case 2 during the disruption phase (0-70 time steps). figure 5 shows the calculated results of the three metrics for cases 1 and 2 with different recovered performance level k 2 . r tran, r nan and r i increase with k 2 in both the cases, indicating that the system resilience improves as the system performance is recovered to a higher level. for metrics r tran and r nan, the resilience results for cases 1 and 2 are almost the same, indicating that the time scale has no effect on the system resilience using the two reported metrics. however, for the proposed metric r i, the value for case 2 is smaller than that for case 1, indicating that the system resilience decreases as the time increases."
disruption process factor. the disruption process factor d d accounts for the total performance maintained by a system during the disruption phase. this factor is calculated as follows
"seven individuals participated in the tests. healthy male graduate students of different ethnic background from our lab contributed to these experiments and we ran multiple trials. the range of bodily features are as follows: weight (56-109 kg), height (173-184 cm), age (22-29 years) and bmi (18-36 kg/m 2 ). we obtained all the values and then extracted the feature vectors mentioned earlier. matlab and weka software tools were used for computational analysis. unlike, activity specific classification and eee algorithms [cit], our focus here is on designing a single robust eee algorithm, that can be applied to a combination of all regular physical activities in a combined manner. table 2 gives the performance results using artificial neural networks and simple linear regression models. ρ indicates correlation between predicted output and actual ee values. rmse is the root mean square error while mae is mean absolute error. raw accelerometer means that only the mean accelerometer values are provided as inputs to machine learning algorithm. 'all fv' refer to the case when all 35 fvs mentioned earlier are included in ann. it can be clearly seen that linear regression gives very poor performance in all cases. there is no improvement in linear regression performance with increase in feature vectors. thus, the utility of using non-linear models for regression is clear. using ann model, we are able to achieve 72% correlation with actual ee values with a rmse of 1.62 using only accelerometer equations. when all fvs are used, correlation increases to 88% and rmse reduces to 1.13. figure 1 gives a plot of output values using ann and linear regression, as compared to cosmed values. the errors are less in ann than by the linear regression model, and less in cases where more fvs are used."
disruption consequence factor. the disruption consequence factor s d accounts for the ability of the system to resist the loss of system performance after a disruption event. this factor is calculated as follows
"in this section, we present the development of an improved integrated metric for resilience based on system performance. first, we describe the system performance measure. second, we present the improved resilience metric."
"the main shortcoming of pedometers or any step-counting algorithms is their poor accuracy in detecting steps at slow speed and insensitivity to gait differences such as the length of the stride leading to unreliable estimation of energy expenditure. another approach is to use accelerometer values directly. the existing algorithms used to estimate ee from accelerometers attempt to find an empirical relation between accelerometer data and energy expenditure data measured by a calorimeter, e.g. cosmed k4b2."
"simple linear regression is the least squares estimator of a single explanatory variable. it minimizes the sum of squared vertical distances between the observed responses in the dataset and the responses predicted by the linear approximation. the resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side. if x denote the vector of inputs (obtained or derived from accelerometer and barometer readings) and y denotes ee obtained using cosmed calorimeter, y denotes eee values obtained from the model:"
"we first profile the different fv extraction algorithms in terms of their computational complexity. since the exact speed of computation is device dependent, we report relative speed (time of execution relative to time of execution of raw acc. values). the values are reported in table 5 . these computations are performed with a desktop processor running at 2.6 ghz and averaged over 200k computations. we show relative performance trend which should scale well to mobile processors. next, our goal is to prune the fvs with higher computational cost without sacrificing the accuracy of eee."
"disruption time factor. the disruption process factor and disruption consequence factor can capture the absorptive capacity of system resilience effectively from the perspective of system performance. however, a measure from the perspective of time has not been considered, although it is another necessarily significant aspect for assessing system resilience. various measures have been introduced to quantify the time effect on system resilience. for instance, a time factor has been represented by a function of time 30, 34 or incorporated into a function of both time and performance. 13 previous studies used the relative time scale to quantify the time factor. the advantage is that the time factor becomes dimensionless. thus, the factors derived from both the time and performance dimensions can be integrated into an integrated metric, in which the performance factor is also dimensionless."
"by considering these three aspects, both the absorptive and restorative capacities of the system resilience are measured by the process, time and consequence factors. the disruption process factor d d is used to capture the dynamic behaviour of the system performance during the disruption phase. the disruption consequence factor s d is used to capture the absorption ability of the system. the disruption time factor r d is used to capture the absorption rapidity during the disruption phase. similarly, the recovery process factor d r, recovery consequence factor s r and recovery time factor r r are used to capture the dynamic behaviour, recovery ability and recovery rapidity of the system, respectively, during the recovery phase."
"2. multiple trials were conducted over 7 individuals and validated using cosmed k4b2 calorimeter. we can obtain high correlation using basic features and low sampling frequency, which will lead to battery efficiency."
"before going into the description of the methodology in details we would like to point out the scopes and limitations of our described model. first and foremost, our analysis have been built and analyzed on the basis of the most basic activities of a normal human being. the results can be extended to other physical activities like running, biking, etc. secondly, our proposed model requires an individual to carry a smartphone at all times. this can be problematic as a smartphone may not always be carried by individuals and the sensor location will not always be known. recognizing the activity type with a non-fixed location of sensor on the body is complex."
"next, we define the additional fvs we derived from triaxial accelerometer data. these features have been useful in human activity recognition and possibly also improve accuracy in our scenario [cit] . these are termed as derived fvs (fv 10-34)."
"in this work, we proposed usage of the accelerometer and barometer body sensors of smartphones for accurate eee in ambulatory settings. to emulate a practical setting, we used a smartphone sampling accelerometer and barometer sensors readings at 2hz only. we then used these values to obtain fvs and fit an ann which can yield up to 89% correlation and rmse of 1.07. with very small computational over- head. we observed significant benefits in fusing the input of barometer sensor to an accelerometer sensor as it allows, with use of simpler fvs, achievement of higher correlation and accuracy. the algorithm we have employed here for our prediction model, ann, is relatively slow when it comes to building the model. however, the smartphone, will be trained using this model offline and the actual prediction in real-time will be fast and hence, not energy extensive. however, for building the model in real time, decision trees can be used."
"another critical task is to measure accurate ee values. direct calorimeter [cit] requires observations in a confined metabolic chamber and is therefore impractical in our scenario. doubly labeled water techniques are inappropriate because they calculate ee over a long duration instead of for a single activity. to calibrate exact energy expenditure values, we used cosmed k4b2 [cit] indirect calorimeter, which is portable and can be used with our setup."
"noticeably, the measure of the restorative and absorptive capacities are almost symmetric. each capacity is measured in both the time and performance dimensions using three quantified factors. the three quantified factors are all normalized and dimensionless and have the same reference value of one during normal operation. hence, it is easy to integrate the three factors to quantify the restorative and absorptive capacities. furthermore, no overlap occurs among the three factors."
"the experimental results validated our assertion that barometric sensor (bar.) has high correlation with eee accuracy. appending the mean of barometer values (µ p ) improve the correlation of eee to actual energy expenditure from 71% to 83% as shown in table 4 . however, the results can be further improved using other fvs. 'all fv' refer to the case when all 35 fvs mentioned earlier are included in ann."
"the term 'resilience' was first proposed by holling 1 in the study of ecological systems. since then, the concept of resilience and its evaluation methods have been developed and applied to many real-world complex systems such as national infrastructure facilities, 2-4 ecological systems 5, 6 and economic systems. 7, 8 in addition, resilience has expanded the library of system attributes such as reliability, robustness, safety and risk. resilience is commonly studied to assess and improve the capability of a system to bounce back from disruptive events to its normal condition. 9, 10 currently, the development of resilience in complex engineering systems is still in its early stage. several reported works have made some progress in defining and evaluating resilience for complex engineering systems."
"considerably higher than that for the other three cases. this is because during the entire disruption phase, only 25 nodes are removed in case 1, whereas 50 nodes are removed in cases 2-4. for cases 2-4, the resilience in each scenario follows the order case 2 \\ case 3 \\ case 4, indicating that the case in which r decreases linearly with the occurrence frequency of node removal has the highest resilience."
"where k 1, a 1, b 1 and x 1 represent the desired performance level, minimum performance, inflection steepness and location of the inflection point on the tàaxis, respectively. a recovery action for the system is represented using notional performance data generated by the following equation"
"in this section, we present our results using ann and linear regression models on data collected from field experiments. the smartphone sensors logged their data using androsensor app into a csv file while cosmed k4b2 calorimeter was used to validate the readings and measure actual ee. the smartphone was held in hand by the participants. for each participant, the following set of ambulatory activities were designed:"
"where d nan, rapi rp, rapi dp, tapl and r nan are the robustness factor, recovery rapidity factor, disruption rapidity factor, time-averaged performance loss factor and recovery ability factor, respectively. the detailed formula for each factor is given as follows 13"
"recovery process, consequence and time factors. the three aspects of the recovery capacity, that is, the recovery process factor, recovery consequence factor and recovery time factor, are defined in a manner similar to those of the absorptive capacity. the recovery process factor accounts for the total performance maintained by a system through the recovery phase and is given by equation (5) . as shown in equation (6), the recovery consequence factor is defined as the ratio of the recovered performance level to the desired performance level. the recovery time factor is defined in a similar manner to the disruption time factor, except that the recovered time (t s à t r ) is placed in the numerator, as shown in equation (7). this adjustment for recovery time factor is adopted to reward quick recovery action"
"we use artificial neural network (ann), a non-linear, non-parametric and data driven machine learning approach in addition to simple regression technique. these non-linear techniques have been successfully used in a number of domains [cit] for successful prediction. inspired by biological nervous systems, anns are simplified representations of the model used by human brain for intelligent functions."
"accelerometer based algorithms have found high degrees of correlation with eee in scenarios such as walking, running and standing. however, active lifestyle often involves climbing up or down stairs. in these scenarios, accelerometer or pedometer based approaches tend to be inaccurate. for example -in a sample trial we asked some volunteers to climb up 4 flight of stairs and then to climb down the same number of stairs. the eee obtained using commercial products such as fitbit and nike+ fuelband (which use pedometer based approach) are shown in table 1 . it is counterintuitive that one will spend more calories climbing down than upwards. the algorithms used in these devices appear to count steps and speed of the movement and attribute higher expenditure based on these variables. given that our volunteers moved faster when climbing down stairs versus up stairs these devices measured higher caloric expenditure for the less vigorous activity of climbing down versus up."
"the most accurate way to measure energy expenditure (ee) is to use direct or indirect calorimeters, however these apparatus are not conducive to track daily intake and expenditure. cosmed k4b2 calorimeter uses pulmonary gas exchange to measure caloric expenditure with a very high correlation of 98.2% [cit] but is impractical for use in daily life because of the high cost, complexity and difficulty of use [cit] . pedometers and accelerometer based approximation algorithms offer an alternative solution that is gaining popularity. many wearable devices, such as fitbit, jawbone up and nike+ fuelband provide a practical solution to monitor the dynamic energy expenditure by unobtrusively collecting data required to make eee. in our objective trials, we found many of these devices were accurate in step counts but inaccurate in eee. additionally, people need to purchase and carry these devices with them all the time to get a comprehensive assessment of energy expenditure value. 7.82 ± 2.85 9.14 ± 2.92"
the rest of the paper is organized as follows: section 2 gives an overview of related works in this area. section 3 discusses the methodology used to process sensor data from the smartphones. section 4 gives a brief summary of the prediction models used in the paper followed by experimental results in section 5. section 6 states conclusions and discusses directions for future work.
"motivated by these strong results, we plan to collect a more extensive dataset, using a higher number of individuals, along with other physical activities like biking and running. using this dataset, we wish to build a more representative model for eee (using artificial neural networks or other machine learning algorithms like decision tress), which will improve eee accuracy. another direction of future work involves building a smartphone application which can be used to accurate estimate energy expenditure of individuals by using artificial neural networks, without draining smartphone energy."
"heart rate monitors have been used as stand-alone devices or along with accelerometer sensors to collect data and predict energy expenditure. some devices such as wahoo heart rate monitor, acquire heart rate data by measuring pulse rate and use a linear relation between heart rate and oxygen uptake to predict energy expenditure. however, heart rate monitors have low accuracy during sedentary behavior and require individual calibration [cit] ."
"for the metric r nan, the resilience follows the order case 1 \\ case 4 \\ case 2 \\ case 3. the metric value for case 3 is maximum because it takes the most time to reach the lowest level and takes the least time to recover to the desired performance level. similarly, the metric value for case 1 is minimum because it takes the least time to reach the lowest performance level and takes the longer time to recover to the desired performance level. in general, the metric r nan reflects the trends of the four cases in terms of resilience evaluation. however, the recovery rapidity factor rapi rp and disruption rapidity factor rapi dp are integrated into the metric r nan in a form of ratio, which is prone to causing fluctuating ranges beyond [cit] and disadvantageous for resilience evaluation. for instance, case 3 obtained a resilience value of 2.167."
"it is not possible to obtain second by second eee from commercial devices such as fitbit or nike+ fuel band. however, we did calibrate these values before and after each trial. we present the summary results in figure 2 . the errors in individual measurements seem to sum up and ce algorithm (calorimetry equation used in calfit) presents an estimate which is within 25% of the cosmed values. ann values are within 10% of the range of cosmed values. we can see that nike+ fuelband tends to underestimate the ee while fitbit tends to overestimate the value. the error bars in the figure show the standard deviation for each device/ algorithm. fitbit has an abnormally high deviation. our algorithm has a smaller deviation over the population considered, which is comparable to actual cosmed values."
"for the metric r tran in the three cases, the absorption factor r, recovery factor d and recovery time factor t are equal because parameters fy o, y min, y s, t d, t r, t s g are the same. as the sum of the performance data for the three cases is the same in this scenario, the resilience values calculated by the metric r tran are equal too. similarly, the above analysis also applies to the results obtained by r nan . hence, it can be concluded that the dynamic process presented in figure 6 cannot be properly captured by the metrics r tran and r nan ."
"with smartphones becoming ubiquitous devices, we assert that they are the most convenient devices for eee, rather than introducing than introducing dedicated wristbands, heart rate monitors or other tracking devices. however, work needs to be done to improve eee accuracy using smartphone sensors. accelerometry equations don't work well in climbing upstairs / downstairs where altitude change is involved."
"the system performance is assumed to be at a normal operating level before a disruption event occurs. when the disruption event occurs at time t d, the system performance begins to drop until the minimum performance is reached. the time period from t d to t r is regarded as the disruption phase. after time t r, the system begins to recover until a stable level is reached. similarly, the time period from t r to t s is regarded as the recovery phase. therefore, the system's dynamic performance of interest is divided into two phases (disruption and recovery phases) according to the occurrence point of the disruption event and recovery action. noticeably, the disruption and recovery phases correspond to the absorptive and restorative capacities of system resilience; this correspondence contributes to the convenience of classifying and quantifying the two capacities."
"the system of interest can be represented as a network, which consists of nodes and links. nodes are individual members within a system, and links are potential paths of information flow. a model based on the one adopted in previous studies 30, 36 is used to simulate information exchange in a network. the goal of the networked system is to successfully enable information sharing between nodes during the operation period."
"note that when a and b equal 0.5, it does not indicate that the absorptive capacity and restorative capacity are treated equally. this phenomenon can be explained as follows. though the areas under performance lines are almost the same, the performance line in disruption phase (recovery phase) is not linearly related to the absorptive capacity (restorative capacity). as a result, the resilience values, which are a summation of absorptive capacity and restorative capacity, are different for the three cases. scenario 3. scenario 3 considers the performance data for four cases, as illustrated in figure 7 . for cases 1-3, all factors except the initial moment of recovery time are the same. case 4 undergoes the same disruption phase as case 2 and the same recovery phase as case 1. the disruption time for cases 1-3 follows the order case 1 \\ case 2 \\ case 3, indicating that the absorptive capacity for case 3 is the highest. similarly, case 3 has the highest restorative capacity among cases 1-3 because it takes the least time. table 3 lists the resilience results calculated by the three metrics. the resilience values obtained by the metric r tran remain the same for cases 1-3. this metric fails to represent the dynamic process as long as the table 2 . resilience calculation for scenario 2 using the three metrics. summation of the performance data remains constant. however, this conclusion is improper for systems where the absorptive and restorative capacities are differently treated."
"scale-free network is adopted for network topology initialization because of its existence in many real-world complex systems. 37, 38 the scale-free topology is created using the baraba´si-albert (ba) preferential attachment model. 39 the ba model begins with a small number of table 3 . resilience calculation for four cases using three metrics. initially connected nodes (m 0 ). the network grows by adding one node to the network at each step, where each added node links with m existing nodes in the network. the probability p(k i ) of a node with degree k i being linked is proportional to the degree of that node according to the following equation"
"obesity is an epidemic both in the united states and all around the world. it is predicted to be the number one preventive health threat in the future [cit] . recent estimates indicate that two-thirds of u.s. adults are overweight. poor dietary habits and lack of physical activity are two main contributors to this growing health crisis. new smartphone applications and research projects aim at helping people track their daily food intake [cit] and a number of smartphone apps are available for consumer download. it is generally very difficult to know exactly how many calories people exhaust during daily physical activity as it depends on the age, gender, weight, height, type and intensity of activity."
"when a disruption event or recovery action is applied to a resilient system, the system will transition from one stable state to another stable one. normally, three important aspects are considered during the transition period. the first aspect is the manner in which the transition is carried out, which describes the transition process of the system performance. the second aspect is the level to which the system state reaches compared to the initial level. the last aspect is the time taken to finish the transition process. once these three aspects of the transition period are determined, the absorptive and restorative capacities of the system can be quantified."
"moderate and vigorous physical activity can lead to health promotion and disease prevention. increased portion sizes and high caloric intake are great contributors to overweight and obesity. provision of tools to accurately measure eee would allow people to actively track expenditure of calories relative to the amount of calories ingested, creating awareness of personal habits that can be modified to promote personal health."
"where s, r, d, z and t are the total performance factor, recovery factor, absorption factor, volatility factor and recovery time factor, respectively. the detailed formula for each factor is given as follows 30"
"as previously mentioned, resilience is rewarded when the time period is shorter in the transition process. when the other conditions are kept constant for both the cases, the system performs better in case 1 in terms of the system resilience. thus, the relative time scale factor in r tran and r nan is not able to capture this trend. in contrast, the absolute time scale factor incorporated in the proposed metric overcomes this drawback. scenario 2. scenario 2 considers three cases for the performance data, as illustrated in figure 6 . the desired performance level y o, minimum performance level y min, recovered performance level y s, initial disruption time t d, initial recovery time t r and time to reach the steady performance level t s are all the same for the three cases. the only difference lies in the dynamic process during the disruption and recovery phases. table 2 lists the resilience results calculated by the three metrics. the results of r tran and r nan are the same for the three cases. the metric r i provides various resilience values for the three cases under varying weight coefficients."
"the summary of these results is given in table 6 . the correlation of eee using raw accelerometer values increases by up to 15% using raw barometer sensor values. similarly, barometer sensor value impacts performance with simple fv and moderate fv by 21% and 24% respectively without we are now in a position to recommend 3 ann models for best performance tradeoffs: (1) using only accelerometer and barometer mean values, (2) adding simple fvs and (3) adding moderate fvs. the exact choice will depend on accuracy required and available computational power."
"the proposed resilience metric does not consider the cost benefit analysis and threat probabilities. for example, given knowledge of specific threat likelihoods, a system designer cannot use the assessments produced by our metric to determine whether enhancing the absorptive capacity or the restorative capacity can improve the system resilience in an economic manner. in addition, the proposed metric may cause confusion due to the summing form of two capacities and the imbalance of the two weight coefficients. we plan to improve the proposed metric in our future study. these aspects will be dealt with in future studies by improving the proposed metric."
"where, v is the voltage across the terminals of the pv panel and i is the current through it. the flow chart of the mppt algorithm is shown in fig. 4 . the mppt algorithm in pv system keeps the pv panel voltage near to mpp voltage. the p&o method is used; the required change in the duty cycle ( d) is calculated based on the change in power ( p) and the change in voltage ( v)."
"we believe these results strongly validate our method and address our first experimental goal. in order to verify the second goal of domain-independence, we have also run another set of experiments in the math domain to study how our method's performance varies when only a subset of the webpages and concepts is given."
"from the perspective of supervised learning, readability measurement can be viewed as a classification problem. for this method, one needs to first define a set of labels representing different levels of readability and use them to annotate a corpus of text documents as the training data. once collected, features can be extracted from the training data to build a model that captures the relationship between the features and the labels. then the resulted model can be used to predict the label of an unseen document based on its extracted features. the readability of this document is the readability level represented by the label."
"different from the wordlist-based approaches, the ontologybased approaches utilize an existing ontology of domainspecific concepts to derive possible indicators for readability. [cit] introduce two additional components into the dale-chall readability formula for medical documents: document scope and document cohesion. the document scope is based on the scope of the medical terms in the document, which is in turn defined as their depth in the medical subject heading 2 (mesh) hierarchy. on the other hand, the document cohesion measures the relatedness of the medical terms in a document. the more associations the terms have in the ontology, the more cohesive a document is. the combined formula is reported to be significantly better correlated with the readability of the medical documents, when compared to heuristic readability measures."
"while our technique is minimally supervised, to properly assess the results, we need to first compile a set of materials that have gold-standard readability annotations. to ensure fairness, we sought additional annotators for our main mathematics corpus. the resulting construction, annotation and validation of the ground truth took 3 man-months. we feel that this was a significant investment of resources and would be a data bottleneck for other comparative work. as such, to encourage comparative work, we have made the resulted corpus and judgments available for download 4 . we follow our own earlier recommendations in corpus collection. we use a corpus of mathematically related webpages extended from our earlier work [cit] for the evaluation of our algorithm. in total, we have chosen 27 common math concepts from mathworld encyclopedia, covering different as-4 http://wing.comp.nus.edu.sg/downloads#mwc pects of math, such as areas (e.g., \"geometry\" and \"number theory\"), operations (e.g., \"fourier transform\"), theorems (e.g., \"pythagorean theorem\") and objects (e.g., \"complex number\"). we chose them specifically to reflect the diversity of concepts in math and ensure the webpages collected cover a wide spectrum of readability. for each chosen math concept, we performed a google web search and incorporated the first 100 results into our corpus. to obtain the ground truth readability judgments for evaluation, we asked 30 undergraduate students to annotate the readability level for 120 randomly chosen, manually segmented, math relevant webpages from our corpus. other dimensions of the webpages were also annotated, but the discussion of these dimensions are out of the scope of this study, and are not mentioned further. the details of the readability levels used can be found in table 1 ."
"where adj(n) stands for the collection of nodes adjacent to the node n. after each iteration, we check whether the termination condition is met (algorithm 4). this is done by computing the change in the ranks of the resource nodes based on their scores to see if it stabilizes. we take the square root of the residual sum of squares (rss) divided by the number of nodes as a measure for the change. more specifically, the change is computed using the following formula:"
"a small-scale experimental hybrid solar-wind-battery renewable energy based microgrid with energy management system is developed and implemented. experiments were conducted to test the effectiveness of the proposed energy management system for different variations in the renewable energy sources and different variations in the load demand. the energy management system and control algorithms were implemented using rapid control prototyping in dspace controller. the experimental results show that the system is flexible and accommodates the different variations in the renewable energy sources and in the load. the controller allows the effective implementation of the energy management system. this test bench provides a platform in which future tests can be performed for different case scenarios and control algorithms for research in the field of hybrid renewable energy microgrid systems. k. victor sam moses babu received the b.e. [cit], and the m.e. [cit] . he is currently a junior research fellow with the department of electrical engineering, university college of engineering, osmania university, hyderabad, india, under the indo-sri lanka joint project, sponsored by dst, government of india. he has five years of teaching and research experience. his research interests include hybrid renewable energy-based standalone and grid-tied systems, energy management systems, artificial intelligence-based controllers, multilevel inverters, special electrical machines, power electronic converters, and electric drives. volume 8, 2020"
"there are already a number of well-established algorithms in the web search domain for computing quality scores for webpages such as pagerank, hits, and salsa. however, as far as we know, our work is the first to apply this methodology for domain-specific readability measurement. we will relate our approach to the the existing graph-based iterative computation algorithms in section 6."
the p-v characteristics of the solar panel with effect of irradiance is shown in fig. 3 . the characteristics are shown for different values of irradiance ranging from the power at the pv panel is
"salsa [cit] combines the strength of pagerank and hits by incorporating the backlink information into the hubs and authority computation. however, the idea of using backlinks as an indication of readability or difficulty does not make much sense in our application."
the dc microgrid is connected to a single-phase resistive load through a single-phase inverter with a controller shown in fig. 8 . a r-l filter is used at the inverter output to filter the undesired harmonic content. the outer loop pi controller is used to regulate the voltage and the inner loop pi controller regulates the current.
the associate editor coordinating the review of this manuscript and approving it for publication was bin zhou . a hybrid energy system was simulated and the performance for different practical load demand profile and weather data was studied [cit] . the simulation system of a coordinated control for microgrid energy management in standalone and grid connected modes is discussed [cit] . a hybrid windsolar-battery ess system is simulated to test the state of charge (soc) control [cit] . a scaled hardware prototype with battery soc control scheme to improve the dc grid voltage control for stand-alone dc microgrid was developed [cit] . a hardware prototype of a low-cost hybrid stand-alone power generation system was developed [cit] . the objective of this research work is to design and develop a small-scale windsolar-battery renewable energy based microgrid. an energy management system is proposed to maintain the power balance in the microgrid and provides a configurable and flexible control for different scenarios of load demand variations and variations in the renewable energy sources. the proposed system can be tested in real time environment with the use of rapid control prototyping. this test bench allows the validation of control algorithms in real time and therefore develop efficient renewable energy management systems.
"the rest of the paper is organized as follows; section ii illustrates the complete system design, converter topologies and control techniques in detail. section iii describes the hardware setup and the rapid control prototyping. following that, the experiments on the microgrid and various results are discussed in section iv. lastly, this paper ends with conclusions and discussions in section v."
"second, in the current formulation, all concepts on a page are considered to be equal regardless of whether they are only briefly mentioned or explained in detail. ideally, if a concept is only briefly mentioned, it should not be considered as very important for the page as well as in the readability computation. if the relative importance of concepts on a page can be determined, we can use a weighted average to suppress the unimportant ones and obtain a more accurate estimation of readability. we believe natural language analysis of the webpage would be needed to compute the relative importance and determine the weights automatically."
"for a concept node, we initialize its score as the average readability of all the resources containing the concept. counter ← counter + 1 8:"
"the collections of domain-specific (e.g., math and medical) resources available online have grown substantially over the years. nowadays, there are not only large commercial web sites, such as paper databases and encyclopedias, but also millions of smaller web sites devoted to discuss domainspecific topics and/or their related resources [cit] . as such, more people have incorporated internet search as part of permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. jcdl '10 their information seeking process for information and resources in a specific domain. for example, there are students looking for the definition of a math concept, professors searching for academic articles, patients trying to find health information and nurses seeking for evidence to support their practices. while these information needs can be satisfied by the resources, the process itself can be challenging. the main reason behind is that domain-specific resources target varying audiences, giving lexical evidence to distinguish different levels. for example, modular arithmetics can be explained in the context of ring theory or disguised as clock arithmetic, while the terminology used to describe symptoms of bird flu can be much more technical in a research article than a health information webpage meant to be accessible to laymen. however, most common search engines do not provide any indication of readability for the search results or allow for readability-based ranking. as a result, even though there are many relevant documents in the search results, users still spend a lot of time figuring out which ones are suitable to their level. this is especially true for the medical domain where the majority of health information are too difficult for the patients [cit] ."
"our research on readability is part of a long-term project towards developing domain-specific search engines, specifically for the math and medical communities. we plan to utilize such readability estimation techniques into appropriate browse/search interfaces, such that users will be able to locate domain-specific resources suitable to their level of expertise quickly. this would help to validate the utility of readability in domain-specific information retrieval and bring real benefits to end users."
"moreover, our current approach models only the relation between one pair of attributes -difficulty and readabilitybetween domain-specific resources and domain-specific concepts. however, there are still much more other correlations between other pairs of attributes as we have observed in our corpus. for example, books are commonly written for generic topics (e.g., linear algebra). this is one of our observed cases where the \"type\" of the resources can be influenced by the \"genericity\" of concepts."
"second, similar to the results on page selection strategies, if concepts are chosen at random, increasing the number of concepts helps to improve the performance. however, if the concepts are chosen by importance, using only the top 60% of the concepts in fact further boosts the performance of our system. this indicates that the concepts with low tf.idf do not contribute positively to the performance and should be removed from the graph using this selection strategy. therefore, we have also incorporated the concept selection by tf.idf into our system and denoted this improved version as ics. the resulting performance is further improved to .87 and .77 on the two metrics as shown at the last row of table. 2."
"there have been global initiatives for the promotion of self-sufficient renewable energy systems. this initiative has led to the development of renewable power generating systems which are capable of providing self-sufficient power generation with the use of more than one renewable source of energy [cit] . the most commonly used hybrid renewable energy sources are solar and wind energy [cit] . both these sources of energy are intermittent in nature; therefore, the use of an energy storage system (ess) is standard in stand-alone applications [cit] . in hybrid renewable energy systems; there are multiple control techniques to provide an efficient power transfer. the system design depends on the type of energy conversion system and the type of converters used at different locations in the system, this needs a lot of technical attention and has attracted research in this area [cit] ."
"as readability measures were devised to facilitate material selection, it is more important to be able to determine the relative importance between pairs of documents rather than assigning exact labels. thus, we evaluate our approach by the pairwise judgments accuracy. for each pair of webpages in the collection, we examine their readability scores from the subjects and those from our system. a pairwise judgement is said to be correct if both scores agree on whether one is more (or less) readable than the other. we ignored pairs of annotated readability values whose difference are smaller than a threshold (0.5) -we considered such pairs indistinguishable even by our subjects -and hence not suitable to be included into evaluation. in total, there were 5,165 qualified pairwise judgements for the annotated webpages."
"readability measures indicate how difficult it is to understand a piece of text. therefore, they are commonly used by educators to select appropriate materials for the target audience. although they have been applied in many different domains such as education, military and law, they are mostly generic, i.e., without the flexibility to allow them to handle the special elements in any domain. only recently have researchers started working on domain-specific readability measures. in the following sections, we first review two major classes of generic readability measures: the heuristicbased measures and the supervised-learning approaches, and then move on to the domain-specific ones."
"in any case, the amount of expert knowledge needed (i.e., knowing whether a concept belongs to a specific domain) is significantly less than the amount needed by other domainspecific readability measures (i.e., understanding the concepts sufficiently well to assign a score or construct an ontology out of them). therefore, we consider our approach to be less dependent on domain-specific knowledge sources and more portable across domains."
"although heuristic readability measures provide a quick and indicative way to compute readability, they use only a small number of features to summarize the characteristics of a piece of text. this is often an oversimplification, as much information, such as the identity of the individual words and the knowledge encoded in the text, is lost in the process."
"to accomplish these two goals, we performed three sets of experiments in two different domains. as our funded project work centers on the domain of mathematics, we carried out a set of experiments to measure the performance of our approach with a collection of math resources and concepts and compared it to four different baselines. second, since a truly domain independent method should rely on as little domainspecific resources and concepts as possible, we have also investigated into how many math resources and concepts our method needs to achieve good performance. last, we evaluated the performance of our approach on medical documents to show its portability across domains. we discuss these evaluations in turn."
"in fig. 11, the functions of the controller are clearly represented in the schematic diagram. the rcp accomadates the variations in the user inputs, varitaions in the control functions and variations in the harware opeartions in real time. the dspace controller recieves the voltage and current measurements from solar and wind energy conversion systems, battery storage system and load end along with the speed of the psmg, wind speed measurement and solar irradiance measuremnt provided by the encoder, anemometer and pyranometer respectively, these measurements are given to the energy management system and control algorithms for the calculation of power at different locations of the microgid and to asses the control action to be taken to maintain the energy balance in the system and also generate the neceaary pwms to the power electronic converters. set references such as irradiance values to maintain the intensity of the solar lamps and also the wind speed values to maintain the speed of the blower inorder to test performace of the microgrid for the variations in the renwable energy sources, these set refrences are sent through the controller. in the matlab enviorment, both the controller design and implementation is executed and also the storage and plotting of results is done. the stored results can also be analysed at a later time."
"our method addresses this need. similar to other domainspecific measures, it derives further information (i.e., difficulty) from a list of domain-specific concepts and it as a indicator for readability. however, this is done without any annotated corpus or expensive knowledge source. therefore, our approach is able to provide better readability estimation for domain-specific resources compared to generic readability measures, and, more importantly, can be ported across a wide variety of domains."
"these are easy requirements to satisfy for most domains: a list of domain-specific concepts is usually available in the form of a domain-specific dictionary, encyclopedia, or the index at the back of a textbook. given such a list, a domainspecific corpus can be constructed by downloading the top n (e.g., 100) results of each of the listed concepts from a search engine. conversely, if a list of domain-specific concepts cannot be found but there are existing collections of domain-specific resources, such collections can be taken directly as the corpus while the list can be constructed by extracting key phrases [cit] or by simply listing all the noun phrases from it. lastly, if neither of them exists, one can manually select a small number of domain-specific concepts as a seed list, and then collect a corpus of domain-specific webpages with the help a search engine. one can then iteratively expand them by extracting phrases from the corpus to expand the list and then using the expanded list to collect more webpages for the corpus."
"in order to properly model these considerations, we are developing a probabilistic framework with bayesian networks [cit] . a bayesian network (bn) is a generic probabilistic framework that has been widely used for inference in many different domains. it is a directed acyclic graph with nodes representing variables and edges encoding conditional dependence. the algorithm discussed here is easily incorporated into this framework when we consider the constructed graph as a two-layer (concept and resource layers) bn and use parameter learning as the iterative computation step. in addition, the bn's probabilistic nature and genericity would allow us to use probability distribution to model any attributes and study their correlation easily."
"besides using higher order n-gram models, schwarm and ostendorf [cit] also attempt to combine word features with other text features. they first compute the perplexity scores which indicate how well the language model of the document to be classified matches with the ones of the 12 grade levels. these perplexity scores are then used as the feature set of an support vector machine (svm) classifier together with other text features, such as fkre score and out of vocabulary rate scores, as well as four parse features, such as average parse tree height and average number of noun phrases. although the set of non-word features considered is not large, this classifier is able to further minimize the error rates compared to the one based on trigrams."
"first, as we do not pre-process the webpages to identify the main content of the webpages, math concepts that are presented as auxiliary information, such as navigational links and advertisements, have added substantial noises to the graph construction process. for example, the math concept \"number theory\" happens to appear at the navigational panel from mathworld. consequently, all the 39 mathworld pages in our corpus, which make up about 10 percent of the pages containing the math concept, are included into the difficulty computation for this concept and have adversely affected the accuracy of our approach. moreover, there are also many webpages in our corpus whose main contents are quite readable, but whose \"related concepts\" section contains a large number of difficult math concepts (e.g., there is an encyclopedic page on polynomial which lists more than 60 difficult concepts). in such cases, the computed readability is artificially inflated. we believe that further pre-processing to exclude certain sections of the webpages would significantly reduce the number of errors."
"as such, in our algorithm, we first construct a graph representing the resources and concepts, and then iteratively update 1) the readability score for a resource based on the difficulty scores of the domain-specific concepts it contains and 2) the difficulty score for a concept based on the readability scores of the resources which it appears in."
"in a nutshell, our method first constructs a bipartite graph with two sets of nodes representing domain-specific resources and domain-specific concepts, respectively, and edges representing the occurrence of the latter in the former. then we iteratively compute 1) the readability score for a resource node based on the difficulty scores of the adjacent concept nodes, and 2) the difficulty score for a concept node based on the readability scores of the adjacent resource nodes."
"the proposed system is shown in fig. 1 . it can be divided into three parts; i) solar and wind based renewable energy sources supported by a battery storage system along with their converters connected to the dc bus, ii) the load side inverter and single-phase load, iii) real time controller implementing the energy management system. the wind energy conversion system consists of a permanent magnet synchronous generator (pmsg) based wind turbine. the solar photovoltaic (pv) panel is operated with maximum power point tracking (mppt) when the power generated by both pv and wind are less than the load demand. when the power generated is more than the demand, the excess power is supplied to the battery and when it is no longer safe for the battery to be charged, the mppt is turned off. the battery storage system is used to maintain the energy balance in the system. an energy management system is used to control the power flow under different conditions in order to supply to the load through a single-phase inverter."
"an alternative approach to combine different types of features is to train one classifier for each type and then fuse their predictions. for example, [cit] extend the work of collins-thompson and callan's [cit] by introducing a k-nearest neighbour (knn) classifier on grammatical features such as the sentence length and the patterns of the parse tree. the predictions from the knn classifier are interpolated with the ones from the svm classifier to produce a final prediction, which is better than using either one of the classifiers alone."
we have two specific goals we wish to achieve in evaluating our algorithm. our primary goal is to demonstrate the efficacy of our approach and our secondary goal is to demonstrate our technique's domain independence.
"the key to the solution of this problem is an accurate readability measure for domain-specific resources. although there are already quite a number of heuristic readability measures and supervised-learning approaches for readability measurement, they employ only generic text features, such as average length of words, part-of-speech and discourse relations. as a result, they are largely ignorant of the domain-specific elements (i.e., concepts) present in domain resources and thus unable to measure their readability accurately. this is compounded by the fact that constructing the annotated corpus required for supervised approaches can be costly as well. in contrast, current domain-specific readability measures attempt to handle domain-specific concepts but largely with hand-built expert knowledge. for example, they use annotated familiarity scores to approximate difficulty or ontologies to obtain the genericness and associations of the domain-specific concepts. despite the improvement on accuracy over the heuristic and supervised approaches, the major caveat is that such expert knowledge is still expensive and not easily available in most domains."
"as for the potential sources of errors, we observe that the medical webpages contain more noise than the mathematical ones. this is because health information webpages are often commercial in nature and contain many advertisements which overwhelm the main content. in addition, they also tend to include more related medical concepts in navigation bars. for example, there is a webpage about snoring which lists more than 100 medical concepts at its navigation bar while its main content only contains less than 20. we believe this higher degree of noise is one of the factors that compromise the performance of our system. nevertheless, this should be readily solvable if we apply pre-processing to exclude certain sections beforehand."
"our approach improves the accuracy of readability measurement over the standard heuristic measures and remains competitive among supervised learning approaches in both math and medical domain. moreover, our approach only requires a list of domain-specific concepts and a corpus of domain-specific resources. these requirements are less strict and less domain-dependent compared to both supervised and domain-specific approaches which require an annotated corpus or expensive expert knowledge. therefore, we believe that our approach is a simple yet effective and portable solution to measure the readability for domain-specific resources."
"the power balance is achieved by the energy storage device by maintaining a constant dc bus voltage of 50v as shown in fig. 13 . the generator speed for different variations of wind speed is shown in fig. 14. the power at different locations of the microgrid is shown in fig. 15, till 10s, the battery is charged as the power generated by both the solar and wind energy conversion systems exceeds the load demand, after 10s the discharging starts and the level of discharge varies at different instants depending on the difference in the power deficit from the renewable energy sources."
where adj(cn ode) stands for the collection of nodes adjacent to cn ode. we then proceed to the iterative computation step in which the new score of each node is as the average of the scores of the neighboring nodes plus its current score (algorithm 3):
"the four baselines include one standard heuristic measure (fkre score) and three supervised learning approaches: naïve-bayes (nb) classifier 5, svm classifier 6 and maximum entropy (maxent) classifier 7 . the three classifiers are trained on the annotated webpages and use only binary features indicating whether a particular math concept appears on the webpage. we intentionally limited these baseline classifiers to use the same inputs as our ic method, as we are only interested in how well they could make use of the concepts to perform readability measurement. we also tried adding discretized versions of average word length, average sentence length and the fkre score into the baselines' feature sets, but this did not manage to improve their performance. for all the supervised learning approaches, 5-fold cross validation was performed to avoid overfitting. as can be seen from the table, fkre showed a modest amount of correlation (.72 on pairwise judgment accuracy and .48 on spearman's rho). this is similar to the results achieved by the nb classifier. in contrast, the two other baselines, svm and maxent, performed significantly better, scoring .80 and .82 on pairwise judgment accuracy and .70 and .67 on spearman's rho respectively. however, our approach still outperformed all the baselines with .85 and .72 on the two metrics."
"in the case where new resources and concepts are added after the iterative computation is completed for the existing resource collection and concept list, we can update the graph structure accordingly, initial the scores of the newly added nodes as the average score of their adjacent nodes, and then carry out further iterative computations on the rss ← rss + (newrank(rn ode) − rank(rn ode)) 2"
"the small scale microgrid consists of a controllable fan blower with which variation in wind speed can be achieved, this provides the necessary input to the 60w pmsg based wind turbine. industrial lamps are used to vary the irradiation levels for a commercial 70w solar pv panel. a standard 12v, 24ah lead acid battery is used. appropriate power electric converters for pv panel, generator, battery and load are selected. necessary encoders, line filter inductors, low power variable load module, interface and voltage and current measuring equipment are utilized. a list of parameters of the hardware equipment is provided in table 1 ."
"another factor that compromises our system is the more limited spectrum of readability levels in the medical corpus, in comparison to mathematics. although we have intentionally chosen concepts of different difficulty levels and from different areas, medical concepts are inherently difficult. none of the webpages are targeted to primary school students. this is rather different from the math scenario, where we can easily find highly readable webpages full of games and animations that explain easy math concepts to younger audiences. without such webpages, our algorithm is limited in its ability to discern and boost basic readability scores. this suggests that one can estimate the effectiveness of our algorithm in a particular domain by measuring the width of the readability spectrum. we hypothesize that the wider the spectrum, the more effective our algorithm will be. we plan to validate this hypothesis in future work."
"in the future, we plan to further improve our approach by constructing a probabilistic framework to generically model the attributes of the domain-specific resources and concepts and the possible correlations between them."
"the measured agreement was substantial but not strong (not above .8). we manually examined the annotations to discover which labels were being confused. we observed that although the subjects are able to determine what is readable and what not, the exact value annotated may still differ slightly between subjects. this is shown by the fact that 67% of the disagreed readability annotations had a standard deviation of less than 0.5. to eliminate these small perturbations, we applied spearman's rho [cit], which converts the values to rank order. the measured correlation is .93 (again, read on a -1.0 to +1.0 scale). this indicates a strong correlation for rank order and confirms our hypothesis that the general order of readability can be reliably distinguished."
all previous works have refined generic readability measures to be sensitive to nuances within a domain by using annotated resources. is there a way to introduce domainspecific readability without the use of expensive supervision?
there were two case scenarios that were carried out on the hybrid renewable energy microgrid. the first case was performed at a constant load condition and the solar and wind energy was varied. the second case was carried out for variations in load demand with constant renewable energy sources. the runtime for the experiment results for both cases is 70s.
"despite the fact that supervised learning approaches offer better accuracy compared to heuristic measures, there are still two main issues that limit their utility in domainspecific readability measurement: first, all previous work require an annotated corpus as the training data. this is costly to construct for domain-specific resources, whose annotation can only be done by experts. second, although language modeling helps to generate useful word features, it is largely ignorant of the domain-specific concepts. in other words, it treats domain-specific concepts as a sequence of tokens without considering their semantics or the relationships among them. therefore, it would not be as effective for domain-specific readability measurement."
"in this set of experiments, we use only a subset of math webpages and concepts selected by four different selection strategies: 1) selecting n webpages at random, 2) selecting the top n webpages with the highest quality, as indicated by their ranks in the search results from which they were collected, 3) selecting n concepts at random, and 4) selecting the top n most important concepts as indicated by a concept-based version of tf.idf. the n mentioned in the selection strategies is set to five different levels: 20%, 40%, 60%, 80% and 100%. the resulting performance of these selection strategies are shown in fig. 3-4 ."
we propose an iterative computation algorithm for domainspecific readability measurement based on the intuition that the readability of domain-specific resources and the difficulty of domain-specific concepts can be recursively estimated from each other.
"first the voltage is perturbed and output power is calculated, if the power increases then voltage is increased further, if the power decreases then the voltage is reduced."
"most recently, pitler and nenkova [cit] examine by far the largest set of textual features. their feature set includes word (unigram language model), syntactic (identical to the parse features in schwarm and ostendorf's work [cit] ), lexical cohesion (e.g., average cosine similarity between sentences), entity coherence (e.g., the transition probability of an entity from being the subject in one sentence to the object in the next) and discourse relations (i.e., language model over discourse relations instead of words). their result shows that word features and average sentence length are strong predictors but the strongest ones are discourse features. moreover, there is also a complex interplay between different types of features. while successful, their study is a proof-of-concept; they acknowledge that automatic extraction for such rich features does not yet exist."
"if the change in the ranks stabilizes (i.e., is smaller than the threshold), the scores of the nodes will be updated a final time as the new scores and the computation terminates; otherwise, the update is followed by more iterations until the termination condition is finally met. upon termination, the scores of the concept nodes and the resource nodes are, respectively, the computed difficulty and readability scores. note that the values of the scores themselves are not of import, but rather that the relative order between individual concept or resource."
"in summary, our approach is able to work with a small set of domain-specific resources and concepts to achieve good performance with simple, automatic selection strategies. therefore, it is highly portable to any domains even for the ones in which it is difficult to collect a sizeable collection of domainspecific resources or a list of domain-specific concepts."
"in short, these measures address two issues of supervised learning approaches: the need for a corpus and ignorance of domain-specific concepts. however, they still require expert knowledge and incur substantial labor cost in constructing their annotated wordlist or ontology. these resources may not be available for other domains. as a result, the applicability of such methods remains limited."
"we ran our system with all the webpages in our corpus and a list of math concepts compiled from mathworld encyclopedia. we present the pairwise judgment accuracy (as well as the spearman's rho) of our system (denoted as ic) and the four baselines in table 2 . the best performance of our system after resource and concept selection (denoted as ics, to be introduced later in section 4.1.2) is also shown."
"the secs consists of a solar pv panel, a dc-dc boost converter and an mppt controller as shown in fig. 2 . depending on state of charge of the battery storage system, the mppt is operated under mppt mode or under off-mppt mode of operation."
"in the second case scenario, the solar and wind power generation is kept constant with the solar irradiance kept constant at 800w/m 2 and the wind speed kept constant at 11m/s as shown in fig. 18 . the power balance is achieved by the energy storage device which maintains a constant dc bus voltage at 50v as shown in fig. 19 . the generator speed is constant at a speed of 5280 rpm as the wind speed is maintained constant shown in fig. 20 ."
"the study in heuristic readability measures has identified word difficulty and average sentence length as the two important readability indicators among all the text features. this is enhanced by the supervised learning approaches which have enabled deeper text features to be extracted automatically and combined with more sophisticated statistical models. in spite of the better accuracy achieved, they require annotated domain-specific corpora and are largely ignorant of the domain-specific concepts. domain-specific readability measures address the two issues by deriving information from domain-specific concepts using expert knowledge; however, the cost and availability of expert knowledge still limit the applicability of such approaches."
"with the occurrence statistics collected, the construction of the graph is straightforward (algorithm 1): we create a representing concept node for each concept in the list (lines 2-4) and a representing resource node for each resource in the collection (lines 5-7). we then add an edge between an concept node and a resource node if the concept represented by the former occurs on the resource represented by the latter (lines 8-11 ). this completes the construction of graph and fig.1 gives an example of a graph constructed based on two resources and a list of concepts. add cn ode to g 5: for all resource r in corpus do 6:"
"our work is inspired by other successful iterative graph algorithms which have made their impact in digital libraries. we relate and contrast our approach to three of them: pagerank, hits and salsa."
according to a comprehensive review on classic readability studies [cit] s to facilitate the selection of textbooks. they are usually expressed as a weighted sum of the values of some features extracted from a piece of text. the features extracted are the ones that correlate well with readability while their weights are computed by linear regression.
"we first construct a resource-concept graph. this graph is bipartite, containing two types of nodes, one representing concepts, the other representing resources. edges are added between a concept node and a resource node to represent the occurrence of former on the latter. after constructing this graph, we start the score computation by first assigning an initial difficulty score to each concept node and a readability score to each resource node. we can then iteratively update the readability scores for the resources based on the difficulty scores of the associated concepts (and vice versa) until the termination condition is met. the final scores at the nodes can be taken as the readability for resources and difficulty for concepts. we describe the details of graph construction and score computation in the following sections."
"the objective of the first case is to deliver a constant power to the load for different generating conditions. the solar irradiance is varied by switching the lamps on/off and thus providing different irradiance levels. the wind speed is varied by varying the speed of the fan blower. the irradiance is varied by 1000w/m 2, 800w/m 2, 400w/m 2 and 600w/m 2 at the start, 20s, 30s and 50s respectively, the wind speed was varied by 12m/s, 8 m/s, 11m/s, 7 m/s and 11 m/s at the start, 10s, 20s, 40s and 60s respectively as shown in fig. 12 . though the variations in the wind speed and solar irradiance are only step variations which can never happen in the real world as the meteorological conditions are always changing. these values are selected in such a manner that they vary between the possible maximum and minimum range of operation of the pv panel and wt in order to check the system operation for these variations. also, in this case, the load is kept fixed which would not occur in a practical situation. but the load is kept fixed in order to observe the performance operation of the renewable energy conversion systems and the battery storage system for the variations in the power generated from the renewable energy sources, this will be difficult to observe if the load is constantly changing."
"to reduce the need for a corpus and better handle domainspecific concepts, domain-specific readability measures have focused on identifying the difficulty of such concepts with expert knowledge. depending on the type of expert knowledge utilized, these measures can be classified into two categories: wordlist-based and ontology-based approaches."
"the score computation starts with an initialization step (algorithm 2). in this step, we assign an initial score to each resource node (lines 1-2), representing its readability, and each concept node (lines 3-10), representing its difficulty:"
"in this way, we can determine the relative readability of domain-specific resources by the relative difficulty of the domain-specific concepts they contain and vice versa. in the web context, we use concepts as the context for assessing resource readability, and webpages as the context for assessing concept difficulty."
"the required inputs for our algorithm are a list of domainspecific concepts and a corpus of domain-specific resources. a key distinction of our work from previous works is that both do not need to be annotated -a flat list of concepts and a corpus of resources is all that is required. note that \"resources\" here connote any textual resource (e.g., an scholarly article, webpage, formalized educational lesson module, or a newspaper clipping), but in the context of this paper, we occasionally use \"webpages\" or \"documents\" to stand in for the more general notion of \"resources\"."
"we ran our system with all the medical webpages and a list of medical concepts compiled from mesh. the results are listed in table 3 . in this experiment, there are 320,976 pairwise judgments."
"subjects were first shown an annotation guide explaining how to use our web-based annotation system and what the readability levels are. after reading the guide, the subjects annotated webpages by reading math webpages and selecting an appropriate readability level as shown in fig. 2 . each subject was asked to annotate 20 webpages in 45 minutes and were given a token amount as appreciation for their efforts. on average, each webpage was annotated by 5 to 8 subjects. we take the average annotated values to establish the ground truth of readability. before the experiment, we also needed to determine whether manual readability annotation is indeed a feasible and reproducible task. to do this, we assessed inter-annotator reliability by first computing the pairwise inter-judge agreement using cohen's kappa coefficient [cit] . cohen's kappa mea-sures the agreement between two annotators, accounting for chance agreement. its values range from 1.0 (complete correlation/agreement) to -1.0 (complete disagreement/negative correlation). a zero value indicates no correlation. the average pairwise inter-judge agreement was .72, indicating substantial agreement. we also applied fleiss' kappa [cit], a multi-rater agreement measure, to calculate the agreement among all the subjects. the result was similar (.73)."
"our experiment in the medical domain also followed the same general methodology. we first selected 27 medical concepts of varying difficulty levels from mesh, covering different aspects of medicine, such as diseases (e.g., cough), injuries (e.g., bruise), substances (e.g., vitamin), symptoms (e.g., snoring), therapies (e.g., blood transfusion) and procedures (e.g., bronchoscopy). for each of these concepts, we then downloaded the top 100 search results and consolidated the webpages for our medical corpus. due to budgetary limitations, this corpus was only manually annotated by the first author. readability values were annotated for a subset of the corpus (946 pages) using the same labels."
"updated graph until the termination condition is (again) met. alternatively, we may rerun the algorithm on the enlarged resource collection and concept list. this should provide more accurate estimation especially when the number of newly added resources and concepts is substantial."
"in this paper, we propose an algorithm for domain-specific readability computation which does not require an annotated corpus or expensive expert knowledge 1 . our approach is an iterative computation on a resource-concept graph based on the intuition that the readability of the domain-specific resources and the difficulty of domain-specific concepts provide accurate estimations of each other. our evaluation has shown that this algorithm outperforms heuristic-based measures, remains competitive even among supervised-learning approaches, and is portable across domains. the rest of the paper is organized as follows. we first review the relevant literature on readability research in section 2. then we describe our intuitions and the resulted iterative computation algorithm for domain-specific readability in section 3. we evaluate our approach in the domain of math and medicine in section 4 and discuss the directions for future research in section 5. lastly, we relate our algorithm to several graph-based iterative computation algorithms in section 6 and make our conclusion in section 7."
"to initialize the score for a resource node, we choose to use the fkre formula as it is one of the classic, widely-used heuristic readability formula as described in section 2.1. where avgsl(r) and avgw l(r) stand for the average sentence length in words and the average word length in syllables of the resource r respectively [cit] ."
"the load is varied at every 10s and the performance of the microgrid is tested for a variable load demand. the power at different location of the microgrid is shown in fig. 21 . initially the load demand is 100w and the power generated from the renewable sources is slightly less than the demand and this is met by the battery. after 10s until 40s, the demand is lower and during this time the battery is charged from the excess power generation. after 40s, the demand is more than the supply and the battery supplies the additional power required. the microgrid operates for a variable load but the voltage is kept constant at the load terminal as shown in fig. 22 . for both lower load demands and higher load demands, the energy management system maintains the power balance when the power from the renewable energy sources is constant. for lower load demands, the battery is charged and for higher load demands, the battery discharges and the variations in the soc of the battery is shown in fig. 23 ."
"the rapid control prototyping of the renewable energy based microgrid is implemented experimentally using a dspace ds1104 controller board as shown in fig. 10 . the dspace controller is a real-time hardware powered by a powerpc microprocessor and supported by texas instruments tms320f240 digital signal processor (dsp). the common control functions for are pre-defined in the dsp processor therefore there is no requirement of compiling a new user defined code. also, by using simulink real-time workshop library the programs are compiled directly from the c-code for the powerpc processor. the pulse with modulation (pwm) and serial peripheral interface are provided by dsp processor and the encoder interface and a/d modules are controlled by powerpc processor. thus, this ds1104 board is an efficient real time controller which can be used for closed loop control applications."
"other features have been considered as well: vogel and washburne [cit] also examined five other classes of features, including sentence structure, part of speech, paragraph construction (e.g., the number of sentences), general structure (e.g., the number of lines in a book) and physical makeup (e.g., weight and size of type). however, among these features, only the number of prepositions and number of simple sentences were found useful. gary and leary [cit] further expanded the exploration of features by examining 64 countable variables in four categories: content, style, format and features of organization. they identified average sentence length, number of pronouns and number of prepositional phrases as useful in addition to word features."
"under this framework, many researchers have re-examined the utility of most text features. starting from word features, collins-thompson and callan [cit] construct one unigram language model for each of the 12 american grade levels based on a corpus of webpages with grade-level annotations. these language models capture the probability of a word occurring in the document of a certain grade level. the readability of a new document is then predicted by finding the language model that most likely generates all the words in it. their evaluation shows that this approach outperforms the traditional reading measure on webpages. similarly, [cit] has adopted this approach in classifying the readability of health information into three levels (basic, intermediate and advanced), achieving a high accuracy of 98%. further along this line, schwarm and ostendorf [cit] explore the effect of using higher order n-gram models (up to trigram) on classification performance and show that it helps to minimize error rates."
"given a list of concepts and a collection of resources, the first and most important step in constructing the pageconcept graph is to count the occurrences of the concepts in the resources. to do so, we first index all the resources using the open source text search engine lucene 3 . we then use each of the concepts as a query to retrieve a set of matching resources. lastly, we derive the number of occurrences from the term frequency vectors of the retrieved resources."
"in addition to the ways to improve our algorithm as mentioned in section 4.1.3 and 4.2.2, we also notice that there are cases where it is insufficient to use a single value for readability or difficulty for a concept. we believe this can be solved by modeling difficulty and readability using probability distributions. for example, we may be able to derive from the corpus that 70% of the pages related to the concept \"geometry\" are highly readable (e.g., pages targeted at primary school students about simple geometric shapes), while the other 30% are much less readable (e.g., pages targeted at university students on differential geometry). then we can more accurately model the difficulty of \"geometry\" as 70% easy and 30% difficult instead of using the average value."
"this intuition helps us solve cases where the generic readability measures lead to incorrect conclusions for the difficulty of domain-specific concepts in isolation. for example, let us say we need to determine whether a resource containing the concept 'ring theory' is less readable than another one containing the concept 'pythagorean theorem'. if we extract normal text features such as the average number of syllabus or the percentage of familiar words, 'pythagorean theorem' would be incorrectly calculated as more difficult than 'ring theory'. however, if we examine a corpus of resources containing these two words, we may discover that 'ring theory' also appears on less readable pages about advanced math concepts, such as 'isomorphism theorem' and 'abelian group', whereas 'pythagorean theorem' appears on more readable pages about basic math concepts, such as 'triangle' and 'sine'. with this information, we can decide that 'ring theory' is more difficult than 'pythagorean theorem' and infer that the resource containing 'ring theory' are less readable than one containing 'pythagorean theorem'."
"note that a, b, f i, and g i must be known to implement the iterative method in theorem 1. in the following sections, stochastic adp/ radp methods are developed to approximate k * and p *, without using the model information."
"lem is studied under relaxed assumptions. second, we solve the robust optimal control problem for linear stochastic systems subject to nonlinear dynamic uncertainties, by using the robust adp (radp) technique [cit] recently developed for deterministic systems. this generalization is important because it tackles the situation where the system dynamics and order are not exactly known, and also where only partial knowledge of input-state data is available for robust optimal control design. different from existing results, in the setting of our stochastic radp design, we conduct the robust optimality analysis from the viewpoints of small-gain theorems [cit] and stochastic h ∞ theory [cit] . it can be shown (see lemma 2) that if the input-dependent noise is small enough, then the stochastic linear l 2 gain of the nominal system under the optimal control policy can be easily assigned by tuning the weighting matrices in the cost. to further illustrate the obtained results, a sensorimotor control example is presented. this example shows that our stochastic adp method can serve as a fundamental tool to develop non-model-based optimal control design for continuous-time stochastic systems."
"next, a policy iteration method is given to approximate p * and k * . theorem 1 is a direct extension of [22, theorem 1] ."
"however, mcoc is not good at dealing with the correlation of different conditional attributes and their contribution to class attribute, especially when data set is composed of many attributes and a large number of data with the aforementioned problems. fortunately, ec can effectively find the intrinsic relation among different attributes and efficiently obtain the dependent degree between an object and its class [cit] . contrarily, the class-overlap and loss induced by misclassification in data set are not considered by ec, so generalization of classification badly degenerates. to some extent, mcoc can tradeoff the overlap of different classes and misclassification loss and obtain a good generalization. consequently, the combination of the advantages of the former two approaches is promising to overcome the above problems and challenges."
"although ec has many advantages for solving inconsistent, non-compatible, and contradictory problems, it is short of the tradeoff between fault tolerance and generalization in the unlabeled data. however, mcoc is good at those aspects. that is to say mcoc can gain the better tradeoff between minimizing the overlapping degree and maximizing the distance departed from adjusted boundary. it does not attempt to get the optimal solution but to seek for the non inferior solution in order to gain the better generalization."
"and any input data is respectively denoted as . (7) where j v is the value of the attribute ij x . in addition, we suppose that the j c is the weight coefficient of the attribute ij x, and it is obvious that we have"
"remark 1: similar to [16, algorithm 1], algorithm 1 is an offpolicy method [36, chapter 6.5], i.e., in the k-th iteration, the control policy used to define p k is different from the control input u k used to generate the on-line trajectory. the main advantage of using offpolicy method is that adding exploration noise e k does not influence the approximation accuracy."
"here, dŵ k is assumed known and exploited for on-line learning. of course, it is not used for feedback control design. next, define four matrices"
"according to the function (11), for given dataset, we may compute the dependent degree factor of input data with respect to class, that is, the dependent degree factor is defined as"
"in this paper, we proposed a classification model based on extension classification method for chinese word-formation analysis. we see that the emcoc extends the capacities of mcoc and effectively deals with the class-overlapping, inconsistent, non-compatible, and contradictory problems in real world applications. experimental results show that it is an effective classifier for predicting chinese semantic word-formation patterns."
"reversely, mcoc is not good at dealing with the correlation of different conditional attributes and finding the corresponding contribution to class attribute, especially when data set is composed of many attributes and a large number of data with the inconsistent, non-compatible, and contradictory problems. nevertheless, ec can effectively find the intrinsic relation among different attributes and efficiently obtain the dependent degree between an object and its class by means of matter-element relation analysis."
"finally, we have implemented the corresponding algorithms of mcoc and emcoc and the experiments in the next section are carried out on matlab 7.0 platform. the linear programming problems of mcoc and emcoc are respectively solved by utilizing matlab optimal tools."
"in our experiment, database is sourced from the chinese semantic word-formation corpus. the datasets are obtained by using computer to automatically label the real and large-scale chinese compound word corpus and then checking and correcting it manually. the database contains 50562 disyllabic compound words. each word-formation represents the three types of word senses: word sense, sense of morpheme 1, and sense of morpheme 2. then each sense type consists of three semantic layers: the big class, medium class, and small class. besides, each word-formation is classified as one of the predefined 8 patterns, which are listed in table 1 . owing to the fuzziness of language and some manual errors, the database may contain potential noise, the inconsistent, non-compatible, and contradictory values of attributes."
"this technical note is organized as follows. in section ii, the system model is presented, and the optimal control problem is formulated. in section iii, the stochastic adp algorithm is derived, along with rigorous convergence analysis. an extension to stochastic radp design is presented in section iv. in section v, a sensorimotor control example is employed to show the effectiveness of the proposed method. finally, the conclusion is drawn in section vi."
"before presenting our main result on the stochastic robust optimal control, let us present a gain assignment [cit] result for the x-system (9) with δ considered as the disturbance input."
"recently the interest in the optimization-based data mining is increasing, and mcoc can be used to solve the classification problem in data mining. the classifier tries to reach a tradeoff between the overlapping degree of different classes and the total distance from the input data to decision boundary. then a linear mcoc [cit] based on comprise solution was proposed and used to analyze the behaviors of credit cardholders. then a multiple phase fuzzy linear programming approach [cit] was designed for solving classification problem in data mining. a linearly penalized mcoc based on comprise solution [cit] was presented for solving the class imbalance problem and analyzing the behaviors of credit cardholders. a rough set-based mcoc [cit] was put forward and applied to the medical diagnosis and prognosis, and a linear mcoc with fuzzy parameters [cit] was used to improve the generalization of mcoc. besides, a kernel-based mcoc classifier [cit] was provided just like the use of the kernel method in svm. then mcoc was used to analyze the behaviors of vip e-mail users [cit] . besides, the above rough set-based mcoc was employed to predict the protein interaction hot spots [cit] . in these applications, mcoc has showed that it can provide better performance than some traditional data mining methods. and mcoc is extensively used for solving classification, regression and other problems."
"the rest of this paper is organized as follows: first, section 2 is the basic principle of mcoc. then emcoc is detailed in section 3. the experiment on the pattern analysis of chinese semantic word-formation, and the results and comparisons are demonstrated in section 4. finally, conclusions will be given in section 5."
"if (7) has unique solutions of p k, b t p k, and σ(p k ), then k k+1 can be derived directly from (4) . now, we give the following adp algorithm."
"for chinese semantic word-formation analysis, the experiment is actually a multi-class classification problem, so it can be transformed into multiple paired two-class classification problems. then we randomly and respectively select 250 positives and the same number of negatives from any two patterns as the training set and the remaining instances are used for the independent test set."
"this technical note has studied non-model-based, data-driven optimal control for continuous-time stochastic systems with unknown dynamics. adp and radp algorithms have been developed to solve the stochastic optimal control and robust stochastic optimal control problems, respectively. an extensive convergence and stability analysis for the proposed algorithms has been given. the effectiveness of the proposed algorithm has also been illustrated by a sensorimotor control example."
"in the model (1) the parameter i ( ) is the distance where the point i deviates from the separating hyperplane, and the sum of i should be minimized. at the same time, the parameter i"
"data mining is very important for us to extract the useful knowledge for some critical decision-making. as for classification there are many different methods for solving this kind of problem, such as neural networks, decision tree, bayesian networks, svm, mcoc, extension classifier, and so on. y extension classifier (ec) is a new classification method which based on extensible transformation. the matter-element models of different classes and unlabeled data are built, and the dependent degree values between the unlabeled data matter-element and class matter-elements are calculated, respectively. according to the dependent degree values, the unlabeled data is classified as the class with the maximum dependent degree. because of its advantages over other classification methods for solving inconsistent, non-compatible, and contradictory problems in practical applications, extension methods are also used for clustering, association, and other problems in data mining [cit] ."
from the above model we know that the classifier has the advantages of both mcoc and ec owing to taking the dependent degree factors into account.
"international journal of systems science 9 experiment, the customer's deadline probability for varying values of 2 is estimated, while varying values are considered for customer's deadline. this experiment shows how the increase of the network's customer's deadline lowers the network feedback rate, and it also displays the effect of increasing service rate on the network feedback probability."
"analysing the status of the second node in the network explains this condition. after receiving service from the second node with probability 1 à ' d, the customers leave the network or return to the first node with probability ' d . if one customer leaves the network, the number of existing customers in the network will decrease, and the network remains far from the blocking state, leading to lower a blocking probability. if the second node is faster than the first node, the blocking probability will be more than the former state. it is due to the state of the second node in the network, as all customers that receive service from the first node are placed at the second waiting line. as these customers rapidly leave the first node, and the second node works slower than the first node, there will be more customers at the second line, leading to more customers in the system, where it increases the network blocking probability. because of the slower speed of providing service in the second node in this state compared with the former state, fewer customers receive service in this node, and the number of customers in the system will increase gradually."
"theorem 4.1: the importance sampling estimator n is asymptotically optimal if n ! 0, \" n / n ! 0 and n\" n ! 1."
"sometimes e is called the 'exit boundary'. the function w(x) which is defined for all x 2 \" d, plays a pivotal role in the change of measure. this function is an approximate solution to a set of equations derived using game theory. the construction of function w(x) happens in three steps. first, three affine functions \" w k ðxþ are formed based on some parameter as follows,"
"in all aforementioned papers, models of queueing networks have been taken account in which each customer enters the network and waits for receiving service with no time restriction. once it receives the service from various nodes of network, it leaves the network. based on such method, various schemes for simulating rare events in the various network models have been presented in the literature. [cit], where customers do not hurry to receive service, as just receiving the service is of importance. this type of modelling is employed in different applications such as some packet-switched telecommunication networks where no time period is guaranteed for packet transfer, and the transfer time depends on the traffic load as well as the network parameters. however, there are other models in which customers wait for service, but wait for a limited time only. in these models, the service time for a customer would be very important. in one node, it is meant; the sojourn time of a customer should not exceed a definite threshold, and if it does, the customer would not wait for receiving service from that node and leaves it. such situations occur in the processing or merchandising of perishable goods [cit] ) . many types of military engagements are similarly characterized. an attacking airplane engaged by antiaircraft is available for 'service' -within range -for only a limited time."
"4. the importance sampling estimator and its asymptotic optimality it was our interest to explore the probability p n that n 1 þ n 2 reaches n before return to 0, given that the system is initially empty. asymptotically, as n rises, it is known that p n plummets exponentially fast (see proposition 3.1 [cit] ), at some rate"
"to the best of our knowledge, we have not seen such a modelling that enables us to estimate the probability of occurrence of rare events in networks where the sojourn time of customers is subject to limitation."
"3. sojourn time and analysis of missing the deadline as stated earlier, we tend to modelling the systems whose sojourn time is of importance in some parts, whereas in other parts providing service to the customers is done without any reference to sojourn time and some applications of this type of modelling are presented in previous section. the simplest model can be a two-node jackson network where in the first node providing service to customers is conducted without any reference to sojourn time, while in the second node, the sojourn time of one customer should not be higher than a threshold. therefore, once a customer receives its service from the first node receives a deadline as it enters the second queue. the deadline of a customer determines how long it can wait in the second node for service completion. if providing a service to a customer in the second node ends prior to the expire of the deadline, the customer leaves the network. however, if the deadline expires, it should return to the first queue even if it is receiving the service from the second node. therefore, there should be a feedback from the second node to the first node."
"where n i (k) is the length of the queue at node i after the kth transition. obviously, n can only take values in z 2 þ and its dynamics can be modelled by"
"in this study, the aforementioned two-node jackson network with feedback for various parameters is analysed including customer arrival rate, service rate to customers in the first and second nodes, and also the feedback rate of customers from the second node to the first one. in this analysis, some parameters related to the network efficiency such as the probability of network blocking and the probability of missing the deadline of the second queue customers are estimated. as stated earlier, the network has a common buffer structure that can keep 20 customers in it at most. figure 2 illustrates the network blocking probability for the set of parameters"
"the selection of these two sets of parameters lies in the fact that in the first state, the second node is the network's bottleneck, whereas in the second state, the first node is network's bottleneck. in this study, we have changed the parameter feedback from 0.05-0.7 with 0.05 increments, and have estimated the blocking probability of each step for both states. figure 2 also shows how the network blocking probability increases as the network feedback probability increases. it is shown that for ' d in the range of 0.05-0.33 the change figure 2 . estimation of the blocking probability."
"let v be the time an arriving customer from node one with an infinite (no) deadline must wait before it completes its service at node two. v is called the offered sojourn time in the network and its distribution function is f v (). therefore, the probability of missing the deadline is"
"the results of this study show that for a specific deadline for customers in the second queue, as the proportion of 2 to rises, that is as the existing load on the second node decreases, then the network feedback probability decreases. the reason is that as figure 3 . blocking probability in two-node jackson network with feedback."
"let n be the probability that one of the customers in the second node misses its deadline, given there are n customers in the second node. n also is called conditional loss rate for n customers and is given by"
"of note is [cit] which is one of the pioneering articles in importance sampling estimation of overflow probabilities in queueing network. here, the estimation of the overflow probability of both a single queue, and of the total population in a network of queues is suggested to be done through a heuristically motivated state-independent change of measure. in this method, inter arrival and service time distributions of the model are simply superseded by other distributions, a replacement that stays constant during the entire simulation. [cit] comes down to transpose the arrival rate with the lowest service rate. however, the general inefficiency of this change of measure was subsequently established [cit], while in some other cases, it conduces to an estimator with infinity variance [cit] . nevertheless, there are few exceptions in the literature that have rigorously addressed the state-independent change of measure. in contrast, these can only be applied to special classes of queueing networks and/or buffer overflow probabilities [cit] ."
"note that depending on which of the three functions \" w k achieves the minimum, the set \" d is broken down into three regions. with each of these regions, the corresponding vector r k determines a constant (not state-dependent) change of measure associated to the related region."
"to calculate the offered sojourn time, first the conditional offered sojourn time should be calculated as outlined below. let v n be the time an arriving customer from node one with an infinite (no) deadline must wait before it completes its service at node two, given it finds n customers at node two. v n is called the conditional offered sojourn time, given there are n customers at node two. clearly the probability distribution function v n, f v n ð:þ, is given by"
"in this article, we deal with the concept of a deadline on a two-node jackson network with feedback in which arrival and service rates are modulated by an exogenous finite state markov process. based on the definition of the deadline for customers in the second queue, we have calculated the probability of missing the deadline, and have shown how the feedback rate of the network is affected by the deadline value. the results have used in an importance sampling estimator. it was found that an increase in the probability of missing deadline raises the probability of total population overflow. likewise, it was known that how the probability of total population overflow affected by service rates in the network."
"5.2. analysis of probability of missing deadline for customers in the second queue in this experiment, we have analysed the effect of different values of the deadline of customers in the second queue, and 2 on network feedback rate, and have estimated the network feedback probability for three values of different \" for customer's deadline with varying and 2 . as indicated in section 3, \" is the mean value of the random variable known as the relative deadline."
". in this article, we consider a model with deterministic customer impatience that has already been studied in barrer (1957b) . deterministic customer impatience refers to the case where a customer whose sojourn time in the second queue is greater or equal to \" becomes a lost customer irrespective of whether it is acquired for service or not. thus, deterministic customer impatience has a probability distribution function described by"
"the contribution of our work is twofold. first, we estimate the probability of a rare event known as total population overflow in a model of queueing network in which in some part of the system, there is no time limitation for receiving service, while the same customers should receive the needed service within a definite time once they enter another part of the system. secondly, and more importantly, we consider the effect of various network parameters on the probability of total population overflow and show that how the probability of total population overflow affected by network parameters."
"the current state x may determine the vector p which can be interpreted as dw \", (x). we can obtain a change of measure from a given function w \", (x). for each state x, equation (34) can be utilised for each of the components \" w k ðxþ to calculate the weighting function k (x), and subsequently define \" â½! i as the corresponding weighted average of the \" â"
"this article is organised as follows. section 2 presents a brief review of related work. in section 3, an analysis of the probability of the missing deadline in the two-node jackson network and its dynamics are derived. in section 4. the importance sampling estimator and the conditions to reach the asymptotic optimality are presented. section 5 examines some examples to illustrate the efficacy of our model. section 6 concludeds this article."
"consider we have a two-node jackson network with feedback ( figure 1 ) whose arrival process to the network is poisson with rate and customers are served in the order of their arrival, or in other words, service discipline is first-come-first-served (fcfs). service times are exponentially distributed with rates 1 and 2 at nodes one and two, and the arrival rate to the first queue is 1 ."
"where e q defines an expectation under the new measure q, 1 a n is the indicator function of the event a n in any simulation run and pða n þ qða n þ is the likelihood ratio or radon-nikody´m derivative [cit] of the path under investigation, which is defined as follows:"
"the difference between the deadline of a customer in the second queue and its arrival time from node one, referred to as a relative deadline, is a random variable with mean value \""
"an important benchmark problem in rare-event simulation, and importance sampling, in particular, is the problem of estimating the overflow probability of the total population of two markovian queues in tandem [cit] . the classical papers on the application of importance sampling in queueing typically rely on a state-independent change of measure. in other words, for any system state, the probabilistic law is changed in the same manner. [cit], the importance sampling with a stateindependent change of measure has been used that is only applicable to special classes of queueing networks and/or buffer overflow probabilities and will not, in general, be asymptotically optimal for networks of queues [cit] ."
"is the same function w(x). keep in mind that w \", (x) is converged on the function \" w ðxþ as \" ! 0. furthermore, the smoothness of w \", (x) along the aforementioned boundaries is designated by the value of \". the state-dependent change of measure in each state x is largely associated to dw \", (x) which is the gradient of w \", (x) in x. as a matter of fact, this gradient can be presented as a statedependent weighted average of the vectors r k :"
"rare event simulation has piqued the interest of researchers ever since the first development of monte carlo techniques on computers at los alamos national laboratory [cit] ) and involves estimating extremely small but important probabilities. despite the amount of work on the topic in the last 60 years, there are still domains needing to be explored because of new the applications [cit] . the estimation of rare event probabilities has garnered a lot of attention in queueing theory. in the wake of their generic structure, queues have figured prominently in applied probability; for example various applications can be found in inventory, logistics, military, call centers, communication networks. the main motivation for studying these rare events lies in the fact that the events under consideration relate to situations whose happening can be very costly for queue network to face. the most common approach in rare event simulation is importance sampling. the general idea of importance sampling is to change the sampling distribution of the system under study to sample more frequently the events that are more 'important' for the simulation. of course, using a new distribution results in a biased estimator if no correction is applied. therefore, through multiplication with a so-called likelihood ratio [cit], the simulation output is converted to the original measure."
"the binding pocket is defined as the set of residues with at least one atom with surface area in contact with an atom of the ligand. surfaces in contact between atoms as well as sas are calculated using a voronoi polyhedra analytical algorithm [cit] using standard van der waals radii used for the protein and the ligand atoms as described elsewhere [cit] . to ensure ligands are specifically bound to the protein, ligands must contain a minimum of five non-hydrogen atoms and to be in contact with at least five residues in the protein."
"for several reasons reducing energy consumption of llcs in qos systems remains a significant challenge. firstly, as the applications are becoming computation-intensive [cit], the pressure on memory system is increasing and to mitigate this pressure, modern processors are using large size llcs. secondly, with shrinking cmos feature size, leakage power has been increasing at an exponential rate [cit] . since leakage accounts for over 90% of the energy spent in llcs [cit], the energy consumption of llcs is becoming a major fraction of chip energy consumption. many existing techniques are designed to save the dynamic energy of cache, however, a large fraction of energy spent in llcs is in the form of leakage energy, and thus, these techniques having limited utility in saving energy in llcs. further, the cache energy saving techniques which require offline profiling are difficult to scale and hence cannot be easily used in real-world qos applications. thus, saving cache energy in qos systems is a challenging, yet important research issue and new techniques are required to effectively address it."
"for estimating program execution time under different l2 configurations, cashier uses the cpi stack technique [cit] . a 'cpi stack' is a stacked bar that shows the different components contributing to overall performance. it presents base cpi and 'lost' cycle opportunities due to instruction interdependencies, cache misses etc., taking into account the possible overlaps between execution and miss events. out of various components of cpi-stack, cashier uses the memory stall cycle component, since the change in l2 configurations shows its effect on execution time in terms of change in memory stall cycles. we assume that, in an interval, memory stall cycles vary linearly with the number of load misses, and thus, their ratio, called spm (stall cycles per load miss), remains independent of the number of load misses themselves. then, the stall cycles under any cache configuration can be computed by multiplying spm with the number of estimated load misses with that configuration. using stall cycle estimates and base cpi value from the cpi stack, the total number of cycles (and hence total execution time) under that configuration can be computed. these estimates are used for computing memory subsystem energy values (section vi-b). also, the execution time and energy estimates are used by esa (section iv)."
"natural ligand database (nldb) [cit] provides the model of protein structures with natural ligands. the idea stemmed from the fact that many ligands in pdb are modified ligands for the sake of crystallisation and the bridge between those modified ligands and natural ligands should be provided to enhance the 3d structural information in pdb. the search query \"gm1 gangliosidosis\" led the user to the causative gene glb1. nldb demonstrated that glb1 was involved in 15 kegg reactions, and these 15 reactions were classified into five pathways according to \"uniprot search view\". the same 15 reactions can be found in \"pathway db\" tool (table 1) . of the five pathways in nldb, glycosphingolipid biosynthesis (hsa00604) contained the reaction of gm1 degradation (r06010). in this reaction, 56 natural ligand complexes were registered in nldb derived from the proteins of various species. the link to human beta-galactosidase, the product of glb1 gene, with galactose (pdb id: 3thc) led to the 3d structures of the ligand-protein complex with reported variation in amino acid residues. the variations in amino acid residues around the ligand-binding site were highlighted on the table of nldb window. in this entry, ten variations were reported around the ligand-binding site, and eight of them were related to diseases, namely three to gm1 type i, one to gm1 type ii, one to gm1 type iii, and two to mucopolysaccharidosis iv-b."
"this research uses the requirement for an up-to-date antivirus application as an example. when a device does not have updated antivirus installed, the check security agent provides the user a summary. it will summarize what actions the user needs to take for a byod to comply with the organization's security requirements."
"organizational risks escalate when there are no access policies. these policies need permission from owners to check devices for viruses, spyware, and malware before connecting to its system. windows, android, and ios mobile operating systems are all vulnerable to cyber-attack (table i) [cit] . malware collects and leaks sensitive data, tracks users, and changes authorization policies (fig. 1) [cit], which means a high degree of vulnerability. no operating system is immune from attack and organizational solutions need to be compatible on all operating systems. [cit] sensitive data is at risk when a personal device is lost or the employee leaves. with more than 9 million smartphones lost or stolen each year [cit], this is a considerable challenge. even when data is deleted from a device and its operating systems, experts can retrieve that data [cit] ."
"an entry in the database consists of a pair of structures of a given protein and a particular ligand that is bound to one of the structures, the holo form for i424 that ligand, while the second protein structure of the same protein represents the apo form with respect to that ligand. the sequences of the holo and apo forms must be identical (100% sequence identity) over at least 80% of the length of either (overlap). these criteria allow us to detect different structures of the same protein in cases where there are small differences in the n and c termini of the proteins, such as when a particular domain of a multi-domain protein is cloned separately and different groups may choose slightly different domain boundaries or the presence of his-tags or other sequences required for purification."
"gnp expression, phenotype, autophagy db, genome explorer and mutation@a glance (table 1) show the search results in a tabulated or graphical form. the user can further analyze the database of each tool by following the link in the search result. (fig. 4) . a big node represents a protein, a small node represents a ligand and an edge represents a protein-protein/ligand interaction. a node in red is associated with a disease (selected in the top-right window)"
"signing and signature verification agents are mobile agents. they check a system access request comes from a known user without being modified during transit. it generates digital signatures for every json policy file and data requests from data owners or byod users (fig. 3) . the signature verification agent within the security manager verifies the digital signature. it compares the decrypted hash value with the original json policy and initiated generated hash to verify it is the same. when the values are equal, the message has not been modified."
"employees have the right to use personal devices in any manner they like as long as they do not breach company policies. unknowingly they can download malware and malicious applications, which can have a negative effect on corporate networks as well as their own devices. 'keyloggers, malware, and cyber-attacks have greatly increased the potential for unauthorized access to, and information theft from, endpoints' [cit] . with most organizations and personal devices vulnerable [cit], risks increase when staff bypass system limitations by rooting or jailbreaking devices to access off-limit areas. this threatens personal devices and the cloud network with a malicious attack when transferring, processing, and storing data."
"once a device meets security policy requirements, the authentication agent starts. every user needs a unique identity. the authentication agent validates a user's identity for access www.ijacsa.thesai.org to the system using two types of authentication for extra security."
"in this study, we built a curated non-redundant dataset of pairs of x-ray protein structures, representing the cognate-ligand bound (holo) and unbound (apo) forms of the same protein, in order to study multiple aspects of side-chain flexibility upon ligand binding. the curation involves the filtering or correction of potential factors (such as the assignment of atoms during refinement) affecting the quality of the structures. the steps involved in creating a nonredundant dataset assure that our conclusions are not biased by over-representation of particular proteins or small samples. the nonredundant dataset created as part of this work could be used as a docking benchmark dataset particularly when the effect of sidechain flexibility needs to be taken in consideration in the analysis of the performance of docking algorithms."
"there are two nodes in red that interact with both hexa and hexb, which suggest disease-disease interactions. by right clicking a node, protein-protein interactions can be extended. the pathway of two nodes in the window fig. 2 the initial search result by vapros. the query word is \"hexa\", the causative gene of tay-sachs disease can be automatically detected using \"path search\" on the top menu. by clicking a node or an edge, the detail information of the node/edge can be displayed on the right bottom of the window. in fig. 5, hexa was selected, hence the detail of hexa was presented on the right. the link to \"3d interaction\" shows the protein 3d structural information of hexa protein. in 3d interaction, sitetable/sitesbyvariants link leads the users to the information that vapros aims for, namely the relationship between variations on dna and protein structure/function."
"correcting erroneous atom assignments requires the analysis of hydrogen bonding interactions. we do so by using the program reduce that adds hydrogen atoms in standard geometry in order to satisfy physico-chemical constraints [cit] . in agreement with the previous study, approximately one-fifth of binding site residues (17% for asn, 16% for his and 20% for gln) required a flip in order to avoid the amide clashing with neighboring atoms [cit] . moreover, we remove hydrogen atoms from phosphates and carboxylic groups, as they are more likely to be deprotonated in physiological conditions. in the case of ligands, the het dictionary provided by the pdb is used to retrieve connectivity information."
"in this section, we discuss particular aspects of side-chain flexibility for the different side chains. a total of 2395 residues were considered for the statistical analysis with an average of 141 per residue. cys has the lowest number of observations with 42 and leu the highest with 234. gly and ala are not considered since they do not have any dihedral bonds as well as pro, because movement of its side-chain inevitably causes backbone movements. cys residues participating in disulfide bonds are also excluded as their conformations are constrained. a detailed list of the residues is accessible at our website."
we explored the current knowledge on lysosomal storage diseases (lsds) and built a tenable hypothesis as a case study to show the usage of vapros. the similar analyses can be conducted on different diseases by accessing http:// p4d-info.nig.ac.jp/vapros.
"once the authentication agent finishes its verification, the check permission agent searches the database for the security classification assigned to the username. the agent sends the username to the relevant personal device to make a preliminary decision about granting access. it then implements the mac policy to authenticate the username against the security classification contained within the mac policy to make a final decision to grant access or not."
"in short, byod boosts morale, productivity, employee satisfaction and job ownership as well as work flexibility and mobility but it has some issues with employers such as poor controlling, violating an employee's personal privacy and rights, spreading malware and malicious applications, and lacking appropriate security policies for employees using byods."
"cloud services have three main models managed by a cloud manager: software as a service (saas), platform as a service (paas), and infrastructure as a service (iaas). we propose a new security manager tool called software as a service (aaas) for public cloud providers. the aaas framework gives any organization's saas the ability to use cloud manager to perform security checks before granting byod access to the cloud environment. we considered several issues when designing the framework. it was important to make the tool easy to add and use without affecting existing byod and cloud environments. we achieved this by limiting operating requirements."
"our earlier study [cit] was based on dihedral angle differences. however, such a measurement is in fact a surrogate measure, a more direct measurement of sidechain conformational changes that is more properly anchored on thermodynamic principles is required to measure the extent with which true side-chain conformational changes are observed as a result of ligand binding. this is particularly important in light of the results of the articles discussed above [cit] suggesting that small-scale changes are sufficient to account for side-chain flexibility in ligand binding."
"the effort for integrating the management of different databases has been made by a number of groups [cit] . linking data with a common framework is one of the possible approaches, and the semantic web technologies are becoming increasingly popular in recent years [cit] . while the semantic web technologies based on linked open data and ontologies are a promising approach, extremely diverse set of ontologies as well as non-uniform uses of uri (uniform resource identifiers) to describe identical resources by different parties make it difficult to integrate various information resources without extensive manual intervention. although some efforts have been dedicated to solve these difficulties (e.g., http://identifiers.org), it will take time for the research community to agree on a unified convention."
"here we launched vapros, a new type of database integration application. vapros enables a quick search of multiple databases with interrelation of each search result. this application can be used as a textbook for acquiring expert knowledge for researchers in different fields, and can be a tool for building a data-driven hypothesis that can be tested by wet-lab experiments [cit] ."
"black and white box tests are used first to examine the functionality and structure of the proposed framework. the validation was completed successfully by validating some of the requirements that are used in our proposed framework. we used four cases to test the proposed framework based on potential attacks, as shown in the following (fig. 9) . we classified the test into four main sub-tasks. first, trusted and untrusted users and devices testing. second, access control policy testing. third, performance and scalability testing. finally, integrity testing."
"proteins bind small molecules as substrates, cofactors and allosteric regulators in order to perform essential cellular functions. as a consequence of induced fit [cit], conformational selection [cit] or more likely a combination of both [cit], the ligand-bound protein may display a wide gamut of structural changes. these changes can range from large movements of entire domains to small side-chain rearrangements in the binding site."
"vapros is unique in the style of data integration. vapros tries to integrate different databases dynamically and relationship amongst the data in the databases is taken by uniprot accession key. central dogma guarantees the relationship between biomolecules in the organisms, hence all the phenotypes should basically stem from the perturbation on biomolecules. therefore, phenomena observed in the organization can be tagged to either dna or protein. we chose a protein identifier to tag all the other data, because vapros is aimed for the analysis of protein variation."
any proposed solution should be compatible with all byod operating systems to reduce the risks from these devices to keep the process simple and flexible.
"whoever is responsible for byod user access control policy, controls the owner/policy administrator device. this can be the chief security officer (cso), policy administrator or an organization's owner. the device can be either a personal device or pc with a trusted operating system like securityenhanced linux (selinux) so they can set security classification levels and the initial data for user access control."
"it was previously suggested that ligand binding induces nr conformations for some amino acid types [cit] . in our dataset, we do not detect any pronounced differences in the number residues in nr conformations between holo and apo forms with 127 versus 104 cases (out of 2395 residues). finally, it was observed that side chains with unfavorable conformations are more mobile than rotameric ones [cit] . assuming that all side chains observed in nr conformation in the apo form underwent considerable movements upon binding, we can place an upper bound on the number of residues in nr conformations undergoing large rotations and compare to the number of residues in rotameric states undergoing equally large rotations. we observe 375 nr conformations in the apo form (an overestimation considering the assumption of movement between nr conformations) compared to 368 rotameric conformations that undergo rotamer change (to another rotamer or an nr conformation). although we do not make a distinction according to amino acid types, we do not observe that nr conformations are more mobile than rotameric ones."
"bring your own device (byod) is the trend where employees use personal handheld devices for work as well as for personal use [cit] . employees own the devices so they take them home each day. organizations with cloud network systems usually allow the use of byods for accessing data and enterprise applications. this has huge advantages for both employees and employers. [cit] . another study said 95% of participants used personal handheld devices to perform work functions [cit] . more and more people are using byods because of the benefits. it boosts morale, productivity, employee satisfaction and job ownership as well as work flexibility and mobility [cit] ."
"mandatory access control is the best mechanism for protecting an organization from the risk of using byods. however, restricting access to certain locations or work hours negates the benefits of byods for both the employer and the employee. there needs to be minimum requirements for security, authentication, and authorization phases for byods to meet. policy administrators need to set access controls to the resources each user requires. organizations must then enforce all technical and access control policies."
"the auditing agent is static. its function is to record all successful and failed attempts to access the system. it records all policy enforcement agent decisions about grant and deny access decisions. the auditing agent records username, date, time, access request to what resources, and the decision. this assists the policy administrator to monitor, analyze, and manage regulatory compliance, understand system access denials, and perform disaster recovery to develop the system."
most other existing solutions solve specific issues without comprehensive consideration of the effects of these solutions on the byod environment or their users. we attempted to reduce the restrictions and increase the flexibility and mobility with a soft implementation of the policy. we also tried to protect user's privacy by avoiding the use of mobile device management (mdm) solutions. we have also built the first prototype of the system by implementing and testing the proposed framework in real environments. the outcome of verification and validation show excellent results and positive feedback. the future work will increase the performance of allowing access decision and enhance the current framework to support federated cloud computing.
"in order to clarify the extent of side-chain flexibility observed upon ligand binding, it is necessary to perform an analysis using a non-redundant dataset of protein cognate-ligand pairs as large as possible to increase the statistical significance of the results. such an analysis must also be based on the detection of side-chain rotamer transitions as a way to assure that observed conformational changes involve the transition across energetic barriers."
"our objective in this study is to observe side-chain conformational changes associated to ligand binding. to maximize the chances that the binding ligand is the only factor affecting the observed side-chain conformational changes, in this study we restrict our analysis to pairs of structures which differ by the presence of one ligand and thus are defined as apo and holo forms for that ligand. the apo and holo forms may be bound to other ligands, as long as such ligands are present in both structures and make the equivalent contacts with the protein. specifically, we remove cases where such extra ligands have more than 10 different contacts in the apo and holo forms."
"four different cases can be used to test trusted and untrusted users or devices. these cases cover the possible situations that may occur when users use their byods, as seen in (table 3 ). these cases are:"
"implementing and testing the proposed framework is required to verify and validate the solution. it is required to ensure that there is no fault, error or failure in the system. the implemented prototype has two core components. the first is the client and owner application, and the second is the security manager as software as a service (saas) in the cloud. mobile agent software is required in these components. there are a variety of agent frameworks can be used, such as concordia, aglets, and jade. in the client and owner byod devices, we built an application by using c# and java in the microsoft visual studio framework. the javascript object notation language (json) is used in these codes to implement the mac. we use real byod devices based on the windows operating system to install the app and connect to the cloud. in the security manager, we used the same above environments to build two software as services. one of them is our security manager, and the other one is the organizational software as a service that is connected to our security manager. these two software as services are deployed in the google cloud platform and use its storage as a database."
"the idea embedded in vapros that is different from other general database integration efforts is that vapros makes much of the relationship among the biological molecules and phenomena. the relationship is governed by the central dogma; hence all the incidents can be described in either gene-centered or protein-centered manner. phenotypic changes of an organism likely derive from changes in the biological system of the organism, which is sustained by the network of biomolecules and those biomolecules are ultimately encoded in dna. this flow of information is nothing but the opposite direction of the central dogma, and hence the organization of data and databases in vapros follows the information flow in the central dogma. technically, the search results of the variety of databases are interconnected using the protein as a hub of information. in the following sections, the detail of vapros and the example of the usage are presented. table 1 lists the databases that are integrated on vapros and the tools that visualize the search results. there are 16 databases and 15 tools of which vapros are made. the latest information of the databases, namely the version and the size, is given at http://p4d-info.nig.ac.jp/vapros/statistics.html. the integration of the databases took the form of either dynamic link or a data copy from the original site to the local site of vapros. ideally, all the databases should be accessed dynamically to avoid time lag of the data and to save the local disk space, but such dynamic access often sacrifices a prompt response to a query. therefore, we downloaded the part of the data from each database and achieved an optimum response speed. the data update is scheduled once in every six month to keep abreast with the latest data in all the databases. vapros deals with the data of humans, rats and mice and focuses on phenomena related to humans."
"as expected, in the subset of binding sites in which no rotamer changes were observed, the apo-bound and holo forms show no significant differences in their steric potential for rigid binding sites. overall, in 32% of flexible cases (28% of all binding sites), side-chain rearrangements are required to accommodate the ligand and to avoid steric clashes. in the context of molecular docking, these results demonstrate the importance of considering side-chain flexibility and define a natural threshold for the accuracy of rigid docking algorithms tested on an unbiased non-redundant dataset."
"in this paper, we present cashier, a cache energy saving technique for quality-of-service (qos) systems. cashier uses a small microarchitecture component called \"reconfigurable cache emulator\" (rce), which uses set sampling idea to estimate program miss rate for various cache configurations in an on-line manner. additionally, cashier uses cpi stacks to estimate program execution time under different llc configurations. using these estimates, the energy saving algorithm (esa) estimates memory subsystem energy under different cache configurations. then, a suitable cache configuration is chosen to strike a right balance between opportunity of energy saving and performance loss, thus making best possible efforts to not miss the deadline. for hardware implementation of cache line switching, cashier employs the gated-v dd scheme [cit] ."
the controller agent is static and manages all other agents. it contains the application programming interface (api) that allows it to communicate with other saas in the cloud. the controller agent creates instances from mobile agents and sends them to devices using individual ip addresses.
"the expansion of the data size has been coped with the increase in the size of the storage and with invention of a new algorithm for searching the whole data swiftly. one of the famous examples of the tool for quick search of a database in this field is blast [cit], a tool to search similar abstract life science research now heavily relies on all sorts of databases for genome sequences, transcription, protein three-dimensional (3d) structures, protein-protein interactions, phenotypes and so forth. the knowledge accumulated by all the omics research is so vast that a computer-aided search of data is now a prerequisite for starting a new study. in addition, a combinatory search throughout these databases has a chance to extract new ideas and new hypotheses that can be examined by wet-lab experiments. by virtually integrating the related databases on the internet, we have built a new web application that facilitates life 1 3 sequences out of the huge nucleotide/amino acid sequence databases. further expansion of the size of the independent database and the increase in the variety of databases may have enhanced chances for performing novel experiments by extending the scope of hypotheses, yet the lack of technology for integrating different types of databases and of an application for searching the multiple databases have precluded extensive application of this approach. the researchers aiming for an integrated search of different databases should approach the databases one by one, learn how to use each database and obtain information relevant for their studies. the users then integrate the data obtained from different databases by themselves. this process evidently requires tedious labour as well as skills for manipulating data in different formats. hence, the biggest hurdle that we have to overcome in the current life science activity is the complexity in integrating databases in a way that enables us to come up with novel ideas and hypotheses. once the up-to-date data is comprehensively integrated, then researchers with an experience in a specific field can start deducing a hypothesis in a data-driven manner."
"a different access control policy has been tested by simulating different and complex attacks during the transfer, process, and storage phases. during the process and storage phases, the proposed framework faced 10 attacks that modified the access control policy."
one problem when working with x-ray structures is that sometimes one or more atoms or entire residues could not be resolved due to uncertainty of atom positioning caused by high movements in the crystal. pdb entries missing any binding site side-chain atoms were removed.
the probability of a residue type i to undergo side-chain conformational changes upon ligand binding (p i ) is calculated as previously described [cit] as follows:
"where k wall is a penalty constant of value 10 6 i is the nth ligand atom; j the mth protein atom; r ij the distance between atoms i and j and r i and r j the van der waals radii. the greater the potential is, the more clashes there are. [cit], developed to prevent steric clashes in docking simulations. we calculate the differences in wall term from the holo to apo forms. a threshold value is empirically set upon visual inspection of apo-bound forms to a value of 150 for all ligand-protein contacts, above which the ligand pose in the apo form is no longer acceptable. this positive value accounts for slight inaccuracies in the radii of ligand atoms. a threshold of 25 is used when considering steric clashes for individual side-chain atoms."
"the advance of the molecular biology has yielded a huge amount of biological data including dna/rna/protein sequences [cit], their expression levels [cit], difference in the sequences of individuals [cit], three-dimensional (3d) structures of the biomolecules [cit], phenotypes of the organisms [cit] and so forth. these data have been stored in independent databases located on the internet and researchers exploit these databases for new knowledge of the target of their study. database mining facilitates the process of knowledge acquisition and that of building new hypotheses for planning new experiments [cit] ."
"in order to understand the contribution of h-bonding flexibility, we calculated the average number of h-bonds formed with the protein and water molecules. we notice that for his, arg, lys, ser and thr, they participate in more h-bonds when they are rigid ( supplementary fig. s7 ). for the remaining residues, we do not observe a statistically significant difference between flexible and rigid. this shows that upon binding of a ligand, the h-bonding interactions are easily broken. glu has a lower probability of moving than expected considering its number of flexible bonds. this may be due to the fact that acidic residues have a high tendency to participate in h-bonding ( supplementary fig. s7 )."
"the interaction network of the proteins encoded in hexa and hexb was found in \"molecular interactions\" window ( fig. 5 ). this window can be displayed by ticking both hexa and hexb in the table shown in fig. 4 and pressing \"details (go)\" button. a protein is represented with a big node and a ligand is represented with a small node. a protein-protein/ligand interaction is represented with an edge. figure 5 tells that eight proteins and four ligands interact both with hexa and hexb, and each protein interacts with a number of other proteins and ligands. these interactions were extracted from different table 1 . in fig. 5, the nodes in red are proteins associated with diseases. the information was extracted from omim (table 1), and the catalog of specific disease is given on the right side of the window."
steric clashes are quantified using a potential (wall) after superimposition of the holo and apo structures. this allows us to judge if the holo ligand pose would be acceptable in the apo form (referred as apo-bound throughput) in spite of side-chain rotameric changes. the potential is described as follows:
"the mandatory access control (mac) policy dictates strict access limits that are difficult to bypass, either intentionally or unintentionally. using a mac policy is effective as it assigns a clearance level to every user. it does this by establishing what each user can and cannot access within the system using javascript object notation language (json). there are four categories for users (subjects) and resources (objects), and these are top secret, secret, confidential, and unclassified. the policy administrator determines the user and resource security classification levels according to the mac. the json file and data is encrypted and signed after the data is digitally signed. the following is an example of a json file:"
"for explaining the algorithms, we define a quantify t i, as follows. using program execution time estimates, in every interval, the algorithms estimate the extra time, which the current configuration is taking over and above the baseline configuration 2 . over all the intervals, the algorithm accumulates these values. at the end of any interval i, this gives the estimate of increased execution time (t i ) due to esas (viz. psm or msm), till that interval i. thus, t i shows the amount of slack already exploited."
"for simplicity, our analysis is restricted to ligands that appear as hetatm in pdb records and otherwise excludes nucleic acids or peptides as the focus of this study lies on the interactions between small molecules and proteins. furthermore, as we are interested in specific interactions of cognate ligands with binding sites that evolved to bind such molecules, the most common molecules found in crystallization buffers are also excluded from our analysis. such molecules do not play any role in protein function in the majority of cases and comprise sulphates (so 4 ), phosphates (po 4 ), glycerol (gol), ammonium (nh 4 ), citric acid (cit), (4s)-2-methyl-2,4-pentanediol (mpd), 2-amino-2-hydroxymethyl-propane-1,3-diol (trs) and 2-(n-morpholino)-ethanesulphonic acid (mes). although water molecules do play a major role in catalysis, their implication in binding has been studied in detail elsewhere [cit] and are not considered as ligands in the present work. finally, covalently bound ligands are discarded, as we are interested exclusively in side-chain movements upon binding as a result of non-bonded interactions."
"it is widely accepted that flexibility is essential for protein function [cit] . studying dynamic aspects of protein structure using nuclear magnetic resonance * to whom correspondence should be addressed. spectroscopy, for example, is still sufficiently costly and time consuming as to prevent its use in the same scale as x-ray crystallography. x-ray crystallography produces a 'snapshot' of a protein that offers little information on dynamic aspects of structure. however, it is possible to compare different 'snapshots' to infer dynamic properties of the protein. for example, one can compare two structures of the same protein crystalized in different conditions, say in the bound (holo) and unbound (apo) forms. although this type of comparison is neutral with respect to the mechanism of binding (induced-fit or conformational selection), it makes possible, given the amount of available data, to perform statistically significant large-scale studies of conformational changes associated to ligand binding."
"organizations have concerns about unauthorized access to cloud-based applications that bypass company policies [cit] . this is referred to as 'shadow it' where activities take place on a company network without specific organizational approval. the use of byods also risks employees accessing social media during work hours contrary to company policy. byods in the workplace exposes companies to greater security risks; in particular, the heightened risk of cyber-attack as it is hard to control access out of hours [cit] . this is a conundrum for organizations as they need to consider user privacy and rights along with protecting networks from attacks. some organizations get the balance between controlling byods for work and personal use right. others' monitoring practices can violate an employee's personal privacy and rights when using personal handheld devices for personal reasons. it is important byod users understand their rights [cit] . it is possible for employers to access private information without permission under the guise of management practices without good security mechanisms in place. this will cause problems for employees and employers if the process for managing access control to enterprise applications after hours is not transparent [cit] ."
"in this study, we generated a database that consists of pairs of identical proteins in holo (bound to a ligand) and apo (unbound to the ligand) forms to analyze side-chain flexibility upon ligand binding. our objective is to develop a dataset where, as much as possible, the only factor that influences flexibility is the binding of the ligand in question. the pri database is the largest dataset that meets our criteria and contains 1812 entries. the database comprises 1270 different crystallographic structures (average resolution of 1.93 å) defining 163 protein families and 1110 different ligands (supplementary table si) . for the purposes statistical analyses, the pri database is redundant. for example, several proteins were crystallized multiple times under similar experimental conditions. furthermore, the same pdb might be considered as holo for a given ligand but as apo for another ligand. two non-redundant subsets (pfam and seq) were generated from pri. in pfam, each ligand is uniquely bound to a protein domain ( supplementary fig. 1 ), in other words one entry per domain-ligand combination. the same domain can bind different ligands in the pfam dataset, which can result in different binding site conformations. this subset contains 631 entries defining 884 structures bound to 631 different ligands. the pfam subset still contains a bias toward particular families of proteins that are over represented in this dataset such as protein kinases (pfam pf00069) and trypsin (pfam pf00089) domains, with 143 and 61 cases, respectively, that were crystallized multiple times with distinct ligands."
the policy enforcement agent is static and its primary function is to enforce access control policies to determine who has access to the cloud. its purpose is to strengthen access control. the policy enforcement agent implements the mac policy using the bell-lapadula model (fig. 5) to match the relevant user classification level. it uses the classification level in conjunction with the 'check permission agent' to verify a user has legitimate access and transmissions were not modified during the process.
"further, the physical pages are divided into n memory regions based on the least significant bits (lsbs) of their physical page number. in fig. 1, these bits are referred to as region id. cache coloring maps a memory region to a unique color in the cache. for this purpose, cashier uses a small mapping table (mt) which stores the cache color assigned to each memory region. by manipulating the mapping between physical pages and cache colors, cashier allocates a particular cache color to a memory region and thus, all physical pages in that memory region are mapped to the same cache color."
"if the number of load misses vary significantly between different cache configurations, the above mentioned linearity assumption does not hold well. however, as shown in section iv, in an interval, cashier only searches for configurations which differ from current configuration in a small number of active colors. thus, the above assumption holds reasonably well and energy estimation accuracy is minimally affected."
"side-chain rotamers are defined as particular combination of sidechain dihedral angle ranges. average dihedral angle combinations of frequently observed conformations represent energetically favorable states. the classification of side chains into rotamers using sidechain rotamer libraries [cit] makes it possible to determine the extent of side-chain conformational transitions between energetically distinct states as a result of ligand binding. such libraries also tabulate the probability with which different rotamers are observed. therefore, in principle rotamer libraries can also help decrease conformational search space with the use of rotamers as representative conformations of energetically favorable states."
"vapros accepts keywords, dna/protein sequence, entrezgene id and uniprotkb accession as a query (fig. 1) . a keyword can be a gene name, a protein name, a ligand (drug) name, a disease (phenotype) name and an identifier found in the databases. input of the keywords is assisted by a keyword-suggestion function. incomplete input makes vapros find a related keywords in the keyword database and it shows a list of candidate words below the query input window. once the search button is pressed, vapros throws the input data to noren, an original tool to search for whole ids in the databases related to the query. noren is based on the id mapping table provided by uniprotkb [cit], and blast [cit] . the result of the query is presented as a list of candidates to the user. the candidate list is categorized into three different types, namely gene/protein, ligand and phenotype (fig. 2) . the user may select the most relevant element in the list, press \"details (go)\" button and obtain the results of the search done by ids relevant to the keywords (fig. 3) . the results are presented through the tools tabulated in table 1 . the search results shown by each tool can be opened by clicking the corresponding icon on the left in fig. 3 ."
"the security manager is at the core of the proposed framework. its function is to manage all the components required for the mac policy to operate. it is located in the cloud and operates when called on by a saas. the framework has four functions: checking byod device security, enforcing the access control policy, working with independent platforms, and securing the access control policy. it works in conjunction with the 11 agents."
"uniprotkb [cit], genecards [cit] and cosmic [cit] assume a similar approach for the integration of relevant data. vapros put stress on a graphical presentation of the search results as found in \"molecular interactions\" and \"tagcloud\", and on the analyses on protein 3d structures as found in \"hgtop\", \"3d interaction\" and \"natural ligand database\"."
"s-var is a special tool that evaluates impact of amino acid substitution to the function of the protein. by providing a specific mutation to the window of s-var, the tool starts a couple of software that evaluate the impact of the mutation [cit] and provides each and consensus results for the user at some intervals. combination of the search results in each tool can be a basis of building a hypothesis that can be verified by wet-lab experiments. the search query \"gm1 gangliosidosis\" on vapros led the user to the current knowledge that the causative gene is glb1, which encodes lysosomal enzyme β-d-galactosidase. by following the link to omim [cit], user can acquire information on the detail of the disease, namely gm1 gangliosidosis is classified into three types in accordance with the onset age and severity; type i (infantile), type ii (juvenile) and type iii (adult) as shown in table 2 ."
"the rotamer-based approach used in this study is more appropriate as each residue has its own rotamer classification that renders unnecessary the use of ad hoc error-prone thresholds. we account in part for the effect of intrinsic disorder in flexibility when using overall, our results show that ∼12% of binding sites do not change conformation upon ligand binding. in other words, in ∼88% of the cases, at least one residue undergo changes which shows the large amount of flexibility that occurs upon ligand binding. thus, our results show that a significant amount of flexibility occurs upon ligand binding when considering rotamer changes as opposed to rmsd values [cit] . furthermore, such widespread rotamer transitions represent changes between energetically separate states, thus placing doubt over the validity of the minimal rotation hypothesis outside the context of docking simulations and the small dataset upon which it was developed [cit] . although it would be desirable to set all residues as flexible when predicting the structure of protein, it is impractical from a computational point of view considering the exponential increase in size of the search space. however, considering at most five side chains as flexible account for all side-chain rotamer changes in ∼90% of binding sites (inset fig. 1) . therefore, the introduction of side-chain flexibility on a limited number of binding site residues can be seen as a realistic simplification."
"in this paper, we introduce a solution to the access control issues in byods and the cloud environment. we aimed to design a solution that maintains the features of byods, such as mobility and improved flexibility. this solution is based on four main requirements, which are checking the byod device security, enforcing the access control policy, working with independent platforms, and securing the access control policy. we integrate all of these requirements and build our proposed framework based on the multi-agent system due to its adaptability, mobility, transparency, raggedness, and self-start and stops."
"hydrogen bonding is a directional interaction in which an acceptor 'a' shares electrons with an hydrogen bound to a donor 'd' (commonly referred as d-h … a). in this study, only strong hydrogen bonding with side chains are considered, i.e. cases where both donor and acceptor are oxygen or nitrogen atoms. thus, in our study, only the following residues are considered: arg, asn, asp, gln, glu, his, lys, ser, trp and tyr. considering the prevalence of multifurcation in hydrogen bonding [cit]"
"we based the framework on a multi-agent system, because the software runs independently on behalf of a network user. this makes it adaptable, mobile, transparent, and it automatically starts and stops. this reduces the costs and the required resources when a byod interacts with other machines. the proposed framework is divided to three parts: the client byod, owner device, and the security manager (fig. 2) . each software agent is explained in this paper."
"we are also interested to understand how enthalpic factors may affect flexibility. we focus on hydrogen bonds (h-bonds) as this is one of the most important i428 fig. 4 . b-factor and sas analysis in the unbound form. in upper part, the freedom of movement in the unbound form is quantified using b-factors whereas the lower part represents the normalized solvent accessible surface areas (sas). rigid (blue) and flexible (green) residues are compared. residues are ordered in respect to the flexibility scale. student t-tests are used for statistical significance ligand-protein-specific interactions. for simplicity, we focus only on the number of h-bonds conserved, gained or lost upon binding and not on a detailed case-specific analysis of rearrangements of the h-bond network that occurs upon binding. this is because we do not have any information on the pathway (within the h-bond network) or dynamic aspects of such changes for each enzyme as we only use x-ray structures representing the initial and final states of the binding process. therefore, any cases where the rearrangement of the h-bond network does not change the total number of h-bonds that occur involving protein and water atoms upon introduction of the ligand cannot be detected."
"to overcome these difficulties in a search of multiple databases in the information of life science, we started developing a new type of application that searches databases in different locations simultaneously by a simple search query and displays the result in a simple interface at http://p4d-info.nig.ac.jp/vapros/. we named the application vapros, variation effect of protein structure and function. the name derived from the aim of the application, namely to focus on analysing effects of dna sequence variations on protein structures and function. vapros aims to realize an idea of \"data cloud\", that is to retrieve data without any knowledge of databases scattered in the internet."
"the process of verifying and validating was completed successfully, as planned, by using white and black boxes testing with no faults, errors, or failures in the system. the results can be explained as follows. first, the proposed framework was able to differentiate between trusted and untrusted devices and between trusted and untrusted users. it prevented untrusted devices from connecting to the cloud and prevented untrusted users from accessing the system. it also enforced the access control policies and provided access to legitimate users only. second, the proposed framework was able to detect attacks that faced access control policies during the transfer, process, and storage phases. it rejected the policies that had been modified after informing the owner of the system. third, the performance tests showed a slight increase in the time response when the number of people increased during the process of enforcing access control policies in the local machine. this kind of test examines the scalability of the system. the result is normal due to specific resource consumption, such as cpu and memories. however, the same test was done in the cloud by a loadimpact tool and showed no increase in the time response. this is because the resources in the cloud are scalable, which means the cloud is able to increase the workload on its current hardware resource on demand with an increase in the amount of billing. fourth, the jmeter testing tool showed the time response for each function in the system. the policy enforcement had the highest time response due to the comparison between the security classification levels of the subjects and objects after retrieval of fourth, we reduced the time needed to make the final decision when an illegitimate request occurs due to the functionality of the check permission agent in the same byod. however, in the case of legitimate access, it takes more time because the decision comes from the policy enforcement agent in the cloud. fifth, these tests were performed using intel core (tm) i7 -5500u cpu (2.40 ghz) and 8.0 gb ddr3 memory, which shows low performance for one user. finally, the integrity of the system is high due to its detection of the attacks, which were unsuccessful."
"cashier has several features which address the limitations of existing techniques. it uses low cost, non-intrusive, dynamic profiling technique which does not affect the operation of llc. also, cashier optimizes memory subsystem (which includes llc and main memory) energy, instead of merely llc energy. simulations have been performed using sniper [cit] suite. the results show that cashier is very effective in saving energy while still meeting most of the deadlines. for example, for 2mb l2 cache with 5% allowed performance slack, the average saving in memory subsystem energy is 23.6%."
"when a client uses their byod to access the cloud environment, the check security requirement agent verifies it meets security policy requirements. once the byod passes and the security agent grant access, three other agents perform their functions when signing in. these are the encryption and decryption, check permission, and signing agents. clients are not restricted to working from one location or to 'hours of work'. they can work from anywhere at any time. people can use any device as long as it meets security requirements. clients create and share data according to their classification levels, which the owner has to approve (or reject). the owner grants access by accepting the request."
"the controller agent creates the check security requirement agent. its purpose is to check all connected devices using an organization's saas in the cloud. the check security agent checks whether byods meet company security policy requirements for being a trusted device. it does this by checking for up-to-date antivirus software, fingerprints, and a vpn connection and installs an agent manager."
"a search by a keyword \"gangliosidosis\", one of the major groups in lsds, resulted in six candidates as shown in fig. 4 . as shown in table 2, gangliosidoses are classified into two types, gm1 and gm2, both of which are further classified into three subtypes. the estimated incidence of gm1-gangliosidosis is 1 per 100,000 to 200,000 births, and those of gm2-gangliosidosis are 1 per 360,000 births for tay-sachs disease and 1 per 310,000 or 1,000,000 births for sandhoff disease. gm2-gangliosidosis ab variant is extremely rare [cit] . each line in the search result happened to correspond to an individual entry of genetic diseases/ disorders in omim database [cit] . the three types in gm1-gangliosidoses (types i, ii, and iii) were related to the same \"molecule symbol\", namely glb1 gene, but tay-sachs disease, sandhoff disease and ab variant in gm2-gangliosidoses were related to hexa, hexb and gm2a genes, respectively. each gene was linked to the databases listed in table 1 . ticking the far left box in fig. 4 and pressing \"details (go)\" button on the top led the user to the further detail of the selected item. in the following section, the search result of each tool listed in table 1 is explained."
"we investigated the role of side-chain flexibility from a steric point-of-view using the wall potential equation (2) in order to understand to what extent side-chain conformational changes are essential to accommodate the ligand in the binding site. to do so we superimpose the apo and holo forms and transplant the ligand to the apo form, what we call apo-bound form. we calculate the wall difference between apo-bound and holo forms for all binding sites in the seq database (fig. 2) ."
one study showed these concern byod owners when 57% of respondents [cit] expressed worry about employers accessing personal devices without their authorization. by far the most concerning issue is the risk of unauthorized access to enterprise systems through byods.
we investigated the latest byod trends to address control systems to protect information security [cit] . we analyzed the requirements for developing a suitable access control system and found there are four requirements.
"to measure performance and scalability, we used different available software based on the required test. [cit] has some useful built-in testing tools that we used to measure the cpu and memory usage. the google cloud platform also has some useful testing tools for measuring traffic, load, cpu and memory usage, and more. we also used the jmeter tool to test scalability because it is a free open source tool specifically for this type of testing. below are the results of these different tests with some comments about each test. the discussion and evaluation of the results are in the next chapter. first, we measured the performance with different numbers of users ranging from 1 to 1000 users for the access control enforcement function, as shown in (fig. 15) . the next test shows the time response for access allowed by the policy enforcement agent and access denied by the check permission agent after the authentication phase and setting up the policies, as shown in (fig. 17) . we used a loadimpact tool to test the load time in the cloud when the number of users is increased as seen in (fig. 18 ) fig. 18 . load time test for the framework in the cloud with increase of number of users"
"where n r i is the total number of cases in which the rotamers differ and n t i is the total number of residues of type i in all binding sites. the second term is the error estimation involved in the measurement. in some cases, the binding of a ligand can lead to major conformational changes, resulting in significant different protein conformations. in order to simplify our analysis, we choose to limit our study to cases where the average backbone displacement of the binding site is below 2.50 å (rmsd). although this threshold may seem a bit permissive, we find that such a threshold offers an acceptable balance as a more stringent threshold leads to a significant loss of data."
"in this study, we show side-chain rotamer changes upon binding are widespread occurring in nearly 90% of binding sites studied. moreover, in 32% of flexible cases (28% overall), steric clashes would prevent ligand binding in the absence of movements. in the context of molecular docking, one may not succeed in finding the right solution in a large fraction of cases if the protein is not allowed to be flexible. at the same time, it is feasible to introduce side-chain flexibility on a limited number of residues as our results show that five flexible side chains account for 90% of the observed cases."
"any solution must meet an organization's security policies, while not breaching user privacy and rights. there has to be the ability to check the security levels installed on each individual device to avoid threats that can change or destroy data. the challenge is to find a solution that does not restrict user access either as it conflicts with the purpose of byod. previous solutions call for device registration before use on a company network. device registration limits the use of byods especially when a device is lost or replaced."
"sequence diagrams for the proposed access control framework are broken into seven sub-frameworks based on the main tasks. we explain the most important tasks, which are: creating and modifying policies or data by policy administrators; clients creating and modifying data; and monitoring the mac policy in the security manager. fig. 6 shows the process for creating and modifying policies and data. to start, the signing agent adds a digital signature to the message when a policy maker creates a new or modifies existing policies or data through the user interface (fig.6 ). then the encryption and decryption agent encrypts the message to transmit it and decrypts it into a readable format for the receiver. a signature verification agent verifies the digital signature. then the policy enforcement agent implements the mac policy to accept or deny illegitimate access requests. there are 'save' mechanisms for both the policy encryption and decryption, and the policy database agents. this is so the user can save new and amended policies and data to the system. the 'ack' method in both agents confirms the save process. finally, the auditing agent records full details of final decisions made by the policy enforcement agent. when clients try to create or modify data, the permission method makes a preliminary decision about granting access based on the username clearances and security classifications. if permission is granted, the 'signing agent' adds a digital signature to the data (fig. 7) . all data is now encrypted by the encryption and decryption agent before being transmitted across the internet. the agent then decrypts the data when received in the cloud. next the signature verification agent verifies the digital signature. the policy enforcement agent implements the mac policy, which denies any illegitimate access requests. the policy encryption and decryption agent and policy database agent save the data. the ack method in each agent confirms the data saving process. finally, the auditing agent records full details of final decisions made by the policy enforcement agent or by the check permission agent during the preliminary decision. fig. 8 shows the process of monitoring the mac in the security manager. monitoring the mac is the main function for protecting the integrity of policies during the processing and storage phases. this process starts with the controller agent activating the policy integrity check agent. it retrieves the policy from the database using the request hash key method. the reply hash key generates a hash value from the policies requested. the value is sent to the monitoring mac policy agent for comparison with the original one. if the values match, the process continuously repeats. when they do not match, the monitoring mac policy agent sends an error message to the controller agent to cease authentication. it records the issue, deletes the existing policies, and sends a message to the owner."
"as flexibility correlates with entropy differences, it is interesting to see if there are differences between flexible and rigid residues with respect to quantities related to geometric constraints. in particular, b-factors and solvent accessible surface (sas) areas. we observe greater sas and b-factors for flexible residues (fig. 4) . these differences suggest that it should be possible to identify flexible residues in the apo form."
"to selectively and dynamically allocate cache to an application, cashier uses cache coloring technique [cit] which works as follows. firstly, the cache is logically divided into multiple non-overlapping bins, called cache colors. the maximum number of colors, n, is given by"
"in this study, we only use x-ray protein structures from the protein data bank (pdb) [cit] . since lower resolution structures (above 3.0 å resolution) raise the level of uncertainty in the assignment of side-chain conformations, only structures with resolution better or equal to 2.50 å are used for this study."
"using the filters and parameters described in the preceding paragraphs, we obtain a redundant dataset that we call the primary (pri) database. nonredundant subsets are derived from the pri dataset."
"although there are small differences on the pattern of h-bonds conserved, gained or lost with water molecules for different amino acid types ( supplementary fig. 8a and b), the overall rates are essentially indistinguishable between flexible and rigid side-chains (left bars, fig. 5 ). approximately 35% of residues lost h-bonds with water, as expected considering that water molecules within the cleft must be displaced during binding, while others may be necessary for enzyme reaction [cit] . interestingly, only around 15% of residues gain h-bonds with water. fig. 5 . rearrangement of the hydrogen bonding network. we plot the proportion of residues that conserve the number, gain or loose h-bonds with water molecules, protein or ligand atoms. we observe a 36% decrease in the number of h-bonds between binding-site residues and water molecules with a concomitant increase of the number of h-bonds for with protein atoms for flexible residues (center left), likely formed with rigid residues as these conserve the number of h-bonds in 80% (center right) of cases. interestingly, ∼75% of residues that could form h-bonds with the ligand upon binding do not fulfill these interactions (right-hand bars) we also compare the number of h-bonds formed within the protein (center bars fig. 5 and supplementary fig. 8c and d) . rigid residues maintain the number h-bonds in 80% of cases compared to 60% for flexible residues. this result supports our rotamer-based approach in the sense that residues that did not significantly move (rigid residues) are less prone to create or lose h-bonds than flexible residues. on the other hand, flexible residues have a slightly higher h-bond gain than loss. this result raises the possibility that flexible residues are replacing h-bonds with rigid residues that occurred before binding with water molecules. we also analyzed the number of h-bonds between the protein and the ligand for flexible and rigid residues. we notice a slight increase in the formation of hbonds with the ligand for flexible residues (28% over 22% for rigid) for residues that initially formed h-bonds with the protein (supplementary table siv) . interestingly, three quarters of the residues that participate (or can participate) in h-bonds do not create h-bonds with the ligand (right bars fig. 5 and supplementary fig. 8e and f ). as such, there is a vast untapped well of potential interactions occurring unfulfilled or fulfilled with protein atoms and/or water molecules that could be exploited in terms of creating new interactions with, for example, an inhibitor."
"case 4: the use of a trusted device with untrusted users. for the first case, the 'check security requirement agent' was able to detect an untrusted device that does not meet the organization's requirement of an updated antivirus program, as seen in (fig. 10) . in this scenario, the application will not be allowed to connect to the cloud. for the second case, 'check security requirement agent' allowed the device to connect to google cloud because it is a trusted device. both 'check permission agent' and 'policy enforcement agent' allowed trusted users to access wanted resources. for case three, the system detects users that want to access illegitimate resources, as shown in (fig. 11), by verifying the mac security classification level of the user and comparing it with the security classification level of the wanted resource using the bell-lapadula model to gain access. the final case is for untrusted users (i.e., users who do not have permission to access the system and certainly do not have a mac security classification level). 'authentication agent' can discover these users and prevent them from accessing the system, as shown in (fig. 12) . www.ijacsa.thesai.org"
"as we are entering into an era of green computing, the primary objective in chip design is shifting from achieving highest peak performance to achieving highest performanceenergy efficiency. in battery-powered mobile systems, such as cell phones and laptops, achieving energy efficiency is especially important, since these systems work on batteries which store limited energy. moreover, since these systems also need to fulfill application quality-of-service (qos) requirements [cit], a fine balance is required to meet the dual goals of energy saving and minimum performance loss."
"lysosomes are subcellular organelles responsible for the physiological turnover of the cell constituents. they contain catabolic enzymes that require a low ph environment for their optimal function. lsds are a heterogeneous group of more than 50 rare inherited disorders characterized by the accumulation of undigested or partially digested macromolecules (table 2) . lsds ultimately result in cellular dysfunction and clinical abnormalities. lsds are caused by deficiencies or defects in enzymes for lysosomes, in proteins necessary for the normal post-translational modification of lysosomal enzymes, in the activator proteins of lysosomal enzymes, and in the proteins important for proper intracellular trafficking between the lysosome and other intracellular compartments. the individual diseases are rare, but lsds as a group affects many people around the world with a frequency of about one in every 7000-8000 live births [cit] ."
"many organizations fail to implement appropriate security policies for employees using byods. where organizations do have security policies, they are inadequate because they do not address technical or organizational requirements for information security [cit] . this makes controlling personal devices the biggest security risk for companies [cit] . although there are applications available to manage and control personal devices, organizations are not using them in an appropriate way [cit] ."
"over three weeks, we found 2,860 access requests in the logs. twenty policy attacks occurred, none of which were successful since they were detected by the system. to measure integrity for a particular type of attack, we need to know the probability that an attack of this type will occur within a given time [cit] . the integrity attack is defined as:"
"iii. system design several real-world applications present soft real-time resource demands. in such applications, the task deadlines are usually more relaxed than the task completion time and as long as a task is completed by its deadline, the actual completion time does not matter from user's perspective. in such systems, cashier can save leakage energy by using cache reconfiguration, while making best possible effort to meet the task deadline. for enabling cache reconfiguration, cashier uses cache coloring technique (section iii-a). for estimating miss rates under different l2 configurations, it uses rce and using cpi stack method, it estimates program execution time with those configurations (section iii-b and iii-c). the energy saving algorithm (esa) uses these values to estimate memory subsystem energy and finds the configuration with minimum energy and bounded performance loss (section iv). we assume that llc is l2 cache and based on this description, cashier can be easily extended to case when llc is l3 cache."
"from the literature review, we see previous studies address single issue without providing a complete access control solution. as a result, these solutions are insufficient and require further research. we integrate several parts of these to develop a new solution for byod access control. we describe this in section iii. this paper focuses on the technical side of the solution. it does not attempt to develop the required processes a user needs to follow to support an access control policy."
"the check permission agent functions to speed up the process if user access is denied before it sends a request to the cloud. it also displays to users their permissions when accessing specific resources. for example, users will see permission details such as read only, read and write, against each file when the system grants access. once a user has access at this level, the next check occurs in the cloud by the 'policy enforcement agent'."
"understanding the factors that affect protein flexibility has important practical applications, as efforts in trying to simulate flexibility (even restricted to side-chain movements) have been limited so far by the drastic increase in the size of the associated conformational search space. for example, docking algorithms are an example in which the flexibility of the protein can have a drastic impact on the results. as such, any knowledge that can be applied to decrease in a sensible way the size of the search space is advantageous."
the policy enforcement agent saves a copy of the first hash value generated/updated by the owner. it continuously uses this as a comparison with newly generated hash values for the same mac policy. these should all be identical. policy monitoring and integrity checking agents check for modifications to a mac policy during transmission and has only been sent by the policy administrator. it informs the policy administrator and controller when there is a security breach.
the encryption and decryption agent makes sure only authorized users and agents access and read the information transmitted. its function is to keep the information in the message secret. it does this by encrypting and decrypting all transmissions traveling between the security manager and user devices. this agent converts messages into an unreadable format to transmit them. it then reverses the process to convert the messages into a readable format for the user. the agent encrypts messages using an asymmetric algorithm (also known as public-key cryptography). during transmission it exchanges this for a symmetric key (which is the advanced encryption standard (aes)) to decrypt the mac policy (fig. 4) .
"here energy spent in l2 and memory is composed of both leakage and dynamic energy. further, we use the symbols e dyn xy z and p leak xy z to show the dynamic energy per access and leakage energy per second, respectively, spent in any component xy z (e.g. l2, memory, rce). to calculate l2 energy, we assume that an l2 miss consumes twice the energy as that of an l2 hit [cit] . the leakage energy is proportional to active area of the cache [cit] . thus,"
"five of them modified during the processing phase, and five of them modified in the database. the hash value changed and was detected by policy monitoring and integrity check agents, as shown in (fig. 13) . during the transfer phase, we tested the 20 accesses of the control policy with different characteristics. five of them had the correct digital signatures, five of them had incorrect digital signatures, five of them had the original cipher text, and five of them had the modified cipher text. both the encryption and decryption agent and the signature verification agent detected all modified access control policies, as shown in (fig. 14) ."
"in this paper, we presented cashier, a dynamic reconfiguration based cache energy saving approach for qos systems. cashier achieves a right balance between the opportunity of energy saving and performance loss and fully adapts itself according to the available slack to maximize energy saving. thus, cashier saves energy with a small and bounded performance loss and may allow using a larger cache for the same energy budget to obtain even higher performance."
"for cache block switching, we use the gated-v dd scheme [cit] . we assume a specific implementation of gated-v dd transistor (nmos gated v dd, dual v t, wide, with charge pump) which results in minimal impact on access latency, but 5% increase in the cell area [cit] . we account for this overhead in our energy model (section vi)."
"the policy database agent is static and communicates with other databases as a service (dbaas), database management systems (dbmss), or distributed database management systems (ddbmss). this agent exchanges the data as it transmits across different software architecture styles and patterns."
"one problem with the non-redundant pfam dataset is that the same or different proteins representing the same pfam domain and bound to different ligands appear as different entries in the database. to remove this source of redundancy with respect to protein sequences, a more stringent dataset called seq was derived from the pfam dataset. in the seq dataset, only entries with protein sequence identity below 50% for a given pfam domain are retained. as before, the choice between different entries is made based on r c . supplementary figure s1 shows the relationship between the pfam and seq datasets. in this case, one is ultimately loosing some bona fide entries by choosing a representative entry among many containing different ligands."
"regarding all the aspects above, we need to both guarantee there is enough words in knowledge base but avoid ambiguous of their meaning in the same time. the detailed steps to generate our knowledge base are as follows."
"\"the first series of standardized forms of words with non-standardized variant form\" is undoubtedly our standard of new ones to eliminate obsolete ones. but we need to notice that it eliminate non-standardized variant form. on the basis of non-standardized variant form and traditional ones. therefore, it can only be used as a main standard for eliminating non-standardized variant form words, but cannot be used as a normative standards to write new ones and simplified ones. even in the field of eliminating non-standardized variant form, its role is quite limited. since there was also some adjustments after the release of the table, some non-standardized variant form is restored in the \"simplified words table\" and \"modern chinese generic word table\" released after. so if there is any inconsistency with \"simplified words table\" and \"modern chinese generic word table\", the latter should be used as standard."
"substituted conception is defined as: for a certain part in the text, we can find something in our knowledge base and replace them with the same or similar text for representation. and the replacement will not change the meaning of the text and it also does not cause ambiguity or error."
"in the concrete realization, firstly, convert the secret message into binary strings. then apply word segmentation operation upon the original text. traverse 的 得 the encrypted message, for each particle ' ' and ' ', reserve or replace them according to the binary string of 地 the secret message. here, we make the rule that ' ' 得 represents 1, while ' ' represents 0. repeat the steps above until finish the embedding work of secret message and then add the flag that represents the end of bit. if the embedding work is not finished after traversing all the words in text, then the embedding work is considered to be failed. when recovering the secret, split the sentence of the encrypted message. for each sentence, if there 的 exists ' ' that can be added or deleted, then decide if 的 there should be a ' ' in the text. if there should be, then add 1 to the recovering message, otherwise, add 0. repeat the steps until the end flag."
"the aim of channel coding theory is to find codes which transmit quickly, contain many valid code words and can correct or at least detect many errors. while not mutually exclusive, performance in these areas is a tradeoff. so, different codes are optimal for different applications. the needed properties of this code mainly depend on the probability of errors happening during transmission. thus the channel coding theory is developing to improve the quality of communication system. the basic method is to add redundancy symbols according to some special rule in the sending messages to ensure the reliable of the transformation. the target of channel coding theory is to construct a good encoding method to gain the minimum redundancy at the expense of the largest anti-interference performance. in this article, we use hamming code to increase the robustness of our system. e.g. we have a piece of article in which there contains secret messages. once the attacker knows the fact, he can do some changes to the article although he can't get the information hide in it. he may delete some '的' randomly, which will cause lose of message. above all, it is import for us to protect the integrality at the cost of declineing the embedding rate."
"in this article, we propose an information hiding method based on the substituted conception. in the second chapter, we show our text hiding system platform which combines the encoding of the secret information with searching of self-definition knowledge base. we introduce the definition of the substituted conception, knowledge base and coding algorithm first. furthermore, we also give a detail description about how to use the conceptions. an improved coding algorithm--lz code along with hamming code, is presented in chapter ii too, which makes the system more robust and capacity. performance testing of the system platform is listed in the third chapter."
"\" \" \" \" \"as attribute e.g.:其其其见--其其的其见 we can conclude that all the function words that meet the rules above can be added or deleted."
"we first take one symbol as the first part and keep doing segmentation. if we meet with the same symbol or symbol sequence as we meet before, we add one more symbol from the sequence next to the current one to make it different from the part already exists. we save all the parts in a table as dictionary. when the dictionary reaches a certain size, we do segmentation just according to it until the end of the sequence."
"by introducing hamming code to information hiding technology, we divide the binary form of secret messages into groups and then embed them into several paragraph of original messages to improve resistance to destructive. the encoding steps are as follows."
"after the release of the table, the number of words in chinese characters is reduced, and the chinese character system is also improved greatly to the direction of standardization. it also curb the font confusion phenomenon in the use of chinese characters."
"the commonly used methods include moving the position of adjunction, adding formal subjects, changing the active/passive form, changing the statement sequence. especially, mimicry steganography formulation uses context-free method to describe the structure of the sentence to construct steganography text, and chooses different sentence structure to hide information. it has greatly helpful to promote syntax-based information hiding technology."
"we need lots of text to hide secret messages in to test its security, capacity and robustness of our algorithm. our test sources is mainly from top magazines in china, such as \"chinese journal of computers\" and \"chinese science\". besides, there are also some article from prp project in sjtu."
it is generally considered that it is mikhail j. and m. attalla of purdue university who first proposed the concept of natural language text information hiding [cit] . natural language information hiding technology takes advantage of natural language processing technology. it embeds secret information by changing the attributes of the original text while keeping their meaning. there are three kinds of text information hiding methods.
"an improve method in encoding information hiding technology is mainly used in secret communication and protection of copyright for digit text. we have to come up with a method to insert as much text information as we can into a piece of article which is not very long itself. and in this same time, it is obvious that the robustness is the most important check point for information hiding technology. we can even improve the robustness at the cost of reduce capacity volume. in this way, we can fullfil the two needs by encoding the secret messages first."
"hamming code, which is a kind of parity code, is a family of linear error-correcting codes in telecommunication, name after the inventor of it, richard hamming. hamming codes can detect up to two and correct up to one bit errors. by contrast, the simple parity code cannot correct errors, and can detect only an odd number of errors. hamming codes are special in that they are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance."
"the method based on the synonym substitution is the most widely used method in chinese natural language information hiding method. in synonym substitution algorithm, we select words that appears in our knowledge base and encode them by some certain encode method to embed information."
"the so-called synonyms, generally defined as \"in one language (chinese here), synonyms shares the exactly meanings between two or more words in some or all the semantic\". [cit] . in the original chinese synonyms replacement method, researchers did not do the word segmentation. they check in the vector text in order to find a synonym in the dictionary and do the synonyms replacement to embed information. this method has an obvious defect; it doesn't do the word segmentation so when it finishes the synonymous substitutions, it is still very easy to change the meaning of the original semantics, resulting in the practical application of this algorithm is of little significance. therefore, the general chinese synonym replacement algorithm must first process an automatic segmentation."
"we pick up some word groups of exactly same meaning. then we cut the number of words of all groups to the same. thus we get a dictionary of synonym words. after the steps above, we finally get a knowledge base of about 17000 words. the knowledge base covers almost all the synonym words and is practical to use. in this way, we can get high efficiency and more accurate when doing information hiding based on substituted conception since we have do some optimization, classification and encoding in advance."
"as mentioned in 2.1, using \"the dictionary of synonym words\" directly cannot meet the condition of large enough vocabulary. and it will also encounter with the problem of semantic inconsistency in the context. in this section, we proposed a method for the construction of a better knowledge base, mainly used in the algorithm of using synonym as replaceable unit, and the other part used in the algorithm of using homophones as replaceable unit."
"kolmogoloy, the math scientist of soviet union proposed an encoding method by taking advantage of the construction feature of source messages. after that, two researchers from israel, j.ziv and a.lempel find another method to create an even higher efficient encoding way than huffman code. the new algorithm is totally different from huffman algorithm and math algorithm. it also enjoys a faster compress algorithm. now the series of algorithm is called lz algorithm."
"on the basis of \"the dictionary of synonym words\", the harbin institute of technology information retrieval laboratory made the \"the dictionary of synonym words modified version\". the extended version referenced many electronic dictionary resources. it get the frequency of a word in people's daily corpus and only retained the words with the frequency no lower than 3 (the statistical results of a small-scale corpus). at last, a total of 77434 words are included by the dictionary. the dictionary organized all the included words together in accordance with the tree hierarchy. the vocabulary is divided into three categories as large, medium and small. there are 12 large categories, 97 medium categories and 1428 small categories. then divide the small categories into word groups. words in each word group are further divided into several lines. words in the same line holds the same meaning (or close meaning) or strong correlation."
"through the incomplete word frequency statistical study on different kind of text styles, we found that the 的 了 是 function words such as ' ', ' ', ' ' are of high frequency in chinese. even if the original text is a science and technology article, the weakness of other semantic based text steganography approach will not appear. when the text carrier contains a large amount of terminology or highly restrictive words, a slightly change on the word order or substitution with synonyms will change the meaning of the original sentence or even make it incomprehensible. in some circumstances, function words can be added or deleted without changing the original meaning or the quality of the text, which will not draw the attention of other detectors. the algorithm proposed in this section will take advantage of this feature to hide information. 的 we use the high frequency function word ' ' as the first object to study. firstly, we summed several rules that can be embedded with hidden information. in these rules, 的 的 the ' ' in the ' ' structure can be added or deleted without other people's awareness. 的 rule 1 ' ' can be removed when using monosyllabic adjective as attribute e.g.:新新新--新的新新 rule 2 nouns as attribute e.g.:玻玻玻--玻玻的玻 rule 3 pronouns as attribute e.g.:我我我--我的我我 其其 、 其其 、 其其 rule 4 \""
"in the concrete realization, firstly, convert the secret message into binary strings. then traverse the text and apply word segmentation operation. determine on each 的 function word, if there existed ' ' that can be added or 的 deleted, then decide to add or delete the ' ' according to the binary string of the secret message. here, we make 的 the rule that the added or originally existed ' ' represents 1, while the deleted or originally not existed 的 ' ' represents 0. repeat the steps above until finish the embedding work of secret message and then add the flag that represents the end of bit. if the embedding work is not finished after traversing all the words in text, then the embedding work is considered to be failed. when recovering the secret, split the sentence of the encrypted 的 message. for each sentence, if there exists ' ' that can 的 be added or deleted, then decide if there should be a ' ' in the text. if there should be, then add 1 to the recovering message, otherwise, add 0. repeat the steps until the end flag.."
"the synonym is a king of substituted unit. there are a lot of researches methods of the substituted method based on synonyms in the foreign country and have a good result in consistency or the encoding of synonyms. however, for the chinese own characteristics, chinese information processing technology is far from mature. it needs to put in more effort in the text information hiding. we will present an integrated replaceable unit definition, knowledge base generation and encoding algorithm of information hiding system platform."
"(1) standardized forms of words and nonstandardized ones. chinese language is profound. in its development process, there appears a large amount of standardized forms of words and non-standardized ones. although some of them is rarely used and even eliminated, the rest are still in using and can be used in the technology proposed in this section.\" the first series of standardized forms of words with non-standardized variant form\" [cit] lists 338 groups of commonly used non-standardized ones and recommend ones, these words can be directly add into the homophones dictionary."
"the first one is based on changing the format of the text. this method is for the text with a certain layout format or file structure. compared to the plain text, the formatted text contains more redundant capacity in format information. for example, reference [cit] hides information by adjusting the line spacing, word spacing, font, and character size, building signature or using special formats like document head. this method possesses favorable commonality. but it can't hide large information, and once the algorithm detail is published, it will be easy to be cracked."
"we first convert the message we want to hide to the binary string. then we do a segmentation of the whole text and go through every word of the whole text which finished the word segmentation to determine whether the word is in the dictionary. if the word is in our knowledge base, we first get the encoded information by the secret messages. then we decide which words in this group should be substituted by the number of the group and the position of the words. the encoding method will be indepth discussion in chapter 5, including the additional length check code, this can be very effective to improve the robustness to prevent the amount of information loss caused by destruction with hidden information file, do this things till all the information hiding finished and joined the logo identifies the end of bit. if we have finished traversing the text but still not get embedded work done, the information hiding process returned as failure one. get hidden information algorithm, the first step is doing the word segmentation of the text which has the hidden information. after traversing each of its words, the user determines whether the word is in the thesaurus base. if it is in the thesaurus base, according to the number and location of the group of words in the word and the embedded algorithm coding and decoding to obtain the response of binary string, and repeat these steps until the end of the flag. this article uses the \"hownet\" and \"the dictionary of synonym words\" as mentioned dictionary above because it has a large size of the lexicon and it has a collection of nearly 70,000 words, all the lexicon compiled by significance. it is such a dictionary that it is suitable for use as a synonym-based text to hide the technology. in fact, due to the use of flexibility of the chinese, direct use of this lexicon will encounter the problem of inconsistent semantics before and after the replacement text. for example, the \"fight\" and \"hit\" are synonyms, but \"taxi\" must not be replaced by \"hitting the car\". please refer to in-depth study on this issue in chapter 2.2"
"the symbol parts in dictionary are then encoded. set m as the number of segmentations in dictionary. if we encode the dictionary as binary code, the length of encoded word is as follow. n log lempel-ziv code's encoding method is very easy and fast. the same is its decoding method. it doesn't need the whole dictionary before decode. it can set up the dictionary in the same time decode the messages. the only thing to send with messages is the size of the dictionary. besides, the feature of lempel-ziv code is that the longer the source message is the higher efficiency it has. it will reach the maximum value according to shannon's theorem"
"(2) structural particle. the homonym substitution words mentioned in (1) are of very low frequency in the text, so the content that can be embedded is strongly restricted. therefore, we come to the idea about using the structural particles like 的 得 地 ' ', ' ', ' ', which appears with high frequency in any text, to optimize the text steganography method based on homophones substitution. 的 since ' ' is already used is the previous section, to avoid confusion, we will discuss on the mutual substitution betw 地 得 een ' ' and ' '. 地 ' ' follows the adverbial when it is used as structural particle, representing a relationship of modification between adverbial and central word, which can be used 的 for homonym substitution with ' '. one of the typical structure is (adverb, 地 adjective) + ' ' + (verb, adjective). 得 ' ' follows the verb or adjective when it is used as structural particle, con-nects the complement that represents the extent or results. or, it is used be-tween verb and complement, representing possibility. it can also 的 be used for homonym substitution with ' '. one of the typical structure is (verb, adjec-得 tive) + ' ' + (verb, adjective, adverb)."
"comparison of performance we will show the performance of three different methods which are all based on substituted in conception in security, capacity and robustness. table 2 shows the statistics of capacity. the frequency of substitutions is highest for the method that based on synonym words. it is up to 36 per 1000 words. but the other two methods don't show good. it is because that the chinese doesn't pay attention to the distinguish of \"的\",\"得\" and \"地\". table 3 shows the security when hiding information. it is apparently that it is hard for the attacker to notice there are secret messages. besides, there is no influence to the meaning of original text. as for the robustness, if the attacker knows the method how we embed secret messages, it is easy for him to delete, add or modify the functional words and auxiliary word, which will cause information lost largely. but he can only attack the text randomly if the hiding information is embedded based on knowledge. so it is more safe if embed messages according to knowledge base. table 4 shows the impression on system execution speed after encoding the secret messages. although the consuming time increase after encoding, it is still accepted even it reaches a level of 50000 words. table 5 shows the lempel-ziv code's impression on system capacity of hidding secret messages after encoding them. as it can be seen, it takes a much larger space to store the information without any encoding. and after introducing lempel-ziv code, it compress about 50% space. it gives a better performance by lz code. besids, the compress ration is increasing while the increase of the text length because of the lz algorithm itself. when the text reaches 50000 words, the compression ratio is less 10%. she is now major in content security lab of sjtu in shanghai as a graduate student. [cit] conference for a paper about information hiding technology."
"hownet is a common sense knowledge base, which used the concept represented by the chinese and english words as the object of description, and used the revealing of the relationship between concept and concept as basic content."
"the cause of semantic inconsistency in the context is that even synonymous will have the difference of exactly same and partly same. in the synonym dictionary proposed in 2.1.1, some of the synonymous groups are of exactly the same semantic, while some are of partly the same semantic. to distinguish these synonymous groups, we need to compare the semantic of each word in the synonymous group. we get the semantic representation of each word by hownet, and then determine the synonymous relationship between each word in the synonymous group."
"information hiding means concealing the information itself and its location. the hidden message can be text, passwords, images, graphics or sound. it also can be the file in one's computer. at the meantime the information carrier can be a general digital image, digital video, software, digital audio, or text file. research in information hiding has a long history and in recent years it becomes a research hotspot again. judging from research papers published in this area, most researches focus on how to hide open information and digital watermark with image file. this is mainly due to that image file has large redundant information capacity, and image processing is more intuitive. since the existence of redundant information, we can hide some information without the suspicion of the observer. but text file does not contain any redundant information for secret information transmission. information hiding in text is not a simple mission."
"other than a large amount of synonyms in chinese, there also exists a large amount of homonym replacement such as standardized forms of words and nonstandardized ones. the difference between the homophones and synonyms is that they are exactly the same in pronunciation and meanings but different in written form. we can also realize text steganography by replacing homophones between standardized forms of words and non-standardized ones."
"data compression is for source encoding. it makes the messages transformation quicker and higher efficient. it also increase the capacity of text information because it can decrease the loss of entropy, thus increase encoding efficiency. its main target is to reduce the redundancy between content. [cit], which is the very start of modern information theory. the shannon's theorem, sometimes also called the noisychannel coding theorem, establishes that for any given degree of noise contamination of a communication channel, it is possible to communicate discrete data (digital information) nearly error-free up to a computable maximum rate through the channel based in part on earlier work and ideas of harry nyquist and ralph hartley. this offers theory evidence for source encoding to compress redundancy information. in this article, we use lempelziv encoding as source encoding."
"volume flexibility is the range of profitable production volume. it is important both in autologous cell products or allogeneic cell products. since autologous cell products are manufactured according to the patient's treatment plan, the date and time of manufacture cannot be significantly changed. therefore, the capacity of the manufacturing facility must be designed to its maximum capacity, and then volume fluctuation also increases. allogeneic cell products on the other hand are usually cryopreserved, so the production schedule can be changed by consulting the donor. therefore, it is theoretically possible to maximize the operational rate of the facility. however, quality testing (including viral clearance) for allogenic cell products have significant costs, so that it is more efficient to make the batch size as large as possible. when the size of a batch exceeds the number of orders in the same time period as the production time of the batch, the facility must be paused to avoid over-production. in both cases, the operational rate of facilities tends to be low, and cutting the costs during idling time is essential."
"labor flexibility is important in chbp manufacturing. a major difference from the automotive industry is in the heterogeneity of jobs. there are many job classes in the automotive industry according to the number of processes. generally, a chbp has fewer processes, thus, fewer job classes are needed. in the simplest case, there may be only two classes, quality control class and manufacturing class. however, in conventional manufacturing facilities, many of the important manufacturing processes are dependent on trained technicians. therefore, the quality variance between operators is an influential element in the quality of chbps (referred as uniformity element in ref. [cit] ). in a cell culture process for chbps, the work date is strictly determined for consistency in the process, and it is not possible to change the work date for the convenience of workers. therefore, it is necessary to always maintain surplus personnel to accommodate issues such as sick leave. therefore, automation of cell culture processes is one remedy for employment redundancy."
"finally, we would like to point out that 'portability' is an important feature of fmp. when it is necessary to relocate a large factory due to changes in economic environments, significant costs are incurred to transfer and restart the facility. in that respect, since fmp is composed of relatively small modules, it is easy to transport and start up. any large factory relies on many skilled workers, but when it is difficult to hire them at one location, new workers need to be hired and educated. an additional important point with fmp is that such labor costs are minimal. this is applicable not only when moving factory location but also when the owner of the factory changes. in other words, it is easy to purchase and resell the facility. such mobility is an asset that may reduce the risks associated with changes in the financial environment (asset flexibility)."
"at present, most cbhp manufacturing facilities adopt a manual operation-based production system utilizing clean rooms and table 1 . biological safety cabinets. this is partially based on the general idea that production by trained operators is more efficient in smallvolume situations than production by automated manufacturing equipment. on the other hand, even if the scale of production increases, problems still exist that make it difficult to introduce automated devices. firstly, the whole manufacturing process of a cbhp, including introduction of sterilized consumables and culture media is too complicated to automate because aseptic handling is required throughout the whole process. secondly, even if automatic equipment is only used for a specific sub-process, introducing the equipment to an existing clean room can add extra-cost. for example, the equipment may need to be re-designed because manufacturing equipment used in a clean room should have dustfree properties and biological cleanliness. alternately, a larger clean room may be necessary to install large-sized apparatus. in either case, the difficulties faced to introduce automatic equipment into cbhp manufacturing facilities have not been solved yet."
"the introduction of biological isolators and industrial factory automation technology allowed us to assemble a novel cbhp manufacturing system, the tissue factory, based on the concept of flexible modular platforms, which we believe will produce costeffective manufacturing processes for cbhps owing to its many 'flexibilities'."
"firstly, multi-layered skeletal myoblast sheets were manufactured. it has been reported that recovery of cardiac function can be expected by transplanting multi-layered skeletal myoblast sheets to heart failure patients [17e19]. in advance preparation, a procedure for manual culture [cit] was modified to be executable by the system. in the cell isolation process by the cell processing module (m2), muscle tissue was subjected to enzymatic digestion and shredding at the same time in a dedicated vessel equipped with cutting blades, though the muscle tissue was minced before the enzymatic digestion in the manual procedure. the digested tissue was filtered in the same vessel to remove any undigested tissue and large debris, then the cells were isolated by centrifugation. as a result of comparing this method to the standard manual method, equivalent yield and myoblast purity were achieved [cit] . expansion of the culture process was accomplished in the large scale culture module (m9) by utilizing a dedicated culture vessel. an important feature of the vessel is its large culture area to reduce the number of vessels required. another feature is its hermetically sealed structure with tubing. clean air with 5% co 2 was injected directly into the vessels. such large culture vessels are difficult to handle manually, and even a minute inclination causes a nonuniform depth in the culture medium. in the large scale culture module (m9), the depth of the culture medium is kept uniform by a mechanism controlling the inclination of the culture vessels. concentrating the recovered cell suspension in the sealed vessel by centrifugation was difficult to automate because it required separating and reconnecting the tubing. therefore, to solve this problem a membrane concentration method utilizing a hollow fiber module was adopted. as a result, it was possible to concentrate a large amount of cell suspension by a simple method compared with separation by centrifugation. the stacking of cell sheets had originally been carried out using tweezers and transfer membranes in manual operations, but now the system integrates a cell sheet manipulator technique [cit] . the gelatin gel preparation module (m4) forms a gelatin gel pad on a manipulator, and the cell sheet stacking module (m5) stacks cell sheets using the gelatin gel. a multi-layered cell sheet is then produced on a temperatureresponsive dish. each manufacturing process was comprised by a combination of several modules as shown in fig. 4 . a total of four runs were performed using swine muscle tissues, and multilayered skeletal myoblast sheets were successfully obtained ( table 2 and fig. 5 ). this procedure was performed without decontamination."
"conversely, because fmp can be decontaminated on demand, it is possible that non-working time costs can be reduced by simply switching off the facility when no production runs are scheduled. also, fmp does not require a large number of workers at all times, as stated above. therefore, fmp may be able to significantly reduce the risk of cost increases for sudden loss of production orders, either planned or unexpected."
"3. according to the pairwise comparisons of evaluation criterions and criteria in the selection of maintenance methods for transformers in hierarchies, the corresponding comparison and judgment matrix based on trapezoidal fuzzy numbers is given 4. according to the methods mentioned in this paper, the accurate weight of criteria, the accurate local weight of its sub-criteria and further the global weight of sub-criteria are obtained."
"for this reason, the paper presents a kind of fuzzy ahp based on the goal programming to solve the selection of maintenance methods for transformers."
"to maintain the aseptic environment after decontamination, the chambers of all modules, except for large scale culture module (m9), are maintained at a slight positive pressure against the outer environment that prevents microorganisms from entering through potential microscopic holes. both immediately after decontamination and during the subculture processes, no falling or adhering microorganism were detected."
"within this system only the transfer module (m1), large scale culture module (m9) and material preparation isolator (m10) were fixed to the floor. the height of the transfer module (m1) was 2460 mm. the side of each hexagon of the transfer module (m1) was 535 mm long. unlike the other incubation modules, the large scale culture module (m9) was designed to be stationary, and the module had no sealed chamber because it uses only sealed culture vessels with pre-assembled tubing (a closed system). the other modules (m2-m8) were movable, and the heights were limited to under 1650 mm to ensure stability during movement. these modules were equipped with casters, allowing one operator to move and connect or detach from the transfer module (m1). base plates of the processing modules (m2-m5) and material loading module (m6) were set at 750 mm above the floor. robots and processing units were attached to each base plate and covered with a chamber that isolates its operational space from the outer environment. clean air was provided from the ceiling of the chamber through a high efficiency particulate air (hepa) filter, and the air was evacuated from the corners of the base plate. in open air operations, such as opening the lids of culture dishes, articles were handled at least 300 mm above the base plates where stable downward airflow was formed, minimizing the risk of particle contamination."
"as mentioned above, fmp could reduce the manufacturing cost of cbhps by minimizing the facility scale and by changing the cleanliness management strategy in comparison with conventional clean room-based facilities. moreover, the 'flexibility' of fmp may further reduce the cost. for example, in the tissue factory, recombination of the system is possible due to its standardized connection interface between modules. when regulatory problems can be cleared, it is possible to improve the operation rate of equipment by manufacturing multiple cbhps and/or multiple batches in parallel. it also makes it unnecessity to completely remake the whole system for a new product. it is possible to establish a new manufacturing process by combining existing modules and adding new modules."
"in designing manufacturing facilities based on fmp, it is necessary to consider handling multiple kinds of cbhps that may differ from its original purpose. in other words, not only does it need to produce the original product, but also be able to adapt to future functions that are not yet envisioned. although such a way of thinking is fundamental to software development, it has not been sufficiently considered in hardware development, especially in manufacturing equipment. we defined interfaces for both software and hardware to extend the future usefulness of the system. dispensing cell suspension into culture vessels or changing medium. this module consists of a unit that injects a specified amount of cell suspension or medium to each vessel from a temporal stock bottle, and an aspiration unit that removes the medium from culture vessels."
"preparing a gelatin gel for stacking cell sheets. this module has a unit for heating and stirring the gelatin solution, a pipetting unit for pouring the gelatin solution into a mold, a cooling unit for solidifying the gelatin gel, and a unit for removing the gelatin gel from the mold."
"when choosing maintenance methods, we need to consider many attributes. some attributes are quantitative, such as hardware and software cost, training cost, time between failures, equipment reliability, etc., while some ones, such as security, flexibility, workers' acceptability, etc. are qualitative data and usually fuzzy and uncertain. fuzzy theory is applied to determine the value of quantitative attributes, and the fuzzy multi-attribute decision-making method can be used."
"another merit of a biological isolator is that it can be decontaminated. in this study, decontamination processes were performed in culturing human chondrocytes and hipscs. in a biological safety cabinet, it is common to clean the chamber at the end of a single culture process in order to prevent cross-contamination among different batches. decontamination of biological isolators provides a more reliable means to prevent cross-contamination than manual wiping with disinfectant. in the tissue factory, decontamination with a 10 à6 reduction by hydrogen peroxide was achieved. decontamination is also useful to clean the outer surface of materials before being carried in. the only disadvantage of decontamination, is that it takes a considerable amount of time to remove the hydrogen peroxide after decontamination, as shown in supplementary material 1. particularly in incubation modules, even a small residual of hydrogen peroxide, under 1 ppm, may affect cell viability, and careful removal is necessary. therefore, new technology for reducing the time required for decontamination is required."
"1. the modeling of problems the decision-making problems of the ahp should be divided into different hierarchical structures. each hierarchy is composed of several decision-making factors. an overall goal is at the top, a group of alternative solutions is at the bottom, and one or more decision-making criteria and sub-criteria are in the middle tiers."
"the software interface for each module was designed to be independent to facilitate development of new modules. each module has its own control software, executes a single operation according to a command from the process control software, and reports the result. the control for maintaining an aseptic environment is done by an environment controller. this allows developers of each module to concentrate on developing culture operations. the schema of the control software is shown in supplementary material 1. both software and hardware handshakes are performed simultaneously at the time the modules are connected to avoid separate management of connection states for software and hardware."
"as discussed above, in manufacturing chbps, flexibility is required from different viewpoints in comparison with the automotive industry. especially in chbp manufacturing, it is difficult to maximize the operational rate of facilities and to equalize the number of required operators throughout the year. in this respect, fmp may be able to provide a better solution. in conventional manufacturing facilities, air conditioning, cleaning and monitoring are always performed regardless of the operational rates because stopping and resuming the facility incurs high costs and takes time."
"consumables used in the tissue factory are listed in supplementary material 1. several types of cell culture consumables were designed specifically for the system. a dedicated cell culture dish was designed with a sbs standard footprint so that the transfer module (m1) could handle them directly. the culture area is 6.5 â 10 cm and the depth is 2.46 cm. the culture surface was treated to handle either a cell culture (o 2 plasma treatment) or for forming cell sheets (temperature-responsive surface) [cit] . the large scale culture vessel is a dedicated culture vessel for the large scale culture module (m9). its culture area is 4200 cm 2, and the large scale culture module (m9) can store a maximum of six vessels. medium/cell suspension containers are used in the seeding and medium change module (m3) for temporary storage of large amounts of medium or cell suspension to dispense into smaller culture vessels within a short time frame. the enzyme treatment container functions as a receptacle for enzyme digesting and mechanically cutting supplied tissue for primary cell isolation. the gelatin gel mold is a container for solidifying a gelatin gel for cell sheet stacking. for other materials, commercially available products are mounted on dedicated stands with sbs standard footprints."
"(1/7,1/6.5,1/ 5.5,1/5) sm (2,2.5,3. table ⅵ and the table ⅷ, and the results are shown in the last line of table ⅷ . therefore, the best maintenance method for transformers is condition-based maintenance. it is more suitable for transformers than post-failure maintenance and scheduled maintenance because it can improve the safety of transformers, bring more additional benefits and reduce production losses, and it is far better than predictive maintenance in the aspects of feasibility, costs, etc., so it is reasonable to regard condition-based maintenance as a maintenance method of transformers. in order to verify the rationality and correctness of the methods mentioned in this paper, we also use the ahp to choose the maintenance methods for transformers. the judgment matrix of ahp can be established by table ⅱ and the qualitative description matrix of fuzzy judgment, and the corresponding eigenvectors for the maximum characteristic roots in a judgment matrix is used to calculate the weight of criteria. the weights obtained from the ahp are shown in table ix, and the evaluation results from ahp are shown in table x . from the last line in table x, it can be known that the best maintenance method for transformers obtained through the ahp is condition-based maintenance. the result is consistent with the calculated result by using the methods mentioned in the paper, and the differences of the global scoring value of alternative maintenance methods are also little, but the selection of maintenance methods for fhp-gp-sms transformers mentioned in the paper can deal with inaccurate qualitative description and fuzzy judgment. for example, in the experts' fuzzy evaluation, the feasibility is one time to two times more important than additional benefits, and in the fuzzy ahp, the uncertain fuzzy judgment can be expressed as (0.5,1,2,2.5), but the ahp cannot solve this kind of problems."
"cell-based health care product (cbhp) is 'health care product that contains or consists of pro-or eukaryotic cells or cell derived biological entities as an essential ingredient' as defined in iso 18362:2016 [cit] . these products were also known as advanced-therapy medicinal products or regenerative medicine products although there are some differences between their definitions. cbhps are revolutionary new medicines that have recently been made possible and are going to form a new industrial field [2e3]. however, current production facilities for cbhps are still dependent on manual techniques performed by skilled workers, with ongoing problems of quality variation, microbial contamination risk, and productivity limitations [4e6] . we believe that a more robust, safer and more efficient manufacturing system will be necessary as we expect this new industrial field to expand significantly in the future. thus, we designed a whole new manufacturing system for cbhps, aiming for a true \"factory\", to replace conventional \"laboratory-like\" manufacturing methods."
"among the modules in the tissue factory, large scale culture modules (m9) adopted a different approach to cleanliness control compared to the other modules that have sealed chambers. large scale culture modules (m9) use sealed culture vessels instead. the sealed-vessel culture system (also referred as closed system) sometimes has a smaller aseptic space than a sealed-chamber culture system (also referred as open system) and has the advantage that the apparatus can be simplified, since airflow control and decontamination of equipment are unnecessary [cit] . like biological isolators, a sealed-vessel culture system is also less susceptible to contamination from the installation environment. however, it should be noted that there is a risk of contamination when injecting cells into a sealedvessel or when retrieving cells from the vessel. in the tissue factory, the connection for transferring cell suspensions was made using a single-use sterile connector (opta® sft-i connector, sartorius stedim). the sealed-vessel culture system has difficulty to cope with some types of culture operations, such as the cell sheet stacking process, but they can be effective general cell culture processes."
"3. the global scoring of alternatives this step includes criteria weight and the pros and cons of alternatives, and prioritizes these alternatives. supposing there are m hierarchies in total and the weight matrix obtained through the judgment matrix is w1, w2,…, wm, the global scoring of alternatives are :"
stacking up cell sheets into a multilayered construct using the gelatin gel by the method previously reported [cit] . this module has a unit to press the gelatin gel against a cell sheet while adjusting the temperature of the vessel.
"five ports that connect to the processing modules (m2em5), the two incubation modules (m7, m8) and the material loading modules (m6) (connectable modules). the interface between the transfer module (m1) and a connectable module (m2-m8) was standardized so that any one of them can connect to any port on the transfer module (m1) (fig. 3a) . when a connectable module (m2em8) is brought into proximity to a port, the connection is made automatically. this connection includes a physical connection, power supply lines, control signal lines, a compressed air line, and a carbon dioxide gas line. the connection of hydrogen peroxide vapor was designed to be manually connected. every port on the transfer module (m1) has a door and each connectable module (m2em8) also has a door to transport articles, such as cells, medium, and other materials. the two doors face each other at a distance and the space between the two doors is sealed and decontaminated by hydrogen peroxide. then, both doors are opened to form one enclosed space. the connectable module (m2em8) has a specific area inside the door defined as an article standby position. the transfer robot in the transfer module (m1) moves articles between the article standby positions of the two joined connectable modules (m2em8). the footprint of any articles was defined to be 128 â 86 mm with reference to the sbs standard for cell culture plates (ansi slas . articles with a height of ca. 15 cm and a weight of 1 kg or less can be conveyed. laser sensors are used at each article standby position to confirm the presence or absence of any articles."
"as for criteria in a hierarchy, decision makers should perform a series of pairwise comparisons for all the sub-criteria and the pros and cons of alternatives under the hierarchy to establish the comparison and judgment matrix. the relative importance of decision-making factors (criteria weight and the pros and cons of alternatives) is obtained through the comparative judgment matrix."
"it is one of important decision-makings in an enterprise to choose the best maintenance methods for different devices. at present there have been a lot of the research achievements in this field. azadivar and shu [cit] have proposed a selection of the best maintenance methods for devices under the environment of just-in-time system. in the thesis, 16 attributes that influence the selection of maintenance methods are taken into account and the maintenance methods for devices are selected according to these attributes. luce [cit], okumura and okino [cit] have put forward a selection of the best maintenance methods according to different production losses and different costs of various maintenance methods. bevilacqua and barglia [cit] have adopted the analytic hierarchy process (ahp) to solve the problem of choosing maintenance methods in an italian refinery. in the paper, relatively comprehensive evaluation attributes are presented in detail. however, the aph cannot entirely and appropriately deal with the fuzzy multi-attribute decision-making problems that are hard to quantify in the selection of maintenance methods."
"6. according to the methods of obtaining weight mentioned in this paper, the weight of all the criteria to alternative maintenance methods is obtained. 7. prioritize these alternatives. to sum up, the transformer maintenance strategies selection can be showed by fig 1."
"a clean environment is critical to cbhp production, so we considered the application of biological isolators [cit] . unlike biological safety cabinets, biological isolators provide a microbiologically sealed space, so the risk of infectious microorganisms being brought in from the outside is extremely low. in addition, biological isolators are often equipped with a decontamination device. once an isolator chamber is decontaminated with evaporated disinfectants, the inner space of the chamber remains clean until the chamber is opened for maintenance or some other purpose. a drawback of biological isolators is the low manual operability owing to the need for thick gloves by operators to separate chambers. we thought that by combining automatic apparatuses and biological isolators it would compensate for any limitations that each had on its own and create a cost-effective manufacturing facility. to enable attachment and detachment of each manufacturing apparatus, they were designed to be covered with a separate isolator. consequently, each manufacturing apparatus becomes a highly independent 'module', which can be attached and detached into a cluster. for this reason, we gave it the name 'flexible modular platform (fmp)' (fig. 1) . in this paper, we developed a fmp-based cbhp manufacturing facility that was named tissue factory, and performed the manufacturing of multi-layered myoblast sheets to demonstrate its cell-and tissuemanufacturability. in addition, we conducted the passaging culture of human articular chondrocytes and human induced pluripotent stem cells to confirm the feasibility of hydrogen peroxide decontamination and the flexibility of the system for multi-product manufacturing."
"normal human articular chondrocytes (nhacs) were purchased from lonza japan ltd. (tokyo, japan; cat. no. cc-2561), one or two passages in cgm-2 medium (lonza, cc-3216), and then cryopreserved. then, expansion cultures were performed by the tissue factory."
"the impact of flexibility on manufacturing and development cost has been a major focus in the automotive industry. in order to respond to the demands of car customers for unique products and respond to rapid changes in regulations or required specifications, it is necessary to design flexible processes such as producing multiple types of vehicles on one manufacturing line. koste and malhotra have listed five important flexibilities in the automotive industry: machine flexibility, labor flexibility, mix flexibility, new product flexibility, and modification flexibility [cit] . additionally, gupta and somers have proposed volume flexibility as another important flexibility [cit] . applying these concepts from the automotive industry, we examined the importance of these flexibilities in the chbp industry. regarding machine flexibility, most current manufacturing processes for chbps have been used with commercially available biological safety cabinets and incubators, and their flexibility is sufficient for manual operations. in other words, almost all types of cells can be cultured with standard safety cabinets and incubators. even when using automatic culture apparatuses, basic cell culture procedures such as cell seeding and expansion culture are relatively common for a wide variety of cell types. machine flexibility is already well advanced in chbps."
"as a source of skeletal muscle myoblasts, a piece of muscle tissue was dissected from the thigh of a miniature pig (nippon institute for biological science, 9 months). then, isolation, expansion, cell sheet preparation, and cell sheet stacking were performed in the tissue factory. this experiment was approved by the institutional animal care and use committee of tokyo women's medical university and was carried out in strict accordance with the recommendations in the guide for the care and use of laboratory animals of tokyo women's medical university. in optimizing and validating each process, we also used human skeletal muscle myoblasts purchased from lonza (basel, switzerland)."
"tissue factory was designed for manufacturing multilayered skeletal muscle myoblast sheets. a total of nine modules and a material preparation isolator were assembled (table 1 and fig. 2 ). although a system could potentially have multiple transfer modules, we made a hexagonal transfer module (m1) to serve as a hub of the manufacturing cluster. modules can be connected to five of the six sides of the central hexagon. an industrial robot designed for silicon wafers with minimal dust emission was adopted to transfer articles between modules. processing modules (m2em5) commonly have connection ports to the transfer module (m1). incubation modules (m7em9) perform long-term processes such as cell culture. the essential difference from a processing module is the standalone operability that can run parallel to the cell culture processes. in this study, three types of incubation modules were assembled. in order to introduce materials into the system, a material preparation isolator (m10) and a material loading module (m6) were produced. the material loading module (m6) has two interfaces. it can be connected both to the transfer module and to the material preparation isolator (m10). the material preparation isolator (m10) is a commercially available isolator for manual processing customized to be equipped with a port for connecting to the material loading module (m6). to introduce materials into the system the materials are first prepared in the material preparation isolator (m10), placed on a rack in the material loading module (m6), and then, the entire material loading module (m6) is moved to an empty port on the transfer module (m1)."
"lastly, we performed passaging culture of human inducedpluripotent stem cells (hipscs) to elucidate any difference in vulnerability for mechanical and/or chemical damage among cell types. the originally distributed cells were subcultured by hand in several passages and introduced to the system in a cell suspension. in the system, the cells were seeded on a dish that had been coated with imatrix-511 recombinant laminin-511 e8 fragments (reprocell, japan). after 4-or 5-day culture, the cells were harvested by accutase (innovative cell technologies) by shaking (without scraping), centrifuged, and re-seeded onto new imatrix-511-coated dishes. in a preceding optimization of culture conditions, we had found that hipscs were damaged even 24 h after decontamination, and we also found that heating the robotic incubation module (m7) at 50 c for 48 h after decontamination archived sufficient hydrogen peroxide removal to successfully culture hipscs. this fact suggests that hipscs were more sensitive to any residual hydrogen peroxide than chondrocytes. after three passages, hipscs cultured in the system showed comparable growth rates and characteristics to the cells manually passaged by a skilled person (fig. 7) . during the culture in the system, observation in the observation incubator module (m8) was carried out as designed. neither falling nor adhering microorganisms were detected during the process."
"fundamentally, the authors agreed that automation of key processes is essential in order to achieve mass production and ensure stable quality [7e9] . generally, there are two types of automatic culture apparatus. one is a versatile type that carries out various processes with one device, and the second is a single function type that carries out only one sub process. the former is usually designed to mimic manual work by industrial robots, and it is not possible to exceed manual productivity. in contrast, the second is very efficient for a specific process although it requires manual work to bridge each process. therefore, we decided to develop an 'open platform' that connects multiple single function apparatuses together and mediates the exchange of materials and information between each apparatus. the characteristics of cbhp manufacturing dictate that the time required for culturing cells is much longer than the time required for cell manipulation. this meant that the design of a cbhp production system should not be a production line method, but rather a cluster-type production method, where articles can be transferred between arbitrary modules via transfer robots. in a production line method, the entire manufacturing facility is engaged during the cell culture period, but in a cluster-type production method, the other apparatuses are separate and can be used during the cell culture period. this dramatically increases the operating efficiency of the manufacturing apparatuses. moreover, to maximize the efficiency of this cluster production method, it is preferable that each production apparatus has the capability to be easily attached or detached based on the specific production needs."
"the fuzzy reciprocal judgment matrix of ã can be obtained through the pairwise comparisons between criteria (sub-criteria or alternatives) in the same hierarchy, and expressed as:"
"decontamination refers to a process that reduces the number of viable microorganisms to an acceptable level by a validated method. the system was designed to be decontaminated by hydrogen peroxide vapor. the effectiveness was verified by placing biological indicators (apex laboratories, hmv e 091) containing 1 million indicator bacteria (geobacillus stearothermophilus) in various places and confirmed that after decontamination there were no bacteria detected (10 à6 reduction). aeration time for removing residual hydrogen peroxide was based on reducing the concentration of hydrogen peroxide to less than 1 ppm."
"when designing the processes to apply in the tissue factory, the existing manual procedures were not necessarily suitable for automation. the tissue factory successfully reproduced three tentative manufacturing processes of cbhps that were minimally modified from established manual methods. although adapting a procedure to automation is advantageous for improving productivity, it is critical to ascertain whether there is any change in product quality due to the process change, after the quality assurance of the manual procedure is already established. in the concept of fmp, it is not necessary to automate everything. manual work via gloves is also acceptable. moreover, a simpler design by integration of a transfer module and processing modules is also possible."
"equipment maintenance cost is the main cost of production in currently manufacturing firms. for these firms, maintenance cost can reach 15 -70% of production costs, varying according to the type of industry [cit] . on the other hand, the money spent on maintenance due to improper maintenance and excess maintenance is about one-third of total maintenance costs, which cause enormously waist. so select the optimal maintenance strategy could reduce overall running costs significantly and increase enterprise productivity。 maintenance strategies can be divided into fault maintenance, scheduled maintenance, condition based maintenance, and reliability based maintenance, total productive maintenance, predictive maintenance and other repairs. fault maintenance was the earliest applied maintenance strategies, which means the maintenance didn't begin until the equipment failure showed. this strategy is apparently unable to ensure the reliability and security of the system. the remaining several maintenance strategies can be summarized to preventive maintenance. this means that maintenance strategies can be divided into fault maintenance and preventive maintenance of two categories. preventive maintenance is made depending on the measured data from a set of sensors system which including vibration monitoring, lubricating analysis, and ultrasonic testing etc. in order to ensure maintain the system or equipment in good condition, preventive maintenance should be the better choices."
"aerobic bacteria, yeast, and fungi were examined by a method in accordance with the japanese pharmacopoeia. for falling microorganisms, agar medium dishes (bd bbl ™ gamma ray irradiated triple packaging scdlp) were placed with the lids opened for 4 h or more. then, these dishes were collected and cultured for five days or more at 30 c, and the absence of any colonies was confirmed. for adhering microorganisms, agar medium dishes were used (bd bbl ™ ic e xt rocking grid gamma ray irradiation triple packaging scdlp), and after bringing them into contact with the five fingers of the gloves, the dishes were cultured for five days or more at 30 c, when the absence of any colonies was confirmed."
"secondary, we performed expansion culture of human chondrocytes with full decontamination process for comparison with manual operations and evaluation of any remaining hydrogen peroxide. three strains of human articular chondrocytes were subcultured. after decontamination, the cryopreserved chondrocytes were thawed and re-suspended in cgm-2 manually, and then the cell suspension was introduced to the system. the cells were seeded in a dish in the cell processing module (m2) and cultured in the robotic incubation module (m7). for passaging, the dish was returned to the cell processing module (m2) again. the cells were treated by tryple select enzyme (life technologies) on a warm plate at 37 c and collected by shaking the dish. the collected cell suspension was concentrated by a centrifuge unit and seeded in new culture dishes. in the first preliminary trials, we found that the chondrocytes in the robotic incubation module (m7) were damaged, probably by residual hydrogen peroxide. hence, the procedure was changed to start culturing at 24 h after decontamination after which time the concentration of hydrogen peroxide in the robotic incubation module decreased to 0.1 ppm or less. three passages were performed for each strain, both by hand and by the system, and the number of cells was counted at the time of each passage and again at the end of the culture. there was no apparent difference in growth rates between passage by hand and the system (fig. 6) . during the culture in the system, observation in the observation incubator module (m8) was carried out as designed. neither falling nor adhering microorganisms were detected in any of the runs."
"the random consistency (cr) criteria in the judgment matrix can be used to tell whether the weight obtained through the ahp is reasonable. the cr value of all the sub-criteria is given in the rightmost column of table 10 . it can be seen from table 10 that the consistency of environmental safety, workers' acceptability, spare parts' inventory and personnel training cost is best, while the consistency of mean time between failures, mean time between repairs and technical feasibility in the judgment matrix is worst. this conclusion is consistent with the mf in table 8. v. summary firstly, the paper reviews the research status of the current selection of maintenance methods and proposes using the fuzzy ahp based on the goal programming to solve the problems. secondly, in order to overcome the disadvantages in traditional fuzzy ahp, a new method for determining the accurate weight based on trapezoidal fuzzy numbers is proposed in the paper, and the accurate weight in a fuzzy comparison and judgment matrix can be given by using this method, so as to avoid the problems caused by sequencing the fuzzy weight in the conventional approach. finally, taking the selection of maintenance methods for the main transformer in jiangzhuang coal mine, zaozhuang mining group as an example, the paper expounds the application process of the mentioned method, compares it with the standard ahp, and explains that the results of the selection of maintenance methods obtained through the fuzzy ahp based on the goal programming are reasonable and feasible."
"the material preparation isolator (m10) has another port for inputting materials to the material loading module (m6) (fig. 3b ). this port has a simpler structure than those in the transfer module (m1), so they need to be connected manually. it includes a physical connection and a non-contact power supply. the doors on both sides are opened manually using the gloves in the material preparation isolator (m10)."
"the concept for a fmp makes it possible to build a given manufacturing process by combining several modules. here, the term 'module' refers to the smallest independent apparatus that can be connected to and detached from the manufacturing system. after defining the fmp concept, a demonstrative manufacturing system, named 'tissue factory', was built."
"the pairwise comparisons between alternative maintenance methods at the bottom layer and sub-criteria at the upper layer are performed, for example, the relative importance of the pairwise comparisons between the sub-criteria of mean time to repair and the alternative maintenance methods of post-failure maintenance and condition based maintenance is obtained. further the corresponding judgment matrix can be obtained, as shown in table vii . (1,1,1,1) (1/4,1/3.5,1/2.5,1 /2)"
"5. perform the pairwise comparisons between alternative maintenance methods and sub-criteria in upper tier, and according to the decision makers' qualitative description, change the pairwise comparisons into the comparison and judgment matrix based on trapezoidal fuzzy numbers."
"decontamination of modules is achieved by hydrogen peroxide vapor, generated by a hydrogen peroxide decontamination apparatus (hydec, shibuya kogyo) mounted on the material preparation isolator (m10), and supplied to each module by dedicated piping. generally, the intensity of decontamination depends on the temperature, amount of hydrogen peroxide injected, holding time of hydrogen peroxide vapor, and other conditions. accordingly, the decontamination parameters differed depending on the combinations of target modules (see supplementary material 1). seven decontamination patterns were developed based on the criteria described in the method section. decontamination of the whole system was broken down into three successive steps because the hydrogen peroxide vaporizing device used in the system was not capable of decontaminating all modules at once."
"mix flexibility and new product flexibility requires special consideration for chbps concerning cross-contamination and potential mix-ups between different batches. generally, it is not appropriate to handle two or more batches in the same place at the same time. however, suitable changeover and mix-up prevention procedures enable multiple parallel processes including different chbps. in this respect, fmp provides a basal technology to realize multi-product manufacturing for chbps."
"modification flexibility is not a current problem, but a future problem. chbp specifications are strictly defined in the approval process, and specification options are rarely proposed. however, some quantity options, such as cell numbers or product sizes, may be allowed in the future."
"human induced-pluripotent stem cells prepared from human fibroblasts (clone 201b7) [cit] were provided by riken (tsukuba, japan), and expanded by hand through several passages by a feeder-free method derived from the method previously described [cit] . then, the cells were introduced to the tissue factory. characterization of passaged cells were performed by flow cytometry for undifferentiation markers: ssea-4 (stage-specific embryonic antigen-4) [cit] and rbc2lcn, a recombinant lectin probe [cit] ."
"in this study, we built the tissue factory, which is a new cbhp production system using biological isolator technology and factory automation technology. tissue factory succeeded in manufacturing multilayered skeletal myoblast sheets, expanding human articular chondrocytes and passaging un-differentiated hipscs. the major challenge for tissue factory was to reduce the manufacturing cost of cbhps by eliminating the necessity for clean rooms with strict specifications. when a cbhp is manufactured using biological safety cabinets, a clean room with high standards is typically required. biological safety cabinets can reduce the risk of extrinsic contamination by airflow control, but floating particles in the room can be brought in with materials or carried in on the hands of workers. therefore, the cleanliness of the installation environment may affect the cleanliness inside the cabinet. on the other hand, with biological isolators, materials to be carried in can be cleaned by a decontamination pass box. also, workers operate through gloves fixed on the front wall of the chamber. therefore, biological isolators are less susceptible to contamination from the installation environment than biological safety cabinets. in iso 18362:2016, as an example of layout a manufacturing environment, the cleanliness of cell operation rooms where biological safety cabinets are installed is categorized as class 7, and for biological isolators, it is categorized as class 8. this difference in cleanliness levels may reduce the costs of construction and maintenance of manufacturing facilities."
"when determining the relative importance between any two attributes, decision makers sometimes are hard to give a definite qualitative description, but they may give a fuzzy and uncertain one, for example, \"one attribute is about twice more important than the other\", \"one attribute is about twice to four times more important than the other\", etc. it's difficult to use standard weight deciding method in ahp under the circumstances, so some researchers have proposed using fuzzy numbers or fuzzy sets to represent the results of paired comparisons between criteria and to establish the fuzzy comparison and judgment matrix. when the two elements of e i and e j in the same hierarchy are compared, the comparative result in fuzzy judgment can be represented by using the fuzzy number [cit] of ã ij . the representation methods and the ways to obtain weight value of the triangular fuzzy number, gaussian fuzzy number and criteria fuzzy number are studied in the literature [cit] . inspired by these ideas, the paper applies the trapezoidal fuzzy number to express the uncertain results of paired comparisons."
"this paper mainly considers four alternative maintenance methods, that is, post-failure maintenance, scheduled maintenance, condition-based maintenance and predictive maintenance. when choosing suitable maintenance methods for large transformers, the transformer servicemen and related decision makers first must be sure the relevant criteria sets(that is, decision-making attribute sets) that need to be considered in the selection of maintenance methods. the decision-making criteria that usually need to be taken into account in the selection of maintenance methods for transformers are shown in table i [cit] ."
denote byφ map;w (k) the map phase estimate over a sliding window of w symbols. this is fed as a noisy measurement of the true time varying phase φ c (k) to an ekf constructed as follows:
"extensions in a locked-down settings environment, while others will be content simply using a state-of-the-art browser. though protecting privacy is enshrined in the american library association' s code of ethics as a fundamental tenet of the librarian profession, librarians can strive to inform our patrons about online security far more than we currently do. if we install firefox or chrome with every available security extension on our public computers, it will still be for naught when our users go into a coffee shop with a public wi-fi connection, open up internet explorer on their laptop, and log in to facebook using unencrypted http. publicizing security services that your library provides as well as universally available ones can help make our users safer no matter where they are."
"the chrome version of https-everywhere, this extension forces traffic onto https sites if available. it is less refined than the firefox add-on, sometimes redirecting the browser to broken or empty https sites, and also slightly more vulnerable to packet sniffing due to a weakness in chrome' s apis."
"librarians must encourage users to develop approaches to internet privacy that best suit their particular modes of browsing. all of the options listed in this article come with their own benefits and drawbacks; there is no panacea. if someone is uncomfortable moving beyond their outdated version of internet explorer, then pushing a new user interface with complicated privacy options on them solves little, but librarians can at least be informed about the risks present. some users may prefer the ultimate defense of a dozen"
"once a library has deliberately chosen a browser, an appropriate configuration can provide a base layer of protection. selecting a targeted set of user preferences helps users to avoid exposing their data, either to others using the same computer or malicious users who have gained access to their files. the goal is to make the browser as amnesiac as possible while still maintaining usability. browsing history, download history, form autofill information, and most especially passwords should all be erased each time the browser is exited. figure 1 shows an example of effective settings in firefox 4' s security menu. both chrome and firefox offer private browsing settings that enforce a certain level of privacy: for the most part, closing and reopening a private browsing window will clear all saved information such as logins and history. this is called \"incognito mode\" in chrome and \"private browsing mode\" in firefox."
"while discussions around firesheep typically note that public wi-fi hot spots occur in coffee shops or airports, many libraries also provide wireless networks. [cit], 85.7 percent of public libraries offered wireless internet access, and another 5.9 percent planned to make wireless connections available within a year. 2 as such, it is the responsibility of librarians to educate users of the risks and insulate them against potential attackers. while not all librarians are able to secure their institution' s networks or even choose the software"
where h(·) is a non linear measurement function. the particular form is chosen to resolve the issue of unwrapping the phase periodically as it grows linearly: the factor of 4 inside the sine and cosine arguments chosen to obtain a period of 90
"web of trust relies on crowdsourced feedback on sites, evaluating them on factors such as trustworthiness, vendor reliability, privacy, and child safety. a small circle appears next to links in major search engines and web interfaces, colored green for safe sites and orange-to-red for more questionable ones. the breadth of web of trust ensures ratings for almost all major sites."
"besides employing appropriate settings, there are browser extensions or add-ons that can augment functionality beyond what is typically available. extensions can range from the somewhat trivial-changing the color or \"theme\" of the browser-to completely transformational, very nearly converting one browser into another. see table 1 while extensions in the official directories listed above are vetted to some degree, there have been instances of malicious code exposing users to further harm rather than protecting them. it is always worth researching an extension before using it in the field, especially when the stakes are as high as they can be in matters of privacy. reading reviews, including the brief ones in mozilla' s add-ons or the chrome web store, can alert you to potential issues that other users have encountered. noting the number of downloads and currency of updates is an indicator of how likely the extension is to be maintained into the future. lastly, testing the extension in a safe environment can help verify that it indeed does what it asserts to do. for instance, one can install https-everywhere on a test workstation, then visit several high-profile websites where a user would want their traffic to be encrypted, e.g., amazon, gmail, google search, facebook, and yahoo! mail. does the url begin with \"https\"? if not, then the extension is not living up to its claims."
"ural to ask whether dsp-centric architectures with samples quantized at significantly less precision (e.g., 1-4 bits) can be effective. shannon-theoretic analysis (for idealized channel models) has shown that the loss in channel capacity due to limited adc precision is relatively small even at moderately high signal-to-noise ratios (snrs) [cit] . this motivates a systematic investigation of dsp algorithms for estimating and compensating for channel non-idealities (e.g., asynchronism, dispersion) using severely quantized inputs. in particular, we consider in this paper a canonical problem of blind carrier phase/frequency synchronization based on coarse phase-only quantization (implementable using digitally controlled linear analog preprocessing of i and q samples, followed by onebit adcs), and develop and evaluate the performance of a bayesian approach based on joint modeling of the unknown data, frequency and phase, and the known quantization nonlinearity. receiver architecture: we consider differentially encoded qpsk over an awgn channel. in order to develop fundamental insight into carrier synchronization, we do not model timing asynchronism or channel dispersion. in the model depicted in fig. 1, the analog preprocessing front-end performs downconversion, ideal symbol rate sampling, and applies a digitally controlled derotation phase on the complex-valued symbol rate samples before passing it through the adc block. the adc block quantizes the phase of the samples into a small number of bins. phase quantization (which suffices for hard decisions with psk constellations) has the advantage of not requiring automatic gain control (agc), since it can be implemented by passing linear combinations of the in-phase"
"the much-publicized \"firesheep\" add-on for mozilla firefox highlights the need for education surrounding privacy on the internet. 1 while traffic sent over http on a public wireless network has always been vulnerable, firesheep makes stealing others' log-in credentials a trivial procedure. simply open firefox, click a button to start capturing log-in credentials from others on the same public network, and soon you can post to their social media accounts, such as facebook. while firesheep runs only on firefox, it can exploit unprotected users in any other browser. the add-on is not in mozilla' s official directory of enhancements for firefox and it was created as a proof of concept, meant to highlight vulnerabilities that already exist and not to create further ones. however, there also is nothing to stop malicious users from employing firesheep to their own ends."
"once an accurate enough phase estimate is obtained in the acquisition step, we wish to begin demodulating the data, while maintaining estimates of the phase and frequency. in the next section, we describe an algorithm for decision directed (dd) tracking. in this dd mode, the phase derotation values θ k aim to correct for the channel phase to enable accurate demodulation, in contrast to the acquisition phase, where the derotation is designed to aid in phase estimation."
"the expression for the unquantized phase is given by eq. where dependence on α has being expressed through a. integral (9) can be computed by observing that f (a) (dropping subscript u) is the derivative of another integral g(a) defined below, which in turn can be easily evaluated by completing squares in the exponent and expressing in terms of the standard q function."
"an emulation of the firefox noscript extension, this add-on attempts to overcome weaknesses in chrome' s extensions apis via html5 storage caching. however, it is not quite as polished as noscript with more frequent bugs and a requisite password that hinders its usability."
"it is worth noting that google' s branded version of the chromium open-source project automatically reports certain information back to the mountain view-based company, including crash reports, mistyped urls, the location where chrome was downloaded, and anything typed into the address bar (or \"omnibar\" in chromium-speak). these tracking activities could be circumvented by using srware iron, a fork of the chromium project that is more or less identical to chrome but with the google reporting stripped out and a few additional privacy measures added in. however, iron does not yet include chromium' s automatic updating mechanism and thus may pose ulterior problems beyond google' s monitoring."
"easily the most powerful security add-on, noscript requires user approval to run any script, making almost all potential attacks opt-in. while a potent tool for power users, if installed on public computers noscript will probably disorient the majority of patrons while blocking harmless scripts on legitimate websites."
"the performance of phase acquisition is evaluated using monte carlo simulations averaging over randomly generated channel phases. fig. 5 plots results for two values of snr: a low value of 5 db and a high value of 15 db. the performance measures are the root mean squared error (rmse), which captures average behavior, and the probability of the phase error being smaller than a threshold, which captures the tail behavior. we make the following observations: (a) increasing the number of bins from 8 to 12 (4 adcs to 6 adcs) provides significant improvement, moving the curves significantly towards the unquantized limits. (b) the greedy entropy policy with 6 adcs performs close to map estimation with unquantized observations, indicating it cannot be far away from the optimal control policy. (c) at high snr, the naive policy of keeping the derotation values constant performs the worst, as expected. the greedy entropy policy drives the mse to zero more quickly than random dithering. (d) at low snr, there is little to distinguish between the different derotation policies for 6 adcs, since the noise supplies enough dither to give a rich spread of measurements across different bins. however, when the quantization is more severe (4 adcs), the greedy entropy policy provides performance gains over random dithering even at low snr. to summarize, we find that efficient dithering policies could be very effective for rapid phase acquisition under the scenarios of more severe quantization and higher snrs."
"conditioned on the past derotation values θ k 1 (which are known) and the quantized phase observations z k 1, applying bayes rule gives us a recursive equation for updating the posterior of the unknown phase as:"
"finally, newer and more secure internet browsing platforms are being designed with security in mind from the start. while security concerns clearly dictated aspects of chromium' s architecture, additional efforts are underway to make even more secure software. opus palladianum (by researchers at the university of illinois) and microsoft' s gazelle project both promise to bring a higher level of security to the browser, but neither is available for public trial yet."
"the easiest defense is at the level of the internet browser itself. selecting software that has historically proven to be secure, such as mozilla firefox or google chrome, immediately places your users in a safer arena. on the other hand, internet explorer is a notoriously insecure piece of software, and apple' s safari browser has proven to be similarly vulnerable. [cit] \"pwn2own\" event at the cansecwest computer security conference, while no participants even attempted to hack firefox and chrome. 3 it should not have to be mentioned, but internet explorer 6 still holds a modicum of marketshare in the united states and is regarded by some as the most insecure piece of software ever developed. 4 part of the reason for this is the activex controls that interactive websites can use in internet explorer. these controls run with the same level privileges as the user running the program, which gives malware opportunities to interact with many elements of the windows operating system, including downloading files and executing programs. if your library or any of your users are still using internet explorer 6, migration to a newer version is absolutely essential. it speaks volumes that microsoft itself has an internet explorer 6 countdown site that advises \"friends don't let friends use internet explorer 6,\" yet that same site shows 1.4 percent market share in the united states as of september 30, 2011. 5 on the other hand, google chrome has developed a few innovative approaches to security. 6 first of all, plug-ins such as shockwave flash are often a source of security vulnerabilities, which is further exacerbated by users failing to update to the latest versions when they become available. to address this issue, chrome packages plug-ins along with its own automatic updates that run seamlessly in the background, never providing the user an opportunity to opt-out of valuable security patches. especially for large, public computing labs not running virtualized software, automatic updates save it staff time and reduce user confusion. second, chrome \"sandboxes\" plug-ins so that even when vulnerabilities are exploited, they cannot break into the larger operating system environment. thus problems analogous to malicious ac-tivex controls with too great of privileges are circumvented. finally, chrome also works to inform users of site security, from highlighting the \"https\" prefix in green font beside a safety lock to warning when users are about to visit a malicious website."
"when eric phetteplace asked me if browser privacy would be an appropriate topic for the accidental technologist column, i asked how soon he could write it. [cit] and moved to start a new job, he presented me with the column this fall. in \"hardening the browser,\" eric poses some questions for consideration about how involved libraries should be in training our patrons on internet privacy. he also provides a lot of practical how-to information that will be useful for your library and for your personal web browsing.-editor a rticle 3 of the current code of ethics of the american library association states that \"[ala members] protect each library user' s right to privacy and confidentiality with respect to information sought or received and resources consulted, borrowed, acquired or transmitted.\" this noble maxim has led many librarians to be advocates for the right to privacy, even to the point of resisting federal legislation such as the usa patriot act. however, merely protecting a patron' s circulation records has become but a small hillock of the privacy terrain in our modern information environment. as more and more time is spent accessing and producing content online, libraries need to position themselves to offer internet privacy to patrons as well."
"since extensions can significantly alter the browsing experience, it is worth contemplating how users will react and even performing usability tests before employing too many on public workstations. updates also can break extensions and disorient the user upon opening the browser, as in figure 2 . some of the best extensions will be nearly seamless, such as adblock, where it is often not evident that content is being hidden, but others will be extremely disruptive, as when noscript breaks the shopping cart application of an online vendor. even web of trust may confuse patrons who do not know what the green circles signify. then there are also extensions which work, but only to a limited degree. blacksheep (https://www.zscaler.com/blacksheep.html), for instance, is meant to detect whether anyone on the same connection is using firesheep. however, mere detection is not preventive and thus there are far more appealing alternatives."
"the success of a bayesian approach for the simplified model considered here motivates future research on a comprehensive framework for receiver design subject to severe quantization constraints, addressing timing synchronization and dispersion compensation as well as carrier synchronization, and extending to larger amplitude/phase constellations. it is also of interest to develop a deeper theoretical understanding of fundamental performance limits under quantization constraints."
"no amount of secure software design and customization can entirely eliminate the threats that exist online. phishing forms can cause users to turn over valuable information regardless of the security of their browsing platform. hidden httpsonly options are very much left to the user' s discretion; a user signed into facebook will be on an https site only if they have selected the option for themselves. the best extensions can be circumvented, whether by innovative attackers or users who do not understand their options. even with the stellar noscript and https-everywhere add-ons, a user can opt-in to malicious scripts and manually override https protection for particular domains. thus educating users and library staff is the best way to enhance online privacy. teaching workshops on the subject, or marketing particular solutions to known problems, is inherently valuable because it spreads awareness of an increasingly important issue."
"the (unrelated) adblock extension is also very good at removing ads and preventing pop-ups. both extensions are among the most popular ones for chrome, with millions of installations."
"the carrier frequency offset ∆f is typically of the order of 10-100 ppm of the carrier frequency. for example, for a 60 ghz link, the offset could be as large as 6 mhz, but is still orders of magnitude smaller than the symbol rate, which is of the order of gsymbols/sec. thus, it can be set to zero without loss of generality in the acquisition step (described in section iii), where we derive estimates of the unknown phase φ c based on a small block of symbols. we do model the frequency offset in the tracking step (section iv)."
"arguably the most important item in this table, https-everywhere detects what domain a user is on and, if possible, sends their traffic over encrypted https rather than insecure http. the add-on works seamlessly and is perhaps the best response to firesheep-style attacks available for any platform. a download link is present on the electronic frontier foundation' s website (www.eff.org)."
"hardening the browser settings on their public computers, simply spreading awareness of online privacy problems and potential solutions is a major step forward in an increasingly important area of information literacy. this column will detail three layers where protections can be implemented: in the choice of an internet browser, the user settings within the software, and, finally, add-ons that extend the browser' s functionality, providing additional security beyond the default architecture."
"and quadrature components through one-bit adcs (quantization into 2n phase bins requires n such linear combinations) [cit] . the quantized phase observations are processed in dsp by the estimation and control block: this runs algorithms for nonlinear phase and frequency estimation, computes feedback for the analog preprocessor (to aid in estimation and demodulation), and outputs demodulated symbols. design of this estimation and control block is the subject of this paper. we break the synchronization problem into two steps (a) rapid blind acquisition of initial frequency/phase estimates, (b) continuous tracking while performing data demodulation. contributions: for the acquisition step, we develop a bayesian algorithm for blind phase estimation, which includes design of the feedback to the analog preprocessor to aid in estimation. the feedback evolves with the posterior distribution of the phase, and we show that an informationtheoretically motivated greedy strategy is useful in improving performance at high snr. since frequency offsets between transmitter and receiver are typically much smaller than the symbol rate, the phase is well approximated as constant over multiple symbols, hence acquisition is performed ignoring frequency offset. for the tracking step, we use a two-tier algorithm: decision-directed phase estimation over blocks, ignoring frequency offsets, and an extended kalman filter (ekf) for long-term frequency/phase tracking. the feedback to the analog preprocessor now aims to compensate for the phase offset, in order to optimize the performance of coherent demodulation. we provide numerical results demonstrating the efficacy of our approach for both steps, and show that the bit error rate with 8-12 phase bins (implementable using linear i/q processing and 4-6 one bit adcs) is close to that of a coherent system, and is significantly better than that of standard differential demodulation (which does not require phase/frequency tracking) with unquantized observations."
"on public computers, appropriate browser settings are vital. someone should never be able to open a browser only to return to the previous users' session in a password-protected service or view their navigation history. while it is simple to choose a good configuration in a browser, maintaining those settings over time may be difficult as users have access to the accidental technologist software' s menus. however, firefox allows administrators to create a permanent set of preferences to which the browser returns each time it restarts. by locking down settings, users can alter preferences during their browsing session, but their changes will be lost when the browser is closed. chrome has an extension that password-protects the preferences menu."
"is solution vector i, with n corresponding to the number of objectives. as we can see from the equations defined above, the solution with higher fitness value has more chance to be selected."
"in this section, the principles of multi-objective pso (mopso) is firstly introduced. then, a problem related encoding strategy is presented. at last, local searches are detailed."
"6 t1 always appeared at the end of the pretarget sequence, and t2 was presented on 50 % of the trials at each of the posttarget positions. the present attentional blink experiment used black letters as distractors, a white letter as t1, and a black capital x as t2."
"this work is supported by the anr (french national research agency) in the framework of the project tcdu (collaborative transportation in urban distribution). this project anr-14-ce22-0017 is labelled by the pôle véhicule du futur, and is jointly performed by four partners, the three french universities of technology (utt, utbm, utc) and the society share and move solutions."
"cross-platform and cross-browser compatible. to run an experiment in tatool or webexp, participants need to have java installed on their computer, whereas scriptingrt is based on adobe flash. importantly, in order to create rt experiments, these solutions typically still require substantial programming skill, and the researcher needs to host a web server to publish the experiment. furthermore, since these methods require specialized software or plugins, they do not meet the amt constraint that the participant may not be required to install additional software."
"the letter sequences consisted of 7-14 pretarget letters and seven posttarget letters, all of which were uniquely selected from the whole alphabet and randomly ordered."
"the stroop task is a classic paradigm that requires participants to identify the word color of congruent and incongruent color words. when word and word color are incongruent (e.g., the word \"red\" in green), rts are slower and people make more errors than when the stimuli are congruent [cit] ."
"method the experimental survey was run on two systems. system 1 was a bto laptop running windows 7 ultimate on a 2.5-ghz intel i5 quad core processor with 8 gb of ram. the experiment was conducted running chrome 27 on a targa crt monitor running at 60 hz. system 2 was a macbook pro running osx 10.5.8 on a 2.4-ghz intel core 2 duo processor with 4 gb of ram. the experiment was conducted running firefox 16.0.2 with the same targa crt monitor running at 60 hz. [cit], both systems used prime95 version 27.9 to manipulate cpu and ram load in a controllable, predictable way [cit] . the four load conditions were (1) low, in which only the browser was running; (2) medium, in which prime95 ran a torture test using 50 % of the cpu; (3) high, in which prime95 ran a torture test using 100 % of the cpu; and (4) maximum, in which the torture test was run using up almost all ram, as well."
"based on the preliminary experiments, fifo is used for rep p best i, and fitness-based on sharing method is employed for rep gbest ."
"since the proposed method is a hybridization of pso and local searches, a validation of each method is necessary. the tests are made on the benchmarks, and the result of instance spdptwpd-r201 is shown in fig.6 . in the figure, pure pso has evolved from the initial population. this can justify that pso itself has the capacity of improving the results. however, the result is still a bit far from the real pareto front. after integrating local searches, the result obtained by the method called hpso is closer to the real pareto front (see in section iii for more information). this can validate the performance of the proposed local searches. this phenomenon can be observed for all the tested instances."
"results and discussion in order to estimate the accuracy of the presentation timing, we compared the actual stimulus duration measured using the photosensitive diode with (1) the duration set by the experimenter and (2) the calculated duration logged in the qrte data file. the mean absolute differences between these measures are displayed in table 1 (for details, see the supplementary material)."
"providing exact statements about the precision of the time stamps and the timing of screen presentation is not possible, because multiple factors can influence this precision. for example, when running many animations in multiple tabs or when a laptop's battery power is low, the performance.now timer resolution is decreased in some browsers in order to save cpu power. furthermore, it may sometimes happen that for unknown reasons, the raf function call skips a frame. in general, very high cpu and especially ram load can be expected to be associated with decreased precision of the timers. to estimate the precision of screen durations with the qrtengine during an experiment, we therefore performed a validation study using external chronometry."
"using the basic qualtrics environment, one can set up sophisticated surveys, publish them, and collect the results. typically, a survey is created by adding questions (which can consist of just text and/or images as well) that are organized in particular blocks. in order to set up an rt experiment in qualtrics, the qrtengine modifies the way that questions in a block are presented. when several questions are set up without a page break, qualtrics would normally present them together on the page. when the qrtengine is included, however, questions are presented one screen after the other, allowing for control over the timing of each screen. furthermore, the qrtengine provides a way (or api) for conveniently adding keyboard responses, setting up conditional and/ or delayed display of screens, and setting more advanced properties. the qrtengine itself consists of a piece of javascript code, called the \"engine,\" that can be easily included in any qualtrics survey. question screens are created using the standard qualtrics visual editor, whereas the properties of the rt task screens can be set by copying and pasting and adapting standard javascript snippets to each question. the qualtrics \"loop & merge\" functionality is used to set the list of trials and their specific properties in a convenient way. to control timing-related aspects, the qrtengine uses a recent html5 method, allowing for more accurate results than provided by comparable older javascript-based methods."
"when the duration for a screen has been set, the qrtengine logs the following time audit information during runtime: duration, which is the intended duration set by the researcher; onsettime and offsettime, which provide time stamps in milliseconds relative to unix epoch; and calculatedduration, which represents the estimated actual duration of screen presentation, based on the difference between the onsettime and offsettime. the calculatedduration can thus be used to estimate how much the actual duration of presentation deviated from the intended duration. figure 1d provides an overview of all of the information that is saved during the example stroop task."
"procedure participants found the task advertised as a hit on amt. they were informed that this hit would require them to respond as accurately as possible and that it required full concentration. after participants had decided to take part, they were linked to our qualtrics survey. this survey first collected some metadata, such as the browser version, operating system, and screen resolution used. then, the survey estimated the speed of the participant's internet connection using the itd estimation described earlier. when the connection speed was too low, the participant was kindly informed that he or she could not participate."
"the qrtengine works by looping a question block according to a predefined list of trial properties that is specified under the \"loop & merge\" functionality. in the case of a stroop task, the loop & merge list specifies the correct response, the color of the word, and the word (see fig. 1b ). each question in the question block is displayed as a screen in the task. one can include as many screens and responses in one trial as desired. it is also possible to set a screen to be displayed in the background through the whole trial-for example, a response key reminder. furthermore, a screen showing accuracy feedback for a specified duration can also be added as an extra question. the timing and response configurations for a question are defined in the javascript code corresponding to that question (see fig. 1c ). after setting these configurations, the survey is finished and can be distributed."
"the trial information, including the individual time audit information, is saved using a json representation within the standard qualtrics wide data format. saving the data in a format that can be directly imported into packages like spss or excel is not possible, due to restrictions in the qualtrics api. therefore, we published the qrteparser, a cross-platform java program that is available on www.qrtengine.com and allows for converting the qualtrics qrtengine comma-separated value (csv) file to a long format (where each row presents the data of a single trial). standard packages such as spss and excel can then be used to read the resulting csv data file and perform data aggregation and statistics."
"procedure for this experiment, largely the same procedure was used as in experiment 1. after the general instructions, participants received task-specific instructions and started the task. each trial started with a fixation cross presented for 1,000 ms, followed by the prime with variable duration, which was followed by the mask presented for 96 ms. next, a blank 48-ms interval was presented. finally, the probe stimulus was presented for 96 ms and then removed immediately. participants were instructed to respond as quickly and accurately as possible by pressing \"s\" when the probe consisted of left arrows, and \"l\" when the probe consisted of right arrows."
"a number of methods have recently been developed to help psychological and cognitive scientists program rt experiments for the internet. currently available solutions include tatool [cit], webexp [cit], and scriptingrt [cit], which have all been developed with the aim of providing precise timing 1 . these libraries can be used to produce rt experiments that are 1 during publication of this article we learned about an alternative javascript method that is worth mentioning: jspsych [cit] . we provide a comparison of qrtengine and jspsych, created in collaboration with the jspsych author, on www.qrtengine.comm electronic supplementary material the online version of this article (doi:10.3758/s13428-014-0530-7) contains supplementary material, which is available to authorized users."
"the trial started with a fixation cross displayed for 1,000 ms, which was followed by a blank screen displayed for 500 ms. then the screen showing the color word was displayed until a response was made. accuracy feedback was given after the response, using the word \"correct\" or \"incorrect\" displayed for 500 ms in a black 30-point font."
"the qrtengine is included into a survey by pasting the javascript code (available via www.qrtengine.com) in the survey's header. then, a number of embedded data fields need to be created that are used by the engine during the runtime of the experiment (see fig. 1a ). finally, the layout needs to be selected; we recommend using the standard qualtrics \"minimal\" layout."
"after participants had given informed consent, they were asked for their amt worker id in order that we could pay the participants later. this was followed by the instruction to maximize the browser window using f11 and reminding the participant that concentration was necessary for successful completion of the task. following these general instructions, the specific instructions for the stroop task were presented along with four examples. after this, participants started the task. when the task was completed, participants were asked for some demographic information before the survey ended."
"although we believe that the qrtengine indeed provides interesting benefits, potential users should be aware of some limitations. first of all, given that the observed accuracy of presentation timing is ± 1 frame deviation in 97 % of trials, the presentation timing capabilities of the qrtengine are certainly not as good as those of software like e-prime [cit] or comparable packages for offline research, since these solutions have considerably more control over the operating system. furthermore, timing is partly dependent on the participant's web browser: for older browsers, the timing is less precise. additionally, when using the time stamps logged during an experiment, it is important to keep in mind that these time audits are subject to measurement error. we expect that the limitations regarding timing will gradually be resolved in fig. 4 mean reaction times (rts) and error rates for each condition in the masked-priming task. note the typical reversal of the compatibility effect for the shorter prime durations the future, since web browsers develop very rapidly with a focus on speed. although our validation studies suggest that the qrtengine is often capable of providing very short presentation times of up to one frame (i.e., 16.7 ms when using a 60-hz refresh rate), inaccuracy will always remain when running online experiments directly in the browser, especially with very short presentation times. considering these validations, it should be noted that we did not perform any validation on mobile devices such as tablets or smartphones. given these considerations, online studies will introduce considerable nonsystematic noise in terms of timing, which might affect the sensitivity to small effects in rts. in extreme cases, measurement error could therefore be speculated to become even twice as high as in typical lab experiments. increasing the sample size is the easiest way to reduce the impact of this problem. as a very conservative rule of thumb, researchers might therefore consider multiplying their sample size, based on conventional power analyses of lab studies, by a factor 4 in order to be equally sensitive to the similar effects observed in the lab."
"here we present an alternative method for conducting online rt experiments that is based on the existing online survey development environment qualtrics. the qualtrics reaction time engine (qrtengine) is different from the previously described methods in a number of ways: it is hosted on the qualtrics server; it does not require specialized software or browser plugins; it is more precise than the previously described javascript method; and because it is embedded in the userfriendly qualtrics interface, most of the basic functionality can be used by researchers who are not experienced programmers."
"attentional blink task the experiment consisted of 80 [cit] . a gray 300-pixel square was displayed in the center of the page, the background was white, and stimuli were presented in the center of the square in a 50-point font."
performing behavioral research online is an interesting approach that is gaining increased popularity. the rapid development of web-browser technology and the emergence of recruiting platforms such as amt have facilitated this development. we think that the qrtengine can help researchers conduct online behavioral research in an accessible and efficient way.
"by using the information provided above, algorithm1 defines the construction of tours. in the algorithm, the insert method adds the pair (supplier, customer) into vehicle of"
"the qrtengine has been developed with the goal of providing timing capabilities as accurate as possible within the limits of modern web browser technology. in this section, we discuss which actions we took to achieve this goal."
"another important timing-related aspect concerns the communication with the qualtrics server. because the implementation of a block of trials depends on the native qualtrics loop & merge functionality, at the end of every trial the data that have been collected are automatically sent to the qualtrics server. the speed of this server communication relies on factors such as the internet connection of the participant and on the load of the qualtrics server. the duration of the server communication time is variable and not under control of the researcher."
"the classic attentional blink effect shows that target identification of the t2 is impaired when it is presented 100-500 ms after t1 [cit] . when t2 is presented directly after t1, there is less impairment, an effect called \"lag 1 sparing.\" when t2 is presented 2-8 places after t1, accuracy typically increases gradually with the distance from t1."
"the pickup and delivery problem (pdp) studies the problem in which all the demands should be transported from pickup points (suppliers) to delivery points (customers) by vehicles. one of the most studied contexts of pdp is the urban area where the capacity of vehicle and the visiting time are limited. in the literature, this problem is called pickup and delivery problem with time windows (pdptw)."
"several applications can be found in the real world. for example, if the resource (vehicles) of a company is limited, as consequence it may not satisfy all the demands. in this case, the decision maker can associate each demand with a profit. maximizing the profit (satisfying the most important demands) is then considered as the first objective. a solution of this problem is a set of visiting sequence of each assigned vehicle. since there may be several solutions corresponding to the first objective, minimizing the distance can be considered as the second objective to choose the most economical solution. in this section, related work about spdptwpd and pso will be presented briefly."
"by reproducing classic behavioral effects in three validation studies, we have provided important empirical validation for the qrtengine. although the stroop and attentional blink effects had been found before in online studies [cit] using javascriptbased methods."
"3 according to the w3c definition, this timer should be accurate to the microsecond, but is at least accurate to the millisecond. the performance.now timer is independent from the system clock, and thus is not subject to system clock adjustments or system clock skew [cit] . in older browsers, the qrtengine falls back to the javascript date.now timer, which is less precise. the qrtengine logs the availability of raf and the type of timer under the \"enginetype\" attribute using the following labels (ordered from most to least precise): \"native-highres,\" when raf and performance.now are both available; \"native,\" when raf is available but only the date.now timer can be used; and \"timer,\" when settimeout is used in combination with the date.now timer. information from the \"enginetype\" attribute can be used to exclude participants whose browser did not support precise raf and/or precise time stamps when an experiment is very sensitive to timing."
"the programming language used is java, and the method is run on an intel core i7-4810mq cpu, 2.80 ghz processor with 16 gb of memory. the value of each parameter is determined after several tests, and is shown in table i"
"the survey that we created for the experiment consisted of a white screen on which a large black square was presented approximately 40 times for durations of 1, 2, 3, 4, 5, 6, 12, 30, and 60 frames (in that order, with each duration condition fully completed before the next condition). the interval between the presentations of two squares was set to 1,500 ms. the total duration of the experiment was about 12 min."
"the vrp is a problem in which a set of vehicles needs to find the best routes to visit all the customers. many contributions have been made regarding this problem or its extensions. to get a better understanding, please refer to the recent survey [cit] ."
"as table 1 shows, the average deviation of both measures is around 6 ms in the low-load conditions. only under conditions of maximum load was a substantial deterioration in performance observed in both systems, leading to an average deviation of around 10 ms. the results also indicate a difference in performance between the two systems. it may have been that this difference was due to the fact that the system 2 hardware and software were quite old [cit] and has lost support by current browsers). however, this difference in performance also illustrates a general caveat in online experimenting: the experimenter simply cannot know all hardware and other factors that will influence performance. our findings show that the mean deviation between the intended duration of a stimulus and the actual deviation of a stimulus was small, and given the 60-hz display rate used here, falls within the range of ± 1 display frame deviation (16.67 ms). a similar accuracy was observed for the calculated duration attribute that is logged in the qrte data file. researchers might use this attribute to get a reasonably reliable estimate of timing errors, which can be used to exclude trials or participants on an individual basis when accurate timing is critical. apart from the mean deviation in milliseconds, we also analyzed the deviation between the photodiode measurement and the intended duration as expressed in the number of frames (16.67-ms units, in our test case). the results of these analyses are reported in table 2 . the results show that across the systems, including the low-, medium-, and high-load conditions, a timing accuracy of the intended duration within the range of ± 1 frame deviation was present in 97 % of the trials. in the supplementary material, we provide tables similar to table 2 for the trials in which only one or two frames were presented."
"the qrtengine provides a number of key features. first, the accuracy of its presentation timing was within ± 1 frame deviation in 97 % of the trials, which we observed by comparing the intended presentation times with a photodiode measurement. further validation for its accurate timing was provided by an analysis of the behavioral results in typical rt paradigms run on amt. a second key feature of the qrtengine is that it works quite simply. in our experience, 60 min is often sufficient for a novice to follow the tutorial in the supplementary material to build a stroop task. when using the method more often, building experiments will likely become very efficient, because elements can be easily exchanged within and between surveys. because the experimental flow is determined by javascript snippets instead of by objects in a graphical user interface, trial procedures can also be implemented in a flexible manner. however, this flexibility also allows for programming the same experiment in different ways, possibly introducing small errors in the experimental design. we therefore recommend that users to share their experiments to facilitate research transparency and replicability [cit] ."
"the qrtengine was developed with researchers in mind who are not experienced programmers but who have used experiment-builder applications such as e-prime or opensesame before (mathôt, [cit] . like most experiment-builder methods, the qrtengine provides increasingly sophisticated functionality for increasingly complex programming. for users without a background in programming, we expect that the example tasks and javascript snippets, provided at www.qrtengine.com, will be sufficient to help them program simple paradigms themselves. for researchers who already have substantial programming skills, we think the qrtengine can provide a method to speed up development: the qualtrics environment provides many tools that make the building process more efficient, and the qrtengine conveniently wraps up the complex calls that are necessary to implement the timing and dynamic properties of an experiment. furthermore, qualtrics provides some handy participant management features. the qrtengine is published open-source under the apache 2.0 license, meaning that experienced programmers are welcome to consult our method when programming solutions from scratch. when doing this, it is important to realize that the qrtengine is not a standalone library and has dependencies on qualtrics and the prototype framework (which is already present in any qualtrics survey). as we will demonstrate in this article, we think that the qrtengine is particularly useful for running plugin-free browser-based rt experiments that present text and pictures, provided that accurate timing and a stable intertrial interval are not critical."
"when setting up a qrtengine survey, the researcher defines an intertrial delay (itd), which is essentially the minimum delay between the end of the current and the beginning of the next trial (the intertrial interval), during which period server communication takes place. please note that preloading (or caching) trials is not possible when using the qualtrics environment. as is displayed in fig. 1d, the itd is the minimum time that will elapse between two consecutive trials set by the researcher. during the itd, the last screen of trial n and the first screen of trial n + 1 are displayed, while in the meantime three processes take place. the first process, initpre, takes care of the server communication during which the collected information is sent to the server and the data regarding the next trial are received from the server. when server communication is complete, the second process, called init, starts, which initializes the next trial. the third process, initpost, fills up the remaining itd that was set by the researcher. if the combined duration of initpre and init exceeds the itd, the duration of the initpost is set to 0. the time stamps of all three processes are logged separately (see also fig. 1d )."
"repository can be seen as an external file used to record non dominated solutions. three related operations need to be defined: update, capacity, selection. if a new candidate is dominated by any solution in the repository, it will not be considered. if it dominates any solution, it is recorded, and all the dominated solutions are removed. if the limited number of solutions (capacity) is reached, a strategy should be proposed to remove several solutions. however, we consider the capacity is unlimited. as to the selection of a leader, two strategies are studied in this paper: first-in-first-out (fifo), fitness-based on sharing method. in fifo, the selection procedure is based on the sequence by which each candidate enters the repository. however, this procedure would be disturbed when updating the repository. in this case, a corresponding strategy is illustrated in fig.2 . iterator points the selected solution, and it moves from left to right. when it reaches the tail, it will restart from the head. the second strategy is based on the idea that solutions in a less crowded area have higher chance to be selected. for each solution i in the population (repository), a probability can be calculated:"
"the growing interest in online behavioral research has led to numerous studies regarding the benefits and validity of such results, mostly focusing on data collected using amt [cit] . although the qrtengine can be used to conduct online research through any channel, it provides the additional benefit of being javascript-based, which complies with amt's regulations that workers should not be asked to download additional software. conducting online behavioral research provides numerous advantages and is starting to emerge as a serious alternative for many researchers. we think that the findings described here show that the qrtengine may provide a valuable tool for conducting such research."
"the results of the first group are shown in table ii . in terms of gap, hpso can perform better in two instances than hg, and is near the real pareto front. as to the number of solutions, hspo can also find more solutions than hg. the solving time of hpso is less than hg. the results of the second group are shown in table iii . in terms of gap, hpso performs better than hg in two instances. in eight instances, the gap is almost 0%. compared to approached pareto front, five instances achieve the gap between 2% and 4.6%. in other instances, a gap around 1% is achieved except for the instance spdptwpd-r1002 in which the gap is 18% compared to hg and 15% compared to approached pareto front. as to the number of solutions and solving time, hpso is always better than hg. the obtained gap for the instance spdptwpd-r1002 is maybe due to the way of comparison which compares only the solutions of same profit. for this instance, the number of common solutions maybe very small, and the solutions obtained by hg are significantly better than hpso. it will then give us the illusion that hg performs better than hpso. however, hpso may perform better than hg for the other solutions non-compared. in terms of number of solutions, hpso always performs better than hg. it maybe due to the fact that hpso stores a copy of the solutions before the apply of local searches, which allows hpso to keep the solution of low profit and high profit at the same time. as to the solving time, hpso is faster than hg thanks to the simplicity of encoding and decoding of pso."
"in this paper, a problem called spdptwpd has been studied. since the optimal solutions of large instances are very hard to obtain, an approached method hybridizing pso with local searches has been proposed. the idea is to take advantage of the two integrating methods: the capacity of global search of pso, and the capacity of intensive search of local search. the experiment has been made on benchmarks from the literature. the results have shown that the proposed method can obtain high quality solutions in short time. another remark from the conducted experiments is that pso needs the help of local search to find better solution. in the future works and to improve the results, more effective local searches will be studied. to deal with much more problems in real life, other objectives or variants may also be considered."
"in the past decade, psychologists have showed increasing interest in conducting research via the internet. through online labor markets such as amazon's mechanical turk (amt), high numbers of participants can be tested in a short amount of time and at low cost. recently, concerns about the quality of data gathered through amt have been addressed, and multiple studies have now shown that data gathered using amt are reliable and comparable to data obtained in the lab [cit] . these validation studies, like most previous research conducted via amt, were survey-based. conducting online experiments that rely on precise recording of reaction times (rts) is much more difficult, although prior work using basic html and javascript has successfully replicated a number of rt tasks online [cit] ."
"in each trial, a fixation cross was presented for 1,000 ms, followed by the stream of letters, each of which was presented for 100 ms. after the stream completed, participants were asked to identify t1 by pressing the corresponding letter on the keyboard. then they were asked to press \"1\" if t2 had been present, or \"0\" if t2 had not. both questions were displayed in a 15-point black font in the center of the gray square."
"a detailed introduction to using the qrtengine is provided in the supplementary material as a step-by-step tutorial on how to create a simple stroop task [cit] . in order to build this task, one only needs a computer with an internet connection and a qualtrics account. in this article, we provide a concise overview of the development process of this same stroop task."
"there are mainly two defaults of the proposed solution representation: 1) when nv increases, a vehicle may satisfy only one demand during its tour in certain cases. as we believe that traveling distance will be increased if vehicles are not charged. 2) unsatisfied demands are not treated. although this is allowed from the selective aspect, the profit achieved by the population will be possibly very weak. two local searches ls1 and ls2 are thus proposed to cover the two defaults. for ls1, it plays two roles: firstly, it tries to minimize the number of vehicles for a given solution. secondly, it calls a method named vnd to minimize the distance for each vehicle. we should notice that the total profit is always the same. for ls2, it tries to insert unsatisfied demands to increase the total profit. the priority of inserting the demand is defined by the distance between the demand and the given solution. the implementations are presented from algorithm2 to algorithm3 (see in appendix section). in the algorithm, the bestinsertion(s, d) method tries to insert a demand d into route s in a way that the distance is minimized. the shortest(s) finds the route in which the distance is minimal. the distance(d, d ) method calculates the distance between two demands by using the equation defined below:"
"before using the qrtengine, a researcher should carefully consider whether performing an online experiment is the right choice in the first place, and whether the qrtengine is the optimal method for the planned study. the pros and cons of online research in general have already been discussed in some excellent previous publications [cit] . when focusing on rt tasks specifically, some additional limitations should be taken into consideration as well. the computer systems that participants use vary widely, and the error involved in measuring rt data and ensuring precise display durations on a given system is largely unknown. in addition, keyboards have different sampling rates (depending on both hardware and the operating system and browser combination), and monitor refresh rates also vary. furthermore, continuously changing operating systems and web browsers both contribute to the uncertainty in precision. when users run multiple applications on their computer, this can also affect timing randomly. essentially, there are many sources of potential timing errors. these errors typically are expected to be random across subjects and conditions, and therefore, multitrial designs and collecting time audit information are necessary so that experimenters can filter out extreme timing errors in order to still obtain reliable results [cit] . hence, we recommend conducting online rt experiments only when it is acceptable to test more participants in order to compensate for the noise introduced when using online acquisition methods."
"second, the itd may occasionally exceed the desired duration between trials. this is because the method is dependent on qualtrics servers and the internet connection of the participant. we therefore recommend not using the qrtengine when it is necessary for the intertrial interval to be exactly consistent or when the intertrial interval needs to be very short. using a test that measures the internet connection quality before the start of an experiment will partly help to reduce the impact of this limitation."
"third, at present the qrtengine allows for presenting text and pictures only, so studies requiring auditory and movie stimuli are not supported. furthermore, researchers should also be aware that designs in which the types of trials cannot be specified in advance in loop & merge lists might be difficult to implement in qualtrics. besides these qrtengine-specific recommendations, we would remind potential users of the caveats associated with online rt experiments in general, which we discussed in the introduction."
"the qrtengine uses an html5 feature called \"requestanimationframe\" (raf) to synchronize the onset of stimuli with the refresh rate of the monitor. when using raf, the qrtengine is notified by the browser (through a highpriority function callback) no more than once every screen refresh that a screen refresh is taking place. hence, every time that raf notifies the qrtengine, it can use a timer to determine whether the elapsed screen presentation time approaches the intended presentation time. in this way, the qrtengine can estimate whether the last screen refresh of an intended screen duration is to be expected, and prepare the next screen accordingly. to correct for potential imprecision due to imperfect resolution of the timer, we recommend subtracting 5 ms from the intended duration. the raf feature is currently supported in all modern browsers. in older browsers, raf may not be available, 2 in which case the qrtengine falls back to the less precise settimeout() method (a low-priority function callback) to control the duration of presentation."
"the time stamps are generated using one of two timers based on browser support. the most precise timer uses the javascript performance.now method, supported by all modern browsers except safari."
"the attentional blink paradigm requires very precise control over the timing of stimulus presentation, and is therefore well suited to investigate the timing capabilities of the qrtengine. stimuli are presented in a rapid serial visual presentation (rsvp), and participants are asked to identify a first target (t1) and to decide whether a second target (t2) was present."
"in this paper, a variant is studied called selective pickup and delivery problem with time windows and paired demand (spdptwpd). the selective variant relaxes the constraint in which all the demands should be satisfied, and the paired demand variant associates one supplier with its related customer. to better explain the problem, a sketch has been presented in fig.1 . in the figure, there are four paired demands which are identified by the same color. the first vehicle departs from the depot, and visits the supplier of the first demand. then it visits the customer of the first demand, and returns to the depot. the second vehicle departs from the depot, and visits suppliers of demand two and three sequentially. then it visits related customers, and returns to the depot. the demand four is not satisfied by any vehicles. when a vehicle is making the route, several constraints should be respected: 1) it departs and returns to the same depot."
"for particle i, v el i represents its velocity, and p op i represents its position (solution). rep p best i is the repository recording all the non dominated solutions ever found by itself. rep gbest i is the repository recording all the non dominated solutions found by the group. h is the index of selected solution. w is the inertia weight."
"in order to demonstrate that typical chronometric effects can be observed when using the qrtengine, we ran a number of classic behavioral rt experiments using this method. accordingly, a stroop task, an attentional blink task, and a maskedpriming task were programmed using the qrtengine and run online via amt. although standard stroop and attentional blink effects have been observed using other online research tools as well, reliable negative masked-priming effects (cf. [cit] ) have not been shown yet using javascript-based methods [cit] . the goal of the present experiments was to test whether the qrtengine timing capabilities are sufficient to show the standard effects in these tasks."
"2) the carried demands should not exceed its maximum capacity. 3) when it arrives earlier than the opening time of a site (earliest arrival time), it should wait until this site is available. it should never arrive later than the close time of a site (latest arrival time). since the studied problem is a generalization of the well-known vehicle routing problem (vrp), it is a np-hard problem."
"in terms of efficient and effective uav and wsn energy consumption, the main problem in this paper is the optimal position and trajectory, minimum time required for collecting data, and the optimal number of uavs to collect data at the network level within a predetermined deadline. in fact, these are vehicle routing problems (vrp) with the requirement of data collection by a certain deadline added to the uav routing problem; hence, the term vehicle routing problem with time window (vrptw) is presented. the present paper solves a mathematical formula for a multi-objective optimization model in which different objective functions are simultaneously optimized. specifically, the proposed optimization model assumes three objectives: the minimization of all distances travelled by all uavs, the minimization of the time required for data gathering at the network level, and the minimization of the number of uavs required for data gathering. this problem produces the optimal position and movement of uavs with the minimum time possible for data collection. increasing the complexity of this problem, in its modeling and solving approach, also involves determining the optimal number of uavs required for covering the desired area.."
"the remainder of this paper is organized as follows. section ii reviews the related work on the design of social robotic faces. section iii presents the design, development, and production of the proposed robotic face. section iv presents the results of our pilot studies on user experiences with the robotic face, while section v presents our conclusions."
"all timings were measured on a second run of the program, where the pre-compiled gpu kernel is loaded from cache. cpu optimizations were evaluated using the alzheimer classification application to compare original with optimized methods, see section 4.4. while in our automatic testing environment (using ctest, part of the cmake package, www.cmake.org) we perform nightly evaluation on both 2d and 3d data, in this paper we only report 3d results. all timing experiments were run on a linux system, detailed in table 1 . this systems contains an nvidia gtx 480 graphical card [cit], while currently [cit] the gtx 780 generation is available. all registrations for the diagnostic classification of ad were run on a cluster of linux systems."
"denote the multiset of the sample pairs whose values differ by m in the first (b − 1) bits (i.e., by right shifting one bit and then measuring the difference) where b is number of bits used to store the value of a pixel. let"
"the developed robotic head represents a new level of integration of emotive capabilities that enables researchers to study socially emotive robots/agents that can generate spoken-language, show emotions, and communicate effectively with people in a natural way as humans do. such systems can be applied in many domains including healthcare, education, entertainment, and home-care. it will also be an ideal platform for designing a new generation of more immersive and effective intelligent tutoring and therapy systems, and robot-assisted therapeutic treatments."
"for implementing algorithms on the gpu we have chosen to build on itkv4's recent addition for gpu acceleration. this module wraps the opencl 1.2 api in an itk-style api, while taking care of opencl initialization, program compilation, and kernel execution. it also provides convenience classes for interfacing with itk image classes and filtering pipelines."
three interesting directions are recommended for future research. the first recommendation is applying the distributed algorithm to achieve an optimal route planning for uavs based on cluster heads. the second direction is extending the suggested framework for supporting mobile wsn. the third area is employing the suggested framework to support real-time applications.
"this article describes our current progress towards designing, manufacturing, and evaluating a lifelike robotic head, called expressionbot, with the capability of showing facial expressions, visual speech, and eye gaze. our eventual goal is to provide the research community with a low-cost portable facial display hardware equipped with a software toolkit that can be used to conduct research leading to a new generation of robotic heads that model the dynamics of face-to-face communication with individuals in different social, learning, and therapeutic contexts."
"the on screen avatar class, such as grace [cit] and second life [cit] are the simplest and earliest robotic faces. animations for these models can be made by developing a model for each expression, morphing between them, and then rendering the result to a computer screen. despite their low cost and high flexibility, they naturally have several limitations due to using a flat display as an alternative to a three dimensional physical head. for example, aside aesthetic unpleasantness, they suffer from lack of establishing mutual gaze (due to mona lisa effect) and physical embodiment that both play vital roles in natural and realistic face-to-face communication."
"i . note that the identity transformation w i,i is also included in (5). the pairwise registrations were performed using a similarity (rigid plus isotropic scaling), affine, and nonrigid bspline transformation model consecutively. the nonrigid b-spline registration used a three-level multi-resolution framework with isotropic control-point spacings of 24, 12, and 6 mm in the three resolutions respectively."
"android faces are other physically implemented robots that are originated from animatronics. they have a larger number of mechatronic actuators controlling a flexible elastic skin; therefore they look more realistic and seem more like a human rather than a robot. example of android faces are albert hubo [cit], hrp-4c [cit], and zeno [cit] . it is an interesting research question as to whether android faces that closely model human looks and behaviors will enter the uncanny valley as their realism mimics humans. due to larger number of actuators and their interaction with skin, they look more expressive than mechatronic faces. however, they are mechanically very complex, expensive to design, build and maintain."
"tissue segmentations were obtained for gm, white matter (wm), and cerebrospinal fluid (csf) using spm8 (statistical parametric mapping, london, uk). for estimation of intracranial volume, a brain mask was required for each subject. this brain mask was constructed using a multi-atlas segmentation approach using 30 atlases (see section 3.3.3). we performed brain extraction [cit] on the t1w images associated with the 30 atlases [cit], checked the brain extractions visually, and adjusted extraction parameters if needed. the extracted brains were transformed to each subject's image and the labels were fused, resulting in a brain mask for each subject."
"to floating point differences between cpu and gpu, sometimes leading to different rounding behavior. example results are shown in figure 7 . figure 6 shows that linear transformations are accelerated less well than nonlinear transformations. this can be explained by (i) the small runtime of the linear transformations on the cpu, which is due to the cpu resampler implementing a highly optimized path for these cases, not possible for the gpu, and (ii) the lower computational complexity of these transformations (commonly more complex operations give more speedup on the gpu since gpu overhead is relatively small in those cases). note that the b-spline interpolator yields higher speedup factors than the nearest neighbor and linear interpolator, for linear transformations (15-20 vs. 1-3), but lower speedup factors for nonrigid transformations (35-45 vs. 45-65) . we remark that the reported speedup factors are a mixture of the speedup factors for the transformation and the interpolation step, related to the time spent in each step. for lower computationally complex transformations, the b-spline interpolator speedup will mostly determine the overall speedup, while for the more complex transformations both speedup factors determine the overall speedup. as a final observation, note the trend that more speedup is obtained for larger images, likely due to a better occupancy of the gpu combined with the copying overhead being less prominent in those cases."
"to evaluate the efficiency of the proposed model in terms of energy consumption, the model output is compared with the method that selects the closest node in each uav movement step. the proposed model and the greedy method are compared in different scenarios from the perspective of energy use. it is observed that energy consumption is directly proportional to distance. hence, minimizing energy is equivalent to minimizing distance [cit] . it should be mentioned that, since energy use depends directly on the travel distance, this distance can be used as a scale for energy comparisons."
"to create the mask we designed a mold using the 3d model of the neutral face in autodesk maya. we 3d printed this mold and used it to vacuum form a 1/8 inch sheet of white translucent acrylic plastic. then, we added a metal band from top of the mask to the projector, which allows us to mount a wig on the robot's head. this makes the expressionbot more aesthetically pleasant and natural, and covers the lights coming out from the sides of the mask due to fish eye lens wide projection-angle (see fig. 1 )."
"it is expected that millions of unmanned aerial vehicles (uavs), which are also called a drones, shall become active in our daily life in the near future and provide wide services [cit] . uavs have numerous applications such as in deadline-based wireless sensor networks (wsns), battlefield surveillance, forest monitoring, and animal tracking in a protected area [cit] . since these devices have small batteries, they usually cannot be ported in long distances due to energy limitations. uavs can move toward wsn devices, collect wsn data, and transfer them to devices that are out of the communication domain of wsn devices."
"it is common to start the registration process (1) using images that have lower complexity, to increase the chance of successful registration. to this end images are smoothed and optionally downsampled, the latter either using linear interpolation (resampling) or by subsampling without interpolation (shrinking). the gaussian pyramid is by far the most common one for image registration, and the computation of this pyramid we target to accelerate. the gaussian filter computes infinite impulse response convolution with an approximation of the gaussian ker-"
"hereafter, we refer to the 3d computer character on the computer screen as the screen-based agent, and the projection of the 3d model onto the robotic head as the physical agent. we used a 23\" lcd display to display the screen-based agent at the same size as the physical agent."
"both rlrmi,j and rtmi,j depend on the integration time tint and on the corresponding original raw pixel value~,j. the integration time associated with rtmi,j is equal to min(t max, tmid)' the time instant tmid can be expressed as a function of operator parameters as follows: 1"
"(b) for original images, if the absolute difference between two samples' values of a sample pair is odd, then it about equally probable that the larger component's lsb of this sample pair is 0 or 1. namely,"
"all initialization, transformation and interpolation kernels are sequentially scheduled on the target device (gpu) using the event list functionality. all kernels are provided with their arguments (inputs), such as input image, resampling domain, etc. the thus generated code is compiled for the gpu at runtime, and then executed. nvidia has implemented a mechanism to cache the compiled gpu binaries, thereby avoiding the need to re-compile the code after the first run. to be able to process large 3d images that may not fit on the gpu memory entirely, we additionally implemented a mechanism to process the input data in chunks, see figure 1 . while the input (i m ) and output (i m (t)) images are loaded resp. allocated entirely, only a relatively small amount of memory is then needed for the intermediate transformation field. this buffer is reused until the full image is resampled."
"in this paper we present a number of cpu and gpu optimizations for the image registration package elastix. the accelerated version of elastix was compared with the original in a study to automatically discriminate between ad patients and age-and gender-matched cognitively normal controls, based on t1w mri. parallelization was used at several places of the image registration framework, exploiting the fork-and-join thread model of itk, i.e., for computation of the cost function derivatives and for joining the results of the several threads. in addition, throughout the registration framework optimizations were performed, for example exploiting the sparseness of the derivative of the b-spline transformation, resulting in an overall increase in performance. compared to the original framework the optimizations only (no parallelization) accelerated image registration by 40-50%, see figures 4, 5. parallelization increases performance until the used number of threads reaches the number of cpu cores. we obtained an overall speedup of 4-5x, using 8 threads on an 8 core system. all registration similarity metrics almost equally well benefit from parallelization."
"where p is the ratio of the length of the embedded message in bits to the total number of samples in an image. as a convention in sequel, when z denote the multiset of the original (clean) sample pairs, let z ′ denote the multiset of the sample pairs after lsb embedding."
"we tested the gpu resampling filter with different combinations of interpolators and transformations. for the b-spline interpolator and b-spline transform we have used third order splines. for brief notation we introduce the symbols t, r, s, a and b for the translation, rigid, similarity (rigid + isotropic scaling), affine and b-spline transformation, respectively. detailed results are shown in table 3 and figure 6 ."
"the proposed fusion method is evaluated on the following experimental setup (the tool used was advanced batch converter 3.8.20, and the interpolation filter was bilinear.): 1) download 3000 originally very high resolution color images in format \"tiff\" from http://photogallery.nrcs.usda.gov, and partition them to 15 groups averagely;"
"in conclusion, the proposed parallelization and optimizations substantially improve the runtime performance of image registration as implemented in the publicly available registration software elastix. this will facilitate medical practitioners and neuroimaging researchers, who commonly rely on image registration to label brain data, classify patients, compare between subjects or image sequences and to perform patient followup. it was shown in a large experiment on public data of patients with alzheimer's disease that the registration results of the accelerated version are very close to the original. this work therefore makes substantially accelerated image registration accessible to a wide audience."
"finally, parameter t s determines how long the control nodes of all pixels must remain connected. during the t s time interval, all control node voltages evolve in the same manner, according to the photocurrent averaged over all pixels. after this time, each control node voltage changes according to the local photocurrent. low t s values make the voltage curves of both the capture and control nodes of a pixel more similar. consequently, the images tend to have low contrast, since the voltages of the capture nodes are close to v mid . as t s increases, contrast and details of the scene are enhanced."
the gpu results for resampling were very close in terms of nrmse to the output produced by the itk cpu code. only for the nearest neighbor interpolator in combination with the affine transformation higher errors are reported. this difference is due timings shown are for all four levels in total.
"real-world scenes usually contain many different illumination conditions, provided by multiple light sources. the luminance (amount of light per unit area) emitted by a source can be roughly as low as 10-3 cd/m 2 (starlight) or as high as 10 5 cd/m 2 (sunlight) [cit] . thus, the luminance values presented in a scene may span several orders of magnitude."
"the above works drive the further researches on quantitative steganalysis. but, we all know that for different images, different quantitative steganalysis methods will obtain different results. we know that fusion of multiple results or features would generate more accurate results [cit] . therefore, we try to fuse the existing quantitative steganalysis methods to estimate the embedding ratio more accurately. in this paper, we consider the main factors influencing the estimation errors of the structural steganalysis and weighted stego image steganalysis, and use the support vector regression to fuse these two typical quantitative steganalysis. the experimental results verify the validity of the proposed fusion method."
"in addition to accelerating the core registration algorithm using the cpu, the gpu was used to accelerate two potentially computationally intensive components that are part of the algorithm. in this paper we accelerated computation of the multiresolution gaussian pyramid and the final resampling step, using opencl. a generic opencl framework was first developed, based on the existing itkv4 gpu acceleration design. to this end a large part of the opencl specification was wrapped in itk classes, following the opencl class diagram and inspired by current itkv4 design. this generic architecture and close integration with itk will ease adoption of opencl for general image processing tasks, not only for image registration. subsequently, we designed a pipeline for pyramid computation and resampling, exploiting the design, notably the opencl queueing and synchronization mechanisms. the developed code is generic and allows extension to other geometric transformations and interpolators. the use of opencl furthermore enables targeting of most accelerator devices (gpu, fpga) available today."
"the opencl implementation was additionally tested with an amd radeon hd 7900 card, and we can confirm portability of the solution. the amd opencl compiler currently does not support caching of compiled binaries, making a timing comparison difficult. the cpu accelerations will be made available as open source in the next release of elastix. the gpu extensions are already incorporated in the elastix testing framework, but are not yet fully integrated in the elastix pyramids and resampler."
"to assess the quality of the images produced by each fptmo version, the resulting images are compared with the ones obtained from two other dtmos, namely the schlick operator [cit] and the rahman operator [cit] . each operator uses a set of parameters, which yields a different image. in order to generate the best results, the parameters from each operator were tuned according to the following design criteria: details should be observed in all darker and brighter regions (i.e. overor underexposing particular regions should be avoided), and the colors look natural. figure 3 shows the images resulting from the application of all operators to three different hdr images, along with the original raw images. for the raw images, no previous tone mapping is performed. as can be seen for all operators, details that were not visible in the original raw images, specially in darker regions, are now made visible: for example, the bottom part of the sunset image, the indoors of the fire extinguisher image and the corridor of the courtyard image, without losing important details in brighter regions."
"additionally, the absolute means of the estimation errors given in figure 4 further show that except that the embedding ratio is close to 1, the fusion method can estimate the embedding ratios with small absolute biases."
"parameter c is the capture node overall capacitance. the photodiode is modelled as a current source in parallel with a capacitor, and c takes the photodiode model capacitance and the parasitic capacitances into account. higher c values make the capacitance discharge slower at the capture node, which leads to higher output voltage values and hence darker images."
"parameter v mid is a voltage threshold that, once reached by the control node, stops the photointegration (i.e. the conversion from photocurrent to voltage) at the capture node, thereby making its voltage ready for readout. this voltage value is typically mapped into the pixel value which corresponds to the middle of the dynamic range. pixel values above or below that voltage threshold are represented equally, which corresponds to reasonable dynamic range usage. the highest possible voltage for a pixel, which maps it into black, is defined as v rst . as v mid is set closer to v rst, the resulting images tend to be darker. on the contrary, brighter images are produced if v mid is set closer to v min, which is the lowest possible voltage for a pixel."
"in this project we decided to adopt opencl for algorithm implementation for two reasons: (i) opencl solutions are independent of the gpu hardware vendor, and can even be run on other hardware accelerators, thereby broadening the applicability of this work; (ii) our image registration package elastix is largely based on the insight toolkit (itk), in which opencl also was adopted recently."
"the filter performs execution row-by-row for the direction x or column-by-column for the direction y, and similarly for direction z. all rows or columns can be processed independently, but columns can only be processed when all rows have finished. this execution model is therefore suitable for the gpu, by assigning each row or column to a different thread, which can then be executed in parallel. the column kernel is scheduled to start after the row kernel, using the opencl queues."
"steganography is the art of hiding the very presence of communication by embedding secret message into innocuous looking covers, such as digital images [cit] . contrarily, one of the main goals of steganalysis is to detect the stego object generated by steganography. steganography and steganalysis have been the key technologies of multimedia information security [cit] . technically, steganography is considered broken when the mere presence of secret message can be established [cit] . however, in order to extract the secret message, the investigators need more details of stego object, such as the length of secret message and the modification ratio of samples [cit] . steganalysis that can estimate the length of secret message or the modification ratio of samples is called as quantitative steganalysis [cit] . the estimation of the secret message's length or the modification ratio can not only be used to distinguish the stego objects, but also help to the estimation of stego positions and the search of stego key [cit] ."
"parallel computation was also implemented at several other places, namely for aggregation of the thread derivatives g t k to a single derivative g k, and for performing the update step of the optimizer, see equation (2). at these places some straightforward vector arithmetic is performed on g k and µ k, which are vectors of possibly very long size (up to 10 6 ). parallelization can be performed here by threads working on disjoint parts of the vectors. implementations using the itk thread model and openmp were created."
"in the opencl design of itkv4 important parts of the opencl specification were missing, most notably the queueing mechanisms and event objects. we implemented a large part of the opencl class diagram, where classes are responsible for a specific task conforming to the opencl standard. opencl event objects are used to synchronize execution of multiple kernels, in case a program consists of multiple kernels. we take advantage of the scheduling and synchronization mechanisms of opencl for the implementation of the gpu version of the resampler, see section 3.2.2, where individual kernels have to be executed in order. in addition, we have added debugging and profiling functionality, which are useful features during development and for understanding performance bottlenecks of gpu architectures. a number of modifications have been made to improve design, implementation, and platform support (intel, amd, nvidia), thereby enhancing the existing itkv4 gpu design."
"based on the support vector regression, this paper proposed a method to fuse the existing structural steganalysis and weighted stego image steganalysis methods for lsb replacement. the proposed fusion method considered two main factors-local variance and saturation pixel ratio-which influence the estimation errors significantly. then, the fusion rule was approximately fitted by the support vector regression on the training set. for the given image, the estimated embedding ratios of the spa and ws methods, the local variance, the histogram of local variance and saturation are fed to the fusion rule to obtain the final estimated embedding ratio. experimental results show that the proposed fusion method can estimate the embedding ratio with higher accuracy as a whole."
"we developed a face animation in c# .net for accurate natural visual speech and show expression based on multi-target morphing method [cit] . recorded utterances are processed by the bavieca speech recognizer [cit], which receives the sequence of words and the speech waveform as input, and provides a time-aligned phonetic transcription of the spoken utterance. the aligned phonemes are represented using the international phonetic alphabet (ipa), a standard that is used to provide a unique symbolic notational for the realization of phonemes in all of the world's languages [cit] . as ipa is intended as a standard for the phonemic and phonetic representation of all spoken languages, having ipa in our system will allow us to add other languages easily as long as the speech recognizer is trained for that language."
"tables i and ii present confusion matrices of the intended and classified expressions displayed on the physical agent and the screen-based agent, respectively. comparing the percentages reported in these tables shows that the surprise and sad emotions were recognized perfectly (100% recognition rate) in both agents by the participants. the joy emotion was recognized perfectly when displayed on the physical agent but was recognized 92% of the time on the screen-based agent. interestingly, anger was recognized correctly 85% of the time for the physical agent, and only 38% of the time for the screen-based agent. disgust was classified as anger more often than it was classified as disgust for both agents, and fear was recognized correctly over 50% of the time in both agents, and confused most often with sadness. in sum, the results showed high recognition rates for joy, sadness and surprise in both agents, lower and similar recognition rates for disgust and fear in the two agents, and superior performance for anger when displayed on the physical agent. in the second experiment, we evaluated the proposed method for visual speech and examined subjects' judgments of speech production quality using the physical agent. two short segments of speech were used in this experiment. segment 1 was a seven second interval of a margaret thatcher's speech with length of 11 seconds while segment 2 was a seven second interval of microsoft anna synthetic speech. we chose these segments to cover a variety of length, speed, accent, and different phonemes (vowels, consonants and labial phonemes). each speech was played two times with different lip synchronization approaches: 1) a basic approach where at each phoneme only the corresponding viseme was displayed without any kernel smoothing; 2) the proposed approach described in section iii-c where lip closure was enforced in labial phonemes and kernel smoothing was applied."
"parameter t max is the maximum integration time. pixels whose control nodes do not reach v mid have the photointegration at their capture nodes ceased at this instant. as t max decreases, fewer pixels have their control nodes reach vmid. their photointegration stops earlier, which yields darker pixels. once the pixel control node reaches vmid, the pixel output value remains unaffected by an increase in t max . hence, an increase in t max affects only the darker pixels. darker regions tend to become gray, while other regions remain unchanged. the net effect corresponds to a decrease in the scene contrast."
"the expressionbot consists of three main sections, the neck control system, the display system and the animation application. the neck system controls the projector and mask position allowing it to be rotated by the application to track faces and head gestures. the display system consists of a small projector with a fish eye lens that projects the animation on a human like (head shaped) face mask. the animation application displays a face animation along with speech and emotion to be projected on to the mask."
"in previous steps, the deadline was determined by the network manager for collecting network data. the aim of this step is to spend the least possible time required for collecting data from the network. it should be noted that the minimum time for collecting network data and the minimum number of uavs required for data collection are two of the present study's objectives. however, these two goals are dependent on each other such that by increasing the number of uavs to a maximum number of clusters, the data collection time is minimized but the minimum number of uavs is not. therefore, by considering this dependency, assigning weights to the dual objectives, and analyzing different scenarios, this step attempts to propose a model that reaches a satisfactory balance between these two objectives."
"this article described the design and creation of a low-cost emotive robotic head, called expressionbot, for natural faceto-face communication (the cost of the hardware system is about $1500). the results of our hri studies on a group of participants illustrated that the subjects perceived the facial expression anger with a much greater accuracy in the robotic face than the screen-based face and they also rated the generated visual speech smooth and realistic on both robotic and screen-based systems. in addition, we studied the perception of eye gaze's direction in two experiments, one in which the head was frontal and only the eye gaze was shifted, and the other with the head rotated but not necessarily correlated with the eye gaze direction. in both experiments, our results showed that participants perceived the robotic face mutual gaze more accurately."
"if it were to be implemented digitally, the proposed fptmo would yield images that would be similar to those produced by other dtmos, as shown by the first version results. the second and third versions generate images whose quality, regarding colors and details, is also comparable to the digital competitors, while retaining the advantages of a focal-plane implementation. the proposed operator successfully generates color tone-mapped images for a focal-plane implementation, therefore achieving the tone-mapping task."
"in order to capture more information of the image's texture, the histogram of the local variance of pixels will also be considered. the local variance of the pixel c k,l is defined as follows:"
"where v loc,min and v loc,max are the minimum and maximum local variances, h min (v) are the minimum and maximum bin value of the pixel's local variance v, s min and s max are the minimum and maximum saturations over all cover training images. on the basis of the svr model trained with the training sets, for a given image, the results of the adopted quantitative steganalysis methods will be fused as follows:"
"to achieve a smooth and realistic look, we used a kernel smoothing technique. during speech production, the avatar system receives the time-aligned phonetic input from bavieca system, converts the phonetic symbols into the corresponding visemes, which specifies the movements of the mouth and tongue, synchronized with the recorded or synthesized speech. the algorithm models coarticulation by smoothing across adjacent phonemes."
"through advanced biological mechanisms, the human visual system adapts itself to the current illumination conditions, accommodating the high dynamic range of luminance values of the scene in such a way that details in both darker and brighter regions are observed. however, when attempting to display the same scene without any pre-processing in a conventional display medium, the same level of detail is not maintained, since such conventional displays can typically represent values in an 8-bit luminance range."
"multi-core computers have enabled the acceleration of a wide variety of computationally intensive applications. nowadays, another type of hardware promises even higher computational performance: the graphics processing unit (gpu), which has a highly parallel hardware structure. this makes them more effective than general purpose cpus for algorithms where processing of large blocks of data can be performed in parallel. the increasing computing power of gpus gives them considerably higher peak computing power than cpus. for example, nvidia's geforce gtx 780 gpu provides 3977 gflop/s and amds hd7970 gpu 3788 gflop/s, while intels xeon x5675 cpu reaches only 144 gflop/s."
"the fptmo circuit depends on component values and time-domain waveforms that affect output image perceptual attributes such as contrast and brightness. the dtmos are typically expressed in terms of parameters that control output image perceptual attributes. therefore, in a digital simulation of the fptmo, six parameters must be set: vmid, c, mph, me, t max, and t s . the parameters are illustrated in figure 1 ."
"to compare registration accuracy between original and accelerated versions of elastix, ∼54k t1w image registrations have been performed with each version in the setting of an ad classification experiment. registration results were similar as shown by visual inspection of the median result and the rmse of the deformations field: 0.521 ± 0.460 mm (voxel-wise) and 0.749 ± 0.446 mm (region-wise). in addition, the classification features calculated with the two elastix versions were very similar. the differences in features between the two versions of the registration software were much smaller than the features themselves: for the voxel-wise approach the template spaces looked very similar, and for the region-wise approach the dice overlap of the rois was very high and the differences between the gm volumes were relatively small. this resulted in a high classification performance, which was not significantly different between the two elastix versions. remaining differences between original and accelerated algorithms are attributed to a combination of algorithmic changes and hardware effects. for example, where in the original version the sample contributions (see equation (3)) are directly accumulated in a single derivative, in the parallel version multiple derivatives are created, which are later joined to a single derivative. this changes the order and amount of arithmetic operations, and depending on machine precision this will lead to slightly different results. in addition, since image registration is an iterative process, small differences will be propagated until the end. in general, all implementation choices influence the final result. in the neuroimaging application the differences in the features (gm volumes) and classification results provide information on the impact of these imprecisions on the final result, which appears to be small. fast registration algorithms have most impact when used in a time-critical setting. an example would be the diagnostic classification of a single patient on a clinical workstation, performed by a neuro-radiologist. generally, interactive speed is desired in such a user setting. the multiple registrations needed for the classification would be performed in parallel on a computing cluster, as was done in this work, which means that total classification time is limited by the runtime of a single registration. an example from outside the image-guided therapeutic domain would be (near) realtime motion compensation for radiation therapy. for research, fast registration enables testing of a wider range of algorithm parameters, or enables testing on large groups of patients within reasonable time. given the general nature of similarity based image registration the results are naturally applicable to a wide range of image registration problems."
"4. the milp is modeled by considering three parts of the objective as normalized, using a coefficient or weight for each part of the objective and examining its effect on output."
"the colors also look natural for all operators, although they seem different in the fptmo second version. in that case, the integration time is adapted according to the local pixel value, in order to avoid over-and underexposures. this tends to concentrate the tone-mapped pixel values around the center 978-1-5386-4881-0/18/$31.00 ©2018 ieee of the dynamic range, and hence, after demosaicing, colors are less saturated. in the third version, since red and blue pixels have their integration times determined by the green pixel next to them, their tone-mapped values may be more spread out along the dynamic range, leading to more saturated colors."
"we identified two independent registration components that allow for parallelism: the gaussian pyramids and the resampling step. the gaussian filtering relies on a line-by-line causal and anti-causal filtering, where all image scan lines can be independently processed; the resampling step requires for every voxel the same independent operation (transformation followed by interpolation)."
"in conventional digital image capture, white balance happens before demosaicing and tone mapping. after the proposed focal-plane tone-mapping approach, the white-balance stage should use a different gain for each pixel, as described next."
"in this table, by considering m ch and m trj, which are respectively the number of uavs that move in the network and the number of uavs that are assigned only to a cluster, the current paper attempts to minimize the traveled path by assigning uav to each cluster in scenarios where the coefficient of the minimum energy objective is the dominant value. however, the number of uavs assigned to each cluster decreases in conditions in which the coefficient of the minimum uav objective is dominant. thus, uavs try to cover all clusters and collect data with the traveling path. if the coefficient of the least uav movement time is dominant, then the uav travel time decreases and uavs attempt to cover all clusters and gather data with the traveling path."
"this subsection will describe the method to fuse different quantitative steganalysis methods based on svr (see figure 1 ). this fusion method supposes that we have a set of cover images and some sets of stego images with different embedding ratios which will be used as training data. we use the existing quantitative steganalysis methods to estimate the embedding ratios of the training images, and extract the statistical features of them, such as the local variance, histogram of local variance and saturation. then some standardization parameters will be computed from the features extracted from the cover training images and used to standardize the features extracted from the cover and stego training images. the standardized features, the embedding ratios estimated by the existing methods and the targets which are set as the actual embedding ratios will be fed to the svr to train the corresponding model which actually is the fusion rule of the adopted quantitative steganalysis methods."
"according to the modification pattern which must occur when the sample pairs transition from a trace submultiset to another one and the probability that the modification pattern occurs, the cardinalities of the trace submultisets x 2m−1 and y 2m+1 can be described by the functions with respect to the embedding ratio and the cardinalities of"
", with i i (v i ) representing the deformed individual training images. the test images were not included in the construction of template . for the test images, the transformation to template space (v i ) was obtained using the same procedure described above: using pairwise registration of each image with all training images, followed by averaging and inversion. brain masks and tissue maps were transformed to template space using v i ."
"in the original circuit, each pixel cell requires two photodiodes, because its control node voltage depends both on the global average photocurrent and on the local photocurrent."
"(a) the message bits are randomly embedded into the lsb plane of the cover object, so the embedded message is uncorrelated with the cover bits replaced, and the probability of that the modification pattern π occurs is"
"using the kernel technique resulted in smoother and more natural looking animations; however, when utterances included the labial phonemes /b/, /m/, /p/, which are accompanied by lip closure, the smoothing algorithm prevented the lips from closing when the duration of the labial phoneme is very short (e.g., 5 msec.) and the adjacent phoneme targets caused the lips to be open (e.g., /6/ as in \"mama\"). to force lip closure for the labials, we extended the duration of labial visemes to include the closure interval (the period of relative silence before the sound is released, thus increasing the chance that at least one frame consisting of just the labial viseme will appear."
"the simulation was done in different scenarios to compare the performance of the milp method with the greedy method. results show that the suggested framework can provide efficient data collection while meeting the limitations of energy and deadlines, which are dependent on the critical level of application."
"given the tremendous effort required to develop robot heads and the number of design choices that must be made, including aesthetic face design, mechanical design and construction, hardware implementation, etc, it is often difficult to redesign or rebuild the head based on user experiences and limitations discovered during research. an alternative approach that overcomes many of these problems is to use state-of-the-art character animation technologies to create 3d avatar models that produce natural speech and facial expressions, and project these models onto a robotic face that can move like a human head. projecting an animated facial character on a 3d human-like head has several advantages over displaying it on a 2d flat screen. the first advantage is the physical embodiment and movement of the 3d head that makes the robot be more believable and better perceived by the users [cit] . the other advantage is perception of mutual eye gaze which plays a vital role in face-to-face communication. it is known that, the perception of 3d objects that are displayed on 2d surfaces are influenced by the mona lisa effect [cit] . in other words, the orientation of the object in relation to the observer will be perceived as constant regardless of observer's position [cit] . for instance, if the 2d projected face is gazing forward, mutual gaze is perceived with the animation, regardless of where the observer is standing/sitting in relation to the display. this effect is significantly reduced by projecting the face on a 3d human-like head."
"where e(i,j) is the euclidean distance among nodes. thus, the research problem is presented as follows: finding the shortest route (m) from node 0 to node n+1 with these conditions:"
"parallelization is performed in the context of the image registration software elastix [cit], available at http:// elastix.isi.uu.nl. the software is distributed as open source via periodic software releases under a bsd license. the software consists of a collection of algorithms that are commonly used to solve (medical) image registration problems. the modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application. a command-line interface enables automated processing of large numbers of data sets, by means of scripting."
"the paper is outlined as follows. in section 2 preliminary information is given about image registration, elastix, opencl and itk. the registration accelerations are described in section 3, together with the methodology for voxel-wise and region-wise diagnostic classification of ad. experiments and results are given in section 4, detailing the obtained speedup factors (section 4.2 and 4.3). in section 4.4 an accuracy analysis is made comparing original and optimized versions of elastix. for this evaluation, we used structural mr data of ad patients and healthy volunteers from the alzheimer's disease neuroimaging initiative (adni) database. the paper is concluded in section 5."
"another advantage of the proposed modification is that the white-balance operation is simplified. since green pixels remain unchanged after the white balance (their gains are set to unity), the integration times for red (and blue) pixels, in the new and reference methods described in section iii-a, are the same. as a result, the white-balance gains used after tone mapping have the same constant gains, as if white balance was done before tone mapping. in this section, the results from three fptmo digital implementations are shown. the first version uses the framework commonly adopted by other dtmos, which is defined as follows: first, the image is acquired through a cmos sensor (using a particular bayer pattern); then, white balance and demosaicing are performed on the raw sensor data, yielding a colored image; the tmo is then applied on the color image luminance channel. to recover color information from the tone-mapped luminance, each original color channel is scaled by the ratio between the tone-mapped and original luminance (5)"
"there are many designs for robotic faces ranging from 2d graphical avatars to mechanically controlled robotic faces. these designs fit into four main categories: mechatronic faces, android faces, onscreen avatars, and light-projected (retro-projected) physical avatars. these categories are discussed in the following."
"we designed the models in three portions: eyes, face and hair. this design allows them to be interchangeable and customizable, and gives us the ability to design any number of characters to easily change the robot's appearance. the system has the ability to control eye gaze independently of the visual speech and facial expression animation, and thus enables functionality to control eye gaze (e.g., in concert with face tracking)."
"2. another promising application of uavs is uav-aided relaying where uavs are sent for providing a reliable wireless connection between two or more distant users or group of users in an enemy environment, such as for communication between front lines and a base for emergency or military operations [cit] ."
"in this simulation, network size is 600 * 600 m 2 . the evaluation was performed with the python software as an exploratory implementation platform in pulp library. to implement the milp optimization objective, a system with core i5-2410m 2.30 ghz central processing unit and 4 gigabytes basic memory was utilized. other parameters of the simulation are presented in table 5 ."
"first, collecting aerial data by uavs can be directed automatically as a mobile data collection. there is no movement limitation in ground transfer and it can be used in specially protected areas where humans cannot access. a deadline for collecting wsn data is determined with regard to the type of application. for example, for many deadline-based wsn applications, data collection time is determined with regard to priority and the critical level of previously collected data. in this case, the distant user can ask different deadline times for data collection at the network level."
2. a framework is provided for energy efficient data collection employing multiple uavs in the wsns and with regard to the deadline and energy of sensor nodes and uavs.
"the objective of the first experiment was to assess how accurately subjects were able to interpret the projected facial expressions. participants watched the robotic agent and the screen-based agent in two different sessions randomly (i.e. some participants observed the physical agent first while the others watched the screen-based agent first). a series of six basic emotions (joy, sadness, surprise, disgust, fear and anger) were displayed in random order. each expression was displayed one time for about 5 seconds. the subject was then asked to select one of the six categories. they could also respond \"none,\" if they were unable to assign the facial expression to one of the six categories."
"where j i are products of the b-spline basis functions, following from the definition [cit] . the derivative of the b-spline transformation is therefore a relatively small and sparse matrix, with repetitive elements, thus only p elements need to be computed instead of d 2 p or even dn. again examining (3) we can see that the multiplication j t ∂i m ∂x can also be accelerated by omitting the empty parts."
"since only green pixels are used to calculate the average photocurrent, a new circuit modification is proposed, that is, the red and blue pixel integration times are determined by the control node voltage of the neighboring green pixels. with this modification, red and blue pixels no longer need a control photodiode, leading to a reduction of the sensing circuit area: for every 2x2 pixels (rggb), only 6 photodiodes are needed (4 capture and 2 control), instead of 8. the schematic diagram (for a red-green pixel pair) is shown in figure 2 ."
"the final category, and the focus of our research, is the light-projected physical avatars; these consist of translucent 3d masks with 2d/3d avatars projected onto them. since the robotic face is projected onto a mask, the robotic face can range from cartoon-like to photorealistic. light-projected physical avatars are thus a highly flexible research tool, relative to mechatronic and android faces. factors such as engagement, embodiment, believability, credibility and realism can be investigated based on the appearance and behaviors of the 3d animated models, and the robotic head and neck movement. moreover, such a system can avoid the mona lisa effect [cit] and hence users can correctly perceive the robot's eye gaze direction. additional features of robotic avatars include relatively low development cost, low power consumption, potentially low weight and fast reaction."
"white balance with constant red and blue gains, followed by tone mapping and demosaicing (which is the conventional operation order), yields images with natural-looking colors. we use this operation sequence as a model to find the correct gains for each pixel after tone mapping is independently performed in each color channel. we refer to this method as the reference method. the reference value for the (i,j)-th red pixel rtmi,j obtained by this method, after tone mapping, is made equal to the corresponding red pixel rlrm . obtained ',j after tone mapping and without any previous white balance multiplied by an unknown gain k~(the same procedure applies to blue pixels), that is,"
"in high dynamic range imaging, the display task is achieved by tone-mapping operators (tmos), which are non-linear mappings from real scene luminance values to display luminance values. some mappings are inspired by photographic techniques [cit], while others are based on the human eye brightness and contrast perception [cit] and become more 978-1-5386-4881-0/18/$31.00 ©2018 ieee sophisticated as they try to incorporate some human vision aspects, such as visual acuity and glare [cit] ."
"voxel-wise features were extracted in a common template space ( template, see figure 2 ) based on the data of the training set. this common template space was constructed using a procedure that avoids bias toward any of the individual training images [cit] . in this approach, the coordinate transformations from the template space to the subject's image space (v i :"
"image registration is a frequently used technique in medical image processing. it refers to the process of automatically aligning imaging data, where a moving (target) image i m is deformed to mimick a fixed (reference) image i f . in other words, registration is the problem of finding a coordinate transformation t that makes i m (t) spatially aligned with i f . the quality of alignment is defined by a cost function c. the optimal coordinate transformation is estimated by minimizing the cost function with respect to t, usually by means of an iterative optimization method embedded in a hierarchical (multiresolution) scheme. [cit] . areas of application include the alignment of data sets from different modalities [cit] to fuse information, comparison of follow-up with baseline scans to follow disease development, alignment of different mr sequences for extraction of quantitative mr parameters such as in diffusion tensor imaging or mr relaxometry [cit], alignment of pre-and post-contrast images [cit] to aid breast cancer detection and diagnosis, and updating treatment plans for radiotherapy and surgery [cit] ."
"writing parallel programs to take full advantage of this gpu power is still a challenge. the opencl c programming language (www.khronos.org/opencl/) can be used to create programs that can be executed on one or more heterogeneous devices such as cpus, gpus, fpgas and potentially other devices developed in the future. cuda (www.nvidia.com/object/cudahomenew. html) on the other hand is nvidia's c language targeted to nvidia gpus only. opencl is maintained by the non-profit technology consortium khronos group. an opencl program is similar to a dynamic library, and an opencl kernel is similar to an exported function from the dynamic library. in opencl programmers can use opencl command queue execution and events to explicitly specify runtime dependencies between arbitrary queued commands, which is different from c(++) where sequential execution of commands is always implied. opencl is based on the c99 language specification with some restrictions and specific extensions to the language for parallelism."
"first, the nodes are clustered in a distributed manner, so as to determine the place where uavs receive data from volume 6, 2018 network nodes. for efficient clustering in terms of wsn energy consumption for data transfer to the network, the proposed clustering minimizes the mean sent distance of cluster members while the wsn consumes the minimum possible energy when sending data to the cluster head. for this purpose, the threshold limit for the sent distance of wsn nodes is determined based on the network domain and by considering the energy parameter; then, each node can recognize its neighbors from this distance. next, the threshold limit of the number of nodes in each cluster is determined based on network density, which is dependent on the number of nodes and the network area size. neighborhoods having an equal or greater number of neighbor nodes than this threshold form clusters. in this case, the number of clusters required for an efficient sent of information by a wsn is determined and the centers of these clusters are considered as the uav meeting place. after cluster formation and uav placement in the center of each node, data are sent from the wsn to the uav, which receives data from the cluster after a certain amount of time passes. the problem in this step is the decrease in wsn energy consumption when the number of clusters is high. because each uav in the center of each cluster stops for a while to receive the cluster's data, more time is spent and thus the distance traveled by uavs increases, which is contrary to the proposed model's objectives. in comparison, if the number of clusters is low, the uav distance traveled is less and wsn energy consumption rises; wsn node energy use grows due to longer sent distances among these nodes. in fact, achieving a balance between these two conditions should always be considered in this step."
"despite significant progress towards development of realistic robotic heads, a number of problems remain to be solved. social robots such as paro [cit] have the robustness and cost effectiveness for large scale, but lack the sophistication for deep social interaction. other robots such as kismet [cit] exhibit facial expressions and head and ear movement using mechanical components, however, once these mechanical platforms are built, they are fixed and cannot be readily modified. on the other hand, more human-like robots such as simon [cit] possess profound capabilities for social interaction, but due to large number of actuators in their mechatronic faces, they are expensive and maintenanceintensive for large-scale trials. another potential problem in the design of robotic heads is the \"uncanny valley\" effect [cit], where the effect of aesthetic design of a robot may influence the user's experience, perception, and acceptance of the robot."
"3) standardize the extracted features by maximum and minimum feature values obtained from the cover training images; 4) feed the embedding ratios estimated by the adopted quantitative steganalysis methods (viz. the spa method and ws method), the standardized local variance, histogram of local variance and saturation to the obtained svr model, viz. the fusion rule, to obtain the final estimated embedding ratio."
"to incorporate color information at the pixel level, the conventional color filter array (red, green and blue filters arranged according to the bayer pattern) is placed on top of the pixel matrix. to yield a color image, white-balance and demosaicing operations are required. to keep focal-plane circuit complexity low, only tone mapping is performed at the focal plane (i.e. before white balance and demosaicing)."
"using different scenarios, this section compares the suggested method with the greedy method. two metrics evaluate the performance of the proposed method. the first metric is the maximum traveled distance, which is defined as the mean length of tour used by all uavs to finish a round. the second metric is total travel time, which is defined as the mean of maximum travel time in uavs to end a round. simulation settings and results of the evaluation are presented in the following."
"the second and third versions use the framework established for the real focal-plane implementation, which is similar to the previous one, except that the processing order is reversed (i.e. the tone mapping is carried out before the white balance and demosaicing). rather than being applied to the original color image luminance channel, the tmo is directly applied to the raw pixel values from the camera sensor. the difference between these last two versions lies in how the blue and red pixel integration time is determined: in the second version, it is determined by the very pixel value, while in the third one it is determined by the value of a neighboring green pixel."
"in order to blend the expressions with the lip movement, the animation uses the following formula to generate facial expressions based on the current viseme and emotion morph targets:"
"collecting data with uavs not only features the flexibility of mobile datasets, which are suitable for wsns on a large scale, but there are also the following advantages [cit] :"
"in this step, after clustering and determining meeting points of uavs, the present work attempts to solve its main problem of the research. in this step, wsn nodes are assumed to be fixed in the network. in order to efficiently determine the route in the network efficiently, the network graph is formed as follows: by choosing the node centers selected in the previous step, the uav meeting points are considered as network nodes. moreover, determining the optimal number of uavs for network coverage is among the goals of the present work. such a solution should suggest some routes for uav movement in these nodes, where uavs simultaneously travel to these points and receive data. in order to control the number of these routes, 0 and n+1 virtual nodes are considered as initial and final nodes in the movement route of all uavs. therefore, by considering node 1 to node n as node centers, set n as network nodes is defined as follows:"
"the optimized registration procedure was validated with an experiment of classification of ad patients and healthy controls. the classification was based on two types of features, voxel-wise and region-wise features, which were extracted from structural mri. these feature extraction approaches involve numerous image registrations steps, which were performed with both the accelerated version of elastix and the most recent release elastix v4.6. the classification performances were compared between the two versions, because then we can see in practice, in an application that makes heavy use of rigid and nonrigid registration, if and how much the results are affected by the acceleration. in this section the methods for the classification experiment are explained."
"some images, such as the courtyard, contain very dark raw pixels that have values close to the noise floor of the sensors. depending on the tmo applied to them, salt-and-pepper noise may arise in such regions, thus degrading the image quality. the salt-and-pepper noise is more evident in higher image resolutions. to suppress noise effects, an additional low-pass filtering stage may be required after demosaicing. noise is not observed in the images shown in figure 3, because they were pre-filtered by a low-pass bicubic filter, and then rescaled to 10% of their original sizes. the non-filtered and filtered versions of the courtyard images with original resolutions are available [cit] ."
"although robots are finding their place in our society as artificial pets, entertainers, and tools for therapists, current technologies have yet to reach the full emotional and social capabilities necessary for rich and robust interaction with human beings. to achieve this potential, research must imbue robots with the emotional and social capabilities -both verbal and non-verbal-necessary for rich and robust interaction with human beings. this article describes research in which robotic heads can model natural face-to-face communication with individuals. human face-to-face communication is based on multiple communication channels including auditory and visual. facial expressions and gaze direction are the most important visual channels in human face-toface communication. these channels of communications are considered in our research."
"as mentioned earlier, to solve the present research's main problem, efficient clustering in the network is performed so as to minimize the energy consumption of sending wsn node data. this is followed by devising a network graph and utilizing milp to arrive at a solution."
"the contributions of this paper can be defined as follows: 1. by identifying the key challenges of the uav in the wsn network, an accurate investigation of wireless communications is conducted using the uav, which constitutes the main features of this paper."
"as shown in eq. (3), the raw pixel values from the input image are required to compute the white-balance gains. in a real focal-plane implementation, such values are not available, since they are directly tone-mapped after acquisition, and their original (raw) values are not stored. the raw values must be reconstructed from the tone-mapped pixels before the whitebalance coefficients are calculated."
"the region-wise features were calculated in subject space ( i ) as the gm volume in each roi obtained from the probabilistic gm maps [cit] . to correct for head size, these features were divided by intracranial volume. all features were normalized to have zero mean and unit variance."
"as described in section 2.1 the image registration algorithm consists of multiple parts: general tasks such as image reading and setting up the registration pipeline, pyramid construction, then iteratively derivative computation and updating of the parameter vector using (2), and finally resampling. to accelerate the registration algorithm, we identified the pyramid construction, the optimization routine and the resampling step as the most dominant parts in terms of performance. acceleration possibilities for the optimization routine are identified by recognizing parallelization options, by manual inspection of the source code, and by the use of the callgrind profiling tool [cit], see section 3.1. this component of the registration algorithm is performed on the cpu. both pyramid construction and resampling are in this work off-loaded to the gpu, because these components exhibit clear opportunities for massive data parallelization, see section 3.2. finally, in section 3.3, we present the methods used for validation of the optimized registration procedure with an experiment on diagnostic classification of ad which heavily relies on image registration."
"a greedy algorithm is an algorithmic paradigm that follows the problem of solving heuristic of making a locally optimal choice at each stage [cit] with the intent of finding a global optimum. the main idea behind a greedy algorithm is local optimization. that is, the algorithm picks what seems to be the best thing to do at a particular time, instead of considering the global situation [cit] . in many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time. this heuristic does not intend to find the best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. greedy algorithms mostly (but not always) fail to find the globally optimal solution because they usually do not operate exhaustively on all the data. for example, all known greedy coloring algorithms for the graph coloring problem and all other np-complete problems do not consistently find optimum solutions. nevertheless, they are useful because they are quick to think up and often give good approximations to the optimum. greedy algorithms appear in network routing as well. using greedy routing, a message is forwarded to the neighboring node which is ''closest'' to the destination. a greedy strategy for our problem (which is of a high computational complexity) is the following heuristic: ''at each step of the tour, visit the nearest unvisited cluster head''. according to the greedy approach, the uav always pursues the closest cluster in its movement. in other words, the uav always flies to a neighboring cluster with the closest distance. also, in the greedy approach, the greedy-uav continues to visit the same neighboring clusters whose cluster heads the greedy-uav found the previous time. in other words, the greedy approach is trapped in the local minima and cannot adapt itself based on uav movements [cit] ."
"for a given language, visually similar phonemes are grouped into units called visemes. for example the consonants /b/, /p/ and /m/ in the words \"buy,\" \"pie,\" and \"my\" form a single viseme class. we categorized english phonemes into 20 viseme classes. these classes represent the articulation targets that lips and tongue move to during speech production. a graphic artist designed 3d models of these viseme classes in maya. figure 2, demonstrates some visemes used in our animation system. finally, natural visual speech is obtained by blending the proper models corresponding to each part of speech with different weights."
"to achieve better performance each thread uses the local gpu memory, which is fastest, but this introduces a limitation on the input image size. current gpus usually only have 16kb of local memory, and the algorithm allocates three floating point buffers the size of the row/column (input, output plus temporary buffer). this results in a maximum image size of 1365 pixels, and therefore our gpu implementation works only for images of maximum size [cit] or [cit] . this limitation can be avoided by using other platforms with a larger local memory (e.g., intel cpus allow 32kb), or by changing the algorithm altogether (e.g., by direct convolution with a truncated gaussian kernel)."
"we separated the facial expression morph targets into upper and lower face morph targets; the upper face includes everything from the tip of the nose upwards. the lower face includes the region below the nose; mainly the lips, lower cheeks and chin. this partitioning of the face enables us to adjust the weight of just the lower face morph target weights so that the upper face remains consistent with the morph targets of desired expressions. in addition, for labial and labiodental visemes (those for the letters m, b, p, f and v) that require the avatar's lips to be closed or nearly closed to look natural, we developed visemes pre-blended with the open mouthed emotions. these are used to replace the viseme and lower face expression when they come up in combination."
"based on the suggested model, the objective function in the suggested model is in multi-objective form, which is considered as the following for solving the suggested model:"
"1. total coverage in which uavs are employed to help available communicative infrastructures to provide integrated wireless coverage in the desired area. in this case, uavs usually act as fixed networks above a desired area in the form of base stations [cit] . uav-based wireless communication has its unique capabilities for quick, reliable, and affordable connections to areas that are weakly covered by ground networks [cit] ."
"3. data gathering is another usage for uav systems. this application is attractive especially for wsns from which uavs can collect data, in which case the operational power of sensors is decreased thus lengthening the lifetime of the network [cit] ."
"the current paper presents a new framework to increase efficiency in data collection with a deadline in a wsn and multiple uavs. to minimize the total travel distance, uav travel time, and network energy consumption, the network is first divided into a set of clusters. each of the clusters has a head and their centers are considered as a place for uavs. then, the initial and final virtual nodes are considered for controlling the minimum number of uavs. next, the present work attempts to complete the suggested solution and achieve the least uav movement time required for collecting data. this problem is formulated as a milp model, normalized, and then some weighing coefficients are added to find the optimal uav routes in regard to energy constraints and the deadline related to the basic application's critical level."
"the remainder of this paper is as follows: in section ii, related work on using uavs in wsns is examined. section iii presents the limitations and assumptions of the network model. in section iv, the steps of the proposed solution are described. in section v, the proposed model is presented with normalization and a coefficient or weight for each part of the objective function is introduced to find optimal solutions to the problem. section vi presents a simulation and performance evaluation of the proposed framework. finally, results and future work are presented in section vii."
"in our experiments, we select the v-svr with radial basis function kernel as the training tool. the proposed fusion method is called as the spa ws svr method because it fusing the results of the spa and ws methods based on svr, and compared with the spa method and ws method before fusing. figure 2 shows the mean of the estimation errors of the proposed fusion method, the spa method and the ws method. it can be seen that the fusion method can estimate the embedding ratios with the errors' mean closer to 0 significantly except that the actual embedding ratio is close to 0 or 1. figure 3 shows that in the aspect of the standard deviation of estimation errors, the fusion method will reduce the standard deviation to about 80% of that the best methods before fusing can achieve."
"during the 3 days before behavioral tests, animals were attached to optical fibers without led stimulation for habituation. on the test day, the optic fiber (200 μ m core, na 0.22, thorlabs) was connected to a blue led source (480 nm, 0-30 hz pulses, 5-ms pulse duration, thorlabs). the led power measured at the tip of the fiber (connected with optic cannula) was around 3-5mw."
"the html5 file api provides an api for representing file objects in web applications, as well as programmatically selecting them and accessing their data. this is interesting in the case of i-search, since users are very likely to compose their query with local files, like audio files, pictures, etc. the file api allows for a new paradigm to deal with files, such as native support for dragging and dropping elements from the desktop to the i-search interface. this convenience feature is not crucial, an html file upload form serves as a fallback."
"another part of our methodology targets the increased complexity of search tasks and the necessity to collaborate on those tasks in order to formulate adequate search queries, which lead faster to appropriate result. the increased complexity is primarily caused by the vast amount of unstructured data on the internet and secondly by situations where the expected results are very fuzzy or hard to describe in textual terms. therefore the cofind (collaborative finding) approach is introduced as a collaborative search system, which enables realtime collaborative search query creation on a pure html interface. real-time collaboration is well-known in the field of document editing (e.g. etherpad [cit], google docs [cit] ); cofind applies the idea of collaborative document editing to collaborative search query composition."
"here, we compared the anxiety level between an autism animal model (16p11.2df, which harbors a chromosome microdeletion) and normal animals. the autism model mouse shows elevated anxiety compared to normal animals (fig. 2b) . the tracking module can also be easily applied to elevated plus maze (epm) test ( fig. 2c) to calculate the percentage time spent in the open arms of the maze, which is also an indicator of anxiety level. in morris water maze test ( fig. 2d), we can calculate the travel distance and time spent in each quadrant. moreover, track-control can be applied simultaneously to multiple spatially nonoverlapping test areas, which greatly expands throughput ( fig. 2e) . the serial coordinates of the animal and boolean values for all frames are stored in a .csv file that can be used for deeper data mining."
"for the user study we recruited seven participants (six male and one female) aged between 20 and 35. all participants were familiar with textual web-based search. we asked all study participants to find three different items (sound of a tiger, 3d model of a flower, image of a car). the participants were shown these items beforehand in their original format and were then instructed to query the system in order to retrieve them via i-search. for the study a windows laptop with a capacitive touch display was used. each participant was interviewed after the completion of the study. our goal was to validate our interface design as well as to measure the impact of the possibility of multimodal search. in general, we observed that the concept of multimodal search was new and unfamiliar to all participants. actually, before the user study all participants considered web search equal to text-based search, and only by using i-search they became aware of the possibility to use different media types and of multimodal interaction at all. our hypothesis (h1) was statistically not supported. it depends highly on the behavior of each individual person whether one or more search items or media types are used. in combination with (h2), one obvious conclusion of the participant interviews was that adding search items as well as customizing them has to be as easy as possible. the participants did not hit obstacles in using one special query modality, however stated that if a query modality was difficult to use, they would replace it by using different query modalities, even if this implied that the search query would become complicated and challenging. the same conclusion applies to hypothesis (h3). in order to allow for multimodal search queries, the following recommendations can be derived from our user study:"
"with the rapid development of computer vision and machine learning, object detection has welcomed its great progress and enabled us to automatically analyze videos, increasing the objectivity. for example, deeplabcut has successfully achieved markerless feature extraction from pre-recorded videos [cit], and another group has demonstrated that the method has potential of being applied in real-time feedback control [cit] . deeplabcut was built upon resnet and not focused on the real-time detection. however, fast methods such as yolo [cit] are available now. in the future, we will implement light-weight and fast network models to facilitate detection and classification of more specific behavioral patterns, e.g. grooming, sniffing and rearing, and to gain a better control in complex behaviors. [cit], or are not written using popular programming languages such as perl [cit] ."
"looming stimulus such as an expanding dark disc which mimics the shade of an approaching predator can elicit defensive behaviors such as freezing and flight in rodents [cit] . the visual stimulation was manually triggered by experimenters in previous studies [cit] . here, we demonstrate that track-control can achieve automation of the visual stimulation with a precise control of visual angle. the test was performed in a rectangular acrylic box, with an lcd monitor placed on top of one side of the box (fig. 3a) . the looming stimulation was coded using psychtoolbox-3 ( http://psychtoolbox.org). before starting the test, users can define the roi to trigger looming stimulation. by controlling the relative position of the animal to the lcd monitor, we can standardize the visual angle of stimulus at the onset of looming stimulation."
"custom mouse detection software was used for online real-time animal detection (written by in python 3.4, www.python.org, with opencv library, https://opencv.org) [cit] . the behavior of the animal was monitored using an infrared camera at 24fps. then each frame was gaussian blurred and then binarized. the gravity center for the detected contour was used to determine the location of each animal. in the two-chamber place preference test, the stimulation chamber was randomly assigned (balanced within the group)"
"context-aware search is one of the features of the i-search framework. this is particularly useful in the case of a user searching on a mobile device, as many mobile queries are location-based. html5 includes the geolocation javascript api that, instead of looking up ip address-based location tables, enables web pages to retrieve a user's location programmatically. in the background, the browser uses the device gps if available, or computes an approximate location based on cell tower triangulation. the user has to agree for her location to be shared with the application."
"with the device api [cit] the w3c currently creates the next standard related to html5. it is mainly targeted to give web browsers access to attached hardware devices of the client computer. therefore the media capture api, which is a part of the device api, will enable access to the microphone and the web camera of the user. we use this api in combination with appropriate fallback routines in order to create audio queries as well as image queries captured on-the-fly."
"the stimulation parameter can be defined in the microchip programmed using the arduino ide ( https://www.arduino.cc/en/main/software). when receiving the usb command, a ttl train (e.g. 20hz, 5ms on, 45ms off) is delivered to a led driver to achieve optical stimulation."
"this behavioral test is easy to apply, and with a collaborative effort, the neuroscience community may soon establish a framework for valence coding in the entire brain."
"next, we extract the object contours based on a border-following algorithm [cit] . the centroid of the extracted polygonal contour is used to define the position of the animal, and the x and y coordinates in each frame are stored in a .csv file for further analysis (supplementary movie 1)."
"for each trial, the mouse was initially placed in the non-stimulation chamber, and led (480 nm, 10 hz, 5-ms pulse duration) stimulation was constantly delivered once the animal entered the stimulation chamber and was stopped when the animal exited. the total duration of each test session was 20 min. animals were returned to their home cage after each test session. the stimulation chamber was randomly assigned to each animal and balanced for the whole group."
"the possibility to generate more complex, but also more effective search queries with multimodal search interfaces, as well as the nature of the internet as an environment where people can assist each other, make the integration of collaborative interaction approaches for search engines interesting. mainly the work of morris [cit] and pickens [cit] described interesting ways of collaborative search approaches. they make use of a search session and state variables in user profiles to transfers changes made in the interface of one user to all other collaborating users and vice versa. further, the survey about collaborative web search practices done by morris [cit] as well as the status quo practices presented by amershi [cit] prove the need and practicability of collaborative search methods."
"for example, location-based presentation of threating visual stimuli can be used to study innate various types of defensive behaviors [cit] . stimulation triggered by head orienting towards goals has been used in brain-machine-interface assisted navigation [cit] . optogenetic stimulation triggered by social contacts has been applied to understand the rewarding system [cit] . and optogenetic stimulation triggered by food intake has been used to understand feeding circuits [cit] . moreover, online feedback control can provide a therapeutic strategy in disease management. for example, ankle position triggered optogenetic stimulation has been studied to potentially restore motor function in paralysis [cit] . neuronal activity dependent electrical stimulation has been used to suppress seizures [cit] ."
"in rtpp test, the roi is pre-defined and static. next, we demonstrate that the roi can be dynamic when applied in studying a social interaction behavior. in this behavioral test, two animals are placed in a long corridor but are separated by a transparent acrylic wall with holes in it so that visual and odor cues are not blocked. we defined the roi as the spatial location where the two animals were close to each other. we expressed chr2 in vta neurons (fig. 5a) in one of the two animals and this animal would receive led stimulation when the two animals were in a close distance ( fig. 5b, supplementary movie 2) . we found that over time the average distance between the two animals decreased (fig. 5c) . this result suggests that vta neuron activity promotes social interaction. thus, we demonstrate that track-control can be used when dynamic stimulation contexts are required."
"another important aspect for context-awareness is the use of hardware sensors integrated or attached to different device types. these sensors are capable of retrieving the orientation and acceleration of a device or capturing the movements of a user in 3d space. with that knowledge the system is able to make assumptions about the user's direct environment or to detect gestures, which further increases the overall context-awareness. many of today's mobile devices have accelerometers and gyroscopes integrated that can be accessed through devicespecific apis. html5 supports events that target those sensors and defines unified events in the specification for the deviceorientation event [cit] . desktop sensors like the kinect provide depth-information for tracking people in 3d space. these sensors do not yet have a common standard for capturing their data in a browser environment. for those sensors we have created a lightweight websocket-based [cit] abstraction library."
"track-control initiates and controls the video acquisition, and processes each frame streamed from a webcam (fig. 1) . in the meantime, the video file is stored to the hard drive of the computer while performing the experiment and the default output file format is .mp4. most brand webcams can satisfy the frame rate requirement for the majority of behavioral tasks (up to 30hz). here, we use the default frame rate of the webcam used."
"our goal is to develop an open source toolbox that can detect the position of the animal in real-time and send low-latency feedback control signals. we also intend to control cost so that each lab can build multiple setups to expand the throughput. after testing multiple methods, we found that using contrast and binarization would be a fast, reliable and low computational cost method."
"for instance, many labs are using real-time place preference (rtpp) test in which the optogenetic stimulation is controlled based on the spatial location of the animal, to reveal cell-type specific coding of valence in certain brain regions [cit] ."
"in order to create a visual platform for multimodal querying between user and search engine, the concept of musebag was developed. musebag stands for multimodal search bag and designates the i-search ui. it comes with specific requirements linked with the need for users to use multiple types of input: audio files or stream, video files, 3d objects, hand drawings, real-world information such as geolocation or time, image files, and of course, plain text. this part of the paper shows the approach chosen to create musebag."
"future work will focus on the following aspects: we will conduct more and broader user studies once the cofind component is up and running, and once the search engine delivers real results and not mocked-up results as in the current study. we will also focus on user-placable tags for search queries, which will allow for the tracking of search results changes over time. from the hardware side we will work on supporting more input device modalities such as gyroscopes and compasses that are more and more common standard in modern smartphones. one of the main results from the user study was that consistency of the different input modalities both from a treatment and usage point of view needs to be improved. we will thus focus on streamlining the usability of the product, guided by to-be-conducted so-called a/b or also multivariate tests. this will allow us to fine-tune the user interface while the i-search search engine is already in real-world use."
"in this section, we present our methodology for context-aware querying of multimodal search engines, split up in three sub-tasks: musebag for our multimodal query interfaces, uiiface for our multimodal interaction framework, and cofind for our collaborative search framework."
"the i-search gui is built using the web platform. html, css, and javascript are the three main building blocks for the interface. the rationale behind this choice is that i-search needs to be cross-browser and cross-device compatible, requirements fulfilled by css3 [cit], html5 [cit] and the therein defined new javascript apis that empower the browser in truly novel ways. however, our strategy also includes support for older browsers. when browsing the web, a significant part of users do not have access to a cutting-edge web browser. if a feature we use is not available for a certain browser version, two choices are available: either drop support for that feature if it is not important (e.g. drop visual shims like css shadows or border-radius), or provide alternate fallback solutions to mimic the experience. we would like to highlight that css and html are two standards that natively enable progressive enhancement thanks to a simple rule: when a web browser does not understand an html attribute, a css value or selector, it simply ignores it. this rule is the guarantee that we can build futureproof applications using css and html. web browsers render the application according to their capabilities: older browsers render basic markup and styles, while modern browsers render the application in its full glory. sometimes, how-ever, we have to ensure that all users can access a particular feature. in this case, we use the principle of graceful degradation, i.e. use fallback solutions when the technology stack does not support our needs in a certain browser."
"the i-search project needs to be compatible with a large range of devices: desktop browsers, phones, and tablets. rather than building several versions of i-search, we use css3 media queries [cit] to dynamically adapt the layout to different devices."
"to each animal. once the mouse entered the stimulation chamber, computer-controlled arduino microcontroller (www.arduino.cc) would generate ttl signals to drive the led light source (thorlabs inc.). the behavior test was run automatically without experimenter's interference and the result was calculated right after each experiment."
"multimodal search engines are still very experimental at the time of writing. when building musebag, we tried to look for a common pattern in search-related actions. indeed, musebag remains a search interface at its core. in order for users to interact efficiently with i-search, we needed a well-known interface paradigm. across the web, one pattern is used for almost any and all search related actions: the text field, where a user can focus, enter her query, and trigger subsequent search actions. from big web search engines such as google, yahoo!, or bing, to intranet search engines, the pattern stays the same. however, i-search cannot directly benefit from this broadly accepted pattern, as a multimodal search engine must accept a large number of query types at the same time: audio, video, 3d objects, sketches, etc. some search engines, even if they do not have the need for true multimodal querying, still do have the need to accept input that is not plain text. first, we consider tineye [cit] . tineye is a web-based search engine that allows for query by image content (qbic) in order to retrieve similar or related images. the interface is split in two distinct parts: one part is a text box to provide a link to a web-hosted image, while the second part allows for direct file upload ( figure 1 ). this interface is a good solution for a qbic search engine like tineye, however, the requirements for i-search are more complex."
"here we introduce the design and application of track-control, a free and opensource toolbox used in our published research [cit], which automatically detects spatial coordinates of the animal and sends feedback control to achieve optogenetic stimulation. we have further optimized and upgraded the track-control toolbox so that it can be applied in a broad spectrum of standard behavioral tests such as open field, elevated plus maze, morris water maze, social preference tests, etc. track-control can achieve realtime detection of triggering events (e.g. specific locations of the animal, static or dynamic) and output statistical results immediately after the behavioral test is complete."
"besides triggering sensory stimulation, track-control can be used to standardize other behavioral tests. here, we used a real-time place preference (rtpp) test to demonstrate the real-time feedback control using the track-control toolbox. dopaminergic neurons in the ventral tegmental area (vta) has long been known for its involvement in motivated behaviors [cit] . we tested our track-control system in recruiting dopaminergic neurons in the rtpp test. to stimulate dopaminergic neurons, we injected bilaterally adeno-associated virus encoding cre-dependent channelrhodopsin2 (aav-dio-chr2) into vta of dat-cre transgenic mice (fig. 4a) . two weeks following the virus injection, we implanted optic cannulas into the vta bilaterally. after one-week recovery, the animal was habituated to the optic fiber connection for 3 days."
"the idea of uiiface is based on the open interface framework [cit], which describes a framework for the development of multimodal input interface prototypes. it uses components that can represent different input modalities as well as user interfaces and other required software pieces in order to create and control a certain application. in contrast to this approach, uiiface is a web-based approach implemented on top of modern html5 [cit] functionalities. furthermore, it provides a command line interface to the web-based gui, which allows for the creation of stand-alone applications outside of the browser window. for the set of uni-and multimodal commands that can be used for i-search interfaces, the results of chang [cit] as well as the needs derived from the creation of multimodal search queries are used. figure 5 depicts the internal structure of uiiface and shows the flow of events. events are fired by the user's raw input. gesture interpreter determines defined gestures (e.g. zoom, rotate) found in the raw input. if no gestures were found, the basic interpreter routes touch and kinect 1 events to basic cursor and keyboard events. gestures, speech commands and basic mouse and keyboard events are then synchronized in the interaction manager and forwarded as combined events to the command mapper which maps the incoming events to the defined list of interaction commands that can be registered by any webbased gui. the command customizer can be used to rewrite the trigger event for commands to user specific gestures or other input sequences (e.g. keyboard shortcuts). this is an additional feature that is not crucial for the functionality of uiiface, but that can be implemented at a later stage in order to add more explicit personalization features."
"open field test. a white behavior test box (60cm x 60cm x 30cm, length x width x height) was virtually divided into a center region (center, 30 x 30 cm) and a periphery field. for each test, the mouse was placed in the periphery and the locomotion of the animal was recorded by a video camera for 20 min to measure the time spent in the center or peripheral area."
"the remainder of this paper is structured as follows: section 2 presents related work, section 3 introduces our chosen methodology, section 4 goes into implementation details, section 5 presents the evaluation of a user study that we have conducted, and finally section 6 ends the paper with an outlook on future work and provides a conclusion."
"we used the following coordinates for injection: vta: bregma -3.0 to -3.2 mm, lateral 1.25mm at 10 degree-angle, ventral 4.35mm [cit] . mice were anesthetized with 5% isoflurane and maintained using1.5-2% isoflurane. a small cut was made on the skin covering the craniotomy position and the muscles were removed. one attached to a micro syringe pump (world precision instruments). for pressure injection, 50 nl of the viral solution was injected at a rate of 15 nl/min. after the injection, the pipette was allowed to rest for 5 min before withdrawal. the scalp was then sutured. following the surgery, 0.1 mg/kg buprenorphine was injected subcutaneously before returning the animals to their home cages. mice were allowed to recover for two weeks. then, animals were anesthetized with isoflurane and an optic cannula (200 μ m core, na 0.22, thorlabs) was stereotaxically implanted into the target region, fixed with dental cement. the mice were allowed to recover for at least one week before behavioral tests. after each experiment, the brain was extracted, sectioned and imaged under a confocal microscope (fluoview fv1000, olympus) to confirm locations of viral expression and the implantation site. weeks before cannula implantation."
"the track-control toolbox can achieve real-time object detection and send low latency feedback control to another hardware through a usb port (fig. 1) . the toolbox is based on the opencv library (https://opencv.org), consisting of an object detection part and a feedback commanding part. from our own experience, one-for-all software may not be so convenient for behavioral labs, especially considering that one computer is often assigned to a specific type of behavioral test. thus, we decided to distribute an array of python modules, from which only a specific one is used for a particular behavioral test. the control settings and analysis methods have been optimized for each individual test, and the module is ready to use without further modifications."
"the html5 audio and video elements make multimedia content a first class citizen in the web browser, including scriptability, rotation, rescale, controls, css styles, and so forth. for i-search, this flexibility allows us to create interesting and interactive visualizations of search results. if audio and video are not available, we fall back to adobe flash [cit] to display media items to users."
"for enhancing the contrast between the object and the background, we suggest using a light color background (e.g. white) for pigmented animals (e.g. c57bl/6 mice), and dark color background for albino animals (e.g. frt fig. 2a), which has long been used for testing anxiety."
"session manager controls opening / closing of collaborative search sessions. content manager broadcast of user interfaces changes to all participants. messaging manager broadcast of status / user messages to all participants. the main flow of a collaborative search session can be described as follows: to join a collaborative search session initiated by a user a, a user b must supply the email address of user a. if user a is online and logged in, she receives an on-screen notification and needs to accept the collaboration request of the user b. upon acceptance, a new session entry is created that stores all participants. every time a change on the query input field or result set occurs, the changed state is transferred to all participants. each participant is able to search and navigate through the result set independently from the others, but selected results can be added to collaborative result set. the search session is closed after all users have left the session or have logged out from the system."
"as a second example, we examine mmretrieval [cit] . it brings image and text search together to compose a multimodal query. mmretrieval is a good showcase for the problem of designing a ui with many user-configurable options. for a user from outside the information retrieval field, the ui seems not necessarily clear in all detail, especially when field-specific terms are used ( figure 2 ). even if the search by image solution seems evident, it is still not suitable for i-search since the interface would require a high number of small icons: camera, 3d, geolocation, audio, video, etc. as a result, we decided to adapt a solution that can be seen in figure 4 . this interface keeps the idea of a single text box. it is enriched with text auto-completion as well as \"tokenization\". by the term \"tokenization\" we refer to the process of representing an item (picture, sound, etc.) with a token in the text field, as if it was part of the text query. we also keep the idea of progressive disclosure for the different actions required by the various modes, e.g. uploading a picture or sketching something. the different icons are grouped together in a separated menu, close to the main search field."
"the track-control toolbox is video-based and written in python programming language (compatible with python 2 and python 3). it works on windows, max os and linux system. due to low computational loads, track-control can run on a single core cpu computer for real-time object detection and feedback control. considering that most neuroscience labs have laser/led components for optogenetic stimulation or customized systems for sensory stimulation, the track-control toolbox can be easily integrated into existing laboratory setups to achieve automation. the source code and detailed tutorials can be accessed from https://github.com/guangwei-zhang/tracon-toolbox/."
the result shows that looming stimulation can robustly elicit an initial freezing (speed decreases to zero) and then a flight (speed rebound) response ( fig. 3b ).
"real-time place preference test. a clear acrylic behavior box (40cm x 20cm x 20cm, divided into two chambers, put in a larger white foam box) with normal bedding materials was used."
"cofind is based on the concept of shared search sessions in which html content of the participants' local clients is transmitted within this session. in order to realize collaborative querying, the concept provides functions for activating collaborative search sessions, joining other online users' search sessions and managing messaging between participants of the search session. figure 6 shows how the parts listed in the following interact during the search process in order to create a collaborative search session:"
"the pipeline for each frame is first to convert the rgb image to greyscale, by averaging the value of the three-color channels. then a 2d gaussian filter is applied to suppress high frequency noise in the image:"
"detailed wiki and step-by-step video tutorials are available on https://github.com/guangwei-zhang/tracon-toolbox. in brief, first download anaconda python 3.7 version and set the path as the system default path(https://www.anaconda.com/distribution/). then in the anaconda prompt command, install the opencv (https://opencv.org/) and pyserial module."
"based on the conflict theory, animals with higher levels of anxiety would spend more time in the peripheral (including corners) than the central region of the test arena where there is a higher risk of exposure to potential predators [cit] . thus, the percentage time spent in the central region is quantified. boolean value based on the spatial location of the animal for each frame is returned sequentially:"
"the looming stimuli is generated using psychtoolbox-3 ( http://psychtoolbox.org). the looming stimulation was constantly on as long as the animal was detected to be in the trigger zone. position of the monitor was adjusted to make sure that the visual angle was 75˚ (from the looming center) when the animal just crossed the boundary of the trigger zone. diameter of the dark disc changed from 2˚ to 40˚ within 250 ms, with a 250 ms interval between sequential presentations of discs."
"here, we demonstrate a 24-h recording and tracking with track-control without needs for any manual interference ( fig. 2f) . in principle, the recording length is only limited by the volume of hard drive storage. such chronic tracking can potentially be used to investigate the circadian rhythm ( fig. 2f), as an alternative to running-wheel assays [cit] ."
"in this paper, we have presented relevant related work in the fields of search engine interface design, multimodality in the context of search, and collaborative search. second, we have introduced our methodology with the concepts of musebag for multimodal query interfaces, uiiface for multimodal interaction handling, and cofind for collaborative search as the core components behind the i-search multimodal user interface, together with their implementation details. finally, we have briefly discussed first results of a user study on the i-search user interface."
"concluding, we feel that we are on a good track in the right direction towards an innovative multimodal search engine user interface design, however, have barely scratched the surface of what is still ahead. it is clear that our current user study can, at most, serve to detect overall trends, however, in order to retrieve statistically significant results we need to scale our tests to more users. given our team composition of both academia (university of applied sciences erfurt, centre for research & technology hellas) and industry (google), we are in an excellent position to tackle the challenges in front us."
"some video-based tracking with feedback control methods are not open source yet [cit] . in comparison, track-control is a free, open source and flexible toolbox that provides a strategy for other labs to quickly implement in automatic video-based behavioral experiments."
"to validate our interface design choices with real multimodal search tasks, we have conducted a user study. we went for a comparative study design to explore how usage of different media types would look like and how they would influence the success rate of search queries. as this user study was mainly focused on the user interface and user interaction parts of i-search, we assumed that the system always had a correct answer to the (limited) set of permitted queries, even if the real search back-end was not yet in operation at the time of writing. we therefore set the following hypotheses: (h1) most users will start a search query with just one media type. (h2) search refinements will be done by adding or removing other media types. (h3) all media types will be handled similarly."
social association test. a clear acrylic chamber (40x20x20cm) was equally separated into two corridors (40x10x20cm). the separating wall was transparent and with evenly distributed holes (3mm in diameter) in it so that the two animals placed in the two corridors could see and smell each other. the trigger zone was defined by the location of the object animal and would change its position accordingly. the test animal could freely explore the corridor but only received led stimulation when located in the trigger zone defined by the object animal.
elevated plus maze test. a cross maze with two closed and two open arms was elevated 30cm above the ground. the mouse was placed in the center of the cross maze and the locomotion of the animal was recorded by a video camera for 5 min.
"interaction is an important factor when it comes to context-awareness and multimodality. in order to deliver a graphical user interface (gui) that is able to facilitate all the possibilities of a multimodal search engine, a very flexible approach with a rich interaction methodology is needed. not only the way search queries are build should be multimodal, also the interaction to generate and navigate in such a multimodal interface should be multimodal. to target all those needs, we introduce the concept of uiiface (unified interaction interface) as general interaction layer for context-aware multimodal querying. uiiface describes a common interface between these interaction modalities and the graphical user interface (gui) of i-search by providing a general set of interaction commands for the interface. each input modality provides the implementation for parts of the commands or all commands defined by uiiface."
"morris water maze test. a circular pool (diameter:100cm) was used, with an invisible platform placed in one quadrant. water was dyed white using an odorless pigment. animal was placed in one quadrant. the test was stopped when the animal found the hidden platform."
"in recent years, noble metal complexes of pyridyl ligands have received much attention because of their rich electrochemical [cit], photophysical [cit] and photochemical [cit] properties, and their potential applications in catalysis [cit], biochemistry [cit] and anticancer activity [cit] . bipyridine (bipy) is one of the most commonly used bidentate ligand of this type in the formation of wide variety of transition metal complexes with a general formula of ni, mn, fe) in which x is an coordinated anionic ligand such as cn, scn and chloride [cit] [cit] . the complex [rhcl 2 (bipy) 2 ]cl.2h 2 [cit] . yet, no crystal structure has been reported for the cationic complex cis-[rh(bipy) 2 cl 2 ] + in its perchlorate form as counter anion, therefore, we report the crystal structure of compound (i)."
"complex (i) crystallizes in the orthorhombic space group p2 1 2 1 2 1 . the molecular structure of (i) depicted in figure 1 . it has a distorted octahedral geometry with the two chloride ions in cis positions. selected bond lengths for the complex are given in table 1 . the rh-n axial bond distance (2.038 (3) å) is slightly longer than rh-n equatorial bonds (average 2.026 (5) å). its may be well compared with the negligible difference between equatorial and axial m-n bonds distances seen in the analogous complexes of platinum metal group [cit], but greater distortion observed in majority of transition metal complexes [cit] . the rh-cl bond distances in (i) are 2.3291 (9) (equatorial) and 2.3344 (9) å (axial"
"for potential applications of noble metal complexes of pyridyl ligands in biochemistry, catalysis and anticancer activity, see: [cit] . for their photochemical and photophysical properties, see : [cit] and for their electrochemical properties, see: [cit] . for related structures, see: [cit] . for similar structures with platinum group metals, see: [cit] ."
"iii atom is coordinated by four n atoms from two bipyridyl ligands and two cl atoms, forming a distorted octahedral environment. the cl ligands are cis. two intramolecular c-há á ácl hydrogen bonds occur in the cationic complex . in the crystal, molecules are linked together by a hydrogen-bond network involving the h atoms of bipyridyl rings and perchlorate anions. an o atom of the perchlorate anion is disordered over two sites, with an occupancy-factor ratio of 0.78 (3):0.22 (3)."
"therefore, system convergence can be ensured by appropriately adjusting design parameters k3, k4, k5, τ3, and τ4. if k2, k3, k4, and k5 are increased while τ3 and τ4 are reduced, a sufficiently lar κ can be ensured so that filtering error and error surface are sufficiently small. this ensures cont accuracy."
"in this paper, we describe a further robotic model designed to look at aspects related to the emergence of compositional semantic structures in simulated agents. our results demonstrate how the agents, trained to execute several actions by responding to linguistic instructions, can generalize their linguistic and behavioral skills to never experienced instructions through the production of appropriate behaviors. the analysis of the best agents and the comparison of different experimental conditions, in which the representation of the linguistic instructions is the same but in which the behavioral set is varied, demonstrates how the emergence of compositional semantics is affected by the presence of behavioral regularities in the execution of different actions. [cit] -0604/$26.00 © 2011 ieee the behavioral and linguistic strategies used by agents equipped with compositional semantics to accomplish the task."
"in our experiments, we compare our method to three related methods, namely linear and non-linear svms for part templates [cit] and flexible mixtures-of-parts [cit] . we also compare our approach to two other state-of-the-art methods, namely pose-specific part appearance classifiers [cit] and spatial hierarchies of mixture models [cit] ."
"while previous work treats all body part templates independently and uses the pictorial structure framework to model spatial and orientation relations between part templates, we propose a more discriminative template representation that already takes co-occurrences and relations to other parts to some extent into account, as illustrated in fig. 1 . to this end, we train joint regressors that use the output of independent body part templates as input and thus predict the location of a joint in dependency of the cooccurrence of other body parts. in this way, joint regressors are already able to resolve some typical problems of tree models, such as the discrimination of left and right limbs."
we developed two real-time image guidance technologies for standard linear accelerators: (1) kilovoltage intrafraction monitoring (kim) that finds the target position in real-time during radiotherapy and (2) multileaf collimator (mlc) tracking that aligns the radiation beam to the moving target.
"for each experimental condition (with-indicate, and with-ignore), we run ten evolutionary simulations for 6000 generations, each using a different random initialization. recall that our objective is to generate agents that are capable of successfully accessing and executing experienced linguistic instructions. moreover, we are interested in investigating whether agents develop semantic structures that are functionally compositional. agents endowed with functionally compositional semantics should be able to successfully access and execute experienced linguistic instructions and to generalize their linguistic and behavioral skills to nonexperienced instructions (i.e., linguistic instructions never experienced during training). we run two different series of simulations to test whether a different training bears upon the development of the required mechanisms for compositional semantics. fig. 3 shows the fitness of the best agent at each generation of ten evolutionary runs per condition. all the curves reach a stable plateau with fitness either firmly fixed or progressing with small oscillation around the maximum score (i.e.,"
"experiments on fashionpose. for the training of the error thres. random forests for the body part templates, independent and parts dependent joint regression, we fixed some parameters intuitively. the patch size, and thus of the feature matrices f f p, is 30x30 pixels. each forest consists of 15 trees with maximum depth of 20 and a minimum number of 20 patches per leaf. for training, we generate 25,000 binary tests (7) at each node, where we use 1,000 random parameter settings for γ\\τ and for each setting additionally 25 random thresholds τ . each tree has been grown on a set of 500,000 positive and 500,000 negative patches extracted from 4,000 randomly selected training images. for computational reasons, we evaluate the split functions at each node for only maximal 200,000 patches."
"ecent research on action and language processing in humans and animals clearly demonstrates the strict interaction and codependence between language and action (e.g., [cit] ."
"the essence of multi-intercepto it is known from lemma 1 that v(t) can converge within finite time. that is, followers can converge to the state of the leader, achieving successful cooperative guidance and control for multiple interceptors."
"where: d and w are further referred to as the discriminant direction and the witness direction, respectively (in this particular case they are both trivial and equal to the diagonal of the unit hypercube in ℝ ℓ ), c d and w d are the orthogonal projections of c and w onto d, respectively, and \".\" signifies the scalar product of two vectors. hence we\"ve proved the following theorem:"
"from eq. 53, an error surface is defined as (53) where is the filtered total velocity of the interceptor. the derivative of sv is obtained as (eq. 54)"
"another research direction has focused on introducing richer body models that overcome the limitation of tree structures. for instance, a body part can be assigned with high confidence to two nodes of a tree in case of weak part templates or occlusions, e.g., the left and right body part are sometimes assigned to a single observation. to prevent this, additional constraints between the limbs [cit] or even a fully connected graphical model [cit] have been proposed. loopy models, however, make the inference more expensive and require approximations for inference."
"hence, it makes sense trying to implement software agents enabled to mimic the crisp theory (13), as close as possible, through a fuzzy iris recognition theory:"
"it can be seen in fig. 4 that the iiv balanced system proves to have a crisp understanding of what it means to be a genuine pair (or a genuine comparison code) for 85.19% of all genuine cases (which are scored with crisp unitary recognition score), a crisp understanding of what it means to be an imposter pair (or an imposter comparison code) for 77.64% of all imposter cases (scored with crisp null recognition score), a fuzzy understanding of what it means to be a genuine pair for 14.81% of all genuine cases, (with fuzzy unitary recognition score), a fuzzy understanding of what it means to be an imposter pair for 22.36% of all imposter cases (scored with fuzzy zero recognition score), a global f-consistent ( [cit] and complete understanding of iris recognition (being able to give the correct biometric decision for each enrollable pair), a huge safety band (a huge f-eer interval; see the statistics of all-to-all comparisons in fig. 4 ) reflecting the artificial consistent understanding of three concepts: \"genuine\", \"imposter\" and \"unenrollable\" (unsafe / uncertain) pair, i.e. the artificial fuzzy 3-valent disambiguated understanding of iris recognition."
"during the cognitivist era, compositionality was supposed to be underpinned by concatenative processes in which the tokens of an expression's constituents (and the sequential relations among them) are preserved in the expression itself [cit] . the difficulties shown by classic symbolic ai in accounting for general associations between semantic representations and sensory-motor profiles, and in particular in accounting for the acquisition of linguistic semantics through behavioral experiences, determined a paradigm shift in which an alternative perspective on compositionality emerged (see [cit] for a critical perspective on classic ai). in the last decade of the previous century, the connectionist approach to cognition proposed the idea of functional compositionality; that is compositional semantics systems in which the tokens of an expression's constituents (and the sequential relations among them) are not preserved in the expression itself [cit] . various connectionist models proved that artificial neural networks can be employed to physically instantiate functional compositional semantic structures [cit] ."
"where ζn and ωn are the damping and bandwidth of the filter, respectively. the use of the filter can effectively address the differentiation problem of the command signals without affecting the amplitudes of the commands and their derivatives."
"the interceptor igc model is a mismatched uncertain system; we designed a nonsingular fas dynamic surface smc model as the control algorithm for the leading interceptor, based on the igc model (eq. 3) and ftdo estimations (eqs. 5 to 7)."
"for the regression, a sampled patch p is additionally augmented with an offset vector v p,k pointing to the location of the corresponding joint j k . during training, the goodness (6) for evaluating the split functions is based on the sum-of-squared-distances; that is"
"when finding the derivative of the virtual control input, is fed through a first-order low-pass filter to obtain the filtered virtual control input (eq. 11)"
"where s is the reference area of the missile; m the missile mass; α is the angle of attack (aoa); ω z is the pitch angular velocity; q is the dynamic pressure; d α and d ω z are the perturbation and uncertain disturbance in each part of the system, respectively; j z is the rotational inertia of the missile; c xx/xx 04/18"
"the difference between the safety bands obtained through hbtdd and iiv balanced system is explicable in two ways: firstly, the training tools are more performant in the second case, and secondly, the learned prototype is accurate only in the second case. this illustrates two things: the importance of turing tests in artificial intelligence (it is the only way of correctly encoding human intelligence in numerical data) and the importance of the informed search. in other words, an informed search (an advanced artificial intelligence tool) targeting a correct prototype will always overcome a blind search targeting an incorrectly chosen prototype."
"each time when the recognition system negotiates between speed and accuracy, if a degree of imprecision is accepted as a counterbalance for gaining speed processing, fuzzification of the two classes of scores is guaranteed. the same situation occurs when the system is not endowed with suitable methods enabling successful recognition of the same iris captured in different acquisition conditions."
"to prevent singularity in the leading interceptor system and to converge to the equilibrium position within limited time, we design a nonsingular fast sliding mode reaching law (eq. 20): (19) to prevent singularity in the leading interceptor system and to converge to the equilibrium position within limited time, we design a nonsingular fast sliding mode reaching law (eq. 20):"
"the numerical results presented in fig. 2 and fig. 3 illustrate an incipient state of training the discriminant directions obtained through hbtdd after 4 iterations, with a safety band of 0.01 in width. it proves that iris code classification based on hamming similarity can be considered a particular case of iris codes classification using discriminant and witness directions."
"if a biometric system for personal use is detached from iiv balanced system and endowed with nearest neighbor based biometric decisional support, its safety band would be really wide having 0.4 in width (see fig. 5 )."
"hence, we came up to this point where we showed that \"state of the art\" in iris recognition means separating the imposter and genuine score distributions with a wide safety band, not just with the so called recognition threshold. still, our future works in this field depend almost exclusively on the socio-economical acceptability of this simple and evident truth."
"figs. 3 to 12. the trajectories of the interceptors and target obtained using the two strategies are shown in figs. 3 and 8. a comparison between the figures shows that when the improved strategy is used, follower trajectories gradually approach the target trajectory with a higher rate of convergence and smoother trajectory curves, and followers are able to hit the target by following the leader."
"based on the ftdo estimates and kinematic model in the pitch channel, the controller for following interceptors is designed using the dynamic surface smc law."
"in this section we aim to clarify the difference between an iris code and a digital identity. an iris code is a binary matrix that follows to be recognized (accepted or rejected) as being representative for an identity which is a symbolic or numeric data structure associated to a person. in the simplest case, an identity is a label -even it is encoded in a numeric vocabulary (like auto-number id fields). in a little bit more complex scenario, a digital identity is a numeric data structure obtained by detecting and extracting common features in a set of samples taken for the same individual, discriminant features between the sets of samples taken for different individuals, and by encoding all of these common and discriminant features in a numerical space. hence a digital identity is a memory that can be trained with iris codes in order to recognize them, or in other words, the digital identity is a recognizer object, whereas the iris code is a recognized object. a digital identity encodes more entropy than an iris code. as a matrix, it may share the same dimension with the iris code, but if this is the case, then the type of its components will be different (all components having longer binary representation). on the other hand, in a multi-enrollment scenario, the enrolled iris codes together define a digital identity (which therefore contains the same type of components as the iris codes contain, but has bigger dimension, [cit] )."
"the dataset is not only challenging due to the large variation of dressing style ranging from casual dresses and figure 4 . the accuracy plots for individual joints using body parts dependent joint regressors with a pictorial structure model. for better readability, we plot only the left joints. as expected, localizing the wrist is the most difficult task, whereas head, shoulders, and hip joints are reasonable well localized. the numbers for all joints at error thresholds 0.1 and 0.15 are provided in table 1. gowns to haute couture, but it also contains a large variation of poses. for evaluation, we grouped the dataset into a training set containing 6,543 images and a set of 1,000 testing images and rescaled all images to a common upper body size of 75 pixels, measured by the distance between the average position of the two hip joints and the average position of the two shoulder joints. the dataset is more challenging than the fashionista dataset [cit] that contains only 685 images. while the fashionista dataset has been proposed for parsing clothes and not for pose estimation, the fashionpose dataset can be also augmented with additional annotations for evaluating methods for parsing clothes in still images as well. the fashionpose dataset is publicly available."
"an improved distributed network cooperative igc algorithm is developed based on the leader-follower topology to address the multi-interceptor igc problem. the controller for the leading interceptor is designed based on an ftdo and the nonsingular fast dynamic surface smc law, whose stability is proved using the lyapunov principle. an improved multi-interceptor cooperative control strategy is proposed based on distributed network cooperative control. the following interceptor controller is similarly designed using an ftdo and dynamic surface smc. the algorithm is validated using simulations. it is demonstrated that the developed algorithm can meet the cooperative guidance and control requirements of multiple interceptors while increasing the rate of convergence for the interceptors that react to cooperative control commands."
"the successfully neural approaches to iris recognition are pretty rare indeed. it is somehow explicable because, even this subject is not present at all in scientific publications, ai community in general share a point of view according to which it is very hard to predict where a training procedure deviates from learning features (learning a concept) to learning specific data (memorizing specific instances of a concept). we don\"t share this view."
"a final point of minor significance is that generalization capabilities with respect to the move blue object instruction are more frequent than that with respect to the touch green object instruction. that is, for both conditions, the number of agents in subtask ii is significantly different from the number of agents in subtask iii (pairwise wilcoxon test with 99% confidence interval). although we have no empirical explanation for this finding, we know that the action move, which requires the agents to rotate both arms around their joints, is an action that, in evolutionary terms, appears earlier than the capability to touch an object, which requires the agents to stop rotating both arms. at the beginning of the evolution, when the agents' linguistic and behavioral skills are rather simple, it pays more to be able to rotate the arms in order to approach the target objects, rather than to be able to stop a not existing yet rotation of the arms. this evolutionary progression of the behavioral skills may explain why the nonexperienced instruction which requires to move a target object turns out to be more easily accessible and executable than the nonexperienced instruction that requires to touch a target object."
"we first evaluated the performance of the part templates (section 4.2), the independent joint regressors (section 4.3), and the body parts dependent joint regressors (section 4.4). the accuracy based on the normalized joint estimation error is given in fig. 3 . the proposed body parts dependent joint regressors clearly outperform the independent part templates and joint regressors. integrating them into a pictorial structure model (section 3), which encodes the kinematic skeleton, improves the accuracy further. the accuracy curves for individual joints are plotted in fig. 4 . we also evaluated the accuracy when the unary potentials for classification (9) and independent regression (11) are multiplied. in this case, the performance has not improved compared to the individual unary potentials. this shows that training the regressors depending on the body part templates (13) is essential for the performance gain."
"in spite of these limitations, these graphs tell us several important things. we first concentrate on the results of the action-transition test. fig. 4(a) indicates that the majority of fullycompositional agents evolved in condition with-indicate, relies on strategies in which the action-label does not influence the agents' behavior during the first phase of the task [see fig. 4(a), black bar on the left]. this suggests that the capability to neglect the action-label while searching for the target object is associated with the presence of compositional semantic structures, since it tends to be observed in fully-compositional agents. however, some of the partially-compositional and noncompositional agents in condition with-indicate proved also capable of accomplishing their task without failing in any transition of the action-transition test [see fig. 4(a), central and right black bars]. thus, the first conclusion we draw is that neglecting the action-label while reaching the target object is not sufficient to attain compositionality, since it does not allow those partially-compositional and noncompositional agents that possess it to access and execute nonexperienced instructions. fig. 4 (a) also shows that the capability to cope with the action-label change is completely absent in the agents evolved in condition with-ignore. this result seems to suggest that the significant differences, illustrated in the previous section, between the two evolutionary conditions in the generation of agents capable of accessing and executing nonexperienced linguistic instructions, could be explained by the fact that solutions based on the combination of independent elementary behaviors are more rare in the with-ignore condition. thus, we further conclude that the condition with-indicate seems to contain the evolutionary pressures that facilitate the emergence of compositionality by indirectly favoring those agents whose behavior is not influenced by the action-label while they reach the target object. fig. 4(b), which refers to the object-transition test, tell us that the capability to neglect the object-label during the second phase of a trial, when the target object is already within an agent's visual field, is completely absent in agents evolved in condition with-indicate, and in particular is completely absent in fully-compositional agents. only some of the partially-compositional and of the noncompositional agents evolved in condition with-ignore seem to be able to cope with the object-label change [see fig. 4(b), central and right white bars]. how do we explain these results? as far as it concerns the unexpected failure of the fully-compositional agents evolved in condition with-indicate, we found out that, contrary to what hypothesized by us, the agents use the object-label during the second phase of the task to keep the target object within their visual field. we observed that, when the object-label does not match what is visually perceived, fully-compositional, partially-compositional, and noncompositional agents perform a search behavior, loosing visual contact with the object indicated by the first given object-label. thus, the object-label influences the agents' behavior during both the first and second phase of a trial, by triggering the agents' response of searching and orienting toward the appropriate object. as far as it concerns the performance of the agents evolved in condition with-ignore, we think that their successes at the object-transition test can be explained by considering the evolutionary circumstances in which they evolved. in particular, the action ignore can be accomplished by executing a common act for all the objects. behavioral inspections have indeed demonstrated that partiallycompositional and noncompositional agents evolved in condition with-ignore and capable of coping with the object-label change, once required to ignore an object simply do not move at all from their position. this is a strategy which can be successfully applied to execute the action ignore regardless of the target object. this may have facilitated the emergence of mechanisms to be able to neglect the object-label while executing the required action. however, this is speculative and further analyses are required to test it."
"the results of our postevaluation analyses also suggests us that there are further distinctive operational principles underpinning compositionality, other than those considered in this work, that are most probably related to the structural and functional characteristics of the agents' neural controller. in future work, we will specifically target these principles, trying to provide a clear description of their nature. moreover, we mentioned that compositional agents tend to appear very rarely during evolution. it is our intention to work on the characteristics of the task to identify the elements that bear upon the evolutionary origins of agents equipped with compositional semantic structures. with respect to this issue, we think that it may be worth to vary linguistic features and behavioral aspects of the task. for example, in this simulation, the objects have fixed positions with respect to the agent (i.e., red object on the left, green object in front, and blue object on the right of the agent). we wonder whether the necessity to evolved more robust exploration strategies, induced by the variability of the object position relative to the agent, facilitates or hinders the development of compositional structures. moreover, we are interested in studying whether the use of more cognitively plausible coding schemes, in which the labels are perceived by the agent in a sequential order and just for a short interval of time, bears upon the emergence of compositional semantics. we are also interested in studying whether the development, during training, of a wider and more heterogeneous behavioral repertoire facilitates the emergence of more robust generalization capabilities."
"for both tests, the agents are evaluated 80 times (i.e., 80 trials) on each transition. in half of the trials, the agents are randomly initialize in the right, and in half of the trials, in the left initialization area. in each trial, an agent can either succeed, if at the end of the trial, or fail, if . following the logic of each test, the fitness components, and are updated with respect to the execution of the second given action-label on the current target object, in the action-transition test, and with respect to the execution of the current action-label on the first given target object, in the object-transition test. for both tests, an agent's overall performance on each specific transition is considered a success if the agent successfully executes the transition in more than 60 out of 80 trials (i.e., 75% success rate). since both tests are indiscriminately done on noncompositional, partially-compositional, and fully-compositional agents, we removed from the two sets of possible transitions, those which, assuming our hypothesis holds, require a response that noncompositional, and partially-compositional agents are not capable of performing. that is, we remove those transitions which require a move blue object, or a touch green object response. 4 fig . 4(a) and (b) show the results of the action-transition test and of the object-transition test, respectively. in both graphs, each bar indicates the percentage of agents that managed to obtain a success rate higher than 75% in all possible transitions of the corresponding test. black bars refer to the agents evolved in condition with-indicate, white bars refer to the agents evolved in condition with-ignore. before commenting the results, the reader should be aware of the following. these are quite severe tests since they demands a high success rate on part of the agents on each experienced transition. if our hypothesis on the mechanisms underpinning compositionality holds, we expect noncompositional and partially-compositional agents to be very bad at least in one of the experienced transitions. this is because we assume that the test can be successfully performed only by agents possessing the capability to functionally and temporally decompose the linguistic and behavioral task into two sequential phases, and that this capability can only be found in fully-compositional agents. however, the agents may not need to fully decompose every single trial into two sequential phases in order to be able to successfully access and execute nonexperienced instructions. in this sense, the test may demand more than what is required to be capable of behavioral and linguistic generalization. moreover, in these tests the agents' performance is influenced by whether the label change takes place exactly at the time when the agents switch the focus of their attention from the object-label to the action-label. for methodological convenience, we treated all the agents in the same way, by arbitrarily making this switch in a single time step randomly located in a 10 time steps interval that starts when the agents see the target object. nevertheless, this may not fully comply with each agent's own strategy, causing failure even in those agents that can functionally and temporally decompose the task."
"where and are, respectively, the initial (i.e., at ) and final (i.e., at the end of the trail ) angular distances between and the target object and is 1 if, 0 otherwise."
"in a perfect world, perfect processing tools would exist ensuring that the crisp human perception of iris recognition is perfectly replicable as a crisp artificial perception. in reality, the crisp human perception of iris recognition is expressed as a set of horn clauses. the world being far from perfect and the processing tools being imprecise enough, the artificial perception of the iris recognition fuzzifies the horn clauses into fuzzy if-then sugeno rules [cit] . it is a special kind of lossy compression that we would call semantic compression and which was previously named, less suggestive, as fuzzification. two actual distinct concepts -namely \"genuine\" and \"imposter\", represented in a space which is large enough to hold them distinct, are \"mirrored\" into a smaller space (are compressed, fuzzified) where their images are no longer distinct. hence the artificially perceived concepts (mirrored concepts) lost an important part of their original meaning, especially the meaning of their distinct individuality. it is clear now why we said that, in our case, fuzzification means a lossy compression of meaning."
"is the angular displacement of the orientation of recorded while, and is touching the target object by activating the touch sensor . (7) with corresponding to the angular position of with respect to . is computed only when the target object is falling within the visual field of the agent and in those trials in which the agent is required to touch or to move the target object. if the current linguistic instruction requires the agent to indicate an object and during the time of a trial in which the agent is not steps-on-target max-steps-on-target for or (6a) max-angular-offset (6b)"
"we saw that above, the discriminator directions are weak digital identities when their training relies on a fuzzy prototype recognition function and stationary learning rules. their weakness is reflected in the width of the safety band. hence, it is desirable to train robust digital identities able to ensure a wide safety band, a wide gap between the safe imposter and the safe genuine scores. therefore, our personal list of challenges [cit]"
"random forests [cit] or in general decision forests [cit] have been used for many classification or regression tasks, for instance, labeling body parts in depth images [cit], predicting the joint positions from depth data [cit], or localizing facial feature points [cit] . in this section, we describe the general training procedure and discuss the details regarding used features, split functions, etc. in the following sections."
"where ζn and ωn are the damping and bandwidth of the filter, respectively. the use of the filter can effectively address the differentiation problem of the command signals without affecting the amplitudes of the commands and their derivatives."
". and with, all the network connection weights, and are genetically specified networks' parameters. at each time step the angular movement of is degrees and of is degrees, where is the heaviside step function and sgn is the sign function. cell potentials are set to 0 when the network is initialized or reset, and (2) is integrated using the forward euler method with an integration time step ."
"fashionpose dataset. since clothing imposes a particular challenge for pose estimation in general, which is not well reflected in current datasets for pose estimation from still images, we collected a new dataset. the proposed dataset consists of 7,543 accurate annotated images downloaded from a variety of fashion blogs, e.g., lookbook. nu and kalei.do. each image contains a person where the full body is visible and is annotated by 12 joints and a point for the head, namely the nose. we did not annotate the head by the top of the head and the neck as in other datasets [cit] since these two points were very difficult to annotate accurately. occluded joints have also been annotated."
"for the trog 15.01 spark trial, in addition to routine departmental procedures, the contours and dose distributions for each patient's plan were independently reviewed. the kim and mlc tracking quality assurance processes were based on previous publications [cit] . system tests (repeated monthly) included coordinate system check, dynamic tracking accuracy, treatment interruption, latency measurement, dosimetric accuracy for standard delivery and kv panel offset correction with gantry angle. we also deployed software-based, patient-specific geometric and patient-specific dosimetric controls as a comprehensive quality assurance program applied pre-treatment, during treatment and post-treatment. the pre-treatment quality assurance included:"
"when put together, kim and mlc tracking enable real-time igart using a standard linear accelerator without any additional hardware. the purpose of this study was to clinically implement and investigate real-time igart using kim and mlc tracking."
"since neural network training is known to by very sensitive to noise, the segmentation procedure used here is designed to minimize the noise presence in the extracted samples by avoiding the chances that eyelids and eyelashes to escape undetected and unfiltered. on the other hand, neural network training is known to be more expensive when the iris codes grow bigger. for all of these reasons, we choose to work with just a quarter of the actual iris segment, as illustrated in fig. 1 . in this way, iris code dimension and noise presence are both kept to a minimum, whereas signal-to-noise ratio is maximized. circular fuzzy iris segmentation (cfis2, [cit] ) is used in order to extract the iris segment as a circular ring. a quarter of this circular ring ( fig. 1 ) is unwrapped and further encoded as a binary iris code using haar-hilbert encoder [cit] ."
"a simple generational genetic algorithm is employed to set the parameters of the networks [cit] . at generation 0, a random population of 100 vectors is generated by initializing each component of each vector to a value chosen uniformly random in the range [cit] . each vector comprises 84 real values (i.e., 75 connection weights, three decay constants, five bias term, and one gain factor shared by all the sensory neurons). hereafter, using terms derived from an analogy with biological systems, a vector is referred to as genotype and its components as genes."
"the interceptor igc model is a mismatched uncertain system; we designed a nonsingular fast dynamic surface smc model as the control algorithm for the leading interceptor, based on the igc model (eq. 3) and ftdo estimations (eqs. 5 to 7)."
"similarly, the disturbances of the leading interceptor in the aoa loop and pitch angular velocity loop, dα and, can be estimated by (eqs. 6 and 7)"
"for evaluation we use two datasets, namely the wellknown leeds sports pose dataset (lsp [cit] ) and a newly figure 3 . comparison of the joint localization accuracy of the proposed unary potentials and comparison with a state-of-the-art method [cit] . while the body part classification (9) and the independent joint regression (11) perform similarly, they are drastically outperformed by the proposed body parts dependent joint regressors (13) . since the body parts dependent joint regressors do not encode any explicit information of the human skeleton, using a pictorial structure model (ps), which models the kinematic chain, gives an additional performance boost. the body parts dependent joint regression together with a pictorial structure model outperforms [cit] . in particular at low error rates like 0.1, the number of correctly localized joints is 20% higher than [cit] ."
"touch and move require the agent to rotate and until collides with the target object. touch requires an agent to remain in contact with the target object with the right side of (that is, by activating the touch sensor ) for an uninterrupted interval of 100 time steps. during this interval, must not rotate. move requires an agent to rotate more than 35 while is touching the object with its right side. the rotation of while is touching the object determines the movement of the object. indicate requires an agent to rotate until the angular distance between and the object is less than 30 . indicate is correctly executed only if remains at less than 30 from the target object for more than 100 time steps. ignore requires the agent to look at anything except the target object. the agent has to move away from positions in which the target object falls within its visual field. during the execution of indicate and ignore, an agent must not collide with any object. during the execution of touch and move, an agent must not collide with the nontarget objects (i.e., the objects not mentioned in the current linguistic instruction)."
"overall, these tests indicate that in fully-compositional agents obtained in condition with-indicate, the \"indicate red object,\" \"indicate blue object,\" and \"indicate green object\" behaviors are executed during the entire trial, as demonstrated by the fact that the agents are able to search for a new object and then keep indicating it when the object-label is modified during the second phase of the trial. the execution of the \"indicate\" behavior during the second phase of the trial is not apparent in normal condition (i.e., when the position or the color of the objects do not change) simply because the execution of this behavior do not produce any movement. thus, during the second phase of the trial, when the action label is \"indicate,\" agents keep producing the same behavior. when the action label is \"touch\" or \"move,\" the agents perform the corresponding elementary behavior that operates in parallel with the \"indicate\" behavior. the key mechanism that enables compositionality, therefore, is the fact that the action-label does not affect the agents behavior during the first part of the trial. in other words, \"touch\" and \"move\" behaviors con-stitute independent behavioral units realized through the execution of the same sequence of micro-actions irrespectively from the object-label. moreover, we can now state that a different training bears upon the development of the required mechanisms for compositional semantics, and that condition withindicate facilitates the emergence of compositionality by favoring the emergence of the functional independence of the action-label from the behavioral experience of searching for the target object."
"this paper showed that, in some conditions, by using tools of artificial intelligence, the memory of distinct individuality can be partially reconstructed / recovered / rediscovered (trained) from a number of compressed samples which taken together as a whole (not individually) host a hidden and apparently lost meaning of the original data."
"). there are runs in which the agents reach the maximum fitness very quickly (e.g., run n 1 condition with-indicate, or in run n 2 condition with-ignore) other in which it takes longer (e.g., run n 4 condition with-indicate, or in run n 3 condition with-ignore). for all the runs, before reaching the last fitness plateau, we have periods of very rapid fitness growth induced by the acquisition of new skills to access and execute either entire linguistic instructions or just single linguistic labels. these periods are always followed by either long or short fitness plateaus characterized by rather small oscillations. just by looking at the fitness curves, we can say that, at the end of the simulation, most of the best agents in both conditions looked capable of correctly solving the linguistic task. however, to estimate the effectiveness and robustness of some of the best evolved agents, with respect to the initial position of the arm, we postevaluated them for a larger number of trials."
"patients were treated on a varian trilogy linac with millennium mlc. positioning was verified with cbct to align fiducials and cross-checked with ctv and ptv structure overlay. framegrabber hardware cables and acquisition software (varian itools) were used to acquire kv and mv images during treatment. the images were streamed to a research computer on which the kim and mlc tracking programs were installed. the research computer was integrated into the linac intranet to enable mlc positions and beam holds to be sent from the mlc tracking software to the linac. the kim software was activated following patient alignment and preceding treatment delivery, requiring the patient's implanted marker positions determined from the treatment plan to be loaded and acquisition of kv fluoroscopy during a 120°imag-ing only arc to populate the kim probability density function [cit] . the mlc tracking software was activated with the mlc positions as a function of gantry angle and monitor unit obtained by reading the dicom rt plan. treatment was delivered with kv fluoroscopy (125 kvp, 80 ma, 13 ms, 6 â 6 cm 2, 10 hz). the estimated additional kv dose from the kim procedure is 0.4 gy [cit] . a gating threshold of 1 cm was applied. following treatment a second cbct was acquired according to the spark protocol."
the derivative of s2 is determined to obtain the error dynamic equation (9) the ftdo-estimated from eq. 5 is substituted into eq. 9 to acquire the virtual control 2 1 2 23 23 2 2 23 22 2 2 3 ( ) ( )
6 is substituted into eq. 14 to acquire the virtual control input for the second dynamic surface (eq. * * * * * 3 3 3 3 3 3
"the problem of the cooperative control of multiple intelligent agents can be described as a graph, which can be analyzed using graph theory. for the guidance and control of multiple interceptor missiles, one missile acquires the state information of other missiles through information exchange to achieve coordination among them. therefore, a leader-follower topological structure that consists of one leading interceptor and n following interceptors can be built. this structure is described using an undirected graph. regarding each missile as a communication node, the information exchange between missiles is expressed as"
subtask iii takes into account the seven scores recorded during the execution of the experienced linguistic instructions plus the score recorded during the execution of the nonexperienced linguistic instruction touch green object.
"the interceptor igc model is a mismatched uncertain system; we designed a nonsingular fas dynamic surface smc model as the control algorithm for the leading interceptor, based on the igc model (eq. 3) and ftdo estimations (eqs. 5 to 7)."
"to validate the effectiveness of the improved distributed cooperative igc algorithm, we assume a communication topology in which the leader can communicate with three other followers and neighboring communication exists between the followers, as shown in fig. 2 . the initial conditions of the leader, followers, and target are listed in table 1 ."
"it is known from lemma 1 that v(t) can converge within finite time. that is, followers can converge to the state of the leader, achieving successful cooperative guidance and control for multiple interceptors."
"the comparison between two experimental conditions, in one of which the action-label indicate is substituted with the action-label ignore, shows that the composition of the behavioral set significantly influences the development of solutions that generalize to nonexperienced instructions. only individuals evolved in condition with-indicate are characterized by a particularly successful linguistic and behavioral organization based on the decomposition of the task into two phases, each of which can be associated with the execution of an elementary behavior. in the first phase only the object-label bears upon the agents' behavior by triggering the object search strategy. in the second phase, both the object-label and the action-label determine the agents' response. in particular, the object-label keeps an agent eliciting the same behavior produced during the first phase (i.e., the agent keeps on searching/pointing the target object with the first segment of its arm). at the same time, the action-label triggers a different behavior that consists in bending the second segment of the arm so to touch or move the object. the capability to decompose the task into two sequential phases as described above, and the use of elementary behaviors employed in different circumstances, are features that, although not sufficient per se to explain compositional semantics, they certainly facilitate its evolution."
"estimating the human pose from still images is a very active field due to its relevance for applications [cit] . one of the most popular approaches in this area is the pictorial structure framework [cit], which models the spatial relations of rigid parts using usually a tree model. pictorial structures have been improved for pose estimation in many ways, e.g., by learning better appearance [cit] or shape models [cit] of the body parts."
"visual inspection of segmentation and that the reported motion corresponded to segmented positions relative to planned positions. software controlled measures (inside kim software) leading to beam hold interlocks on the linear accelerator, including: loss of communication between kim, mlc tracking or mlc controller; detection of motion outside tracking zone; reduction of correlation below a threshold (to detect migration, or segmentation error); change in inter-marker distances (to detect deformation, segmentation error, or 2d ? 3d conversion error); acceleration of centroid over a threshold value (to detect 2d ? 3d conversion error)."
"it is known from lemma 1 that v(t) can converge within finite time. that is, followers can converge to the state of the leader, achieving successful cooperative guidance and control for multiple interceptors."
"the essence of multi-interceptor cooperative attack is to coordinate the positions of following interceptors and the leading interceptor. therefore, to realize the distributed network synchronization strategy, each interceptor in the network must follow the velocity commands provided by the synchronization strategy given by eq. 42."
"where τ4 is the time constant of the filter. hence, the derivative of the virtual control input after error surface filtering is (eq. 17) is fed through a low-pass filter to obtain (eq. 16)"
the results of our method using body parts dependent joint regression with a pictorial structure are given in table 2. the comparison with a pictorial structure model that uses linear svms [cit] or a cascade of non-linear svms [cit] as part templates shows that our proposed unary potentials achieve a much higher accuracy. the accuracy with respect to the normalized joint localization error for individual joints is plotted in fig. 5 .
"for each subtask, the agents are allocated to one of eleven possible categories (from to ). for a given subtask, an agent is assigned to with, if its lowest score among those considered for that particular subtask, is within the interval ( ]. comprises all agents that failed to complete a single trial out of 80 attempts on at least one of the instructions. the higher the category, the better the overall performance of the agent. for example, subsumes those agents for whom the lowest score among those considered in a given subtask is within the interval (40, 48] ."
"it can be seen that the commands provided by the cooperative control strategy can be converted to velocity and flight path angle commands. the controller for following interceptors adopts the dynamic surface smc algorithm to achieve command signal tracking for the missiles in the cooperative network. assuming that missile velocities are controllable and air resistance and gravity can be neglected, the flight velocity of a follower can be expressed as (eq. 52):"
"let us consider the simplest case of two unmasked iris codes ic 1, ic 2 of the same dimension w h. their hamming similarity score (the complement of hamming distance relative to the unitary score) is:"
"collected dataset that we call fashionpose. while the lsp dataset contains a high variation of poses, the variation of appearance and dress style within each of the eight sport classes is rather small. we have therefore collected a new dataset that has very high variation in cloth and appearance."
"determined to obtain (eq. 45) (45) considering v(e) ≠ 0, eq. 45 can be rewritten as (46) from eqs. 46 to 47, (47) it is known from lemma 1 that v(t) can converge within finite time. that is, followers can converge to the state of the leader, achieving successful cooperative guidance and control for multiple interceptors."
"the essence of multi-interceptor cooperative attack is to coordinate the positions of following interceptors and the leading interceptor. therefore, to realize the distributed network synchronization strategy, each interceptor in the network must follow the velocity commands"
"what kind of operational principles do agents employ to be able to access and execute both experienced and nonexperienced instructions? what are the mechanisms underpinning compositional semantics? by visually inspecting the behavior of some of the agents, we notice that, contrary to the behavior of the agents evolved in condition with-ignore, the behavior of compositional agents evolved in condition with-indicate is the result of the combination of two types of elementary behavior: an \"indicate red object\" or \"indicate green object,\" or \"indicate blue object\" behavior produced during the first phase of the trial, eventually followed by a \"touch\" or \"move\" behavior, in the second phase of the trial. during the first phase of the trial, regardless of the action to be performed on the object, the agents search the target object by rotating in order to reduce the angular distance between the target object and, keeping bent as at start until the target object falls into the agent visual field. during the second phase of the trial, regardless of the target object, the agents rotate without moving if touch is required. they rotate until this segment collides with the target object and then they start rotating again if move is required. they keep pointing to the object and fully bent as at start if indicate is required. this qualitative analysis of the behavior of compositional agents suggests that the agents have developed behavioral skills that, being independent from the particular nature of the linguistic instructions in which they are employed, can be used in contexts already experienced as well as in context not experienced during training."
"the results obtained by training discriminant directions (as robust digital identities) on iiv infrastructure [cit] are illustrated in fig. 4 and fig. 5 . the new recognition system obtained in this way is an iiv balanced system based on learning robust discriminant and witness directions. it is \"balanced\" because the values of genuine and imposter absolute safety rates (77.64% vs. 85.19%) are more balanced than the values obtained in the previous iiv simulations [cit] ."
"it can be seen that the commands provided by the cooperative control strategy can be converted to velocity and flight path angle commands. the controller for following interceptors adopts the dynamic surface smc algorithm to achieve command signal tracking for the missiles in the cooperative network. assuming that missile velocities are controllable and air resistance and gravity can be neglected, the flight velocity of a follower can be expressed as (eq. 52) (52) where pi is engine thrust. from eq. 53, an error surface is defined as (53) where is the filtered total velocity of the interceptor. the derivative of sv is obtained as (eq. 54)"
"based on eqs. 19 and 20 and the ftdo-estimated from eq. 7, the improved nonsingular fast dynamic surface smc law of the leading interceptor is (eq. 21)"
"the previous part potentials are calculated independently. that is, during both training and evaluation, each sampled patch is evaluated without taking its spatially surrounding potentials into account. for the task of joint localization, this can result in ambiguities, e.g., for left and right knees as illustrated in fig. 1 . to resolve this issue, we propose a third potential that predicts the joint locations as in (11), but also takes neighboring part potentials into account:"
"with the advancement of anti-missile technology, cooperative multi-missile attack and defense is attracting increasing attention owing to its unique strengths. as a result, the development of cooperative multi-missile guidance and control technology, which is a key element for ensuring the attack and defense performance of a weapon system, has gained momentum. th rough coordination between missiles, cooperative engagement integrates multiple interceptor missiles as a united combat group that is informationsharing, function-complementary, and tactics-cooperative. using the group advantage, a multi-missile system can execute a multilayer all-around attack on an enemy's defense system or a target with overall-promoted penetration capabilities and carries out tasks that are diffi cult for a single interceptor missile to perform. th erefore, it is of practical signifi cance to study the cooperative guidance and control of multiple interceptors [cit] ."
"when finding the derivative of the virtual control input, is fed through a first-order low-pass filter to obtain the filtered virtual control input (eq. 11)"
"where l is the set of body parts. in this work, we use the term 'joint' for any landmark point like a skeleton joint or the nose, whereas 'body parts' are defined as regions around the joints as illustrated fig. 1 . as regressors, we use random forests [cit] . for completeness, we give a brief introduction to random forests in section 4.1. in sections 4.2, 4.3, and 4.4, we discuss three variations, namely part templates using random forests, independent joint regressors, and parts dependent joint regressors."
"during evolution, each genotype is translated into an arm controller and evaluated more than once for all the object-action experienced instructions by varying the starting positions. the agents perceive experienced instructions and they are required to execute the corresponding behaviors. agents are evaluated 14 times initialized in the left and 14 times in the right initialization area, for a total of 28 trials. for each initialization area, an agent experiences all the experienced linguistic instructions twice. the nonexperienced linguistic instructions and are never experienced during the training phase. at the beginning of each trial, the agent is randomly initialized in one of the two initialization area, and the state of the neural controller is reset. a trial lasts 25 simulated seconds ( time steps). a trial is terminated earlier in case the arm collides with a non target object. in each trial, an agent is rewarded by an evaluation function which seeks to assess its ability to execute the desired action on the target object."
"in the first postevaluation test, the best five agents of each generation, from generation 4000 to generation 6000, of each evolutionary run in both conditions, have been repeatedly postevaluated in each experienced and nonexperienced linguistic instruction. we decided to test the best five agents instead of the best one of each generation, because, during evolution, the agents have been ranked according to their fitness, which does not take into account the agent capability to access and execute nonexperienced linguistic instructions. recall that nonexperienced linguistic instructions have not been presented during evolution. thus, with respect to the capability to access and execute nonexperienced linguistic instructions, the best agent of each generation may not represent the most effective solution that appeared at each evolutionary time. overall, 100 000 agents per condition have been postevaluated (i.e., [cit] generations, times 10 runs)."
"during this postevaluation test, each agent is required to execute 80 times each of the nine instructions [40 trials with the agents randomly initialized in the right initialization area and, 40 trials in the left one, see also fig. 1(b) ]. the position of the objects is also randomly varied as explained in section iii. in each trial, an agent can be either successful or unsuccessful. it is successful if, otherwise it is unsuccessful (see (4), section vi for details on ). at the end of the postevaluation test, an agent capability to solve the linguistic and behavioral task is represented by nine scores, one for each linguistic instruction. recall that each score ranges from 0 to 80, and it represents the number of times an agent is successful at the execution of the corresponding linguistic instruction."
the clinical implementation of real-time image-guided adaptive radiotherapy on a standard linear accelerator using kim and mlc tracking is feasible. this achievement paves the way for real-time igart to be a mainstream treatment option.
"it can be seen that the commands provided by the cooperative control strategy can be converted to velocity and flight path angle commands. the controller for following interceptors adopts the dynamic surface smc algorithm to achieve command signal tracking for the missiles in the cooperative network. assuming that missile velocities are controllable and air resistance and gravity can be neglected, the flight velocity of a follower can be expressed as (eq. 52) (52) where pi is engine thrust."
"the velocity curves of the interceptors obtained using the two strategies are shown in figs. 4 and 9. convergence is reached at 6-8 s using the improved strategy, whereas it is reached at 8-10 s using the regular strategy. comparisons between figs. 5 and 10, 6 and 11, and 7 and 12 suggest that the improved strategy proposed in this study provides a higher rate of convergence and smoother transition with strong robustness."
"therefore, system convergence can be ensured by appropriately adjusting design parameters k 2, k 3, k 4, k 5, τ 3, and τ 4 . if k 2, k 3, k 4, and k 5 are increased while τ 3 and τ 4 are reduced, a sufficiently large κ can be ensured so that filtering error and error surface are sufficiently small. this ensures control accuracy."
"by the term \"compositional semantics,\" we refer to a functional dependence of the meaning of an expression on the meaning of its parts. compositional semantics in natural language refers to the human ability to understand the meaning of spoken or written sentences from the meaning of their parts, and the way in which these parts are put together. for example, the meaning of an unknown sentence like \"susan likes tulips\" can be understood by learning the following three sentences: \"julie likes daisies,\" \"julie likes tulips,\" and \"susan likes daisies.\" in this example, the meaning of the original sentence is achieved through compositional semantics by generalizing the meaning of single words from a known (already learned) to an unknown (yet to be learned) context."
"to validate the effectiveness of the improved distributed cooperative assume a communication topology in which the leader can communicate with and neighboring communication exists between the followers, as shown i conditions of the leader, followers, and target are listed in table 1 . the derivate of s i2 is determined (eq. 64):"
"in object detection, one of the best performing methods relies on so called deformable part models [cit], which use mixtures of star models over templates of parts. recently, [cit] showed that mixtures of part templates can also be efficiently used in a tree model, leading to very powerful pose estimation models. in particular, instead of modeling the transformations of a single body part template as in the classical pictorial structure model, the transformations of the"
"the interceptor igc model is a mismatched uncertain system; we designed a nonsingular fast dynamic surface smc model as the control algorithm for the leading interceptor, based on the igc model (eq. 3) and ftdo estimations (eqs. 5 to 7)."
"the agent controller is composed of a continuous time recurrent neural network (ctrnn) of 18 sensor neurons, three interneurons, and four motor neurons [cit] . at each time step sensor neurons are activated using an input vector with corresponding to the sensors readings. in particular, and are the readings of touch sensors and, respectively; to are the readings of the camera sensors; is refers to the normalized angular position of with respect to (i.e., ); to are the linguistic input and their value depend on the current linguistic instruction."
"therefore, system convergence can be ensured by appropriately adjusting design parameters k2, k3, k4, k5, τ3, and τ4. if k2, k3, k4, and k5 are increased while τ3 and τ4 are reduced, a sufficiently large κ can be ensured so that filtering error and error surface are sufficiently small. this ensures control accuracy."
"in a multi-missile topology with a leading interceptor, the leading interceptor features an independent state that does not change with followers. the purpose of including the leader state into the synchronization algorithm as part of the cooperative control strategy is that follower states should approach the leader state."
"the eye image database [cit] was split into two parts: the training set -containing 5 samples per iris, and the test set which contains 15 samples per iris or less (14) in the cases of failed segmentation (there are 3 cases of failed segmentation between all 1000 images of the database). then robust discriminant directions have been learned as double matrices on iiv infrastructure from binary iris codes of dimension 64 64 extracted as in fig. 1 . fig. 4 and fig. 5 together illustrate what we call intelligent, consistent and logically argued / motivated biometric safety."
"the action-transition test and the object-transition test have been run on a pool of agents selected on their results at the first postevaluation test (see section vii.a). in particular, for each evolutionary condition (i.e., with-indicate, and with-ignore), we chose the agents that proved to be more than 75% successful at executing each experienced instruction. for the purposes of these tests, these agents have been further selected, and the following three categories have been created: 1) noncompositional agents, referring to those agents that, at the first postevaluation test, proved to be less than 10% successful at executing each of the nonexperienced instructions; 2) partially-compositional agents, referring to those agents that, at the first postevaluation test, proved to be more than 75% successful at executing only one of the two nonexperienced instructions, and less than 10% successful at executing the other nonexperienced instructions; and 3) fully-compositional agents, referring to those agents that, at the first postevaluation test, proved to be more than 75% successful at executing each of the nonexperienced instructions."
"experiments on leeds sports pose dataset. due to the small size of the lsp dataset [cit], we trained only 10 trees using 100,000 positive and 100,000 negative patches sampled from the 1,000 training images. the other parameters are the same used for the fashionpose dataset. in order to compare with previous works, we use the pcp criteria. to this end, we added the neck and the top of the head as joints and converted our joint representation into a limb representation by using the joints as endpoints of the limbs. the torso is obtained by the line between the average position of the two hip joints and the average position of the two shoulder joints."
"determined to obtain (eq. 45) (45) considering v(e) ≠ 0, eq. 45 can be rewritten as (46) from eqs. 46 to 47, (47) it is known from lemma 1 that v(t) can converge within finite time. that is, followers converge to the state of the leader, achieving successful cooperative guidance and contro multiple interceptors."
"a joint representation as in (1) has the advantage that limb transformations like foreshortening do not need to be explicitly modeled in the pictorial structure model, which reduces complexity and running time. the independence assumption of common part templates is relaxed by training the regressors on image features and confidence maps of other body parts, i.e.,"
"it can be seen in fig. 2 that for any enrolled iris code, the ensemble formed by the corresponding discriminant and witness directions act like a lens through which the iris code see its farthest friend (the farthest iris code from the same personal cluster, with which it forms the lowest scored genuine pair) as being closer to him than the nearest enemy (the nearest iris code from any different personal cluster, with which it forms the highest scored imposter pair)."
"the agent has means to perceive whenever reaches the right or the left bound through the activation of the camera sensors. that is, when reaches the right bound, and are set to 1 (i.e., the first camera sector perceives a white background). when reaches the right bound, and are set to 1 (i.e., the third camera sector perceives a white background). finally, two binary touch sensors (i.e., ) are placed on the right, and left side of . collisions between the agent and an object are handled by a simple model in which whenever pushes the object the relative contact points remain fixed."
"our study shows that behavioral and linguistic competences can coevolve in a single nonmodularized neural structure in which the semantics is fully grounded in the sensory-motor capabilities of the agents and fully integrated with the neural mechanisms that underpin the agent's behavioral repertoire. owe to the use of artificial evolution, we leave the agents free to determine how to achieve the goals associated to each linguistic instruction. this allows the agents to oreganos their behavioral skills in ways that facilitate the development of compositionality thus enabling the possibility to display a generalization ability at the level of behaviors (i.e., the ability to spontaneously produce behaviors in circumstances that have not been encountered or rewarded during training)."
"evaluation measurement. in our experiments, we measure the joint localization error as a fraction of the upper body size. this measurement is well established for other computer vision tasks, e.g., fiducial point detection. it is independent of the actual size of the image and more precise than common measures derived from bounding box-based object detection like pcp [cit] . pcp declares a limb as correctly detected if the error of the predicted endpoints are within 50% of the limb length from the ground truth endpoints. we use the imprecise pcp measure only for comparison with other reported results on the leeds sports pose dataset; otherwise we use the more informative normalized joint localization error."
"let us recall that p c is a conjunction of prerequisite conditions regarding image acquisition and all image processing steps that must be undertaken in order to generate iris codes, c is a comparison code, s(c) is a similarity score, i and g are the sets of imposter and genuine comparison codes respectively, and ⨂ denotes logical exclusive disjunction. above in this paper there are two variants of formal iris recognition theories that have been discussed already, namely:"
"it is worth noting that, the results of this test gave us a rather heterogeneous picture, with performances that, even for a single agent, vary remarkably from one linguistic instruction to the other. we felt that readings and interpreting these data by only concentrating on general trends, it would have significantly impoverished the message or this research work. therefore, we chose a way of representing the results which gives the reader a coherent and exhaustive, although a bit articulated, synthesis of what the postevaluated agents are capable of doing at the linguistic task. in particular, for each condition, the performances of the agents are compared with respect to four different subtasks. for each subtask, the comparison were accomplished by grouping the 100 000 agents in eleven different categories. we first describe what the subtasks are and then we explain the meaning of each category."
"the essence of multi-interceptor cooperative attack is to coordinate the positions of followin interceptors and the leading interceptor. therefore, to realize the distributed networ synchronization strategy, each interceptor in the network must follow the velocity command"
"based on the ftdo estimates and kinematic model in the pitch channel, the controller for following interceptors is designed using the dynamic surface smc law."
"in this paper, we described a robotic model that allows a simulated robot to interact with three colored objects (a red, a green, and a blue object) located in its peripersonal space by executing three actions (indicate, touch, and move) during a series of trials. in each trial, the agent receives as input a linguistic instruction and is rewarded for the ability to exhibit the corresponding behavior. the results of this study show that dynamical neural networks designed by artificial evolution allow the robot to access and correctly execute the linguistic instructions formed by all the possible combinations of the three action-labels and the three object-labels with the exception of the instructions \"touch green object\" and \"move red object,\" which are nonexperienced during training. postevaluation tests showed that some of the evolved agents generalize their linguistic and behavioral skills by responding to the two nonexperienced instructions with the production of the appropriate behaviors."
"obviously, it is important to emphasize the fact that the evolutionary conditions detailed in previous sections, and in particular those in condition with-indicate, generate the neural mechanisms required by the agents to go beyond what was experienced during evolution. nevertheless, the fact remains that in either condition, the agents capable of generalizing their skills are only a tiny fraction of the agents capable of successfully accomplishing the evolutionary task. this can be explained by the fact that: 1) evolution only seldom produced agents fully capable of generalizing their skills; and 2) the selective process does not differentiate between compositional and noncompositional agents since they tend to produce equally good performance with respect to the conditions in which they are evaluated. we noticed that agents capable of generalizing appear only in six runs out of ten, and they are never more than one or two per generation. 2 when they appear, they generally have the highest fitness recorded at that particular generation, which almost always is the highest possible fitness. however, they tend to appear when there are already many more agents with the same fitness in the population that are nevertheless not capable of generalizing their linguistic and behavioral skills to nonexperienced linguistic instructions. the selection mechanism, which can not distinguish on the basis of the fitness alone, agents capable of generalizing from those not capable of generalizing, tends to favor the latter, to the detriment of the former, simply because the latter are more frequent in the population."
"therefore, system convergence can be ensured by appropriately adjusting design parameters k k3, k4, k5, τ3, and τ4. if k2, k3, k4, and k5 are increased while τ3 and τ4 are reduced, a sufficiently larg κ can be ensured so that filtering error and error surface are sufficiently small. this ensures contro accuracy."
"parts dependent joint regressors figure 1 . the dark gray rectangle on the l.h.s. illustrates a pictorial structure (ps) model with independent part templates. each classifier estimates independently the probability that an image region belongs to a specific body part, e.g., head (red), right hip region (blue), and right knee region (green). the confidence maps are used as unary potentials for a ps model with 13 joints. neither the independent classifiers nor the tree structure of the ps model are able to resolve the ambiguities between the left and right leg. the light gray rectangle on the r.h.s. illustrates the proposed approach where two layers are used. while the first layer consists of the same independent classifiers, the second layer regresses the locations of the joints in dependency of the independent part classifiers. the confidence maps of the regressed points, e.g., nose (red), left hip joint (blue), and left knee (green), are more discriminative and resolve the ambiguities between the legs."
"knowing irides and recognizing irides are two very different things. it must be stated clearly if our approach here is intended to announce and describe new iris recognition results or to formulate new points of view about irides and iris codes. in order to improve previously iris recognition approaches or to formulate new iris recognition methodologies, it is necessary to find new knowledge about irides and iris codes. one of our hypotheses is that the confusion between the genuine and imposter score distributions is motivated by the relative position of some iris codes in the iris code space. all iris codes extracted from samples taken for the same eye of the same person are viewed here as clusters in the iris code space, namely personal clusters. the separation between the personal clusters is fuzzy, or otherwise the genuine and imposter score distributions should not collide into each other. in this context, defuzzification between the two score distributions should be achievable if each personal cluster would be endowed with a suitable discriminant charged centroid, strong enough to alter the space in its immediate proximity by discriminately attracting the members of the personal cluster and repulsing non-members away from the personal cluster. this paper shows that it is possible to learn such special centroids as digital identities, using neural network support."
"our work is focused on improving the body part templates or the likelihoods for the joint positions within a pictorial structure model. in contrast to previous works, which run each body part template independently and use a tree structure or loopy models for modeling the dependencies among body parts, we propose to take the dependencies between body parts already into account for predicting the joint locations. in this way, the joint or part templates are already able to discriminate left and right limbs and compensate already for some limitations of tree models. since the templates are implemented by efficient randomized regression forests that predict directly the joint locations, our approach is comparable in running time to a state-of-theart method [cit], while providing a higher joint localization accuracy."
"the post-treatment quality assurance included: kv/mv triangulation as ground truth and comparison with kim real-time trajectory to assure accuracy of prostate motion trajectory feeding mlc tracking. reconstruction of delivered dose utilizing prostate motion trajectory, mlc logfiles and original treatment plan as described elsewhere [cit] ."
"based on the ftdo estimates and kinematic model in the pitch channel, the controller for following interceptors is designed using the dynamic surface smc law."
"a well suited neural network for learning discriminant directions is a recurrent neural network which must be compatible with the following space requirements and with the learning algorithm described below. system memory is designed to hold: the current recognition threshold t, the safety band sb, the numbers of identities k, stopping flag stop the discriminant directions which are currently trained, the iris codes ic on which the current discriminant direction is trained on, and other calibration variables as learning rates r and b. in the above heuristic procedure, denotes the binary complement of c j,i . hbtdd procedure stops only if all discriminant directions are trained i.e. they produce similarity scores positioned correctly for each comparison code and outside the safety band."
"after computing the unary potentials for an image, the unary potentials for each joint are normalized to be within the range [cit] . during training, a random forest can minimize both splitting criteria, i.e., (8) and (10), simultaneously. this is achieved simply via randomly alternating between the two goodness measures while the samples are recursively split down the tree, c.f . [cit] ."
"for all enrolled iris codes the difference between the lowest genuine similarity score and the highest imposter similarity score is at least 0.03. hence, hbtdd is a reliable solution for any personal-use application of iris recognition being able to deliver a kind of nearest-neighbor based biometric decisions which are safer than those based on hamming distance/similarity."
"it can be seen that the commands provided by the cooperative control strategy can be converted to velocity and flight path angle commands. the controller for following interceptors adopts the dynamic surface smc algorithm to achieve command signal tracking for the missiles in the cooperative network. assuming that missile velocities are controllable and air resistance and gravity can be neglected, the flight velocity of a follower can be expressed as (eq. 52)"
"from this observation, we hypothesized that compositional semantics is underpinned by simple mechanisms by which, during the first part of the trial, the agents regulate their actions on the basis of the object-label and not on the basis of the action-label, and vice-versa, during the second part of the trial. this quite intuitive hypothesis suggests that, in any given trial, there may be a first temporal phase, which starts right at the beginning of the trial, in which agents access the part of the linguistic instruction that defines the target object (i.e., the object-label) and act in order to execute the appropriate search behavior. during this phase, the other part of the linguistic instruction (i.e., the action-label) should not influence the agent's behavior. the first phase would be followed by a second one, which begins roughly when the target object is visually found. in the second phase, the agents regulate their behavior on the basis of the action-label only (i.e., the object-label does not have any influence) in case the instruction is touch or move. in the case of indicate, instead, the agents keep producing the same behavior during the entire trial. on this account of compositionality, linguistic instructions not experienced during training (i.e., move blue object, touch green object), would be:"
"ó 2018 elsevier b.v. all rights reserved. [cit] xxx-xxx until now, real-time image-guided adaptive radiation therapy (igart) has been the domain of dedicated and often expensive cancer radiotherapy systems such as the cyberknife synchrony [cit] and mitsubishi/brainlab vero [cit] . the purpose of this study was to clinically implement and investigate real-time igart using a standard linear accelerator."
"the paper is structured as follow. section ii reviews the most relevant works in the literature and in particular those described in (see [cit], which have been particularly inspiring for our work. section iii describes the task investigated in this research work and the agents' morphological structure. in sections iv, v, and vi, we describe the agent's control system, the evolutionary algorithm and the fitness function used to design it. in section vii, we illustrate the results of a series of postevaluation analyses. in section viii, we express some reflections on potential connections between empirical studies of child language learning and robotic models trying to indicate fruitful directions for future work. conclusions are presented in section ix."
"to assess whether the composition of the behavioral set affects the developmental process and the generalization capabilities of the agents, we run two sets of evolutionary experiments. in the with-indicate experimental condition, the task consists in the execution of the following instructions: table i ). the object-label and the action-label are given to the agent concurrently and for the entire duration of a trial."
"genotype parameters are linearly mapped to produce network parameters with the following ranges: in in with, with, and, with, and, gain factor . decay constants with, are firstly linearly mapped into the range and then exponentially mapped into . the lower bound of corresponds to the integration step-size used to update the controller; the upper bound, arbitrarily chosen, corresponds to about 4% of the maximum length of a trial."
the parameters of the ftdo are provided below. the regular and improved distributed cooperative control strategies are examined in the simulations. the formula for the former is (eq. 71):
"computational approaches to language learning are an intensely researched topic in several disciplines (for recent reviews, cf. [cit] . as yet, however, there is still a marked gap between language learning research in cognitive robotics on the one hand and language acquisition studies in computational linguistics on the other. one reason for this is the different thrust of typical research in the two disciplines: in robotics, the focus is commonly on semantic issues to do with the grounding of individual linguistic symbols in agents' sensory-motor experience [cit] . in computational linguistics, the focus is usually on structural issues to do with the induction of complex grammars from unrestricted text [cit] . in a nutshell, roboticists tend to concentrate on words as carriers of meaning (but neglect their combinatorial properties), while linguists tend to concentrate on their grammar (but neglect their meanings)."
subtask ii takes into account the seven scores recorded during the execution of the experienced linguistic instructions plus the score recorded during the execution of the nonexperienced linguistic instruction move blue object.
"subsumes those agents for whom the lowest score among those considered in a given subtask is within the interval (48,56], etc. let's consider an agent whose performances at the postevaluation test are represented by the following nine scores vector (80, 80, 80, 80, 80, 80, 80, 52, 67), in which the first seven scores refer to the performances while executing experienced instructions, the eighth score refers to the performance while executing the nonexperienced instruction touch green, and the ninth score refers to the performance while executing the nonexperienced instruction move blue object. this agent would be assigned to the following categories: 1) category as far as it concerns subtask i; 2) category as far as it concerns subtask ii; and 3) category as far as it concerns subtask iii and subtask iv. table ii shows the number of postevaluated agents for each category and for each subtask. these results can be summarized in the following:"
"rewards the agent for extending when it is perceiving the target object and it is required to touch or to move it (4), and are computed as follows:"
"in with-indicate, the fitness attributed to an agent in trial is the sum of three fitness components, and . rewards the agent for reducing the angular distance between and the target object."
"if an iris recognition theory of type (5) is learned through discriminant and witness directions, the correspondence: (10) maps all imposter comparison codes into a hypersphere within the unit hypersphere in ℝ ℓ and all genuine comparison codes in between the two hyperspheres."
"the interneuron network is fully connected. additionally, each interneuron receives one incoming synapse from each sensory neuron. each motor neuron receives one incoming synapse from each interneuron. there are no direct connections between sensory and motor neurons. the states of the motor neurons are used to control the movement of and as explained later. the states of the neurons are updated using the following equation: (1) for (2) for (3) with . in these equations, using terms derived from an analogy with real neurons, represents the cell potential, the decay constant, is a gain factor, the intensity of the perturbation on sensory neuron the strength of the synaptic connection from neuron to neuron the bias term, the firing rate (hereafter, ). all sensory neurons share the same bias, and the same holds for all motor neurons"
"where ζ n and ω n are the damping and bandwidth of the filter, respectively. the use of the filter can effectively address the differentiation problem of the command signals without affecting the amplitudes of the commands and their derivatives."
"based on the distributed network synchronization strategy given in eq. 42, the missile velocity reference commands are based on the distributed network synchronization strategy given in eq. 42, the missile velocity reference commands are (49) using eqs. 48 and 49, the total velocity and flight path angle of the interceptor are obtained as (eq. 50) (50) the signal must be filtered to obtain the derivative of the total velocity and flight path angle, ."
"in this paper, we have addressed robust human pose estimation from still images by proposing novel discriminative part template predictors within a pictorial structure framework. our joint location regressors consist of random forests that operate over two layers. while the first layer acts as an independent body part classificator, the second one takes the predicted distributions of the first layer for estimating the joint locations into account, thus allowing to put the body parts into relation. in the experimental part, we have shown that our model yields higher accurate human joint predictors than independent part templates and outperforms state-of-the-art methods that also use a tree structure for the human model."
"it can be seen that the commands provided by the cooperative control strategy can be converted to velocity and flight path angle commands. the controller for following interceptors adopts the dynamic surface smc algorithm to achieve command signal tracking for the missiles in the cooperative network. assuming that missile velocities are controllable and air resistance and gravity can be neglected, the flight velocity of a follower can be expressed as (eq. 52) (52) where pi is engine thrust."
the main goal of this paper is showing that discriminant directions and an iris recognition theory of type (5) can be learned heuristically by using neural network support.
is the derivative of v -mi . the sliding mode reaching law provided below is used to ensure that interceptor velocity can rapidly follow the system command.
"three factors affecting the patient's treatment were analyzed: feasibility, geometric accuracy of the kim system, and dosimetric fidelity of the integrated kim-mlc real-time igart system. 1. feasibility was measured using maximum likelihood estimates (matlab's binofit function) assuming a binomial distribution of a successful or unsuccessful treatment. a successful treatment was defined as the entire treatment fraction was delivered with kim-guided mlc tracking. 2. the geometric accuracy of the kim system was measured by comparing the kim-measured motion to the motion measured using post-treatment kv/mv triangulation. 3. the dosimetric fidelity of the integrated kim-mlc igart system was measured using a previously published dose reconstruction technique [cit] . the dose reconstruction method combines the original treatment plan, the kim-measured motion files and the treatment log files that have the mlc leaf positions, gantry angles, couch shifts and monitor units delivered, to estimate the dose delivered in the presence of motion, both with and without igart. a limitation of the dose reconstruction method is that the dose reconstruction is performed on the initial plan-"
"from eq. 53, an error surface is defined as (53) where is the filtered total velocity of the interceptor. the derivative of sv is obtained as (eq. 54)"
"indeed, by looking at the phylogeny of fully-compositional and partially-compositional agents in condition with-indicate, we notice that in early stages of their evolutionary history, one of the first behavioral skill to appear is indeed related to the capability of these agents to systematically reduce the angular distance between and the target object regardless of what type of action the current linguistic instruction is demanding. for example, the ancestors of fully-compositional agents, when required to move or to touch an object, they successfully bring in correspondence of the target object, and they keep bent until the end of the trial, by systematically failing to execute the action move and touch. in other words, these agents proved to be capable of accessing the linguistic label that defines the object without being able to appropriately execute the linguistic label that defines the touch and move actions. the ability to handle these type of actions is developed later. this can be considered a further evidence that, since the early generation of evolution in condition with-indicate, fully-compositional and partially-compositional agents learn to decompose the trial into two parts, in the first one of which their behavior is entirely triggered by the object-label. it is important to note that the early appearance of the capability to decompose the task into two parts is not enforced by any means by the design of the fitness function, it emerges through the dynamics of evolution, and it is facilitated in condition with-indicate by the presence of the instruction indicate. however, in the absence of further tests, we can not exclude that these phenomena are induced by design constraints, such as the fact that the segment and the vision system are bound together. this is because, this particular constraint makes it impossible for an agent to develop a visual search strategy without concurrently acting as required by the instruction indicate."
"the essence of multi-interceptor cooperative attack is to coordinate the positions of following interceptors and the leading interceptor. therefore, to realize the distributed network synchronization strategy, each interceptor in the network must follow the velocity commands"
"it is known from lemma 1 that v(t) can converge within finite time. that is, followers can verge to the state of the leader, achieving successful cooperative guidance and control for ltiple interceptors."
"it is known from lemma 1 that v(t) can converge within finite time. that is, followers can verge to the state of the leader, achieving successful cooperative guidance and control for ltiple interceptors."
"given this apparent opposition, it is interesting to note that a currently influential theory of child language acquisition assumes both a phenomenological continuum and a developmental connection between these two seemingly complementary learning targets (i.e., meaningful \"words\" and meaningless \"rules\" in traditional terminology). in usage-based models of language learning, children are assumed to acquire linguistic \"rules\" (i.e., grammatical categories and constructional patterns thereof) through piecemeal abstractions over utterance-length concrete \"words\" (i.e., unanalyzed holophrastic strings like \"there you go\" and \"look at this\" that are associated with a holistic communicative intention, see [cit] ). learners' discovery of the internal structure of these units, coupled with the realization that the segmented building blocks can be productively recombined within the abstracted constructional patterns, marks the crucial transition from finite lexicons to open-ended grammars. from this perspective, the above experiment is therefore concerned with the emergence of a genuine breakthrough on the way to language."
"on the other hand, why would or should somebody try such a complicate solution for such a simple problem? actually, the neural networks are simple enough, much simpler than the current state of affairs in the field of iris recognition. for example, it was shown recently [cit] that the artificial understanding of iris recognition couldn\"t be binary and logically consistent simultaneously if the imposter and genuine score distributions collide into each other, whereas a fuzzy 3-valent disambiguated model of iris recognition [cit] can guarantee logical consistency. the utility of a neural network is to deconfuse the two score distributions."
"the relation between fuzziness and intelligence is an open problem these days. fuzzy instruments are usually being used to attempt intelligent problem solving in conditions of incertitude / imprecision and this is also the case discussed here. the main topic of this paper is how to use intelligence in order to achieve biometric decision defuzzification. a neural training model is proposed here as a possible solution for dealing with natural fuzzification that appears between the intra-and the inter-class score distributions computed during iris recognition tests. are the sets of iris codes somehow separable in a neural perspective? are the genuine and imposter pairs two separable classes in some space? is there a neural network structure able to decrease the degree of confusion between inter-and intra-class distributions of scores? it is shown here that using neural-network support leads to an improvement in the artificial perception of the separation between intra-and inter-class distributions of scores by \"moving\" the two score distributions away from each other."
"generations following the first one are produced by a combination of selection with elitism and mutation. for each new generation, the three highest scoring genotypes (\"the elite\") from the previous generation are retained unchanged. the remainder of the new population is generated by fitness-proportional selection from the 50 best genotypes of the old population. new genotypes, except \"the elite,\" are produced by applying mutation. mutation entails that a random gaussian offset is applied to each gene, with a probability of 0.4. the mean of the gaussian is 0, and its standard deviation is 0.1. during evolution, all genes are constrained to remain within the range [cit] . that is, if due to mutations a gene falls below zero, its value is fixed to 0; if it rises above 1, its value is fixed to 1."
"where τ 3 is the time constant of the filter. hence, the derivative of the virtual control input after error surface filtering is (eq. 12):"
"based on eqs. 19 and 20 and the ftdo-estimated ˜ d ω z from eq. 7, the improved nonsingular fast dynamic surface smc law of the leading interceptor is (eq. 21):"
"in our experiments, we show that the proposed body parts dependent joint regressors achieve a much higher joint localization accuracy than independent part templates or joint regressors. integrated into a pictorial structure framework, the approach achieves a better joint localization accuracy than a state-of-the-art method [cit] at comparable running time of a few seconds per image."
"based on eqs. 19 and 20 and the ftdo-estimated from eq. 7, the improved nonsingular fast dynamic surface smc law of the leading interceptor is (eq. 21)"
"is the penalty factor. it is set to 0.6 if the agent collides with a non target object, otherwise to 1.0. the angle between and the target object can be measured clockwise or anticlockwise . in (5), and are the minimum between the clockwise and anticlockwise distance, that is . see (6a)-(6b) at the bottom of the page, where if otherwise . for the action indicate, steps-on-target refers to the number of time steps during which, and does not touch the target object. for the action touch, steps-on-target refers to the number of time steps during which touches the target object by activating the touch sensor, and does not change its angular position."
"with-ignore differs from with-indicate only in the computation of and during the execution of the linguistic instructions ignore blue object, ignore green object, and ignore red object . during the trials in which an agent is required to ignore an object if at the end of the trial the target object does not fall within the visual field of the agent, otherwise ."
"when finding the derivative of the virtual control input, is fed through a first-order low-pass filter to obtain the filtered virtual control input (eq. 11)"
"where τ 4 is the time constant of the filter. hence, the derivative of the virtual control input after error surface filtering is (eq. 17):"
"similarly, also the second major finding of this study, that is the significant effect of learning condition (with-indicate versus with-ignore) on agents' generalization performance readily translates into a concept of usage-based models of child language learning: if the above assumptions about what makes the behavior indicate more similar to move and touch than ignore are plausible, agents' poorer generalization performance in condition with-ignore would be said to be the outcome of a lower cue consistency (i.e., regularity of form-function mapping) of the category \"verb\" in this condition. furthermore, since such constellations of inconsistency, competition and syncretism are in fact taken to be the norm in natural language processing and learning, a look to usage-based acquisition models in linguistics could also suggest certain useful extensions of the present approach that might be worthwhile to explore in future work (e.g., studying agents' generalization performance across more than one construction, with or without semantic similarity between actions and/or referents, with balanced or statistically skewed training input, etc.). in other words, we will investigate the characteristics that favor the emergence of compositional solutions (i.e., that ensure behavioral generalization) and/or that reduce the chance to converge on noncompositional solutions. we will also investigate the possibility to scale the model with respect to the number and the complexity of the linguistic/behavioral repertoire of the agent."
"the reconstruction achieved with iiv balanced system has so much quality that the recovered memory of distinct individuality allows a wide and comfortable safety band inbetween the numerical representations of the artificial perceived concepts of \"genuine\" and \"imposter\" comparisons."
"purpose: until now, real-time image guided adaptive radiation therapy (igart) has been the domain of dedicated cancer radiotherapy systems. the purpose of this study was to clinically implement and investigate real-time igart using a standard linear accelerator. materials/methods: we developed and implemented two real-time technologies for standard linear accelerators: (1) kilovoltage intrafraction monitoring (kim) that finds the target and (2) multileaf collimator (mlc) tracking that aligns the radiation beam to the target. eight prostate sabr patients were treated with this real-time igart technology. the feasibility, geometric accuracy and the dosimetric fidelity were measured. results: thirty-nine out of forty fractions with real-time igart were successful (95% confidence interval 87-100%). the geometric accuracy of the kim system was à0.1 ± 0.4, 0.2 ± 0.2 and à0.1 ± 0.6 mm in the lr, si and ap directions, respectively. the dose reconstruction showed that real-time igart more closely reproduced the planned dose than that without igart. for the largest motion fraction, with real-time igart 100% of the ctv received the prescribed dose; without real-time igart only 95% of the ctv would have received the prescribed dose."
"based on the design approach of the first dynamic surface, the ftdo-estimated ˜ d a from eq. 6 is substituted into eq. 14 to acquire the virtual control input for the second dynamic surface (eq. 15):"
"determined to obtain (eq. 45) (45) considering v(e) ≠ 0, eq. 45 can be rewritten as (46) from eqs. 46 to 47, (47) it is known from lemma 1 that v(t) can converge within finite time. that is, followers ca converge to the state of the leader, achieving successful cooperative guidance and control fo multiple interceptors."
note that the proposed sucosamp algorithm works in the same way as the cosamp algorithm. the cosamp algorithm is basically based on basic omp. the sucosamp algorithm has incorporated some other ideas from the literature to present strong guarantees that omp and cosamp cannot provide and to speed up the algorithm as compared to omp.
"w z,n represents arbitrary noise vector w z,n represents the additive white gaussian noise of nth antenna group for zth ofdm symbol unrealistic assumption, specifically in wireless communication scenario. there are two differences between sucosamp and cosamp. firstly, sucosamp estimates the channels; i.e., it recovers the high dimensional sparse vector by utilizing structured sparsity of massive mimo channels from one vector of low dimension. secondly, sucosamp adaptively acquires the sparsity level. in contrast, the cosamp recovers the sparse vector without exploiting the structured sparsity and it requires prior information of correct sparsity level."
follows the same step of cosamp. the sparsity level s is not fixed (i.e. initialization with sparsity level equals 1). it adaptively acquires the correct sparsity level.
"this paper proposes a nonorthogonal pilot design and a cs based algorithm sucosamp to estimate the massive mimo channels. the reduction of high pilot overhead in massive mimo systems and the recovery ability when the sparsity level of massive mimo channels is unknown are the main focus of this research. by taking advantage of spatial and temporal common sparsity of massive mimo channels in delay domain, the proposed nonorthogonal pilot design and channel estimation scheme under the frame work of cs theory significantly reduce the pilot overheads for massive mimo systems and also outperform the conventional algorithms in performance. this research may be extended by incorporating the proposed schemes in the multicell scenarios."
"the major advantage of sucosamp over omp, cosamp, and basic subspace pursuit (sp) algorithms is that it does not require prior knowledge of sparsity level which is an follows the same step of cosamp."
". after converting to structured form, we will have equations to be solved simultaneously corresponding to each subgroup. the received pilot vectors of z th ofdm symbol for subgroups can be expressed as follows."
"where is the noise variance. there are many algorithms that can solve the problem. for example, projected gradient methods or interior point methods can be used for applying convex optimization. a famous greedy algorithm is orthogonal matching pursuit (omp). step 1 (initialization) 1.ȟ 0 ← 0 trivial initial approximation 2."
the delay domain spatial sparsity with specific system parameters will be detailed in section 5. figure 1 shows the common sparse pattern of cirs for different transmit-receive antenna pairs.
"in this paper we propose a cs based efficient nonorthogonal pilot scheme for massive mimo communication systems by exploring the temporal and spatial sparsity of massive mimo channels. and then we propose a channel estimation scheme, i.e., sparsity update cosamp (sucosamp), which exploits the temporal and spatial sparsity of massive mimo channels. the proposed pilot scheme is significantly different from the conventional schemes and substantially reduces the pilot overhead. the proposed pilot scheme employs fully identical subcarriers for pilots of several transmit antennas in a specific antenna group. the antennas placed at base station (bs) are subdivided into groups based on the observation that the coherence time of path gains is inversely proportional to the system carrier frequency whereas the variation in path delays is inversely proportional to the signal bandwidth. therefore the decision of making antenna groups and determining the number of antennas to be included in one antenna group is taken according to the given system parameters, i.e., systems frequency, system bandwidth, and antenna spacing at bs. furthermore, considering the antenna array geometry of bs, the proposed nonorthogonal pilot scheme is a space-time adaptive pilot scheme that adaptively changes its design according to the given system parameters. the proposed cs algorithm based channel estimation scheme sucosamp considers the initial sparsity level as 1 and then regularly updates the sparsity level until the stopping criteria are met or a correct sparsity level is achieved in a scenario where sparsity level is unknown. compared with the conventional cs algorithms sp and omp and with other available cs based algorithms, the proposed cs based pilot and channel estimation scheme is tested through simulations on systems with different parameters. it was verified that the proposed schemes provided improved channel estimation performance."
"one of the major issues in massive mimo systems is the accurate acquisition of the channel state information (csi) for beamforming, resource allocation, signal detection, etc. due to large antennas placed at the bs, the estimation of channels linked with hundreds of transmit antennas is required at users which results in high pilot overhead. hence, the precise channel estimation with the low pilot overhead is a challenging task [cit] ."
"in this research paper cs theory is applied to reduce the high pilot overhead based on the fact that the wireless channels undergo spatial and temporal sparsity. cs based pilot design significantly reduces the pilot overhead as compare to conventional pilot design. figure 3 demonstrated the proposed, uniformly distributed, and identical pilot subcarrier for multiple antennas, while figure 4 shows the convention pilot design (i.e., orthogonal pilots for different antennas). the proposed pilot design allows the system to provide more resources to data."
"the organization of the paper is as follows. sections 2 and 3 present the details about delay domain spatial and temporal sparsity of massive mimo communication channels, respectively. section 4 presents the proposed nonorthogonal pilot scheme based on cs theory. section 5 illustrates the cs based massive mimo channel estimation scheme. section 6 provides the simulation results. and in section 7 we present the conclusion notation. uppercase and lowercase boldface letters represent the matrices and vectors, respectively. the matrix transpose, inversion, and conjugate transpose are denoted by (.), (.) −1, and (.) *, respectively. the moore-penrose inversion of matrix is denoted by (.)"
"mostly wireless communication systems detect the data with the help of pilot signals. specifically the purpose of pilot signals is to assist the receiver in estimating the wireless channels and then the receiver coherently detects the data on the basis of estimated channel [cit] . in massive mimo communication systems, due to large number of transmit antennas at bs, the number of channels gets prohibitively high and thus results in high pilot overhead for estimation of these channels. therefore there is a need for methods to reduce the high pilot overhead in massive mimo communication systems to achieve the target of high data rate."
pilot subcarriers of n th group are the null pilots for all the other − 1 subgroups whereas − − subcarriers are available for data.
"the system equation of each subgroup exhibits structured sparsity, which provides the motivation to apply cs theory to recover the high dimension structured sparse signalh z, from the low dimension received pilot vector y z, . the cs based sparse signal recovery/channel estimation will be explained in section 5."
"there are several research challenges which are required to be solved before massive mimo can be fully integrated into future wireless systems [cit] : (i) beamforming will involve a huge amount of channel state information, which will be challenging for the downlink. (ii) mimo can be unfeasible for fdd systems, but can be employed in time division duplex (tdd) systems due to having channel reciprocity. (iii) conventional channel estimation approaches require a large pilot and feedback overhead, which typically scales proportionally with number of bs transmit antennas, which results in unfeasible condition for large-scale fdd mimo systems. (iv) in addition, massive mimo experiences pilot contamination from other cells if the transmit power is high; otherwise it will be affected by thermal noise. methods will be required to solve this in order to deliver the promised performance. (v) there is a need for channel models of massive mimo systems, without which it will be difficult for researchers to perfectly validate the algorithms and techniques. (vi) for channel estimation, tdd scenarios are only considered for massive mimo due to the excessive cost of channel estimation and feedback. even for tdd to work, massive mimo channel calibration can prove to be a big achievement. new methods and schemes will be needed for the purpose of channel estimation in massive mimo systems. (vii) extremely fast processing algorithms will be required for processing the massive amount of data from the rf chains."
"multiple-input multiple-output (mimo) systems have multiple antennas at both the transmitter and receiver ends. by addition of multiple antennas, higher degree of freedom in wireless channels (in terms of time and frequency dimensions) can be obtained in order to achieve target of high data rates. for this reason, major performance progress can be attained in terms of system reliability and both spectral and energy efficiency. and also such higher degrees of freedom can be exploited using beamforming given that channel state information is available. there are large number of antenna elements (around tens or even hundreds) deployed at both sides, the transmitter and receiver. it is important to note that the transmit antennas may be distributed or colocated in different applications. also, the huge number of receive antennas can be acquired by one device or distributed to many devices [cit] . additionally, massive mimo systems help in minimizing the effects of noise and fast fading, and also intracell interference can be reduced using straightforward detection and linear precoding methods. by appropriately implementing multiuser mimo (mu-mimo) in massive mimo systems, the design of medium access control (mac) layer can be more simplified by getting rid of complicated scheduling algorithms [cit] . with mu-mimo, signals to individual users can be easily separated using the 2 wireless communications and mobile computing same time-frequency resource, at base station. therefore, these main advantages make it feasible to introduce the massive mimo system as a promising candidate for 5g wireless communication networks."
"in this section we shall propose a nonorthogonal pilot scheme based on the above-mentioned two observations. first the basic idea to divide the antennas placed at bs into subgroups is proposed. the proposed scheme of dividing the antennas into subgroups is based on system parameters and the temporal and spatial sparsity of massive mimo channels. then after creating the antenna groups, a specific nonorthogonal pilot scheme is proposed."
