text
"most of the biochemical reactions require catalytic molecules (called enzymes) that increase the rate of the reactions. in enzymatic reactions, the molecules at the beginning of the process are called substrates (s), and the enzyme selectively converts them into products (p). the kinetic description of such systems was expressed by l. michaelis and m.l. menten [cit] . the derived equation of their results by (referred as michaelis-menten kinetics) are widely used in kinetic modeling. the scheme of a one-substrateone-product reaction (with one active site) is presented in table 2 . the catalytic step is supposed to be irreversible and the rates of the reactions are given by the law of mass action. (1) rate of es complex formation"
"on wafer dataset, our solution has an accuracy slightly inferior to that obtained by the full length 1nn method and an average time of prediction of 4% of the full length. this speed in prediction is due to the identification of a very early pattern in time series from class 2."
"secondly, when modeling has to deal with several assumptions of phenomenological modules, the freedom of compositionality is reduced. the hidden parts of the modules may contain important linkage between models that are chosen to be merged together. several biological models are built each day making compositionality to be one of the most important key features of process algebra that offers systematic modeling of complex biological networks. compositionality has been addressed as an issue of model-construction from elementary reactions with the basic operators [cit], as the translation of one approach to another [cit], or as the combination of different types of models (odes with process algebras, boolean, hybrid models) [cit], but the compositionality of complex rate functions has been attracted less attention."
"we simply merge the modules and insert the boxes (enzymes and intermediates) of the novel entities. we also replace the events corresponding to the complex functions for the ones from the \"unpacked\" modules. this method can be easily automatized as it does not require the modification of the reactions that are independent of the substituted complex functions and the functions calculating the rate of complex reactions does not involve binders. the novel internal behavior of the boxes can be easily parallelized with the original ones. the composition of modules is shown in figure 9 . simulation of the \"unpacked\" system ( figure 10 (1-2a) ) shows larger noise than the original (figure 10 (1-2b) ), but still produces regular oscillations. it has been shown in several works [cit] that with process calculus based languages dynamic models can be constructed and existing continuous models can be transferred into the stochastic framework providing additional predictions of the biological system results to the existing models [cit] . herein we have to remind the reader that generally distributed reaction times have been also implemented into the blenx framework recently [cit] . it provides choices of the reaction time distribution for the stochastic simulation algorithm of gillespie. in this way, abstracted rate laws can be handled stochastically that leads to a better quantitative tool for matching wet-lab experiments and in-silico results without breaking down the complex reactions into elementary steps. the use of this extension fits well the idea of a template based modeling framework as, depending on the question the user asked, biological models might be characterized through complex rate laws and handled by generalized distributions of time; while templates (including only elementary steps) offer a straightforward, flexible and more precise way of compositional modeling in blenx."
"the paper is structured as follows. section 2 and 3 show the stochastic decomposition of michaelismenten kinetics and of hill functions, respectively. section 4 uses the stochastic decompositions to compositionally build a model of the circadian clock. we conclude the paper with a discussion of the results and of the future research directions."
"the proper rate constants describing our compound michaelis-menten module have been selected by taking the minimum amount of substrate during the reaction and setting the initial (total) concentration of enzyme to s min · 0.1. as a consequence, we get"
"with θ − t an old fixed version of the deep-q-network. by doing so, the agent progressively estimates the relationships between being in a state o t, choosing an action a t and getting future rewards g t ."
"computational systems biology is a novel approach to understand how biological systems are orchestrated altogether [cit] . biology, physics, computer science, systems theory and mathematics have joined to propel a multidisciplinary research that provides tools for the analysis of biological studies. particularly, stochastic approaches are becoming more and more popular as novel experimental techniques -such as quantitative flow cytometry [cit] and fluorescence microscopy [cit] -provide single level measurements of cell physiology. while the average behavior of a cell population has been described by continuous modeling approaches (e.g. with ordinary differential equations, odes) [cit] from a long time, single cells are analyzed in a stochastic framework as fluctuations may have a significant effect on the physiology of the cell. the influence of noise also in gene expression and signal transduction processes have been shown to be important by both theoretical and experimental approaches [cit] . computer science through the discipline of algorithmic systems biology [cit] enables this research. here we concentrate on a class of formal languages, namely stochastic process algebras that have been used for interacting and distributed systems and serve as a modeling framework for biological systems as well (for a survey see [cit] ). starting from stochastic π-calculus [cit] and passing through beta-binders [cit], a real programming language has been designed specifically to model and simulate biological systems: blenx [cit] ."
", then action a 1 is a better choice to make when the agent is in state o than action a 2 . the goal in reinforcement learning problems is to find the optimal policy π * the agent should follow in order to get the maximum rewards g t . one way to achieve an optimal policy π * is to find the optimal q-function q * . if the optimal qfunction is known then the optimal policy can naturally be inferred from it by acting greedily, that is to say by picking the action maximizing q * (eq. 10)."
"two basic complex rate functions that are widely used in biological modeling have been decomposed into elementary reactions in the blenx framework. the work proposed in this article provides a proper structure for gillespie's algorithm. furthermore, the templates offer a straightforward method for compo- sitional modeling. the usage of templates provides a less error prone method in modeling as assumptions are taken into account during the model composition. we exemplified our approach on a circadian clock model. on the top of the first set of predefined templates, a higher level of abstraction and higher level of model composition might be exploited in the future by building larger templates. for instance, a biological switch might be built from two michaelis-menten functions, often called goldbeter-koshland module [cit] . although the simulation time increases by decomposing complex reactions into elementary ones, we think that the modeling advantage is worth pursuing. optimizations and simplification can be postponed to the model of the overall system. we will continue investigating the decomposition of complex reactions in order to build a library that will allow drag-and-drop style of modeling complex behavior relying on predefined bricks."
"with n being the number of examples in the dataset d. to evaluate the performance of a classification model c on a test dataset d, we measure the classifier accuracy: the ratio of correctly labelled examples among all predictions (eq. 4)."
"in the next subsection, we provide a brief description of how to code enzymatic reations in blenx with elementary steps and we give a hint how to search for unknown parameters in a michaelis-menten module."
"the michaelis-menten rate law is often found to be a good approximation to describe enzymatic reactions. the rates of complex formation and its dissociation are rarely available in biological systems, while the key parameters (v max and k m ) of a michaelis-menten reaction might be easily determined from measured data through linear graphical representations (e.g. lineweaverburk plot, haneswoolf plot, eadiehofstee diagram) or by nonlinear regression methods."
the system is divided into the following modules: (1) transcriptional regulation following hill kinetics (2) translation mechanism (assumed to follow mass action kinetics in this study) (3) homodimerization of clock proteins (cp) (4) formation of an inactive complex providing a negative effect inside the loop (5) there are three degradation term catalytically activated by enzymes (following michaelismenten assumption) and the system also contains linear (so called background) degradation of the elements. this network (figure 8 ) is built with complex rate functions and provides a 24h periodic oscillator ( figure 10 (1-2b) ).
one advantage of the proposed solution is that it gives the user the freedom to evaluate the agent performance regularly over training and then pick the agent's policy that performed best according to his criteria and will to compromise.
"on gun-point dataset, the agent predicted once it received 22% of the full sequence in average and reached an accuracy superior to that of full length 1nn method."
"\"unpacked\" version of these modules, representing complex reactions, has to be available as an option for merging them properly when a hidden link is becoming to play a role in the whole system. usage of decomposed modules containing only elementary steps lengthens simulation time, but provides formal grounds for composition. after merging the modules properly, reduction and abstraction techniques can be applied to the overall system. it is our belief that \"unpacked\" modules could be a natural way of modeling biological processes by connecting small models into large ones. modern modeling techniques should provide a framework where the model-building process can be carried out correctly (assumptions are taken into account), while it is straightforward and is also suitable for the stochastic simulations. the main goal of this article is to present a possible extension of process algebra tools to carry out compositional modeling in a proper and an easy way."
"transcriptional factors (abbreviated as t fs) often form multimers during transcription [cit] creating sigmoidal response of the system to the change of the transcriptional factors. cooperativity widely occurs and the hill equation is a good approximation of the underlying mechanisms, although it assumes simultaneous binding of the t fs to the promoter region that is far from the realistic picture. as intermediate states have to occur during the reaction, sequential binding of the transcription factor to the promoter has been considered for this study. the following scheme approximates the hill function when the intermediate state (t f2) does not accumulate and positive cooperativity is present. several other interactions are plausible [cit], but are not investigated in this article for the sake of simplicity. blenx offers a formal and efficient definition of join and split events of boxes as complex formation and decomplexation occur in biological systems. transcription factors (t fs) form multimers (t f2, 3, 4) through a join event, enhancing the affinity of its binding onto the gene's promoter region (g). positive cooperativity results in a low dissociation constant of the gt f2 trimer. the joined complex is able to transcribe mrnas (ms) of the gene through a split event that creates m boxes with the release of the active trimer. bindings are assumed to be reversible. elementary reactions of the system are summarized in table 4 ."
"the proposed definition of states, actions and rewards allow us to train an early classifier agent with reinforcement learning. to find the optimal policy π * (eq. 10) that will lead to maximum rewards g t, we adapt the dqn algorithm [cit] and look for the optimal q-function parametrized using a deep neural network with parameters θ."
"from the experiment illustrated in figure 1, we chose to keep as the output of the learning process the agent's policy that performed best during training: among the most accurate policies, we selected the fastest one (policy surrounded by a fig. 1 . evolution of the early classifier agent behaviour on gun-point dataset. the scatter plot shows the relationship between accuracy (in percentage) and average time of prediction of the agent over training. we evaluate the agent on the whole training set every 5,000 iterations. each evaluation corresponds to one dot. dot points are coloured according to iterations of training: blue dots correspond to early training while yellow dots correspond to the agent's performance after 100,000 iterations of training. we evaluate the agent's policy surrounded by the red star on the testing set and we report its performance in table i. in this experiment, the agent learned to slow its predictions down and improved its accuracy over training. red star in figure 1) . in table i, we report the performance of this policy on gun-point testing set. we also trained early classifier agents on wafer and ecg training sets. we selected policies that performed best during training and reported their performance on wafer and ecg testing sets in table i. as a comparison, we indicate the performance of early classification on time series (ects) [cit] and early distinctive shapelet classification (edsc) [cit] methods. we did not reproduce their experiments but simply reported results mentioned in the original papers. we also indicate the performance of a 1 nearest neighbor (1nn) classifier on full time series with results provided in the ucr archive."
"surrounded by information in our environment, we often collect data over time and make decisions based on these sequences of information. a physician will, for example, prescribe a new treatment to his patient as a response to the underlying identification of particular patterns in his medical exam history. a production chain manager will anticipate a breakdown after having identified atypical behaviours in the machines' signals. in time sensitive applications such as medical diagnosis, disaster prediction, intrusion detection and process control, particular situations should be recognized as soon as possible and decisions taken quickly in order to take the best possible actions. in this paper, we present a solution capable of analyzing time series, and more generally, streaming data, and predicting their class label early in time, using as little data as possible. such a solution is referred to as an \"early classifier\" in the scientific literature. just like humans do while making strategic decisions, these early classifiers emails: coralie martinez (corresp. author): martinezcoralie.mc@gmail.com guillaume perrin: guillaume.perrin@biomerieux.com emmanuel ramasso: emmanuel.ramasso@femto-st.fr michèle rombaut: michele.rombaut@gipsa-lab.grenoble-inp.fr * institute of engineering univ. grenoble alpes should balance the two contradictory costs of classification accuracy and earliness. an early classifier should find an optimal critical point between acquiring more information to gather enough evidence and providing early results."
"we compared the deterministic and stochastic simulation results with the \"unpacked\" and complex modules with a parameter set shown in table 3 . the module built up with complex reaction and the one with elementary reactions shows us a good accordance with each other and also with the deterministic scheme ( figure 3) . simulation results of the blenx model fits the deterministic saturation curve well, although when the original assumptions of michaelis and menten do not match, the exact solution diverges from the complex function as it has been shown previously [cit] . when the enzyme is in excess of the substrate, the solution of the unpacked model differ greatly from the packed version as the assumption made for the qssa is not more valid for the system (figure 3 and figure 4 ). this is one limitation of the compound function (also in deterministic simulations). arkin and rao assumed [cit] that the reactions are isolated and the amount of enzyme is fixed -but in complex network this assumption seems to be \"weak\". enzyme concentration has to be much less than the substrate concentration and in e.g. oscillatory systems the substrate is changed over time. in those cases, the minimum value of the substrate has to provide the base of the calculation (see in section 4). we emphasize that decomposition of michaelis-menten kinetics is not always necessary, but in a compositional modeling framework it has to be available (as a library). assumptions have to be checked and the decomposition might be especially useful for further extensions of the model (when hidden details are becoming important). for instance, when an inhibitor of the enzyme is present or two substrates of the same enzyme are introduced, details of the complex reactions have to be elucidated. in section 2, we provided a brief description of a template for enzyme kinetics in blenx with a parameter search based on basic mathematical calculus. implementation of the template-library into the cosbi lab platform [cit] might automatize the method of parameter estimation as it is a software including inference tools (kinfer [cit] ). research proceed in that direction."
"by using blenx primitives, subsystems (called motifs) are defined, templates are built and decomposition of complex formulas into elementary steps is achieved. furthermore, compositional modeling is applied to an oscillating biological system (called circadian clock [cit] ). we show that template-based modeling in blenx improves compositionality."
"to tackle the problem of early classification, we propose an original use of reinforcement learning in order to train an endto-end early classifier agent with a simultaneous learning of both features in the time series and decision rules. our experimental results show that the early classifier agent can achieve effective early classification with fast and accurate predictions. in this work we suggest a static setting of the reward function but more efforts could be put onto the identification of optimal parameters. as future work, we plan to improve the proposed approach with a dynamic adjustment of the reward function parameters over training based on the user trade-off criteria. we will also propose a new management of the agent's replay memory which could be more suitable for the problem of early classification."
"in this study we concentrate on blenx. we propose a novel framework for constructing models by previously coded model fragments stored in a template-library. building large models starting from the basic principles of a process calculus language is time consuming and error-prone, thus a template library should ease the work of programming as it happens in all the fields affected by computer technology. important motifs with special dynamics have been already proposed in biology [cit] and in computer science [cit], but they rely on compound phenomenological description and mathematical assumptions."
"when non-elementary reactions occur and compound mathematical formulas are used, the direct translation of mathematical terms into the stochastic context is a well-liked approach. usage of these general functions for calculating the rate of a reaction is also possible in blenx [cit] . however, these implementations have been pointed out by several authors to be incompatible for some cases [cit], thus modelers have to be careful with them. stochastic modeling of complex functions is only an approximation and assumptions have to be handled globally. thus the blenx framework calls for a semi-automatic method of describing these complex rate functions with intermediate steps (we refer as an \"unpacking\" mechanism) not only owing to ease the compositional programming process, but to provide a correct (and generalized) way of stochastic simulations."
"on ecg dataset, the agent gave fast predictions (17% of full length) with an accuracy comparable to that of other methods. with all three datasets, we experienced over-fitting when the neural network architecture was not appropriately sized."
"as enzymes are specific to their substrates and the formation of the enzyme-substrate complex (es) is assumed to be relatively fast, the equilibrium is reached rapidly and the production of p is the limiting step in the overall system. therefore the es complex is assumed to be stable, thus the change of its concentration approaches zero (quasi-steady-state assumption (qssa)). another important assumption is that the concentration of the substrate highly exceeds the one of the enzyme ([s]"
we define an early classifier as a mathematical model capable of analyzing time series and identifying their class label early in time. it should compromise between accuracy and earliness of classification under some criteria accepted by the user's application. an early classifier c has to meet two requirements:
"depending on its policy π, the agent can evaluate the expected return g t it can get by starting the game from state o, picking a particular action a and then following its policy π. this is the action value function, also called the q-function (eq. 8)."
the use of a deep neural network as a function approximator of the q-function leads to a end-to-end learning of both features of interest in the time series and of decision rules about when and which class label should be predicted.
the agent's goal is to receive the maximum total discounted reward g t which represents the immediate reward plus the future discounted rewards (eq. 7).
"from these experiments, we showed that the proposed solution can achieve early classification and can retain an accuracy comparable to that of the full length 1nn classifier."
we measured the time average of the bound form (gt f2) in case of several levels of initial t f and calculated the response coefficient (r) and the actual hill coefficient (n ) for each set of parameters. the derived hill coefficient equals to log81/logr [cit] . the derived dissociation constant (j ) is calculated from the points as well as the root mean square error of the fit to the simulation results and the simulation point's error to the theoretical hill function curve.
"a. background 1) reinforcement learning: reinforcement learning is an active field of research for sequential decision making. it is used to train agents interacting with an environment such as artificial intelligence playing video games. agents are trained while playing what's called \"episodes\". at the beginning of an episode, the agent receives an observation of its initial state o 1 . at each time step t, after it receives an observation o t, the agent picks an action a t . in response, the environment gives the agent a feedback: a reward r t and a new observation o t+1 . these interactions go on until the agent reaches a terminal state, leading to the end of the current episode and the start of a new one. the agent is characterized by his policy π modelling how it picks its actions. the policy π is a behaviour function (eq. 6) which returns the probability of picking an action a given a particular state o."
"we consider an early classifier as an algorithm allowed to make decisions. while receiving streaming data, the algorithm analyzes the sequence and can make an action. it can predict a class label based on the assumption that it collected enough information to identify discriminant patterns of interest. or it can wait for future additional information. we propose to train an early classifier as an agent evolving in a reinforcement learning framework where:"
"we run experimental tests on the ucr archive [cit] which is widely used as benchmark for classification and clustering of time series. we evaluate the proposed solution on gun-point, wafer and ecg datasets. these datasets have various amount of training data and allow to evaluate the suitability of the solution when few training samples are available."
"the simulation results also shows behavior coincident to frequency modulation theory [cit] where we see dense bursts of active transcription factors (gt f2) that results burstlike transcription giving similar downstream results that occur in concentration dependent transcription in deterministic models (data not shown). table 5 : multiple simulation results on the module of the hill kinetics. set 0 refers to the deterministic version of the complex hill function. set 1-set 8 are different sets of parameters for the \"unpacked\" module."
"sigmoidal curves are often measured in experiments providing specific hill constants (such as n, j) to the hill function. the hill curve describing e.g. a transcriptional regulation scheme is not the proper way to apply stochastic calculations. elementary steps of the sequential binding scheme contains four rate constants (k 1, k 2, k 3, k 4 ) that have to be determined from the constants n and j. derivation of the missing parameters are calculated from the mass action kinetics describing the system. at equilibrium the intermediate complexes are assumed to be stable, thus"
"we \"unpacked\" the transcriptional step and the three degradation terms following michaelis-menten kinetics. we chose the number of enzymes having role in the system to be less than the corresponding substrates, making the assumptions of michaelis-menten kinetics valid. the parameters originating from the complex functions are also scaled up to be fast enough. thus the reactions assumed to be in equilibrium do not limit the system. the products of the enzymatic reactions are degraded immediately (with an infinite rate) after their production in order to serve the catalyzed degradation scheme in the system."
"we also see that the hill coefficient is not equal to the number of binding sites on the gene, but provides only a minimum value."
"[etot]). when a critical substrate concentration is reached, the enzyme is saturated and additional amount of substrate does not influence the velocity of the reaction; it is already maximal (v max ). if the last reaction is assumed to be irreversible and all the previously mentioned statements are valid, the rate of the substrate turnover to product is approximated by"
"this section analyzes the problem of ripples in the fs-mpc with a short-horizon prediction and then derives the stability conditions from the state feedback control laws to solve the problem. next, a new fs-mpc is proposed with the stabilized finite-states based on the robust model prediction mentioned above. fig. 3 illustrates the ripple problem originated from the shorthorizon prediction. as seen in the trajectory of the conventional fs-mpc (''con'') in fig. 3, an optimal switching-state (e.g., s con ) at each time-step (e.g., k th time-step) is selected based on the horizon-one prediction where s con is selected just because the predicted error is smallest among other switching states. however, due to the unguaranteed stability, s con may bring the temporary divergence of the trajectory as shown in fig. 3 . hence, there is a possible ripple in the trajectory of the conventional fs-mpc as reported widely in the literature [cit] . in the expected trajectory (''pro'') seen in fig. 3, if the other optimal switching-state (e.g., s pro ) is selected with a constrained stability at each time-step, then the convergence of trajectory is maintained to cancel out the possible ripples. hence, the key improvements in control performance of the fs-mpc are on the constrained stability and robust prediction, which are the target for control design in this paper. the following subsections propose a stabilized direct search by monitoring the stability condition via the equivalent gains of feedback control rules. the idea of divergence in horizon-one fs-mpc can be extended to a general reference-tracking control problem, which is not limited to a constant-reference tracking as illustrated in fig. 3 . in case of a sudden reference change, the dynamic response may be changed from a short rising time with a large overshoot for the conventional fs-mpc method into a smooth, reduced overshoot but a longer rising time for the proposed fs-mpc method. then, a specific investigation should be considered for a specified application with a detailed system configuration."
"pan workshop data sets. the interpretation of vandalism differs amongst wikipedia users, which can lead to incomplete or inconsistent labeling of vandalized revisions on wikipedia. potthast [cit] develop two corpora by crowdsourcing votes on whether a wikipedia revision contains vandalism using amazon's mechanical turk. the corpus pan-wvc-10 contains around 32,000 revisions sampled from the english wikipedia, where 7 percent of the revisions contain vandalism. the corpus pan-wvc-11 contains less than 10,000 revisions for each of the english, german, and spanish wikipedias, where approximately 11 percent of all revisions contain vandalism."
"in order to evaluate the prediction accuracy of link predictor, e ij is divided into a training set, e ij t, and a test set, e ij p such that . [cit], respectively. in this regard, three years of data has been used for training phase and the last two years has been selected for the test. table 7 shows some statistics of four network samples. evaluation metrics. to measure the prediction accuracy, two evaluation metrics for link prediction, auc and precision, are commonly used. the auc (area under the receiver operating characteristic curve) is found by the probability that a randomly chosen missing link (i.e. positive pairs in test set) is given a higher score than a randomly chosen non-existent link 32 . however, this measure has been found as a deceptive evaluation measure 32, 33, epecially in imbalanced data problems. the major drawback of auc evaluation is that it is required to define a negative set, which is composed of all the missing (non-observed) links in the network except for the removed links for the test. while it is not known in advance which of the missing links are truly negative, undetected, or will appear in future. therefore, the negative set can not be properly determined in the link prediction task. ignoring the above fact causes that the auc evaluation gets biased towards a negative set in unbalanced datasets. also, such evaluation gives more importance to the methods that overfit the network structure rather than offer a more general prediction ability. in this regard, the prediction performance of all methods in both experiments have been measured under the precision which is a measure of correctness achieved in positive prediction."
"features f13 to f20: we count the different word types. by looking at the letters of each word, some indications of possible vandalism are uppercase words, words with digits, and words that are single letters. these features are common indicators of vandalism in related work [cit] ."
note that the condition (18) (18) can be used as a sample to develop the stability constraints for the proposed finite-set mpc applied into other applications.
"based on these considerations, the mos for the playout of a video with k segments, n quality levels and played quality level q k for segment k can be estimated by (6) ."
wherê and measurement noises are dealt with by the kalman filter [cit] . the observer-based model not only improves the robustness of model-based prediction but also simplifies the model by using the error dynamics (4).
"as we are only interested in the textual features derived from the revision content, we reduce data size by focusing on the difference in the content of the flagged revision with the previous revision. we use the python unified diff 5 algorithm to obtain lines (marked by a full stop or period) unique to each revision and the lines changed."
"we split the data into training [cit] and testing [cit] sets as described in section 3 and seen in table 2 . the data set is highly imbalanced, so we undersample (without replacement) the regular revisions to match the number of identified vandalized revisions for the training and testing sets. this allows the random forest algorithm to improve its classification performance with many balanced tree samples. we address the issue of training data balancing in section 6.5, where we compare other ratios of regular revisions to vandalized revisions to show there are no statistically significant changes in classification results for different sampling ratios."
"since the agent's goal is to maximize the numerical reward, we want the reward function to be a measure of the qoe. there are three factors that impact the video quality as perceived by the user [cit] : (i) the average segment quality level, (ii) the switching behaviour of quality levels and (iii) video freezes, (1), (2) and (3)."
"the english wikipedia is the largest wikipedia, where the majority of vandalism detection research is performed. we demonstrate that cross-language classification is possible without significant loss in classification quality. this allows vandalism detection in english to be applied to other languages without needing specific classifiers or additional inputs. note that we have selected text features that avoid problems of required cultural knowledge of the target languages. additional languages may require different selections of text features."
"to distinguish revisions made by bot editors, we obtain lists of bot names for each language from wikipedia articles and categories maintaining these lists. 6 we split the revisions into those made by bots and those made by users. we do not distinguish edits made by counter-vandalism tools, nor anonymous and registered users, which we leave as future work."
"and connecting the parasite resistance r f in series with of the inductance l f as a test for the case of an unmodeled parameter. it is noted that the parameter uncertainties are selected as ±100% from the nominal values as intensive cases while the normal tolerance provided by the manufacturers is typically within ±10% [cit] . fig. 7(a) and (b) show the simulated waveforms to prove the robustness of the proposed observer-based prediction and overall control system under two mismatch cases, respectively. in this figure, the values (i.e.,î * id,î * iq,v * id,v * iq ) observed from the nominal parameters (l f, c f ) using (7), (10) hence, fig. 8 proves that the proposed fs-mpc can significantly reduce both sees and thds compared with the conventional fs-mpc by constraining the stability conditions. in particular, the thds difference (i.e., 1.1% and 0.7%) between the two control schemes under the same sampling rate and lc filters comes from the reduced low-frequency ripples as mentioned in section iv-a. note that this difference is important because it is a challenge to filter out the low-frequency harmonics in practice. in addition, the smaller sses help improve the ac supply quality of the inverter system under critical loads such as ups applications."
"difficult or ambiguous vandal edits that can only be identified by consensus. we believe this is the reason for the comparatively lower auc-pr scores for the pan data sets compared to our results in tables 8 and 6 for all matching pairs of training and testing languages in the pan data sets. however, for the full wikipedia data sets our features have strong classification performance within and across languages. many features presented in related work show strong classification performance on the pan data sets, but we believe they also need to be evaluated on the full wikipedia data set to gauge their effectiveness in distinguishing vandalism within and across languages on large scale data."
"2) performance reconvergence: an important property of a learning agent is its ability to adapt to an environment change. therefore, we evaluated the client performance when streaming different video sequences during a single learning session. for this purpose, we selected the big buck bunny and elephants dream video sequences since these have similar quality level bitrates. this is a plausible assumption since a content provider will most likely offer all videos encoded using the same settings. fig. 3 shows the performance ratio of the rl-based client, compared to the mss client, in a situation where the video sequence shifts every 100 episodes. the graph shows that the self-learning client is able to instantly adapt to a new video sequence without performance loss. the previously obtained knowledge can be reused when learning on a new video sequence. it is clear that after a learning phase of comparable length as in the single video case, the self-learning client is able to outperform the mss client by 15.48% in the converged state of the last 50 episodes."
"we immediately see that our novel features are generally more effective in distinguishing regular revisions from vandal revisions from the repairs-with the lower percentage of failure, especially in the much larger full wikipedia data sets. some of the borrowed features from the pan workshops (f10 to f20) are not effective in the pan data sets, and are less effective in the full wikipedia data set. the small size of the pan data sets may also hinder many other features that are effective in distinguishing vandalism in the full wikipedia data sets. for example, the size of the lines changed (f02 and f03), and words with many repeated characters (f26)."
"vandalism is an increasingly important and urgent issue on all language editions of wikipedia as wikipedia's popularity and number of articles grows. [cit] . the granting of editing capabilities to bots have allowed bots to become the power editors on wikipedia [cit] . as bots take the lead from users in identifying vandalism on the english wikipedia, this maintaining of quality is deterring new and experienced editors [cit] . counter-vandalism bots may be solely responsible for the decline in the retention of new contributors because of their strict enforcement and poor communication of policy [cit] ."
this section derives the continuous-time state-space model in the d-q frame of a typical three-wire 3φ-vsi accompanied by an output lc filter. the discrete nature from the switching-states to a 3φ voltage system is also mentioned for a fs-mpc design.
"a three-wire 3φ-vsi system with an output lc filter. fig. 1 depicts a three-wire 3φ-vsi system with an output lc filter which is popular in various ac power conditioning systems [cit] . it is noted that a four-wire 3φ-vsi system can be easily implemented with a −y connected power transformer after this system [cit] . for a three-wire 3-phase vsi system, the dynamic model can be simply presented in the synchronously rotating d − q reference frame via the first-order ordinary differential equations as follows [cit] :"
"results of the matching training and testing languages show auc-pr in-between those of bots and users in tables 6 and 7 . similarly, the t-test p-values of the rows show values greater than 0.05, except when combining the training data of all languages. the last row of table 8 shows t-test p-values less than 0.05 for most of the other rows. this suggests by using all training data, we have a statistically significantly better vandalism detector for all languages."
"prediction between different object types. the precision performance of the mmi in recommending relevant publishers to the authors is reported in tables 5 and 6, based on top-100 and top-10% rated links, respectively, and compared with lcp-based methods. as the extracted network for the second experiment is a bi-partite network between the authors (a) and venues (v), the possible meta-paths are restricted to the form of (av)"
"we use the random forest classifier and evaluation metrics from the python based scikit-learn toolkit [cit] . this classifier was shown to be the most robust and generally best performing classifier from related works, hence we did not compare different classifiers in this paper."
"using all wikipedia revisions. features extracted from the metadata of revisions allow all wikipedia article revisions to be processed because of their relative simplicity compared to the revision content. [cit] explore a variety of features generated from the metadata of all wikipedia article revisions for detecting vandalism. the reputation features on article, user, category, and country show interesting variations and sources of vandalism."
"in these equations, q k and b k respectively denote the quality level requested for segment k and the buffer filling at the time of that request. the components drive the agent to higher quality levels, less switches and higher buffer filling. since the highest priority is to avoid video freezes at any time, a strong penalization is given to an empty buffer. the exact penalization value is of less importance, but has to be significantly higher than the other components of the reward function. the total reward function can be defined as specified in (4)."
"counter-vandalism bots and counter-vandalism applications also suffer backlash from users (see section 8), which could be attributed to incorrect identification of vandalism. these bots-in particular, cluebot 1 and cluebot ng 2 -are evolving to use machine learning techniques to detect more sophisticated forms of vandalism, which takes time to learn correct cases of vandalism. the counter-vandalism applications that come with user-interfaces are changing their design to guide editors in identifying and softer handling of potential vandalism cases from incoming edits. some examples are huggle, 3 one of the most popular; stiki [cit], developed from research on user reputation for vandalism detection; and snuggle [cit], developed through research on user interface design and socialization of bots on wikipedia."
"the aim of cross-language learning is to overcome the limitation of the small data set size in many wikipedia languages. our hypothesis is that using language invariant features, we can use large wikipedia languages to learn and apply vandalism models to smaller wikipedias without needing to build classification models specifically for those wikipedias."
"as seen in fig. 7(a), the estimated waveforms (î * id,î * iq, v * id,v * iq ) are fitted with the calculated ones (î * id,î * iq,v * id,v * iq ). besides, the measured outputs v l are fitted with the predicted values v l, which verify the correctness of the proposed observer-based model prediction under the distorted parameters. specially, fig. 7(b) confirms the robustness in case of an unmodeled parameter (i.e., parasite resistance r f in series with the inductance l f ). as shown in fig. 7(b)"
"the rest of this paper is organized as follows. section 2 reviews the related work. section 3 describes the wikipedia data sets we use in our work, and section 4 details and ranks the language invariant text features of vandalism. section 5 describes our cross-language learning method, and section 6 summarizes and compares our results to the pan data sets. section 7 compares our results to related work, and section 8 discusses our findings, advantages, and limitations. finally, section 9 provides conclusions and future directions of research."
"for the pan-wvc-11 data sets, west and lee [cit] develop 65 [cit] pan workshop. these features are described generally as language independent, ex post facto (developed after recognition of vandalism), and language driven features. [cit] pan workshop for each language [cit] . however, classification in non-english wikipedia revisions showed very poor performance in the auc-pr scores (0.708 for german, and 0.489 for spanish) compared to english wikipedia revisions (0.822), but comparable performance in the auc-roc scores (0.969 for german, 0.868 for spanish, and 0.953 for english)."
"in the second category, the link prediction is defined as a two-class classification problem. in this regard, a feature vector is extracted for each node pair and a 0/1 label would be assigned based on the existence/not-existence of that link in the network. any similarity indices mentioned in the previous category could form the required feature vectors. then, any conventional supervised learning algorithms might be applied to train a supervised link predictor 9 . the third category is probabilistic based methods. the main idea is to optimize a target function in order to establish a parametric model that can best fit the observed data. the posterior probabilities are obtained by defining a conditional probability model over the learned parameters. an excellent survey on these categories can be found in ref. 6 ."
"overall, the regular revisions show bots are actively working in other languages and namespaces of wikipedia with activity similar to users working on regular revisions. we see bots sharing a large portion of the workload of over 7 percent, but with vandalism detection, there is significantly lower usage of bots in nonenglish wikipedias. nevertheless, bots are an important resource for wikipedia across its languages, and their contributions to vandalism detection cannot be ignored or neglected."
"the remainder of this letter is structured as follows: first, section ii describes the proposed client quality selection algorithm (section ii-a) and the design of the environmental state and reward definition (section ii-b). in section iii we define how the self-learning approach is evaluated (iii-a) and how the experiments have been performed (iii-b). the description of the evaluation results follows in section iii-c. finally, section iv presents the conclusions."
". in this formula, br i k denotes the bitrate of segment k in quality level i. for the learning agent to be able to converge, each simulation consisted of 400 episodes of streaming a 10 minutes video trace. to evaluate the results in the converged state, only the last 50 episodes are considered."
"for the same language of training and the same editor types for testing (bold diagonal language entries of tables 6 and 7), we only find t-test p-values greater than the 0.05 level. thus, there is no statistically significant difference when learning vandalism from both bots and users or each individually."
"the benefits of cross-language learning of vandalism is to generalize classification models to wikipedia languages without sufficient cases of identified vandalism to learn from. our results show that learning from languages with many instances of vandalism, such as english, does generalize well to smaller wikipedia languages. this means past and future work on feature engineering for vandalism detection in the english wikipedia can be used on other languages without statistically significant loss in classification quality. our results also show that related languages (such as english and german, and spanish and french) are less affected by cross-language learning, where classification quality seems to be dependent on the target language."
"however, employing multiple meta-paths raises another challenging issue that is how they could be contributed with different semantics. for instance, assume dblp network schema, as shown in fig. 1, is given in order to investigate co-authorship dependencies between a pair of authors, namely (a i, a j ). three different dependencies with three various semantics between a i and a j are demonstrated in fig. 3 . the first dependency is described by the first meta-path seeks the similarity of two authors based on the citations between them. while the second and the third metapaths consider the published papers between two authors with the same keyword(s) and same publisher as the similarity search patterns, respectively. a, p, v, and t stand for author, paper, venue, and term, respectively. the first meta-path, 1 , where two object types, i.e. papers (p) and terms (t), declare the authors dependencies based on their publications with similar keywords. roughly speaking, such dependency states that two authors are more likely to have a future collaboration if they publish more papers with similar keywords. through the second meta-path,  2, the authors dependencies are defined by their direct citations. the third dependency is defined by the third meta-path, 3 , which is an asymmetric meta-path including three object types. accordingly, the published papers by the same venue and the published papers with the probable similar keywords describe another semantic dependency between two authors. since each meta-path has its own number of object types with a specific length, integrating all the semantic information provided by each meta-path is not straightforward."
we compare our proposed features and features directly from pan workshops by repeating our experiments with only these isolated sets of features. our proposed diff based features are those described in sections 4.1 and 4.3. we isolate these sets of features and repeated our experiments. we compare the combined classification scores (as in table 8 for all features) for different subsets of features.
"features f04 to f09: similar to the line counts, we count the changes of words before and after a repair. these changes in the words of the repair show the subtler cases of vandalism that modify specific words. the difference of word lengths and number of words show the extreme changes needed to repair vandalism, whereas the ratios show the relative size of changes needed for repair. similarly, the lengths and the counts of the unique words show the relative change in size and the absolute number of changes needed in repairing vandalism. these combinations ensure that we can identify some of the repairs made by bots and users of subtler vandalism."
"in this subsection, the load step-change scenario is selected as the most critical case of load disturbances to investigate the dynamic behavior of the controlled system. then the fig. 8 . the zoomed-in windows are shown on the right side with the reference waveforms for an easy evaluation on the control performance."
"we present plots summarizing the classification scores in fig. 4 for classification within the same language, and fig. 5 for the average scores for classification out of language. we include our experiments on the pan baseline data sets as a comparison."
"overall, we have answered our research questions with some interesting results. our evaluation over all revisions of each wikipedia language shows more comprehensive and better results than sampling. we have shown bots and users differ in identifying vandalism, and that contributions of bots are important when analyzing vandalism on wikipedia. from our discussion, the trust of users in bots is lacking [cit], despite the high recognition of vandalism by bots. as we build better counter-vandalism bots, we will also aim to develop social aspects of bots to gain the trust of wikipedia users [cit] ."
"the error dynamics model (4) is simplified with nominal parameters, which will be used for model-based prediction in the proposed fs-mpc in section iv-b. to enhance the control robustness, the model-based prediction is proposed from (4) using the steady-state kalman filter-based observers for"
"we presented a comparison of bots and users in the vandalism detection task on wikipedia across five languages. vandalism is a major issue on wikipedia, where bots are increasingly being used. we compared how bots and users differ in their identification of vandalism by learning from their identified cases. we developed text features that include features commonly used in vandalism detection tasks, and use the classifier to rank these features by their importance to bots and users across different languages. we generated training and testing data sets based on languages and editor type, and evaluated the classifier on their combinations. we showed and discussed differences in the identification of vandalism between bots and users across different languages. our comparison to related work showed that our techniques are comparable and often achieve better performance on the entire wikipedia data set compared to previous research. our contributions showed we can learn vandalism from one wikipedia language and apply a classifier to other languages with only a small loss in classification quality. contributions of bots need to be acknowledged in research as bots are essential tools for wikipedia to manage content quality."
"features f21 to f25: these features look at the ratios of letters to words. we select these features with definitions from velasco [cit], but apply them with modifications to the equations as need to suit the word level instead of the document level. we take the maximum or minimum of these ratios for each word as a strong indicator of vandalism."
"to obtain precision, a similarity score is calculated for each node pair. after sorting the scores, if there are l r links belonging to the test set among top-l candidate links, then precision is obtained as ref. pathsim. given a symmetric meta-path , pathsim gives a similarity score to a node pair (x, y) with the same type as: table 7 . some statistics of the selected network samples."
"meanwhile, fig. 10(b) shows the experimental performance of the proposed fs-mpc (scheme 2) with sinusoidal voltages in the steady-state. in the transient-state by a load step-change, the recovering time of the scheme 2 is 1.8 ms and its voltage dip is 25.0 v. in this figure, the measured sse is 0.3 v rms and the thd is 0.8 %. once again, the experiments verify the reduced thds and sses of the proposed fs-mpc compared to the conventional fs-mpc by constraining the stability as explained in the simulation results. this sudden load change is typically used as a critical condition to evaluate the voltage quality of an ac power supply. in addition, the small voltage dip and fast recovering time are needed to meet the high requirements of critical loads such as data center and sensitive modern electronics loads."
"we begin by discussing bots and editing applications used for vandalism detection to show the problem of detecting vandalism in the larger context of the wikipedia community, and then summarize vandalism research by the type of data sets."
"for users in each language, we find consistent high performance on vandalism identified by bots for most languages. this suggests users look for similar patterns of vandalism as bots. the numerous users in the english wikipedia identify a higher portion of vandalism across languages than users from other languages. this suggests with more users, more vandalism patterns can be identified."
"for the full wikipedia data sets, the results of combinations of training and testing data are presented in table 6 . the rows of the table are the language and user type data set a classifier is trained on, and similarly the columns show testing set for the classifier. we show in bold results of the same language and the same user type of the training and testing set, and also the highest scores of each column."
"similarly to the previous section, we find no statistical significant difference when comparing the rows of table 7 to rows of table 6, with the exception of russian bots. combining training data from all languages from bots or users, and both, we also find no differences at the 0.05 level. this shows there is no difference in learning vandalism from bots and users across all languages considered."
"cross language learning of vandalism means to train the classifier in the training set of one language and apply it to the testing set of another language. it is a form of transfer learning [cit] which has strong advantages for smaller wikipedias that do not have the user base to identify and repair vandalism. these few vandalism cases result in low quality vandalism data and a vandalism class imbalance, which are both significant problems in non-english wikipedias. however, both problems can be address with extracting appropriate features [cit] and feature selection [cit], which our features demonstrate in section 6. cross language application of classification models has been successful for metadata level vandalism detection on wikipedia in our past research [cit] ."
"recent researches [cit] pay much attention to significantly improving the voltage control performance of the pulse-width modulation (pwm) vsis that employ the internal model-based disturbance observers [cit], repetitive control [cit], and disturbance-observer based cascaded control [cit] . by utilizing the available model of the system, signals noise or disturbances, these methods improve the control performance on achieving the reduced thds of the controlled voltages/currents, small sses, and some robust performance. however, all the mentioned methods are developed based on the assumption of unconstrained continuous-inputs. meanwhile, the discrete nature of switching-states and its control-input limits are not considered in the control design. in fact, the pwm vsis generate discrete pwm voltages with limitations up to the dc link voltage, so an advanced control design method which can utilize the available models, discrete nature of pwm inverters, and control-input constraints is expected with promising performances."
"n recent years, over-the-top (ott) video delivery, where content is transported over the best-effort internet, has gained a lot of popularity. for ott video, http adaptive streaming (has) is becoming the de facto standard. the general has concept is illustrated in fig. 1 . a has video file consists of multiple segments with a typical length of 2 to 10 seconds, encoded at multiple quality levels and resolutions. at the client side, the information about the video segments and quality levels is provided in the form of a manifest file. a standard has client requests a video segment on arrival of the previous segment. based on the perceived network state and the information in the manifest file, a quality selection heuristic dynamically adapts the requested quality level. each segment is downloaded in a progressive manner, while at the client side a buffer is used to bridge temporary anomalies such as a late arrival of a video segment during a small time window. finally, the video segments, stored in the buffer, are played back as a single continuous video stream."
"bots and editing applications. bots are an integral part of wikipedia because they provide automation to repetitive and mundane tasks, but their contributions are often ignored in research or by the wikipedia community [cit] . for example, activities of some bots do not appear on the list of recent changes provided by wikipedia [cit] . the prolific editing activity of bots and their discreetness have lead to mistrust by some editors because the perceived aggressiveness of bots in completing their task without regards to the social dynamics of the editing communities surrounding each article [cit] . interruptions of bots in tasks such as detecting vandalism can greatly increase exposure and longevity of vandalism, but they also show the resilience of wikipedia to eventually restore order [cit] . the importance of bots to wikipedia is seen through their editing contributions and their influence on the editing culture of wikipedia through interactions with users across many languages of wikipedia [cit] ."
"the random forest classifier has shown good classification performance for vandalism detection [cit] including in cross-language vandalism detection [cit] . to maximize performance, we conduct a grid search with 10-fold cross validation on the training data over a wide range of the classifier parameters for each language, such as the number of estimators (trees in the forest), maximum number of features, minimum number of samples per leaf, minimum number of samples for split, and minimum density."
"in addition to these similarity measures, meta-path based topological features have been conducted in a number of works for similarity search. collective classification in heterogeneous complex networks 17, social link prediction in multiple partially aligned social networks 18, co-author relationship prediction in heterogeneous bibliographic networks 13, and a meta-path based prediction model based on a topic discriminative search space 4 are some of the interesting applications of similarity searches using meta-path based topological features. the proposed meta-path based similarity indices suffer from two major drawbacks. firstly, the similarity indices are strongly dependent on the amount of reachability between the nodes while they do not consider the information within meta-paths. therefore, they tend to bias to highly visible or concentrated objects. secondly, most of these indices are originally designed for a single and usually symmetric meta-path. in other words, even though a set of useful meta-paths might be available, benefiting all the meta-paths to enhance the quality of predictions is not straightforward."
"note that the dynamic model (1) is considered as a second-order system with input v i and output v l . therefore, the two-step ahead prediction of the controlled outputs is employed in the proposed fs-mpc. for convenience, the superscripts ( ++ ) and ( + ) are used to denote the predicted values in the time steps (k + 2) and (k + 1) in the following section, respectively. the horizon-one prediction in (11) shows its effectiveness with least prediction-steps, which is widely applied in direct mpc in power electronics area. however, despite the reduced online computation, the control performance and stability are two critical issues of the fs-mpc in the literature [cit] ."
"as seen in fig. 8(a) and (b), both investigated schemes show a good performance on the reference voltage tracking and transient response after a load step-change. a fast recovering time after the same voltage dips is observed from both schemes within 2.2 ms, which represents the traditional fast dynamic response of the fs-mpc as shown in the literature [cit] . in the steadystate, the differences between two schemes are observed with smaller steady-state errors (sses) from the proposed scheme. the sses based on root-mean-square (rms) values are measured with 0.65 v rms and 0.2 v rms for scheme 1 and scheme 2, respectively. as seen in fig. 8(b), the proposed fs-mpc shows the superiority on removing the ripples from the v ld and v lq waveforms. then, the measured thd fromscheme 2is smaller than the one from scheme 1 (i.e., 1.1% and 0.7% in the full load condition for schemes 1 and 2, respectively)."
"in order to allow fair comparison between the deterministic mss algorithm and the proposed approach, an objective video quality metric is required. de [cit] define the qoe of video delivered using has to be dependent on the average segment quality and the standard deviation of the segment quality. using this quality level model, its parameters were tuned based on the results of a small subjective test by experts in the field of multimedia streaming."
"in the second class, assuming a meta-path is given, the objective is to explore new similarity measures or to develop efficient similarity searches. there are a number of meta-path based similarity measures proposed in the literature. two basic measurements, named as path count (pc) and random walk (rw) 12, are based on the number of path instances between the given node pairs 13 . the higher the pc and rw in value, the more the similarity can be obtained. hetesim (hs) was proposed in ref. 14 as a new path-based relevance measure. it is a symmetric and self-maximum measure which has a much smaller computational complexity than simrank 15 . another similarity measure, called as pathsim (ps), is only applicable on symmetric meta-paths 16 . the basic idea is that two similar objects not only be strongly connected, but also share comparable visibility. in comparison with random walk based measures, pathsim is able to find more meaningful similarities."
"features f00 to f09 are generated from the revisions before and/or after a repair. features f10 to f20 are generated from the words changed in the repair, which isolate possible vandal words and captures distributions of words in the repair. note that duplicate words can exist and we count these in some features. features f21 to f31 are applied on each word that was repaired, where we select for values that indicate vandalism. although some features are derivatives from related work, we justify their novelty by our application to lists of single words-further polarizing vandalism cases-and show their effectiveness on the full wikipedia data set."
"our contributions are (1) developing novel text features that capture language invariant aspects of vandalism, and have greater effectiveness compared to features from related work as demonstrated by a statistical test and feature ranking; (2) contrasting the differences between bots and users by learning vandalism identified by bots and users; (3) demonstrating that cross-language application of classification models do not have significant loss in classification quality; (4) conducting our experiments on the entire wikipedia data dumps (over 500 million revisions), which comprehensively includes all random samples of revisions in the pan baseline data sets; and (5) replicating our experiments on these much smaller baseline data sets, showing and contrasting the performance of features often used in related work on these data sets and on the full wikipedia data dumps."
"most research studies on the pan data did not balance their data sets. the ratios of regular revisions to vandalized revisions are 10:1 and 13:1 [cit], respectively. hence we observe high auc-roc scores because of many non-vandalism cases being correctly classified. looking at the auc-pr for the pan data sets, our classification results in table 8 are higher for the matching languages, suggesting a comparable performance in identifying true cases of vandalism, at the cost of a lower recall rate (seen in lower auc-roc scores). the lower auc-roc scores for the classifiers suggest we may have more false positives of vandalism. our results show that we have obtained classification performance comparable to related work (see table 9 ), while demonstrating the differences between bots and users, and learning across languages."
"as illustrated on the top of fig. 9(a), the conventional fs-mpc (scheme 1) shows the steady-state error (sse) in the phase-u voltage with a measured value of 0.6 v rms and the thd of 1.5 % in the steady-state with a three-phase rl load. as shown on the top of fig. 9(b), the proposed fs-mpc (scheme 2) under the same rl load exhibits smaller sse of 0.3 v rms (i.e., largest on phase-u), less ripples observed from the voltage waveforms which results in smaller thd of 0.9 % on the same load conditions and parameter distortion."
"however, most real networks compose of different types of nodes which opened a new research topic called as heterogeneous networks. as a consequence, most of the link predictors proposed in homogeneous complex networks become infeasible in heterogeneous ones. note that, in a heterogeneous complex network, two objects might be connected via different paths while the semantic underneath them are not identical. in this regard, a meta-structure known as meta-path has been proposed in order to exploit nodes dependencies in heterogeneous complex networks. meta-path is a sequence of node types that acts as a similarity search pattern between node pairs. without restriction on either the structure or length of the meta-paths, the number of possible meta-paths is unbounded. however, generating informative meta-paths and selecting the best set of them are some of interesting issues in meta-path based similarity searches. accordingly, the studies on meta-paths are roughly divided into two classes."
"for bots, we find some of our new features are consistently important for most languages. for example, features f01 and f00 both show cases of mass deletions and insertions, respectively. feature f24 is important for german, spanish, and russian wikipedias, indicating high uses of non-alphanumeric characters in vandal words. features f04 and f07-important for the english and spanish wikipedias-show the total difference and ratio of lengths of words before to after the repair, which indicates many insertions of vandal words in sentences and insertion of long words in the case of the french wikipedia. interestingly, slang words is one of the most important features in the english wikipedia, indicating frequent use in vandalism cases. in general, bots identify vandalism features that show changes in text and word sizes, and introduction of vulgar or slang words. 7 . http://www.wiktionary.org/ 8. http://docs.scipy.org/ for users, we see a common set of important features across most languages, namely the word modification features f04 to f07, and in particular f05 for all languages. feature f05 suggests the vandal words are disproportionate in ratio size to the repaired words. these features-f04 to f07-suggest vandal words are out-of-place with respect to the sentence they were in and these types of potentially subtle vandalism are consistently being identified by users across all languages."
"in future work, we plan on looking at the contributions of anonymous users in identifying vandalism, as they are an understudied group of users because of difficulties in assigning an identity. the languages we chose are closely related to each other, so we would like to explore different languages, such as arabic and mandarin chinese to complete the united nations working set of languages. non-european languages may need very specific techniques in tokenization or specific features need to be developed for vandalism [cit] pan-wvc-10 0.737 0.958 [cit] pan-wvc-10 0.731 0.946 [cit] pan-wvc-10 0.525 0.915 [cit] pan-wvc-10 -0.955 [cit] pan-wvc-10 -0.930 [cit] pan note that there are significant differences in data sets and techniques."
"a more comparable recent study is from our past research [cit], where we developed wikipedia vandalism data sets from article revisions and views. the data sets are balanced for classification and contain only metadata and temporal features. in our past study, we did not consider the contribution of bots, nor looked at content features for vandalism detection. we focused only on content features in this work mainly because we see these features as better discriminators of bots and users, because vandalism detection is mainly conducted on content only. in future work, we plan to incorporate metadata features to further analyze differences between bots and users. bold entries are the same match ups of languages, and the highest score in each column."
"furthermore, [cit] define the influence of the frequency and duration of video freezes on the estimated mean opinion score (mos). while the proposed formulation only considers three discrete levels of freeze frequency and length, we use a continuous interpolation of these levels to measure the influence of freezes on qoe. the result of this interpolation is given in (5), where f freq and f avg represent the number of freezes relative to the number of segments and the average duration over all freezes respectively."
"these novel features are modified from related work to suit our word level analysis, instead of the full content of articles. in a sentence difference, we expect a single oddity in a word to indicate vandalism, hence we do not aggregate or average values as a vandal can avoid detection by simply masking vandalism with unrelated but legitimate words."
"the higher failure of k-s tests may be explained by the pan data sets containing more difficult or ambiguous cases of vandalism that require manual analysis. this means the features may be capturing specific types of vandalism that are abundant in the full wikipedia data sets but not the pan data sets because of different vandalism selection methods. the k-s test only provides an indicator of the effectiveness of features, and thus we advocate for evaluation of features on both the pan data sets and the full wikipedia data sets, as we have done in this paper."
"this section develops the observers using a steady-state kaman filter to enhance the robustness of the model-based prediction. the error dynamics model (4) is used not only for predicting the system states but also for developing the asymptotic stability conditions in section iv-a. to estimate i * i, the following dynamics equations are extracted from (3) and (4) aṡ"
"sampling wikipedia and small wikipedias. vandalism detection research is often performed on samples of the english wikipedia. a featureless compression method for detecting vandalism is presented by itakura and clarke [cit] on randomly selected articles. words can be predictors of whether an article will be reverted as demonstrated by rzeszotarski and kittur [cit] . revisions made by bots are analyzed, but evaluation and comparison of classification performance is only for revisions of one wikipedia article."
"we generate our features from words extracted from the difference of the content of the repaired revision with the previous revision, which contains vandalism. from the diff algorithm, we have lines (separated by periods) unique to the revision before the repair, lines unique to the revision after the repair, and the lines changed in the repairing process. we ignore common lines to accurately determine changes in content. the common lines can show the ratio of the vandalized content to normal content, but for cases such as mass deletes, the size of lines unique to the repaired revision is sufficient to show this case. we further perform a sentence difference to extract vandal words that were repaired. our text processing uses unicode (utf-8) encoding and language specific alphabets. all features are shown in table 3 with a summarized description, an average time of generating features in milliseconds (ms), and a kolmogorov-smirnov (k-s) statistical test [cit] (described in section 4.4). we order our features in groups of relatedness, where bolded features are our novel contributions to detecting vandalism. note that our features are applied specifically to diff words instead of the full diff of revisions as in previous works. [cit] workshops [cit], where they first appeared for the use of detecting vandalism."
"at first, we introduce some basic definitions including heterogeneous complex network, network-schema, meta-path, and path-instance. then, some discussion are given about the generation and selection of meta-paths in heterogeneous complex networks. afterwards, the proposed model would be introduced."
"in this paper, we investigate these questions by learning vandalism collectively recognized by bots and users, and evaluating these models against both bots and users across 500 million revisions from five different languages: english (en), german (de), spanish (es), french (fr), and russian (ru). we propose a new set of computationally efficient features that are language invariant, and have classification performance comparable to the previously proposed features. we show bots and users have similar vandalism identification scores when we apply them on the other's recognized set of vandalism cases. furthermore, we show that combinations of vandalism classification models generalize well across languages without statistically significant loss in classification quality. to strengthen our results, we replicate our experiments on the baseline vandalism data sets of approximately 62;000 revisions from competitions held for the pan workshops [cit], and discuss limitations with these data sets."
"this subsection investigates the robustness of the proposed observed-based prediction in case of mismatched models by setting the actual values (l f, c f ) which are distorted from"
", etc. the majority of link prediction approaches have been proposed on homogenous complex networks. they are divided into some categories as local/global similarity indices, supervised, and probabilistic methods. in the first category, the aim is to extract some local (node-based) or global (path-based) similarity features for vertices or links. common neighbors (cn), jaccard (jc), prefrential attachment (pa), adamic adar (aa), and resource allocation (ra) are among popular local indices, while katz, leicht-holme-newman, average commute time, random walk, and simrank are known as global indices 6 . while the local indices are simple in computation, the global ones may provide more accurate predictions. recently, the integration of both node and link based topological information has been studied by introducing local community paradigm (lcp) 7 . accordingly, two nodes are more likely to be connected if they have some common neighbors belonging to a densely formed local community. the authors proposed cannistraci variations of cn, jc, aa, ra, and pa called as car, cjc, caa, cra, and cpa. it has been demonstrated through extensive experimental evaluations that lcp based indices could provide better performance predictions compared to other conventional indices. this approach has been also successfully extended on the bipartite complex networks 8 ."
"as a further investigation, we combine the training data of bots and users for each language, for each editor type, and for all languages and both editor types. these classification results are presented in table 7 . this investigates the common practice of learning vandalism without distinguishing contributions of bots and users. by learning from both bots and users for each language, we find some differences in classification performance. related works do not make this distinction, which can result in higher classification scores because of the predictability of bots in detecting specific types of vandalism."
"for the correct predictionx on v l . consequently, the controlled outputs can track the reference values correctly, which proves the robustness of the proposed observer-based prediction against parameter uncertainties and an unmodeled parameter."
"the initial approach to a self-learning has client [cit] consisted of an environment model with more than 2.5 million states, leading to issues in terms of convergence. furthermore, the vast environment model made the client unapplicable in situations with variable bandwidth. in the proposed learning agent, the state is constructed by only two parameters, found to be essential to model the networking environment: the available bandwidth perceived by the client and the current client buffer filling level, i.e. the total duration of the segments stored in the client buffer. both of them are continuous values, which need to be discretized to be modeled as state variables. the value ranges and number of discretization levels of these state elements are shown in table i . in this table, b max denotes the maximum client buffer size in seconds while t seg and n respectively denote the segment duration in seconds and the number of quality levels. bw max is the highest possible throughput, e.g. the physical link capacity."
"this paper proposed a new fs-mpc scheme with the stabilized direct search and robust prediction in a three-wire 3-phase vsi system. the proposed algorithm is quite simple and flexible thanks to a direct search of the fs-mpc algorithm. the proposed fs-mpc improved the control performance by the constrained stability, which can significantly reduce the thds (less than 2.2% under nonlinear loads) and sses (less than 0.5%) to achieve a high-quality ac power supply. in addition, the observer-based prediction ensured the robustness of the proposed fs-mpc under parameter mismatches and an unmodeled parameter by both filtering the noises and estimating the unmodeled terms. both comparative simulated and experimental results were presented to prove the advantages of the proposed mpc under a wide-range of parameter uncertainties (±100%) and critical load changes to maintain the high-quality voltage supply. this algorithm can be extended to the fs-mpc used in other power electronics applications such as motor drives and power converters. for further works, other kinds of online optimization algorithms can be combined with stability conditions to target the desired control performance. his research interests include electric machine drives based on microprocessor, control of distributed generation systems using renewable energy sources, and power conversion systems and drives for electric vehicles. his research interests include dsp-based electric machine drives, distributed generation systems using renewable energy sources, and power conversion systems and drives for electric vehicles. volume 7, 2019"
"we use the pan workshop data sets in our research as a baseline comparison. the samples in the data sets do not have many revisions made by bots to learn from. the pan-wvc-10 data set contains 14 bots with a total of 101 revisions (0.3 percent), where one bot is a counter-vandalism bot that made a total of 25 revisions (0.07 percent). the pan-wvc-11 data set contains a total of seven bots across three languages, with a total of 34 revisions (0.1 percent), where one bot is a counter-vandalism bot that made a total of five revisions (0.02 percent). clearly, we cannot effectively learn and compare bots and users with these few revisions made by bots."
"we also present our results for training on the balanced data set 1:1 (tr), and applying to the unbalanced testing sets 10:1 (te) and 13:1 (te). these results simulate the real-world effects of learning on a balanced data set and applying to a non-balanced data set, such as in the full wikipedia corpus."
"west and lee [cit] evaluated their set of vandalism features on the multilingual pan-wvc-11 data set. the classifiers are evaluated within the same language, showing a lower auc-pr score when applied on german and spanish. this suggests the range features as developed with the english samples as the focus may be too broad, or simply are not suited to the differences seen in the german and spanish samples. our set of text-based features shows high auc-pr scores for spanish, and across languages."
"peter christen received the diploma degree in computer science engineering from eth z€ [cit] . he is currently an associate professor in the research school of computer science at the australian national university. his research interests include data mining and data matching (record linkage). he has published more than 100 [cit] the book data matching published by springer. he is the principle developer of the freely extensible biomedical record linkage (febrl) open source data cleaning, deduplication, and record linkage system. \" for more information on this or any other computing topic, please visit our digital library at www.computer.org/publications/dlib."
"when looking specifically at the testing data of users for each language (users columns), we find there is a difference in classification quality between the row of bots and users for each language, with many t-test values less than the 0.05 level. this suggests that there is a difference in how bots and users recognize the vandalism identified by users across languages. however, we do not see this difference between bots and users for the vandalism identified by bots (using the bots columns). this suggests users identify a wider range of vandalism that includes vandalism that bots can identify."
"an advantage of our approach is immediate text analysis of a revision with its previous revision to determine vandalism. we do not need additional metadata, derived data, and profiling of users to determine vandalism. our new textbased features show comparable performance and improve on work that was based on samples of wikipedia revisions. our chosen features are specifically designed to generalize to the languages considered, which is reflected in the classification performance."
"transporting the video segments over http provides both seamless interaction through firewalls and reliable delivery. on the other hand, the best-effort nature of the internet makes these http-based techniques prone to bandwidth fluctuations manuscript received november 29, 2013. the associate editor coordinating the review of this letter and approving it for publication was g. reali [cit] . in this way, a common ground was established between the vast amount of available implementations. the bitrate adaptation heuristics are, however, not standardized, and thus implementation specific."
"we show the increasing use of bots to detect vandalism each month in the english wikipedia in fig. 1 . in the other wikipedia languages, we do not see this trend because there may be a bias towards developing bots for the english wikipedia, a mistrust of bots, or a smaller number of articles for each editor to maintain."
"in the first class, the focus is on efficient discovery or selection of meta-paths, especially in large scale networks. automatic discovery of meta-paths in large scale complex networks has been studied in ref. 10 . the users are asked to provide some examples of node pairs that exhibit high proximity. then, a greedy algorithm would be employed to generate the meta-paths that can appropriately explain the relationship between the example pairs. the experiments on real-world heterogeneous information networks, dblp and yago, show the effectiveness of the method in finding some important meta-paths. the difficulty of discovering reasonable meta-paths by human in large scale complex networks has been discussed in ref. 11 . here, the most interesting meta-paths are discovered based on conventional knowledge discovery principles from the hundreds-of-thousands of possible choices."
"vandalism is often caught and repaired quickly [cit], but the number of cases of vandalism grows in proportion to the fast growth of wikipedia. our large data sets (discussed in section 3) totalling over 500 million revisions of over nine million articles show editors identified an average of over 2;100 [cit] for the english wikipedia. to identify and repair this many cases each day, automated vandalism detection programs-known as bots-have been developed to partially relieve the burden on editors. through keyword search of edit comments, bots (bot editors-0.67 percent) and users (human editors-1.33 percent) repair vandalism in nearly 2 percent of all revisions in the english wikipedia [cit] . this contrasts with other studies-using crowdsourced votes from manual inspection of a sampled set of revisionsshowing vandalism may appear in 7 to 11 percent of all revisions [cit] . these missing cases of vandalism (approximately 5 to 9 percent) suggest very difficult or ambiguous forms of vandalism that may require up to eight rounds of majority consensus from three different annotators in each round [cit] ."
this subsection proves the existence of at-least one finitestate of the 3φ-vsi which can asymptotically stabilize the controlled outputs provided that the stability is guaranteed by the continuous feedback laws.
"we borrow these features directly from the winners of the pan workshops, where they have been often used by related work (see section 2). the features are adapted for our data sets where needed and we provide clearer sources for vulgar and slang words. features f10 to f12: three types of words common or indicative of vandalism are pronouns, slang, and vulgarity. we extract these words from wiktionary 7 for each language, where available. for all languages considered, we have 105 pronouns, 8;465 slang words, and 2;250 vulgar words. we search for all these words in the sentence diff for all languages. for example, if english vulgarities are used in german vandalized revisions, these vulgar words are counted in the features for the german revisions. these features have previously been used in related work [cit], but for english only and with an unknown source of the vocabulary. our visual inspection shows that vulgar and slang words are not likely to be benign words in other languages. interestingly, some vulgar words from other languages are included in english."
"inspiring from these studies, we have developed a new mutual information model for link prediction in heterogeneous complex networks. the proposed model estimates the link likelihoods via introducing the meta-path based link entropy following a given meta-path. furthermore, it provides an information-theoretic framework such that multiple meta-paths are employed to facilitate link prediction by providing different semantic information about the target node pairs. before introducing the proposed model, we recall two basic definitions from information theory 27 . definition 5. let x be a random variable and x be an outcome of x with probability p(x). then, the self-information of x quantifies the uncertainty of the outcome x and is defined as follows:"
"the use of counter-vandalism bots is changing the way wikipedia identifies and bans vandals [cit] . however, contributions by bots are often not considered nor discussed, despite their importance to wikipedia and some bots becoming the most prolific editors [cit] . the increasing delegation of vandalism detection to bots poses interesting research questions: how do the detection rates of bots and users compare to each other, and how do they differ across different wikipedia languages?"
"features f00 to f03: these features are a count of types of lines from the diff algorithm. high counts of unique lines in the vandalized revision (before the repair) indicate mass insertions, and high counts in the repaired revision (after repair) indicate mass deletions. the count of line changes indicates small changes that may show vandalized insertions or changes of text."
"we present our classification results as the area under the precision-recall curve (auc-pr) instead of the area under the receiver-operator characteristic curve (auc-roc), following the study of the relationship between auc-pr and auc-roc by davis and goadrich [cit] . the precision-recall (pr) curve plots the fraction of vandalism that is truly vandalism (precision) against the fraction of vandalism that is correctly classified (recall) by the classifier. thus, the auc-pr gives the probability that a randomly selected case of true vandalism is correctly labeled by the classifier. auc-roc gives the probability that a randomly selected revision contains vandalism. auc-pr is an alternative measure to the auc-roc that is often used to evaluate binary classification problems [cit] . davis and goadrich [cit] demonstrates that a binary classifier with a curve that shows strong performance in auc-pr scores will also show strong performance in auc-roc scores, but not vice versa. this is evident in related work that promotes strong performance in auc-roc scores, but have poor auc-pr scores (as we show in section 7). this shows the effects of unbalanced classification classes not being considered. our classification results are for balanced classification classes, but we demonstrate in section 6.5 that auc-pr scores do not decrease significantly for unbalanced classes. hence, we opted to present our results as auc-pr."
"this scenario is experimentally implemented to validate the control performance of the two control schemes under a nonlinear load step-change in both the steady-state and transient-state. especially, the control performance under the state-variable constraints (i.e., maximum phase voltage is limited within ±20% of reference voltage at ±186.7 v peak ) is also verified to highlight the advantages of the fs-mpc schemes in comparison with the continuous-input design method. this validation can be considered as the most challenging case for a vsi system that supplies ac voltages. the distorted nonlinear loads become popular with the emerging installation of complicated electronics-based devices in modern applications. in this regard, the thd is an important index. also, owing to the nonlinearity of the loads, the voltage and current response in the transient-state should be monitored by the controller. at this point, the fs-mpc exhibits its unique advantages on regulating the physical constraints of the controlled system such as controlled inputs, output voltages, and inverter currents. fig. 11(a) shows the experimental waveforms captured from the conventional fs-mpc (scheme 1). in this figure, the recovering time is measured with 2.8 ms and its dip voltage is 55 v after the load step-change. in the steady-state under a nonlinear load, the controlled output voltages of the conventional fs-mpc contain some distortions due to the high harmonic components of the nonlinear load currents with the thd of 3.2 % and sse of 0.9 v rms, respectively. next, fig. 11(b) shows the experimental control performance of the proposed fs-mpc by constraining the stability. in the transient-state, the proposed fs-mpc exhibits a fast dynamic response with a recovering time of 1.8 ms and a reduced voltage dip of 43 v. in the steady-state under the same nonlinear load, the thd and sse are 2.2 % and 0.3 v rms, respectively. thus, the proposed fs-mpc method shows a better control performance, especially reduced thds and sses in comparison with the conventional fs-mpc."
the cross-language application of classification models is a type of transfer learning [cit] . chin and street [cit] apply transfer learning to detect vandalism on wikipedia by learning vandalism from one article and applying the models to another article. revisions from the webis wikipedia vandalism corpus [cit] are segmented and placed into similar clusters. the best performing vandalism classification models built on each cluster are then evaluated on clusters from revisions of two selected english wikipedia articles.
"for bots in each language, we find they have generally high classification performance on vandalism identified by bots from another language. this suggests bots have consistent behavior, so there is little variation in the way they identify vandalism. when we applied these models to users in different languages, we find lower classification performance. this suggests users are identifying a wider range of vandalism types than bots, which is expected."
"while current quality adaptation algorithms are hard-wired to fit specific network configurations [cit], in this letter, a q-learning-based has client is proposed to dynamically adjust its behaviour by interacting with the network environment. applying learning enables the client to adapt its behaviour to network conditions that were not under consideration when designing the typical deterministic quality selection algorithms. using a network-based video streaming simulation framework, we performed extensive simulations of the proposed client for multiple video sequences with different dynamic scene variations. the simulations allow evaluation of the selflearning client in terms of convergence speed, performance and applicability. furthermore, the evaluations are used to compare the performance of the proposed self-learning client to the traditional microsoft iss smooth streaming (mss) algorithm."
"we also observe the same statistically significant difference when looking specifically at the testing data of users for each language (users columns); and the same non-difference of the testing data of bots for each language (bots columns). so, combining observations from bots with users may not improve detection performance for vandalism identified by users. this suggests users do identify a wider range of vandalism, where the contributions of bots may not be different across languages, but can provide some small improvements to classification performance."
"we sampled (in section 5) the overrepresented regular revisions from the random forest classifier because this allows more balanced decision trees to be built in the classifier to distinguish vandalism, reduces the size of the models and data needed for training, and reduces learning time. however, data sampling raises questions about bias in performance. we present the 1:1 (one to one) ratio of regular revisions to vandalized revisions in table 8, but we have repeated our experiments for the sampling ratios of 2:1, 4:1, 10:1, and 13:1. the ratios of 10:1 and 13:1 [cit] data sets, respectively."
"we use the random forest classifier from the python based scikit-learn toolkit [cit] to rank these 32 features by their importance. this is further statistical evidence showing the general effectiveness of our feature sets before use in classification. table 4 shows the top five features ranked by their information entropy (ie) scores (as used by the random forest classifier) for each language and for bots and users. the scores show the features that give the most homogeneous branches in the forest of decision trees (i.e. the amount of information gained after splitting on that feature in a decision tree). for example, for bots in the english wikipedia, we gain twice as much information when splitting on feature f01 (0.012) than on feature f07 (0.006), while for users the differences in the top five features are less. the ie scores are an average of 10 training iterations of the classifier."
we use the two-sample kolmogorov-smirnov statistical test [cit] from the scipy toolkit 8 to determine whether the features distinguish the regular revisions from the vandal revisions-from repairs made by bots and users-at the 0.05 significance level. the k-s test provides an indicator of whether features may be beneficial to statistical machine learning algorithms. we have 10 data sets for the full wikipedia (full) data set (five languages with bots and users for each language) and four data sets for the pan data set [cit] . we show the percentage of data sets failing the k-s test at the 0.05 significance level in table 3 .
"to apply rl on the has use case, the available actions, the environmental state and the reward function have to be modelled. the agent's action is to select one of the available quality levels to request for the next video segment, given the perceived network state. a state definition is needed to model the behaviour of the networking environment the agent interacts with. when defining the state definition, care has to be given to the introduced number of states. too many states will slow down learning and introduce the danger of overfitting, while too few states hamper the ability to correctly model the environment, leading to unsatisfactory results."
"within the same language and user type (diagonal bold entries), the classifier shows some of the highest scores amongst the language combinations. the exceptions are scores of the german and french bots, where the classifier trained on data of the english bots show better classification performance. this suggests english bots can identify more vandalism cases identified by bots in the german and french wikipedias than the german and french bots."
"on the bottom of fig. 9 (a) and (b), the comparative results prove the superiority of scheme 2 over scheme 1in the steady-state under a three-phase nonlinear load (i.e., a fullbridge diode-rectifier depicted in fig. 4 ). the measured values from scheme 2 are the sses of 0.2 v rms and thd of 2.3 %. meanwhile, the large values are measured from scheme 1with the sses of 0.8 v rms and thd of 3.1 %."
"the wikipedia data dumps contain revisions for every article, but we only use the encyclopedic articles (namespace 0) as these articles are the reason people access wikipedia. every edit made on an article on wikipedia generates a new revision with the full content of the article. when vandalism is discovered, it is usually repaired by correcting the vandalized content or by reverting to a past revision, which copies the past revision to become the current revision. in either case, the repaired revision may contain keywordssuch as \"rvv\" (revert due to vandalism), \"vandalism\", \"... rv...vandal...\", and analogues in the other languages-in its comment indicating vandalism was detected and repaired."
"a limitation of our work is its reliance on text features, which may not capture vandalism that is apparent when looking at metadata and user reputation features. our classification method uses an undersampling method to balance and reduce the size of the training data set. however, in section 6.5 we have shown that undersampling does not statistically affect classification results in a significant way by repeating experiments with different training and testing ratios. we have shown the performance of only one classifier, which although is commonly used for vandalism research, may not be the best for cross-language learning [cit] . our sets of features are language independent only for the languages considered. for some languages, such as mandarin chinese, many word based features are no longer useful because of tokenization issues and differences in the language. it is evident from the poor performance of the russian language model that other techniques or features need to be developed that are suitable for the language. vandalism is handled differently in each language community, and research is needed for non-english and especially non-european languages."
t is constant and ω is fixed) [cit] . note that the model (1) is simplified without zero components because the zero components of all phase currents do not exist in case of a three-wire 3φ-vsi system.
detection. our ultimate aim is to build the next generation vandalism detection bot based on machine learning approaches that can work effectively across many languages.
"recently, information theory has been employed for link prediction problem in homogeneous complex networks [cit] . the main contribution of these works is to measure the information provided by common topological features, such as common neighbors, instead of using them as simple topological features. in this paper, we propose a mutual information model to perform link prediction in heterogeneous complex networks. the proposed model, called as meta-path based mutual information index (mmi), provides an information theoretic framework in which multiple meta-paths with different semantics are mutually employed to improve the similarity exploitations. here, the link likelihood of a node pair is formulated as a conditional self-information of the existence of that link when a set of meta-paths are available. we have evaluated the proposed approach using a bibliographic network, dblp. the results of prediction accuracy under precision indicate the efficiency and validity of the proposed mmi method. it will be also shown that the efficiency would be kept even when the network sparsity or noisy connections are increased. the main contributions of this paper can be summarized as follows:"
"to find the best set of informative meta-paths, an exhaustive search is required in a search space with size of θ(k l ) which is a np-complete problem 10, 12 . thus, as the network schema grows in size, generating all informative meta-paths as well as selecting the best set among them become a nontrivial issue. however, all the possible meta-paths might not necessarily provide the proper information or meaningful semantics. in addition, it has been found that a typical meta-path might lose its importance as its length increases 16 . taking these facts into account, the negative impact of highly computational time meta-path generation and selection issues could be reduced by using a prior knowledge. it is important to mention that, using the most informative meta-path could not entirely explore semantic dependencies between node pairs. so, the contribution of selected meta-paths leads to more accurate link prediction. we will address the above mentioned issue in experimental results."
"the russian training and testing sets for bots are relatively small compared to other languages, as shown in table 2 . these few vandalism observations generally result in poor classification performance from all languages for the russian bots training and testing sets. however, the training set provides many common patterns for those few observations, where performance is poor compared to the training sets of other languages. the relatively large number of vandalism cases in the russian training set for users show higher classification performance on other languages."
"the continuous control inputs (v id, v iq ) based on the feedback control laws (12) can be presented via the following finite states [cit] :"
"over this topology, multiple video sequences with varying content types were streamed. each video sequence, encoded using the h.264 advanced video coding (avc) codec, has a duration of about 10 minutes, a segment length of 2 seconds and quality levels as shown in table ii . the different bitrates are obtained by applying a different quantization parameter (qp) in the encoding. the average relative rangewidth ω is defined as an indication of the dispersion of segment bitrates, calculated as the average over all quality levels i of the relative rangewidth"
"overall, there are differences in the importance of features for bots and users. bots seem to handle more prominent vandalism features such as mass insertions and deletions of text, and slang and vulgar words. features important to users are based on the changes made and the length of words used in the vandalized revisions. this suggests users are repairing subtle vandalism that requires deep inspection of words."
"owing to the importance of steady-state performance in constant-voltage constant-frequency (cvcf) control of the vsis in applications of high-quality power supplies, improved indexes on sses and thds under different types of loads help increase the efficiency of the supply system, ease the filter design, and reduce the hardware cost. this improvement ensures the high-quality power supply for ac loads at a reasonable price, especially for critical loads in ups applications. (i.e., 0% to 100%). as shown in fig. 10(a), the recovering time of the conventional fs-mpc (scheme 1) is 2.2 ms and its voltage dip is 35.0 v. as expected from the simulation results, the experimental results also show high thds and sses in the controlled outputs of the conventional fs-mpc."
"for prediction, the error dynamic model (5) in the matrix form is converted to the discrete-time model [cit] as 2) predict the state variables v le (k + 1), i ie (k+1) in onestep ahead using (11)."
"while the media bolster approvals of counter-vandalism bots, 9 signs of frustration by users are appearing in social media outlets such as reddit 10 and facebook. 11 this lead us to investigate the differences between bots and users in the task of identifying vandalism with the overall aim to develop more accurate vandalism detection bots based on features and user identified cases of vandalism. our results show that distinguishing the vandalism identified by bots and users show statistically significant differences in recognizing vandalism identified by users across languages, but there are no differences in recognizing the vandalism identified by bots. this shows humans recognize a wider range of vandalism patterns beyond the capabilities of bots with our considered set of features. while this result is intuitive, we now have evidence of bots identifying similar vandalism to users. this suggests bots are becoming more sophisticated by handling more and more non-obvious cases of vandalism."
"where ξ s and ξ i are set to large values (e.g., 10 6 ) when the relevant constraints are violated and to zero when the relevant constraints are satisfied. in this paper, the large value of ξ c is selected with 10 6 in comparison with the basic objective function of a performance index to make the priority for the stabilized switchingstates [cit] . then, the exhaustive search is based on (21) to find out the optimal control input. fig. 4 shows the block diagram of the proposed fs-mpc implemented with a ti tms320f28335 dsp. note that the fs-mpc methods do not need a modulation stage such as space-vector modulation (svm) to realize the continuous control inputs (v id, v iq ). instead, the fs-mpc directly offers the optimal switching-state s uvw without any modulation. in the literature, even if the fs-mpc is considered as a simple method, in return, the high thds and ripples are reported due to the lack of an svm method."
"in this letter, a q-learning-based http adaptive streaming (has) client is presented, able to dynamically adjust its behaviour to the perceived networking environment. by using only two state elements, a self-learning client was built that outperforms the deterministic traditional microsoft iss smooth streaming (mss) algorithm by up to 13% in a mobile network environment. furthermore, the q-learningbased client is shown to be very well suited to react to video shifts. since previously obtained knowledge can be reused when learning on a new video sequence, the learning agent can be trained offline in a variable bandwidth environment before being applied in an online has client. after an offline training phase of about 200 episodes, the self-learning has client is able to instantly adapt to network and video changes for all considered scenarios, making it applicable in a practical environment."
"we collect results of related work in table 9, where auc results are available. we compare these results within the context of knowing differences in data sets, sampling, and classifiers. we select the most appropriate results for comparison where possible, such as results for the random forest classifier, and similar sets of features."
"the performance of the mmi, when multiple meta-paths are given, depends on the contribution weight of each meta-path, as represented in eq. (9). since we have selected two meta-paths for each problem in the first experiment, the above equation for co-authorship prediction problem can be rewritten as: to study the impact of α on the precision of the mmi, it has been changed from 0 to 1. the the results are depicted in fig. 5 for both problems. referring to table 3, the meta-path 2  is more informative than  1 . it can also be drawn from the fig. 5 that the precision rate is decreased when we simply give high contribution weight to the second meta-path, 2  . that is, the higher the precision rate, the more the proper selection of α. in practice, the best value of α is obtained in α ≈ 0.3. the observation for citation prediction problem is similar except that the best value of α is found in α ≈ 0.4. that is the effectiveness of  3 and  4 in expressing citation relations are more close to each other compared to  1 and 2  in predicting co-authorship relations. to study the impact of choosing the number of top-l candidate links on the precision rate, l has been changed from 10 to 100 and the average results of 100 independent trials have been found, as shown in figs 6 and 7 . the results demonstrate that the mmi still keeps its higher ability of retrieving latent links when l changes, specially using the more informative meta-path. another interesting achievement is that the stability of the mmi in retrieving the latent links when l changes is improved by integrating both meta-paths. as depicted in fig. 8, the mmi overcomes all other indices in term of precision while it is robust to the changes of l."
"these experimental results clearly confirm the advantages of the proposed fs-mpc with stability constraints on the voltage regulation of a three-phase vsi with an output lc filter. to summarize the comparative experimental results, table 2 shows the comparative performance indexes between the conventional fs-mpc scheme and proposed fs-mpc scheme."
"for each row to each other row of results, we apply the ttest to find if there are any statistically significant differences in performance when learning across languages and editor types. we do not include the full matrix of paired t-test p-values for all row combinations. we find that in general learning on any language and any editor type does not show significant differences in classification performance across all languages and both editor types, at the 0.05 level. there a few exceptions, but mainly when learning from the russian bots, because of the notably fewer number of training samples for russian bots. the t-test p-values on rows suggest vandalism can be learned from any of the presented training sets (except russian bots) and applied to other languages and editor types without significant differences in classification quality."
"a a 2 6 . this indicates that the link likelihood existence of (a 2, a 6 ) is about twice of (a 3, a 5 ) which has a likelihood score of 2.61. the above calculations are summarized in table 1 ."
"to complete the cross-language learning and have data comparable to related work, we combine the editor types for the training and testing data. table 8 presents cross-language classification results for each language and combined training for bots and users of all languages, and all training data."
"the source author cites the target one. another interesting observation is that although the precision rates of all meta-path based measures are close to that of simrank in many cases through employing each of meta-paths, the mmi benefits both of them, i.e. 1  and 2 , by integrating their information. as a consequence, the mmi overcomes the other indices in term of the precision when both meta-paths are employed. accordingly, by integrating both meta-paths, the precision rate of the mmi is increased about 20%, 9%, 11%, and 3% in s3, s5, s7, and s9, respectively, for co-authorship prediction problem. for the citation prediction problem, such improvements are about 4%, 6%, 10%, and 7%, respectively. the results indicate that, the precision rates of the citation predictions are lower than those of co-authorships for all indices. however, the citation prediction problem has been known to be more challenging than the co-authorship predcition problem as the former one is a directed link prediction task with more noisy connections. moreover, it is not cleared where the links between the papers could be determined in the network, in advance. this is in contrast with a co-authorship network where authors are more likely to form scientific communities with each other that would result in more accurate predictions. it should be noticed that simrank has been considered as a homogeneous proximity measure since its accuracy does not depend on the employed meta-path."
"in the literature of graph theory, many topological graph measures have been proposed to characterize the structural information through measuring the graph complexity. furthermore, it has been well investigated that information-theoretic measures based on graph entropies provide positive structural information and meaningful interpretations 23 . to obtain such graph entropy measures, some graph invariants should be considered like the number of vertices, the vertex degree sequences, extended degree sequences, eigenvalues, and connectivity information 24 . accordingly, a number of studies has been done to analyze complex networks by introducing some graph entropy measures [cit] . moreover, graph entropy has been recently employed in the problem of link prediction in complex networks. in ref. 19, a mutual information similarity index was proposed where common neighbors is used to provide structural information to estimate the links likelihoods. in ref. 20 structural information, including common neighbors, were employed to facilitate the link prediction task. the extension of this work has been employed in a weighted complex network 21 . these entropy-based similarity indices have been evaluated over a number of complex networks and compared to common proximity measures. the results show that the proposed entropy-based indices improve the prediction accuracy with reasonable lower computational time complexity."
"prediction between similar object types. the prediction accuracy under precision has been reported for co-authorship and citation prediction problems in tables 3 and 4, respectively. in both problems, the mmi overcomes all other indices by employing either the first or the second meta-paths in most cases with higher precision rate. these results are clearly indicated for s3 and s5. when the network gets enriched by considering more publications for each author, i.e. using s7 and s9, the precision rate of all indices becomes more competitive. the reason is that the mmi relies on the information of meta-path and its relevant path-instances through measuring the link entropy. meanwhile, other indices do not consider the information provided by the available paths, rather they are dependent on the amount of reachability between two nodes. therefore, it can be inferred that the mmi could better capture the similarities between the nodes with lack of efficient structural and reachability information."
"some limitations with the pan data sets are unrepresentative samples of bots (described in section 2) [cit] -especially in the english wikipedia, and the potential bias with sampling from 'important' articles [cit] . however, the value of the pan data sets comes from the manual evaluation, which may contain very the training data for bots and users are combined for each language, and all training data for bots, users, and bots and users are combined. bold entries are the same match ups of language (diagonal) and the highest score in each column. statistical significance of results are discussed in section 8."
"wikipedia provides monthly data dumps of every language edition. [cit] to december 31st 2012 (our cut off date) for these five languages: english, german, french, spanish, and russian. we chose these languages because they have some of the highest number of articles on wikipedia, where four are the united nations official languages and the most spoken languages in the world. we can provide our data parsing scripts and data sets on request."
"for within language classification, our proposed features have higher classification scores compared to previously used features across all five languages. similarly, for out of language classification, we also find higher average classification scores. this suggests some regularity of vandalism within the same language and across languages."
"as mentioned before, dblp network data has been employed 28 in our experiments which is available to download in ref. 31 . this data set originally contains 2,092,356 papers with 8,024,869 citations, and 1,712,433 authors with 4,258,615 collaborations 31 . in fig. 12, some detailed information of the network has been depicted for 15 [cit] . as an example, we have selected four samples from this network as s3, s5, s7, and s9 in which there are 3, 5, 7, and 9 [cit], respectively. the more the number of papers per each author, the better link reliability in the network. similarly, the lesser the number of papers per each author may cause to some noisy information of some observed parts of the network."
"we compare classification scores in fig. 2 for within language classification from the bolded diagonal of table 8 . for out of language classification, fig. 3 shows the average auc-pr scores with the standard error of the mean. from our figures and from statistical significance tests showing no difference at the 0.05 level, but we conclude that data sampling has a slightly decreasing effect on the classification scores as seen in the results."
"features f26 to 29: feature f26 shows the length of the longest repeated character in a word as used in velasco [cit], which is often a clear case of vandalism. to complement this feature, the compressibility of words can identify abnormally long repeated sequence of letters. we compare three compression algorithms and take the lowest compression ratio, indicating the highest compressibility of a word. features f28 and f29 are provided to extend and contrast the compression feature f27 from mola-velasco [cit] . these are the most computationally intensive features as they require compression, but we maintain a lookup table of compressed words to avoid repeated computation."
"features f30 to 31: we count the longest unique words and the total size of the unique words in the sentence difference. these are intuitive features from mola-velasco [cit] and west [cit], but with a different interpretation and application."
"our results are consistent for the testing language, suggesting related languages, such as english and german, and spanish and french, do not affect the classification results. this is further evidence for the language independent nature (for the languages considered) of the proposed features."
"there are two sources of structural information for link prediction in complex networks. for a candidate node pair, one can explore the neighborhood properties to extract structural data, such as common neighbors, which can be used to measure the link likelihood. on the other hand, similarity search could be performed through the existing paths. for instance, counting the number of available paths between the candidate pairs. however, when the network becomes heterogeneous, considering only the amount of reachability would not be sufficient to exploit nodes similarities. in other words, measuring the information provided by the network heterogeneity, through exploring different nodes or relation types, might lead to extract more detailed information to enhance the similarity search and make the predictions more accurate (fig. 11) . we have proposed a new similarity measure based on mutual information model in heterogeneous complex networks. the proposed model introduces an information-theoretic framework in which link entropy is defined as a semantic measure for link prediction. this measure is reinforced by the number of path instances of an employed informative meta-path. moreover, to obtain more accurate predictions, the mutual contribution of meta-paths are utilized with associating a contribution weight to each of them. to investigate the efficiency of the proposed model, different experiments were conducted and compared with three popular meta-path based link predictors, an efficient path-based homogeneous link predictor, and five lcp-based link predictors. we have selected four samples from dblp network which are differed in the amount of reachability between the node pairs."
"3) multiple video sequences: to show the general applicability of the rl-based client, experiments have been performed using the six video sequences shown in table ii . this set of video sequences contains multiple types of content with differences in scene variations. for each of the sequences, 400 episodes have been streamed. fig. 4 shows the average performance and its standard deviation in the converged state of the last 50 episodes for each of the videos. on average, the rl-based client is able to outperform the mss client by 9.12% in terms of average mos, while reducing its standard deviation on average by 16.65%. observations show an inverse relationship between the performance gained by the rl-based client and the dispersion of segment bitrates within a quality level. high values of ω affect the performance of the learning agent. this is illustrated by the lower performance increase for the silence of the lambs video with high bitrate dispersion and the high performance increases for the other videos with moderate dispersion."
"previous wikipedia vandalism detection studies have focused mainly on the pan data sets as described in section 2. we use the pan data sets as a baseline comparison of results by evaluating our features under the same conditions as the full wikipedia data set, with a 1:1 ratio of classes. [cit] data set (as far as we are aware, we are the first to do so)."
"actually, it is so important to achieve the good voltage control of three-phase voltage source inverters (3φ-vsis) in various alternating current (ac) power conditioning systems such as uninterruptible power supplies (upss), automatic voltage regulators (avrs), energy storage systems (esss), and distributed generation systems (dgss) [cit] . generally, the voltage control capability of 3φ-vsis is evaluated via the desired quality of controlled sinusoidal voltages such as fast transient response after sudden disturbances, small total harmonic distortions (thds), and small steady-state errors (sses) in the output voltages. furthermore, the mentioned control performance should be robust under system parameter uncertainties [cit] and critical load conditions (e.g., load step-change, nonlinear load, etc.)."
"in particular, the inherent redundancy of the considered rov can be exploited for fault accommodation using an adaptive scheme for the allocation weight matrix ω. as output of the fdi module one gets two indices, namely i * and j *, such that"
"after a failure has been detected and isolated by the fd and fi module respectively, control reconfiguration is applied to preserve the desired performances in face of the failure occurrence. while in previous works this has been supposed as part of the human supervisor duty (see for instance [cit] ), in this paper we propose an automated reconfiguration method."
"in the scenario considered in this paper, each thruster is an actuator potentially affected by faults. the basic idea is that, whenever a failure is detected and identified, a supervisor performs a control reconfiguration exploiting thrusters redundancy (three thursters are enough to control the rov trajectory). in this framework, it is convenient to rewrite the model (3) as follows:"
"proof. the proof is straightforward. it simply consists in checking the eventual violation of the sliding mode existence condition, according to (11). it is worth recalling that, according to assumption 4, the sliding motion has been established, and the sliding surface (6) should be zero in the absence of a fault affecting the actuators."
"the quantities t x, t y and m z appearing in (1) are the decomposition of the thrust and the torque provided by the four vehicle propellers along the axes of r: fig. 1 )."
"the equations describing the rov dynamics have been obtained from classical mechanics [cit] . the rov considered as a rigid body can be fully described with six degrees of freedom, corresponding to the position and orientation with respect to a given coordinate system. let us consider the inertial frame r (0, x, y, z) and the body reference frame r a (0 a, x a, y a, z a ) [cit] shown in fig. 1 . the rov position with respect to r is expressed by the origin of the system while its orientation by the roll, pitch, and yaw angles ψ, θ, and φ, respectively. being the depth controlled by the surface vessel, the rov is considered to operate on surfaces parallel to the x − y plane. accordingly the controllable variables are x, y, and the yaw angle φ. it should be noticed that the roll and pitch angles ψ and θ will not be considered in the dynamic model: their amplitude, in fact, has been proved to be negligible in a wide range of load conditions, and with different intensities and directions of the underwater current as well [cit] . therefore, the rov model is described by the following system of differential equations [cit] :"
"in this section, a reduced order observer, specifically designed for the rov, will be proposed. in fact, while position measurements are usually available and are in general sufficiently reliable, velocity measurements are either difficult to be gathered and poorly reliable."
"a robust control law, coupled with the above observer, will be here presented aimed at solving the regulation problem for the variables z 1, z 2, z 3 with respect to reference variable"
"the two new algorithms are suitable for any type of p2p swarming system, independent from the type of content that is shared. the major difference compared with previous state of the art (mainly [cit] ), is that the proposed algorithms are fully distributed, and not require nodes starting the protocol at the same time. time synchronization is not required and the algorithms are designed for continuous systems (the estimate reflects the current state of the swarm and is available as long as the swarm exists). additionally, the availability of the estimates for the flash-crowd, at each peer, allows for instance the usage of distributed load balancing policies. the nodes can perform dynamic bandwidth allocation between different swarms. needless to say, the existing offline methods for estimating the occurrence of flash-crowds are not allowing for this live, online reconfiguration. although a centralized approach using for example a tracker can provide faster and better accuracy than a distributed approach, nowadays it becomes more difficult to maintain such systems given that they have to face tremendous legal challenges [cit] . our distributed online estimation can act as a back-up solution when the trackers are temporarily out of use. it does not have to completely replace the centralized solutions, just to complement it when needed."
"we assume that object related variables o are intrinsic properties (they do not change as the object moves) and action variables a regard a static hand pose, at the moment of grasping, expressed in the local coordinate frame of objects. a variables are agent-specific because humans and robots differ in the kinematics of their end-effectors. each objectgrasp complex, as shown in fig. 1, can be described by a set of object, action and constraint variables o, a, c and the task(s) t that the grasp can afford. while o, a, c variables can be measured or extracted using robot's vision and haptic sensors, the high level task semantics such as \"grasp a water bottle in order to pour water\" or \"grasp the cup to put it in dishwasher\" have to be provided by a human teacher. therefore while o, a, c is a set of variables that are instantiated by robot's sensor measurements, t is instantiated by human labeling that states if this grasp could afford the given task label."
"for our experiments, we have extended the bittorrent simulator designed by microsoft research [cit], in order to support our flashdetect and trackernetsize algorithms. this is a very detailed simulator, where all the elements of a bittorrent system are modeled with great accuracy, from the creation of the overlay to the exchange of files between peers. for this reason, this simulator has been widely used, also for simulating bittorrent-like streaming protocols [cit] ."
"a number of studies have shown that flash-crowds occur often in p2p swarming systems, affecting a large fraction of their users [cit] . furthermore, it has been shown that these phenomena have severe impact on the performance of the participating users, especially in the context of live streaming applications [cit] . hence, identifying the occurrence of flash-crowds could greatly help p2p swarming systems set up the necessary resources/actions to ''mitigate'' them. giving peers the ability to detect flash-crowds allows them to distributively perform load balancing in the system and hence significantly decrease the burden at the content provider side. for example, a peer that concurrently participates into multiple swarms, can decide how much of its bandwidth to allocate to each one of them, based on the realtime status information of the two swarms (e.g., proportional to the flash-crowd intensities they exhibit). to the best of our knowledge, only one recent study briefly discusses how peers can detect a flash-crowd by inspecting the file-completion level of their neighbors [cit] . as such, this algorithm is only suitable for filesharing and video-on-demand applications, where the file size is known beforehand, but not for live streaming applications. on the other side, the flashdetect online distributed flash-crowd detection algorithm proposed in this paper is completely decoupled from the type of data being exchanged in the swarm and therefore can be adopted by all types of swarming applications, including live streaming. it presents itself as one of the first algorithms that targets specifically the online detection problem."
"in addition, the current bayesian network is a static model encoding task information only at the moment of final grasp configuration. however, the dynamic manipulation of object after lifting contains rich information about the task. especially for some tasks (e.g., pouring and drinking) that are difficult to differenciate by only the static grasp configuration, sequential data of object manipulation will be much more powerful to solve the ambiguity. we are now investigating how to exploit this by modeling the sequence of grasping and manipulation actions using dynamic bayesian networks. the model should be used for both task recognition during human demostration and action reproduction on a robot platform."
"humans possess a profound capability of learning though imitation. imitation learning has been addressed frequently in robotics for enabling bootstrapping in object grasping and interaction with the environment in general, [cit] . an important challenge in imitation learning is the \"correspondence problem\" [cit] due to the differences in embodiments between humans and robots, being especially problematic in grasping applications [cit], see an example in fig. 1 . one solution to the correspondence problem is \"goal-directed imitation\" [cit] where the learner infers the intent of the activity from the teacher and then achieves the same goal using own sensorimotor acts (see a flowchart of goal-directed imitation process in the context of robot grasping in fig. 6 a) human demonstrates a grasp with an intention to hand-over an apple. b) robot imitates the power grasp configuration used by the human, and fails to hand-over because there is not enough free space for regrasp. c) robot estimates that human intends to hand-over the apple. it also learns the task requires leaving enough free space on the object. it applies a precision grasp to achieve the task."
"we first turn our attention to the results concerning the accuracy of the flash-crowd detection algorithm (see fig. 6 ). for the synthetic scenario (fig. 6(a) ), one can notice that above 80% of the initial peers succeed in detecting the flash-crowd, for the scenarios of high intensity and sudden peer arrivals. on the other hand, in the scenario with low intensity arrival, the accuracy quickly decreases with the reset interval. the explanation for this behavior is that, for the low intensity scenario, peers are joining the system at a much slower rate than in the other two scenarios, which means that the swarm size increase is close to the threshold n and the slightest detection error leads peers not being to detect the ongoing flash-crowd. fig. 6 (b) and (c) showcase the accuracy for the algorithm when using real traces. we notice that the accuracy decreases with the reset interval in all traces. in fact, all the filelist.org traces exhibit low intensity, with the swarm size reaching its peak after around 9-10 h. another explanation for this behavior is that in these traces, peers leave after a while, hence when using larger reset intervals fewer initial peers are able to detect a flash-crowd before they leave the system. the detection accuracy can be improved by lowering the threshold for the flash-crowd detection, as shown in fig. 6(c) ."
"in parallel with these operations, the sbc is required to manage multimedia contents, namely messages and videos. this is a fundamental requirement of our wsn and it is relevant for the public transportation company we collaborated with. this information is related to the bus line, ride and stops, or advertisements. therefore, the single board computer is interfaced with the screens and displays presents on the bus, as shown in figure 1 . a given bus line or ride is associated to a unique wsn node (or a single sbc) using a keyword. alternatively, it is possible to discriminate the node relying on the ip address. the node interrogates the server for video and/or messages (\"get\" query in http protocol). the client, i.e., the raspberry, does not know the list of videos and messages associated to it. the query is handled by the content management system (cms, shown in figure 3 ), which is developed in php language using the open-source software drupal (dries buytaert, us). given the node identifier, the cms checks if the list elements to be sent to the node are present in the folders of the server, while also checking if the download can be performed. the client/node does not disconnect for 15 min. when the download starts, the files are saved in a temporary folder. the sbc cancels any old files and then copies the new ones into an intermediate folder, in its memory. during the download phase, at the sbc level, a video stream, written in python language, runs on the monitor the videos copied from the aforementioned intermediate folder. to avoid problems, the refresh of the intermediate folder is independent from the refresh of the streamer. in particular, after all videos in the folder are played, the new check is performed. in this way, the search for new videos is not performed during the reproduction of the others. in particular, a thread runs the file transfer from the server to the device, while another independent thread executes the streamer, and these processes are not allowed to access the intermediate folder at the same time."
"2) gaussian mixture model: the result of the first step is a selected number of clusters k and the corresponding cluster assignment on the original data x. this result allows for the learning of a gaussian mixture model that encodes the density distribution in the original data space (see fig. 2 e),"
"coverage area increases, more transmitted power is required to provide an acceptable signal quality for cell edge users, which increases the overall bs energy consumption. recently, femtocells have been deployed to enhance in-building coverage and provide higher data rates. due to their small coverage area, femtocells require much less transmission power than a macrocell, and hence their bss consume less energy. however, a deployment with only small cells would require a large number of bss. this increases the handoff rates of mobile users among adjacent cells and also may degrade the overall energy efficiency of the network. a joint deployment of bss with different cell sizes is desired. a balance of different cell sizes is required for the most energy-efficient layout [cit] ."
"the aforementioned quantities can be monitored for purposes not strictly related to the hardware perspective. indeed, in the its framework, data derived from such heterogeneous quantities are combined and integrated to develop a real-time information system or to devise intelligent feedback strategies [cit] . for example, the monitoring of temperature, relative humidity, and cinematic parameter can be exploited to setup a smart system for improving the quality of passenger during the route [cit] . in this work, relative humidity, temperature, and acceleration are employed to setup a real-time information system which can send alert messages to the driver or the public transportation technicians, as shown in figures 1 and 2 ."
"we have extended the msr bittorrent simulator [cit] in order to support the flashdetect algorithm presented in section 4. every time a new peer joins a swarm it asks the tracker for a list of 50 peers to connect to. these peers are selected by the tracker at random among all the peers participating in the swarm. furthermore each peer requests a new list of 50 random nodes from the tracker every 30 min. this behavior closely resembles that of real bittorrent clients [cit] . each experiment has been repeated multiple times, and we report means and standard deviations based on these."
"the main idea behind the trackernetsize algorithm is that all the nodes run diffusionreset and reset to 1. the tracker node maintains an estimate of the network n e ½r and resets to 1 à n e ½r. the reasoning behind is that the overall mass in the network should converge to 0. if there is an imbalance between n e ½r and the real network size, the diffusionreset mechanism converges to a nonzero value, giving an indication to the tracker node on how to adjust its network size estimate."
"in general, while cooperation in wireless communication networks results in performance gain, it incurs some overhead. specifically, for our framework, this cooperation overhead includes the synchronization required among the cooperating bss of different networks. the importance of this synchronization is due to the fact that an unsynchronized switching action among different bss can significantly degrade system performance in terms of call blocking, for example. such synchronization can be achieved through control signaling among the cooperating bss."
"the node design is mainly oriented in understanding the influence of environmental parameters, namely temperature and relative humidity, and the operative conditions (vibrations, impacts, and mechanical stresses) on the core unit of the wsn node, i.e., the raspberry unit. therefore, the variables to be monitored are chosen to understand and evaluate how and if the functioning of the wsn node can be compromised. the relative humidity has been recognized as a limiting factor in any wireless transmission, while being a severe threat for electronics [cit] . temperature is a physical quantity already considered in the preliminary evaluation on the hardware for its applications [cit] . the accelerometer is an obvious design choice since the shocks and vibrations on the hardware may be significantly relevant [cit] ."
"in the following, we present a series of experiments that showcase the characteristics of the two proposed algorithms. the simulations were performed both on synthetic data-sets as well as real traces. for comparison we chose a gossip-based algorithm that estimates the network size in a time synchronous manner [cit] ."
"initial peers are those peers already present in the swarm at the time a flash-crowd starts. we have not considered peers joining during the flash-crowd itself, as their detection would be influenced by the swarm size at the time they joined (which is different from the initial swarm size). on the other hand, as soon as a peer has detected a flash-crowd, it can notify the newcomers connecting to it (for example by appending this information to the greeting message they exchange at the beginning of a connection establishment), thus quickly diffusing the information into the swarm."
"the node is fed using the automotive power system, which operates at 24 v. therefore, two dc-dc converters, one for the raspberry and one for the monitor, are necessary, as shown in figure 5 . for the down-conversion of voltage to the 5 v required for the sbc operations, the yeeco (germany) b1700526eu dc-dc converter esd employed. in regards to the monitor power supply, the zc104300 cpt was used to lower the voltage to 12 v. it should be pointed out that the capacity of the vehicle battery is about 243 kwh and the maximum, overestimated, power consumption of the wsn node is about 80 wh, implying that the energy efficiency is a negligible issue for this wsn."
"the equation that models the diffusion-reset mechanism 2 exhibits exponential convergence over time (see fig. 2 ). no matter the initial mass value, the aggregate on the nodes converges exponentially fast."
"currently, there are different wireless networks that offer a variety of access options. such wireless access networks include third-generation (3g) cellular systems, ieee 802.11 wifi networks, and ieee 802.16 wimax systems. these networks have complementary service capabilities. for example, ieee 802.11 networks can support high-data-rate services in hot spots, whereas 3g cellular and ieee 802.16 networks can offer broadband wireless access over long distances and serve as a backbone for hot spots. in spite of fierce competition in the wireless service market, these successful wireless networks will coexist. in such a heterogeneous wireless access environment, cooperative networking will lead to better service quality for mobile users and enhanced performance of the networks."
"fig . 2 illustrates this process. first, a large set of prototypes -much larger than the expected number of clusters k-is formed using the som (fig. 2 a) . the prototypes are combined in the next level to form the actual clusters (see fig. 2 d) . the u-matrix (fig. 2 b), which represents the distance between neuron weights, can be used to determine the clusters and their boundaries. given a predefined number of clusters k, clustering is performed on w using the k-means algorithm. to determine k, we use the davies-bouldin validity index, defined as i db"
"we focus next on the second proposed metric -delay (see fig. 7 ). all graphs clearly show that the delay increases with the reset period. this was to be expected, since, as the reset period increases, a peer has to wait longer before reaching an estimate of the swarm size increase. we can observe that the delay follows a similar pattern for both synthetic and real traces. furthermore, a lower threshold corresponds to a lower delay, in line with the considerations at the beginning of this section, i.e., that a lower threshold allows to detect smaller increases in the network size."
"in green radio communications, a main network design objective is to reduce the amount of energy consumption while maintaining satisfactory quality of service (qos). two motivations are behind this design criterion. one is the service provider's financial considerations. almost half of a mobile service provider's annual operating expenses are energy costs [cit] . each base station (bs) in a cellular network consumes roughly up to 2.7 kwh of electrical power [cit] . with densely deployed bss to achieve wide area coverage, high energy is consumed per annum. such high energy consumption results in a significant environmental impact due to the associated co 2 emissions. this is the other motivation behind green radio communications network design: environmental considerations. currently, the telecommunication industry is responsible for about 2 percent of co 2 emissions, and given the industry's growth it could increase to 4 [cit] . as a result, political initiatives start to put requirements on operators to lower the co 2 emissions of communication networks [cit] . in europe, companies such as orange (france), ericson (sweden), and vodafone (united kingdom) aim to reduce their co 2 emissions by 50-80 [cit] . hence, the reduction of energy consumption in the telecommunication industry sector will result in a positive impact on both the environment and operators' profits [cit] ."
"wireless sensors networks (wsn) are an expression of internet of things and are a powerful, promising and low-cost platform for several different applications [cit], e.g., the development of an intelligent and automated system for irrigation [cit], for the monitoring of cultural assets [cit], or to the management of the container terminals logistic [cit] . basically, wsn are systems composed of radio-frequency (rf) transceivers, sensors, micro-controllers or processing units, and power sources [cit] . compared to the traditional wired sensors networks, wsn technology is cheaper, typically presents short deployment time and makes use of a higher number of sensors of various nature, e.g., temperature, humidity, acceleration, and concentration of chemicals [cit] . therefore, with wsn, it is possible to perform a multi-variable monitoring of a given industrial process of interest [cit] . moreover, the deployment of the sensors and nodes can cover large scale areas or be mobile [cit] . the wsn can also ensure a robust digital transmission of information, with acceptable"
"flash-crowds have been loosely defined as a sudden surge of users trying to access a particular service. in this work, we adapt this definition to p2p systems as follows:"
"this wsn architecture is flexible and can be exploited for several different applications in the automotive field, especially to develop intelligent transport systems [cit] ."
"the very first fundamental and most general requirement for wsns in its is that the node should be low cost, since a high number of nodes might be deployed [cit], i.e., in the case under analysis, a moderate to high number of buses may be equipped with the wsn node presented in figure 1 . moreover, in general, the nodes and the networks must be energy efficient [cit] . this last specification is not an issue for the peculiar case of automotive application investigated in this work. indeed, the power supply of the vehicle can be exploited without any concern since the power consumption due to the sbc operation is negligible. the fault tolerance is another key criterion, since the nodes are deployed in a hostile and harsh environment which can damage them or cause malfunctions [cit] . furthermore, both the hardware and software employed in wsn for automotive applications must be integrable with other communication technologies [cit] . indeed, vehicle-to-vehicle and vehicle-to-infrastructure communications are often required [cit] . in addition, a reliable and stable transmission of data between the nodes of the networks is a necessity. finally, wsn security across all layers and electromagnetic compatibility requires particular attention [cit], as in the case of its for public service infrastructure [cit] . in particular, the fundamental emc aspects for a system for automotive applications are that the designed system must not cause interference with other systems and that it should not be susceptible to emission from other devices [cit] ."
"in order to detect relative increases in the number of nodes in the network, the following mechanism is proposed (see algorithm 2): each new node joining the network needs to engage in the diffusionreset mechanism, resetting itself once to 0 and in subsequent rounds to 1. the nodes already present in the network need to reset periodically to 1. the aggregate computed by diffusionreset will be a function of the relative network size, as shown below. the proposed approach has the advantage that produces correct results even in the presence of churn, in the sense that nodes that leave the network do not need to properly announce their leave (their departure has an immediate effect in the computed aggregate)."
"the sensors and the raspberry unit were connected to the power source. furthermore, the chosen sbc was interfaced with the lte modem for the transmission of the data from the sensors, as well as for the query and download of contents from the cms. the huawei e3372 megafon dongle (huawei technologies co., ltd., china) was employed for the wireless data transmission (see figure 5 ). this off-the-shelf device can operate in the gsm (2g, 900 [cit] mhz), umts (3g, 900 and 2100 mhz), and lte (4g, 800, 1800, 2100, and 2600 mhz) bands. the data rate ranges from a minimum of 236.8 kbps to 150 mbps, thus implying a significant flexibility and performances comparable to other devices which make use of different communication protocols [cit] 18, 21] ."
"peer-to-peer (p2p) applications generate most of the internet traffic [cit] and have become an important determining factor for renewing the current internet infrastructure [cit] . these applications most noticeably include file sharing [cit], voice over ip [cit], and streaming of multimedia content [cit] . typically, peers collaborate in the download of content, creating a swarm."
"the dynamic planning solutions in literature focus on switching off either some of the bss or some of the resources of the bss. it is more beneficial to combine both strategies and not only switch off some bss, but also switch off some of the resources of an active bs to further improve the amount of energy saving. also, with the existence of different wireless networks with different overlapped coverage areas, network cooperation can achieve energy saving and avoid dynamic planning's shortcomings. in this article, we aim to provide an optimal resource on-off switching framework that captures the random behavior of traffic arrivals, adapts to fluctuations of the traffic load, and maximizes the amount of energy saving under service quality constraints in a cooperative networking environment."
"it has been shown that these systems are able to attain high performance (and consequently deliver a good quality-of-service to their users) once the demand for a certain content has become steady. on the other hand, they might suffer of low performance in dynamic scenarios, characterized by high churn or sudden surge of new users (i.e., a flash-crowd) [cit] . therefore, detecting the occurrence of such phenomena becomes of crucial importance when having to allocate additional traffic capacity."
"we observe that the goal inference is quite good for all the four goals even under partial observations (above 0.80 auc). hand-over is more difficult than others since it is a less constrained task. to hand-over an object, the object can fig. 6 . flowchart showing the full process of goal-directed grasp imitation. robot is equiped with the knowledge of human grasp intentions (modeled in bn h ) and its own grasp capability (modeled in bn r ). given its observation of a human grasp demonstration, it first uses bn h to infer the intention of the human, and then decides, using bn r, which object and action that are most suitable to achieve the same intention on its own embodiment. the image of the real robot platform is on the humanoid torso, tombatossals from the robotic intelligence lab at universitat jaume i."
"for distributed systems, centralized approaches are always thought to be unscalable. nevertheless, in some p2p systems, most noticeably live-streaming, there is at least one ''special node'', the source, available for the whole swarm lifetime. the source can be configured to be a ''tracker node'', which, performing a slightly different version of the diffusionreset mechanism, can obtain an estimation of the absolute network size. the other nodes of the network do only local information exchange, without the need of directly making a connection to the tracker node."
"the general scheme of the proposed wsn is presented in figure 1 . the wsn is composed of three different layers. the physical and sensing layer is composed of sensors and a single board computer (sbc) for processing and storing, and which acts as data sink and gateway [cit] . the sbc is equipped with a proper commercial radio-frequency transceiver. the sensors monitor in real-time the relative humidity, the air temperature, and the acceleration, experienced by the hardware, due to the bus vibrations. more details about the sensors and the node are given in section 2.3. these data are immediately elaborated and analyzed by the sbc. the digitalized processed data are transferred to a dedicated server, where they are stored in \".json\" format and organized in a database using the open-source software elasticsearch (elasticsearch, us). the kibana (elasticsearch, us) application is employed to visualize the data and manage the database. at this level, the information automatically derived by the processing unit from temperature, relative humidity, and accelerometer sensors can be used to verify if harsh or risky events are occurring. the application elastalert, included in elasticsearch, performs the monitoring of the aforementioned physical data and detects whether a given threshold value is overcome. elastalert is then allowed to send an automatic alarm message through a dedicated telegram bot, called ctmalarm, as shown in figure 2 . the drivers or the technicians from the public transportation company receives these messages on their smartphones or electronic devices, such as the bus screens. the single node is composed of relative humidity, temperature, and accelerometer sensors. a raspberry pi 3+ unit is employed as core, sink, and gateway node. the raspberry unit process and elaborate the sensor data in real time. the bus monitor is connected to the single board computer (sbc). the raspberry transfers data to and from a server. the elastalert application monitors the data elaborated by the raspberry and, if fixed threshold values are overcome, it sends alert messages through a telegram application. the content management system (cms) recognizes the device and the line to which is associated. the cms transfers data and messages to the raspberry unit, which displays them on the bus screens."
"from the perspective of mobile users, cooperative networking solutions for heterogeneous wireless networks enable them to enjoy an always best connection. the always best connection is facilitated by internetwork vertical handoffs, which can be based on service cost, coverage, transmission rate, qos, information security, and user preference. taking advantage of cooperation activities among multiple access networks, internetwork vertical handoffs can be supported in a seamless and fast manner. hence, a reliable endto-end connection at the transport layer can be provided to preserve service continuity and minimize disruption. also, future mobile terminals (mts) will be equipped with multiple radio interfaces for network access. multihoming techniques maintain multiple simultaneous associations of an mt with different radio access networks. facilitated by cooperation across different network domains, multihoming can support applications"
"finally, the three-axial accelerometer adxl-345 (sparqee llc), based on the mma7361 component, was chosen to measure and monitor the mechanical stresses, e.g., vibration and/or impacts. this sensor has the twofold goal of allowing to identify if harsh conditions or potentially risky events may harm the raspberry unit during the travels, while ensuring a constant and continuous observance of the conditions during the bus rides. therefore, in addition to the hardware perspective, the data acquired and elaborated at the node level can be very useful to provide information to the driver or the passengers using the developed automatic alert system. as regards the monitoring of vibrations and impacts, the placement of the accelerometer in the pcb is a critical aspect (see figure 4 ) [cit] . indeed, the x-axis of the sensor is required to point toward the front of the bus, whereas the y-axis should coincides with the sides of the vehicle and the z-axis has to be oriented with the top of the bus."
"in fig. 9 we show the results of the two algorithms, namely trackernetsize and syncalg, working in parallel to showcase the differences. despite the fact that our algorithm has a higher standard deviation, it is able to work with very dynamic topologies, with churn is present. the key result here is that we achieve this without any assumptions on synchronization and no mechanism for tracking departing nodes."
"while centralized solutions exist (e.g., by means of a special allocation policy at the initial server that is being used for bootstrapping), we note that these approaches lead to a high burden for the content provider, which would need to track the swarm size and dispatch extra bandwidth resources and/or instruct all the participating peers on the bandwidth allocation policy. furthermore, a centralized solution is susceptible to the well-known single-point-of-failure problem. hence, in this paper, we focus on the distributed online estimation of the swarm size in dynamic scenarios. we tackle the problem of determining the swarm size by means of diffusion algorithms (also known as gossiping algorithms [cit] ). this class of algorithms allows easy dissemination of information, and has been used to compute network aggregates such as averages, sums, random sampling distributions, quantiles, network size estimates etc. (see [cit] ). unfortunately, using gossiping for computing distributed aggregates is subject to the strong assumption of no mass loss, in other words, the algorithms are guaranteed to work if nodes properly advertise their leave and there is no message loss. in dynamic networks this is usually not the case, and the constant departure of nodes, especially during the start phase of the gossip algorithms, has significant consequences on the computed aggregates (see [cit] for an analysis)."
"are the mean and covariance of the components of the gmm model (eq. 1). we can then sample from the above distribution in order to find the most likely locations over the continuous space. this approach will be henceforth referred to as ml. in ml, however, there is a danger of information loss, especially when p k does not have a strong preferred single state, but it spreads out to multiple states. an alternative method would be to estimate the expected value using a weighted sum of the component centers that takes probabilities of all the states into account. the expected value of variable z is then defined as,"
"(algorithm 1, lines 1-6). vector k i ½k allows flexibility in the algorithm analysis. for example, if all the elements in k i ½k are zero, except for two entries (corresponding to i and a random neighbor j) equal to 0.5 each, the diffusion corresponds to the push-pull type of gossiping. if all the entries in k i ½k are made equal to 1 n i ½k then the diffusion is actually a push gossiping mechanism."
"the increasing scale of p2p systems (such as bittorrent, multimedia streaming services, voip etc.), generates a growing need for performing aggregate computations via distributed, robust and scalable algorithms to achieve load-balancing and adaptive system properties. of particular interest is the online estimation of network (swarm) size in dynamic scenarios characterized by varying topologies, churn, and the presence of sudden increases in peer arrival rates (flash-crowd). in this paper we introduced two algorithms (the flashdetect and trackernetsize algorithms) as a novel extensions of the classic gossiping algorithms with the addition of periodic computational resets. the solution was presented in the form of two algorithms -one that gives estimates of the relative network size increase (information available at every node) and one that tracks the absolute network size (information available at the tracker nodes). they were analyzed and discussed with respect to existing state-of-the-art. the performance of flashdetect and trackernetsize was characterized for various peer arrival rates. the experiments covered a large set of scenarios, ranging from theoretic -exponentially decreasing arrival rates distributions -to actual p2p system observations. the distributed, simple and scalable characteristics of flashdetect and trackernetsize make them very good alternatives to existing algorithms for network size estimation and flash-crowd detection in p2p networks. as future work we want to extend the algorithms for automatic parameter selection as a function of the system dynamics."
"the following traffic and mobility assumptions are made: a1 new calls arrive to the coverage area of cell n according to a poisson process with mean arrival rate v n . a2 handoff calls from adjacent cells arrive to cell n according to a poisson process with mean rate υ n . a3 the dwell time of an mt in a cell is exponentially distributed with mean 1/η, where η is the average cell boundary crossing rate. a4 the call duration follows an exponential distribution with mean 1/μ."
"the reset mechanism (lines 7-11 in algorithm 1) works as following: every r time steps, a node resets its state value to flg. the reset phase of each node is / i (see fig. 1 ). let d½k be the discrete dirac function."
"due to the standard deviation that characterizes diffusion protocols, the results (eqs. 8 and 10) need to be smoothed with a low-pass filter, omitted in this description."
"in this article, network cooperation as a means of energy saving in green radio communications is investigated. for the system model with overlapped coverage from different networks, the proposed technique can achieve energy saving without increasing transmission power. it relies on cooperation among different networks to save energy on two scales. on a large scale, networks with overlapped coverage alternately switch their bss on and off according to the long-term fluctuations in traffic load. on a small scale, each active bs switches its channels on and off according to the short-term fluctuations in traffic load. numerical results demonstrate satisfactory service quality in terms of call blocking probability and a large percentage of energy saving for each network. in the proposed framework, the service quality constraints can be extended to include metrics other than call blocking probability, such as the minimum achieved throughput for data applications, and delay and delay jitter for video streaming applications."
"from an environmental perspective, the objective of green radio communications is to reduce co 2 emissions [cit] . this can be achieved by using renewable energy sources at the bss, such as locally generated wind and solar power. this can reduce the amount of electrical power consumption taken from the grid. also, it can complement the fossil fuel power generators in off-grid sites. moreover, air cooling and cold climates can be used to cool the electronic devices in the bss [cit] . however, the renewable energy sources cannot replace the traditional energy sources in the bss due to their required high reliability, since any power shortage will disturb the wireless network's service provision."
"the sns-dh11 capacitive sensor was employed to measure relative humidity in the range 1-90%, with a precision of about ±5%. the choice of this physical quantity is also due to the fact that little is known about the influence of humidity on the operative condition of raspberry unit, as shown in table 1 ."
where n is the number of samples. the principle is to first use som to project x to a set of prototypes (map units) that are then combined to form final clusters.
"given a set of 3d object models and a scene observation (fig. 3 a), we need to estimate the position and orientation of the objects present in the scene (fig. 3 b) in order to initialize tracking. the employed object recognition algorithm [cit] consists of an offline pre-processing phase and an online recognition step. in the offline phase, pairs of points with corresponding normals are sampled from the model and a geometric descriptor signature is computed for each pair and stored in a hash-table. the complete representation for all models is computed by processing each model in this way using the same hash table. thus, new models can easily be added without recomputing the whole hash table."
the authors investigate network cooperation as a means of energy saving. they present an optimal resource on-off switching framework that adapts to the fluctuations in the traffic load and maximizes the amount of energy saving under service quality constraints in a cooperative networking environment.
"the tracker nodes updates their estimate of the network size at each round, based on eq. (8) . n e ½r converges to n with the same speed as the diffusion process. an important observation is that the residual value on each node is not 0 after convergence. due to distortions introduced by the periodic resets, the convergence value of the residual is computed with the following equation:"
"in the literature, there have been several proposals for designing an energy-aware infrastructure in wireless communications networks. in the following, energy saving techniques at the network level are discussed. the limitations of the existing techniques are pointed out. we investigate network cooperation as a means of energy saving in green radio communications. the objective is to develop a framework that enables networks with overlapped coverage in a given geographical region to cooperate with each other to achieve energy saving."
"the extension of the algorithm to several tracker nodes is straight-forward. for example, to ensure redundancy, all the tracker nodes run their own estimation, as parallel algorithms. in the following we will focus on the existence of a single tracker node."
"financial and environmental considerations have motivated a trend in wireless communication network design and operation to minimize the amount of energy consumption. this trend is referred to as green radio communications. in this article, network cooperation is investigated as a means of energy saving. networks with overlapped coverage can cooperate to reduce their energy consumption by alternately switching on and off their resources according to the traffic load conditions. we present an optimal resource on-off switching framework that adapts to the fluctuations in the traffic load and maximizes the amount of energy saving under service quality constraints in a cooperative networking environment. for the system model under consideration, unlike the existing solutions in the literature, the proposed technique can achieve energy saving while avoiding an increase in transmission power. numerical results demonstrate the validity of the proposed technique."
"the strength of the signal, together with the connection quality were checked and monitored during the bus rides. if low quality or disconnection was detected, the elastalert application immediately sent the alert message. an eventual disservice can be due to a low signal level in urban or suburban areas. however, it is possible that the temperature, humidity, or acceleration due to the bus vibration plays a disturbing role. it is relevant to analyze the influence of these quantities on the hardware performances. moreover, remembering that the management of multimedia content is a primary requirement for the wsn under investigation, it should be pointed out that both the csm and server operations contribute to the efficiency of data download. therefore, to perform a quantitative analysis of the wsn architecture performances, we defined the download time efficiency γ as follows:"
"algorithms such as push-sum [cit] can be used to determine mean values, sums and other network aggregates including network size. they are very flexible with respect to the underlying network topology, achieve exponentially-fast convergence rates [cit] even in multihop and mobile networks and are resilient to failures to a large degree [cit] . the introduction of the synopsis diffusion [cit] boosted the capabilities of the ''traditional'' diffusion algorithms with the use of statistics. the main disadvantage of these approaches is that their performance cannot be guaranteed if the network exhibits churn [cit] or ''mass'' loss."
"as seen in fig. 8(a), when the reset interval is smaller than 10 min, flash-crowd estimation accuracy is small. at values higher than 10 min, it exhibits a similar performance to the synchronous algorithm."
"the traffic load in a wireless network can have spatial and temporal fluctuations due to user mobility and activities [cit] . an example is in a city scenario, where the traffic load during daytime hours on weekdays is heavy in office areas and light in residential areas, while the opposite happens in the evening. in the literature, researchers propose to exploit such traffic load fluctuations, by switching off some of the available resources when the traffic load is light. this is known as dynamic planning. on one hand, these resources can be the radio transceivers of active bss [cit] . however, when a bs is in active mode, power supply, processing circuits, and air conditioning take up to 60 percent of the total energy consumption [cit] . hence, significant energy saving can be achieved if the entire bs is switched off when the traffic load is light [5, references therein] . while bs on-off switching can avoid resource overprovisioning in low traffic load conditions and hence achieve energy saving, radio coverage and service provisioning for the off cells face some challenges. since most dynamic planning solutions are limited to the operation of a single network, the proposed solutions for service provisioning for the off cells rely on the active resources of such a network. as a result, an increase in the transmission power of the active bss is required to increase their cell radii in order to provide radio coverage for the off cells. this also may result in coverage holes if the maximum allowed transmission power of the remaining active bss cannot achieve radio coverage for the off cells; as a result, service disruption is expected in these areas. also, an increase in transmission power may result in intercell interference if more than one active bs try to achieve radio coverage for the switched off cells, and as a result additional interference management schemes are needed. two solutions are proposed in the literature to avoid the aforementioned shortcomings of dynamic planning. one relies on the mobility of relay nodes to migrate traffic from off bss to active ones [cit] . however, such a solution is not reliable in case of delay-sensitive applications such as voice telephony. the other solution exploits cooperation between two cellular operators to achieve energy saving by allowing traffic to be carried on for one operator's off bss through the other operator's active bss [cit] . however, the proposed solution assumes that the traffic profile can be expressed in terms of a deterministic function that varies with time. this cannot accurately capture the random behavior of traffic arrivals and traffic load fluctuations."
"the always best connection is facilitated by internetwork vertical handoffs, which can be based on service cost, coverage, transmission rate, qos, information security, and user preference."
"in this section, we investigate the application of network cooperation as a means of energy saving. first, we present the system model under consideration. then we discuss the challenges in the resource on-off switching decision making problem. based on the challenges, an optimal resource on-off switching framework is presented."
"the sensors were integrated on a dedicated pcb designed using eagle autodesk (eda solutions, uk). the board was manufactured onto a fr4 board by exploiting a lpkf protomat c100 hf (germany) cutter. the schematics and the realized pcb are shown in figure 4 ."
"the algorithm parameters have been chosen empirically. in all our experiments, when using reset intervals smaller than 5 min and larger than 30 min, the peers are not able to accurately estimate the network dynamics. the reason for this behavior is due to nodes either not being able to track the flash-crowd (during a large reset interval the peers ''forget'' the initial conditions) or the diffusion-reset mechanism not having enough time to converge (during a small reset interval). an extensive analysis on optimal parameter selection is not trivial and requires additional effort for both the analytical model as well as the implementation details. the approaches for automatic parameter tuning are work in progress. the preliminary experiments show us that there is a high dependency between the network diameter and the reset interval."
we have tested the performance of flashdetect using both synthetic traces as well as real traces from the bittorrent community filelist.org. a short description follows.
"in this section we shortly describe the use of bn for grasp planning and learning of goal-directed grasping policies. goal-based grasp planning involves decisions over (a) object selection and (b) grasp realization so that a given task, out of several, can be afforded. let also t be a selection over common house-hold task set t that an agent might intend to do after grasping an object. let o be the set of variables describing object category, size, and shape-related features. let a be the set of variables that parameterize a grasp action such as hand position, orientation and articulation. planning a grasp is then to decide o and a given t . in addition to o, a, we also introduce a set of \"constraint\" variables, denoted as c that further describe the configuration of the object and hand in the final object-grasp complex. an example c variable is free volume that quantitize how much free space on the object is left uncovered by the hand in final grasp configuration."
where s k is intra-cluster distance and d kl the inter-cluster distance. k is the minimizer of i db (see fig. 2 c) . each data point of the original data set x is assigned to the cluster of its prototype w j .
"estimating a continuous representation from soft evidence is an \"inverse problem\" in soft discretization as it recovers the continuous value from discrete states. this is also known as de-fuzzification in fuzzy set theory, which does not have a simple solution [cit] . a practical approach for interpreting the output of a discrete bayesian network is to use the most likely point of the inferred variable as the output. we can define a distribution over the continuous space x associated with the multi-nominal distribution p k,"
"we base our solution for the distributed online flash-crowd detection on an extension of the gossiping-based algorithms that incorporates periodic computational resets. we have shown in previous work [cit] that this mechanism, called diffusionreset, retains the properties of the original diffusion mechanism, achieving convergence exponentially fast, and actually counting on the existence of mass loss."
"the optimal decisions regarding the bs working mode for different periods are given in table 2 . the bs working modes vary according to the traffic load fluctuations in each cell, such that the optimal number of bss which maximizes the amount of energy saving and provides a satisfactory service quality level are on."
"in the online recognition phase, a point pair is sampled from the input scene, the two normals are estimated and the same geometric descriptor is computed. next, the descriptor signature is used as a key to the hash-table to retrieve all model point pairs similar to the one sampled from the scene. note that two pairs of points with normals are sufficient to compute a unique rigid transform which aligns them. this results in a number of hypotheses each one consisting of a model and a rigid transform which maps it to the scene. a hypothesis is accepted as a solution if it passes several verification stages. this process of sampling, hypothesis generation and verification is repeated a number of times such that all objects in the scene are recognized with a certain user-defined probability."
"trackernetsize is shown by algorithm 3. the tracker nodes reset to 1 à n e ½r while all the other nodes asynchronously reset to 1. let q½r be the overall residual value of the mass in the network, at the end of round r (under convergence assumptions, this has the same value for all the nodes). let q½r 1 be the residual value after the first gossiping step of the round (i.e., first one after the leader node resets). it follows:"
"the proposed wsn architecture was also carefully analyzed in terms of strengths, weakness, opportunities, and threats (swot), as shown in figure 9 [cit] . the engineering tool investigated and characterized in this work offers a versatile multimedia management, combined with a real time messaging system. furthermore, the system is selective, since it can easily track and discriminate the nodes and vehicles, implying that the overall quality of service can be improved. as regards the cost-effectiveness of the system, considering the functionalities, the choice of the raspberry pi as sbc allows reducing the cost by about 30%, with respect to the nano-ult3 and the vb0x-3120, as shown in table 5 . this low-cost solution for automotive application can be appealing for the automotive field because of the open source hardware employed, which is a flexible, reliable, easy to update, and portable platform. the flexibility and high modularity add new functionalities with respect to the other devices currently available, at a lower price (see table 1 ). however, despite the advantages, strengths, and promising findings of the proposed wsn architecture, it must be pointed out that the design and results are at a preliminary stage. hence, one major limitation is that the automotive certification for the raspberry pi unit is not available at the moment, but some recent literature findings encourage the possibility to easily obtain it [cit] . furthermore, even though it has been demonstrated that the temperature, humidity, and bus vibrations do not affect the management of multimedia content, the data transmission in upload and download should be optimized and enhanced to avoid instabilities, since this is a primary requirement for the case company. to these weaknesses, it should be stressed that the threat of wsn security is a pivotal aspect that deserves to be investigated in the future. future works may deal with the empowerment of the proposed wsn node. for instance, a thermographic or optical camera [cit] could be integrated to derive useful features and information to measure some parameters related to the quality of the service or the security during the transportation, e.g., the count of passengers or the detection of passengers smoking on the bus."
"be grasped from all directions, but in pouring, the hand can not cover the opening. to achieve the goal of hand-over, a grasp should not cover the entire surface of the object, thus it should induce larger free volume. so when f vol observation is missing, we observe a performance drop in the blue roc curve. similarly, a tool-use task requires grasp to be on the handle part of an object such as a hammer, therefore when the part-relevant observation gbvl is missing, we observe worse performance in goal inference. the most interesting results is when we tested the goal inference with real demonstration using the model trained with the simulated data. here the observation is partial where information of constraint variables is missing. the red triangles superimposed on the roc curves on fig. 5 show the result at the optimal classification thresholds. we observe that goal inference on real human data is as good as the results on the simulated data. this confirms the ability of our model to generalize to real-world data."
"all the signals from the accelerometer, temperature, and relative humidity sensors are digital in nature. therefore, all the elaborations steps and the filtering were carried out using software routine written in python and handled by the raspberry unit."
"on the other hand, service providers can take advantage of network cooperation to enhance network performance. multiple heterogeneous networks can cooperate to provide a multihop backhaul connection in a relay manner. this can increase the coverage area of such networks. moreover, cooperation among different networks can provide load balancing among these networks. as a result, traffic overload situations in one network can be avoided. also, network cooperation can be exploited to save energy in green radio communications. for geographical regions where two or more networks have overlapped coverage, cooperation among the networks can achieve energy saving and avoid the dynamic planning shortcomings. such networks can alternately switch on and off their resources according to the traffic load conditions, and the traffic is carried on by the remaining active resources. in such a case, an optimal resource on-off switching framework is required to adapt the available resources to the traffic load fluctuations and to maximize the amount of energy saving under service quality constraints."
"from the analysis of this signal, it is possible to derive the peaks related to the maximum stresses which arises from the vibrations and stresses."
"for better insight on the benefits of cooperative networking, the next section provides some discussion of the future heterogeneous wireless access environment and the potential of cooperative networking."
"wireless sensors network are recognized as a powerful expression of the internet of things. however, despite the great advantages offered by this technology, their application has been partially investigated in automotive applications, especially public transportation and intelligent transportation systems. a first goal was the investigation of whether open-hardware software can be safely and effectively employed in a peculiar scenario such as a bus. moreover, the aim of this study was to develop a cost-effective wsn node that can monitor several physical parameters (i.e., acceleration, temperature, and relative humidity), while managing the download and streaming of multimedia content, as well as set up an automatic alert system. therefore, in this work, several off-the-shelf, open, and cost-effective hardware components were compared to identify a suitable choice for the node and wsn design to be employed in the automotive field. the raspberry pi unit is a good candidate, given the fact that the emc has been recently studied [cit] . the wsn was set up in the bus vehicles and it was tested for three months, i.e., [cit] . during this period, the physical quantities of interest were monitored and analyzed. after preliminary test of the efficiency of the download rate of multimedia contents, given the variability of this quantity, a correlation analysis was performed. then, a linear multivariate regression was investigated to derive a model that could describe the variation of this relevant quantity with respect to the environmental and operative parameters. the findings indicate that the lower is the magnitude of the vibrations experienced by the wsn node, the higher and the better is the download rate. therefore, as a conclusion, the proposed solution can be employed in automotive applications."
"when all the nodes reset to the same value, diffusionreset is not really useful. its power comes when two sets of nodes reset to two different values. the overall mass in the system will converge to an aggregate that is a function of the number of nodes in each set, thus allowing each node to estimate the sizes of the sets. this is the basic idea behind the algorithms proposed in this paper: some nodes (the new nodes in the network or a single special node) reset to a different value than all the others. this enables nodes to obtain an estimate of the churn (or network size in general), with no need to advertise their departure and no need for synchronization. by design, diffusionreset does not require a constant mass in the system. the variation in the mass value at each node is flattened by means of gossiping (also called the anti-entropy effect)."
"let n½r be the number of nodes at round r. the number of new nodes joining at round r is n i ½r and nodes leaving is n o ½r. the percentage of nodes joining at round r compared to the previous time round, r à 1, is"
"the paper is structured as follows: in section 2 we describe the existing state-of-the-art. in section 3 we introduce the underlying gossip-based diffusion mechanisms, while in section 4 we present the flashdetect and trackernetsize algorithms. the results of simulations and experiments are covered by the next two sections. the paper concludes with section 6."
"energy awareness in wireless communication networks has been studied for a long time in mobile devices and wireless sensors, due to their limited power capabilities. recently, such awareness has been extended to include cellular network bss, due to financial and environmental considerations. three categories of solutions can be defined to provide an energ-aware infrastructure in wireless communications networks. these are discussed below."
"the data acquired from the temperature, relative humidity, and accelerometer sensors were analyzed to investigate if any quantitative relationships exist among them. to this aim, the pearson's correlation coefficient between two generic variables x and y can be evaluated as follows [cit] :"
"in this way, the time sequence of the vibrational acceleration was evaluated (i.e., a v,x [n], a v,y [n], a v,z [n]). finally, these three filtered signals were combined and used to calculate the norm a v,n [n] [cit] :"
"the simplest approach to the problem of estimating the network (swarm) size relies on individual peers interrogating each other for availability. other approaches ask nodes to advertise to special bootstrap entities their joining or leaving of the swarm (e.g., bittorrent [cit] ) or diffuse membership information via a gossiping approach [cit] . while these approaches are straight-forward and can be implemented by p2p systems, they suffer from the problem of information being lost due to firewalls [cit], poor network connections, sudden death due to various software or hardware glitches etc. another approach being used to track a swarm size, with the additional benefit of reduced message complexity, is the random walks class of algorithms [cit] ."
"the first approach, namely flashdetect algorithm, is fully distributed and provides a measure of the relative variation of the network size. the second algorithm -(trackernetsize) provides an absolute measure of the network size. for diffusionreset, this is possible only if a ''special'' node (resets to a different values than the rest of the nodes) is present. the algorithm remains fully distributed though -each node is involved only in local interactions with its neighbors. although flashdetect solves the problem of flashcrowd detection, we provide the second algorithm for the sake of completeness and, also, as an alternative solution for the case of live streaming systems, where the source peers are (supposed to be) always online."
"the signals of the accelerometer, temperature, and relative humidity sensors were high-pass filtered using a 16-tap moving average filter to slightly smooth the signal without adding a delay."
"where both the discrete time n and the fourier index k assumes values between 0 and n − 1, being n the number of time samples. then the spectrum of each channel (i.e., a x [k], a y [k], and a z [k]) is high-pass filtered to extract the components above a given threshold frequency called k th, equal to about 1 khz. these operations allow removing, canceling, and eliminating the frequency components due to the gravitational acceleration (a g [n]) and to the low-frequency noise (rn[n] ), e.g., the road curves or slowly time-varying increment of velocity [cit] . in this way, the remaining spectrum is the spectrum of the accelerations due to the vehicle vibration [cit] . in mathematical terms:"
"object grasping actions in humans is then an integral part of enabling robots to act and interact with the environment. the intention in the scope of object grasping and manipulation is what kind of tasks a human / a robot intend to do after sucessfully grasping the object. an example of such highlevel tasks is to hand-over an apple as shown in fig. 1 . most of the work that addressed this problem is either evaluated in simulated environments where perfect knowledge of objects and grasps is available [cit], or in situations where the human teacher is equipped with magnetic sensing or datagloves [cit] ."
"accuracy: the percentage of succeeding peers, i.e., initial peers who succeeded in detecting a flash-crowd before its end; delay: the delay (in minutes) between the occurrence of a flashcrowd and its detection from the succeeding peers."
"in order for inferring the intention given natural observations, the latter need to be translated into the domain of the proposed probabilistic model. in our work, the observations are acquired using a rgbd sensor [cit] . translation is achieved by means of a novel hand-object 3d tracking system that combines state-of-the-art methods for object recognition [cit] and hand-object 3d tracking [cit] ."
"the main contribution of the paper is two algorithms. the first one, named flashdetect, is a fully distributed online method for estimating the occurrence of a flash-crowd. it is based on the idea that new nodes joining a swarm reset once to a different values than all the other nodes (i.e., as part of the diffusionreset algorithm). the aggregated value, available at all the nodes in the swarm, will reflect the change in the set of nodes. to the best of our knowledge, flashdetect is the first online flash-crowd detection mechanism."
we have tested the performance of the proposed algorithms under different flash-crowd scenarios by means of extensive simulations performed with our extended version of the bittorrent simulator designed by microsoft research [cit] . the results bring evidence toward the validity of our approach.
"abstract-the main contribution of this paper is a probabilistic method for predicting human manipulation intention from image sequences of human-object interaction. predicting intention amounts to inferring the imminent manipulation task when human hand is observed to have stably grasped the object. inference is performed by means of a probabilistic graphical model that encodes object grasping tasks over the 3d state of the observed scene. the 3d state is extracted from rgb-d image sequences by a novel vision-based, markerless hand-object 3d tracking framework. to deal with the high-dimensional statespace and mixed data types (discrete and continuous) involved in grasping tasks, we introduce a generative vector quantization method using mixture models and self-organizing maps. this yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. experimentation showed that the model trained on simulated data can provide a potent basis for accurate goal-inference with partial and noisy observations of actual real-world demonstrations. we also show a grasp selection process, guided by the inferred human intention, to illustrate the use of the system for goal-directed grasp imitation."
"diffusionreset works, in short, as following (see algorithm 1). each node i has a local state variable m i ½k at the beginning of the communication time step k (we will refer to m i as ''mass''). for the case of push-pull gossiping strategy, nodes average their mass by exchanging half of their value with one random neighbor. for a push gossiping strategy, at each time step, nodes split their local state variable in several shares that get distributed to their neighbors. at the end of each time step, the nodes add all the shares of received variables and update their local variable m i . the effect of this mechanism is that, with time, all local variables converge to the same value (the average of the original variable set). an important aspect is that the convergence is achieved regardless of the synchronization type (or lack of it) [cit] ."
"in this section we introduce the main contribution of this paper: two approaches for detecting and characterizing the flash-crowd phenomenon. both approaches can be characterized as distributed and ''online'', in the sense that the aggregate information is available in real-time to all nodes in a swarm, by using only local exchanges of information. to utilize the concepts of diffusionreset within p2p swarming systems, we have considered each swarm as a separate network. hence, in the scope of this paper, we will use the term ''network'' to refer to one swarm only."
"in all scenarios, the initial network size is 10 peers, while the total flash-crowd size is 150 peers. the arrival rates for the three simulated scenarios are as follows:"
a larger diameter requires a larger reset value and viceversa due to the fact that diffusion speed via gossiping is a function of the network diameter.
"from a pure network size estimation accuracy, as seen in fig. 8(b), the synchronous algorithm achieves close to 100% accuracy almost independently of the reset interval size. for trackernetsize, the accuracy is worst at small and high reset intervals, while still close to the syncalg. the explanation for this behavior is the following. at small reset intervals, the diffusion process does not have enough time to converge. although the graphs do not show it, at reset intervals smaller than 5 min, the accuracy of the algorithm is very bad. when the reset interval is large, the tracker is not able to follow the network size fluctuations. in other words, the diffusion-reset mechanism is so slow that it cannot follow the dynamics in size."
"the second algorithm we introduce, named trackernetsize, is an extension that can be applied to the case of a swarm in which a special node (or a small set of nodes) can be singled out (e.g., the content injector or the stream source). the ''special node'' executes a modified version of the diffusionreset algorithm, by dynamically adjusting its reset value. this leads to all nodes in the swarm being able to compute an aggregate which reflects the absolute swarm size. each node can track the absolute swarm size to detect the occurrence of a flash-crowd."
"where f t is a non-negative convex function representing price elasticity. under this setting where we further impose parametric constraints on the revenue function д t, we obtain improved results on finding a competitive ratio π ."
"in this paper, we focus on an important class of online optimization problems that has proven challenging: online optimization under inventory (budget) constraints (ooic). in these problems, a decision maker has a fixed amount of inventory, e.g., airlines selling flight tickets, and must make sequential decisions without knowledge of future revenue functions or the stopping time t . further, the strict inventory constraint means that current actions have consequences for future rounds. as a result of this entanglement, positive results have only been possible for inventory constrained online optimization in special cases to this point, e.g., linear revenue function considered in the one-way trading problem [cit] ."
"applications. beyond that, ooic also captures a variety of other applications. three examples that have motivated our interest in ooic are (i) power contingency reserve markets [cit], (ii) network spectrum trading [cit], and (iii) online advertisement, further illustrated in the main work."
"in the one-way trading problem [cit], a trader owns some assets (e.g., dollars) and aims to exchange them into other assets (e.g., yen) as much as possible, depending on the price (e.g., exchange rate). there is a long history of work on one-way trading, e.g., [cit], and ooic includes both the classic one-way trading problem and variations with concave revenue functions and price elasticity."
"we next focus on practical settings where we can further improve this approximation ratio. in particular, we consider the one-way trading problem, except with price elasticity, i.e., the set of all possible revenue functions can be expressed as"
"our cr-pursuit framework focuses only on achieving competitiveness under worst case inputs, limiting its application. intuitively, a \"better\" online algorithm would be more opportunistic and sell more of its inventory when the incoming revenue function is \"not adversarial\". by design, cr-pursuit is pessimistic: it only maintains a fixed competitive ratio π * for the whole trading period, even if some inputs are not adversarial. one way to improve the performance of cr-pursuit is to instead adaptively choose π t to maintain at time t, the smallest attainable competitive ratio at time t."
"we remark that suchv t always exists, because (i) д t (·) is a continuous and increasing function and (ii) the quantity required to maintain π is in [д t (0), д t (v t )], wherev t is the maximizer of д t (v). while cr-pursuit(π ) can be defined for any π, the solution obtained by cr-pursuit(π ) may violate the inventory constraint in ooic and be infeasible. this motivates the following definition."
"in this section, the experimental setup and results analysis of the pbc along with borderflow clustering algorithm and the entire system are analyzed. it also includes the prediction validation and dataset collection for the experiments. finally, the effectiveness of the proposed pbc is measured by taking the mean, variance and standard deviation of absolute residuals."
"the comparison among the reported on-chip stimulators in recent literature [cit] and this design is summarized in table ii . to sustain high stimulus voltage caused by the product of loading impedance and stimulus current, high-voltage processes are usually used. in this work, the stimulator has been designed to deliver the required stimulus current by using low supply voltage (3.3 v). since the other circuit blocks of the implantable device use the same supply voltage, the stimulator can be easily integrated into the implantable device as a system in package (sip)."
"we study the effect of the number of publishers on the message throughput. two different machines carry a varying number of publishers and one machine hosts 1 subscriber. the throughput of received and dispatched messages as well as their sum which we define it the overall throughput. the overall message throughput reaches its maximum rate at 1200 msgs/s for 3 or more publishers. hence, the number of publishers has very little influences on the jms server throughput. as a consequence, we use in the following experiments at least 3 or more publishers."
"before the jms api existed, most messaging products supported either the point-to-point or the publish/subscribe approach to messaging. the jms specification provides a separate domain for each approach and defines compliance for each domain. a standalone jms provider may implement one or both domains. a j2ee provider must implement both domains. in fact, most current nt implementations of the jms api provide support for both the point-to-point and the publish/subscribe domains, and some jms clients combine the use of both domains in a single application. in this way, the jms api has extended the power and flexibility of messaging products."
"3) loosely coupled: a component sends a message to a destination, and the recipient can retrieve the message from the destination. however, the sender and the receiver do not have to be available at the same time in order to communicate."
ptp messaging has the following characteristics and is illustrated in figure 2 . the common example of point to point messaging model is banking transaction in which transactions like crediting for debiting money from his own account. as one person performs operations on his accounts it is one to one communication.
"the comparison of mean value of pbc, borderflow and the entire system are graphically illustrated in figure 2 . in this figure, it is clearly visible that the mean value of pbc is small in 4 datasets out of 8, compared to borderflow algorithm. and it also highlights that the mean value of pbc is smaller than the entire system in all datasets."
"to deliver the stimulus current with the varied loading impedance, some approaches have been studied. for example, the stimulator can monitor the loading impedance first, and then setting the operating voltage [cit] . of course, the monitoring circuit adds more area and complexity to the stimulator."
"and total power consumption of stimulator verses loading impedance. although the measured was little higher than the simulated one, the 40-stimulus currents can be constantly provided. the average power consumption of the stimulator is 1.1-1.4 mw. the higher power is consumed in the comparator (c1), which is mostly consumed as clock transition, so the comparator design can be further optimized to reduce the power. for example, the clock frequency used in the comparator can be decreased to lower the power consumption. all of measurement results are summarized in table i ."
"to compare pbc with existing approaches, an experiment has been conducted on open source software which are ant and xalan from promise repository [cit] . at the beginning of the experiment, selected software were partitioned into multiple clusters using pbc. then the linear regression model has been applied to each cluster to find predicted defects. finally, to show the importance and effectiveness of the proposed pbc, results are compared to the prediction model considering borderflow [cit] and the entire system. in this context, by considering the oo classes relationships and similarities, pbc outperforms the entire system in all datasets and performs better than the borderflow clustering algorithm in 4 datasets out of the total."
as quickmq is mainly provider for java message service the name itself suggest that it is mainly implemented in java as specifications are available in java language. j2ee version 1.5 is used in implementation of quickmq. as shown in fig. 4 1) connection factory -an administered object used by a client to create a connection. 2) connection -an active connection to a jms provider. 3) destination -an administered object that encapsulates the identity of a message destination or ca be called as virtual channel for communication. 4) session -a single-threaded context for sending and receiving messages. 5) message producer -an object created by a session that is used for sending messages to a destination. 6) message consumer -an object created by a session that is used or receiving messages sent to a destination.
"in this paper, to perform software clustering using related and similar properties, a new clustering approach named as package based clustering (pbc) is proposed. it is based on the related and similar oo classes that form packages in java programing convention. usually, package holds the related and similar features of software project. to group software based on the package information, it uses textual analysis on source codes to identify oo classes from software project, and then it lists those files for further analysis. to form clusters, it extracts the package information from each oo files by searching the package name. it then groups all oo files based on the package information where files having the same package belong to the same cluster, and different files from different packages reside to the different clusters. after forming clusters, it selects linear regression as the defect prediction model to predict defects by training the model using the clusters of dataset. in special cases, if the number of oo classes in a cluster is smaller than the number of independent variables used in the prediction model, it combines small clusters to enable those for the prediction model. finally the selected linear regression model is conducted on ant and xalan considering pbc, borderflow and the entire system."
"the code or software metric is a quantitative measure to define the quality of software. there are lots of software metrics such as method level, class level, component level, process level, etc. [cit] . those software metrics have multidimensional usage in the software industry such as schedule and budget planning, cost estimation, quality testing, software debugging, software performance optimization, etc. apart from those, it has been used for defect prediction to improve software qualities. among all the metrics, the method level metric is widely used in structured programming and object oriented programing (oop) paradigm and the class level code metric is only used for oop paradigm. all of the above metrics' properties are not significant for defect prediction [cit] . to analyze the importance of those metrics, many researches have already been carried out to identify the best set of metrics for defect prediction. some of these researches are listed below."
"software defect prediction (sdp) models use software code metrics and knowledge from previous projects to predict defects for future releases of software. it can help practitioners to assess their current project status, and to reduce the software development cost by estimating the faultiness of software in advance. many researches have been conducted using different defect prediction models, for example, neural network, naive bayes, regression modeling, decision tree, etc. [cit] ) to predict by training the model using the entire system before software testing process. due to variabilities in software data (such as the heterogeneities among different code metrics in dataset), prediction models considering the entire system do not always provide the desired results [cit] . many researches show that grouping the software source code based on their properties improves the performance of the defect prediction model [cit] . so prediction model trained from clusters of the dataset might produce better results than those considering the entire system [cit] ."
"in case middle scale enterprises, some these functionalities are not being used and those functionalities are mandatory to be taken in whole console also these functionalities are running in background irrespective of their use, as middle scale enterprises mainly focuses on resource efficient products. for example \"distributed transactions\" are mainly used in critical business applications like banking transactions were these transactions are need to atomic(acid property) [cit] these can be achieved through \"commit and rollback\", which results in more processing time and large memory space. but in middle scale enterprises do not always process critical business application for example simple news broadcasting or sports updates do not require to use acid properties, which leads to unnecessary usage of resources."
"the schematic circuits of the output driver and adaptor are shown in fig. 3 . in the output driver, the stimulus current source consists of a current mirror (mp1 and mp2) and bias circuit (mn1, mn2, mn3, mp3, and ). the 3.3-v supply voltage, variable voltage, and stimulation signal (stim.) are used in this circuit. the is used to provide the bias voltage for the gate terminal of mn1. during the stimulation \"on\" interval, the stimulation signal is high, mn2 is turned off, mp3 is turned on, and mn1 is biased through mp3. the mn1 is biased to operate in saturation region under stimulation. the mp1 and mp2 are also designed to operate in saturation region under stimulation. the voltage will increase or decrease to keep delivering 40-stimulus current under stimulation. besides, if the is initially stored at higher voltage and applied to the lower loading impedance, the can limit the at . the can also prevent the tissue from large glitch current. during the stimulator \"off\" interval, the stimulation signal is low . mn2 is turned on and mn1 is turned off, so there is no stimulus current delivered."
"under measurement, agilent e3631a is used to provide the fixed 3.3-v . hp 33120a is used to supply the 25-mhz reference clock signal and to send the stimulation signal. tektronix 3054b is used to observe the stimulus voltage/current in the loading."
"the switches are used to conduct positive and negative stimulus currents ( and ). besides stimulation intervals, the switches are shorted to ground to prevent from charge accumulation in brain tissue."
it is the measure of counting the total number of methods in a class that are not related through the sharing of some of the class fields.
"t here are of the people in the world affected by the epilepsy [cit] . this disease is caused by transient abnormal discharge in brain, and it is one of the common neurological diseases [cit] . if this seizure cannot be well controlled, the patients will be affected in sensations, emotions, memories, and other related activities. typically, the epileptic seizure can be treated by pharmacologic or surgical treatments. the most common way to suppress epileptic seizure is the pharmacologic treatment [cit] . there are more than 20 types of medications for specific types of epileptic seizures. however, there are still many patients who can not be cured by the medications [cit] . for these medically refractory patients, an alternative is the surgical treatment [cit] . in this treatment, some tissues with abnormal discharge will be removed from the brain. it is a risky treatment because patients may loss some physical functions permanently after taking the surgery. in addition, some patients still can not be completely cured by the surgery [cit] ."
"the paper is organized as follows. in section 2, the methodology for improving the accuracy of software defect prediction model has been described. section 3 is dedicated to describe the software defect prediction model, the selection of independent variables and existing clustering algorithm. section 4 illustrates the experimental setup and result analysis of the proposed technique which also includes prediction validation approach and the selected datasets. the related work section has been included in section 5. and finally the conclusion is provided in section 6."
"during the stimulator \"off\" interval, the clk signal keeps at low to inactive the charge pump circuit. although the charge pump circuit stops pumping, some charges still store in the output capacitor to keep the at higher voltage within a short time. the used in this work is 15 pf, and the time constant to discharge the during the stimulator \"off\" interval is ."
in publish/subscribe (pub/sub) application; clients address messages to a topic. publishers and subscribers are generally anonymous and may dynamically publish or subscribe to the content hierarchy [cit] . the system takes care of distributing the messages arriving from a topic's multiple publishers to its multiple subscribers. topics retain messages only as long as it takes to distribute them to current subscribers. pub/sub messaging has the following characteristics.
"software defect prediction model predicts defects of a class by analyzing the relationships between software code metrics and software defects. in this paper, the linear regression model is selected as the prediction model to predict defects of a particular class because the relationships among code metrics are linear [cit] . the linear regression model uses a set of independent variables which are wmc, cbo, dit, lcom, loc, npm, rfc, noc (see section 3.2 for detail description) to predict the it is the sum of all methods in a class."
"in quickmq messaging system applications exchange messages through virtual channels called destinations (i.e. queue or a topic). when message is sent, it\"s addressed to a destination, not a specific application. any application that subscribes or registers an interest in that destination may receive the message. in this way the applications that receive messages and those that send messages are decoupled. senders and receivers are not bound to each other in any way and may send and receive messages as they see fit."
"existing clustering techniques consider a number of source code characteristics, for example, source code dependencies or similarities, code metrics' relationships, lexical similarities to group software into multiple clusters. no such technique yet considers software project's structural information such as package information, which holds the related and similar source code together. in this paper, a new package based clustering (pbc) algorithm is proposed to group software using the package information because package holds the related and similar objects in an oo system [cit] . a java project hierarchy to represent project, packages and classes is depicted in figure 1 . in java programing convention, a package is a name space that organizes a set of related and similar classes and interfaces. conceptually, packages are similar to different folders in a computer. a package allows a developer to group classes and interfaces together. these classes are related in some way that they might perform a specific set of tasks. programmers also typically use packages to organize classes belonging to the same category or providing similar functionality. as the goal of this paper is to group related and similar classes, clustering using package information (described in algorithm 1) can easily meet the purpose."
"let, a cluster x, is a subset of v, b(x) is the set of border nodes of x, and n(x) is a function used to identify the set of direct neighbors of x. ω is a function that assigns the total number of edges such as dependencies from a subset to another subset using equation 2. then borderflow ratio can be measured using equation 3."
"a client that subscribes to a topic can consume only messages published after the client has created a subscription, and the subscriber must continue to be active in order for it to consume messages [cit] . the jms api relaxes this timing dependency to some extent by allowing clients to create durable subscriptions."
"quickmq provider for jms provides a common way for java programs to create, send, receive and read an enterprise messaging system\"s messages. in comparison with traditional mom it provides addition with the following features 1) asynchronous messaging: sender doesn\"t require waiting till acknowledgement of previously sent message is received. it results in increased speed [cit] ."
it is the length of the longest path from a given class to the root class in the inheritance hierarchy. number of children (noc) it simply measures the number of immediate descendants of a class. coupling between objects (cbo)
"now days rmi (remote method invocation), rpc (remote procedure call) are mainly used communication in heterogeneous systems, but these methods are synchronous and are tightly coupled [cit], we can\"t do any other kind processing till the current operations gets completed, so we have to wait till current operation completes, which may lead to starvation in some cases [cit] . these synchronous methods may affect performance of system in case of large record processing. sometimes when we need to upload certain big record like csv(comma separated files) files of say 1,00,000 elements then till all elements are not being acknowledged we cannot process any other request, which may lead to blocking of system and starvation. in tightly coupled systems there are many-to-many problems of managing connections between these systems [cit], when you want to add some more application to mix, we need to go back and let all other system know about it, which is rather more tedious and leads to problems in up-gradation of system. also if some part of system goes down, everything halts. in this case there is more load on server as all processing load is being handled by server because of which leads to system bottlenecks and system halts."
there is a still room for improvement in software defect prediction process by incorporating the impact of other code metrics impact to the software defects. finally the proposed approach can be performed on some real life software projects by working collaboratively with some software companies.
"the authors are with the nanoelectronics and gigascale systems laboratory, institute of electronics, national chiao-tung university, hsinchu 300, taiwan, and also with the biomedical electronics translational research center, national chiao-tung university, hsinchu 300, taiwan (e-mail: cy.lin@ieee.org; mdker@ieee.org)."
", which leads to lower than, the output of comparator will become low . the high voltage generator will stop pumping to provide lower voltage until the returns 40 . the feedback design of adaptor keeps the stimulus current at . the comparator used in the adaptor is shown in fig. 4 [cit] . under stimulation, the comparator turns on. when the clock signal is low, the comparator is pre-charged, and there is no dc current path from power to ground. when the clock is high, the comparator compares the and, and the feedback loop latches the comparator into steady state. in steady state of comparator, there is also no dc current path from power to ground. the comparator without dc current is very power-saving."
"similarly to the above, we investigate the impact of the number of subscribers on the jms server throughput. to that end, we have 3 publishers\" threads running on one machine and vary the number of subscribers on two other machines. the overall throughput of the jms server starts at 1200 msgs/s for 1 subscriber, it increases with an increasing number of subscribers to a value of about 3200 msgs/s for 20 subscribers, and it decreases then to 1300 msgs/s for many subscribers. thus, the previous experiments led to a untypically low capacity due to the single subscriber. the received message rate decreases significantly with an increasing number of subscriber\"s n. this can be explained as follows. no filters are applied and all messages are delivered to any subscriber. thus, each message is replicated n times. this requires more cpu cycles for dispatching messages and increases the overall processing time of a single message. as a consequence, the received message rate is reduced because the overall throughput capacity of the server stays constant. the throughput of a jms server can be measured in messages per the message body size has certainly an impact on these values. we test the maximum throughput depending on the message size. we set up 5 publishers on one publisher machine and 5 subscribers on a single subscriber machine; a single subscriber is not able to fully utilize the server cpu. the throughput in msgs/s is measured but the throughput in mbit/s is derived from these data. the calculation of the corresponding overall message size takes into account various message headers, i.e., 40 bytes jms header, 32 bytes tcp header, 20 bytes ip header, and 38 bytes ethernet header, as well as tcp fragmentation. an increasing message body size decreases the message throughput and increases the data throughput significantly. for small message bodies, the message throughput is limited by 3000 msgs/s while for very large message sizes, the data throughput increases significantly up to 300 mbit/s. obviously, the network interface of the jms server becomes the system bottleneck."
"we build application of having multiple functionalities in different languages and we try to make these functionalities run asynchronously on different language processors without waiting for other, except for those functionalities having dependencies between them. now a day\"s all applications are complex and support different functionalities which can be implemented effectively in their respective languages depending on functional requirements [cit] . these different functionalities are now a days can be executed by using java but these facilities are rather synchronous,only after performing single functionality the other can be executed which may take longer for complete execution, these mainly avoided in quickmq and different functionalities are performed asynchronously which rather reduces clock time of complete execution we have implemented light version of jms provider but still we able to maintain the original performance nearly as that was for heavy weight jms provider. in this work, we have tested quickmq for the performance and we tested it in various conditions."
jms api for enterprise messaging created by sun microsystems. it is not just messaging system itself; but it is abstraction of the interfaces and classes needed by messaging clients when communicating with messaging system [cit] . figure 1 illustrates the way these parts interact. administrative tools allow you to bind destinations and connection factories into a java naming and directory interface (jndi) api namespace [cit] .
"during the stimulator \"off\" interval, the stimulation signal (stim.) is equal to 0 v, so the last-stage inverter of comparator outputs high signal . even so, the stim. stops the high voltage generator from pumping to high voltage in this interval."
"the error value is the ratio of two techniques' mean difference and the standard deviation of the technique to which another technique is compared. as a result, it assumes values in between -1 and +1. for a software release, negative values of error indicate the proposed approach pbc outperforms borderflow technique, while positive values indicate the borderflow outperforms the proposed pbc."
"quickmq being provider it provides mom facilities which reduces load on server and reduces bottlenecks in turn, .quickmq being asynchronous, we don\"t have to wait till completion of operations, as in case synchronous uploading of csv files of 10,000 records we need to upload all records first and then perform processing on them, but till all records gets uploaded it needs to wait and cannot process another processing\"s, this problem is now being solved in quickmq by using asynchronous message communication in which some amount of records can be uploaded and can perform processing\"s on those uploaded records and can also upload remaining records simultaneously, which may lead to save much more time and does not lead to halt of system."
durable subscriptions can receive messages sent while the subscribers are not active. durable subscriptions provide the flexibility and reliability of queues but still allow clients to send messages to many recipients [cit] .
"during the stimulator \"on\" interval, the stimulation signal is high, and the clock controller starts to send the frequency-modulated clock signal to the 4-stage charge pump circuit. if the ctrl. signal is low, the reference clock signal is modulated to the lower-frequency output (clk). likewise, if the ctrl. signal is high, the reference clock signal is modulated to the higher-frequency clk. during the stimulator \"off\" interval, the stimulation signal is low, and the clk signal also keeps at 0 v, so the 4-stage charge pump circuit stops pumping."
"the high voltage generator of stimulator uses the charge-pump-based circuit, as shown in fig. 5(a) . the high voltage generator consists of the 4-stage charge pump circuit, clock controller, clock buffers, and output capacitor . the clock controller is utilized to generate the frequency-modulated clock signal (clk) which depends on the reference clock signal (clock), stimulation signal (stim.), and the feedback signal from comparator (ctrl.)."
"the main purpose of creating new implementation of java message service is its weight. most of the implementations of jms are very costly and bulky consisting of many features, which are not required for many small or middle level organizations. such implementations also do not provide any provision to turn off those features. so we targeted those small and middle level organizations and came up with elegant solution quickmq. it provides absolutely necessary and sufficient features with very efficient implementation of those. so to be abstract its \"as light as \"light\" and as fast as \"light\""
"in this experiment, the detection electrodes were bilaterally implanted over the area of the frontal barrel cortex (anterior 2.0 mm, lateral 2.0 mm with regard to the bregma). the stimulus current is conducted by a 4-microwire bundle, each made of teflon-insulted stainless steel wire, to stimulate the rightside zona incerta of long-evans rat (posterior 4.0 mm, lateral 2.5 mm, and depth 6.7-7.2 mm). the diameter of the microwire (#7079, a-m systems) is 50"
"in this paper, a new technique to group the source code for software defect prediction is proposed to improve the accuracy using package based clustering (pbc) rather than the entire system. pbc groups java based software into a number of clusters using package information of each object oriented (oo) classes. to find clusters, pbc lists all oo classes by using textual analysis of source code. it then reads those classes to extract the package information. the clustering approach ensures that the classes having similar package information reside in the same cluster. then the linear regression based defect prediction model is applied on those clusters. the prediction model predicts defects by establishing the relationships between software defects and a set of independent variables such as number of public methods, lack of cohesion in classes, coupling between objects, etc. in the case of small clusters when the number of observation in a cluster is smaller than the number of variables used in the defect prediction model, it combines those clusters to form a joint cluster in order to enable those for the prediction model. now the prediction model gets better dataset in training phase, which results better prediction accuracy."
"the proposed stimulator has been simulated in hspice with the 0.35-3.3-v/24-v cmos process. fig. 6 shows the voltage/current waveforms of, tissue, and as one stimulation with loading impedance is 300 . during 0-2, the stimulation signal is low, and the stimulus current keeps at 0 a. in the same duration, the is initially set at 3 v, which simulates the voltage stored in the output capacitor from previous stimulation. during 2-10, the stimulation signal is high, and the stimulus current is sent. the required time for stimulus current to reach 40 is 1.26 . after that, the stimulus current keeps at once the stimulation signal keeps at high. in this time period, the voltage of moves around, and the voltage of also moves around, as shown in fig. 6(a) . although it seems an oscillation, the waveform is controlled by the adaptor. as is higher than, the high voltage generator pumps the to higher voltage. while is lower than, the high voltage generator stops pumping, so the decreases. the stability and reliability of this circuit have been considered in design phase [cit] . in this stimulation of current pulses with amplitude, 0.5-ms pulse width, 2.5-ms period, and 0.5-s duration, the total power consumption of the stimulator is 0.9 mw in which 0.6 mw is consumed in the high voltage generator, 0.11 mw is consumed in the output driver, 0.1 mw is consumed in the adaptor, and 0.09 mw is consumed in the switches."
"to find group of related classes, this algorithm iteratively selects oo classes known as nodes from n(x) and inserts oo classes in x until f (x) is maximized. the iterative selection of classes ends when n(x) equals to 0 for each set of classes."
"xalan: it is an xslt processor for transforming xml documents to html, text or other xml document types. the available releases from xalan 2.4 to 2.7 have been considered here [cit], (accessed on march 29, 2015 ."
"the above discussions on the software code metrics show the class level metrics, especially ck metrics suite, is the most popular and widely used metrics in the software defect prediction. among the ck metrics, cbo, rfc, lcom, wmc are seem to be most significant and noc and dit are less significant metrics. the widely used size metrics loc and npm are also significant in defect prediction. so, to incorporate all the experiments, the ck metrics along with loc and npm should be used as selected code metrics for the defect prediction. in this paper, the ck metrics and loc and npm are considered the selected code metrics for the defect prediction."
the proposed bi-phasic stimulator to suppress epileptic seizure with loading impedance adaptability and low-power consideration has been fabricated in the 0.35-3.3-v/24-v cmos process. fig. 8 shows the chip photograph of the fabricated stimulator. the occupied area of the proposed stimulator is about 1000 700 . the chip has been assembled in package for measurement.
"the proposed pbc algorithm groups software into multiple clusters using related and similar oo classes. the functionality of pbc can be classified as oo class identification, cluster formation and cluster validation. these are summarized below."
"1. it hides provider-specific details from jms clients. 2. it abstracts jms administrative information into java objects that are easily organized and administrated from a common management console [cit] . 3. since there will be jndi providers for all popular naming services, this means jms providers can deliver one implementation of administered objects that will run everywhere. [cit] thereby eliminating deployment and configuration issues."
". the ground electrode was implanted 2 mm caudal to lambda. whenever the system detects an epileptic seizure, the proposed stimulator is activated. the stimulus current of pulse train with amplitude, 0.5-ms pulse width, 2.5-ms period, and 0.5-s duration is used to suppress the epileptic seizure of long-evans rat. the injected charge density is . the other microwire with wider diameter can also be used to decrease the charge density, since the proposed stimulator has considered the loading impedance adaptability."
it counts the number of other classes to which a given class is coupled. response for classes (rfc) the rfc metric measures the number of different methods that can be executed when an object of that class receives a message.
"one long-evans rat with spontaneous absence epileptic discharges is demonstrated in fig. 14 . the electroencephalography (eeg) signals of long-evans rat without and with applying the stimulation are shown in fig. 14(a) and (b), respectively. each experiment is conducted for 10 minutes. in fig. 14(a), the epileptic discharges are observed during 3.5-12 s. when the seizure controller is applied in fig. 14(b), the seizure is detected during 3.5-5.5 s. upon the detection of the seizure, the intensive and rapidly brain activities are suppressed by the stimulation at . at this measurement, the loading impedance of stimulator is . according to the experiment results, the functionalities of the proposed stimulator in the closed-loop epileptic seizure monitoring and controlling system have been successfully verified."
"to perform the whole experiment, the r statistical environment has been used to configure the experiment. in this paper, the widely used r simulation tool that is rstudio [cit] ) is used to simulate the whole experiment."
"in this work, the integrated circuit design of implantable stimulator for epileptic seizure suppression with loading impedance adaptability and low power consideration is proposed [cit] . this proposed stimulator can deliver bi-phasic stimulus currents by two leads electrode per stimulus site [cit] in this work for the chip implementation. the measurement results in silicon chip will be presented in section iii. the comparison among the reported on-chip stimulators in recent literature [cit] and this design will be summarized in section iv. the animal test results in the closed-loop epileptic seizure monitoring and controlling system will be presented in section v. fig. 2 shows the proposed stimulator, which consists of a high voltage generator, an output driver, an adaptor, and switches. the proposed stimulator adopts two leads electrode set per stimulus site (electrode 1 and electrode 2) to generate bi-phasic stimulus currents. once the detector of implantable device in fig. 1 detects the epileptic seizure, the stimulator becomes active. under stimulation, the positive stimulus current will flow through the output driver, electrode 1, brain tissue, electrode 2, and adaptor, while the negative stimulus current will flow through the output driver, electrode 2, brain tissue, electrode 1, and adaptor. the high voltage generator is used to supply the variable voltage to output driver, and it is controlled by the feedback signal from adaptor (ctrl.). since the targeted stimulus current is 40 and the loading impedance varies from 100 to 250, the voltage difference between two electrodes is between 4 v and 10 v. in other word, the required supply voltage for output driver must be as high as 10 v, which is much higher than the supply voltage . therefore, the high voltage generator is used to provide the variable high voltage. to reduce the power consumption in stimulator, the high voltage generator can output the different voltage level according to the different loading impedance."
"after the identification of the oo classes, pbc finds the package name of each class. to identify the package information of a java file, it reads the file and retrieves the package name by matching the pattern structure, for example, package packagename;. at the end of the identification of the package name, it groups the files into multiple clusters based on the package. for each distinct package name, it creates an array to store the file's name that resides under the same package. if package name is already considered, it adds an oo class to the existing array of that package, otherwise it creates another for the new package. the whole process is performed using the following psedo code. in a nutshell, pbc finds the package name of each class and lists all package name from the source codes as shown in lines 1 − 2. if package name is already considered as cluster, it adds oo class to the existing package using lines 4 − 5, otherwise it creates another cluster to add oo class as shown in lines 6 − 7."
"the literature in defect prediction focuses on predicting software defects by establishing relationships between software defects and code metrics such as cbo, lcom, loc, wmc, npm, etc. existing software defect prediction techniques use different types of prediction models, for example, statistical inference and machine learning model, and datasets such as promise repository [cit] and nasa dataset [cit] to predict defects. some prediction models predict defects considering the entire system and other use clustering of source code to predict defects. here, the widely used defect prediction models using clustering of source code along with the selection of the most significant code metrics are described."
"to compare the proposed method with borderflow in terms of defect prediction accuracy, the error is computed using the equation 4 [cit] ). the error value shows how much better or worse the proposed dimension reduction technique is in the context of software defect prediction [cit] ."
"the stimulation with different loading impedance is also simulated. the stimulus current always keeps at as the loading impedance varies from 10 to 300 . the simulated as the loading impedance varies from 10 to 300 fig. 6 . simulated voltage/current waveforms of (a), (b), and (c), as loading impedance is 300 ."
"jms clients look up configured jms objects using the jndi api [cit] . jms administrators use provider-specific facilities for creating and configuring these objects. this division of work maximizes the portability of clients by delegating providerspecific work to the administrator. it also leads to more administrable applications because clients do not need to embed administrative values in their code [cit] . jndi is used mainly in naming services to locate administered objects, and administered objects are objects that are created by system administrator [cit] . these administered objects are bound to name in naming service. a naming service associates name with distributed objects, files, and devices so that they can be located on network using names instead of cryptic network address [cit] ."
"design of bi-phasic stimulator for epileptic seizure suppression with loading impedance adaptability is proposed in the 0.35-3.3-v/24-v cmos process. the stimulator consists of the high voltage generator, output driver, adaptor, and switches. with adaptive loading consideration, the adaptor is used to detect the loading impedance, and the high voltage generator is able to adjust the suitable level of operating voltage. while the loading impedance varies from 10 to 300, the proposed stimulator can constantly deliver 40-stimulus current. the power consumption of this work is only 1.1 mw-1.4 mw. the stimulator is successfully integrated into the closed-loop epileptic seizure monitoring and controlling system for animal tests to verify its performances."
here are some of our findings that we found during our analysis. 1) at least 3 subscribers and 3 publishers sending in a saturated mode are required to fully utilize server cpu and to make server maximum message throughput.
"a point-to-point (ptp) product or application is built around the concept of message queues, senders, and receivers. each message is addressed to a specific queue, and receiving clients extract messages from the queue(s) established to hold their messages. queues retain all messages sent to them until the messages are consumed or until the messages expire."
"at the beginning of the clustering process, the proposed algorithm lists all files from software project and then it identifies potential files that will be considered for constructing clusters. in this context, software project is developed by java, so the proposed algorithm performs oo searching by considering the file extension with .java. the overall oo class identification process for pbc algorithm is illustrated in the following pseudo code. in this pseudo code, it traverse all files using line 1 and checks the file extension using line 2. then it only stores files having extension .java using line 3. this process stops when no files available for traversing."
the common example for pub/sub model is stock update in which main server at mumbai stock exchange will publish stock updates to virtual channel called as topic which is being subscribed by more than one subscriber. thus it seems to be one-to-many communication.
"we have developed a \"quickmq\" messaging platform that facilitates asynchronous messaging between heterogeneous systems. we analysed many implementations of the jms specification and gathered the information about the features that are not required by small and middle level organisations. we eliminated those features in the implementation of quickmq but still maintaining the high degree of performance [cit] ."
"digital object identifier 10.1109/tbcas.2012.2200481 with the traditional treatments, the electrical stimulation is more harmless, flexible, recoverable, and less-destructive [cit] . therefore, many epilepsy control systems by using electrical stimulation have been studied. fig. 1 shows the functional block diagram of a closed-loop epileptic seizure monitoring and controlling system in an implantable device [cit] . this implantable device consists of the detector, signal processor, and stimulator to form a closed-loop seizure controller."
"in recent years, the new techniques of electrical stimulation, such as the vagus nerve stimulation and deep brain stimulation [cit], have been demonstrated to suppress the abnormal discharge signal before epileptic seizure happen. comparing . this work was supported by national science council (nsc), taiwan, under contract of nsc 101-2220-e-009-020, and by the \"aim for the top university plan\" of national chiao-tung university and ministry of education, taiwan. this paper was recommended by associate editor m. sawan."
"packages may contain different number of classes. some packages may have lots of classes and other may have only few classes. in this paper, the sdp model was implemented using eight independent variables, (described in section 3.2). so if the number of classes in a package is less than the number of independent variables used in the prediction model, it would fail to predict defects. in this case, to make defect prediction model successful, small packages are combined to form a joint cluster by applying the following lines of code. this section describes the selected prediction model, its validation process and the independent variables used by the prediction model. it also highlights the clustering method that is used to compare results and finally it outlines the datasets on which the experiment were performed."
"the most influential and important code metrics for oo system is ck metrics. it was proposed by chidamber and kemerer, to help the designers and managers by providing the inner design details of an oo system [cit] . these metrics are calculated by inspecting the relationship among different classes from an oo system. these are weighted methods per class (wmc), depth inheritance tree (dit), number of children (noc), coupling between object classes (cbo), response for class (rfc) and lack of cohesion in methods (lcom). these metrics are widely employed in defect prediction to improve the quality of an oo system. the detail description of these metrics are given in section 3.2."
"in this particular experiment series, the jms server could only utilized to 90% due to the limitation of a single subscriber. [cit] msgs/s for the non-persistent mode in contrast to about 1200 msgs/s for the persistent mode. due to the non-persistent mode, the dispatched message rate is lower than the received message rate for a large number of publishers which leads to about 10% message loss in the end."
"to evaluate the quality of the defect prediction model achieved by the linear regression model, the mean (m), median (md) and standard deviation (std) of absolute residuals (ar) are computed. the ar value, widely used in the performance measure of the linear regression model [cit], is the difference between predicted defects and actual defects of a particular oo class. the smaller value of ar shows the better accuracy of the prediction model [cit] ."
"the adaptor of stimulator consists of a current mirror (mn4 and mn5), resistor (r1), and comparator (c1), as shown in fig. 3 . during the stimulation \"on\" interval, the positive (negative) stimulus current passes through tissue and electrode 2 (electrode 1), and then flows into the adaptor. in the, it can be divided from . whenever the stimulator is turned on to stimulate the brain tissue, the comparator in the adaptor compares these two voltage signals ( and ) and distinguishes the amplitude of stimulus current. the variable supply voltage for output driver can be controlled by the output of comparator (ctrl.). if the is lower than 40, which leads to higher than, the output of comparator will become high . the high voltage generator will keep pumping to provide higher voltage until the reaches 40 . once the is higher than 40"
"results show that the software defect prediction using the proposed pbc outperforms the prediction models considering the entire system because pbc uses source code similarities and relationships to group software. for borderflow algorithm, pbc also performs better when software package contains similar and related classes. in this context, the prediction model considering pbc outperforms the entire system in all datasets and it also performs better than the prediction models considering borderflow in 4 datasets which are ant-1.4, ant-1.7, xalan-2.6, xalan-2.7."
"the proposed pbc for software defect prediction has been experimented on 8 releases of 2 open source software built by java. all of those defect datasets have been downloaded from the promise repository [cit] . those datasets contain the corresponding software code metrics and defect information which are usually used by the prediction model to predict defects for future releases. the source code for ant and xalan are downloaded from apache repository [cit], (accessed on march 29, 2015 which are used by pbc and borderflow to find clusters from the source code. the short description of those software are given below."
"quickmq is lightweight mom which mainly supports both messaging models point to point and publish-subscribe. this quickmq mom is mainly used to transfer messages from one application to another across network. it ensures that all messages are properly distributed among applications, although different enterprises uses different formats and network protocols for exchanging messages, but their semantics are same. quickmq uses standards of sun microsystems jms api to create message, load information in format, assign routing information and send message. the same api is used to receive message on other end. thus quickmq increases reusability by reusing same api for different systems. enterprise messaging products are also available such as ibm websphere mq, sonicmq, microsoft message queuing and tibco rendezvous are being in use for many years. recently open source messaging products such as activemq have entered market and are being used in enterprise environment, but all these need to be embed in whole console, that is they come in complete package of functionality while some of functionality might not be used, this in turn increases cost and memory requirement of product, also these products do not provide any feature to have customization of required functionalities."
"as now a day\"s messaging has become an important part of communication between different users and systems therefore large numbers of way of communication as been invented. as messaging supports loose coupling [cit] between two systems it has got lot of importance mainly in heterogeneous integration and communication. over the years system has grown in terms of complexity and sophistication [cit], to have system more reliable, scalable and flexible, it has given rise to system which is having more complex and sophisticated architecture [cit] . in order to increase in demand for better and faster systems, messaging is found to be one way of solving these complex problems."
a jms client can then look up the administered objects in the namespace and then establish a logical connection to the same objects through the jms provider.
"the used 4-stage charge pump circuit is shown in fig. 5(b) . this charge pump circuit has been studied to output the high voltage, but it will not suffer the gate-oxide reliability issue [cit] . there is a compromise between capacitor size and clock frequency, so the used reference clock frequency is 25 mhz, and each capacitor in 4-stage charge pump is 15 pf. the output voltage of the 4-stage charge pump circuit with 3.3-v supply can be ideally pumped up to . at the beginning of operation, there is no charge stored at the output capacitor of high voltage generator, and the output voltage is initially 0 v. under stimulation, the charge pump circuit starts to pump to high voltage. if the ctrl. signal is high, the charge pump circuit keeps pumping to higher voltage. therefore, the output voltage of high voltage generator increases, and stimulus voltage is delivered from the charge pump. the charge pump circuit keeps pumping until the stimulus current is slightly higher than 40 to cause that is lower than in the adaptor, that is to say, the ctrl. signal becomes low. the inactivated charge pump circuit causes that output voltage of high voltage generator and stimulus current decrease. therefore, by changing the state of charge pump circuit instantaneously, the output voltage of high voltage generator keeps at the required high voltage."
"as in this kind of heterogeneous world there are so many programming languages, script languages and platforms each having their own distinct characteristics, for example \"shell script\" mainly good enough in dumping up for backups and other side it is not good as it need to launch a new process for each new shell command executed also has portability issues which can be removed through using platform independent languages like java, dot net, thus every language has its own advantages and disadvantages. if an enterprise wants to develop application which requires efficient back up technique and also it need to be portable i.e. platform independent, then it to satisfy desired needs it requires to import specific characteristics from different programming languages. how this can be achieved? earlier there were solutions like rmi(remote method invocation) but this solution was rather synchronous in nature i.e. system would get blocked and was unable to process other tasks. then came the concept of mom (message oriented middleware) which mainly uses messaging to communicate between two systems that mainly remove synchronous problems."
"the error value shows that how much better or worse the sdp model is compared to other. the error values are calculated from mar and stdar values using equation 4 (see section 4.1 for detail description). usually, the negative error value shows high accuracy and positive value shows low accuracy of the sdp model [cit] figure 3 . in this figure, all points under the horizontal line represent software releases; for those the sdp model has better prediction accuracy. since, the negative value means that the clustering methods using pbc outperform the clustering methods using borderflow. in this figure, 4 datasets which are ant-1.5, ant-1.3, xalan-2.5 and xalan-2.4 have positive error values, that mean pbc fails to perform better in these dataset. for datasets ant-1.7, ant-1.4, xalan-2.6 and xalan-2.7, the error values are negative which mean pbc performs better in these datasets because of their proper structure in the development time. figure 4 outlines the error values considering pbc and the entire system. here, all software releases (such as xalan-2.5, xalan-2.4, ant-1.3, etc.) reside under the horizontal line that means that sdp model has better prediction accuracy using pbc than the entire system. pbc outperforms the entire system in all datasets because, when sdp model uses the entire system to learn, it gets some erroneous dataset which hampers the learning process. on the other hand, when sdp model takes dataset using pbc, it gets better dataset for learning compared to the entire system. as a result, sdp model has better prediction accuracy when it considers pbc. concisely, it can be concluded that the pbc based sdp model performs better than the entire system because the entire system hinders the learning procedure of the sdp model. and it also outperforms borderflow in some software projects having proper coding structure in the development phase."
"ant: it is a library and command line tool for automating software build processes. in this experiment, the ant releases 1.3 to 1.7 have been selected because its dataset is widely available and it is developed by java [cit], (accessed on march 29, 2015 ."
"this section presents the result analysis of the sdp model considering pbc, compared to borderflow algorithm and the entire system. the result of the sdp model using different it is also needed to mention that inconsistencies exist in ar values produced by the sdp model considering pbc compared to borderflow. in some cases, for example ant-1.7, the sdp model considering pbc performs better than borderflow, and for ant-1.5, the borderflow based sdp model performs better than pbc. from the experimental results, it can be concluded that pbc works well when the package information can accurately group the source codes. in contrary, borderflow works well when each cluster get sufficient number of objects so that sdp model gets proper dataset in the training purpose."
"the source code clustering based defect prediction model divides the whole software dataset into multiple clusters for training the prediction model more accurately. the software engineering datasets always contain lots of variabilities such as heterogeneity among the code metrics. these variabilities cause the poor fit of machine learning algorithms or statistical inferences to the dataset. if the variablities among the datasets can be minimized by clustering, it will increase the probability of fitting the data to the machine learning algorithms or statistical inferences. many researches have already been carried out to group the software engineering dataset for predicting software defects. some of those experiments are outlined below."
"in summary, in the first section, the widely used existing clustering techniques are summarized, that have been already used in the software defect prediction to improve the training procedure. in the next section, it discusses the most significant works that support the selection of most influential code metrics in the context of software defect prediction. this thesis combines both existing work to improve the accuracy and performance of the sdp model."
"software clustering can be accomplished by using different clustering algorithms such as borderflow [cit], subtractive clustering [cit], etc. those clustering algorithms use software code metrics (for example, class level, method level, process level, etc.) similarities or source code dependencies such as class reference to form clusters. although those algorithms help defect prediction models improving the accuracy and performance by providing better dataset in the training phase of the prediction model, all of those cannot always provide the best results, because of their inadequate technique (such as class references, code metrics similarities, etc.) of analyzing the source code similarities and relationships."
"due to the broadcast property of the coupler, potori requires a proper mac protocol to efficiently coordinate the data transmissions among the servers in a rack. the existing mac protocols can be classified as distributed and centralized. in distributed mac protocols, every server determines its own resources for data transmission based on the direct exchange of control information with the other servers in the network and without the involvement of the centralized controller. there are some typical examples of distributed mac protocols such as carrier sense multiple access with collision detection [cit], which has been widely employed in the old version of ethernet (10 and 100 mb/s), and the highly efficient distributed access protocol, which was proposed for the passive optical cross-connection network optical interconnect architecture in ref. [cit] . however, the control overhead in the distributed mac protocol should be taken into account. given the large amount of short-lived traffic flows within a rack in data centers, the control overhead brought by the distributed mac protocol might be significant, and the performance of the network (packet delay, packet drop ratio, etc.) will decrease. as a consequence, the distributed mac protocols might not be good candidates for potori."
"in this subsection, we investigate the impact of network configuration on performance and present the average packet delay as a function of n and α. the t tu and t m are set the same as in the previous subsection (i.e., t tu 50 ns and t m 1.2 μs). because the proposed lf obviously outperforms islip, we choose lf as the dba algorithm for potori. in the rack with α 100% (corresponding to 40, 80, and 160 available wavelengths, respectively). under low load (i.e., load 0.2) and heavy load (i.e., load 0.8), the difference in the average packet delay is very small. this is because, under the low load, the wavelength resources are sufficient and most of the traffic demands can be transmitted in one cycle regardless of n, while under high load the system gets saturated, always resulting in high delay. for the medium load (i.e., load 0.4 and 0.6), the packet delay performance degrades with the increase in the number of servers. this is due to the fact that the number of available wavelengths (which is equivalent to the maximum number of assigned traffic demands in each cycle) increases linearly with the number of servers, but the total number of traffic demands increases quadratically."
"optical interconnect architectures that are able to provide ultra-high transmission speed and switching capacity in a cost-and energy-efficient way are considered to be a promising solution to address the limitations of electronic packet switches (epss) in dcs. by replacing epss with optical switches, the reduced power-demanding electricalto-optical and optical-to-electrical (o/e) conversion is expected to dramatically decrease the power consumption of data center networks [cit] . several optical interconnect architectures for dcs have been proposed in the literature in recent years, e.g., [cit] . these architectures employ alloptical switches based on different topologies and technologies at the aggregation/core layer, but rely on conventional epss at tor to interconnect servers in the racks. however, the epss at tor are responsible for a large amount of the overall dc traffic. for example, it is reported in ref. [cit] that in the dcs running extensive data exchange applications (e.g., mapreduce) around 80% of the total traffic is confined to the access tier. moreover, the epss at tor contribute to the majority of the overall dcn power consumption [cit] . therefore, efficient optical interconnect architectures are required for the access tier in dcs."
"in the previous subsections, we considered a tunable transceiver with an ultra-fast tuning time t tu 50 ns and maximum transmission time of t m 1.2 μs for each cycle. note that with t m 1.2 μs, packet-level switching granularity is achieved by potori since, in each cycle, there is at most one packet transmitted by a server. however, with the larger tuning time, it may become challenging to efficiently realize the packet-level switching granularity. thus, we relax the constraint of transceiver tuning time to 2 μs, which has been reported by the commercially available products [cit] . with t tu 2 μs, the performance of potori in terms of average packet delay and packet drop ratio with different dba algorithms is shown in fig. 7 . if we keep t m 1.2 μs, the packet delay of both dba algorithms increases tremendously and the system has a high packet drop ratio due to the increased tuning overhead. however, the performance can be much improved by increasing t m . with t m 10 μs, the packet delay for potori with the lf dba algorithm employed can still be maintained below 100 μs when the load is lower than 0.3, and below 400 μs under loads up to 0.7. when employing islip, on the other hand, potori performs worse because the packet delay is around 600 μs under the medium load (from 0.3 to 0.6). in addition, lf achieves lower packet drop ratio compared to islip. nevertheless, for both the lf and islip settings, t m 10 μs outperforms the case with t m 1.2 μs. the reason for this is that, with larger t m, more packets can be sent by servers within one cycle, which reduces the tuning overhead."
"in this paper, a gramian-based interaction measure is extended to support the descriptor systems. the proposed interaction measure is used for pairing and the controller structure selection."
"t he technological world of today has been witnessing the increased complexity due to the rapid development of the process plants and the manufacturing processes. the computational complexity, the reliability problems and the restrictions in communication make the centralized control of such large-scale complex systems expensive and difficult. to cope with these problems, several decentralized control structures have been introduced and implemented over the last few decades [cit] . the decentralized controllers have several advantages, which make them popular in industry. the decentralized controllers are easy to understand for operators, easy to implement and to re-tune [cit] . the decentralized control systems design is a two-step procedure. the controller structure selection and inputoutput pairing is the first main step and the controller synthesis for each channel is the second step of the decentralized control. the focus of this paper is on pairing and the controller structure selection of the decentralized control systems. this issue is a key problem in the design of h. r. shaker is an assistant professor in the institute of energy technology, aalborg university, denmark (shr@et.aau.dk ). j. stoustrup is a professor in the section for automation and control, department of electronic systems, aalborg university, denmark (jakob@es.aau.dk )."
"t he growing popularity of modern internet applications such as cloud computing, social networking, and video streaming is leading to an enormous increase of data center (dc) traffic, including not only the northsouth (client-server) traffic, but also the east-west (serverto-server) traffic exchanged within the dcs [cit] . according to cisco, the overall data center traffic will keep increasing at a compound annual growth rate of 25% [cit], reaching 10 zb per year [cit] . therefore, it is important to evolve the current data center network (dcn) infrastructure to support the continuously growing traffic demand."
"the rest of the paper is organized as follows. we present the related works on optical interconnect architectures for dc in section ii. in section iii, we illustrate the potori architecture, including both data plane and control plane. the proposed centralized mac for potori is elaborated in section iv, and in section v we introduce and analyze the proposed dba algorithm. the simulation results of potori and a conventional eps are presented and discussed in section vi. we conclude the paper in section vii."
the elements of the pm shows which elementary subsystems are significant and should be considered in for example consider a 3 x 3 process model with pm:
"the information of the channel interaction which is obtained from the gramians of elementary systems is encompassed into the so-called participation matrix ( the participation matrix highlights the elementary subsystems, which are more important in the description of mimo systems, and in this way it shows the suitable pairing and the appropriate controller structure to select."
"the traffic model used in the simulations is derived from refs. [cit] . each server generates 10 6 packets, where the packet inter-arrival time follows a lognormal distribution. the size of packets follows a bimodal distribution (i.e., most of packets have a size of either 64-100 bytes or around 1500 bytes), which is shown in fig. 5(c) . the data rate (r) per server is set to 10 gb/s and we assume that the buffer size on the oni is 10 mb. the propagation delay is set to 50 ns, which corresponds to 10 m fiber for interconnect within the rack. the network oversubscription rate o r is set to 1:4 (i.e., if we consider 64 servers in a rack, there are 16 tunable transceivers for communication with the aggregation/core tiers, and the coupler with 64 16 80 pairs of input and output ports interconnecting all servers and uplink transceivers). servers and uplink ports generate packets with random destinations. we assume that 80% of the traffic generated by servers is intra-rack traffic. the destination of the intra-rack traffic is uniformly distributed to all the other servers in the rack. the remaining 20% of traffic is inter-rack, whose destination is uniformly distributed among the uplink interface transceivers. meanwhile, each uplink interface transceiver generates packets with destination uniformly distributed to all servers, representing the traffic from aggregation/core layer to the rack. we define n as the number of servers in a rack and α as the ratio between the number of available wavelengths for a rack and n1 o r (i.e., the sum of the number of servers and uplink transceivers), reflecting the sufficiency of the wavelengths. in addition, the tuning time of the tunable transceivers is defined as t tu, and the maximum transmission time of a cycle is defined as t m . table i summarizes all the notations."
"it can be seen in fig. 5(a) that, when the load is lower than 0.5, potori with the proposed lf dba algorithm can achieve a packet delay lower than 10 μs, which performs similar to eps. when the load is increased to 0.7, potori with lf is able to introduce up to 50% lower delays compared to eps thanks to the lf's feature of prioritizing the large traffic demand. moreover, it performs slightly better (around 2% difference) than eps in terms of the packet drop ratio. on the other hand, potori with the islip dba algorithms has the worst performance. when the load is lower than 0.5, the average packet delay when employing islip is twice as high as that with lf. in addition, islip shows the highest packet drop ratio, which is greater than 10% when load is larger than 0.75."
"in the control plane, the rack controller can be connected to a higher-layer dc controller in a hierarchical control architecture [see fig. 1(b) ]. in this way, the dc operator can employ a single control infrastructure to manage all the resources in the dc. depending on how the dc controller interacts with the rack controller, two different modes of operation can be defined, namely fixed mode and programmable mode. in the fixed mode, the dc controller is not able to influence the resource allocation inside the rack. the rack controller performs layer 2 functions, such as switch table lookup, and computes the resource allocation according to a deployed dba algorithm. on the other hand, in the programmable mode [see fig. 1(b) ], the dc controller can influence the resource allocation inside the rack, e.g., by changing the employed dba algorithm dynamically. a possible way to realize a control plane operating in programmable mode is to equip the rack controller with a configurable switch table (e.g., an openflow [cit] switch table) and a configurable resource allocation module [see fig. 1(b) ]. using software-defined networking [cit], the dc controller is then able to dynamically change the flow rules and dba algorithm employed by the rack controller. in this paper, we consider only the control plane in fixed mode and leave the programmable mode for a future work."
"in the following lemma, we show that the system gramian for a descriptor system can be expressed in terms of the gramians of the elementary systems. be the controllability and observability gramians for the elementary systems (22) . then:"
"the proposed centralized control entity for potori, namely the rack controller, is shown in fig. 1 . the rack controller exchanges control information with the onis using dedicated control links. the servers report the relevant information (e.g., buffer size) to the rack controller and tune the transceivers according to the feedback from the rack controller. the potori mac protocol defines the exchanging procedure and format of the control messages between the servers and rack controller, which will be elaborated on in section iv. on the other hand, the rack controller collects the necessary traffic information from each server and creates the traffic matrix. then it runs a dba algorithm, determining the wavelength and time slots assigned for all the servers in the rack. finally, it generates the control messages that include the outcome of the dba and sends them to each server."
"where l nct is the length of the timestamp, l ts is the length of the starting/ending timestamp for data transmission, and l wi is the length of the wavelength identifier. the remaining symbols are the same as the ones for the request message. if we consider a timestamp of 8 bytes and wavelength identifier of 1 byte, the length of a grant message would be 52 bytes, which is small enough to be encapsulated into one ethernet frame."
"the potori architecture can be interconnected with any solution for the aggregation/core tiers to build large dcns. in the data plane, proper interfaces are needed to interconnect the potori with the aggregation/core switches. these interfaces can employ o/e conversion for connection to the conventional eps in the aggregation/core tier or they can be optical (e.g., to directly connect the potori to an optical core switch and realize an all-optical dcn [cit] ). in the latter case, a strategy for the joint allocation of the optical resources in the access and aggregation/ core tiers needs to be developed."
the request message can be encapsulated in an ethernet frame. the length in bytes of the request message (l r ) can be calculated as
"in this section, we evaluate the performance of potori and compare it to conventional electronic tor packet switches in terms of average packet delay and packet drop ratio. to be more specific, the packet delay consists of queuing time at the source node (servers and uplink interfaces), transmission time, and propagation time. in potori, the oni at servers drops packets when the buffer is full. moreover, we examine the impact of different system configurations (e.g., selected dba algorithms, tuning time of the transceivers, etc.) on the performance of potori and build a customized discrete-event-driven simulator implemented in java for the performance evaluation."
"in this section, an interaction measure for the mimo processes with descriptor form is built upon the notion of the gramians. the trace of the cross gramian is used as a convenient basis to present the channel interaction and to select the most appropriate controller structure. for a mimo dynamical system with descriptor form (6), we have: if we add all these equations:"
"we further illustrate the structure of the request and grant messages in the following sections. figure 2 shows an example of the request message. the first field of the request message contains the current time cycle identifier (e.g., cycle c 1 in fig. 2 ), which is used by the rack controller to identify whether the received control messages are outdated or not. if a request message is not synchronized with the cycle identifier at the rack controller, it is discarded by the rack controller. the second field of the request message contains the mac address of the source server, i.e., the server that generates the request message (e.g., server 1 in fig. 2 ). the request message should also contain n fields in total for all the possible destination mac addresses (n − 1 for the other servers in the rack and one for the interface toward the aggregation/core tier), along with the corresponding number of buffered bytes, i.e., the bytes to be transmitted at the source server."
"several optical interconnect architectures for dcns have been proposed in the literature. the c-through [cit] and hybrid optical switching [cit] are two examples of hybrid electronic/optical interconnection solutions. in these hybrid interconnect architectures, optical circuit switches (ocss) are employed to provide high capacity for transmitting long-lived and bandwidth-consuming traffic flows (e.g., elephant flows) due to the long reconfiguration time of ocss, and eps is used to transmit short-lived traffic flows that do not need large bandwidth (e.g., mice flows). these solutions require pre-knowledge or classification of traffic patterns in order to distinguish large and small flows and properly configure the ocs, which is challenging for dc operators."
"a traffic demand matrix can be built by the rack controller after receiving the request messages from all servers and uplink interfaces. an example is shown in fig. 3 . the rows and columns of the matrix indicate the input port (source) and output port (destination) respectively, and the matrix element represents the amount of traffic (in bytes) that needs to be transmitted from the source to the destination. the dba algorithm should find a solution for assigning available wavelengths to the different traffic demands without any collision in the data transmission. a collision occurs when different traffic demands are assigned to the same wavelength at the same time. to avoid collisions, one traffic demand at most can be assigned to an available wavelength, i.e., each row and each column in the matrix should be associated with exactly one wavelength. the right side of fig. 3 gives a feasible solution to the wavelength assignment without any collision. the wavelengths assigned for serving the different traffic demands are distinguished by colors. the rack controller forms the grant messages according to the dba solution, indicating the wavelengths for transmitting and receiving at every server."
"the controllability and the observability gramians are well-known matrices, which are widely used to check the with: controllability and the observability of the linear dynamical systems. the controllability and observability gramians show how difficult a system is to control and to observe. the gramians are also widely used in the process of model order reduction [cit] . for dynamical systems with the minimal realization:"
"a set of elementary siso systems can be associated to this mimo system, such that each siso system has a single input u j (t) and single output y equivalently: (s),g22(s), ...,gpp(s) )."
"the results on the gramian-based interaction measures, which have been proposed so far, only support systems in the ordinary state-space form. however, in industrial practices it is often the case that the system, which is needed to be controlled, is either in the descriptor form or can be represented in the descriptor form. singular systems and the differential algebraic equation (dae) systems are among these systems. descriptor systems appear in the variety of fields to describe the practical processes ranging from power systems, hydraulic systems to heat transfer, and chemical processes [ 18], [ 19] ."
"the islip algorithm is easy to implement in the hardware. it achieves 100% throughput for uniformly distributed bernoulli arrivals but may not be efficient for bursty arrival traffic patterns [cit], which are often more suitable for modeling the real traffic pattern in dcs [cit] ."
"the paper is organized as follows. in the next section, we review the concept of the gramians for the ordinary systems as well as descriptor systems. the interpretation of the controllability and observability gramians is also discussed in this section. section iii presents how gramians can be used to quantify the channel interactions for descriptor systems. the application of the proposed interaction measure in pairing and the controller structure selection is explained in this section. in section iv, the proposed interaction measure is used for pairing and the controller structure selection for cstr model in the descriptor from. section v concludes the paper. the notation used in this paper is as follows:"
"the decentralized and distributed control systems, which directly affects the stability and the performance of the control systems. the interaction measures play an important role in the suitable pairing and the controller structure selection for the decentralized and the distributed control. interaction measures make it possible to study input-output interactions and to partition a process into subsystems in order to reduce the coupling, to facilitate the control and to achieve a satisfactory performance."
"for pairing and controller structure selection, the nominal model go (s) needs to be obtained. the nominal model is a model, which is obtained by keeping some of the elementary subsystems of the actual mimo process and assuming the rest as zero. for example, assume that one of the ordinary methods for pairing is used and a decartelized control is synthesized. if the inputs and outputs are re-labeled, one only needs to design p independent siso controller loops, for elementary diagonal subsystems. in this case:"
"the simulation results have shown that, under realistic data center traffic scenarios, potori with the proposed dba algorithm obtains average packet delay on the order of microseconds, which is superior to the performance of conventional eps. moreover, we quantify the impact of network configurations (including interconnect size and number of available wavelengths) and transceiver tuning time on the packet delay. for potori, the packet-level switching granularity is feasible if the tuning time can be kept small enough (less than 30% of the packet transmission time). in the case of long tuning time (i.e., on the magnitude of microseconds), setting the maximum transmission time of each cycle greater than three times the transceiver's tuning time still allows for an acceptable packet delay performance."
"in order to successfully transmit data, both onis at the source and destination server need to be tuned to the same wavelength. to avoid data collision in the coupler, concurrent communications inside the rack can be carried out on the different wavelengths. this calls for a proper control plane design to efficiently schedule resources in both spectrum and time domains for managing the intra-rack and inter-rack communications."
"with a longer transceiver tuning time, t m should be increased to reduce the tuning overhead. in fig. 9, we present the packet delay as a function of τ, which is defined as the ratio of t m over t tu given t tu 2 μs and 5 μs, respectively. with a small τ (i.e., τ 1 and 2), the packet delay is as high as 10 4 μs even under the medium load (i.e., 0.4 and 0.6) while better packet delay performance can obviously be achieved with a larger τ (i.e., τ 3, 4, or 5) under the same load. in addition, with τ 5, the performance will decrease a little at load 0.6. this is due to the fact that, even at larger τ, more traffic can be sent in one cycle but not all servers can fully utilize the cycle and may transmit much less traffic than is allowed in one cycle. this is caused by the bursty feature of the traffic generated by the servers, which may lead to quite different traffic demands per server in each cycle. the larger the t m, the larger the difference in traffic requested by the servers in one cycle. in order to use the resources more efficiently and achieve acceptable delay performance, we conclude that given a tunable transceiver with long tuning time (i.e., on the scale of microseconds), the maximum transmission time of one cycle should be at least three times longer than the transceiver's tuning time."
"it can be seen that the potori's data plane is passive, except for the wss, which is needed to dynamically filter out spectrum for the inter-rack communications. actually, the wss can be replaced by a passive wavelength filter, in which a fixed configuration of the spectrum for intra-and inter-rack communications may result in low flexibility of resource usage. due to the key component coupler, the data transmission in potori follows the broadcast-and-select scheme. the traffic transmitted by one server is broadcast and received by all the other servers and the wss. the destination server (or the wss) then selects the traffic destined to it, while discarding the other traffic. in this way, the servers are able to send/receive traffic to/from each other in the rack (e.g., server 2 sends traffic to server 1 in fig. 1 ). the wss receives and drops the intra-rack traffic while forwarding the inter-rack traffic to/from the upper tier (e.g., server n sends traffic to the aggregation/core tier in fig. 1 )."
"in refs. [cit], we proposed several passive optical tor interconnect architectures for the access tier in dcs. these architectures use passive optical components (i.e., arrayed waveguide grating and/or optical couplers) to interconnect the servers in the rack. it has been demonstrated that the passive optical components offer cost and power saving as well as high reliability. while in refs. [cit] we focused on the data plane architecture, in ref. [cit] we proposed a mac protocol and novel dba algorithms to achieve efficient bandwidth utilization in potori. the current paper extends our previous work by covering both data and control planes, and provides a complete design of potori architecture along with a detailed analysis of network performance and an extensive comparison with the conventional eps solutions. figure 1 illustrates the potori architecture, including both data and control planes. each server is equipped with an optical network interface (oni), which consists of two optical transceivers. the first one is a tunable transceiver connected to the potori data plane. the second one is a gray, small form factor pluggable transceiver connected to the potori control plane. in the following subsections, we elaborate on the potori data and control planes, mapping potori to the use case of dcns."
"the physical layer experiments [cit] have shown that more than 500 ports can be supported in the poi at a capacity up to 5 tb/s. we have also shown that the poi provides lower cost, lower energy consumption, and higher scalability with respect to the conventional epss. specifically, it has been demonstrated in ref. [cit] that the energy consumption per bit in the dcns can be reduced by at least a factor of 7 by using pois at tor compared to the ones using eps. we also proposed a mac protocol and dynamic bandwidth allocation (dba) algorithm, namely largest first (lf), for achieving efficient bandwidth utilization when applying the passive optical interconnect at tor [cit] . this paper extends the work in ref. [cit] with a focus on passive optical tor interconnect (potori) and introduces the following new contributions: (i) we illustrate how potori can be interconnected with other network architectures in the aggregation/ core tier to build large dcns, where both the data and control planes are considered; (ii) we perform an extensive performance comparison among different dba algorithms for potori; (iii) we study the impact of different network configuration parameters (e.g., tuning time of the optical transceivers, duration of the cycle time in the mac protocol, etc.) on the performance of potori; and (iv) we compare the performance of potori with a conventional eps in terms of the average packet delays and packet loss probability. the results show that, using our proposed lf dba algorithms along with ultra-fast tunable transceivers, potori can outperform the conventional eps."
"data center operators are addressing this problem by upgrading the transmission data rate and switching capacity of their network equipment. for example, facebook has already deployed 10g ethernet network interface cards for all servers and top-of-rack (tor) switches [cit] . optical fiber can be deployed in dcns to interconnect servers and switches in order to simplify cabling and avoid electromagnetic interference [cit] . higher data rate and switching capacity (e.g., 40g, 100g) are also taken into consideration by the network operators in the future dc design [cit] . however, it is hard to develop large electronic packet switches operating at high data rates due to the bottleneck of input/output bandwidth and power budget of the chip [cit] . as a consequence, a large number of electronic switches need to be deployed to scale out the number of servers in the dc, which brings a serious scalability problem to the dcn in terms of cost and power consumption [cit] ."
"in this paper we focus on potori, an efficient optical tor interconnect architecture for dcs. the potori's data plane is based on a passive optical coupler that interconnects the tunable transceivers of the servers within a rack. in the control plane, potori relies on a centralized rack controller, which is responsible for managing the intra-and inter-rack communications. a cycle-based centralized mac protocol and dba algorithm (largest first) are proposed and tailored for potori, with an aim to achieve collision-free transmission with good network performance. potori can be applied to optical dcns with any aggregation/core tier architecture, given the proper interface design."
"note that each server only transmits the granted data according to the grant message, which might be only a portion of the ones reported in the request message. it is worth mentioning that the granted traffic, which will be transmitted in the next cycle, is not reported in the next report message. for example, in fig. 2, server 3 receives the grant message of cycle c, allowing it to send 800 bytes to server 1. at t c, server 3 subtracts the 800-byte traffic and reports the remaining data with the destination of server 1 in the request message of cycle c 1."
"the lf is a greedy heuristic algorithm. it prioritizes the largest element in the traffic demand matrix, i.e., a larger amount of traffic demand has a higher probability to be assigned a wavelength. first, the matrix elements are sorted in descending order into a one-dimensional array (line 5 in fig. 4) . then, starting from the first element in the array, a traffic demand is assigned a wavelength if and only if neither the transmitter nor the receiver associated with this demand have already been assigned another wavelength in the current cycle (lines 7-9 in fig. 4) . the corresponding information, such as wavelength, source, destination, and transmission time, is used to generate the grant message (lines 10-13 in fig. 4 ). if one of the transmitters or receivers of this demand is assigned another wavelength, the demand is not served and is left for the next cycle (line 15 in fig. 4 ). the lf algorithm stops when all the available wavelengths are assigned, or the last traffic demand in the array is served (line 16 in fig. 4 )."
"figure 6(b) shows the average packet delay with different values of α. we consider 128 servers per rack (i.e., n 128) and test α 100% (160 wavelengths), 75% (120 wavelengths), and 50% (80 wavelengths). the average packet delays are almost the same for all the considered values of α under the low load (i.e., load 0.2), indicating that in this condition, using less wavelengths in potori under the low load still maintains good packet delay performance (less than 10 μs). however, the packet delay with α 50% increases dramatically under the higher load while, with α 75%, the performance decreases significantly at load 0.6."
"where l ch is the length of the first field in the time cycle, l srcmac and l dstmaci are the lengths of the mac address of the source and i th destination (6 bytes) server, and l si is the length of buffed packet size for the ith destination. if we assume l ch 8 bytes and l si 4 bytes, a request message as an ethernet frame with the maximum size (1518 bytes) can support up to 150 servers in a rack, which is sufficient for the access tier in dcs."
"ksm, a knowledge modelling tool is used to model the knowledge. in ksm the important thing is knowledge area, which is further divided into supporting sub knowledge areas (also called supporting knowledge bodies) and their functionality. there are two languages used by the ksm to formulate the common terminologies and strategies of reasoning. common terminologies are explained in concel language and the strategies are defined in link language."
"by referencing [cit], we can say that knowledge modeling and management can be helpful in software engineering by understanding the business needs correctly, better decision making process, accessing domain knowledge and sharing knowledge about local policies and practices."
the sound recognition problem has been an active area of research for decades. several feature extraction techniques and recognition models have been proposed in an attempt to tackle the problem. the feature extraction involves finding the most prominent features that can enhance the accuracy of the model used. the process of hand-crafting the features is a timeconsuming stage that requires experimentation with a wide variety and combination of features to find the most effective ones for the recognition task.
data collection procedure was performed as it is defined in the section design of experiment. questionnaire is filled by all the participants after the completeness of software development. no participant was left to fill the questionnaire and to give feedback about the proposed knowledge management practices.
"as we have two groups (controlled and experiment), so in each group we have 3 students, each group has 1 supervisor or expert and 1 reviewer. participants are responsible to develop a program, documentation as well as testing. because the basis of all steps, we can analyze the proposed model (its complexity, benefits and obstacles)."
the input of this phase is captured knowledge from learning and exploring stage. we store all the required knowledge in our database. most researchers use rdbms [cit] in this stage to store the knowledge. in our case we have store knowledge related to police station automation system in database.
"esc-10 is a dataset of 400 files of 5 seconds each for 10 categories of environmental sounds released in 5-folds: dog bark, rain, sea waves, baby cry, clock tick, person sneeze, helicopter, chainsaw, rooster and fire cracking."
"to ensure the validity of results, we conduct the experiment in a controlled environment. no participant was allowed to leave the room/lab before completion of questionnaire. also, as mentioned earlier, both the developments were made one by one not at the same time or at the front of each other. group 1 was unknown about the group 2 activities and vice versa. also, groups were designed in such a way, so they were not aware about the experiment. basically it was a surprised experiment. the results of questionnaire of both groups are mentioned in chart 3."
esc-50 [cit] files of 5 seconds each for 50 categories of environmental sounds released in 5-folds. a subset of the classes in this dataset was used for the esc-10 dataset.
"a filterbank is a group of filters used to subdivide the frequency bins of a spectrogram into bands. this transformation tackles the frequency shifts that occur in raw spectrograms due to the smearing of the energy of a frequency bin across nearby frequency bins. a filterbank allows a frequency shift-invariant representation, which provides a better comprehensible presentation of the energy across the bands as it progresses through time. the spacing between the center frequencies of the filters in a filterbank follows a specified scaling. for example, the mel-spaced filterbank follows the mel-scale, which is used in mfcc and mel-scaled spectrograms."
"the band-like pattern enforced over the network's weights is applied using a binary mask. the mask overlap can be assigned positive or negative values, where the negative values specify the non-overlapping distance between the successive columns as shown in fig. 5 .c. this pattern clarifies another important role for the mask, which is automating the feature combination selection process by providing several shifted versions of the filterbank-like pattern. this allows different hidden nodes to learn about different feature combinations. for example, in fig. 5 .c. the first column maps to the first hidden node. accordingly, the 1st hidden node will learn about the first three features permitted by the 3 consecutive ones in the first column. similarly, the 4th hidden node that maps to the fourth column will learn about the first two features of the input vector and the same applies to the 7th column allowing the node to learn about one feature only. these patterns embed a mix-and-match behavior inside the network itself, which automate the consideration of different feature combinations concurrently while preserving the spatial locality of the learned features. the masking operation is applied through an element-wise multiplication between the mask and the matrix at index u belonging to the weight tensor as in (6) where � is the original weight matrix, � is the masking pattern and ̂ is the new masked weight matrix to substitute the weight matrix in (3)."
"in an attempt to exploit the performance of a hybrid model for the sound recognition task. [cit] introduced the convolutional recurrent neural network (crnn), where they used a cnn to extract the features and an rnn to capture the long-term dependences across the sound signal frames for music tagging."
"the input of this phase is the knowledge which employees have learned as well as knowledge that has been explored related to software organizations as well as in our case about the software system, which has to be developed. in this phase we focus on \"what\". what knowledge is required etc.? in our particular scenario of police station automation system, we have identified the methods, roles and privileges etc. in this stage."
"we conduct two developments of same project from different group. one is control group and other is experiment group. after complete the requirement gathering and completion of software development we can analyze the results in a statistical way: (see table 3 ). now we will analyze the results according to the experience of the participants. our hypothesis of this step is: the results will shows that the experience of participants is a main factor in software development, but by using proper methodology we can overcome the minor differences in experiment in development ( table 4) ."
"in proposed model, the activities which are being defined in this learning stage are helpful to learn about the problems. employees can learn knowledge of domain, system, control and explanatory from past papers, historical related software problems, previous coding and development methodologies, books and case studies etc. in next section of experiment design, we are using the case study of police station automation system, which were developed by university students. the experiment group, go through the whole learning stage by getting knowledge of manual system from their own resources like internet, columns, manuals, relatives etc."
"as mentioned above, after the development the software system, participants were asked to fill the questionnaire to give the feedback about the proposed knowledge management based methodology of requirements gathering. the results show that the experiment group gives better results. experiment group is go through with all the steps proposed in a proposed solution section. later on, when the control group compares their results with experiment group, they also give their positive remarks about that knowledge management based solution."
"the software system which is being developed during this experiment is named as \"police station automation system\". [cit] and programming language c# with ms sql server as database storage."
yornoise is a dataset focusing on rail and road traffic with 1527 sound files of 4 seconds each. the dataset is released into 10-folds following the same settings of the urbansound8k dataset.
"finally in requirements management phase, issues and conflicts of users are resolved. according to andriole [cit], the requirement management is a political game. it is basically applied in such cases where we have to control the expectations of stakeholders from software, and put the requirements rather than in well-meaning by customers but meaning-full by developers, so they can examine that, and actually full fill the user's requirements."
"in this last phase of knowledge model, the output in our case was a uml diagram of whole project. the group of students who develop this project previously, without any restriction of using re or software development model was facing difficulties in this particular area, because they did not get any domain knowledge properly, and they were not clear about what to do."
"to check the correctness and verification, we have designed a questionnaire for all participants of both groups as well as their supervisors. the questionnaire contains following questions (see table 2 )."
"whereas, the requirements specification (where analysis and negotiation of requirements are performed), and requirements of users are specified to make them understandable and meaningful for developers. specifications can be formal as well as non-formal [cit] . formal techniques include the set of tools and techniques based on mathematical models whereas informal techniques are based on modeling the requirements in diagrams or making architecture of system. there are many and many techniques in both types of specification. like in formal techniques of specifications, we have different formal specification languages like z, vdm etc. and in in-formal or non-formal techniques, we have uml diagrams which include use-cases, sequence diagrams, collaboration and interaction diagrams etc."
"to be able to above mention qualities, a requirement engineer may use the knowledge modelling. in this study, authors relate the requirement engineering with cognitive theories. as re is a team work, so a process should be well defined, use of prototypes, use of common vocabulary for communication, supportive environment and address the issues in form of notations and symbols to resolve them separately [cit] ."
"in knowledge sharing phase, tasks in form of knowledge are divided into the project team. in our implementation phase we suggest one student group member to make knowledge model of the police station system, he/she will store the knowledge in their particular database in form of textual or descriptive data as well as in form of uml diagrams. [cit] using c# as programming language. and the third member will assist both members as well as make the project documentation. a sharing of knowledge can be presented in form of simple use case diagram to identify the user's interaction with the system (see figure 3) ."
"in requirements validation the completeness of the requirements is checked which means either gathered requirements are correct, complete or not. the main objectives are to analyze the validation and verification of re process, to identify and resolve the problems and highly risk factors of software in early stages, and to make strengthen the development cycle [cit] ."
"the study is implemented in an academic environment and has a small industry contribution. however we can further extend our work by implementing the proposed solution to an industry level. as there are small number of participants which is also a limitation of the study. because if we have large number of participants, we can analyze the results in more better way."
"the conditional neural network (clnn) and its extension the masked conditional neural network (mclnn) are designed for multi-dimensional temporal signals. the clnn considers the inter-frame relation across a temporal signal, and the mclnn extends the clnn by enforcing a systematic sparseness through a binary mask following a band-like pattern. the mask allows the network to learn in bands rather than bins, mimicking the behavior of the filterbank used in spectrogram transformations such as mel-scaled analysis. additionally, the mask is designed to include several shifted versions of the filterbank-like pattern, which automates the hand-crafting process of the feature combinations. this allows each node in the hidden layer to learn distinct localized features in its scope of observation. we benchmarked the mclnn using the urbansound8k, yornoise, esc-10 and esc-50 environmental sounds datasets. mclnn have achieved competitive results compared to models based on state-of-the-art convolutional neural networks (cnn) in addition to hand-crafted attempts. we applied the mclnn on a time-frequency representation, but mclnn still preserves the generalization of applying it to other multi-dimensional representations of temporal signals, which we will explore in our future work."
the experience of participants have small variation because some students usually starts internship during the studies. that's why some participants have affiliation with industry and others.
"a knowledge model can be based on the principle of task oriented, problem-solving and domain oriented. in task oriented the input and output of system is analyzed. problem is then transformed in tree based architecture. on the other hand, in problem solving technique, a major task or goal is divided into sub tasks. knowledge is transformed in hierarchical architecture. and in domain oriented, an ontology is being used to explicit the do-main elements."
"software requirement engineering involves requirements elicitation, requirements specification, requirements validation and requirements management [cit] . requirements elicitation involves the ways of gathering the requirements which include many traditional, cognitive, model based etc. techniques."
"in the future, we can analyze this model by changing different experiment objects which are basically other software systems of real enterprises and more complex. by changing objects, our variables will also change, like in this case we apply it on graduate students, further we can apply it on software engineers having rich experience. another future work of this study is use of ontologies, experiencing different data sets and including some other emerging things."
to execute the experiment we have assigned the tasks to two groups of students. one group is performing the role of control group and the other is experiment.
"in above mentioned proposed knowledge model, we have presented the knowledge exploring phase in shape of database storage. basically, in knowledge exploring phase, we explore the existing knowledge or related knowledge using case studies to achieve competitive advantage. in our scenario, we have been search about different approaches of implementation of police station system, and then we come to know about an easy approach using knowledge model. many automated systems or projects can be found on internet with named as e-justice, automated police station system etc. but our focus is to learn the implementation techniques to solve the situation according to our scenario and requirement."
"as said earlier, there are two groups of students of university located in rawalpindi, pakistan. to analyze the results, there are 3 members and 1 supervisor (manager/professor). in figure chart 1 and chart 2 we have mentioned the experience and knowledge of participants. further, we allow control group to develop a software by using their existing knowledge and experience and collect the requirements from either station or by else. but the experiment group is restricted to follow the complete proposed knowledge management model to elicit the requirements. knowledge management helps the participants in software reuse, software maintenance, evolution and quality control of software system."
"knowledge learning stage knowledge exploring knowledge capturing knowledge storing knowledge sharing knowledge exploitation after these steps, we will merge our proposed model into a complete knowledge management model. here, we did not involve the users or its basic requirements about the system, here we just focus on the techniques which we will use after getting the basic requirements from users. the input of this model can be a uml dia-gram as well as the requirements in descriptive form. the knowledge model is illustrated in figure 2."
in g1 the rules and frames are defined. in g2 a symbolic representation is used to abstract the design phase. and then convert these abstractions into a model based system development.
software requirement specification (srs) is a standard and official document which describes what the developers will develop. it includes detailed requirements specifications [cit] . a standard ieee format of srs [cit] describes the recommendations for a good srs document.
"knowledge modeling is not a new area, a lot of work has been done using knowledge modeling techniques in last some decades. in software engineering, knowledge management techniques are used to provide an easy way to learn the software organizations. in software engineering there using of software development process or life cycle is used and it is called an \"experience factory\" in terms of software engineering. in knowledge management, we collect these experiences from different resources like past papers, case studies etc. and store them into a single repository to easy reuse of software life cycle or development process [cit] ."
"urbansound8k is composed of 8732 files for 10 classes of environmental sounds released into 10-folds: air conditioner, car horns, children playing, dog bark, drilling, idling engines, gunshot, jackhammers, siren and street music. the maximum duration for the files is 4 seconds."
"many definitions of knowledge modeling and knowledge management can be found in literature by authors having different background. the one of them is: \"a method that simplifies the process of sharing, distributing, creating, capturing and understanding of company's knowledge\" [cit] . the summary of knowledge management in software engineering refers to this definition which says that the main purpose of knowledge modeling is to learn the software organizations. the more elaborated definition is: \"a software organization that promotes improved actions through better knowledge and understanding\" [cit] ."
"the goal of this experiment is to empirically investigate the proposed knowledge model. by implementing it on a particular case study, which we can analyze its maximum features and attributes in the first stage. our experiment based on small project named as police station automation system, of graduate students."
"r-trees are inherently balanced trees, which provides a great deal of efficiency. every time a node is split, its children are distributed amongst the new nodes in order to maintain the same depth throughout the r-tree and avoid empty nodes. this property allows for insertions, deletions, and searches to be made in worst-case o(md log m n) time for n data elements of d dimensions. our proposed method to execute queries requires even less time than searches, as we will explain in detail in section 6.2. this method for generation of a hierarchy improves upon linsen's [cit], which does not maintain tree balance and generates many empty nodes. furthermore, while linsen's [cit] method applies a more accurate automatic generation of clusters, it necessitates specification of a density function and introduction of another preprocessing step to evaluate densities and quantities of clusters."
"as measuring instruments advance technologically, datasets increase in both dimension and quantity. it becomes very difficult to interactively explore and analyze large, multidimensional datasets because of the high computational complexity required. visualizing these dimensions also becomes a major problem for large dimensionalities and large datasets since standard visualization interfaces are constrained to a small number of dimensions and resolutions."
"to examine an outlier, we execute an overlap query in red followed by a number of overlap refinement operations until we obtain the lod required. after just a few overlap refines, we achieve a very specific outlying region visualized in both the parallel coordinates and star coordinates, while avoiding clutter due to the region-specific refinement operations."
"we propose using r-trees to generate this hierarchy and examine the benefits for doing so in section 4. r-trees provide functionally visualizable aggregate items within an lod-hierarchy while also increasing efficiency, which improves upon the problems encountered in some previous proposals (see subsection 2.2)."
"alternative coordinate systems attempt to provide a visualization for any number of dimensions. we have implemented and built on two of these techniques, specifically parallel coordinates [cit] and star coordinates [cit] ."
"for single elements within star coordinates, the technique is, again, explicitly defined [cit], and we propose extending this idea to also represent hyperboxes, as fua [cit] did with parallel coordinates. the dimensional axes are represented by a set of lines which all emanate from a single point (the star coordinate origin). the data elements in star coordinates can either be represented as a polygonal line which connects dimensional values, or as a single point which is translated in the direction of each dimensional line by the magnitude of the value. we use the latter. in order to represent the hyperboxes, we cannot simply plot the minima and maxima of the range as with parallel coordinates, because the area between the minimum point and maximum point no longer accurately represents the range. instead we introduce a method that plots all possible combinations of the minimum and maximum values in each dimension-the corners of the hyperboxand fills the area between those points. we fill the area by calculating the convex hull of these points and constructing its respective convex polygon."
"because overlap queries allow searching for any data within the specified range, they are useful for drawing conclusions about the data regardless of the internal r-tree structure, and therefore is based on the data elements rather than the data structure. fig. 2 . we highlight visualized hyperboxes in parallel coordinates (left) and star coordinates (right). by increasing the lod of a large hyperbox containing sparsely distributed data, we obtained detail of a small hyperbox (at the end of the axis numbered 10) within that region containing densely distributed data (the small red hyperbox)."
"one principal drawback of our hyperbox visualization method is that it requires o(2 d ) complexity to calculate the hyperbox corners. the effects are rather detrimental if visualization of 20 or more dimensions is required, so improved methods fig. 3 . we show hyperboxes as colored regions in parallel coordinates (left) and star coordinates (right) for a dataset of forest fires. each numbered line in the star coordinate visualization corresponds to a dimension. these numbered lines can be manipulated in direction and length. if we extrude the line numbered 11, the red hyperbox is extruded more than any other hyperboxes. therefore, the red hyperbox contains data with a high range of values in dimension 11."
"the user may construct and execute two types of queries on the r-tree: 1) bounded and 2) overlap. both query methods iterate over nodes of the r-tree and execute comparisons between the constructed query and each node processed. both also require the same input: a set of 3-tuples, which each specify 1) a dimensional index, from 1 to d inclusively, 2) a value within that dimension, and 3) a margin value."
"we examine methods to visualize aggregate items as well as data items within the r-tree in section 5. some of these methods are already well-known, and we introduce a new method to visualize multidimensional r-tree aggregate items based on some existing proposals. we examine two types of interactive operations, queries and refinement, in section 6. finally, we apply our proposed methods on real datasets in section 7."
"as 178 elements is a fairly small number of data, we choose a small value for m, 2, in order to increase lods available. next, we execute bfs refinements to draw initial conclusions about where outliers may lie. from this step, we can see distributions of values in each dimension. some are densely packed around certain values, like dimension 10 and dimension 5. the outliers in each dimension are those values which lie outside of the densely packed regions. in order to show the efficacy of removal as well as examination, we remove the outliers in dimension 5 and examine in detail the outliers of dimension 10."
"would be necessary to provide real-time visualizations at this level of dimensionality. note that this complexity applies to the visualization, rather than the interactive operations."
"we have implemented and built upon several existing methods for multidimensional visualization and visualizable hierarchical structuring of multidimensional datasets. we have introduced a novel method to generate an efficient lodhierarchy for large, multidimensional datasets using r-trees, we have examined methods to visualize hyperboxes and elements within that lod-hierarchy, and we have examined the use of interactive operations on the data to facilitate analysis. we have used existing visualization schemes, parallel and star coordinates, in order to introduce a new method for visualizing hyperboxes, while retaining the ability to use existing visualization methods as well. our method for lod-hierarchy generation provides a great deal of efficiency and functionality in contrast to previous ones, and in combination with the introduced visualization schemes and interactive operations, added benefits for analysis and exploration of data."
"multidimensional visualization explicitly involves the problem of how to project d dimensions onto the a small number of dimensions available on visualization interfaces. popular methods to do so include using visual cues, multiple visualizations, and alternative coordinate systems."
"parallel coordinate visualization was defined by inselberg [cit], and has been extended to represent multidimensional value ranges, hyperboxes, by fua [cit] . in parallel coordinates, each dimension is denoted by a single line such that all lines are unique and parallel to each other, and points are represented as polygonal lines with values plotted on each respective dimensional line. to represent hyperboxes, we simply plot two data elements in this fashion, the maximum and minimum, and fill the area between both segments, so that we attain a polygon which covers all values within the range of the hyperbox."
"a possible improvement to the drawback of complexity mentioned in section 8 could be to apply linsen's [cit] splat-based ray-tracing method to these hyperboxes, in which case the complexity would be constrained by screen resolution, rather than the data dimensionality. another possible improvement, for more accurate hierarchical cluster generation, could be to develop new node-splitting algorithms based on factors other than proximity."
"chernoff [cit] introduced a method using visual cues which involved transforming individual features of a face geometrically, and visualizing each multidimensional element as the resulting face. however, he stated that this technique is constrained to a small number of dimensions. this is an inherent problem; visual cues must be explicitly defined for each dimension."
"the fact that r-trees are an lod-hierarchy allows for several methods to remove clutter, both programmatically and interactively. clutter is defined as the ratio of lod to available screen resolution; thus, lod corresponds directly to the amount of clutter in the visualization. in an lod-hierarchy, it is possible to refine down the hierarchy and therefore alter the lod of the visualization dynamically. dynamic alteration of lods, in turn, allows for dynamic removal of clutter."
"uniform programmatic we introduce one uniform programmatic method for refinement: a simple breadth-first search (bfs). this method refines all hyperboxes of a single level within the hierarchy. in this way, it is possible to alter the lod uniformly-all elements visualized have the same lod at all times. in this way, the user can draw initial conclusions about the dataset as a whole and determine which areas are more of interest than others. when the user determines a region within the dataset that is particularly of interest, the ability to refine non-uniformly and interactively becomes crucial."
"after initial setup, we determine a good roi and begin rearranging the visualization methods in order to analyze correlations. we rearrange the parallel coordinate axes to observe dimensional correlations: high values in dimension 4 correlate with low values of dimensions 11 and 12. in the star coordinate view, we increase the magnitude and vary the direction of certain dimensions, in this case 4, 7, 11, and 12, shown in figure 7.2. as we can manipulate these axes, we observe to what extent the shape of the aggregates is affected. the blue aggregates are fairly unaffected by manipulation of dimensional axis 11 and highly affected by manipulation of dimensional axis 4; therefore, these aggregates have low values in dimension 11 and high values in dimension 4. furthermore, large hyperboxes represent very sparse distributions of data, observed in red, whereas small hyperboxes represent dense distributions, observed in blue. we conclude that a large quantity of the data has fairly high values in dimension 4 and extremely low values in dimension 11."
"in fields of algorithms and complexity, efficient methods for data processing are often introduced through the use of hierarchical data structures. we have applied a hierarchical structure to large multidimensional datasets by generating an r-tree that contains the dataset. we utilized the efficiency of r-trees by implementing several interactive operations for analysis, and also used the hierarchical properties of r-trees to visualize the data at increasing levels-of-detail (lods) in order to reduce visual clutter."
"we introduce a method to generate a hierarchical structure of data which allows for efficient interactive operations as well as methods for visualization of data within this hierarchical structure. in contrast to previous work, our method provides a great degree of efficiency and requires minimal data-specific information, while also adding functionality for analysis."
"to achieve appropriate low-dimensional visualization of high-dimensional data within a hierarchy, we have implemented existing methods of hierarchical multidimensional visualization and have extended upon one of these methods in order to accommodate it for more beneficial and efficient use within an r-tree structure. specifically, we have examined hierarchical parallel coordinates and built on previous work to develop a new method for hierarchical star coordinates."
"in order to provide 1) a scalable hierarchy for large multidimensional datasets, 2) visualizable and accurately representative aggregate items within that hierarchy, and 3) efficient interactive operations on the structure, we propose organizing datasets into r-trees."
"the r-tree aggregate items are ranges of values for each of the total d dimensions, which we will denote as hyperboxes. in one dimension, a hyperbox is a range of points, or an extension of a single point, which is a line segment. in two dimensions, a hyperbox is a range of lines, or an extension of a single line segment, which is a rectangle. we continue this process of extending lower-dimensional hyperboxes in order to generate hyperboxes of unlimited dimensionality."
"non-uniform interactive to facilitate interactive non-uniform refinement, we introduce a method to execute queries. these queries allow the user to define which dimensions and regions are of interest, and then refine the corresponding hyperboxes as desired."
"these hyperboxes are accurately representative aggregate items for visualizing internal levels of a hierarchical data structure because they denote where the children of their respective nodes are as well as how sparse or dense the elements within that hyperbox are, due to their bounding property. these characteristics allow the user to draw conclusions about what values within the dimensions of the data are most common as well as how varied the dimensional values are in comparison with each other."
"in order to generate a visualizable hierarchy from a dataset, several proposed methods involve hierarchical clustering algorithms. fua [cit] presented one of these algorithms based on proximity information and linsen [cit] presented another one based on density functions. though effective for generation of a hierarchy, they both involve an added preprocessing step to cluster the data. these approaches have high computational complexity for generation and interactive operations, since they are not guaranteed to be balanced trees."
"it is crucial for our application to interactively operate on datasets that are not only large in quantity of elements, but large in dimensionality as well; therefore, generation, queries, and refinement operations must be low in computational complexity. the use of r-trees allows us to execute hierarchical generation and interactive operations very quickly, even with large datasets of many dimensions."
"the visualization of massive multidimensional datasets as organized within rtrees requires a transformation from the d dimensions of the r-tree data into the two dimensions available on screen space, as well as effective methods of visualizing both aggregate items and individual data elements."
"r-trees generate an lod-hierarchy of aggregate and data items in a \"bottomup\" fashion. all individual data elements are inserted into the bottom level, and nodes are split into two new ones when their respective number of children exceeds the maximum number of children, m. whenever the root node is split, a new lod in the hierarchy is introduced. every internal node contains a number of children and a region which bounds all of its children. node splitting in r-trees is a widely covered research topic, as the optimal solution requires factorial time complexity [cit] . our implementation uses linear splitting, a method which delivers accurate enough results for our application as well as linear time complexity. r-trees allow for alteration of their internal tree depth, and different tree depths directly affect both user preferences and storage space. we can control the depth of the hierarchy by specifying different values for m. a larger value of m corresponds to fewer splits and, therefore, fewer levels within the hierarchy. with more children per node, there are more available refinable nodes per level and less total aggregate items. this means that there are more detailed specification of rois within a level, and less total internal storage space is required. however, having too many children per node contributes to visual clutter and less lods within the hierarchy. for smaller datasets, a small value of m is useful, because more lods organize the data more efficiently for interactive operations and introduce more lods. lower values do, however, increase the storage space. an illustration of the differences between high and low m values is shown in figure 1."
"to be more explicit, refinement means breaking down certain regions within the r-tree into their more detailed components. this is done by removing a hyperbox from the visualization and replacing it with its child hyperboxes or data elements. this provides us with a more accurately detailed visualization."
"in order to remove an outlier, we construct a query which contains it. as explained in section 6, when we are looking for specific elements, like outliers, overlap queries are more effective. after running the overlap query and coloring the result white, the outliers in dimension 5 barely contribute to the visualizations."
"in order to allow for a scalable representation, we require a structure that not only organizes the data into an lod-hierarchy, but allows for appropriate visualization of levels within it. for this reason, it is crucial that we generate aggregate items that are accurately representative to the actual data, as well as usable in various visualization schemes. an item that is accurately representative of the data is one which does not remove semantic information from the dataset."
"future implementations of our method could significantly influence areas which use progressive refinement, such as rosenbaum's [cit] technique for device adaptation. as progressive refinement methods require generation of lodhierarchies for many types and sizes of multidimensional data, our method provides much of the necessary functionality."
"we can examine correlations between dimensions and between individual clusters/elements by performing refinement operations until we achieve the desired lod in a roi, and arranging the visualization to show correlations. as example we analyze a dataset of forest fires within the northeast region of portugal [cit] ."
"this type of query facilitates interactive searching for programmatically generated clusters of data-because it tests for nodes that completely encompass the query region, this is an effective way for the user to find bounded clusters created by the r-tree generation."
"we examined and implemented two alternative coordinate systems for multidimensional visualization. we show that r-trees are visualizable using hierarchical parallel coordinates, and introduce a method which builds upon kandogan's [cit], which we denote as hierarchical star coordinates. in both cases, we describe how to represent multidimensional data elements as well as bounding hyperboxes."
"it is essential in visual data analysis to isolate and either extract more detail from or eliminate outliers. with our system, it is possible to discover outliers quickly and either decrease their significance or refine them in order to examine them more closely. we explain the process by example, with a dataset of regional wines [cit] of 13 dimensions and 178 elements."
"wright [cit] introduced the use of multiple visualizations, scatterplot matrices, where a matrix of two-dimensional scatterplots is displayed such that every dimension is plotted against every other dimension. similar multiple visualization schemes have been developed as well; however, with all these techniques, either the number of dimensions is constrained by the screen space available for multiple plots, or the visualization cannot display all dimensions at once."
"the present work builds upon this study, since it encompasses algorithms that are applicable in runtime-sensitive online environments such as often found in educational settings. moreover, their results have shown that simple recommendation mechanisms based on the base level learning equation (bll) and most popular (mp) outperform other state-of-the-art algorithms."
"both synthetic and real biology-inspired benchmarks were used for the evaluation. the experimental results show that the proposal achieves not only a competitive speedup against the serial implementation, but also a good scalability when the number of nodes grows. the paper can be useful for those interested in the potential of spark in computationally intensive nature-inspired methods in general and de in particular. to the best of our knowledge, this is the first work on using spark to parallelize de."
"second, we evaluate the performance of two tag recommender approaches that imitate human behaviour, in particular the process of human categorization and the retrieval of words from memory. the algorithms, minerva and baselevel learning equation (bll), as well as mp as a baseline, were applied as within-subject variables, either on the basis of the collective or the personal tagging history."
"after estimating the normal vector, we segment the 3d point cloud utilizing the topological structure of 3d space environment. our proposed method is basically based on the region growing approach. in the method, the similaity s i,j between the ith and jth nodes is defined as the inner value of the normal vectors between nodes."
"the two plots illustrated in figure 3 present the development of the tag vocabulary on a group level as described in section 3.1. the graphs put side by side, the tag growth occurring in the collective group vocabulary condition (c) with the tag growth happening in the personal vocabulary condition (p), where students received their tag recommendations either based on collective tag traces or on personal tag traces, respectively. figure 3a depicts the tag growth function according to the macro tag growth method and shows that while initially the vocabulary growth overlaps in both groups, group c starts to introduce less new vocabulary in relation to tags than group p. in other words, we can observe that students in the collective condition start to pick up the vocabulary of their peers faster. this result is even stronger when considering that a greater number of users contributed to the tagging data of the collective condition than to the data of the personal vocabulary condition (see table 2 ). this indicates a positive effect of collective tag recommendations on semantic stabilization. figure 3a provides additional insights into the timing of the process. we can observe that the two tag growth functions clearly diverge after about 40 added posts."
"the pceo algorithm first computes the oversubscription factor (os), (line 4), which describes the relation between the number of parallel concurrent region invocations and the number of available cores. an oversubscription factor larger than 1 means that there are more parallel concurrent region invocations than available cores. the algorithm completes if it finds a new merging configuration (line 40) or if the work list is empty. from line 9 to line 20, the algorithm handles concurrent regions that cannot be merged with other concurrent regions. line 10 to line 15 check if the execution time of the concurrent region is smaller than t_ser. furthermore, the algorithm checks if the concurrent region is currently compiled to parallel code. if so, the pceo algorithm issues a serializing recompilation (line 12 and line 13). from line 15 to line 19 the algorithm checks if the execution time of the concurrent region is larger than t_ser. if the concurrent region is compiled to sequential code, the pceo algorithm issues a parallelizing recompilation."
"the cend cir-node delimits the end of a concurrent region. cend represents the final inter-thread action of a concurrent region. consequently, all inter-thread actions in the concurrent region must be ordered before cend and all inter-thread actions that follow cend in program order must happen after cend. child threads commit the local view of memory to global memory. consequently, cend has release semantics for child threads."
"from the new programming models that have been proposed to deal with large scale computations on cloud systems, mapreduce [cit] . in short, mapreduce executes in parallel several instances of a pair of user-provided map and reduce functions over a distributed network of worker processes driven by a single master. executions in mapreduce are made in batches, using a distributed filesystem (typically hdfs) to take the input and store the output. mapreduce has been applied to a wide range of applications, including distributed pattern-based searching, distributed sorting, graph processing, document clustering or statistical machine translation among others."
1. c 1 and c 2 are in the same basic block and there is no instruction between c 1 and c 2 2. all instructions between c 1 and c 2 are concurrency invariant to either c 1 and/or c 2 3. bb 1 dominates bb 2 and (a) there is no @sync annotated method on any path from c 1 to c 2 and (b) there is no synchronization action in a sequential region (or concurrent region that is compiled to sequential code) that is on a path from c 1 to c 2 .
"for the purpose of this study and taking into account data provided by the evaluation environment, the associative component cannot be calculated, as this component is based on tags other users have assigned to the very same content or item. wespot, however, is a narrow folksonomy (such as for instance flickr), where content is generated and tagged only by one user. we thus make the assumption that bll is the most accurate approach for our data set."
"instructions that change the control flow are not concurrency invariant, since the reordering potentially changes the condition under which a piece of code can be executed in parallel. consider the example shown in figure 7, which depicts the effects of reordering the cstart cir-node with an if ir-node. figure 7 (a) shows a sequential region (sr) that conditionally executes a concurrent region (cr). i.e., the then part contains a concurrent region and the else part contains sequential regions. figure 7 (b) shows the result of moving the if-node into the concurrent region. in figure 7 (b) the child thread executes the then part but potentially also the else part of the if-statement. such a behavior is, in general, illegal since the else part must be executed by the same thread as the sr. for example, the else part can write to a global variable that is read after the if statement by the thread that executes the sr. however, if the else part is executed by a separate child thread, the code transformation shown in figure 7 (b) introduces a data race that is not present in the original version of the program."
"multi-carrier modulation has become the key transmission technology for high data-rate, wideband wireless communications. orthogonal frequency division multiplexing (ofdm) has been adopted by the majority of wireless communication standards such as ieee 802.11, ieee 802.16, and 3gpp's lte-advanced, due to its capability of combating intersymbol interference caused by frequency selective fading channels. a new variant of ofdm, known as multi-carrier index keying ofdm (mcik-ofdm), has been recently proposed as a means of extending the conventional two dimensional m -ary signal constellations to a third dimension, which is the sub-carrier index [cit] . similar to the spatial modulation (sm) concept [cit], in every mcik-ofdm transmission only a subset of sub-carriers is activated, according to the incoming data, to convey constellation symbols."
"for mcik-ofdm with ml, the receiver makes a joint decision on the active sub-carriers and the constellation symbols. however, by considering only the active sub-carrier detection, the pep of ml can be formulated as,"
"this section presents an overview of how concurrent calls are integrated into the java platform. we chose the java platform for the following reasons: first, java has a welldefined memory model [cit] that exactly defines the semantics of parallel java code. second, java code runs in a managed runtime environment, the java virtual machine (jvm), which is typically equipped with a just-in-time compiler (jit compiler) and a dynamic profiling infrastructure. third, java supports annotations, which can be used to attribute java language constructs without having to change the language itself. finally, there exist several open source implementations of the jvm."
"if all instructions between c 1 and c 2 are concurrency invariant to either c 1 and/or c 2, the instructions can be moved into c 1 /c 2 . as a result, the concurrent regions are contiguous and therefore mergable."
"tagging, as a simple mechanism to annotate resources individually or socially, has demonstrated its potential to facilitate search, to improve recommendations and to foster reflection and learning on the web precisely as in technologyenhanced learning environments [cit] . for instance [cit] investigated the effect of learning item annotation in the context of ibl and found that tagging encourages students to reflect upon retrieved learning contents. moreover, bateman and brusilovsky [cit] argue that according to bloom's taxonomy of learning [cit], learner's engagement in the tagging process fosters the development of a metacognitive level of knowledge, and hypothesize that the evaluation of peer learners tags might even lead to a deeper level of learning."
"when testing each one of the previous approaches, even using the version that has shown best benchmarking results, the penalty due to broadcast the whole population to workers in each iteration was unaffordable. for instance, using one of the benchmark functions used later on in section 5, the f 15 function, and a stopping criterion based on a predefined effort of 800,000 evaluations, the execution time of seqde was 30 s, while the execution time of smspde using 4 nodes was 263 s. the main conclusion of our experience with the masterslave implementation of the de algorithm was that this approach does not fit well with the distributed model of spark. therefore we decided to implement a new parallel version of the algorithm using an island-based approach which in advance seemed to be a more promising one. figure 4 shows the scheme of the spark-based island de (sipde) implementation. as it can be seen it has some steps in common with the master-slave implementation: i.e. generation of the population, checking of the termination criterion and selection of the best individual. the main difference resides in the way the population evolves. every partition of the population rdd has been considered to be an island, all with the same number of individuals. islands evolve isolated during a number of evolutions. this number can be configured and is the same for all islands. during these evolutions every worker calculates mutations picking random individuals from its local partition only. to introduce diversity a migration strategy that exchanges selected individuals among islands is executed every time the number of evolutions is reached. this evolution-migration loop is repeated until the termination criterion is met."
"this section presents the results of our evaluation study in respect to recommendation accuracy. table 3 provides the number of observations (see column nt ) and accuracy estimates (r, p and f) for each recommender."
"condition 3: bb 1 must dominate bb 2 so that the merged concurrent region can safely be issued at bb 2 . if bb 1 does not dominate bb 2, the concurrent region that is defined in bb 2 is eventually never executed. consider the examples as shown in figure 9 . figure 9 (a) illustrates the original program. the arrows in figure 9 indicate transitions between basic blocks that are triggered in a sequential region. i.e., the transition from bb 2 to bb 3 and bb 2 to bb 4 is not determined in c 2 ."
"this paper presents a real-world evaluation investigating the application of tag recommender approaches from two perspectives: first, by dividing students in two groups receiving either tag recommendations based on personal or collective tag traces, we gain insights into the effect of collective tag recommendations on the semantic stabilization process of collective learning groups."
"the aim of this paper is to explore this direction further considering a parallel implementation of differential evolution (de) [cit], probably one of the most popular heuristics for global optimization, to be executed in the cloud. the main contribution of the proposal is an analysis of different alternatives in implementing parallel versions of the de algorithm using spark and a thoroughly evaluation of their feasibility to be executed in the cloud using a real testbed on the amazon web services (aws) public cloud."
"monitorenter and monitorexit the bytecodes monitorenter and monitorexit implement the synchronized statement. as specified in the jls, monitorenter acquires the intrinsic lock of an object and monitorexit releases the intrinsic lock. moving either of both bytecodes into a concurrent region changes the inter-thread semantics of the child thread and the parent thread, since acquiring/releasing the lock is then performed by a different thread. as a result, both bytecodes are not concurrency invariant."
"the overhead factor is a metric for the impact of the parallel overhead on the execution time of a concurrent region. i.e., a low overhead factor indicates that the concurrent region incurs a low parallel overhead compared to its execution time."
"prior to the first lesson, students and their parents were presented with the goals and benefits of using ibl and the online learning environment, as well as with the aims of the study. afterwards, parents and students were asked for their consent in written and verbal form, respectively. throughout the study students' participation was not obligatory, in either the platform or in tagging and did not contribute to their grading."
"the data sets used in this study were collected on a dedicated log data server, from which we extracted the eight inquiry groups that participated in our experiment. all groups consisted of students attending a high school in graz and worked on the projects in the course of biology classes, on altogether four different research topics. as one setting took place in the course of an extra-curricular specialisation, ten students participated twice in the experiment. the data was collected over a period of three school semesters (i.e., [cit] )."
"diversity combining is a well known technique for mitigating the fading effects in wireless communication channels [cit] . based on the fact that independent signal paths have a low probability of experiencing deep fades simultaneously, receiver diversity combines the signal of multiple paths in a way that the fading of the resultant signal is reduced."
"the most frequent tags of the user's inquiry group are considered, however, in order to continue collecting context information (i.e. tags that are new to a user). this is implemented in an additional recommendation approach denoted by bllu + mpg."
"jit compiler profiling system adaptive optimization system runtime system extensions figure 2 . system overview. figure 2 depicts the system overview. we extend the java programming language by two annotations: @concurrent and @sync. @concurrent provides the declaration of concurrent calls. @sync provides the declaration of synchronization calls, which can be used to synchronize concurrent calls."
"in above equations; ± sign refers to two different configurations of the four bar mechanism. a, b, c, d, e and f expressions are then written as ( 1) cos"
here an example is included to show comparative results on ga and fmincon. the results for ga and fmincon are shown in table 1. table 2 presents target and traced point with ga. these points are calculated by using eqns (10.1) and (10.2) . figure 3 shows the target and the traced points in x-y with ga. since fmincon yields only one result which is included in table 1 as a separate column. ga results in different values presenting their optimum at the end satisfying the requirement. table 1 presents 6 precision points on the coupler curve. objective functions are the same with ga.
a method declaration that includes the @concurrent annotation declares a concurrent method. concurrent methods can be executed sequentially or in parallel. the semantics of a sequential execution and a parallel execution of a concurrent method are defined as follows:
"with equation 5, we estimate how useful an item (tag) has been for an individual person in the past, with n determining the frequency of tag use in the past, and tj standing for recency, i.e. the time since a tag has been used for the j th time. the parameter d models the power law function of forgetting and is in line with [cit] set to 0.5."
"individual user's tagging of items shows great potential in the organization of knowledge within and across information systems [cit] . however, the usefulness of such annotations is conditioned by the development of a shared terminology that leads to a meaningful description of resources [cit] . the attainment of an implicit consensus on a collective vocabulary within a group, which is stable over time and in meaning, is called semantic stability [cit] . in this work, we use the notion of semantic stabilization not to refer to a point in time, at which such consensus is reached and remains stable thereafter, but, we use it and a simple measure thereof (see section 3.1) to merely characterize and compare the evolution of convergence in tag choices of two groups of students over a short period of time (few weeks)."
"the wespot space supports collaborative learning in defined groups. each student group is thus provided with a sub-environment that forms an inquiry space addressing a specified research interest. teachers take on a supportive and administrating role. in the platform, they are provided with a configuration interface, for designing inquiry spaces by selecting phases (tabs) and activities (widgets) that suit the purpose of their student's inquiry projects. teachers also add students and initial learning content to the group environment."
"global optimization problems arise in many areas of science and engineering [cit] . most of these problems are np-hard, so many research efforts have focused on developing metaheuristic methods which are able to locate the vicinity of the global solution in reasonable computation times. moreover, in order to reduce the computational cost of these methods, a number of researchers have studied parallel strategies for metaheuristics [cit] . however, all these efforts are focused on traditional parallel programming interfaces and traditional parallel infrastructures."
"the complex amplitude term, h σ, results in an snr at the combiner output, γ σ, with a distribution that depends on the number of diversity paths, the fading distribution on each path, and the combining technique. following the same detection process as described in the previous subsection, the instantaneous pep for gd with diversity reception is given by,"
"the table discloses the impact of the two variables algorithm and data set on performance: bll appears to reach higher estimates than minerva (relative to mp ) under the personal vocabulary condition, with the opposite being true for the collective condition. the plots show the development of tagging vocabulary on a system (inquiry-based learning group) level. the two line graphs depict the between-subject variables of the study, that distinguish between the settings: collective (c) and personal (p). table 3 : properties of the analysed data set, structured by the applied algorithm."
"the java memory model (jmm) [cit] defines the semantics of parallel java code. the jmm is a relaxed-consistency model that allows the jit compiler (and the hardware) to reorder independent instructions. despite the possible instruction reordering, the jmm guarantees sequential consistency (sc) [cit] for correctly synchronized programs, i.e., programs that contain no data races. the jmm defines strict rules that define precisely which values can be obtained by a read of a shared field that is updated by multiple threads. concurrent methods and synchronization methods can read from and write to shared memory. the explicit representation in the cir requires a precise definition of the jmm semantics to ensure compliance with the jmm."
. the offset angle is notated by θ0 and the input angle is θ2. the position vectors are used to get complete four bar linkage as in eqn.(1).
this section discusses the conditions that enable the jit compiler to merge concurrent regions. merged concurrent regions have a coarser ptg compared to unmerged concurrent regions.
"the main source of parallelism in the java grande benchmarks are parallel loops. the java versions implement parallel loops by assigning each thread an equally-sized chunk of the iteration space of the loop. the versions using concurrent methods are parallelized by outlining the body of each parallel loop to a separate method that is annotated with @concurrent. to provide the opportunity to merge concurrent regions, we manually unroll parallel loops. we must unroll parallel loops manually, since the loop unrolling pass in the jit compiler of the jikes rvm produces unstable code. note that the manual unrolling of parallel loops is only necessary due to this missing standard optimization in the jikes rvm. production jvms like hotspot perform loop unrolling and it is easy for the jit compiler to identify concurrent regions that are in a loop. the jit compiler can force loop unrolling (and therefore generate mergable concurrent regions) if the overhead factor of the concurrent region suggests to merge the concurrent region. we unroll parallel loops of all benchmarks by a factor of 8. as a result, the pceo algorithm can coarsen the given parallel code granularity by a factor of 8."
in future work we are planning to strengthen our argument by introducing students' learning as an additional dependent variable. this will allow for further investigation of the correlation between individual's learning progress and the development of a common terminology in their learning group.
"due to the growing quantity of learning resources and learning data available in digital learning repositories and on the web [cit], learners often struggle with the organization, the retrieval and even the awareness of relevant learning content [cit] ."
"figure 5 also shows the speedup and efficiency when using the quality of the solution as stopping criterion. speedups are larger than the ones obtained for predefined effort because the cooperation among islands in the parallel searches modifies the systemic properties of the algorithm, improving its convergence and outperforming the serial one. in the figures reported in table 2 it can be seen that table 2. for the f 15, f 17 and f 20 benchmarks, again due to their short execution times, most of them converged after two migrations. however, when the number of nodes grew, some benchmarks required only one migration (so giving an average of 1.7 in some experiments), thus, improving the efficiency obtained for 16 nodes. the f 22 benchmark is a highly multimodal function that frequently fall into an undesired stagnation condition. thus, the dispersion of the experimental results is very large (see the standard deviation in table 2 ) and the number of migrations also varies from a minimum of 3 to a maximum of 19 in different runs."
there is an increase in computer technology which has permitted us in developing routines that apply methods to the minimization of a goal function. there is a common goal function that is the error between the points tracked by the coupler (crank-rocker) and its desired trajectory in general. the aim is to minimize the goal function applying optimization techniques here. initially the link lengths are chosen according to the grashof's theorem. many cases a continuous rotary input is applied and the mechanism must satisfy the grashof criteria. the first part computes the position error in the objective function. the sum of the squares of the euclidean distances between each point is defined and a set of target points indicated by the designer that should be met by the coupler of the mechanism. these points can be written in a world coordinate system as are the target positions on the coupler.
"if a thread commits a new sample to the global database, the committing thread wakes up another thread (which belongs to the aos) that evaluates the new data asynchronously. the next section describes the evaluation process of the samples in detail."
n represents the number of points to be synthesized. the geometric magnitudes of four-bar mechanism are previously described in fig. 2 . the design variables and the input angle θ2. the second part of goal function is derived from the constraints which are imposed on the mechanism and set as the following: (i) the grashof condition allows for full rotation of at least one link.
"with the aim of better understanding spark intricacies and assess the performance of different alternatives when implementing de, we have developed three different versions of the algorithm: (1) the classic sequential algorithm (seqde) which has been implemented for comparative purposes and it is the only that does not make use of spark; (2) three different variants of the master-slave parallel implementation (smspde); and (3) an island-based parallel implementation (sipde). all of them have been coded using the scala language [cit] which is the one used to implement spark itself although apis for python and java also exist. the rest of this section features relevant facts about each of the implementations. as it will be demonstrated, the main conclusion is that the island-based parallel implementation is the best suited to the distributed nature of spark and obtains the best performance results."
"to evaluate semantic stability, which we measured in tag growth (tg), we selected the class which created the highest number of posts in both inquiry groups. we consider this sample as the most significant to our investigation. to measure the recommendation accuracy (ra), we subsume all samples under the independent study variable vocabulary. the resulting data set properties are presented in table 3b ."
"the jikes research virtual machine (jikes rvm) sampling profiler can only provide an estimate of how much time the application spent in a particular method (t total ). t total does not enable the aos [cit] to draw inferences about t exec and i. a large t total can have two reasons: first, t exec is indeed long (and executed infrequently). however, a large t total can also result from a frequently invoked method with a short t exec . the sampling technique of the jikes rvm is also unable to count the number of invocation i of a concurrent region. to precisely determine t exec as well as the i, we add an instrumenting profiler to the jikes rvm."
"normal loads and stores reordering ro 1, ro 2, and ro 3 from figure 8 are concurrency invariant to c if the conditions listed in table 3 hold. assume that i 1 and i 4 do not change the control flow and that a load instruction (ld) loads the value from a location on the heap (o.f ) into a local variable l and that a store instruction (st) stores the value of v to the field o.f ."
"where h α and hα denote the channel amplitude of an active and an inactive sub-carrier, respectively. similarly, the pep of gd is formulated as,"
"(a) original code. figure 9(b) shows a variant of the original program in which the concurrent regions c 1 and c 2 are merged. bb 1 and bb 2 can be combined into a single basic block. as a result, c 1 and c 2 are contiguous concurrent regions and can therefore be legally merged. figure 9 (c) shows an example in which c 2 is merged with c 3 and c 2 is merged with c 4 . note that either merging c 2 with c 3 or c 2 with c 4 results in an illegal program, since c 2 is then not guaranteed to be executed in any path from bb 1 to bb 5 . finally, figure 9 (d) shows a variant of the original program in which c 2 is merged with c 5 . this code transformation is legal if c 3 and c 4 are compiled to parallel code. since bb 2 dominates bb 5, both concurrent regions are guaranteed to be executed. merging c 2 with c 5 determines a particular schedule of execution."
the performance results presented in this section are gathered by executing the benchmarks 50 times in the same jvm instance. we use 10 warmup runs and 40 profile runs. the warmup runs are not included in the results to measure steady-state performance. the presented results are the arithmetic average over the 40 profile runs. the error bars indicate the 95% confidence interval.
"we choose 0.99 for x and for y. figure 16 illustrates the merging function. we use this merging function for the following reasons: first, the merging function provides a high oversubscription factor (number of tasks per core) for a low overhead factor. as a result, the system is well balanced if the parallel overhead does not dominate the execution time of the application. recall that an overhead factor of 0 means that there is no parallel overhead. an overhead factor of 1 means that the parallel overhead is equal to the execution time of the task. second, the oversubscription factor decreases exponentially with an increasing overhead factor. as a result, the efficiency of the system is well maintained. the jvm is equipped with this merge function by default. figure 18 shows the performance results obtained by our automated approach to determine the ptg compared to three different java versions. the java versions differ regarding the ptg: the ptg in figure 18 (a) is chosen such that the benchmark yields best performance. to find the best-performing ptg, we manually try different ptgs. i.e., for each parallel loop, we divide the iteration space of the loop into 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024 equally-sized parallel tasks that are executed in parallel by 32 threads. figure 20 in appendix b illustrates the performance results for the manual ptg performance runs. the sequential execution times of the evaluated benchmarks that are the baseline (a value of 1 on the y-axis) for figure 20 are as follows: crypt: 4.9s, series: 40.7s, sor: 2.1s, lu: 1.4s, matmult: 1.6s, and montecarlo: 24.2s. the ptg that yields the best performance for a benchmark is the baseline in figure 18 (a) (512 tasks for crypt, 1024 tasks for series, 64 tasks for sor, 16 tasks for lu, 64 tasks for matmult, and 512 tasks for montecarlo -in this order). it is easy to see that finding a good ptg is difficult."
"starting and joining threads starting and joining threads is not concurrency invariant, since both operations are implemented as native function calls that cannot be inlined."
"where p k and s ik are components of vector p and si respectively. finally, we calculate the activation value t out j of tag j as the weighted sum of tag activation values over all items in the data set."
"unfortunately, current programming techniques support only a manual determination of the ptg. in particular, programmers must decide which parts of an application are best executed in parallel and what ptg yields good performance. at the same time, we see a wide range of parallel systems in use, ranging from modest dual-core systems to systems with 64 cores (or more). ahead-of-time custom-tuning software for each platform is expensive, if possible at all. this paper presents a software platform that automatically determines the ptg at run-time. our system asks the programmer to overspecify parallelism. the runtime system evaluates the ptg as provided in the source code and adapts the ptg towards a good tradeoff between load balance and parallelism overhead. more specifically, we show that the runtime system effectively removes overspecified parallelism by (i) serializing parallel tasks that incur a high overhead and (ii) merging two (or more) parallel tasks into a single parallel task. note that if performance suffers from parallelism underspecification, the runtime system would have to auto-parallelize (parts of) the application to speed up execution. since current runtime systems cannot effectively auto-parallelize applications, performance problems due to parallelism underspecification cannot be fixed by the runtime system."
"in mcik-ofdm the receiver must follow a two-step detection process to first determine the indices of the active sub-carriers and subsequently detect their corresponding data symbols. in the first step, the gd measures the received power on each sub-carrier and estimates k sub-carriers with the greatest received power as active. hence,"
"second, we systematically vary two variables underlying the design of these recommenders in order to derive more precise and practical design implications for specific learning settings. the first variable is the vocabulary, from which the algorithm selects the tags and which can either be the learner's personal (p) or the collective vocabulary of the whole group of learners (c). the second variable is the type of information the algorithm takes into account to estimate the current probability of a tag being retrieved from the learner's memory. while mp only considers a tag's usage frequency (baseline), our two cognitively inspired algorithms extend this approach by the information of recency of usage (bll) and the extent to which a tag matches the current (i.e., the resource's) semantic context (minerva)."
"rdds are computed lazily the first time they are used in an action, so transformations can be pipelined to form a lineage. by storing enough information about their lineages, rdds do not need to be materialized at all times, as every rdd can recompute its partitions from previously persisted rdds or data in stable storage at any time. this feature is the one used to provide fault-tolerance in case of an rdd partition lost."
"for complex problems, like the circadian benchmark, the number of migrations clearly decreases with the number of nodes, demonstrating the potential of the parallel algorithm for improving the convergence of the de method. the harder the problem is, the most improvement is achieved by the parallel algorithm, since the diversity introduced by the migration phase, although using a naive strategy as explained in section 4.2, actually improves the effectiveness of the de algorithm. thus, for the circadian benchmark superlinear speedups are obtained, as well as efficiency above 1."
"we have also done some experiments on our region growing based segmentation method. fig.8 shows an example of feature extraction and segmentation result by proposed method. as can be seen from this figure, our proposed method can estimate the normal vector and roughly segment the 3d point cloud such as walls and floor. in addition, fig.9 shows the transition of the processing time which includes all the processings. the average time is 0."
is not used in sr2. note that changes to local variables that are performed by the child thread are not visible to the parent thread.
"cilk [cit] extends the c/c++ programming language with parallel calls and a construct to synchronize parallel calls. the semantics of concurrent statements correspond to the semantics as proposed in cilk. the main difference to cilk is that our approach integrates concurrent regions into a runtime system that allows dynamic optimizations such as profile-based recompilation [cit] . the current cilk runtime cannot perform such optimizations. [cit] present a flow-sensitive, context-sensitive, inter-procedural pointer analysis algorithm for cilk programs."
"the section presents the experimental evaluation of the overhead that comes from concurrent region profiling as well as the performance evaluation using the java grande benchmark suite [cit] . the performance evaluation uses our prototype implementation of the extensions presented in section 4, section 5, and section 6 in the jikes rvm [cit] ."
"each benchmark is executed with the same jikes rvm configuration. in particular, we use a profiling interval (threshold) of 1 million cycles, a serialization threshold of 10,000 cycles, and the following function, which describes the merging and demerging threshold."
"this has presented a study for synthesis of planar mechanisms; specifically on a one degree of freedom (dof) planar mechanism. the algorithm is developed only for a grashof's type four bar mechanism. the idea is applicable to all types of planar mechanisms. the only difference will be kinematics analysis of the mechanisms and related constraints. the main advantage seen during implementation is that of simplicity. utilization of optimization toolbox is performed and a fast convergence to optimal solution is observed. since the routine is performed directly, there will be no need for superior knowledge during optimization. it is seen that use of ga during optimization study is more advantageous to use fmincon. it presents the objective function's optimum each time. the results are similar, but not the same. (figure 3 ) therefore ga toolbox can be easily applied to mechanism synthesis problems. only problem becomes to derive related kinematics for related mechanism as constraints [cit] ."
"the csync cir-node is an inter-thread action since the parent thread can determine if all child threads have finished execution. in terms of the jmm, csync has the same semantics as calling join() on every child thread. the join() operation ensures that all children have finished execution and updated their local view of memory to global memory. the parent thread synchronizes the local memory with global memory."
"the aos of a jvm selects methods that are good candidates for dynamic recompilation. we extend the standard aos by the parallel code execution optimization (pceo) algorithm, which uses profile information collected by the extended profiler to analyze the behavior of a particular ptg. the pceo algorithm can change a particular ptg by triggering a recompilation that, e.g., merges concurrent regions that were not merged in the previous configuration."
"spark runtime is composed of a single driver program and multiple workers which are long-lived processes launched by the driver. workers read data blocks from a distributed file system and persist rdd partitions in ram across opera- tions. developers write the driver program where they define one or more rdds and invoke actions on them. whenever an action is executed on an rdd, the spark job scheduler uses its lineage to compute a directed acyclic graph (dag) of stages. the scheduler then launches tasks to compute missing partitions from each stage until it has computed the target rdd. assignment of tasks to workers takes into account data locality. tasks end up being assigned to workers that already hold the rdd partitions of interest in memory. an example of how spark computes job stages is showed in figure 2 . in the figure, boxes with solid outlines are rdds. partitions are shaded rectangles, darker if they are persisted in memory. each stage contains as many pipelined transformations with narrow (one-to-one or one-to-many) dependencies as possible. the boundaries of the stages (boxes with doted outlines in the figure) are the shuffle operations required for wide (many-to-one or many-to-many) dependencies or the presence of an already computed rdd in the lineage. in the example, to run an action on rdd g, as output rdd from stage 1 is already in ram only stages 2 and 3 need to be executed."
"additionally, our work is motivated by a more technical stance: when selecting a proper tag recommendation strategy, tel specific requirements need to be taken into account. for instance, in tel scenarios, data is typically of a sparse nature [cit] . furthermore, sensitivity to an algorithm's complexity is crucial when calculating real time recommendations on limited computing resources [cit] . approach and methods. a very simple, though relatively effective, tag recommendation strategy is the most popular (mp) algorithm [cit] . we however assume that a frequency-based, computationally simple recommendation strategy may be even more successful, if it is grounded on a thorough understanding of how humans process information. our hypothesis is that in online social learning environments, semantic stabilization can be fostered by cognitively inspired tag recommendation approaches. offline data studies have indicated that the modelling of cognitive processes underlying tagging habits leads to an increased accuracy of recommendations [cit] . however, offline data studies are limited to evaluating the prediction of user behaviour. in our previous work [cit], we have intensively investigated the suitability of two tag recommendation approaches via offline studies [cit] : the first of these, known as bll implements the base level learning equation [cit], which models the frequency and recency of past tag use. the second algorithm, known as minerva [cit], incorporates tag use frequency as well as semantic context. both approaches aim to imitate cognitive processes of retrieving words from memory."
"example shown in figure 19, moving a volatile store into a concurrent region breaks the happens-before relationship between the volatile store and the preceding (in program order) store instructions. the reason is the asynchronous execution of a concurrent region."
"the dimensional synthesis problems can be broadly classified as motion generation, path generation and function generation [cit] . (i) motion generation: a rigid body has to be guided in a prescribed manner in motion generation. motion generation is related with links controlling the links in the plane. the link is required to follow some prescribed set of sequential positions and orientations."
"following the same approach as in the previous subsection, the approximate pep for mcik-ofdm with gd-mrc over rayleigh fading is obtained by averaging (6) over (12) as,"
our spark-based master-slave de implementation (smspde) follows the scheme shown in figure 3 . a key-value pair rdd has been used to represent the population where each individual is uniquely identified by its key. some de algorithm steps have been selected as appropriate to be executed in a distributed fashion:
"results from both views are shown in table 2 . for each experiment, the number of nodes (#n) used, the mean execution time and the standard deviation (in seconds) of the 10 independent runs in each experiment, the average number of migrations (#m) performed in the sipde method, and the speedup (sp) achieved are shown. it should be noted that the stopping criterion is evaluated during each island evolution but, when it is met by one or more islands, the algorithm only stops after the reduce operation at the end of the stage (see figure 4) . thus, because no communication among workers is possible in spark, the parallel sipde implementation cannot stop just right when the stopping criterion is reached (as the serial one does)."
"for the purpose of the study, students of each class were divided in two groups per class, which led to groups of 9 to 18 students, depending on class size. in the first two lessons, students were introduced to the online learning environment (see section 2.3) and the concepts of ibl. then, each group used the virtual learning environment during at least eight school lessons over a period of four weeks or longer to complete an ibl project. the teacher provided each of the classes' learning groups with similar learning content and learning tasks and acted in a supporting role. the variation between groups is constituted by the nature of tag recommendations. tag recommendations of one group are based on individual user's personal tag data, whereas the second group's tag recommender draw on the group's collective tagging traces. according to the group, tag recommendation strategies were randomly selected either from the personal or the collective pool of recommendation strategies, as illustrated in table 1 . each student was provided with a tablet computer available during class. students were encouraged to use the tagging functionality when creating or uploading new content, and were also provided with information in verbal and written form, on how to do this."
"vocabulary algorithm personal (p) mpu bllu minervau collective (c) mpg bllg minervag bllu + mpg consider bll+mp for the collective vocabulary condition. this recommendation approach is of particular interest, as in offline studies on tel data sets it clearly outperformed remaining algorithms [cit] . the dependent variables were semantic stabilization and recommender accuracy (see section 3)."
"each thread stores the gathered samples to a data structure that is private to the thread. the bytecode index of the concurrent region and the signature of caller of the concurrent region form the key for a hash map that holds the reference to a circular buffer that stores the performance samples. if the circular buffer collected a certain amount of new samples (report threshold) the arithmetic average of the samples is reported to the global database, which collects the samples of all threads. the report threshold starts at 1 and increases up to 100 in strides of 10. such a reporting strategy ensures that samples of newly discovered concurrent regions are reported immediately and frequently to the global database. the samples of concurrent regions that have been profiled for a longer time are reported less frequently. each thread maintains the number of concurrent region invocations in a similar manner."
"volatile load/store operations volatile load operations are never concurrency invariant, since making volatile loads concurrency invariant breaks the ordering guarantees provided by volatile semantics. consider the example in figure 19 . figure 19 figure 19 (b) can be executed prior to line 5. such a behavior is illegal in the original code. reordering a volatile load with the cend cir-node (e.g., line 9 in figure 19 (a)) causes an equivalent behavior for a concurrent region that follows line 9."
"this section discusses the java language integration of concurrent calls. more specifically, we present concurrent calls in the context of polymorphism, exceptions, and discuss the interaction with the standard java threading api."
"differential evolution is an iterative mutation algorithm where vector differences are used to create new candidate solutions. starting from an initial population matrix composed of np d-dimensional solution vectors (individuals), de attempts to achieve the optimal solution iteratively through changes in its vectors. algorithm 1 shows the basic pseudocode for the de algorithm. for each iteration, new individuals are generated in the population matrix through"
"we are aware of the limited evaluation data which is a consequence of the selected real-world learning setting. data in such learning environments is typically sparse, which has also been the reason for restricting the experiment to the three algorithm set-up. by contrast, results from offline data studies can compare a multitude of options. however, we argue that those results are limited in their reliability, as unlike real-world studies, offline data do not allow for investigation of recommendation strategies' ability to support users in their tasks, but solely evaluate the prediction of a user's behaviour."
"line 21 to line 48 handle concurrent region merging/demerging. shouldmerge() and shoulddemerge() return a boolean value that indicates if a concurrent region is merged or demerged. both functions are shown in figure 14 and figure 15, respectively. if the algorithm decides to merge concurrent regions, the algorithm first removes all concurrent regions from the list of potential merging candidates that are already merged with the concurrent region (line 25). in the next step, the algorithm sets the isparallel flag of the current concurrent region to false. the reason is that the execution time of the (merged) concurrent region can be lower than t_ser. from line 27 to line 41, the algorithm iterates over the merging candidates. for each merging candidate, the pceo algorithm (i) removes the merging candidate from the work list, (ii) adds the candidate to the existing set of merged regions, (iii) clears the existing merging configuration of the merging candidate, and (iv) sets the isparallel flag to false. line 34 to line 35 compute the execution time of the merged concurrent region. if the merged execution time is larger than t_ser, the merged concurrent region is compiled to parallel code (line 38 and line 39)."
"if we look at table 2, we can observe that the tagging frequency varies greatly among the groups. students that participated in the study used the environment in the course of biology lessons. however, the ibl project work did not contribute to their marking. also, they were encouraged to tag, but there was no particular monitoring of this process taking place. thus, some groups showed more motivation and participated more actively in the projects and within the environment than others."
the profile overhead decreases with an increasing concurrent region execution time and an increasing record interval. the highest profiling overhead is incurred with the shortest concurrent region execution time (10 ns) and a record interval of 0 cycles. the lowest recording overhead is 13% for the longest concurrent region execution time and the largest record interval. we choose a record interval of 1000k cycles for the experiments presented in section 7.3.
"the use of the sub-carrier indices as an additional degree of freedom enables the transmission of extra information bits without any additional bandwidth and power requirements. as a result, mcik-ofdm constitutes a promising modulation technique for providing high data-rate services especially for low-cost, energy constrained wireless systems such as deviceto-device (d2d) communications. mcik-ofdm has attracted significant attention as it can provide a balanced trade-off between error performance and spectral efficiency [cit] . to this end, different mcik-ofdm transceiver architectures have been proposed and analyzed over different conditions including frequency selective fading, high mobility, and time varying channels [cit] ."
"to test our hypothesis that in online social learning environments, semantic stabilization can be fostered by cognitively inspired tag recommendation strategies, we implemented a real-world evaluation in the context of high school biology lessons, engaging students in ibl projects. to this end, we used an online environment for open social inquirybased learning. ibl itself is very well-suited to the purpose of a collaborative tagging study as throughout the learning process, students are constantly challenged to find, create, upload and share content. in the course of the study, four secondary school classes with students at the age of 15 to 17 used a dedicated social learning environment to work on their biology projects."
"the current design supports the @concurrent and the @sync annotation for private (and private static) methods. in java, private methods can be called only from the class in which the method is declared. as a result, a concurrent method cannot be called \"accidently\" by, e.g., deriving from a class and calling the concurrent method of the superclass. the current design is conservative, since it allows only the implementor of a class to invoke concurrent calls. we choose such a design, since the programmer must ensure that concurrent calls yield a legal program execution. however, the current design can be changed easily and changing the design (i.e., relaxing the constraints which methods can be annotated with @concurrent and @sync) has no effect on the runtime system extensions."
"type of action type of optimization intra-thread inter-thread intra-region reordering jls jmm inter-region reordering ci ci table 1 . reordering rules for intra-and inter-region reordering optimizations. the rules are defined in the java language specification (jls) [cit], the java memory model (jmm) [cit], and in the subsequent definition of concurrency invariance (ci)."
the results indicate that the application of recommenders using collective tagging traces fosters semantic stabilization in collaborative learning settings. the consideration of frequency and semantic context further contributes to the adequacy of tagging recommendations. in respect to individual learning we find that a frequency and recency based approach (bll) performs best.
"at present, we identify three main lines of research related to our work: tagging as a support in learning, semantic stabilization in social learning systems and tag recommendation approaches in the context of tel."
"this section presents the results of our evaluation. we evaluate the suitability of the algorithms described earlier for supporting learners' tagging processes. in line with the study design, all algorithms are applied in two modes: personal (p), where the recommendation strategy draws on a single user's posting history, and collective (c), where the recommendation strategy draws on the prior posts of an entire group."
"assume there are two merged concurrent regions c x and c y . c x and c y can be merged if and only if every concurrent region that is merged into c x is mergable with every concurrent region that is merged into c y . figure 11 shows concurrent region merging in cir. figure 11(a) shows the original program and figure 11 (b) depicts the cir of the merged version. in particular, function calls b() and c(), which are contained in a separate concurrent region in figure 11 (a) are merged into one concurrent region in figure 11 (b). note that the cbarrier cir-node is required to guarantee sequential consistency for data racefree programs."
"by using the law of total probability, the upper bound for the overall average pep for mcik-ofdm with gd-mrc over rayleigh fading is given by,"
"the pceo algorithm takes the cir, the new profile information, and the merging function (see later this section) as an input. the pceo algorithm uses two thresholds: the serialization threshold (t_ser) and the merging function. t_ser represents the overhead that is associated with a parallel execution. i.e., t_ser corresponds to the execution time of a parallel task at which a parallel execution of the task is equally fast as the sequential execution. consequently. t_ser can be used to determine if a concurrent region is executed sequentially or in parallel. our experimental evaluation yields a value of 10,000 cycles for t_ser."
"many segmentation methods of the 3d point cloud have been proposed in recent years [cit] . the researches of segmentation can be divided into two main streams from the viewpoint of the sensor devices. one stream is to combine the 3d distance sensor and a camera devise such as rgb-d sensor [cit] . these researches can effectively cluster the material using color and luminance features and synthesizing the 3d coordinate system and camera coordinate system. however, it is unstable to cluster because these researches strongly depend on a lighting environment. another stream is to use only a 3d distance sensor such as distance and surface feature based segmentation methods. for segmenting the 3d point cloud, [cit] applied the region growing based algorithm using the distance and normal vector. these researches are very useful to divide the area in the 3d space because these kinds of methods do not depend on the lighting conditions. however, robust real time application is still not achieved [cit] because almost methods do not consider the time series 3d point cloud."
"the purpose of this study is to perform a comparative study on synthesis of mechanical linkages using genetic algorithm. some recent studies on the subject covering more than ten years are surveyed. since the optimum synthesis of a mechanism requires a repeated analysis to find the best possible one to meet requirements, dimensional synthesis will be preferred here. a simulation study will be performed on a four bar linkage. the linkage parameters will be tabulated as a guide for the user. the computational synthesis methods are also applied [cit] . the science of motion is related with the analysis and synthesis of mechanisms in study of kinematics. it also deals with the relative geometric displacements of points and links of a mechanism. dimensional synthesis looks for determining optimal dimensions of a prescribed type of mechanism. the type and dimensional levels are the main factors in the mechanisms for the study of kinematic synthesis of mechanisms [cit] . the objective is to apply an evolutionary method for synthesis of planar mechanisms and present a design guide for its use in linkage mechanisms. the evolutionary process is not related with the results which are obtained from enumeration of mechanisms. some algorithms are included in matlab as toolbox facility. this study is organized as follows; first part outlines an introduction with synthesis of planar mechanism, statement of problem. literature survey is also given on mechanism synthesis using gas. matlab optimization toolbox is introduced with genetic algorithm toolbox. some illustrative examples are done on optimization based synthesis problems for 4 bar mechanism. an example application is given by using two optimization approach based on matlab environment. matlab optimization toolbox with constrained optimization is compared with genetic algorithm toolbox (ga)."
"(b) cir of figure 4 each call to c() is contained in a separate concurrent region. concurrent region 1 (c 1 ) ranges from line 1 to line 3 and concurrent region 2 (c 2 ) ranges from line 5 to line 7. 2 we use the term instruction (or ir node) in the jvm context. all instructions of a concurrent region are enclosed between two cbarrier cir-nodes, a cstart and a cend cir-node. the csync cir-node is followed by a cbarrier cir-node, the synchronization method call, and another cbarrier cir-node. the jit compiler compiles concurrent methods and synchronization method calls to regular ir nodes."
"the jit compiler instruments concurrent regions as illustrated in figure 12 . in particular, the jit compiler uses the rdtsc instruction to measure the execution time. the execution time is only recorded at certain intervals. the if statement in line 2 checks if the time since the last record (thread.rec) is larger than threshold. to perform this check, each thread has a field, rec, that stores the time stamp counter when the last sample was recorded. the rec field is initialized to 0 when a thread is created. if the if statement in line 2 evaluates to true, the execution time of the concurrent region is recorded and the rec field is updated. otherwise, the sample is not recorded. we do not record every taken sample to keep the overhead from executing instrumented concurrent regions as small as possible. concurrent region instrumentation has two sources of overhead. the first source of overhead is the execution of the rdtsc instruction. the overhead of the rdtsc instruction is negligible, since the rdtsc instruction is (i) not serializing and (ii) not ordered with other instructions. the second source of overhead is related to storing and maintaining the gathered profile data. to measure t exec with a low profiling overhead the instrumenting profiler records samples only at a certain interval, the record interval. the impact of the record interval is explained in section 7. there is a tradeoff between the overhead of the sampling instrumentation and the precision of the measurement of t exec . if threshold is too large, the number of recorded samples is small. consequently, if t exec varies heavily upon every invocation, the sampled execution time can be imprecise. however, if threshold is too small, the profiling overhead can be significant."
with the advent of cloud computing effortless access to large number of distributed resources has become more feasible. but developing applications that execute at so big scale is hard. new programming models are being proposed to deal with large scale computations on commodity clusters and cloud resources.
"formally, an rdd is a read-only fault-tolerant partitioned collection of records. users can manipulate them using a rich set of operators, control their partitioning to optimize data placement and explicitly persist intermediate results (in memory by default but also to disk). rdds are created from other rdds or from data in stable storage by applying coarse-grained transformations (e.g., map, filter or join) that apply the same operation to many data items. once created, rdds are used in actions (e.g. count, collect or save) which are operations that return a value to the application or export data to a storage system."
"in the present work, we study the performance of the algorithms in an online, real-world scenario to explore whether the promising results from offline data studies generalize to online environments. to this end, we carried out a field study in which students used an online ibl environment in a realistic school context for a duration of about four weeks. our aim is to investigate the effectiveness of the two cognitively inspired recommendation mechanisms bll and minerva that mimic student's tagging behaviour, taking into account either temporal or semantic context. contributions. our contributions are twofold: first, we investigate the question of whether semantic stabilizationa socio-cognitive process supporting individual learning in an online ibl environment [cit] -can be supported by tag recommendation mechanisms that have been developed and tested previously in offline studies on a variety of data sets."
there is a second condition that must be fulfilled so that a thread reports a sample to the global database. the arithmetic average over the collected samples must differ by least by 30% compared to the last reported average. reporting similar samples provides no new information. we choose 30% based on empirical evaluation.
the rest of this paper is organized as follows. the fundamentals of mcik-ofdm and the principle of the gd are described in section ii. section iii presents the concept of applying diversity reception in the gd. in section iii novel closed-form expressions for the exact and approximate overall pep for mcik-ofdm with diversity reception are derived. the performance of the proposed scheme is analyzed in section iv through analytical and simulation results. section v summarizes and concludes the presented work.
"within technology enhanced learning (tel) research, recommendation mechanisms are part of adaptation or personalization strategies that support students in their individual learning processes [cit] . recommendation mechanisms, extract and draw on relevant data from learning traces, leveraging learning analytics [cit] . this is one way of tackling the often criticized lack of support for the self-organization of learning content in tel environments [cit] ."
"the hidden layer stores feature vectors of all data set items in a matrix s, such that s ik is the activation of feature k in item i. furthermore, in a tag matrix a, each data set item is associated with a tag vector ai. specifically, a tag activation value aij is defined as 1 if tag j was present in item i, and 0 otherwise. taken the input vector as stimuli, the activation of single tags can be calculated. to this end we first compute the cosine similarity for the input feature vector p with each feature vector si in our matrix, following equation 6:"
"wait(), notify(), and notifyall() the functions wait(), notify(), and notifyall() are low-level communication primitives. since wait() requires that the calling thread owns the monitor of the object on which the thread calls wait(), wait() is not concurrency invariant. if the jit compiler moves the wait() call into a concurrent region and the concurrent region is executed by the child thread, the child thread does not own the monitor. such a code transformation raises an exception that does not occur in the original program. notify() and notifyall() are not concurrency invariant for similar reasons as wait(). both calls require that the thread which performs the call owns the corresponding monitor."
"another aspect is that there is significantly less data available for vocabulary condition p, where users tag recommendations were based on their individual tagging history. due to the cold start problem, students in condition p had no initial tag seeds provided but rather had to come up with their own personal tag traces to initiate the tag recommendation process. we believe the resulting lack of tagging support, played a crucial role when students did not tag their contributions or tagged their contributions in unusual ways (see section 2.4). this is in line with previous findings (e.g., [cit] ) that underline the need for support students have in the tagging process."
"in order to explore how parallel metaheuristics could take advantage of the recent advances in cloud programming models, in this paper spark-based implementations of two different parallel schemes of the differential evolution (de) algorithm, the master-slave and the island-based, are proposed and evaluated. early benchmarking results showed that the island-based solution is by far the best suited to the distributed nature of spark. thus a thorough evaluation of this implementation was conducted on the aws public cloud using a real testbed consisting on virtual clusters of different sizes."
"this work is supported by the eu funded projects wespot (grant agreement 318499), learning layers (grant agreement: 318209) and afel (grant agreement: 687916), the austrian science fund (fwf) projects merits (grant no p25593-g22) and omfix (grant no p27709-g22), and the know-center. the know-center is funded within the austrian comet program under the auspices of the austrian ministry of transport, innovation and technology, the austrian ministry of economics and labor and by the state of styria. we are also very grateful to verein für bildung und erziehung der grazer schulschwestern and particularly to the teacher jürgen mack for the great support in implementing the study in his lessons."
"generally, the standard gng-u removes the node in the node insertion. in addition, our method removes the node if the number of input data generated so far is an integer multiple of a parameter κ for controlling the number of nodes. furthermore, our method uses the weight vector w to learn the topological structure of the 3d space environment. fig.2 and 3 show a 3d point cloud data with color and an example of gng-u with and without the weight vector. gng-u with the weight vector can learn the topological structure of the 3d environmental space with color ( fig.3 (a) ), while gng-u without the weight vector cannot learn the topological structure of the 3d environmental space."
"the automatic determination of the ptg is enabled by concurrent calls. concurrent calls are special source language constructs that can be executed sequentially or in parallel. we discuss the integration of concurrent calls into the java programming language and the java virtual machine (jvm), including the just-in-time compiler (jit compiler), and show how the jvm adapts the ptg. in particular, we present a compiler analysis that determines if concurrent calls can be merged (collapsed into a single concurrent call) and the merged concurrent call provides sequential consistency [cit] for data-race free programs. we therefore present an integration of concurrent calls into the java memory model (jmm) [cit] . the performance evaluation of a prototype implementation in the jikes research virtual machine (jikes rvm) [cit] shows that our runtime system performs competitively to java programs for which the ptg is tuned manually. compared to an unfortunate or unlucky choice of the ptg, our approach performs up to 3x faster than standard java code."
"the organization of this paper is as follows. section 2 briefly presents the background and related work. some new programing models in the cloud are described in section 3. section 4 describes the proposed implementations of the differential evolution algorithm using spark. the performance of the proposal is evaluated in section 5. finally, section 6 concludes the paper and discusses future work."
"the execution of concurrent methods and synchronization methods can raise unchecked exceptions. an uncaught exception aborts the execution of the concurrent method. if the exception occurs in the child thread, the jvm propagates the exception to the parent thread. exceptions that occur in a synchronization method are handled according to the java language specification."
"this section describes the parallel code execution optimization (pceo) algorithm. the pceo algorithm aims at finding a setting to execute concurrent regions efficiently. the pceo algorithm determines if a concurrent region is (i) executed sequentially, (ii) executed in parallel, and (iii) is merged with other concurrent regions. the pceo algorithm uses profile information collected at run-time to determine the setting of the concurrent regions at method-level granularity. figure 13 shows the pceo algorithm in detail."
"a population matrix with optimized individuals is obtained as output of the algorithm. the best of these individuals are selected as solution close to optimal for the objective function of the model. however, in some real applications, such as parameter estimation in dynamic models, the performance of the classical sequential de is not acceptable due to the large number of objective function evaluations needed. as a result, typical runtimes for realistic problems are in the range from hours to days. parallelism can help improving both computational time and number of iterations for convergence. in the literature, different parallel models can be found [cit], being the most popular ones the master-slave model and the island-based model. in the master-slave model the inner-loop of the algorithm is parallelized. a master processor distributes computation operations between the slave processors. therefore, the parallel algorithm has the same behavior of the sequential one. in the island-based model the population matrix is divided in subpopulations (islands) where the algorithm is executed isolated. sparse individual exchanges are performed among islands to introduce diversity into the subpopulations, preventing search from getting stuck in local optima."
"a four bar mechanism has four revolute joints that can be seen with numerous machinery applications. there is a relationship of the angular rotations of the links that is connected to the fixed link (correlation of crank angles or function generation). if there is not any connection to the fixed link which is called the coupler link. this position of the coupler link can be used as the output of the four bar mechanism. the link length dimensions determine the motion characteristics of a four bar mechanism according to the grashof's theorem. the link lengths are the function of the type of motion and are identified for a four bar chain as follows [cit] . here l is the longest link length, s is the shortest link length, p and q are the two intermediate link lengths. the input-output equation of a four bar is taken as by looking at link lengths. figure 1 . shows all possible mechanism configurations as crank rocker, double rocker and double crank."
"the variables can be optimized in case of problem without prescribed timing. structural error is the error between the mathematical function and the actual mechanism. accordingly, the first part of goal function can be expressed by minimize:"
"in online social learning environments, tagging has demonstrated its potential to facilitate search, to improve recommendations and to foster reflection and learning.studies have shown that shared understanding needs to be established in the group as a prerequisite for learning. we hypothesise that this can be fostered through tag recommendation strategies that contribute to semantic stabilization. in this study, we investigate the application of two tag recommenders that are inspired by models of human memory: (i) the base-level learning equation bll and (ii) minerva. bll models the frequency and recency of tag use while minerva is based on frequency of tag use and semantic context. we test the impact of both tag recommenders on semantic stabilization in an online study with 56 students completing a group-based inquiry learning project in school. we find that displaying tags from other group members contributes significantly to semantic stabilization in the group, as compared to a strategy where tags from the students' individual vocabularies are used. testing for the accuracy of the different recommenders revealed that algorithms using frequency counts such as bll performed better when individual tags were recommended. when group tags were recommended, the minerva algorithm performed better. we conclude that tag recommenders, exposing learners to each other's tag choices by simulating search processes on learners' semantic memory structures, show potential to support semantic stabilization and thus, inquiry-based learning in groups."
"assume there are two concurrent regions c 1 and c 2 and that c 1 as well as c 2 correspond to a single concurrent method in the source code. i.e., c 1 and c 2 are not merged with other concurrent regions. furthermore, assume that c 1 is defined in basic block (bb 1 ) and c 2 is defined in bb 2 . c 1 and c 2 can be merged if and only if:"
"in order to evaluate the efficiency of the spark-based parallel implementation of the island de algorithm (sipde), different experiments have been carried out. its behavior, in terms of convergence and total execution time, has been compared with the sequential implementation (seqde). for the experimental testbed spark was deployed with default settings in the aws public cloud using virtual clusters formed by 2, 4, 8 and 16 nodes communicated by the aws standard network (ethernet 1gb). for the nodes the m3.medium instance (1 vcpu, 3.75gb ram, 4gb ssd) was used. each experiment was executed a number of 10 independent runs on every virtual cluster, and the average and standard deviation of the execution time are reported in this section. it must be noted that, since spark runs on the java virtual machine (jvm), usual precautions (i.e. warm-up phase, effect of garbage collection) has been taken into account to avoid distortions on the measures."
"jmm semantics of cstart and cend : concurrent regions begin with the cstart cir-node. cstart is an interthread action (an action that can be observed by another thread), since the instructions of a concurrent region can be executed in parallel with instructions that follow the concurrent region in program order. as such, cstart semantically starts a new thread that executes the concurrent region. since starting a thread is an inter-thread action in the jmm, the cstart cir-node is an inter-thread action as well. starting a new thread has release semantics in the jmm. release semantics imply that all inter-thread actions that happenbefore cstart must be visible to and ordered before all inter-thread actions after cstart. to provide release semantics, the local view of memory (e.g., field values that are cached in registers) of the parent thread is transferred to the child threads. cstart has acquire semantics for the child threads, because child threads must update their local view of memory with global memory."
"to the best of our knowledge, there is no previous work that explores the use of spark for evolutionary computation. also, previous works using mapreduce have rarely evaluated their proposals in a real testbed on a public cloud."
"the main issue found was the implementation of the mutation strategy because the population is partitioned and distributed among workers. for the mutation of each individual, random different individuals have to be selected from the whole population. how to access to individuals of other partitions from a given worker, having the constraint that only the driver has access to the complete population, was the main difficulty to be tackled. three different variants have been considered for solving this problem:"
the figures for the vertical view (predefined effort) show that the proposed sipde method accelerates the computation of seqde by performing the same number of evaluations in parallel. the figures for the horizontal view (quality solution) also demonstrate that the proposed sipde method reduces the computation time needed to achieve the vtr of the seqde by improving the convergence of the algorithm.
"all spreadsheet programs are arranged cells as rows and columns; this depends on the requirement given by the user. here the optimization results are taken and drawn on a spread sheet, freudenstein's equations are utilized for the synthesis. initial crank angles are changed successively; different solutions are found and drawn with the mechanism. it is possible to draw coupler curves and its coordinates with velocity and acceleration as well. then they can be seen on the screen in animated sense. some study is needed to draw mechanism in excel. a previously prepared four bar mechanism code has been applied [cit] . fig. 4 shows the four bar mechanism. it is possible to get complete behavior of the mechanism by changing input angle. referring to figure 2, the inputs are given as r1, r2, r3, r4, rcx, rcy and θ2 found from optimization. the mechanism is drawn next. if required, complete kinematic analysis can be seen as positions, velocities and accelerations for each point separately as well."
"if n is concurrency invariant to c t, n can either be moved into c t by reordering n with cstart (if n is defined before (in program order) c t ), or by reordering n with cend (if n is defined after (in program order) c t ). an ir-node n is concurrency invariant if and only if:"
"in this paper diversity reception has been proposed as a means of improving the performance of a low complexity detection scheme for mcik-ofdm. a novel closed-form expression for the average pep of greedy detection with mrc diversity reception is derived. this expression is used to analyze the performance of mcik-ofdm with diversity reception over rayleigh fading channels. furthermore, a closed-form expression based on the approximate pep has been derived. the obtained expression reduces the computational complexity by achieving accuracy levels of less than 1 db. our results indicate that the greedy detection scheme can achieve similar performance to the ml detector with no additional computational complexity. future work will focus on applying different diversity combining techniques to the gd, and extending the analysis to account for ser."
"data defines whether the algorithm was calculated on a user's personal word trace p, an inquiry groups collective word traces c or a mixed approach pc considering both type of data. nt depicts the number of tagged resources, we derived from the online evaluation. the metrics recall, precision and f-measure are mean values and standard deviations of r@5, p @5 and f @5, respectively."
the two last steps are arranged into a loop that is executed until the termination criterion is met. after that the final selection of the best individual is also executed as a spark reduce action (a distributed min operation).
for implementing the migration strategy a spark feature known as partitioner has been used. in spark the partitioner is responsible for assigning keyvalue pair rdd elements to partitions based on their keys. default partitioner implements a hash-based partitioning using the java hash code of the key. for this work we have implemented a custom partitioner that randomly and evenly shuffles elements among partitions. it must be noted that this partitioner leads to a migration strategy that randomly shuffles individuals among subpopulations without replacement. this partitioner proposal is intended to evaluate the migration communications overhead and not to improve the searching quality of the algorithm. adding migration strategies with that purpose in mind are left for future work.
"elgg is an open source social networking engine that is extendable via plugins and follows a mvc (model-view-controller) pattern, which makes it convenient to extend. when a user enters content (e.g., question, hypothesis, file or discussion entry) to an inquiry, this happens through an input form which includes a \"tag view\". the tag recommendation plugin is an extension of this \"tag view\" and adaptively suggests tags to users. the tag view (thus also our tag recommendation functionality) is by default included in all plug-ins that allow users to create content, for instance in discussions, file uploads or blog entries. figure 2 shows such an input form with our recommendation plug-in embodied as marked by the orange frame. recommendations are calculated in a backend web-service component, on the basis of the randomly selected recommendation strategy. following common practice in social tagging systems, we set the number of tag recommendations to five. however, due to the cold start of the user and group environment, fewer tags may be presented if fewer tags are available. learners can either select from the suggested tags by clicking on the tag or they can manually enter their own tags. tagging interface. figure 2 shows an extended version of the environment's standard input form. the tag recommendation plugin that extends the form is marked with an orange frame."
"after benchmarking the performance of the three variants, broadcasting the population showed to be by far the best option. this is not surprising because the broadcasting feature of spark is highly optimized and it is the recommended method for iterative algorithms to distribute data to workers that has to be reused by different iterations. only the size of data to be broadcasted could discourage its use, but this is not the case with de where the size of populations is small (usually in the range of 5d and 10d being d the problem dimension)."
"(ii) path generation: if a point on floating link of the mechanism has to be guided along a prescribed path, then such a problem is classified as a problem of path generation. path generation controls the points that follow any prescribed path. (iii) function generation: the function parameters (displacement, velocity, acceleration etc.) of the output and input links are to be coordinated to satisfy a prescribed functional relationship. the function generation is related with functional relationship between the displacement of the input and output links [cit] ."
"recently, various types of robots has emerged in many fields as a progress of a robot technologies. especially, the expectation of disaster robots, which can be robustly utilized in a disaster area, is increasing for preventing the second disaster in the area [cit] . it is important to extract the environmental information related to a movable area of the robot and a dangerous area such as rubble with the high possibility of collapse in order to act safely and quickly in the disaster area. in this paper, we focus on an environmental sensing technology using a 3d point cloud for extracting the efficient and effective information in the disaster area. specifically, we propose a real-time feature extraction and segmentation method using the time-sequatial 3d point cloud because segmenting the 3d point cloud enables the robot to utilize some objectives such as an environmental perception and recognition."
"the section discusses concurrency invariant inter-thread actions. inter-thread actions are normal loads and stores, volatile loads and stores, monitorenter and monitorexit bytecodes, wait() and notify(), as well as starting and joining threads."
"the existing profiling infrastructure must be extended to obtain two necessary performance characteristics of concurrent regions that enable effective merging of concurrent regions: the execution time (t exec ) and the number of invocations (i) of a concurrent region. t exec is important, because the overhead from using a separate thread to execute the concurrent region must be smaller than the performance gain due to a parallel execution to speedup the application. i is important since it represents the number of tasks that are generated at runtime. a large number of tasks provides good load balance but introduces a running-time overhead."
"the y-axes in figure 18 compare the performance of the java versions against our approach. a value of 1 on the y-axis means that our approach is equally fast compared to a java version. a value smaller than 1 means that our approach is slower than standard java; a value larger than 1 means that our approach is faster. the error bars indicate the 95% confidence interval over the 40 performance runs. to enable a fair comparison between our approach and the java versions, our approach and the all java versions use the same thread pool implementation. the performance results of our approach include the overhead from dynamic profiling and dynamic recompilations that adapt the ptg. table 4 in appendix a shows the number of generated parallel tasks for each benchmark. figure 18 (a) shows that our approach performs slightly slower than the manually optimized java version. this is not surprising, since the manually optimized version uses offline profiling to determine the ptg. the performance penalty due to dynamic recompilation is insignificant, since the number of recompilations imposed by adapting the ptg is small. on average less than 30 recompilations are needed to determine the final ptg. figure 18 (b) shows the performance of our approach compared to the java version that assigns the iterations of parallel loops to 8 parallel tasks. our approach outperforms java for crypt, series, matmult, and montecarlo, since parallelism is underspecified for a 32-core system. our approach performs slower for the sor and the lu benchmark, since our approach cannot effectively remove all overspecified parallelism. the reason is that parallel loops are manually unrolled by a factor of 8. if merging all 8 concurrent regions results in a ptg where parallelism is overspecified, our approach cannot remove more parallelism. this is, however, an implementation feature of the jikes rvm. if the jikes rvm can determine the unroll factor, e.g., an unroll factor larger than 8, more parallelism can be removed. figure 18 (c) illustrates the performance of our approach compared to the java version which splits the iteration space of parallel loops into 512 parallel tasks. our approach performs faster for the sor, lu, and the matmult benchmark, since parallelism is overspecified in for these benchmarks."
"-the driver distributes the random generation of keys to the workers, collects them, selects from the whole population the individuals corresponding to the generated random keys and distributes selected individuals to the workers that perform mutations and replacements. -the driver itself makes the random pick of individuals for each member of the population and distribute them to the workers that perform mutations and replacements. -the driver broadcasts the whole population to every worker using spark broadcast variables. this spark feature allows workers to have access to a local memory-cached read-only copy of the complete population. therefore each worker can perform mutations picking the needed random individuals from its local copy of the population."
"although students were provided with initial instructions on the tagging interface and the tagging process itself, a relatively large number of students did not tag at all, or provided tags in unusual ways. consequently, we manually pre-filtered the data sets by mainly excluding posts with tags in form of sentences or tags concatenated with special characters. students with no remaining posts were also excluded from the data sets, which led to the data samples given in table 3a ."
"operations performed among individuals of the matrix (mutation -f), with old solutions replaced (crossover -cr) only when the fitness value of the objective function is better than the current one."
"in this paper, we proposed the real-time feature extraction and segmentation method from the 3d point cloud. we first proposed the modified gng-u algorithm for learning the topological structure with color information and applying to the non-stationary data distribution efficiently. the we proposed the surface feature extraction and the region growing based segmentation method by utilizing the topological sturucture efficiently. as can be seen from the experimental result, the modified gng-u ourperformed gng and the standard gng-u for the time serise 3d point cloud. however, our segmentation method could provide rough segmentation. therefore, we will combine the rough and detail segmentation method by proposing the hierarchical approach of the modified gng-u algorithm as the future work."
"the circadian 20 migrations were performed. the efficiency results show that the overhead of the migrations barely affects on the performance when the execution time between two of them is significant (case of the circadian benchmark, where the efficiency is above 0.9), but it may greatly impact if it is small (case of bbob benchmarks, where the efficiency is below 0.8 and significatively decreases when the number of nodes grows)."
"finally, to better illustrate the improvement of the proposed sipde method versus the seqde method, figure 6(b) shows, for the circadian benchmark, the convergence curves for different number of nodes. as expected, convergence time is considerably reduced by sipde with respect to seqde. it must be noted that these results could be further improved using more skilled mutation and migra- tion strategies and adding enhancements, like local search [cit], which have not been considered in this work."
"concurrent regions are profiled if they are executed sequentially, in parallel, and if they are merged with another concurrent regions. i.e., if two concurrent regions are merged, the execution time of the two individual concurrent regions is profiled. such a fine-grained performance profile allows the jit compiler to adapt the merging configuration based on individual changes of the execution time of a particular concurrent region."
"the running time overhead of execution time profiling is determined by using a synthetic benchmark. the synthetic benchmark consists of a loop that contains one concurrent region that in turn contains one method call. the loop body is executed 10 7 times to get stable execution times. the baseline for the profiling overhead evaluation is the execution time of an unprofiled version of the synthetic benchmark that is compiled to sequential code. sequential code is better suited to measure the profiling overhead, since sequential code does not incur parallel overheads. the results are therefore more stable. to measure the profiling overhead of parallel threads without including the parallel overhead, the parallel version of the synthetic benchmark starts java threads that execute the same concurrent region that is compiled to sequential code. to ensure that all threads execute the main benchmark loop at the same time the synthetic benchmark has a barrier before the main benchmark loop."
"to implement a master-slave parallel version of the de algorithm using spark, some previous insight into the way data is distributed and processed by spark is needed. spark uses the rdd abstraction to represent fault-tolerant distributed data. rdds are inmutable sets of records that optionally can be in the form of key-value pairs. spark driver (the master in spark terminology) partitions rdds and distributes the partitions to workers (the slaves in spark terminology), which persist and transform them and return results to the driver. there is no communication among workers. shuffle operations (i.e. join, groupby) that need data movement among workers through the network are expensive and should be avoided."
"while students work on their inquiry projects, they engage in activities that typically create content by, e.g., posting questions, starting or contributing to discussions or by uploading documents and pictures. these and other learning activities are tracked, saved and fed into different user profiles, to be later used in learning analytic diagrams, to issue badges and to provide personalized recommendations of learning resources and tags. technical insights. the core of the wespot environment is an online platform which is based on elgg 2 ."
"the complex number notation can be substituted next by using scalar lengths of the links as r1, r2, r3 and r4. it is given in eqn. (2)"
"to measure the number of concurrent region invocations (i), the jit compiler adds a local variable to each method that contains a concurrent region that is compiled to parallel code. the local variable is incremented in every parallel manifestation of a concurrent region. in a merged concurrent region, the counter is incremented only once. similar to the record() function in figure 12, the jit compiler adds a function call record_invocations() as the last operation before the method returns to the caller. this function reports i to the instrumenting profiler."
this section presents additional performance results. figure 20 illustrates the results of manually tuning the parallel task granularity of the java grande benchmarks. table 4 shows the comparison of the number of parallel tasks created by our approach and the number of parallel tasks created by the manual tuning as illustrated in figure 20 .
"sequential -as well as the parallel -executions of concurrent methods must result in legal program behavior. it is the responsibility of the programmer to identify code regions that have such a behavior. other explicit parallel programming languages have similar requirements. for example, the cilk programming language [cit] has the same requirements for the spawn keyword as our approach has for concurrent calls. another popular example that has similar requirements is openmp [cit] . parallel regions in openmp are executed by a team of n threads. the team consists of the master thread and n -1 child threads. the master thread is the thread that encounters the parallel region. if the openmp runtime system decides to use only a single thread to execute a parallel region that thread can be the master thread. this setup corresponds to a sequential execution of a parallel region."
"if there is no instruction between c 1 and c 2, c 1 and c 2 are contiguous concurrent regions. merging two contiguous concurrent regions is trivially possible. contiguous concurrent regions must be contained in the same basic block. otherwise, there would be an instruction (e.g., a goto) between c 1 and c 2 ."
"spark provides a language-integrated programming interface to resilient distributed datasets (rdds), a distributed memory abstraction for supporting faulttolerant and efficient in-memory computations. according to authors [cit] the performance of iterative algorithms can be improved by an order of magnitude when compared to mapreduce (using hadoop)."
"when it comes to iterative algorithms as those that are typical in areas like machine learning or evolutionary computation, mapreduce has shown serious performance bottlenecks. computations in mapreduce can be described as a directed acyclic data flow where a network of stateless mappers and reducers process data in single batches (see figure 1 ). all input, output and intermediate data is stored and accessed via the file system and map/reduce tasks are created in every single batch. having several of these single batches executed inside a loop has shown to introduce considerable performance overhead [cit] mainly because there is no way of reusing data or computation from previous iterations efficiently. although some extensions have been proposed to improve the support to iterative algorithms in mapreduce like twister [cit], imapreduce [cit], or haloop [cit], they still perform poorly on the kind of algorithms we are interested in, mainly due to those systems inability to exploit the (sparse) computational dependencies present in these tasks [cit] . new proposals, not based on mapreduce, like spark [cit] or fink (which has its roots on stratosphere [cit] ), are designed from the very beginning to provide efficient support for iterative algorithms."
in this kind of stochastic problems it is also important to evaluate the dispersion of the experimental results. figure 6 (a) illustrates how the proposed sipde method reduces the variability of the de execution time. this is an important feature that can be used to more accurately predict the boundaries in the cost of resources when using a public cloud like aws.
"standard profiling systems of modern jvms are designed to track down performance bottlenecks of sequential code. we present the design of a concurrency-aware dynamic profiler that measures the behavior of parallel code and provides the necessary information for deciding which concurrent calls are compiled to sequential code, to parallel code, and what concurrent calls are best merged to achieve good performance."
the most popular approach (mp) is a simple mechanism to rank tags according to their frequency of occurrence [cit] . the algorithm is used as a baseline.
"an overhead factor larger than 1 implies that execution time of the concurrent region is shorter than the parallel overhead. as a result, such a concurrent region is always merged with another concurrent region, or the concurrent region is serialized. the y-axis illustrates the oversubscription factor (os). recall that an oversubscription factor of 1 means that the jvm generates 1 task for each available core. the jvm computes the overhead factor for every concurrent region and the oversubscription factor for every method that contains at least one concurrent region. the main idea behind the merge function is to provide the user of the jvm with the opportunity to specify the efficiency at which parallel code is executed without having to adapt the source code. we define the efficiency of a parallel execution as the number of generated tasks in relation to the potential performance gain."
"(ii) the sequence of input angles, θ2 can be from the highest to the lowest (or the lowest to the highest). (iii) the range for the design variables should be given. (iv) the range of variation for the input angle should be given. the first three conditions are imposed and the fourth condition is taken as to perform full 360˚ rotation of the crank in the results presented here. in order to use this definition of the problem when the optimization algorithm is implemented, the constraints are retained and the values are assigned to the design variables x."
future work will be focus on developing new migration strategies and including further optimizations to improve the convergence of the spark-based island parallel de proposed.
the kinematic analysis of a four-bar mechanism is considered first. figure 2 shows four bar mechanism in general coordinate system [cit] . the design procedure of a four-bar linkage starts with the vector loop equation referring to figure 2 . the position vectors are given as
"results indicate that in the personal setting the bll approach, which mimics the activation of words in a person's memory as a function of frequency and recency, performs best. on the other hand bll applied in the collective vocabulary condition performed very poorly (see also figure 4 ). also, we can see that the recommender m inerva showed better performance in the collective, than in the personal vocabulary condition. while a model that categorizes according to semantic context should be able to depict both, personal and collective data, it is fair to assume that the size of the data set plays a crucial role. we believe the approach will become more accurate with the growing extent of the data set. hence, we draw two conclusions. firstly, m inerva performs better on collective than on personal tagging traces, as the data set is likely to be more extensive. secondly, the performance of the algorithm will enhance with the time of use. this corroborates our expectations, as we can assume that student's interests within a group differ but are individually relatively stable within the short period of a school project. the individual developments of the students within a topic can be further depicted with the introduction of recency, as implemented in bllu ."
writing fast and scalable parallel code is difficult. the reason is that the performance of parallel code depends on numerous hardware and software properties that are not always known (and obvious) to the programmer. one such important property is the parallel task granularity (ptg).
"-the random generation and initial evaluation of individuals that form the population, implemented as a spark map transformation. -the mutation strategy including random pick of individuals and replacement of old individuals with new improved ones, implemented using three different variants that are explained later. -the checking of the termination criterion, implemented as a spark reduce action (a distributed or operation)."
"the study was implemented in the collaborative online learning environment wespot 1 . wespot is a european research project, and stands for working environment with social and personal open tools for ibl. in the course of the project, a theoretical framework and corresponding tel tools for science learning and teaching have been developed. the tool set aims to support teachers in the application of ibl as a classroom activity [cit] ."
"our results demonstrate that selecting recommendations from the collective vocabulary, i.e., exposing a learner to others' tags, is much more effective to promote semantic stabilization than drawing from the personal vocabulary and thus, displaying only individual tags. furthermore, the results suggest that searching for relevant tags in the collective's vocabulary benefits strongly from considering usage frequency and semantic context, i.e., from a strategy implemented by minerva. the information of recency, on the other hand, appears to show advantages when aiming to identify relevant tags within the personal vocabulary."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. the ptg describes the tradeoff between the overhead that is associated with parallel task execution (e.g., object allocation, scheduling) and the potential performance gain. a fine ptg provides better load balance than a coarse ptg. however, executing fine-grained parallel tasks incurs more run-time overhead than executing coarse-grained parallel tasks. figure 1 illustrates this tradeoff. the x-axis in figure 1 shows the number of parallel tasks used to execute a fixed amount of work (w). for example, a value of 2 on the x-axis corresponds to an execution in which w is split into 2 equally-sized parallel tasks. the y-axis shows the speedup over a sequential execution on a 32-core system 1 . the graph in figure 1 can be divided into three regions. the left region is characterized by a poor load balance and a low overhead. the load is imbalanced, since a 32-core system requires at least 32 parallel tasks to provide all cores with work. the overhead is low, since only a small number of parallel tasks is generated. the middle region has a good load balance and incurs an overhead that has no significant impact on the execution time. the third region is characterized by a good load balance (a large number of parallel tasks) but incurs a significant overhead due to parallelism. for 100,000 parallel tasks or more, the parallel version performs slower than the sequential version. to summarize, parallelism is underspecified in the left region, wellspecified in the middle region, and overspecified in the right region. both, overspecification as well as underspecification of parallelism have a significant negative impact on the performance."
"this paper presents a novel approach for an automatic, dynamic, and feedback-directed determination of the parallel task granularity. our approach is based on two principles that are not present in the current java runtime environment. the first principle is the separation of the declaration of parallelism from its execution. this principle is provided by concurrent calls and allows the system to shift the responsibility to determine how parallel code is executed from the application/library level to the jvm level. the second principle is the concurrency-awareness of the jvm, which enables the jvm to gather profile information that is relevant to optimize the performance of parallel code."
"one practical design implication is thus that semantic stabilization within the setting of inquiry-based group learning can be supported well by recommenders that both draw on data of the whole collective and are sensitive to the semantic context of learners' search results in order to estimate tag choice probabilities. in case of an individual learning setting, however, we suggest applying recommenders that focus on information about time and frequency of past tag choices to predict their current availability in a learner's memory and hence, relevance for the current learning episode."
"manually tuning the parallel task granularity is tedious, time-consuming, and often impossible in practice. our approach represents an important step towards automating this important task."
"the runtime system extensions include extensions to the jit compiler, the profiling system, and the adaptive optimization system (aos), which identifies methods that are recompiled by the jit compiler. we extend the jit compiler by a new intermediate representation (ir) that represents concurrent calls and synchronization calls explicitly. this ir is called concurrency-aware intermediate representation (cir). the jit compiler optimizes the parallel task granularity (ptg) in cir."
"all experimental results are obtained from a system that is equipped with 32 intel xeon e7-4830 cores @2.13 ghz and 64 gb of main memory. the system runs ubuntu 11.10 (64-bit) with kernel 3.0.0. we use java version 1.6.0_24, a minimum (and maximum) heap size of 2 gb, and four parallel garbage collector threads. the jikes rvm is compiled in the fulladaptivecopyms configuration. we use this setup since other setups (e.g., the production configuration) result in an unstable behavior of the jikes rvm."
"the baseline is compared against an instrumented (profiled) version of the synthetic benchmark that is also compiled to sequential code. to measure the impact of the record interval on the profiling overhead, the synthetic benchmark is executed using different values for the record interval (0 cycles to 10 6 cycles). the reported profiling overhead is the sum of the overhead from storing a sample into the threadlocal database and the overhead of committing a thread-local sample to the global database. figure 17 . profiling overhead using 32 threads. figure 17 illustrates the profiling overhead using 32 parallel threads. the x-axis contains the record interval for different concurrent region execution times (11 ns, 25 ns, 125 ns, and 1000 ns). the y-axis illustrates the slowdown factor over the execution without profiling. higher bars indicate a larger profiling overhead."
"also, tagging in open and social learning settings can be applied as an alternative to the unpopular, since resource intensive, description of learning items through the adding of metadata that is typically done by expert users [cit] . contrary to indexing mechanisms with controlled vocabularies, tagging allows for unrestricted extension of verbalism: social tagging systems are not bound to the use of predefined language or terminology, but its classification vocabulary grows with its users' interactions. this entails advantages, such as the support of an adaptive level of granularity, but also challenges such as the lack of a coherent and useful tag vocabulary [cit] . along these lines, research [cit] indicates that students seek assistance in the tagging process, regarding two aspects: (a) the take up of the process and therefore, the finding of initial vocabulary and (b) the achievement of a semantically stable vocabulary amongst their learning peers."
"to that end, we first select the class that generated the most extensive tag data sets for both conditions (personal and collective) as representative groups. the selected data set is described in table 2 as study tg. then, for each group, we sort the posts (tag assignments) according to their timestamps, ending with the most recent item annotation. the tag growth after each post, is calculated as a value pair (tgi, f (tgi)), where tgi is the cumulative number of tags, and f (tgi) is the cumulative number of unique tags occurring in i posts."
the performance results presented in this section are gathered by executing the benchmarks 20 times in the same jvm instance. we use 10 warmup runs and 10 profile runs. the warmup runs are not included in the results to measure steady-state performance. the presented results are the arithmetic average over the 10 profile runs. the error bars indicate the standard deviation.
"distributed frameworks like mapreduce [cit] or spark [cit] provide high-level programming abstractions that simplify the development of distributed applications including implicit support for deployment, data distribution, parallel processing and run-time features like fault tolerance or load balancing. we wonder how much benefit can we expect from implementing parallel metaheuristics using these new programming models because, besides the many advantages, they also have some shortcomings. cloud-based distributed frameworks prefer availability to efficiency, being the speedup and distributed efficiency frequently lower than in traditional parallel frameworks due to the underlying multitenancy of virtualized resources."
"the cdm project cycle involves seven steps. first, the project participants prepare a pdd with a detailed description of the proposed cdm project, including estimated emission reductions, a methodology supporting their estimates and importantly, evidence of the additionality of the project. the pdd is then submitted to and reviewed by an accredited designated operational entity (doe) contracted by the project participants. these does are approved third-party auditors. after the review, the doe proceeds with the validation of the cdm project by preparing a validation report which confirms that the proposed project is a valid project. the pdd is then made publicly available by the doe on the unfccc cdm website for comments. second, the project developer secures a letter of approval from the designated national authority (dna) of the host country. the letter of approval confirms that the project meets the host country's sustainable development criteria, complies with the country's laws and regulations and fulfils any other requirement specified by the dna. third, following the host country's approval, the doe validates the pdd. fourth, after determining that the proposed project meets all relevant requirements of the cdm, the doe submits the project to cdm executive board (eb) with request for registration. the project is registered if there are no objections from member countries or at least three eb members. fifth, if the project is registered and operating, the project participant monitors actual emission reductions made by the project according to approved methodology and submits a monitoring report to the doe. sixth, the doe verifies the actual emission reductions and if satisfied prepares verification and certification reports. the doe who verifies the emission reductions cannot be the same that validates the project except for small-scale projects. the doe submits these reports to eb with request for issuance of cers. finally, eb issues cers to the project participants through the cdm registry. a significant number of projects fail to complete the project cycle [cit] ."
where p bs is the power consumed by the base station (bs). the power will be consumed while routing the packets in the adaptation layer (6lowpan). there will be slight overhead also taken in to account while calculating the power consumption. p cack is the power occupation during data collection and acquisition. data collected by these nodes consume power. since the data is collected either in one of the forms during a time slice (data will be collected during every time slice). also the nodes will be collecting data whenever there is an event triggered.
"finally, we need further methodological advances in the use of bl. we need to find ways to demonstrate that departures from bl really do indeed point to data manipulation or mishandling rather than something else. one way to do this is through comparing conformity with bl before and after some policy change influencing firms' or even governments' decision whether to misreport data. another important task will be to establish criteria for the use of bl e.g. how many orders of magnitude should be present in the data in order to expect it to conform to bl?"
"ieee 802.15.4e standard is developed specifically for the layer like media access control (mac) and physical layer. this protocol is managed by ieee 802.15.4, [cit] . it is used in the application like zigbee, mi-fi and wireless hart. as a network adaptor, it is used in 6lowpan and the rfc are defined for the upper layers."
"finally, we include the kullback-leibler (k-l) statistic. this provides a measure of the information lost when one distribution is used to approximate another. we use the k-l statistic as a means of ranking distributions in terms of their conformity with bl. where p(d) observed represents the observed distribution and p(d) benford represents the benford distribution, the k-l statistic d kl is for d1:"
"devices are believe to interact with each other in conceptualise wireless network. based on the osi model, network layers are defined. the standards are only defined for the lower layers in network the communication with the upper layer is made using logical link control. implementations may depend on other outside devices and are embedded with self-working devices. this standard is the best advisable for reliable networking and lower power networks. this is defining in the rfc 5673 which helps to improve availability, security and reliability in industry."
"a benfords set also possesses the property of scale invariance [cit] . this means that units of measurement do not influence whether data conform to bl. for example, irrespective of whether monetary amounts are expressed in dollars, sterling or euros, this should not affect whether data conform to bl. the digit frequencies corresponding to bl are the only set of frequencies possessing this property [cit] ."
"one way to validate the use of bl is by comparing data collected by different parties, one of which has no incentive to manipulate the data. data manipulation is suspected if data collected by the disinterested party conforms to bl whereas the other data does not. [cit] compares beijing municipal environmental protection bureau (bmepb) air pollution data with data recorded by an air pollution monitor run by the us embassy. whereas data from the us embassy conforms to bl throughout the entire period of investigation, conformity is poor in early years for the bmepb data. this discrepancy persists when aerosol optical density data from satellites is used instead of the us embassy data."
"we now make the simple observation that the combined lengths of the distances on the xaxis provide an approximation of the areas under the entire probability density function. this approximation, moreover, becomes better and better as the number of orders of magnitude increases. the approximation could, however, still be poor if the probability density function is not smooth."
"with data manipulation, there is usually a plausible direction of bias. along with evidence that the distribution of digits does not conform to bl, this provides another opportunity to assess whether data might have been manipulated. the distortion factor (df) [cit] to measure the bias in us income tax data. this is achieved by comparing the mean of the reported numbers after they have been 'collapsed' (so that they all fall in the range 10-99) with the mean of the numbers in a benford set. the df model rests on a number of assumptions. first, those who manipulate data do so in a way which does not alter the order of magnitude (this is deemed too suspicious). second, the percentage change caused by data manipulation is on average the same across all orders of magnitude. the expected mean (em) of the collapsed dataset is as follows:"
the cdm is an arrangement under the kyoto protocol whereby emission reduction projects in developing countries earn cers which can then be sold. the two objectives of the cdm are to help annex i parties cost-effectively meet part of their emission reduction targets under the kyoto protocol and to assist non-annex i parties in achieving sustainable development. cdm projects can be bilateral whereby an annex i country develops a project or unilateral whereby a non-annex i country develops projects and sells the cers.
"energy is a scarce resource that is being consumed by sensors of iot and hence needs to be modelled. before modelling, the energy consumption details of a sensor have to be collected. energy is consumed during transmission, reception of packets, by radio awakening and radio usage, by the system and data dissemination. energy is depleted usually when the nodes are idle, sleeping and when they are active. figure 2 shows the system model for a sensor node in iot system. active, process and txrx state consumes power to the maximum as the processor, radio, the transmitter/receiver and the sensors are active during these states. the sleep state is a low power state that consumes just a meagre power to enable the antenna alone in the on state. the idle state consumes power slightly higher than the sleep state, as the processor and the antenna are on in the idle state. table 1 shows the status and states of a given mote. the actual power consumption for these states in the micaz motes is sleep (10 mw), active (1,000 mw), process (620 mw), idle (270 mw) and txrx (420 mw). so reducing power consumption of these sensors is a must. there are several modelling approaches that have been done in these years to improve the lifetime of these motes. this work adopts the reinforcement learning technique to model the sensor motes for the iot systems. the reinforcement learning (rl) model uses q-factor that identifies the low power consumption state without compromising the performance."
"energy efficient iot framework (eeit) deploys sensor nodes that senses data, transmits to the gateway nodes and then processed. this is the useful cycle in the life of sensors. the power consumption is decided based on the repetition of this process over a period of time. the power consumption factor is decided based on the duty cycle of the transmitter/receiver. as per the model suggested from table 2, the motes are programmed to do processing for more time and whenever it is busy in sensing and transmitting later it can be switched to idle and then to sleep state. for deploying the iot application, there are three different types of nodes were used: sensor nodes (sn) or things nodes, border nodes (bn) or the relay nodes and the base station nodes (bs) as shown in figure 5 . these three types of nodes form a left-right framework that has a greater flexibility, manageability and also scalable for huge number of nodes. for better handling, the sensor nodes do not communicate themselves with the other nodes in their vicinity. instead they can communicate to the border nodes. the border nodes might send some signals to the sensor nodes that can be discarded or neglected as the size of the data is very small. the border nodes can communicate with other neighbour relay nodes and they can send the data to the base station nodes. also the base station nodes can communicate with other base station nodes which are directly connected to the internet and can upload the data to the cloud or the servers. the processing of data happens at the border node; data gathering at the sensor nodes and the base station nodes consolidates the data and upload to the servers."
"conformity with bl amounts to the claim that the areas under the probability density function of emission reductions between the values of 1.0-1.9, 10.0-19.9, 100.0-199.9 and 1000.0 [cit] .9 tons of carbon etc. sum to 0.301. now take the log 10 of these quantities to obtain log 10 (1.0)-log 10 (1.9), log 10 (10.0)-log 10 (19.9), log 10 (100.0)-log 10 (199.9), log 10 (1000.0)-log 10 (1999.9) etc. the distance on the horizontal axis corresponding to each of these 'stripes' is also 0.301. now consider the areas under the probability density function between the values of 2.0-2.9, 20.0-29.9, 200.0-299.9 [cit] .0-2999.9 tons of carbon etc. here, conformity with bl amounts to the claim that the areas under the probability density function amounts to 0.176. once more taking the log 10 of the quantities, the distance on the horizontal axis corresponding to each of these stripes is 0.176. the same process is performed where the leading digit is 3-9. the distances on the horizontal axis corresponding to each of these stripes range from 0.124 to 0.045."
"in view of the linking of the two schemes, it is interesting to compare the auditing requirements of the cdm with those of the eu emissions trading scheme (euets) where the auditing requirements seem less stringent [cit] . some researchers have also drawn attention to the critical role of the does and argued that does are affected by misaligned incentives: does might not want to be too strict if the effect is to scare away new business (drew and drew, op cit) ."
"fugitive and stack emissions, as well as off-site transfers, have all been subject to analysis using bl. critically, however, some of these are easier to conceal than others. [cit] demonstrate that data for off-site transfers conforms most closely to bl. changes in the extent of nonconformity with bl are also shown to coincide with changes in emission monitoring regulations."
"other situations where bl does not apply are those where there is a minimum (other than zero), or a maximum. critically for our purposes, however, there is no minimum or maximum value for the cdm ghg emission reductions."
"the power occupied by a mote in the given network is given below. since there are three types of nodes, each of these motes consumes power during their operation. the power consumption is occupied in three different ways. power during sensing (p sense ), power consumed during data collection and acquisition (p cack ) and power consumed by the base station nodes (p bs ). there will be power consumption by other means (p others )."
"the kuiper test provides another means of determining whether two probability distributions differ. [cit] recommends using this test rather than the k-s test to investigate whether data conform to bl. it suffers from the same problem as the k-s test in that the critical values assume the distribution is continuous. the test involves calculating the maximal extent of the deviation of f(d) observed over f(d) benford (d + ) as well as the maximal extent of the deviation of the observed cumulative distribution below the theoretical one (d − ). where n is the sample size, the kuiper test statistic v n * is as follows:"
"it has long been understood that if data underpinning environmental policy are unreliable then the policies based upon these data may be inefficient. one cause of data unreliability is deliberate manipulation by those reporting it. the threat of deliberate manipulation of ghg emission data is, however, inadequately addressed in the climate change literature. this is not because of any consensus that the scope for misreporting ghg emissions is limited and certainly not because the consequences of misreporting are benign. indeed, it is widely agreed that formulating an appropriate response to climate change is vital and the integrity of ghg emission data is the cornerstone upon which national and international climate policies are built."
"given that the digital frequency analysis of eers found in the pdds of projects in china does not correspond to bl, we repeat the analysis for cers for projects in china. now the results are quite different: although the χ 2 test for d1 is ambiguous, the χ 2 tests for d2 and d1d2 are not significant, and neither the k-s nor the kuiper tests are significant either."
"in rl, there will be five states and a total of ten state-action pairs. therefore q(x, a) denotes the q-factor of state x when there is an action a. so for a given pair, the q-factor is"
"lifetime of a network is the next fundamental reason for a wsn, as the network dies when the first node runs out of battery. an approach to solve this problem is to reduce the transmission power of the node so that the energy will be maximised and also increase the throughput of the network. also it improves the channel contention [cit] . this will increase the network disconnectivity due to the decrease in the neighbourhood nodes. another idea is to make the radio of the nodes to the sleep mode when they are idle without affecting the connectivity and the coverage of the nodes. this method increases the complexity of the system. the connectivity and the coverage issues suit only to the small scale sensor networks and it does not work well with the large scale iot networks because of the scalability issues. theoretical analysis and experimentation results show that these techniques fail when the size of the network is increasing [cit] ."
"finally, we investigate the sensitivity of tests used to detect data manipulation by randomly replacing a percentage of observations with data that we have deliberately manipulated. more specifically, data for eers for india are altered by adding 1 to each digit unless the digit is already 9 in which case it is left unaltered. this obviously inflates expected emission reductions. for example, with this heuristic, 4,486,341 becomes 5,597,452 and 64,996 becomes 75,997. we then examine how the tests for nonconformity with bl respond as the percentage of manipulated observations increases."
"before investigating the source of nonconformity with bl for the case of chinese eers, we explore whether our inability to reject the null of conformity with bl in the case of india reported are the statistics of the χ 2, k-s and kuiper tests and k-l statistic for d1, d2 and d1d2 of eers for cdm projects and those registered, including projects hosted in all countries, as well as china and india. statistics for cers of all countries and those hosted in china are also reported. '**' and '*' denote a significant difference from the benford distribution at 1% and 5% level of significance respectively happens for other countries too. we use the k-l statistic to compare the loss of information from using bl to approximate the distribution of digits observed for these countries. table 5 ). the k-l statistics are lower than for china in the case of d1 but in the case of d2, the statistic for mexico is higher. given the small number of projects, to determine whether nonconformity in chinese eers is caused by projects of a particular type, we drop major project categories, one by one. dropping hydro projects, however, fails to resolve the problem of nonconformity with bl with all tests continuing to reject the null hypothesis that the distribution of digits conforms to bl at the 1% level of confidence."
"an rpl protocol is used for lossy network and also in network having low power. directed acyclic graph (dag) is used to broadcast the message. to construct a tree etx (expected transmission count) probes dag is used and its send periodically send etx probes to the neighbours. this protocol reduces the cost required to develop each object. rpl depends on memory and processing energy, and work for networks having low data rate, high loss such as wsns. icmpv6 message format is used; this message is called as rpl control message. this protocol also permits the host server to send the packets in fragment and does not include checksum. the dhcp protocol is configure automatically, which means manual involvement is not require. ieee802.15 and ieee 802.15.4e) zigbee is protocol which work on the standard specified in ieee 802.15.4. zigbee methodology is developed for high level communication which is used for personal area network (pan) having small low power digital radio. zigbee is simpler and less costly as compared to wireless personal area network (wpan) such as wi-fi or bluetooth."
throughput and energy consumption is improved [cit] in mac 802.15.4 with the help of cross layer design that could improve energy optimisation by at least 10%.
"the full form of 6lowpan is ipv6 over low power wireless area network. instead of using zigbee, bluetooth and other protocols 6lowpan protocol has compression mechanism for header and encapsulation. this protocol is used in many platforms by using wi-fi, ethernet cable and other. 6lowpan protocol is defined in the rfc6282 standard. 6lowpam can also be used over low power rf or bluetooth the range is very big as compare to other protocols."
"the relay nodes or the border node mainly collects the data from the sensor nodes and they are responsible for data acquisition and dissemination. data collection happens in either of the two ways, during regular intervals or during an event. the data collected by the relay nodes only when there is an event is triggered or fired and other times the relay nodes do not collect data. in another case, the data collection happens at regular intervals (at every t time units)."
"the total power from equations (7), (8), (9), (10) is calculated using the cooja [cit] framework that emulates the behaviour of the wismotes and skymotes. the border nodes were mainly in the processing state as per the rl model shown in table 2 . so during regular intervals, the border nodes collect data from the sensor nodes. the sensor nodes senses as and when the data is available. because of this the sensor nodes were programmed with the web-sense module. the border nodes and base station nodes were programmed with rpl [cit] border router. parameters used:"
"where sup is the supremum function and f(d) is the cumulative distribution of d. the k-s test relies upon the maximum absolute difference between the theoretical and observed cumulative listed are summary statistics of expected and certified emission reductions from all cdm projects, as well as those hosted in china and india distributions. the null hypothesis of this test is also that the distribution of digital frequencies observed corresponds to bl. although widely used to compare discontinuous distributions, an important weakness is that the critical values assume a continuous distribution. the critical values are too high when the distribution is discontinuous."
"commencing with the full sample of eers contained in the pdd data, the correspondence with bl is poor. table 4 contains the χ 2, k-s and kuiper test results for d1, d2 and d1d2. all the tests reject at the 1% level of significance the null hypothesis that the distribution conforms to bl. with a dataset that ought and yet fails to conform to bl, it can be helpful to stratify the data in an attempt to identify the source of nonconformity. accordingly, we consider separately the eer claims contained in the pdds for china and india. the χ 2, k-s and kuiper tests for d1, d2 and d1d2 are again reported in table 4 ."
"p sense is the power consumption during the sensing of nodes. this usually includes the power occupation in the data link layer or the mac layer. the exact sensing happens through the radio, transmitter/receiver and the system power due to the switching of semi-conductor components within the nodes."
"turning to type of project, table 2 reveals that the most common project involved wind (3065 projects corresponding to 23.79%) closely followed by hydro (3020 projects corresponding to 23.44%). for wind, these percentages are even higher for china and india (32.04% and 32.43% respectively). whereas in china, hydro constitutes 34.50% of all projects, in india, the figure is only 9.78%. india has numerous biomass energy projects (659) constituting 19.60% of indian projects. by contrast, china has only 220 such projects constituting only 4.37% of all its projects. table 3 presents information on eers and cers broken down for china and india. note that in total, there are 12,675 observations on eers rather than 12,880. the reason for this discrepancy is that some projects do not include information on emission reductions. likewise, the number of observations on eers for registered projects is 8035 rather than 8038 since for three projects, this information is missing. in no case has the same project inadvertently been included multiple times (the unique project identifiers contained in the data are all different). evident from comparing the minimum and maximum values in table 3 is the fact that both eers and cers the data span 4-6 orders of magnitude. this means that if the probability density function of emission reductions is smooth, these data ought to conform to bl. we start by visually comparing the frequencies of the significant digits of the observed data with the expected (benford) frequencies. then, we use the χ 2, kolmogorov-smirnov (k-s) and kuiper statistical tests described below to test for conformity with bl the distributions of d1, d2 and d1d2. the latter combination is preferred because it captures more information; there are 90 possible digit combinations (10-99 inclusive). we delete all positive numbers that are less than 10 as they do not have a second digit. the χ 2 test provides good insight into the general fit over the entire range of the distribution. other things being equal a higher χ 2 value indicates a larger deviation of the observed frequencies from the expected benford frequencies. the χ 2 statistic is for d1 calculated as follows:"
"we now analyse the cers issued to each project. these have been subject to additional auditing. beginning once more with the full sample, the visual conformity of d1, d2 and d1d2 with bl is now excellent (see fig. 3 ). the χ 2, k-s and kuiper tests contained in table 2 confirm the distributions of d1, d2 and d1d2 possess the expected frequencies."
"this paper examines the integrity of the emission reduction claims of cdm projects by subjecting eers and cers to digital frequency analysis. we find that eers do not always reported are the statistics of the χ 2, k-s and kuiper tests and k-l statistic for d1, d2 and d1d2 of eers for cdm projects hosted in india. a percentage of these data have been contaminated by adding 1 to every digit apart from when the digit is 9. '**' and '*' denote a significant difference from the benford distribution at 1% and 5% level of significance respectively conform to bl, specifically those from china which distortion factor analysis suggests might have been inflated. our findings are therefore not inconsistent with the possibility that the eers in chinese cdm projects, and particularly those involving wind energy, have been manipulated. interestingly, however, we cannot reject the null hypothesis that the distribution of cers does conform to bl, implying that the full cdm auditing process is effective. given the prevalence of self-reporting emissions, the growing use of regulatory systems that incentivise the manipulation of emission data and the high resource costs of environmental auditing, we believe that digital frequency analysis, which is both rapid and low-cost, has an important role to play in the analysis of self-reported ghg emissions and emission reductions. bl can improve the chance of detecting data manipulation compared with random auditing. we also perceive a role for employing the same techniques to analyse countries' self-reported ghg emissions. we suggest that emission (and emission reductions) data for ghgs whether reported by firms or governments be routinely screened for nonconformity with bl whilst recognising that this is only a first step and will never supplant the need for environmental auditing."
"iots is a new concept that just evolved and claims that there will be 5 billion devices connect to this medium within the next five years. since power and energy are the two words that power the entire iot system, it is necessary and sufficient to handle these issues in this era. this work is based on the power modelling strategies of sensors in iot, the literature mainly collected from iot and large scale wsns. though there is a difference between the iot system and wsn system based on the number of parameters and metrics like 6lowpan, rpl and coap, all other underlying technologies like ieee 802.15.4 (zigbee), mac layers, sensors like skymote, telosb, micaz, etc. are common for both technologies. the gap is filled by collecting literature for the missing components like 6lowpan, ipv6 and coap [cit], weighted topology of wsn is not robust and fault tolerant but node break down is avoided in iots. this paper deals with sensor energy, transmission distance and flow of packets when dealing with the topology. [cit] address the ipv6 over 6lowpan, a protocol that is much suitable for iots in low power consumption. this combination is implemented in the layer 3 of osi layer. [cit] address the security features of constrained devices. uses the inbuilt tsch, dtls and ip security mode of ieee 802.15.43 and embedded into 6lowpan of osi layer. [cit] deploy random number of sensors in industrial iot systems to maintain good coverage and connectivity between them. the sensors are deployed in a poisson point process. [cit] implements three contributions to achieve green iot system. first, a framework for iot deployment is made; secondly, designing an organisation model to realise green iot; and finally, an algorithm that minimises the energy thereby achieving the greener iot. the energy occupation is broken down to communication, acquisition, and processing [cit] . power modelling is done on all these aspects by considering factors like lifetime, reduced cost and residual energy."
"data are sourced from the unfccc cdm website. the dataset includes detailed information on cdm projects, including project title, project type, project classification, host country, methodology, project status, type of crediting period and cers issued. there are in the dataset we use 12,880 cdm activities in total including 12,382 project activities and 498 programmes of activities. the dataset that we use was last updated on 10 [cit] . it includes projects starting the validation process from 1 [cit] to 5 [cit] . table 1 describes the data in terms of project status. project status is also presented for the two main host countries: china and india. of the 12,880 projects contained in the data, 8038 (62.40%) are registered. the next largest category refers to 2856 projects (22.17%) whose validation was terminated."
"in this paper, rl algorithm is used to compute the power optimisation models that suggest the preference of being in a state. q-factors represent the value function of the algorithms. as per figure 2, each state of the system contains two actions, one action refers the low power mode and the other action refers the high power mode. between any two states there will be two actions on either side with low power and high power mode. when the sensors need the high power mode for critical tasks, the model will suggest the high power mode, other times it suggests the low power mode. the deciding factor is the random probability between the states and the step size suggested the q-factor. the step size suggested is given below"
"however, whilst there is fairly obviously underreporting of taxable income, with a baselineand-credit system such as the cdm, the incentive is to exaggerate (although in the case of cdm projects, the dependency of registration fees on eers might point in the opposite direction). for the full sample of eer claims in pdds, the df test displayed in table 7 shows that reported numbers are indeed 2.92% above those normally found in a benford set whereas for china, they are on average 8.58% higher. both distortion factors are, moreover, significant at the 1% level of significance (table 6 ). we cannot be certain why eer claims contained in the pdds of projects in china do not conform to bl or why, according to the df test, the eer claims appear to have been inflated. our findings are, however, not inconsistent with the possibility that data for some chinese cdm projects, particularly those involving wind, might have been manipulated. [cit] . nevertheless, it is important to stress that, even if the reason eers do not conform to bl is data manipulation, attempts at data manipulation have not survived the full auditing process: data on cers conforms to bl. in addition, we cannot rule out the possibility that, rather than manipulation, data from the pdds of chinese projects might at some point have been mishandled. also possible is that statistical processes resulting in conformity with bl do not describe the processes generating eer claims contained in chinese pdds (although the ones from india and two other host countries do)."
"p e is the probability of the occurrence of the event these above power details are evaluated based on the packets sent over these nodes and their battery profile is captured before and after the packets is being sent. for easier verification, the nodes are pinged with 1 to 5 packets/sec. our methodology calculates power occupation by the cpu and the low power mode of the cpu. the low power mode works similar to our methodology defined in figure 5 ."
"the industry is moving towards the context aware computing [cit] for iot applications. industries develop and deploy iot products based on the context aware framework that helps in understanding the research trends. aggregation of data is another area where it consumes more energy and hence has to be optimised for energy efficiency. data quality and energy are inversely proportional to each other, so there should be a trade off between these two. [cit] solves this problem using an optimisation algorithm that solves energy usage issue on a per node energy constraint network."
"a notable feature of the literature is the desire to demonstrate that the data are such that bl ought to hold absent manipulation or other sorts of data mishandling. an alternative strategy is to validate bl by demonstrating that bl holds only when either the opportunity/incentive to manipulate the data is absent. interestingly, there are no published attempts to substantiate the use of bl by demonstrating that numbers with excess frequencies are associated with use of heuristic techniques. there are likewise no attempts to analyse environmental data known with certainty to have been manipulated. finally, there are no prior attempts to use bl to analyse ghg emissions; something we now seek to correct."
"routers can help the nodes to transmit packets to other nodes that ensure the data connectivity between the nodes. even more number of routers can be used to ensure the connectivity. at times, the sensors might fail due to the depletion of energy, out of transmission range and due to failure of power supply. in some cases, the connectivity is affected because of the signal attenuation due to the interference and the noise. in these cases, maintaining the connectivity is a challenge. connectivity leads to excess power consumption when the nodes move away from the neighbour nodes and also during the signal attenuation. the nodes boost extra energy in locating the neighbour node to get it connected ."
"a survey of various trends that were adopted [cit] using game theory is being used to decide between the services provided by the sensor nodes or to maximise the lifetime of the nodes. [cit] suggested to use a discrete radio model that improves the lifetime of the node. they predict the performance and packet loss calculated for a single hop and multi hop networks. [cit] address the sleep cycles of sensor nodes in deep state or light state. deep sleep states leads to higher energy and higher latency when activated to active state, whereas light sleep mode uses less energy and less latency when activated. so this paper handles adaptive sleeping mode that uses deep sleep or light sleep based on the system demand or state. life time of sensor motes decided based on the transmission power of the nodes [cit] . analytical model and simulations were carried out for verifying transmission power to improve the lifetime of sensor network. the slotted channel hoping (tsch) [cit] and a model was formed using markov chain for mobile node association. also implemented and tested the mobile tsch that improves latency in mobile association process."
"in sum, this paper provides the first statistical analysis of the integrity of emission reductions from cdm projects; the first application of bl to any baseline-and-credit system; and the first application to ghgs. more generally, and in contrast to the largely us focus of the existing bl literature, this is also the first study to use bl to examine environmental data across a range of countries."
"the ubiquity of bl is due to many common statistical processes having bl as a limiting distribution. for example, geometric sequences will generally follow bl [cit] ). [cit] considers the possible distributions of pieces of some conserved quantity. assuming each distribution is equally likely, the expected distribution turns out to conform to bl. when random variables are repeatedly multiplied, divided or raised to an integer power, the data that ensue eventually conform to bl [cit] . if distributions are randomly selected and samples are randomly drawn from these distributions, and furthermore supposing that the average distribution is scale or base-invariant (see below), the distribution of significant digits will conform to bl [cit] . lastly, any distribution will approximate bl if it spans several orders of magnitude and is smooth [cit] ."
"apart from being a pillar of global climate policy, the cdm provides an interesting application of bl for other reasons. first, there is a clear incentive for manipulating emission reductions since projects with greater emission reductions will be more attractive. second, it is possible to observe emission reductions both prior to and after significant auditing activity. specifically, we take data from the united nations framework convention on climate change (unfccc) cdm website and examine the expected emissions reduction (eer) claims contained in project design documents (pdds) and the eventual issuance of certified emission reductions (cers). third, it is possible to stratify the data enabling us to locate the source of any nonconformity. fourth, there are good reasons to suppose that, in the absence of data manipulation, cdm ghg emission reduction projects should conform to bl in view of the processes involved. our reasons for believing cdm ghg emission reductions ought to conform to bl are discussed below."
"apart from emissions and off-site transfers, bl has also been applied to pollution concentrations. [cit] analyses the d1 frequencies of 8 selected uk datasets comprising pollution concentrations. results indicate that the annual average and weekly concentrations of 12 measured heavy metals at 17 monitoring sites conform very closely to bl. the key determinant of whether data conform to bl appears to be how many orders of magnitude are present in the data, with more than four orders of magnitude resulting in a close correspondence. brown (op cit) also examines the effects of data mishandling where, for a percentage of the data, d1 is dropped such that d2 is then misinterpreted as d1, d3 as d2 and so on. conformity with bl is very sensitive to data being mistreated in this way. another form of accidental data mishandling discussed is where d1 and d2 become accidentally transposed. [cit] analyse data from the chinese air quality index (aqi) taken from 35 different sites in beijing. they use bl to test the frequency of d2. [cit], the period immediately after the blue sky days initiative had ended. blue sky days are days when the chinese aqi is below a threshold of 100 and this was once used as a performance indicator for evaluating local officials (hence the incentive to misreport). they find that although hourly data conform to bl, daily data do not."
"there are some forms of data manipulation that bl cannot detect e.g. multiplying all figures in a dataset by the same amount (a consequence of the property of scale invariance). bl cannot distinguish between accidental mishandling and deliberate manipulation. even the innocent practice of rounding-off terminal digits results in nonconformity with bl. most fundamentally there is the problem of type-i and type-ii errors when testing the null hypothesis that data conform to bl. put differently, statistical tests may point to possible data manipulation when it is absent and the absence of data manipulation when it is present. bl is accordingly best viewed only as a first step in examining data for possible data manipulation and not as a substitute for auditing."
"having explained when bl holds and why we expect it to do so, here it is important to mention situations where bl does not apply. assigned numbers such as telephone numbers do not follow bl. numbers influenced by human psychology, including manipulated numbers, likewise tend not to conform to bl. indeed, the manner in which a distribution containing manipulated numbers departs from bl reveals the emphasis placed on particular numbers. when individuals manipulate numbers they likely use heuristic techniques. [cit] contends that someone using a computer keyboard is likely to use their more dominant index and middle fingers and hence type 4, 5, 6 or 7 more often than other digits."
"the results of this admittedly simple experiment are presented in the table 7 . they indicate that as the percentage of data that is manipulated increases, the tests against the null of conformity to bl quickly begin to show statistical significance. for example, when 5% of the data are manipulated, the χ 2 test for d1 becomes statistically significant at the 1% level of confidence along with the k-s and kuiper tests."
"dropping wind projects by contrast changes things. now, whether the null hypothesis is rejected depends on the specific test and the digit(s) under consideration. for d1, although the χ 2 and kuiper tests are significant at the 1% level of confidence, the k-s test is significant only at the 5% level of confidence. none of the tests is significant even at the 5% level of confidence for d2. for d1d2, the χ 2 and kuiper tests are significant at 1% level of confidence, but the k-s test is significant only at the 5% level of confidence. more importantly, the k-l statistics are lower: 0.005 for d1 for wind compared with 0.041 for all projects."
"the remainder of the paper is organised as follows. section 2 explains bl and the statistical processes that give rise to it. section 3 reviews previous attempts to apply bl to emissions and pollution concentrations. section 4 describes the cdm project cycle and section 5 presents data on cdm ghg emission reductions. section 6 discusses the various tests used by us to assess conformity with bl. section 7 presents the results which are then further explored in section 8. finally, section 9 considers the role bl might play in testing for manipulation of ghg emission data, whether by firms or governments."
"one interesting case study might be to use bl to analyse data from carbon trading schemes. the number of facilities involved in the euets for example, is extremely large and includes many different countries across which regulation is perhaps, unevenly applied. the pressure on those entities liable to the euets to cheat is intensified by competition from unregulated entities outside the eu. furthermore, although with more than 14,000 installations, the euets carbon trading scheme is the largest, there are numerous other carbon trading schemes in operation. bl might also be a tool with which the relevant authorities could scrutinise data from the chinese emission trading scheme, which is expected to launch soon."
"since this power is consumed during a message transmission by the sensor nodes to the relay nodes, the power calculation is based on the message per transmission. the modulation effect is neglected and is not calculated for the power calculation if a message is retransmitted then,"
"the coverage area problem in wsns can be categorised in two sections: area coverage and target coverage problem, which deals with covering whole, points in given area and for moving targets respectively. target coverage is addressed in this problem in which sensors have multiple [cit] power levels that extends the lifetime of networks. a new concept called maximum network lifetime with adjustable ranges (mnlar) is designed to avoid the toughness in recharging a battery in harsh environments. mnlar overcomes the problem with less energy consumption [cit] ."
"the nodes were formed according to the figure 5 (for setting up the nodes, figure 5 setup was used exactly) and there are as many as 25 nodes were created and programmed. later the power consumption is computed based on the number of packets sent over the ipv6."
"china is the largest host country for cdm projects in terms of the number of projects and the largest supplier of cers on the cdm market. measures for the operation and management of clean development mechanism projects is the regulatory framework for cdm implementation in china. this includes detailed guidance on the eligibility, application and approval procedures for cdm projects [cit] . all hydro and wind projects as well as all new combined cycle natural gas power plants are required to be submitted through the cdm [cit] )."
"where n is the number of transmissions, the sensing power shown in equation (8) is mainly depends on the radio and the messages generated by the radio. the message generation usually follows a probability distribution like poisson or random process. also the expected time between the messages is also averaged. the terms p trans and p recv will work only during the radio is sending the messages to the next nodes. so these two terms are sporadic in nature (they consume power only when the radio is sending or receiving the messages). power consumed by base station is due to the mathematical operations like (mul, add, div, sub, etc). since the base station is powered with huge energy, the energy calculation for bn and sn will be very minimal. however, the processing of data happens at both the border nodes and the base station nodes."
"wireless sensor networks (wsns) have wide applications in surveillance, defence, animal habitat monitoring, health care, etc. since all these applications need sensors and those applications depend mainly on the networking and communication aspects of sensors. internet of things (iot) is a technology that drives these sensors to capture and captivate the environmental aspects and reports this information via an iot layered architecture (shown in figure 1 ). iot system comprises of many sensor nodes that are limited in power, coverage area and size. batteries power the sensors, so replacing them or recharge them will be a tedious task [cit] . each node is equipped with components of sensing and processing data as well as for communicating to other nodes. these nodes send or relay data to the sink which are at a receiver end. because of the advancement in the technologies like mems and wireless technologies, the deployment of nodes in a large scale sensor networks gather data from these nodes and replay it to the sink. in such networks, the real challenge lies in the centralised control of managing the nodes [cit] . among them, a unique characteristic of iot is that the desired global system performance will be achieved based on local information [cit] and decisions collected from each individual node within the network."
"all of these above mentioned protocols are optimised for use in the constrained devices (motes). since this paper is addressing the power and energy as an issue, these protocols were used in the experimental evaluation of iot systems."
"because we are relying in part on the argument that multiple orders of magnitude combined with smoothness are a reason for expecting conformity with bl, it is perhaps worth providing a more in depth explanation of the point."
"we expect cdm ghg emission reduction project data to conform to bl. first, the data comprises random samples from random distributions, each one relating to a different sort of project. as argued by hill (op cit), this is sufficient to generate a distribution that conforms to bl. second, the distribution of emission reductions spans many orders of magnitude (see below) and there is no reason why the distribution should not be smooth. finally, emission reduction claims are the consequence of mathematical operations e.g. emissions avoided multiplied by the number of wind turbines (although multiple mathematical operations are generally required before a distribution conforms closely to bl)."
"reinforcement learning is a simulation-based learning that solves markov decision problems. the preference of being in a state of a finite state machine (fsm) is decided by the models of dynamic programming (dp), heuristics and rl. dp and rl give a best solution when compared with the heuristic which provides a low quality solution. but rl is having upper hand over dp in the modelling effort. with less effort, rl gives the best solution since dp uses more iteration and rl uses less."
"this section will show the ietf suggested protocol stack for iot. for a constrained network, the following protocols from the physical layer to application layer are optimised for energy and power as shown in figure 1"
"to anticipate our main findings, there is evidence that eers do not always conform to bl. such a finding is not inconsistent with the suggestion that there is data manipulation at this point in the cdm project cycle. it is notable, however, how well bl often describes cdm ghg emission reductions. this finding throws into sharp relief those instances where it does not and supports in our view, the application of bl to other ghg emission datasets. most importantly, however, we cannot reject null hypothesis that the distribution of cers conforms to bl."
"based on these results, we investigate whether the distribution of digits for eers for wind projects in china conforms to bl. here all the tests reject the null of conformity at the 1% level of significance. by contrast, for eers for wind projects in india, none of the tests rejects the null of conformity to bl, even at the 5% level of confidence. the k-l statistics too are much higher for china. analysing the distribution of digits for cers for 547 wind projects in china, we are unable to reject the null of conformity with bl using any test and moreover, the k-l statistics point to only a minor loss of information from using bl to approximate the distribution. by contrast, the distribution of digits for eers for those exact same 547 wind projects that went on to earn cers continues to display nonconformity with bl. all the tests are significant at the 1% level of confidence and the k-l tests indicate a significant loss of information from using bl as an approximation. even if eers are measured in terms of annual emission reductions and cers are in terms of cumulative emission reductions, it is hard to reconcile these findings because of the scale invariance property of benford sets."
"in this paper, we have modelled the sensor motes using rl algorithm that computes the q-factor and the rewards for state transition. these factors decide the preference of being in a state and the network is modelled according to the outcome of the rl model. also we have investigated the power consumption of various system components of iot and model a hierarchical eeit framework that minimises the cost of deployment, flexibility and scalability all these with the less power consumption. our work focuses mainly on the routing layer, adaptation layer and the network layer. the power profile can be adjusted also by the mac and the physical layer along with the upper layers. our work plays a great role in matching the predictions and the experimentation that validates our model in real iot applications. to enhance this work, superframing [cit] can be introduced to minimise more energy consumption using the slotted mac. these models will be helpful in deploying greener iot systems for the industrial applications and services."
"and n and m are the integer channel gains corresponding to the channel input and interference, respectively. the vector interference s[i] is assumed to be non-causally known at the transmitter as side information. the channel gains n and m are fixed during communication and are assumed to be known at both transmitter and receiver. following the result of gel'fand and pinsker [cit], the capacity of the linear deterministic dirty-paper channel (3) is given by"
"the rest of the paper is organized as follows. in sec. ii, we first take a deterministic view at costa's dirty-paper channel and provide an approximate characterization of the channel capacity to within half a bit. note that even though a precise characterization of costa's dirty-paper channel is well known [cit], the proposed approximate characterization establishes a framework for studying side-information problems via the deterministic approach. building on the framework of sec. ii, in sec. iii we extend the deterministic approach to the problem of secret writing on dirty paper and provide an approximate characterization of the secrecy capacity to within half a bit. a different but closely related communication scenario known as secret-key agreement via dirty-paper coding is discussed in sec. iv. finally, in sec. v we conclude the paper with some remarks."
"automated analysis of large-scale multidimensional image data has become an integral part of current biological research. particularly, 3d imaging techniques like confocal, light-sheet or electron microscopy easily produce terabytes of image data that cannot be assessed manually [cit] . although many comprehensive and user-friendly graphical user interfaces (guis) for the automation of image analysis problems have been presented in the past, their applicability to terabyte-scale data remains limited [cit] . on the other hand, tools that are usable for large-scale analyses are mostly single-purpose command-line tools with a substantial lack of flexibility. the insight toolkit (itk), for instance, often serves as a basis for such specialized highperformance implementations, as it offers plenty of features for multidimensional image analysis and has an active community in the biomedical field that constantly improves and extents the functionality of itk [cit] . however, many available solutions based on itk require advanced programming skills, are limited to a certain task due to specialized cþþ pipelines or suffer from slow execution times, e.g. caused by multiple i/o operations for separate filter execution or inefficient implementations. to overcome these limitations, we developed xpiwit, an xml-based wrapper application for itk that allows a graphical setup and rapid prototyping of image analysis pipelines while preserving the performance of a pure cþþ implementation ( supplementary fig. s1 ). created xml pipelines can directly be used to interface xpiwit on large clusters, and the current version of xpiwit already incorporates about 70 different filters, including i/o filters, preprocessing filters, edge detectors, morphological operators and segmentation filters (supplementary table s1 ). we also integrated our recently published algorithm for the efficient segmentation of fluorescently labeled cellular nuclei as an exemplary pipeline [cit] . to ensure"
"in this paper, a switched pid controller-based fast setpoint control method has been developed for the nanopositioning system. e overshoot constraint of the fast setpoint process in the switched closed-loop system is investigated. e corresponding switched controllers can be determined based on solving the properly formulated synthesis algorithm with an optimized performance under the overshoot constraints. e performance of the switched pid controller is experimentally evaluated using a closed-loop nanopositioning system driven by a pzt actuator. e experimental results demonstrated the e ectiveness of the switched pid control method to achieve a fast setpoint operation with a limited overshoot."
"costa [cit] was the first to study this communication scenario, which he whimsically coined as \"writing on dirty paper.\" based on an earlier result of gel'fand and pinsker [cit], costa [cit] proved the surprising result that the capacity of writing on dirty paper is the same as that of writing on clean paper without interference. since [cit], dirty-paper coding has found a wide range of applications in digital watermarking and network communications, particularly involving broadcast scenarios."
"where x is an i.i.d. bernoulli-1/2 random vector and independent of s. based on the equivalence relationship (5) between the gaussian and the linear deterministic model and the success of sec. ii for costa's dirty-paper channel, the optimal choice (19) of auxiliary variable u for the linear deterministic model (11) suggests the following choice of auxiliary variable u for the gaussian model (18):"
"in this paper, we took a deterministic view and revisited the problem of wiretap channel with side information. a precise characterization of the secrecy capacity was obtained for a linear deterministic model, which naturally suggests a coding scheme which we showed to achieve the secrecy capacity of the degraded gaussian model (dubbed as \"secret writing on dirty paper\") to within half a bit. this paper falls in the line of using the linear deterministic model to provide approximate characterization of gaussian network capacity, an approach which has become increasingly popular in information theory literature. however, our method is somewhat different from most of the practices along this line of research. in literature, a common practice has been to first gain \"insight\" from the capacity-achieving scheme for the linear deterministic model and then translate the success to the gaussian model at the scheme level. to the best of our understanding, such translations are more art than science. for the problems that we considered in this paper, the translation of success from the linear deterministic model to the gaussian model was done at the level of a single-letter description of channel capacity and hence was much more systematic. our ongoing work aims at understanding to what extent this method can be applied to more complex network communication scenarios."
"at present, various nanopositioning methods have been considered to meet the requirement of high precision and reliability in the control system [cit] . however, for conventional lead-lag controllers, such as proportional-integral controllers [cit], although they are simple in structure and convenient in design, they are prone to have large overshoots and are difficult to meet the high-speed requirement [cit] . in order to achieve high positioning speed, the h ∞ controller can be designed to speed up the response of the system and improve the robustness of the system by establishing an auxiliary objective function based on the system's response speed [cit] . chen and francis convert the rapid solution of positioning control into a closed-loop system h 2 optimization problem by introducing a virtual integrator [cit] and then using the classical h 2 optimal controller to meet the requirements of high speed in the positioning control system. however, these high-speed positioning methods may bring out large overshoot for the closed-loop system and cause a collision with the samples in the nanopositioning systems."
"to apply a predefined xml processing pipeline to a desired set of images, xpiwit has to be executed from the command prompt with command line input arguments. alternatively, a configuration text file can be piped to the executable (supplementary listing s2). this configuration file needs to contain the output path, one or more input paths, the path to the xml file to be processed and may be customized using further optional parameters as described in the online documentation. this concept allows to heavily parallelize the data processing, e.g. using distributed computing environments such as apache hadoop, where one xpiwit configuration file is generated per image and all jobs are distributed among available processing nodes. in addition to the image output, metadata generated by one of xpiwit's processing operators is saved. the used csv format (comma separated value) is able to store large tables in a text file where each column is separated by a specific character (e.g. ',' or ';') and each row by a line break. many powerful data analysis tools (e.g. excel, matlab, gait-cad, spss, knime and rapidminer) provide functionality to import such csv files and therefore allow to explore the generated data in detail. all data formats used by xpiwit are either simple text files or common data formats. this allows third party software to easily automate the creation of input files, to interpret the generated data and to embed xpiwit into existing image processing pipelines."
"next, let us use the result of theorem 1 to determine the secrecy capacity of a linear deterministic wiretap channel with side information. in this model, the received signals (at time index i) at the legitimate receiver and the eavesdropper are given by"
combining the above two cases proves that the lower bound in (27) is always within half a bit of the upper bound. this completes the proof of the theorem.
where the maximization is over all possible binary random vector z. the maximum is achieved when z is an i.i.d. bernoulli-1/2 random vector. now let
"in information theory, an interesting and useful communication model is a state-dependent channel where the channel states are non-causally known at the transmitter as side information. of particular importance is a discrete-time channel with real input and additive white gaussian noise and interference, where the interference is non-causally known at the transmitter as side information."
"based on the closed-loop system represented with the switched pid controllers as in (26), the following lmis conditions can be used to design the controller parameters in c 2 c with respect to h 2 performance. theorem 2. consider the closed-loop system (26) where the matrix b p is full-rank matrix. if there exist a positive definite"
"consider the nominal closed-loop system subject to an input r. an additional controller design objective is considered where it is desired to find a pid controller that minimizes the l 2 norm of e, that is, ‖e‖ 2, by considering only the nominal system in the closed-loop system. since the sensitivity function s 0 relates the input r to the error e, the solution of this design constraint can be obtained by considering a standard h 2 optimal control problem where it is desired to minimize the h 2 norm of the system z/(z − 1)s 0 . in order to avoid the unstable pole introduced by z/(z − 1), the system z/(z − 1)s 0 can be approximated by the system z/(z − 1 + α 0 )s 0, where α 0 is a small positive constraint. erefore, the state-space equation of the integral part introduced is z :"
"based on the proposed synthesis algorithm, the following steps are used to solve the parameters of the smooth mode controller. with the controller parameters k p1 c 0.006097, k i1 c 0.012194, and k d1 c 0.005874 and the preset constants δ 0.1, r 1 0.7, r 2 0.8, α 0.01, and β 0.1, the optimal feasible solution is obtained by iterative computation with a switching surface of n 0 0.513, which means that σ s1 of the approach control mode will switch to σ s2 of the smooth control mode when the distance between the optic lens and the desired position drops to 513 nm. e corresponding controller parameters of the smooth mode controller are k p2 c 0.0038121, k i2 c 0.0076242624, and k d2 c − 0.0060265. namely, the smooth control mode controller is represented as"
"based on the obtained controller, the positioning response of the h 2 optimized performance controller is evaluated in the nanopositioning system, as shown in figure 4 . it can be seen from figure 4 that the h 2 optimized controller tracks the step response quickly, and the adjust time is around 0.05 s, and it takes 0.2 s to be fully stabilized. e rapid response capability of the closed-loop system is obtained, but the overshoot is large, which is not allowed for the nanopositioning system."
"where x is standard gaussian and independent of s. compared with the optimal choice (2), the choice (6) of auxiliary variable u is suboptimal. however, for this suboptimal choice of auxiliary-input variable pair (u, x),"
where r represents the initial distance between the near-field optic lens and the desired position and e indicates the actual distance between the near-field optic lens and the desired position.
"xpiwit is implemented in cþþ using itk (http://www.itk.org) and the qt sdk (http://www.qt-project.org). platform-independent project files have been realized using the cmake build tool (http:// www.cmake.org). both xpiwit and the associated gui have been successfully compiled and tested under windows, linux and mac os x. the software is licensed under apache 2.0, and we host a public repository that encompasses source code, example pipelines and a detailed documentation on https://bitbucket.org/jstegmaier/ xpiwit/downloads/. to encourage non-programmers to use the software for their image analysis tasks, precompiled binaries for the three major operating systems are hosted in the repository as well."
"e simulation and experimental results of the closedloop system with the single approach mode pid controller are shown in figure 5, which shows a large overshoot of more than 10%."
"finally, let us consider the gaussian wiretap channel where the received signals (at time index i) at the legitimate receiver and the eavesdropper are given by"
"here, x[i] is the channel input which is subject to a unit average power constraint, n [i] and s[i] are independent standard gaussian noise and interference and are independently identically distributed (i.i.d.) across the time index i, and h and g are the (real) channel coefficients corresponding to the channel input and interference, respectively. the interference s[i] is assumed to be non-causally known at the transmitter as side information. the channel coefficients h and g are fixed during communication and are assumed to be known at both transmitter and receiver."
"the fact that the choice (6) of auxiliary variable u leads to an achievable rate which is always within half a bit of the dirtypaper channel capacity is well known (see [cit] for example). however, it is interesting to see that such a choice comes up naturally in the context of the deterministic approach."
"where x is standard gaussian and independent of s, as long as (40) is satisfied. substituting (42), (21)-(23), and the degradedness assumption (25) into (39) and (40), we have the following lower and upper bounds on the secret-key capacity c k of the degraded gaussian model (26):"
"note that when the channel state s is deterministic, a semideterministic wiretap channel with side information reduces to a regular semi-deterministic wiretap channel without side information. in this case, let s be a constant in (10) and we have"
"in this section, the problem of the switched pid controller design with h 2 performance for σ s2 on the closed-loop system is investigated. since the switching only happens once, for the optimization of h 2 performance, we only consider σ s2 after switching. in the following, the convergence rate of the closed-loop system with respect to the static inputs is performed by minimizing h 2 performance specification."
"in order to improve the positioning speed with a constrained overshoot in nanopositioning systems, in this paper, a switched pid controller-based fast setpoint approaching method is proposed to ensure high positioning speed within a limited overshoot. e controller design constraints are represented using a set of properly formulated bilinear matrix inequalities (bmis). en, a switched controller synthesis algorithm is proposed based on h 2 -optimized performance under the overshoot constraints. e performance of the switched controllers is experimentally evaluated in a near-field optics positioning system driven by a pzt actuator, and the experimental results are presented to illustrate the effectiveness of the proposed fast setpoint control method."
"a general concept of automated image processing, which is also inherent to itk, is the arrangement of processing operators in a feedforward pipeline structure. we specifically developed an xmlbased pipeline format that allows to create flexible image analysis pipelines using a variety of different processing filters (supplementary listing s1). each of the specified filters has a set of inputs that either point to files on disk or the output data of a preceding filter. in addition to image inputs and outputs, xpiwit also has an internal metadata handling system that can be used to share meta-information such as extracted object locations or image statistics between processing operators. the optimal execution order of the individual processing operators is internally determined by xpiwit and ensures that the requested image and meta-inputs are available as soon as a processing operator is executed and that memory is released as soon as the data is not needed anymore. the xml structure allows to adjust all available parameters of each filter using key-value pairs. the xml pipelines are decoupled from the i/o parameters, which facilitates sharing customized pipelines with other users and additionally allows to use a single xml pipeline to process multiple images."
"consider the linear deterministic model [cit] for costa's dirtypaper channel (1), where the received signal y [i] at time index i is given by"
"given this equivalence relationship, the optimal choice (4) of auxiliary variable u for the linear deterministic model (3) naturally suggests the following choice of auxiliary variable u for the gaussian model (1):"
"e results of the closed-loop system with the switched pid controller are presented in figures 6 and 7 . e control signal of the switched pid controller, which provided the input voltage to the pzt actuator, is shown in figure 6, and the corresponding output of the closed-loop system is shown in figure 7 . it can be seen that the whole setpoint process in the nanopositioning system has been divided into two stages by the switching point at 513 nm from the sample to the optic lens. compared with the previous h 2 optimized controller, the switched control method not only restricts the large overshoot of the closedloop system to avoid the collision but also possesses a high convergence speed. ey have the similar fast-response ability, but the switched controller possesses the main advantage to limit the overshoot. e experimental results indicate that the proposed switched pid controller can be used in the nanopositioning system for a fast approaching operation."
"since the overshoot constraint formulated in eorem 1 can be applied to the general closed-loop systems with different controller structures, the idea of the switched controller design approach can also be extended to other kind of controllers, for example, the dynamical output feedback controller. however, the controller synthesis algorithm needs to be reformulated according to different controller structures."
"in this section, the fast anticontact approaching method based on the switched pid controller is experimentally tested in a near-field optics positioning system driven by the pztactuator. experimental results are presented to show the performance of the resulting closed-loop system with the designed switched pid controller. e schematic diagram of the experimental setup of the near-field optics positioning system is shown in figure 1, where the optic lens needs to quickly move from the initial position to the point with a gap of 100 nm above the sample. e picture of the corresponding nanopositioning experimental system is represented in figure 2 . a multilayer pzt actuator (model pl 112.11, physik instrumente) is attached to the suspension beam and serves to adjust the position of the beam tip. e pztactuator is driven by a power amplifier (e-650 lvpzt amplifier, physik instrumente). e real-time measurement of the tip position is performed using a laser doppler interferometer (polytec ofv-072, ofv-552, and ofv-5000). a pci 6221 input-output card from national instruments and a personal computer are used to implement the controller and to interface it with the rest of the system. e control algorithm is implemented using the real-time module and simulation interface toolkit with matlab simulink."
it shows that the proposed switched pid controller can control the nanopositioning system to restrict the overshoot and realize the fast setpoint approaching to the desired position when eorem 1 is satisfied.
"in the following, based on the obtained model of the nanopositioning system, the initial distance between the sample and the near-field optic lens is set as 700 um. e ratio of the voltage and displacement of the interferometer is set as 0.01 v/um; therefore, the given reference signal r is 7 v. e control of nanopositioning system needs to consider the overshoot to avoid the collision, while ensuring high positioning speed."
"the converse part of the theorem follows from the upper bound (9) and the fact that y 1 is a deterministic function of (x, s), so we have"
"in this section, the problem of the switched pid controller design with a restrict constraint for the overshoot on the closed-loop system is investigated. based on the closed-loop system represented with the switched pid controllers as in (5), the following conditions can be used to design the controller parameters with respect to the overshoot constraint. theorem 1. consider the switched closed-loop system (5) where the matrix b p is full-rank matrix with initial state"
"for semi-deterministic channels where the channel output at the legitimate receiver is a deterministic (bivariate) function of the channel input and state, the lower (8) and the upper (9) bounds coincide, leading to a precise characterization of the secrecy capacity. the result is summarized in the following theorem."
"give the same value for az h . thus, the number of different values that az can take for any given value of b equals the number of cosets in the null space of b, which is given by ."
"i.e., the received signal y 2 [i] at the eavesdropper is degraded with respect to the the received signal y 1 [i] at the legitimate receiver. following [cit], an interesting interpretation of the degraded gaussian model (26) is \"secret writing on dirty paper.\" in this scenario, a user intends to convey (to a legitimate receiver) a confidential message on a piece of paper with preexisting dirt on it. the legitimate receive has access to the original paper with the message written on it and hence can decode the intended message. on the other hand, the eavesdropper can only access a noisy copy of the original paper, from which essentially no information on the conveyed message can be inferred."
"the matrix b is a horizontal stack of two down-shift matrices with rank n 2 and m 2, respectively. since both submatrices are in reduced row-echelon form, it suffices to count the number of nonzero rows of b to find its rank: combining the results from the above five cases completes the proof of (12) and hence theorem 2."
"we developed an easy-to-use gui that allows to create and modify xml pipelines based on the filters compiled into the xpiwit executable. the gui is entirely decoupled from the command-line tool and offers a more convenient environment for rapid prototyping and parameter optimization than directly adjusting the xml text files. in figure 1, the basic layout of the gui is depicted. filters can be placed via drag and drop from the filter list (1) to the pipeline drawing area (2). all parameters are adjustable by selecting the respective filter in the drawing area and by setting the parameters in the parameter customization panel (3). once the input and output parameters (4) are defined, the pipelines can be used to process the specified data and may be saved and re-opened using the menu bar (5)."
"in this section, the switched controller synthesis on the closed-loop system is investigated, and the corresponding synthesis algorithm based on the matrix inequalities in sections 2.2 and 2.3 is summarized as follows."
"nanomanipulation has been drawing continuous attention recently [cit], among which nanopositioning technology as one of the key technologies is increasingly used for a wide range of applications, such as near-field optics system [cit], scanning probe microscope [cit], and micro-nano operation [cit] . for example, in the near-field optics system, nanopositioning technology is used to operate the optic head toward the sample surface quickly and thus control the interaction between the lens and the samples precisely [cit] . however, for achieving near-field operation successfully, not only the accuracy and positioning speed of the nanopositioning system are considered but also the overshoot in the positioning process should be balanced [cit] . generally, the response speed of the nanopositioning system should be fast enough to improve the efficiency. however, when the response speed of the closed-loop system is so fast, the output of the system is prone to have large overshoot, which leads to the collision between the nano-operated lens/ tip and the sample, thus resulting in their damage."
"xpiwit is a new command-line tool based on itk that combines the performance of a native cþþ application with the ability to create filter pipelines at runtime using a specialized xml format. the functionality is completed with an interactive gui that simplifies rapid prototyping, parameter optimization and the generation of xml pipelines. xpiwit is explicitly not meant to replace any of the general purpose guis that exist for biomedical imaging but aims at filling the niche of an easy-to-use and flexible tool that is highly efficient, lightweight and platform independent. both xpiwit and the gui open up a wide area of possible biomedical applications fig. 1 . exemplary screenshot of the xpiwit gui on windows. the main control elements of the gui are the filter list (1), the pipeline drawing area (2), the parameter adjustment panel (3), the input/output definition panel used for pipeline test runs (4) and controls for saving, loading and executing pipelines (5). generated pipelines are independent from the gui, e.g. to perform largescale analyses on computing clusters ranging from automated analyses of 2d image data on single workstations to large-scale analyses of multidimensional image data on high performance computing clusters. we successfully applied the presented software solution to segment and visualize terabytescale 3dþt light-sheet microscopy images of developing embryos. further work will be put on a shared library-based plugin system to simplify adding new filters to xpiwit and to combine a stepby-step execution with result previewing for interactive parameter optimization. moreover, we will try to identify possibilities to integrate xpiwit with widespread tools like fiji, icy or knime [cit] ."
"in order to deal with the overshoot shortcoming of a single h 2 controller, in the following, the switched pid controllers, which include a fast approaching mode and a smooth transition mode with a switching surface to achieve a fast setpoint approaching, are designed and evaluated. first, the approach mode pid controller with a rise time of 0.1 s is designed to aim at a fast approach positioning to the sample and is represented as"
"the channel coefficients h 1, h 2, g 1 and g 2 are fixed during communication and are assumed to be known at all terminals. a single-letter expression for an achievable secrecy rate was given in (8), which involves an auxiliary variable u . however, it is not clear what would be a reasonable choice of u, letting alone finding an optimal one that maximizes the achievable secrecy rate expression (8) . on the other hand, for the linear deterministic model (11), it is clear from theorem 1 and proposition 2 that the following choice of auxiliary variable u is optimal:"
"to prove the reverse inequality, let us consider the null space of b and its coset partition based on the null space of a b ."
"since the system (37) is a nonminimum phase system, it is normally difficult to find a single pid controller that can achieve a fast speed with limited overshoot. in the following, the traditional pid controller is first evaluated with the matlab pid tuner toolbox under the overshoot constraint. e obtained fastest response of the closed-loop system with the satisfied overshoot limit is shown in figure 3, where the system takes more than 0.4 seconds to fully settle down. by introducing a virtual integral in the system, the h 2 controller, as a kind of optimal controller, can be designed to maximize the system track performance [cit] . erefore, the design of the h 2 optimal controller is also evaluated to control the motion of the optic lens. based on the matlab robust control toolbox, the h 2 optimal controller is obtained by using the h2syn function as"
"where n [i] is gaussian with zero mean and variance 1 − β 2 and independent of n 1 [i]. thus, for the special case of (25), the channel (18) can be equivalently written as"
"where x is standard gaussian and independent of s, and α is chosen to maximize the achievable secrecy rate. a closed-form expression for the maximizing α can be written as"
"each of the virtual factories is modeled around the logical groupings of similar functions and activities. every activity results in a deliverable which could be of both a physical or non-physical nature. the virtual factories are partially based on the concept of a real life factory with some similarities to the factory design pattern. the logical model of the factory design pattern is based on the idea that products are created in a factory in which a client focuses on its discrete role in the application, without concerning itself with the details of how the product is created [cit] . the predominant purpose of the network factory (nf) is for the profiling and registration of community members."
"another ever increasing and popular technology that could be used to describe metadata is javascript object notation (json). json is a lightweight data-interchange format, which is both human readable and easy for machines to parse and generate (\"introducing json,\" n.d.). json is a language-independent data interchange format that provides a simple text representation of arbitrary data structures [cit] . the json standard is a popular technology for consuming content via an application programming interface (api) provided by various vendors and software services [cit] )."
"in this paper, we apply cyclostationary analysis to simulated data obtained by convolving the measured time-varying channel impulse with the transmitted symbols; we also include the symbol phase variations as found in the data [cit] . we then estimate the signal parameters such as the carrier frequency including the doppler shift, and symbol rate. as shown previously, the dopper shift for a moving source can vary rapidly with time [cit] . as a result, the conventional analysis using the second order spectral correlation function (scf), which treats the channel as time invariant, does not always work. to estimate the fast varying doppler shift, one must apply the cyclostationary analysis to short segments of the signal. to observe the spectral lines in cyclic frequency, very high cyclic frequency resolution is required which demands a high sampling rate. to overcome the computational burden in obtaining high frequency resolution, a dynamic varying resolution algorithm is adopted where high resolution is obtained only around the cyclic frequencies of interest. we note that most of the signal parameter estimation methods reported in the literature do not consider extensive multipaths (frequency selective fading) as found in the underwater channel. while the multipaths are expected to modify the features of the scf, we find, based on our analysis, that the effect of multipaths can be minimized using the spectral coherence function (sof) as suggested before [cit] . as a result, the proposed method using the high resolution sof is shown to be able to blindly estimate the carrier frequency (and doppler shift) and symbol rate of the signal."
"the fourier transform of the caf is defined as the spectral correlation function (scf). scf can be measured by the normalized correlation between two spectral components of x(t) at f + α 2 and f − α 2 over δt interval. then, the ideal measurement of scf can be expressed as:"
"v. conclusions based on the second order cyclostationary features of the received signal, we proposed a new method to blindly estimate the carrier frequency (and corresponding doppler shift) and symbol rate of underwater communication signals. the method does not require any a priori knowledge of the transmitted signal. specifically, a two-stage estimation algorithm is developed: first, a coarse estimation based on spectral analysis is performed; second, a fine estimation based on cyclostationary analysis is performed to obtain high resolution estimation"
"esteban-gil, fernández-breis, castellanos-nieves, valencia-garcía, & garcía-sánchez (2009), however, argue that scorm which incorporates the use of scos could be seen as the most important learning object standard used in e-learning systems. [cit] point out that the lom metadata schema acts as the basis on which scorm classifies and describes scos which utilize xml. in research presented by buitendag and van der walt (2011a) it was highlighted that kos could be classified using metadata wrappers, based on existing ontologies."
"as part of the review process, the referenced sources in the ko, such as video, pdf, and web page, are called and rendered in an applicable browsing area. some dkss could be stored locally as part of the knowledge object repository (kor). both the knowledge seeker and external experts have the opportunity to validate and verify the content, i.e., answers (solutions) of the result set, which represents the existing knowledge."
"the research is based on a ll designed for an agricultural community of practice (cop) which is also applicable to any knowledge intensive and dependent domains, such as higher education, or research and innovation organizations in the public and private sectors. within the agricultural sector, extension officers often fulfill the role of knowledge agents. extension offices are key enablers in the provision of knowledge and alternative methods, to persuade clients such as emergent farmers to apply new or improved practices out of their own free will. this includes actions of facilitation within the principles of help to self-help and to prepare clients to better handle future problem situations. some of the knowledge-operation related problems identified by buitendag and van der walt (2011b, p. 70-72) include the following:"
"4. a semantic tagging and classification service, which forms part of the semantic layer, to aid in the understanding of research notes, of both current and future knowledge related documents."
"in view of this description the researchers support the notion that in ll environments, the learning process constantly adds knowledge to the ll knowledge base, therefore using pre-existing knowledge from the knowledge base can also aid in the generation of new knowledge. if the process of accessing pre-existing knowledge is adequately supported by the ll knowledge support services, the knowledge acquisition learning process can be significantly enhanced."
"however, due to the extremely complex and dynamic environment of underwater acoustic communication, it is not clear if the cyclostationary analysis is still applicable in these tasks. the underwater acoustic channels are extremely complex and dynamic [cit] . the underwater acoustic communication signals experience severe doppler shift, multi-path effect, phase noise and variation over time. as a direct result, the cyclostationary analysis faces significant challenges in analyzing underwater acoustic communication signals."
"the semantic metadata integrator provides functionality for the semantic extrapolation process. this process generates tags which are compared with existing metadata, using semantic pattern clustering in the semantic knowledge repository, which matches existing classes, relations, axioms, functions and instances of prior searches and results."
"the knowledge activities of a ll are enabled not only by the various systems, services, and tools but also by the knowledge workers themselves. from an agricultural community of practice (cop) perspective, extension officers will in most cases fulfill the role of knowledge workers. the term knowledge worker has different definitions presented from different perspectives. [cit] explains that knowledge workers could be seen as investors of knowledge who make informed decisions on when and how much of their knowledge and energy to invest in a company, where the company does not necessarily have much control over the investments. mládková (2011, p. 248) explains that the quality of the work presented by knowledge workers, not only depends on their ability to create, distribute, and share the knowledge that they work with, but also on the way that the knowledge is organized and maintained in the organization and in this case the ll. the success of a knowledge organization could also depend on the way that tacit knowledge is maintained and disseminated, which is often problematic. the problems identified by mládková (2011, p. 248) could be addressed by the implementation of the various ll knowledge support services."
"as indicated in the previous sections, learning and knowledge dissemination play a cardinal role in the envisioned functioning of a ll. this section of the paper presents an overview of two important concepts which facilitate and enable the various knowledge support activities envisioned, namely, learning objects and knowledge objects."
"it is foreseeable that not all questions posted or posed would be answered by means of the available kos which form part of the ll knowledge repository. in such cases applicable ll research methods as depicted in figure 1 will have to be initiated to aim to answer the questions posted. in other situations existing kos could be revaluated and adapted by experts to provide a complete answer to a question. various techniques could be applied, e.g., combination, internalization, the addition of annotations, and new patterns generation."
"in revisiting the main research question presented, the researchers highlighted the role and benefits questions may play if used as metadata tags in conjunction with standard metadata in the form of a knowledge object wrapper. in this paper we suggest and support the idea that a ko could be implemented as part of a ll environment to provide an additional dimension to the management and categorization of knowledge based on questions. by implementing this approach the researchers suggest that the various knowledge worker activities will be greatly catalyzed, and that the implementation of the knowledge support framework with enable this practice, of efficient knowledge generation, storage and dissemination. this paper has argued that the additional dimension and the utilization of json for the description of metadata wrappers will not only enable interoperability but also provide a unique approach to the storage of information and knowledge references. prof. jacobus (potjie) van der walt has been intensively involved into ict research over the past 30 years at the tshwane university of technology (tut). his core research interests currently pertain into the study of emergent community-oriented ict support, with specific reference to portal based applications for emergent farmers. he has successfully supervised numerous post graduate students. he was also one of the first academia in south africa to publish a paper regarding community oriented living labs. he also started the soshanguve ll initiative."
"by using the collaborative efforts of various agricultural extension officers, existing web based documents in different formats could be tagged and annotated with relation to specific cases, utilizing standard ontologies and metadata wrappers. [cit] developed the paper-of-a-paper (poap) ontology. the poap ontology represents the network of concepts and associated external resources derived from the tagging activity of related web documents. the poap ontology is modeled on the friend-of-a-friend (foaf) ontology which facilitates interaction and interrelations based on people's connections and friends."
"without proper knowledge support services, neither the kf nor the pf would be able to function successfully. the sf provides the service infrastructure and resources; prospective innovators, infopreneurs, and entrepreneurs need to create innovative ll intended support products and tools. [cit] listed the following sf services as a collection of knowledge support services for use as part of the sf, to drive the knowledge activities of the kf (cf. figure 1 ):"
"it is clear that the signal of interest is contained in the red rectangular area centered at f c, with bandwidth approximately the vertical width of the red area. there exist some interferences and distortions out of the red rectangular area. to coarsely estimate the f c and the f b, we 1) find the maximal and minimal levels of the spectrum for different times t:"
from the problems listed it is evident that access to the correct and satisfactory knowledge relating to the questions posed is of cardinal importance for the successful functioning of agricultural extension services.
3) update the spectrum by setting the spectrum values to be zero if the magnitude of the spectrum is less than the threshold. the interested area (the red rectangular area containing the signal in fig. 3
in literature there are different interpretations regarding how knowledge objects (kos) and learning objects (los) relate to one another. there is no precise accepted definition for either one. [cit] contrasted the various concepts relating to los and kos and highlighted the fact that a ko is sometimes regarded as a component of a lo. others suggest a lo and a ko to be equivalent to one another [cit] .
"the rest of the paper is organized as follows: in section ii, we present the system model, the cyclostationary analysis. section iii describes the short time dynamic varying resolution scf and sof algorithm. section iv shows numerical results of the estimation of carrier frequency and symbol rate via simulated data."
"to summarize, the network factory aims at establishing groups and virtual teams of people based on the social networking concept. the various virtual teams will often participate or initiate different research activities. the research activities conducted as part of the kf will sometimes result in the generation of new knowledge such as the identification of a new product or service that is required. in most cases the research activities will be driven by a question. the data, information and knowledge generated as part of the research process are classified and categorized for future use within the kf."
"the knowledge factory (kf) promotes, stimulates, and provides an environment for the generation and discovery of knowledge through the application of various research activities and methods."
"an api is a set of functions, routines, and protocols published by an enterprise or public application, for use by other developers in the development of services and other software tools. apis are often accessible over the internet as web services. within a living lab (ll) the service factory promotes the development of custom services by using existing services through apis. utilizing existing apis from service providers in the development of custom tools provides a unique opportunity for service development in a ll."
"the product factory as well as the sf utilizes knowledge objects (kos). we regard kos as artifacts enabling the transfer of knowledge which are based on questions. the proposed knowledge discovery and support services also aid in the classification of knowledge obtained through some of the ll activities. some of the envisioned activities include research, experimentation, innovation, and artifact development. through the research activities new knowledge is derived and processed as part of the knowledge factory. the created knowledge objects are categorized, stored, and made available for future referencing and use. the knowledge objects are also part of the ll domain which could be available for use by other lls and communities. in some instances the knowledge objects could be sold to generate income (buitendag. 2013; [cit] ) ."
the process of conducting research in a living lab environment generates new information that needs to be classified and appropriately applied to answer not only the current research questions but also those that may arise in the future.
"1. a question and answer service (qas) which allows users to post questions and obtain answers. the questions as well as the responses are semantically tagged to provide inference services for future questions posted, which will speed-up the knowledge acquisition processes."
"within an agricultural ll environment the use of learning objects could play a significant role in the teaching and learning processes of both extension officers and farmers, and significantly improve the knowledge support objectives."
"in each of the services, questions play an important role and are regarded as the driver and catalyst for the embedded and interrelated activities of which each service is composed."
"where l is the maximum number of multi-path components, l is the index for multi-path component, α l is the fading gain of the l th path, θ l is the phase offset introduced by the channel on the l th path, τ l is the time delay of the l th path, δf is the doppler shift, and n(t) corresponds to the additive white gaussian noise. note that the fading, doppler shift and phase noise are all functions of time."
"the provider field refers to the name of the content provider. in certain cases logon details will be required for access, and in such instances the login object will store and access the required verification details by implementing standard authentication protocols. the type is used to indicate the format of the digital resource and is often required to identify the asset and to access the content for instance .pdf readers and .flv players."
"the kf uses standard tagging and metadata descriptions in the classification of all knowledge resources. often new services will be designed based on the knowledge generated as part of the kf. the services are rendered and created to form part of the sf. the development and deployment processes of the service could initiate new research activities involving some or all of the virtual team members. various activities of the kf are enabled through the utilization of services previously designed and developed in the sf. the sf and kf activities often result not only in the creation of services, but could also lead to the creation of artifacts."
various living lab activities are aimed at generating new knowledge or to extrapolate existing knowledge from other ll community members for use in various problem solving tasks. one way of generating new knowledge is through the application of standard research practices as depicted in figure 1 .
"the knowledge object (ko) is represented by the main class. each ko is uniquely identified by an id field and is tied to a particular case which references a knowledge request instance posted by a knowledge seeker. each object is marked as being verified or not. if the object was verified, it contains a link pointing to the external expert who has verified if the applicable digital knowledge source (dks). the verification process aims at ensuring that each dks does indeed provide an applicable and correct answer posted or referenced as the result of a question."
"the researchers believe that another cardinal aspect in any ll environment is that the generation of knowledge leads to the promotion and support of learning, which also catalyzes the knowledge-sharing activities amongst all ll stakeholders. part of the objectives of the knowledge support services is to promote learning through the provision of tools, of which the qas is an example."
"in a presentation relating to the metaschool llp project (cf., lemil. [cit] ) the idea that a ko is a granular component of a lo is supported. the researchers are of the opinion that most of the predominant characteristics and attributes of los are also applicable to knowledge objects. [cit] supports the idea that the ko concepts are broader than that of a lo, where los are more granulized and applicable in educational contexts, and that kos often include only content and not an objective or some additional instruction information."
"the qas utilizes kos, as depicted in figure 7, by means of a knowledge seeker posting a question to the service, which could be hosted on a standard collaboration platform or social media tool utilized by the ll, e.g., facebook. an analytics service evaluates and analyzes the question posted and allows for a search with semantic matching in an existing semantic repository. the analytics service implements a question and answer extrapolation tool (qaet) to match and search existing kos, based on keywords supplied in the question as well as the question itself. the qaet operates as follows:"
"cyclostationary analysis has been accepted as an important tool to perform signal detection, signal parameter estimation, and modulation detection of radio frequency (rf) signals [cit] . cyclostationary analysis is based on the fact that communications signals are not accurately described as stationary, but rather more appropriately modeled as cyclostationary. while stationary signals have statistics that remain constant in time, the statistics of cyclostationary signals vary periodically. these periodicities occur for signals of interest in well defined manners due to underlying periodicities such as sampling, modulating, multiplexing, and coding. this resulting periodic nature of signals can be exploited to detect the existence of the signal, estimate important parameters of the signal, and determine the modulation scheme of the unknown signal."
"it can be easily shown that the channel effects are removed from the sof, and the sof is preserved as a reliable feature for parameter estimation and signal identification."
"the concept of a learning object (lo) is frequently encountered in literature relating to digital libraries, e-learning, instructional design, and classroom and experimental environments. [cit] explains that a lo should be regarded as any digital resource that can be reused to support learning which has been intentionally designed for learning purposes."
"material published as part of this publication, either on-line or in print, is copyrighted by the informing science institute. permission to make digital or paper copy of part or all of these works for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage and that copies 1) bear this notice in full and 2) give the full citation on the first page. it is permissible to abstract these works so long as credit is given. to copy in all other cases or to republish or to post on a server or to redistribute to lists requires specific permission and payment of a fee. contact publisher@informingscience.org to request redistribution permission."
the predominant purpose of service factory (sf) is to deliver and utilize current available services to help meet the objectives and functioning of the ll. added as part of this research we present a collection of services for knowledge support as part of the sf. these services utilize the proposed knowledge object design incorporating questions as meta-tags.
"knowledge integration plays an important role in each of the factories. [cit] explains that the process of knowledge integration, which could be seen as the process of recombining existing knowledge for exploitation, is of key interest in the formation of new ideas. existing knowledge undergoing knowledge integration leads to and promotes innovation."
"where finite time fourier transform of x(t) is it is important to note that fig. 1 is generated from a large number of data. in reality, we can only obtain a limited length of signal to perform scf."
"each of the factories that are part of the factory framework utilizes and generates new data, information, and knowledge for further processing and study. as knowledge applicable to the ll domain is created, the opportunity for the creation of new services is enhanced. new services are based on new concepts generated, which in turn are based on new facts obtained inter alia through the various research activities the practice of grounded theory, activity theory, action research, prototyping, and design and creation."
"existing kos could be reclassified and reworked to provide a full answer to a question, which was not described before. the ko itself is not changed, only the interpretation of the information and tags could provide new insight. research activities could also generate and create new kos. figure 3 highlights this concept."
"if we calculate the scf of the underwater acoustic communication signal over a long period of time, the varying multipath fading, doppler shift and phase noise would distort the cyclic frequency features in the scf, making the parameter estimation impossible to achieve. hence, we have to apply a time domain window onto the received signal to obtain a short length of received signal so that within this length the channel and doppler shift can be considered stable. mathematically, this truncated signal corresponds to"
"many popular web based software services and tools, e.g., delicious.com (for social bookmarking), digg.com (for news and bookmarking) as well as zotero (for research purposes) use json as the interchange format. this is particularly appealing to this research due to the fact that json could be used as a data format for the description of a knowledge object wrapper (kow). figure 5 presents the researchers' simple class diagram model that represents the basic structure of a kow, which is implemented as part of the knowledge support services of the ll. in essence a kow adds an 'additional layer' to current metadata models, with the specific objective to also include questions as metadata. questions are tied to unique cases, which are also referenced as part of the metadata. the main idea behind the implementation of the kow is not to substitute or replace existing metadata models, but to extend the models with new functionality. we believe that the implementation of questions in conjunction with standard tags and tagging operations would not alter the operations or intended design of exiting models, but would add an additional metadata dimension. the additional dimension which could add contextual data would be beneficial for both knowledge seekers as well as the knowledge workers."
"2. a knowledge interchange (ki) service, which supports the sharing of information among people (online collaboration, i.e. question and answer postings) or through doing research e.g. grounded theory."
"within a ll knowledge is constantly created and classified as part of each of the factories' standard activities. [cit] explains that artifacts (which could be seen as facts, concepts, processes, procedures, and principles) are often used in the process for the creation of new knowledge."
"within south africa, the ll has become a popular platform to facilitate community engagement and collaboration. lls within south africa often aim at creating, innovative products and solutions to address the needs of the community [cit] ."
"where α is the cyclic frequency from which features can be extracted for identifying transmitted signals [cit], and r α x (τ ) denotes the cyclic autocorrelation function (caf):"
"in this paper we also aim to provide new insights, thoughts, and perspectives on the utilization of learning objects (lo), based on the lo metadata standard. we revisit the concept of the sharable content object (sco) as part of the sharable content reference model (scorm), and the classic dublin core (dc) metadata standard, based on a literature review. the authors provide their own interpretation of the concept of a knowledge object (ko) and highlight how the various activities of a ll may be supported by the implementation of kos, based on the researchers' definition. we also provide a more detailed description of the functionality of the proposed question and answer service that utilize knowledge objects, based on questions. the subsequent description and framework proposed in the latter part of this paper are based on design and creation research principles."
"the notion of a ko and its use within a ll environment has been described. figure 4 presents some additional examples and highlights the concept that kos could be seen as digital assets. this is similar to sharable content objects (sco) implemented as part of the sharable content reference model (scorm). scos are often aggregated into collections of activities that may contain other collection aggregations or other scos. [cit] explain that sharable content objects (scos) form the basis of the scorm model in which scos are digital assets in various formats such as sound clips, text videos, and images. the american department of defense and the advanced distributed learning group (cf., [cit] ) explain that a sco is the smallest unit of lesson content. it aids learning as part of a learning management system and each sco is described by the xml based lom standard. figure 4 presents the idea that a ko, like a sco, could also be aggregated. the principle concept behind aggregations is the grouping of related content, which aims at enhancing the learning process by conveying the same content with different assets in different formats."
within a ll environment there is close collaboration between both external and internal experts as well as internal and external knowledge seekers; this interaction is orchestrated through functionality provided as services in the nf.
"living labs (ll) rely heavily on knowledge for new innovation and value chain optimization. questions are the driving force in the discovery of new knowledge (bergvall-kåreborn, ihlström, ståhlbröst, & [cit] ). the qas utilizes knowledge objects which essentially aim to encapsulate and tie a question to an information source that will provide an answer to a posted or posed question."
"each ko is tied to a single question; multiple questions are represented as new kos with a common case reference. therefore, a common case could refer to many questions with many aggregated dkss. some important aspects of a dks are that:"
"learning in a living lab (ll) involves the community of practice (cop), the individual knowledge seeker, and the idea of knowledge negotiation. the addition of meaning to existing practices is (in the researchers' opinion) what drives innovation in a living lab. [cit] provide the following definition that aligns the notion of creation of meaning with that of a cop in a ll environment: \"learning is described as an ability to negotiate new meanings within a cop, to create engagement in cop and to deal with boundaries between cops.\""
"an important consideration for this research is the fact that each ontology and metadata model implements a unique identifying field. in the case of ontologies, this unique identifier is realized through utilizing uris and rdf ids. the identity of resources on the web (irw) ontology was specially developed to aid the description of various uris"
"living labs encourage innovation practices as well as the exchange of new ideas and information, which promote the concepts of innovation and co-creation [cit] each of the current established lls within the network in sa has distinctive uniqueness focusing on the capacity building of the communities involved, as well as the enhancement of the innovation skills of the individual community."
over the years various standards and technologies have evolved to describe the metadata of digital sources such as extensible markup language (xml) and resource description framework (rdf) which includes owl. ontologies are also increasingly becoming popular as the semantic web (web 3.0) is constantly evolving and becoming more popular by the day.
where t w is the length of the rectangular window w(t). in this article we assume the length t w is 0.125 second and it is consistent to the literature.
"it is well known that most of communication signals are cyclostationary, and cyclostationary features such as spectral correlation function (scf) and spectral coherence function (sof) can be used in signal detection, parameter estimation and modulation detection. a signal x(t) is considered to be cyclostationary in wide sense if"
"within the scope and description of this research the utilization of knowledge objects (kos) are seen as one of the prime enablers of the knowledge support activities of a living lab (ll). kos are also cardinal to the functioning of a ll, to promote effective and efficient extension services and positively contribute to annotation and stimulation of new knowledge generation."
"various knowledge models, such as ontologies, use different tools to describe the content presented. one of the oldest and most popular ontologies is the dublin core (dc) ontology, provided by the dublin core metadata initiative (dublincore. [cit] ) [cit] describe the dc ontology as a metadata element set which is a standard for cross-disciplinary resource description where the semantics of the 15 elements of the dublin core (i.e., title, subject, description, creator, publisher, contributor, date, type, format, identifier, source, language, relation, coverage, and rights) are described by using a combination of standards, including xml and rdf."
"in order to provide a better clarity of the activities relating to the knowledge factory we need to provide our definition of knowledge support in a living lab (ll) environment. the researchers define knowledge support as an activity-oriented, question based process aiding knowledge creation, facilitation, sharing, codification, and application for its intended user group."
"the practice of verification is important to ensure that kos reference dkss that do indeed supply correct answers to the question posted. in cases where the dkss are not accessible, the applicable indication is adjusted as part of the ko wrapper. the semantic knowledge repository acts as a catalogue mechanism where json knowledge wrappers are stored. the knowledge object repository stores local ko data such as documents, videos, notes as well as previously downloaded content from other sites."
"the authors see a ko as being intrinsically tied to a question where the ko aims to address and supply an answer to a question posted by a knowledge seeker. the ko and a question are tied to an applicable domain, which may overlap areas of other knowledge domains. where los often aim to collate and address a specific learning outcome, a ko aims to represent content. the content aims at providing sources that could provide an answer to a particular question. the ko aims to address questions such as how, why, and what. a ko is classified and described using semantic metadata for future access and interoperability. the ko itself is considered as a resource, just as a lo would. figure 2 illustrates how kos relate to questions posed in the ll environment. the central idea is that a complete answer and/or solution are provided which is then tied to a question. parts or a combination of parts of kos could be classified and described to provide a solution to a question. it is also possible that a ko could provide complete answers to more than one possible question."
for the purposes of this study the focus is placed on all non-physical sources of knowledge that could be captured and classified in a digital format.
"figure 7 presents the general flow and composition of the question and answer service (qas) as one of the knowledge support services provided for use by the living lab knowledge factory. the semantic/metadata integration service implements kos and a process for the generation and maintenance of existing kos. the knowledge support activities in a ll are dependent on the collaborative efforts of both the knowledge seekers as well as the knowledge experts (i.e., knowledge workers) and in the case of an agricultural cop, the extension officers."
"the operation of the question answer service (qas), the role of the knowledge workers as well as the quality of the meta data used that ties questions to possible answers, will determine how effective the ll is in solving problems of the community it is serving."
"in this paper, we extend one state of the art cfi solution, binc-fi, and propose a new binary-only cfi protection scheme, bincc, which provides finer-grained protection for stripped binaries. by duplicating a little amount of code and performing static analysis, we divide the binary code into several mutually exclusive code continents, and classify each indirect transfer as either an intracontinent transfer or an inter-continent transfer. we further apply strict cfi polices to constrain these two kinds of transfers. under our policy, intra-continent transfers have determined targets within the continent, and inter-continent transfers are only permitted to reach specific types of targets across continents. as a result, we significantly refine the legitimate transfer targets."
"timesten is main memory dbms created at timesten company, is shown the latest version of 5.1. and timesten currently has no.1 global market share in main memory dbms. for the recovery, it provides immediately commit and group commit and supports transaction consistent check point and update ping-pong technique [cit] ."
this policy is to constrain the inner nodes that represent indirect transfers within a code continent: direct returns and indirect jumps related to jump tables. their targets are determined and can be obtained from the super-cfgs that compose the code continent.
"suppose in a cfi enforced binary a return instruction was fully controlled by attackers, how likely this return could be used to reach a call preceded gadget in this binary."
"this policy is to constrain the border nodes, which are indirect transfers whose targets cannot be statically determined from supercfgs. based on the transfer types, we apply the following policies."
"in figure 10 we see that on average bincc reduced the legitimate targets for an indirect call by around 40% compared to binc-fi. according to bincfi implementation, all the possible constant code pointers are potential targets of indirect calls. however, some of those constants are actually addresses of call sites and jumptable's case entries, which are not function entries. in our implementation, we remove them from the target set. for indirect jumps, there could be many indirect jumps that reside in the plt section, and these jumps have certain targets within the current module. bincc takes them into account and thus reduces the legitimate targets by 35% on average compared to binc-fi, as shown in figure 11 . figure 12 shows that bincc reduces legitimate targets for a return by 87% on average compared to bincfi, this is because bincc significantly refines the targets for direct returns by enforcing a much more strict policy on them as compared to bincfi. we see gcc achieved the largest improvement. there are more direct returns than indirect returns in this binary, and each direct return"
"estimating the 3d human pose using a single image is a severely under-constrained problem, because many different body poses may have very similar image projections. in order to disambiguate the problem, one common approach is to assume that an underlying deformation model is available. linear models [cit] or sophisticated dimensionality reduction methods have been used for this purpose [cit] . alternatively, other techniques have focused on learning the mapping from 2d image observations to 3d poses [cit] . in any event, most of these generative and discriminative approaches rely on the fact that 2d features, such as edges, silhouettes or joints may be easily obtained from the image."
"control flow integrity [cit] plays an important role in combating control flow hijack attacks. it forces the control flow transfers in the program to follow the policy represented by the control flow graph. the policy can be strict based on the source code as the the control flow graph tend to be completed. however, for stripped binaries, because of the lack of source code or debug information, the cfi policy is coarse-grained. although many advanced cfi implementations such as ccfir and bincfi can prevent the vast majority of control flow hijack threats, they may still be vulnerable to sophisticated rop attacks as illustrated in the works [cit] ."
"after identifying the functions to be duplicated, we duplicate all the instructions in their cfgs. all the duplicated code and the original code are put into a new code section after being instrumented with cfi policy (discussed at section 5). the original code section is marked non-executable and all existing data sections are kept unchanged. the constants with duplicated instructions are unchanged as well, so the values in the data sections are still accessed correctly. additionally, in the duplicated functions, we need to fix up the operand values of relative calls and jumps. for relative calls, the operands are adjusted to simulate the call instructions of the original version of the function. similarly, for relative jump instructions, the operands are adjusted to jump to the corresponding target branches in the duplicated function."
"the cfi policy towards stripped binaries is coarse-grained as the cfgs are imprecise. the permissive policy is still likely to be violated although it has the ability to mitigate the vast majority of common control flow hijacks. figure 1 shows the possible attack models to ccfir and bincfi. the solid line indicates the execution flow at run time, while the dashed line indicates the possible targets that could be leveraged in an attack. for ccfir, as shown in figure 1(a), the target, residing in the springboard, is only required to align against the constant m_r, so any call-site's address would be regarded as legitimate. similarly, for bincfi, a controlled return would be able to reach any call sites in the binary, as shown in figure 1 (b). those returns are left unprotected and give adversaries chances to leverage call preceded gadgets to launch attacks. one recent practical exploit has been shown [cit] ."
"group completion technique is not recorded each transaction log information in the log file. transaction will commit after all partially commit transaction log is recorded log file in batches saving the each transaction log information in log buffer. it can reduce the number of disk io. this technique provides better performance than immediately commit technique because of reduction of the number of disk reflection about log buffer information. but, it has a disadvantage that degrade concurrency to maintain lock until partial commit of transaction will commit. one way of log techniques, it solve the degrade concurrency retaining lock of corresponding transaction's resource through transaction's commit is delayed up to specific point at group commit technique [cit] ."
"as discussed earlier, to enforce our policy, we duplicate icfs that fall in the intersection of the icfs and dcfs. however, in some cases, due to compiler's optimizations, some icfs are likely to have common returns with dcfs, and each of those returns cannot be given a clear transfer type (either intra or inter), which violates our cfi policy. to resolve this issue, if an icf has common returns with a dcf, as determined by referencing these two functions' cfgs, we also duplicate this icf. the duplicated function will become a new icf in the binary."
"apart from that, we instrument each direct return as shown in figure 7 . the start and size, embedded in two prefetchnta instructions, indicate where to find the corresponding address translation table. we use another thread local variable %gs:0x50 to store the first prefethnta instruction's address, _addr. the translation routine addr_trans_dret uses %gs:0x50 at runtime to access start and size for locating the right table, and then perform the same operations as addr_trans as shown figure 5 . for the indirect jumps related to jump-tables, we can use a similar structure. however, as mentioned above, bincfi has instrumented this kind of indirect jumps by restricting them to only go to their corresponding case branches, so we retain its instrumentation."
"because of new security mechanisms such as aslr and dep, rop has gained much more popularity among attackers as a technique for launching exploits. cfi implementations such as cc-fir and bincfi significantly mitigate rop attacks, since the vast majority of rop gadgets are instruction-misaligned and no longer feasible for generating exploit. however, the work [cit] recently showed that in some cases it is still possible for attackers to leverage call preceded rop gadgets to build rop chains and thus bypass the protection from bincfi and ccfir."
"to achieve our enforcement, we make two extensions to address translation tables. firstly, we introduce new table entries for the newly introduced indirect transfer targets due to the code duplication, which are duplicated functions and the return call sites within them. secondly, we modify the relevant table entries of the functions that have been duplicated, so that if the original function is called at runtime the corresponding new function will actually be executed."
"in addition, we lay consideration on the indirect returns in the orphaned continents, denoted as orphaned returns. as we cannot identify the invoker of orphaned continents by static analysis, we allow their targets to be any call sites."
"the operations of transaction are processed very quickly in main memory dbms, but it is required relatively long time to write the log on disk. thus, if you release the lock before perform this task, transactions waiting for subsequent transaction's resource is able to escape from wait state and it should increase the concurrency."
"without using prior information, monocular 3d human pose estimation is known to be an ill-posed problem. in order to be disambiguated, many methods to favor the most likely shapes have been proposed."
"bincfi instruments indirect calls/jumps and returns. for the indirect jumps associated with jump tables, their operands are replaced by expressions of the form *(ce1+ind)+ce2, where ce1 and ce2 are constants, and ce1 indicates the jump table associated, and *(ce1+ind) indicates all possible case branches. also, bincfi introduces a new jump table based on every ce1, with transformed case branches' addresses inside. for the remaining in-direct transfers, they are instrumented as shown in figure 4 . we illustrate it by taking an indirect call as an example, the other indirect transfers are handled in the same way. firstly the runtime target(i.e.,%eax) is saved to a thread local variable(i.e.,%gs:0x40), and then the control is transferred to a routine, addr_trans, which performs checking and address translation. figure 5 shows how addr_trans works. in the routine, it checks whether the transfer is against the cfi policy. if not, it performs the address translation. as %gs:0x40 stores the address that falls in the original code section, namely, orig_addr, bincfi consults the relevant address translation table for the corresponding translated address, namely, new_addr. if found, it jumps to new_addr. otherwise, it calls the global lookup routine, which helps address translation across different modules. the global lookup routine works by consulting the gtt (global translation table) . for every loaded module, the gtt records the relationship between the base address of the module and the address of the module's addr_trans. for the above example, the global routine checks which module the address %gs:40 belongs to, if no module is found in the gtt, an alert is triggered, and if found, the control is transferred to that particular module's addr_trans routine which takes care of the address checking and translation as mentioned above. the global lookup routine and the gtt are added in the loader, and they will loaded at different memory address every time. also, the loader will update the gtt when a new module is loaded during the runtime."
"after code continent construction, we perform instrumentation on different nodes in the continents to enforce our cfi policy. this is implemented on top of bincfi, so we briefly describe the basic instrumentation structure bincfi provides and then give details on our enforcement."
"for a non-pic module, we use a integer-sized window (e.g., 4-byte in 32-bit systems) to scan the constants within the data, .rodata, .init_array, exported symbol sections and any other sections possibly containing integers representing valid code addresses. we also collect constants used within the code section. by consulting the disassembly, if the constant or the sum of the constant and the code base is a valid code address and meets one valid instruction's boundary, we considered it as a constant function pointer candidate."
"this technique is no change of the number of log buffer's disk output and the response time of individual transaction by comparing the group-commit. but, it reduce other transaction's lock waiting time waiting for a possessed lock by corresponding transaction and it has the advantage of improving concurrency of transaction performance than group commit [cit] ."
"although our solution has made considerable improvements for restricting indirect control flow transfers, the policy for indirect calls still need improvement compared to returns. in our solution, the protected binaries are still vulnerable to attacks altering indirect call targets. indirect call targets are permitted to reach any icf, and it would be considered as valid that if the target of an indirect call is modified to another icf in the binary. however, other binary only cfi solutions face the same problem. existing solutions [cit] make improvement in protecting specific indirect call targets such as c++ virtual functions. it is not hard for bincc to combine them to achieve more strict protection. we leave this as our future work."
"finally, we present qualitative results on the tud stadmitte sequence [cit], which represents a challenging realworld scene with the presence of distracting clutter and occlusions. in addition, since the ground truth is unknown, no sample frames from the humaneva dataset for both the walking (a1) and jogging (a2) actions. the first three columns correspond to successful 2d and 3d pose estimations for the three subjects (s1,s2,s3). the next two columns show typical failures cases of our algorithm. in the fourth column we see that occasionally we suffer from depth errors, where the 3d pose is correct but its depth is not. in the last column we plot other failures, mostly caused by very large errors of the 2d detector, due to mis-classifications or self-occlusions. we present results for both the walking and jogging actions for all three subjects and camera c1."
"to construct code continents, we identify all the dcfs and icfs in the binary, and then perform control flow analysis to compute the cfgs of those functions, which helps determine what functions we need to duplicate and also to compute super-cfgs. after the code duplication, we finally construct the code continents based on super-cfgs. we now discuss these phases."
"dynamic code such as jit code could not be identified simply through static analysis without source code, so our solution can not handle this code. this is also a common open issue with other current binary only cfi solutions. we leave it as our future work."
"in this paper, we get rid of the strong assumption that data association may be easily achieved, and propose a novel approach to jointly detect the 2d position and estimate the 3d pose of a person from one single image acquired with a calibrated but potentially moving camera. for this purpose we formulate a bayesian approach combining a generative latent variable model that constrains the space of possible 3d body poses with a hog-based discriminative model that constrains the 2d location of the body parts. the two models are simultaneously updated using an evolutionary strategy. in this manner 3d constraints are used figure 2 : method overview. our approach consists of a probabilistic generative model and a set of discriminative 2d part detectors. our optimization framework simultaneously solves for both the 2d and 3d pose using an evolutionary strategy. a set of weighted samples are generated from the probabilistic generative model and are subsequently reweighted by the score given by the 2d part detectors. this process is repeated until convergence of the method. the rightmost figure shows results at convergence where the red shapes are the estimated poses and the green ones correspond to the ground truth."
"there is one corner case that we need to consider. there might be orphaned code pieces, which are not ever reached by static control flow analysis, for instance, unresolved target branches for indirect jumps or dead code. we need to restrict the indirect transfers in such code pieces as they may be invoked at runtime. to achieve this, we also generate a code continent, denoted as an orphaned code continent, for each of them. an orphaned code continent is composed by the super-cfg of a orphaned code piece, and the super-cfg is constructed by applying algorithm 1 by considering the instructions of the code piece as its \"cfg\"(which is cf g func ). also, ibrnch of the super-cfg(i.e.,sg.ibrnch), composed by icall, iret and ijmpu, are border nodes, and inn of the super-cfg(i.e.,sg.inn) are inner nodes. considering that an orphaned continent is not discovered as a function and thus cannot be invoked by a call, we assign the entry of an orphaned continent as a border node, instead of a root node."
"we know that in general the functions in a binary fall into two categories, indirectly called functions (icfs) and directly called functions (dcfs). there might be some functions that are called in both ways, and by duplicating those functions, we can partition the functions into two mutually exclusive sets by considering the duplicated functions as icfs, as shown in figure 3 . at an indirect call site, we perform runtime dispatch to execute the duplicated function when the original function is called. as such, a function will be called only in a certain way, either indirectly or directly, and because of this, a return will go back to a specific type of call site. we thus can divide all the returns into two mutually exclusive sets: direct returns, which only target direct call sites, as well as indirect returns, which only target indirect call sites."
"efficient recovery technique is important to stability of database in main memory dbms because all stored data in main memory may lose when unexpected situation occurs such as computer failure, system crash, system error, etc."
"note that not all obtained candidates are function entry points. some constants are actually the addresses of case branches in a switch-case jump table. case branches are within a function and cannot be function entries. some candidates are actually the addresses of some return call sites but not function entry points. so we remove these two kinds of constants from the candidates, and the remaining are the final icfs."
"in the definition, t j represents the legitimate targets of indirect transfer ij by bincfi, and tj represents the ij's legitimate targets by bincc. figure 9 shows the statistics about this metric. on average, bincc reduced indirect transfers targets by 81.34% from bincfi."
"action consistent check point technique performs check point when action is completed, because transaction consider as continuous act of data base. while performing a check point, all the update operations on the database need to stop and undo log and redo log should be recorded in this technique."
"additionally, when evaluating a part detector at a point, we consider a small window from which we use the largest detector value in order to give additional noise tolerance to the detector. we find this necessary as small 2d errors can have large consequences in 3d positioning."
"we consider three error metrics: 2d error, 3d error and 3d pose error. the 2d error measures the mean pixel difference between the 2d projection of the estimated 3d shape, and the ground truth. 3d error is the mean euclidean distance, in mm, with the ground truth, and the 3d pose error is the mean euclidean distance with the ground truth after performing a rigid alignment of the two shapes. this error is indicative of the local deformation error. we evaluate three times on every 5 images using all three cameras and all three subjects for both the walking and jogging actions, for a total of 1318 unique images."
"existing binary-only cfi solutions apply relaxed cfi policies due to the lack of source code or debug symbols. although they can mitigate common control flow hijack threats, it is still possible for adversaries to launch sophisticated attacks, for instance, rop attacks launched by leveraging call preceded gadgets."
"as mentioned above, each part l i has an associated scale parameter s i . this parameter is used to pick a specific scale among the evidence maps. intuitively, if a part is far away, we visualize the response at different octaves for three different part detectors. we overlay the projection of the 3d ground truth in white to get an idea of the accuracy. the outputs are normalized for visualization purposes, with the dark blue and bright red areas corresponding to lower and higher responses respectively. note that while some detectors, such as the head one in the first row, generally give good results, others do not, such as the left hand detector in the middle row. our approach can handle these issues by combining these 2d part detectors with a generative model. it should be evaluated by a detector at a small scale (high resolution). we therefore approximate the scale s i as:"
"this technique, when check point occurs, first all preceding transaction will commit and creation of subsequent transaction is stopped. if preceding transaction is committed, all the actual check point transaction performs and if perform of check point is completed, creation and perform of subsequent transaction is allowed. this technique is maintained transactional consistency on database using check point. in recovery status, there is no need to undo about performed transaction after current check point, it can be restored due to redo about commit transaction's log. however, it has a disadvantage of performance degradation because create, perform of subsequent transaction is stopped during performing check point."
"we make an extra effort to handle one kind of border nodes, the indirect jumps that reside in plt entries. they are used for dynamic symbol resolution. for such an indirect jump, only two legitimate targets should be allowed, one being the next instruction's address, the initialization value before the symbol being resolved, which is statically determined, and the other being the symbol's resolved address, which is determined at runtime. we can further restrain such a runtime target, since the loader can intercept the symbol resolution process and retrieve the resolved address. to this end, we arrange all the targets of these indirect jumps into one address translation table and put it into a newly introduced readonly data section. also, we modify the loader to be able to change the property of the data section to writable, update the corresponding table entry with the resolved address after symbol resolution, and change the property back. the change is restricted within this new data section, so there is no impact on other read-only address translation tables."
"each dcf is called through a relative call. the function entry's address could be computed through the relative call instruction's address and operand value. through this, we identify all the direct call sites from the disassembly to obtain all the dcfs."
"we believe the model we have presented in this paper is a step forward to combining the works of 2d pose estimation with 3d pose estimation. we have shown it is not only possible to estimate 3d poses by applying part detectors used in 2d pose estimation, but that it is also beneficial to the 2d pose estimation itself, as the 2d deformations are being generated by an underlying 3d model."
"aslr [cit] and dep [cit] have mitigated traditional threats to computer programs. however, attackers are still able to launch attacks through code reuse [cit] even if aslr and dep are enabled. return oriented programming [cit] (rop) is one such code reuse technique. over time this technique has gained popularity and presents a challenge to program safety. several works [3, 6-8, 13, 19, 24] have proposed solutions to tackle these kinds of attacks and they have all made improvement to some extent."
"there are representative three check point technique in main memory dbms; transaction consistent check point, action consistent check point, fuzzy check point. transaction consistent checkpoint perform check point when performing transaction all commit [cit] ."
"it released corresponding transaction's resource when transaction is partially commit status, not commit. first of all, it unlock without waiting for disk output the log buffer's contents and allow access to subsequent transaction's resource. and all transaction log about partially commit status in log buffer is recorded in log file in transaction order in batches. then corresponding transaction commit."
"we evaluated both space and runtime overhead. for space overhead, we compared the increase of address translation tables between bincc and bincfi, and also evaluated code size increase and total file size increase compared to the original file. for runtime overhead, we ran binaries enforced by bincc as well as bincfi and finally made comparisons."
"as in the sample code, icfs are composed by main, foo, qux, bar, while dcfs is composed by foo. only foo is called in both ways. suppose the execution starts from start, originally, the function foo is indirectly called at first time by 9 and directly called at second time by 3, so the return 7 would go back to those two call sites respectively. in bincc, foo' is a new function generated from duplicating foo and will be executed when foo is indirectly called at 9. this makes 7 become an intra-continent transfer and 7' become an inter-continent transfer, which means that 7, as a direct return, would only return to 3, while 7', as an indirect return, would return to 9, as shown by the two dot arrows."
"the performing of check point is completed when it is all reflected in the database file about changed data in main memory data base. at that time, the value of check point position in anchor file is changed information about other database file to be used in performing the next check point."
"this technique is synchronized with performing of transaction such as transaction consistent check point, it has a disadvantage of performance degradation because create, perform of subsequent transaction is stopped during performing check point. fuzzy check point technique is performed check point asynchronously with other transaction by creating process for perform of check point when check point occurs. fuzzy check point has superior performance than other checkpoint techniques. because it proceeds regardless of transaction perform [cit] ."
icf identification is based on the observation that the address of an icf stems from the constants in the binary. we search for any constant that could be used to represent or to compute a function's address.
"however, for bincc, this probability is much smaller, as both direct returns and indirect returns are restricted to have far less legitimate targets. specifically, if the return was an indirect return, it would be allowed to reach any indirect call sites. if the return was a direct return, it would only be allowed to reach gadgets starting at specific direct call sites. we calculate the probability under bincc as well as bincfi for all tested samples, and present the statistics in table 2 . from the table, we see that bincc considerably degraded the probability to leverage call preceded rop gadgets for rop attacks, only around 0.70%, with comparison to 100% for bincfi."
"this technique can be performed simultaneously update operation transaction and check point about page of same main memory data base. thus, if a system error occurs during performing of check point, recovery is performed a different data base files. update page of main memory database must be all recorded two database file in fuzzy check point based on ping-pong update. this technique has a disadvantage of performance degradation to recovery system because twice disk io on the same page. also, it need additional space to maintain two database file, occurs management maintenance cost for them."
"we organize the remainder of paper as follows. we discuss background and related work at section 2, and then present the concept of code continent and our policy in section 3. we describe code continent construction in section 4 and cfi enforcement in section 5. section 6 presents our evaluation. discussion is in section 7 and conclusion is in section 8."
"code continents are constructed from super-cfgs of functions. a super-cfg (super control flow graph) is constructed, for a function, from its cfg (control flow graph) by connecting all direct call sites in the cfg to the entry point of the callee's super-cfg and the end point of the callee's super-cfg is connected back to the call site. this process is repeated recursively until all the direct calls in the function are handled. a code continent is a directed graph that is constructed from merging functions' super-cfgs based on their common edges. therefore, code continents are mutually exclusive."
"end if 6: end for 7: return sg algorithm 2 shows how we construct code continents and classify nodes in code continents. the algorithm takes all icfs as input. it computes super-cfgs of each icf and then merges these supercfgs based on their common edges (e.g., the common callees), which is achieved by m ergegraph. the algorithm finally produces the set of mutually exclusive code continents, with root nodes, border nodes and inner nodes classified in each code continent. from the algorithm we see that the nodes are classified based on the types of nodes of super-cfgs that compose a code continent. the root nodes comes from the entry points of icfs in the current continent. the border nodes comes from the nodes representing indirect calls(icall), indirect returns(iret) and indirect jumps whose targets are not statically determined(ijmpu). the inner nodes are composed by the nodes representing non-control flow instructions and control flow transfers whose targets are determined (e.g.,direct calls, direct returns) in the current continent."
"another problem we need to pay attention to is c++ exceptions. in c++ programs, the necessary information for exception handling is stored in the .eh_frame section. when the exception triggered, the system would use current execution context to perform stack unwinding to identify the corresponding catch branch. we introduce new code through code duplication and there is no exception metadata about this code in the .eh_frame, so if a duplicated function contains c++ exceptions logic and actually triggers the exception, no exception handler could be found through stack unwind and the program would thus run incorrectly. to avoid this problem, we do not duplicate the functions containing c++ exceptions logic, and treat the returns from such functions similar to orphaned returns, allowing their targets to be any call site."
"the former technique is the way of electronic hardware solutions, non-volatile memory is used as log storage device to improve performance of system such as battery backup ram, flash memory. however these techniques have to use jasc 12-2-5 immediate commit stores changed data during transaction execution in main memory log buffer, log file is recorded all logs of the transaction on the disk before changing the partially committed to committed, and then transaction will complete. this technique has the advantage of simple structure and a short response time of individual transactions in comparison with group completion or pre-completion technique. but because the all log is recorded each transaction log on log file upon completion of transaction, this technique has a disadvantage that frequent disk io."
"priors on the initialization of the 3d pose were provided, except the rough bounding boxes for each person (fig. 6-right) . some results can be seen in fig. 9 . note that while the global pose seems generally correct, there are still some errors in the 3d pose due to the occlusions and to the fact that the walking style in the wild is largely different from that of the subjects of the humaneva dataset used to train the generative model."
"in this paper, we analyze the existing log technique, checkpoint technique and main memory dbms recovery techniques for research of main memory dbms recovery technique."
"dali is main memory dbms created at at&t bell laboratories [cit] . redo log and undo log is managed by each transaction in order to reduce performance degradation caused by contention between transactions. in transaction is partially commit status, it used pre-commit that transaction redo log in system global log buffer is moved and it is unlocked before record log on dick. in dali, it has a disadvantage of performance degradation using act consistent checkpoint technique [cit] ."
"most of the aforementioned solutions, though, oversimplify the 2d feature extraction problem, and typically rely on background subtraction approaches or on the fact that image evidence, such as edges or silhouettes, may be easily obtained from an image, or even assume known 2d [cit] ."
"code continents are constructed from the super-cfgs of functions. algorithm 1 shows how we compute a function's super-cfg. we search the function's cfg for direct calls, and at a direct call site we add the callee's super-cfg into the graph through addsupercf g. this function introduces two new edges, one being from the the direct call site(i.e., node i) to the callee's entry, and the other being from the callee's return to the direct call site."
"we train our 3d generative model separately for the walking and jogging actions of the humaneva dataset using the training sequence for subjects s1, s2 and s3. to avoid overfitting we use part of the training sequence exclusively as a validation set. the part weights k i and scale factor β are learnt conjointly on the walking and jogging actions."
"therefore, it is necessary to maintain log information to disk for backup of the database files and transaction processing for recovery. however, log records for recovery and incurred disk io during the course of performing checkpoint are important factor to degrade the performance of the system. therefore, we need the efficient recovery system which can reduce disk io in order to maximize the system performance."
"we perform static control flow analysis to compute the cfgs of all the functions, which are composed of identified icfs and dcfs. our analysis is launched in a conservative way. we try to identify all possible targets for each indirect branch. the main difficulty is resolving the possible targets of indirect jumps. in most common cases, an indirect jump is used to dispatch execution from a jump table, and all legitimate targets of this jump are the corresponding case branches, which can be identified by the previously discovered constants. therefore, for an indirect jump, we check whether or not it is associated with a jump table and connect it to the corresponding case branches if so. as bincfi is able to perform this, we reuse that component to get the desired result to help construct cfgs."
"for the sample above, the root nodes are entry nodes of icfs (1, 5, 5', 8 and 12), the border nodes contain indirect call nodes (2, 6 and 9) and indirect return nodes (4, 11, 13 and 7'). in bincc, the indirect call nodes (2, 6 and 9) are permitted to call the icfs (1, 5, 5', 8 and 12). the indirect returns (4, 11, 13 and 7') are allowed to return back to indirect call sites (2, 6 and 9)."
to update image evidence while 2d observations are used to update the 3d pose. as shown in fig. 1 these strong ties make it possible to accurately detect and estimate the 3d pose even when image evidence is very poor. we evaluate our approach numerically on the humaneva dataset [cit] and qualitatively on the tud stadtmitte sequence [cit] . results are competitive with the state-of-the-art despite our relaxation of restrictions as we do not use any 2d prior and instead use directly raw detector outputs.
"current main memory dbms occurs a lot of disk operations and disk space become less efficient. because it is a way to save a copy of the data space to perform transactions at the checkpoint that has been updated by the dual disk. thus, recovery techniques are very important to maintaining the consistency of main memory dbms through managing transaction log while minimizing the overhead for disk io in main memory dbms. and also, changed data by transaction perform an effective checkpoint to maintain consistency."
"ii indirect return nodes can only go back to the border nodes that represent indirect call sites. the policies are enforced based on the following insights. after the functions are partitioned into two mutually exclusive sets, an icf can only be called by indirect call instructions (i), and due to this, indirect returns from an icf will only go back to indirect call sites (ii). indirect jumps normally have associations with jump tables, where their targets can be determined by static analysis, but there may be some indirect jumps whose targets cannot be known statically. their targets should be constants present in the binary, and they can either be the entries of icfs or call sites (iii)."
"one of the most straightforward approaches consists of modeling the pose deformations as linear combinations of modes learned from training data [cit] . since linear models are prone to fail in the presence of non-linear deformations, more accurate dimensionality reduction approaches based on spectral embedding [cit], gaussian mixtures [cit] or gaussian processes [cit] have been proposed. however, these approaches rely on good initializations, and therefore, they are typically used in a tracking context. other approaches follow a discriminative strategy and use learning algorithms such as support vector machines, mixtures of experts or random forest to directly learn the mappings from image evidence to the 3d pose space [cit] ."
"the fast and efficient processing about complex spatial data has been urgently required in gis applications such as rtls(real time location system) depending on development of geographic information system technology. to achieve this, a study on main memory dbms about that entire database resides in main memory are currently underway. these main memory dbms can lose the whole database by an unexpected glitch such as system error, transaction error, computer malfunction, etc [cit] ."
"mr.rt is main memory dbms created by etri in priority [cit] . this system uses ecbh and t-tree as the indexing system for efficient access to data that resides in the main memory. it is applied bach configuration to configure the new index at system restart. and it used 2pl-pi based static 2pl for concurrency control. recovery system is used fuzzy check point, tuple logging technique that is record log related tuple in order to reduce amount of log."
"in addition, log buffer is used nysimm of stable memory in order to improve the real-time processing. undo logs and redo logs are managed separately to each transaction unit, it reflects only redo log to disk when transaction is committed like dali's recovery technique. c. system m system mis main memory testbed created at university of princeton. this system consists message server to simulate the application, transaction server to transaction management, log server and check point server to recovery management, lock server to concurrency control. database is being loaded in shared virtual memory, log server and check point server provides a diversity of options to seek appropriate recovery algorithm on main memory structure."
"the approach presented in this paper addresses the illposed problem of estimating the 3d pose in single images using a bayesian framework. we use a combination of a strong kinematic generative model based on latent variables with a set of discriminative 2d part detectors to jointly estimate both the 3d and 2d poses. the results we have obtained are competitive with the state-of-the-art in both 2d and 3d, despite having relaxed the strong assumptions of other methods. furthermore, the bayesian framework used is flexible enough to allow extending it further to multi-view sequences, temporal sequences and to handle occlusions."
"in order to handle the complexity of directly modeling p (x), we propose approximating x through a generative model based on latent variables h. this allows us to finally write the problem as:"
"in binaries that contain relocation tables, the table entries, which are constants, cover the addresses of icfs. we check whether the sum of the each constant with the code base address is present in the code section. if so, this sum is resolved as the address of an icf. in binaries that contain no relocation tables, we scan the binary for the constants and handle it differently for non-pic and pic modules."
"in this paper, we propose a new binary only cfi protection scheme bincc, which provides finer-grained protection for x86 stripped elf binaries. through code duplication and static analysis, we divide the code into mutually exclusive code continents. we further classify indirect transfers in each code continent to be either intracontinent transfers or inter-continent transfers, and apply strict c-fi polices to constrain these transfers. to evaluate bincc, we introduce new metrics to estimate the average amount of legitimate targets of each kind of indirect transfer as well as the difficulty to leverage call preceded gadgets to generate exploits, and make comparisons with bincfi. the experiments show that bincc makes great improvement on both aspects with a reasonable overhead."
"we compared the runtime overhead of the programs enforced by bincc and bincfi in figure 13 . in our test environment, the results showed that bincfi increased around 18% percent, while our solution increased around 22% percent, 4% higher than bincfi. the reason why our overhead is a bit higher than bincfi is because we add extra instructions to help instrumentation for direct returns, and perform separate address checking and translating. we believe the overhead is reasonable and acceptable while comparing to the finer-grained protection provided to stripped binaries."
"to evaluate our policy, we introduce new metrics to estimate the average amount of legitimate targets of each kind of indirect transfer as well as the difficulty to leverage call preceded gadgets to generate exploit. as compared to bincfi, the experimental results show that bincc makes great improvement on both these aspects. bincc reduces the legitimate transfer targets by 81.34% compared to bincfi. especially, bincc provides much finer-grained protection for returns and degrades the average legitimate targets by 87%, thereby significantly increasing the difficulty to launch sophisticated rop attacks by leveraging call preceded gadgets. apart from that, bincc has a reasonable performance, 4% higher runtime overhead and 14% less space overhead than bincfi."
"in this paper, we analyzed existing log technique, check point and recovery technique of existing main memory dbms for research of recovery technique on main memory dbms. results of the analysis of these previous studies, in the future, we should be studied effective recovery technique to solve the problem that is duplicated check point processing on the same page and waste of space in fuzzy check point based update ping-pong which is using the majority of main memory dbms."
"cpi [cit], rockjit [cit] also focus on control flow integrity. rock-jit [cit], an extension of mcfi [cit], is able to prevent control flow attacks caused by jited code. it computes the program's precise cfg using the jit compiler's source code and updates the cfi policy when dynamic code is generated at runtime. the work [cit] introduces code pointer integrity and code pointer separation. it can guarantee the program safety by selectively protecting code pointer accesses, which are susceptible to control flow hijacking attacks."
"fuzzy checkpoint technique is difficult to data base consistency during performing of check point. it can be performed simultaneously renew operation transaction and check point about the same main memory data base's page. thus, when system error occurs during performing of check point, database file on disk will not be able to use recovery. many systems is used ping-pong update that have two database files on disk to perform the checkpoint alternately [cit] . looking at you through the process of performing this technique, as shown in figure 1, when the checkpoint occurs, create a process for check point. after then, check the value of check point position recorded in anchor file, check point performs in corresponding data base file. at that time, check point is performed with other transaction asynchronously."
"we propose a finer-grained cfi policy that is able to significantly refine the legitimate targets of indirect transfers. in general, we achieve this by duplicating some necessary code and performing static analysis to separate the binary into several mutually exclusive code continents, and more importantly, assigning each indirect transfer to be either an intra-continent transfer or an intercontinent transfer. this enables us to enforce separate, strict policy to achieve finer-grained protection. in the following sections, we first describe the concept of code continent through a sample and then present our cfi policy."
"we use figure 2 to illustrate code continents that represent the sample code. suppose a binary originally contains the functions, main, foo, bar, qux and start. start is the binary's entry point. the ordinals within graph nodes represent the corresponding instructions in code. in figure, cc1 represents the code continent generated from the super-cfg of main. foo's super-cfg, which is constructed by 5, 6 and 7, is included because foo is directly called at 3. cc2 represents the code continent generated from bar's super-cfg, which has only four nodes. as no direct call site is present in bar, no callee's super-cfg needs to be added in."
"record of the log is essential for recovery in main memory dbms. but at this point, overhead about occurring disk io is important factor to degrade performance of the system due to occupy the much part of entire transaction execution time. existing research to solve that problem can be divided into two techniques; non-volatile memory is used as log storage device and the way to improve the performance of recording the log techniques."
"altibase is main memory dbms created at domestic altibase company, is shown the latest version of 4.3.1. and altibase currently has no.1 domestic market share in main memory dbms. this system solved the problem that is limited space on existing main memory through integrating main memory dbms to disk dbms by hybrid method. also, it provides log technique based on wal, support fuzzy check point based on update ping-pong for recovery."
"cfi related implementations can be generally classified into two categories, namely, source code based and binary only based. since in practice a large number of binaries we face are closed source, we lay more emphasis on binary only based solutions. particularly, we discuss more on two state of the art implementations ccfir as well as bincfi and the possible attacks towards them."
"intra-continent policy constrains indirect transfers whose targets could be statically determined. in this part, we constrain the rest of the indirect transfers in the binary, which correspond to the border nodes of continents. we enforce the inter-continent policy as described in section 3.2 on these nodes. the same instrumentation as described in figure 4 is performed, except that each kind of transfer is given its own address translation table and address translation routine to conduct address checking and translation."
"this policy is to constrain the inner nodes representing indirect transfers whose targets can be determined statically. their targets are always present inside their own code continent and are always determined by the super-cfgs that compose this continent. within a continent, there are only two kinds of indirect control flow transfers we need to be concerned with, one being direct returns and the other being indirect jumps associated with switch-case jump tables. for each direct return, the legitimate targets are its corresponding target call sites within the current code continent. for each indirect jump, the legitimate targets are all the case branches in the corresponding jump table. the case branches in a jump table can be identified by static analysis, and we connect the indirect jump with all its case branches when building super-cfgs, which makes its targets deterministic."
"recent literature proposes two principal alternatives for discriminative detectors: the shape context descriptors built by applying boosting on the limbs [cit], and the hog template matching approach [cit] . for our purposes we have found the hog-based template matching to be more adequate because it matches our joint-based 3d model better as we can place a detector at each joint part instead of having to infer the limb positions from the joints. in addition, the hog template matching yields smoother responses, which is preferable when doing inference."
"to constrain direct returns, we prepare each of them a separate address translation hash table to store its legitimate targets. for each direct return, the targets are the call sites which this direct return is connected with in the super-cfgs. we get those call sites and put them into the corresponding address translation table."
"for the sample illustrated, there is one intra-continent indirect transfer in those continents. it is the return at 7 in cc1. it is only allowed to return back to the call site 3 (or say the instruction 4)."
"for a pic module, functions can be reached by pc thunks. therefore, in addition to performing the same approach as for non-pic modules to collect constants, we identify all the pc thunks and see whether they are used to have access to functions. if so, we consider those functions as constant function pointer candidates."
"suppose ij is an indirect transfer of a specific kind and tj is the legitimate target set for ij in a cfi enforced binary, and n represents how many indirect transfers of this kind exists in the binary. we calculate this metric for three kinds of indirect transfers and illustrate the percentage reduction of legitimate targets, given by"
"we divide the graph nodes contained in a code continent into three categories: root nodes, border nodes and inner nodes. root nodes represent entry points of indirect called functions, and they are represented in grey in the figure. by dividing the nodes, we are able to classify indirect transfers into two categories, intra-continent transfers and inter-continent transfers. intra-continent transfers are the indirect transfers originating from inner nodes, while inter-continent transfers originating from border nodes. more importantly, we guarantee that each indirect transfer is either an intra-continent or an inter-continent transfer and thereby enforcing separate, strict policies. this is achieved by performing code duplication before code continent construction."
"with regard to the problem of directly predicting 2d poses on images, we find that one of the most successful methods is the pictorial structure model [cit] (later extended to the deformable parts model [cit] ), which represents objects as a collection of parts in a deformable configuration and allows for efficient inference. modern approaches detect each individual part using strong detectors [cit] in order to obtain good 2d pose estimations. the deformable parts model has also been extended to use 3d models for 3d viewpoint estimation of rigid objects [cit] ."
these parameters are learned by translating and rotating random pose samples from the 3d training set and using them as negatives such as those seen in fig. 5 . the parameters are optimized over the difference of the logarithm of expectations of the score from eq. (3):
"to evaluate the ability of bincc in preventing rop attacks, we introduce gs (gadget survivability) to represent the difficulty for attackers to leverage call preceded gadgets while establishing rop chains for exploits under cfi protection. we define this metric as follows."
"as a prerequisite to use this technique, concurrency control technique should be used to ensure log's disk output that is completed in order after the start of commit task. for example, if you are using the strict 2pl(2-phase locking), dependence between transaction is maintained and even the resiliency about failure of the system can be guaranteed. because it does not occur transaction commit than one's depending transaction in advance even if it apply pre-commit."
"future work includes handling occlusions -a weakness of approaches based on 2d detectors-, and improving the handling of rotations by learning a prior distribution and incorporating it in the model. more exhaustive research on different graphical models that can better represent human poses and a deeper analysis of the hyper-parameters chosen are also likely to improve the current method. detection results for subject s2 in the jogging action for all three cameras. for this specific action, most of the error comes from only allowing the 3d shape to rotate around the vertical axis. we show two specific frames corresponding to large and small error. the ground truth is displayed in green and the estimated 3d pose in red. on frame 131 we can see that, although the 2d detection is quite accurate, the 3d solution shows large amounts of error due to the strong inclination of the pose. in contrast, frame 271 is accurately detected."
"as we use a similar infrastructure to bincfi to manipulate binaries, the reason for file size increase is as the same as that for bincfi, due to the new code section for instrumentation and new read-only data sections for address translation hash tables. the new code size increase 1.4 times the original code size, while the figure for bincfi is around 1.2. we introduce four kinds of tables for storing targets of indirect calls, indirect jumps, indirect returns and direct returns, while bincfi uses two kinds for storing indirect calls/jmps and returns. the total introduced tables size decrease around 20% of the figure for bincfi. overall, the space overhead is around 125% of the original binary size, 14% lower than the figure for bincfi. the reduction of tables size contributes to the improvement of space overhead as compared to bincfi. according to bincfi's implementation, the hash table size is a power of 2 and is relevant to the number of legitimate targets, so the table size decreases greatly as we refine the legitimate transfer targets size."
"we simulated three haploid sequencing data sets from 1 mbp of human (chr22:28,000,000-28,999,999) using 100 bp single ended reads, each giving 100x coverage. first, we generated 'perfect coverage' -an error-free read starting at every base. second, we generated 'stochastic coverage' -read starts distributed uniformly across the 1 mbp genome. third, we generated 'reads with error' -stochastically sampled reads with a uniform 0.5% rate of single base errors."
"in summary, our proposed algorithm utilizes the data of all virtual elements, a toeplitz matrix instead of the spatial smoothing operation to decorrelate the equivalent sources, and a root-music algorithm to avoid costly angle grid search. the new algorithm achieves better doa estimation performance and reduces computational complexity."
"we start by describing the de bruijn graph and its benefits compared to the string graph. we then describe an augmentation (ldbg) that allows long-range information to be kept. theoretical results and simulations are used to characterise its properties. we demonstrate its value by application to variant discovery and characterisation of genomic context for drug resistance genes in klebsiella pneumoniae. finally, we consider the possibility of using such structures for regular analysis of human-scale genomes."
"in constructing a ldbg we are effectively compressing reads against the de bruijn graph. however, since read start/end positions are not important for assembly we do not store them, so although it is possible to recover the underlying genome (losslessly) through assembly, it is not possible to recover the original set of input reads."
"to assess ldbg on real data, we examined short read data from klebsiella pneumoniae, a gramnegative bacteria that usually lives harmlessly in the mouth and gut of humans. however in the event of a weakened immune system, it can establish pathogenic colonies in the lung leading to inflammation and bleeding. it is also found in some cases of urinary tract infections. antibiotic resistant strains of k. pneumoniae have been found in patients. we used mccortex for two tasks where long-range information is likely to be beneficial -finding large differences from a reference and analysis of genomic context for drug resistance genes, which we validated using a pacbio reference assembled for the sample [cit] ."
"we have implemented the ldbg data structure and associated algorithms as part of mccortex, a modular set of multi-threaded programs for manipulating assembly graphs written in c. mccortex supports fasta, fastq, sam, bam & cram file formats and is released under the mit license."
"a ldbg is a lossless representation of a genome when generated from error-free reads, as long as the genome starts and ends with unique k-mers, there are no k-mer coverage gaps and each repeat is spanned by at least one read (proof in online supplement). this is true regardless of the value of k."
"one shortcoming of the proposed array geometry is that the new array relies on the structures of the known subarrays, and therefore not all gnsa geometries with any number of sensors can be obtained. for example, when the total number of sensors is a prime number, it is impossible to construct a gnsa. however, the new array geometry provides a closedform solution to generate a suboptimal coarray, and can easily provide a larger array employing the known mra. in addition, owing to the structure of multiple identical subarray, the gnsa enjoys some advantages in real applications, such as low cost and easy extension to a larger array, etc."
"various non-uniform linear array design methodologies have been reported in the literature, for example, the minimum redundancy array (mra) [cit], the nested array [cit], and the coprime array [cit], etc. an mra is a linear array designed to minimize the number of array elements by reducing the redundancy of the element spacing. the virtual array derived from an mra is a hole-free ula with the largest possible aperture for a given number (k ) of physical sensors."
"unfortunately, no closed-form expressions for the sensor positions and for the number of achievable dof as a function of k have reported in literature. other relatively ad-hoc approaches exist, for example, design of mras for k 17 sensors has been found by exhaustive search routines [cit] . ishiguro [cit] discussed a method of using a combination of two mras to find an mra for a large number of antennas. however, only a constructing method with several combinations of the two mras is given. therefore, a systematic mathematical tractable design approach for larger mras is lacking."
"multi-colour ldbgs can be constructed by building single sample ldbgs and loading them together into mccortex. for graph traversal tasks, such as assembly, we only store a single bit per sample per k-mer and per sample per link to record which k-mers/links are present in each sample. these are stored in a packed bitset. graph traversal of a colour through a multi-coloured ldbg proceeds as per for a single-sample ldbg, only using links and k-mers of the given colour. at coverage gaps, traversal can fall back to using any k-mers in the graph (but not other colour's links)."
"from (28) we know that noise only exists at the (l v + 1)-th element of z, i.e., z l v +1 . therefore when considering noise, y can be expressed as"
"first, we obtain the observation data of the dca by exploiting the data of all virtual elements, instead of removing the repeated ones. then we impose a toeplitz structure matrix on the observation data to decorrelate the sources. this toeplitz matrix is constructed by reshaping the observation data and thus no multiplication is required. finally, a polynomial root music algorithm is employed to calculate the doa, which avoids the angle grid search in the standard spectral music [cit] . the use of the toeplitz matrix and root-music algorithm make the computation of our approach more efficient than the ss-music algorithm [cit] . to be more specific, we analyze the computation of both ss-music and our method in section vi-d, and the corresponding comparison results are enumerated in table 5, which shows the superiority of our method clearly."
"contigs harbouring the kpc sequence within the 21 isolates were identified by aligning to the kpc-2 allele sequence with lastz [cit] and extracting the longest such contig from each assembly. these were aligned back to both the reference data sources and the validation data [cit] . for alignments that ran off the end of a sequence owing to the circular nature of the plasmids, we attempted to shift the contig sequence such that a linear alignment of maximum length was achieved; where this was not possible we have reported the length of the aligned region. the contig selected from each assembly was evaluated for correct kpc allele recovery, correct identification of plasmid background (i.e. sequence context of kpc allele), and mismatches/gaps to the relevant reference sequence. these results are shown in table 1 ."
"in principle, the virtual array is constructed by vectorizing the covariance matrix of the received data collected from a properly designed non-uniform linear array [cit] ."
"4) perform evd on y (30) or r ss (29) to obtain the noise subspace and signal subspace. since the dimension of r ss is the same as the dimension of y, the computational complexities of evd on y and r ss are the same, which is o (l v + 1) 3 . 5) doa estimation. the roots of a root-music polynomial is used to estimate the doas of the sources in our algorithm. the main computational costs focus on the coefficients and roots computations of the polynomial f music (r) (32), and they are [cit], respectively. the computational cost can be much lower when using some real-valued polynomial [cit] or multi-taper root music method [cit] . for the spectral music algorithm [cit], the computational complexity is o (j (l v + 1) (l v + 1 − q)), where j is the number of angle girds. j will be very large for a large angle scope and/or a fine angle grid."
"without link information, we find that in 57% of cases the plasmid background on which the kpc allele resides cannot be identified. in such cases, lastz reports alignments of the short contigs with 100% sequence identity to plasmids 1 and 2. moreover, for the cav1360 isolate, the aligner determines the background incorrectly as the e. coli plasmid due to the presence of kpc-3."
"the location set v v of the virtual array generated from a gnsa can be calculated by (8) . substituting (11) into (8), we obtain"
the proof is provided in appendix c. proof: the proof for the dof is obvious using the commutative law of multiplication because both numbers of the dof associated with the two gnsas are equal to
"we have implemented a very simple read mapping, which trusts all k-mers from a read if there is a perfect match in the graph, and which only attempts to fill gaps using linear time graph traversal. optimal mapping is ultimately np-hard, but more advanced heuristic methods are available which may perform better than our approach [cit] . improvement may be most noticeable for high error rate sequencing data and in low complexity regions of the graph. finally, there is scope for reducing memory consumption, given very few k-mers actually have links attached (see supplementary figure 6 ) and could be further reduced with better encoding in memory of the junction choice tree held by a k-mer (i.e. l(v)). for example, using a binary encoding of the tree of junction choices, or generative path encoding proposed to compress sequence data [cit] ."
"as links should provide useful guidance to navigating junctions in a graph, we examined their utility in calling large variants (insertions or deletions greater than 100 bp in length). we implemented a \"bubble caller\" (named for the characteristic motif produced by a biallelic mutation in a graph wherein paths diverge from one k-mer and rejoin at another) and tested it by calling variants in cav1016, a k. pneumoniae isolate for which a high-quality pacbio sequence was available for validation. we constructed dbgs of the canonical reference sequence (gcf 000016305.1 asm1630v1) and illumina data for cav1016. from these, we built ldbgs using the single-end illumina reads for link construction. we applied our bubble caller to the dbg and ldbgs, allowing for a minimum event size of 100 bp and maximum of 200 kbp, and removing duplicate events. we validated called alleles by aligning the reference and alternate alleles to the canonical reference sequence and cav1016 pacbio sequence, respectively. the resulting callsets without and with link information are presented in supplementary table 1 . our bubble caller discovered 55 large indels in the dbg and 59 indels in the ldbg. all 55 variants from the dbg callset were recovered in the ldbg callset. the four remaining variants exclusive to the ldbg callset are all insertions of varying size (134, 246, 7, 952 and 11, 946 bp) ."
"where m r and n r are the number of redundancies for an m -sensor mra and an n -sensor mra, respectively. substituting (18) into (14), we obtain"
"fm-index construction has high memory requirements for high coverage data sets as all reads must be loaded into memory. this requires building indices for subsets of the reads and recursively merging the indices [cit] ) . in comparison, dbg memory is mostly determined by genome size, with only the errors in high coverage data causing a slower increase in memory requirements. similar memory efficiency advantages are seen when building multi-colour dbgs from the same species."
"links are cleaned by building a tree of links l(v), and trimming junction choices with coverage below threshold t. this link cleaning threshold is determined by applying the same model as used for k-mer cleaning to the link coverage distribution of the first junction choice of all links."
"due to the double stranded nature of dna and the fact that we don't know which strand a read originated from, storing all k-mers from reads results in k-mers occurring separately in the graph in both their forward and reverse complement orientations. to overcome this it is common to store only the lexically lower of each k-mer x and its reverse-complementx [cit] . requiring that k is odd prevents a k-mer from being its own reverse complement (a dna palindrome). when visiting a vertex in the de bruijn graph we can visit it in its forward or reverse complement orientation. the orientation in which we arrive at it determines if we leave by its out-or in-edges (forward, reverse respectively)."
"while increasing k can overcome the problem of short repeats, it also has the effect of reducing the number of k-mers given by each read and increases the number of k-mers lost to each sequencing error. both these effects reduce k-mer coverage, which is determined by the k-mer size, the read length and the error rate [cit] . as k-mer coverage drops, read overlaps are lost and gaps in coverage increase. together with tangles, coverage gaps interrupt assembly and shorten contigs."
"recently there has been work on implementing low memory dbg construction [cit] and representations [cit] . these have both provided great improvements over the naive hash table based implementation, extending the contexts in which dbgs can be used."
"are the location set forming the virtual array of subarray b and subarray a, respectively. thus, the location set of the virtual array generated from a gnsa can be calculated by the cross summation of v b and v a . we have defined in section ii that the location set d of the dca of a linear array only contains the unique elements of the location set v of its virtual array. we denote the location sets of the dca associated with the gnsa, subarray b and subarray a as d v, d b and d a, respectively. the dcas of both subarray b and subarray a are holefree ulas with symmetric property in our definition. therefore, d b and d a can be expressed respectively as"
"2) compute the observation data z (28) of the dca. we use a reduced dimensional matrix t (27) to compute z. the matrix t can be determined once the element locations of the original array are given, and every column of t is a unitary vector. the essence of (28) is to average the amplitudes of the data of the repeated virtual elements, and it just needs one multiplication to compute the average value. because the number of repeated elements in the virtual array is k 2 − f v, the maximum number of multiplications is k 2 −f v to obtain z from z. for ss-music algorithm, the repeated elements are removed, and hence no multiplication is needed. however the signal power of these repeated elements is lost, and the snr of the observation data will be decreased simultaneously for ss-music algorithm."
"if the number of sources q is less than l v + 1, the rank of matrix r ss can be restored [cit] . therefore the music algorithm [cit] can be applied to r ss for doa estimation. it has been shown that the ss-music algorithm can resolve more sources than sensors, and can achieve a high angular resolution [cit] . however it involves the average of several covariance matrices, which implies a high computation cost. hereby, we employ a computationally efficient method to decorrelate the signals. we directly use the observation data z to construct a toeplitz data matrix y. it can be proved that y is a full rank matrix and it share the same signal subspace with r ss, that is the coherency problem of the equivalent sources can be solved by y. therefore we can directly use y, instead of the spatially smoothed matrix r ss, to estimate the signal subspace and noise subspace, and subsequently perform doa estimation."
"one shortcoming of long links is the accumulated probability of encountering an error during traversal. if a link takes the wrong branch of an error-induced bubble, cleaning that junction choice trims off all the remaining information about the junction choices made beyond the bubble. this shortcoming results in link coverage dropping off quicker than expected as links get longer, resulting in truncated links. this could be addressed by error-correcting groups of links that start at the same k-mer."
"the proposed array is composed of n identical subarrays, denoted by subarray a, and the locations of these subarray a follow the geometry of another subarray denoted by subarray b."
in this section we conduct simulation experiments to evaluate the doa estimation performance of the gnsa using our proposed doa algorithm and the ss-music algorithm described in section vi.
"bearing the above concerns in mind, in this paper we propose a unified array geometry named generalized nested subarray (gnsa) with desired array properties and develop a new algorithm for the underdetermined doa estimation based upon the proposed gnsa. the contributions of this paper are threefold."
"when the snr is low or the number of snapshots is small, the doa estimation performance of the root-music degrades. we can adopt some approach such as pseudo-noise resampling [cit] to improve the estimation performance."
"our method is useful for reconstructing complex loci across multiple samples using a common panel of pre-determined haplotypes. link information derived from a haplotype panel cannot add k-mers or edges to the graph that were not observed in the original dataset. nevertheless, assembly is enhanced in regions where the links are consistent with the graph, and naturally defaults to link-uninformed navigation in regions of discrepancy. threading a panel of haplotypes from multiple samples through each graph thus identifies only the relevant sections of each donor haplotype."
"from (5) and (6), we observe that the vector z is equivalent to the data received by a virtual array with elements located at the location set"
"a diversity of algorithms [cit] have been proposed to resolve more sources than sensors using the virtual array derived from a non-uniform linear array, i.e., to solve the underdetermined doa estimation problem. one representative approach is the spatial smoothing music (ss-music) algorithm [cit] . a disadvantage of this approach is that not all the elements of the virtual array are used when performing doa estimation, which may result in a reduction of the signal to noise ratio (snr). another representative approach is the sparse signal reconstruction method that takes advantage of the fact that the signal spectra are sparse [cit] . however, algorithms used to implement this approach typically suffer from heavy computational workload."
"the standard spectral music algorithm [cit] can be directly applied to y or r ss to perform doa estimation. however the spectral music algorithm needs an angle grid search, which leads to a high computational cost especially for a large angle scope and/or a fine angle grid. here we apply a polynomial rooting method to perform doa estimation. this method does not need costly angle grid search, and hence reduces computational complexity."
"although the new doa estimation algorithm is designed for the gnsa, it also works for any non-uniform linear array whose dca is a hole-free ula, e.g. the mra [cit] and the nested array with two levels [cit] . when the new algorithm is applied to a coprime array whose dca is not a holefree ula, the reduced dimensional matrix t can also be constructed by (27) . however, the toeplitz matrix and the root-music algorithm can only use the ula part of the dca generated by a coprime array."
gaps between paired-end reads are treated like gaps in reads caused by sequencing errors. ldbg naturally captures information from paired end reads once the insert gap is filled. links can be generated in two passes: first with single-end reads against a dbg to create a ldbg; then with paired-end reads against the ldbg. this allows the single-ended read links to be used to aid traversal between read pairs.
"in the proposed unified array geometry, the subarray can be an mra, a nested array, a ula, or any other linear arrays with hole-free dcas. therefore, there are multiple combinations of array geometries. in this section, we will analyze the characteristics of the gnsa with different array geometries using the properties presented in section iii-b."
"the nested array with more than two ula subarrays [cit] has less mutual coupling by increasing the inter-element spacing of last few subarrays. but the virtual array generated from the nested array is not a hole-free ula, which may cause difficulties with spatial smoothing based doa estimation."
picking a value for k is ultimately a trade-off. it is common to run analyses multiple times with different values of k and pick the best results according to a quality metric (e.g. assembly n50 or number of variants called) [cit] . alternatively the genome and read data can be sampled to estimate which value would be optimal [cit] .
"throughout the paper, matrices and vectors are represented by capital letters and lower case letters in boldface, respectively. superscript (·) t, (·) * and (·) h respectively denote the transpose, the conjugate and the conjugate transpose of a matrix or a vector. statistical expectation is denoted by e(·), and vec(·) is the vectorization operation that stacks all columns of a matrix into a vector. table 1 summaries the abbreviations used in the paper."
in this section we develop doa estimation algorithms applied to the virtual array of a gnsa to resolve more sources than sensors. recall the equivalent source signal
"next, we compared assembly performance between our ldbg implementation and the string graph assembler (sga) [cit] . as stated above, string graphs are notable as their construction is based on the direct computation of read-to-read overlaps, facilitated by an fm-index on the reads. similar to our work, sga is able to use the full length of the read during assembly and should thus be able to assemble repeats shorter than a read length. however, sga differs from our work in some key ways. overlaps between reads need not be perfect, but rather are parametrised at run time to accept a minimum overlap of τ min bases and maximum error rate max . additionally, sga attempts to correct sequencing errors, rather than discarding them completely. in practice, these two factors may yield longer contigs, but at the expense of accuracy [cit] ."
"direction-of-arrival (doa) estimation is an important topic in various applications, such as radar and sonar. it is well known that the maximum number of sources that can be resolved by a k -element uniform linear array (ula) using traditional doa estimation methods, such as music [cit] and esprit [cit], is k − 1. the underdetermined doa estimation problem, i.e., resolving more sources than sensors, has received considerable interest in recent years [cit] . an effective approach to solve this problem is to increase the number of degree of freedom (dof) by designing an equivalent virtual array (see, e.g., [cit] ). the number of dof is an important array design criteria because more dofs mean more sources can be resolved by the array system."
"when designing an array for which it is possible to resolve more sources than sensors, the following features are desired: (a) a large number of dofs and a large aperture, which means low redundancies in the virtual array, (b) a closed-form expression for sensor positions and the number of achievable dof, (c) a dca that is a hole-free ula, and (d) a configuration that reduces mutual coupling by preventing elements from being too close to each other. the proposed gnsa array geometry, especially the nmra, described in next section meets all these design criteria."
"that the ldbg-exclusive events should all be insertions (particularly large ones) is perhaps not surprising; in a graphical framework, calling insertions against a high-quality reference sequence with comparatively lower quality illumina data is expected to be more difficult than calling deletions. with insertions, sequencing error in the study sample will produce spurious paths in the graph, not all of which can be removed successfully, and thus graph traversal from the 5' to the 3' end of the alternate allele has many opportunities to fail. with deletions, the graph navigation burden is on the reference allele which should have substantially fewer errors (and thus fewer spurious paths) to confound traversal. link-informed traversal helps alleviate this insertion/deletion detection bias, enabling the recovery of large events like the 8 and 12 kb events listed above. this improves our access to large variants underrepresented in current variant call sets [cit] b) ."
"if both subarray b and subarray a are nested linear array, we will obtain a nested subarray of ''na + na''. fig. 3(f) gives an example of ''na + na'', from which we can see that the array geometry is composed of several nas. this geometry is different from the nested array with multiple levels [cit] which consists of multiple level ulas with an increased inter-element spacing in each level. although the na with more than two levels can obtain more degrees of freedom, its difference coarray is not a hole-free ula [cit] . therefore in our paper we will only consider two level nested array whose dca has no holes, for a fair comparison. the ''na + na'' structure is more suitable for distributed array application owing to its multiple subarray structure."
"this paper proposes a robust structure identification method (rsim) based on incremental partitioning learning. rsim starts with an open region (initial domain) that covers all input samples. the initial region starts with one fuzzy rule without fuzzy terms and then evolves through incremental partitioning learning, which creates many subregions for system error minimization. the three major contributions of the proposed rsim are as follows: it locates sufficient splitting points provided through a robust partitioning technique, determines the optimum trade-off between accuracy and complexity through a novel partition-selection technique, minimizes global error through global least square optimization. these contributions offer many remarkable advantages. first, rsim provides a solution for the curse of dimensionality. second, rsim can also be applied to low-dimensional problems. third, rsim seeks to produce few rules with low number of conditions to improve system readability. fourth, rsim minimizes the number of fired rules. therefore, rsim can achieve low-level complexity systems. three low-dimension and six high-dimension and real-life benchmarks are used to evaluate the performance of rsim with state-ofthe art methods. although rsim has high interpretability, the results prove that rsim exhibits greater accuracy than other existing methods."
"de novo assembly offers a means to overcome some of the limitations of reference-based analyses. rather than aligning reads to a reference, reads are aligned to one another. these alignments are encoded in a graph data structure, a collection of \"vertices\" encapsulating sequence data and \"edges\" representing overlaps of different sequences [cit] . graphs from different samples (and any reference) can then be compared to discover variation directly [cit] . should the variation be in a locus unrepresented in the reference genome, the graph-based comparison can still capture the event [cit] ."
"if subarray a is a ula, the gnsa is a repetition of multiple ula subarrays. this kind of array has been widely studied [cit] because the array aperture can be easily extended without causing significant increase in software and hardware costs. however, there is no criteria for the intersubarray spacing. in our proposed gnsa, we properly design the inter-subarray spacing to form a hole-free dca, and derive closed-form expressions for the sensor positions and the number of achievable dof."
"there are a small number of sensor pairs spaced by d in an mra, e.g., four at most for mras with less than 17 sensors [cit] . the nmra is composed of multiple mra subarrays and thus an nmra may have fewer closely spaced elements than the nested array. let m denote the number of sensor pairs, which are separated by d in an m -element mra. then the nmra constructed by n identical m -element minimum redundancy subarrays has mn pairs of sensors separated by d. we plot the number of sensor pairs separated by d in the nested array (na) and the nmra with some total number of physical sensors k in fig. 5 . we see that the number of sensor pairs m 1 in the nested array is proportional to k, and these numbers are larger than the number of sensor pairs in the nmra. thereby the nmra is subject to lower mutual coupling than the nested array with same number of sensors. at the same time, the nmra has a larger aperture and more dofs than the nested array which will be shown in fig. 6 and is discussed next."
"our design assumes the structures of subarray a and subarray b are known. we denote the parameters of the two subarrays in table 2 . next we derive the properties of a gnsa using the parameters of its components, subarray a and subarray b."
"in order to overcome the limitations of the existing underdetermined doa estimation algorithms, we develop a new doa estimation algorithm for the gnsa. the new algorithm has achieved better doa estimation performance than traditional ss-music algorithm with lower computational cost. we demonstrate the superiorities of the proposed array in resolving more sources than sensors, doa estimation performance, and probability of resolution using the new algorithm over the ss-music algorithm."
"de bruijn graphs are used in many areas of sequence analysis, including in mapping-based calling, as in the local alignment step of the variant caller platypus [cit], in de novo assembly as in velvet [cit] and abyss [cit], and in de novo assembly for variant calling [cit] ."
"lowering the value of k in a dbg raises k-mer coverage and reduces coverage gaps but it also reduces the length of the longest repeats that can be traversed. if we improve the ability to resolve repeats with links, we hypothesised that we should reduce the assembly performance's sensitivity to the parameter k. therefore we simulated an assembly task with different k values."
"in this section, we use the nmra as an example of the proposed gnsa to compare its characteristics with other arrays, including the mra, the nested array and the coprime array."
"the most common sequencers in use today (second-generation) produce tens of millions of short reads (typically 75 to 150 bp in length) per sequencing run [cit] . it is common to assemble such data using a so-called \"de bruijn\" graph approach [cit] . vertices are constrained to be fixed-width substrings of length k (or \"k-mers\"). edges represent observed sequence adjacencies in the reads. with sufficient coverage, overlaps are implicitly encoded because two reads which overlap will share k-mers. thus the graph is built up one read at a time at the cost of storing the graph in memory. graphs of multiple individuals can be compared in memory [cit] . however, there is a penalty for this approach: long-range information in the read is sacrificed. this is particularly problematic as genomes tend to have many repetitive regions and without context it is often not possible to determine the origin of a random k-mer [cit] . however, as k increases, so does the specificity of its location. string graphs address the issue of storing long-range information by avoiding the read fragmentation step and instead find explicit overlaps between reads. unfortunately string graphs are not well suited to multi-sample comparison and have a high per-sample memory cost [cit] ."
"next, we compare the number of achieved dof from the dca of the gnsa. let the total number of sensors be some integers from 9 to 36, and choose the optimal number of sensors in the subarrays of different array geometries as listed in table 4 . the number of achievable dof and the aperture length for different array geometries can be obtained using the equations in table 3 . the comparison results are shown in fig. 4 . we can see that the ''mra + mra'' structure can achieve the maximum number of dof, the ''ula + ula'' structure has the minimum number of dof, and the numbers of dof from other array geometries are somewhere in between. this feature gives us more options to choose an appropriate array geometry from these combinations based on practical applications."
"3) decorrelation of the equivalent sources. the toeplitz matrix y in (30) is used to decorrelate the equivalent sources in our algorithm, and it is just the reorganization of z. therefore no multiplication is required. however, the computational complexity of r ss in (29) is"
"are the result of sequencing errors near the end of reads and gaps in coverage. tips are removed if they are shorter than a user specified value, the default being k, the maximum number of erroneous k-mers generated by a single-base sequencing error near the end of a read."
"our doa estimation algorithm has advantages of reducing the computational cost. in this subsection we analyze its computational complexity and compare it with the ss-music algorithm. the doa estimation algorithms can be divided into the following steps: 1) compute covariance matrix r xx (4) and construct the data vector z (6) of the virtual array. it needs o(k 2 t ) complex multiplications to compute r xx . since the data vector of the virtual array can be obtained by vectorizing the covariance matrix, it does not need multiplication. these steps are the same for the two algorithms."
"the computational complexities of the proposed algorithm and ss-music are summarized in table 5 . two examples of 12 and 24 physical sensors are also included, where j is set to obtain 20 angle gird points in a half power beam width of an array. it can be concluded from table 5 that the computational complexity of our algorithm has been reduced significantly, i.e., it is around 1/3 of that using the ss-music algorithm for the two examples."
let ⊕ denote the cross summation of every element in u n and every element in u m . in this way the sensor positions of the entire array may be expressed by the vector
"as with a de bruijn graphs we can look up any k-mer or edge between k-mers in time o(1) and we can start graph traversal from any k-mer. as in a multi-coloured dbg, a multi-coloured ldbg stores which samples have which k-mers and links."
"therefore it can be concluded that an nmra can provide o(m 2 n 2 ) dofs using only mn physical sensors when subarray a and subarray b are both mras. fig. 2 is an illustration of 12-element nmra. we will compare the characteristics of the nmra with the mra, the nested array and the coprime array in section v."
"in this paper we proposed a unified array geometry, dubbed generalized nested subarray (gnsa), which can be easily constructed by the cross summation of two subarrays. it is possible to predict the sensor positions and the number of dof when these parameters of the subarrays are given. we have derived the aperture length of a gnsa, the number of dof in its dca, and have unveiled the parameter relationship of a gnsa with its subarray configurations. we have used the nested mra (nmra) as an example to compare its characteristics with the nested array and the coprime array. the simulation results have demonstrated that the nmra has a larger aperture as well as a higher number of dof than the two-level nested array and the cacis with the same number of physical sensors. furthermore, the nmra enjoys lower mutual coupling than the nested array."
"since it is constructed from the reads without breaking them up, a string graph is an assembly graph that stores all the connectivity information contained in the single-ended input reads [cit] . string graphs do not naturally lend themselves to storing information on read pairs, although one such data structure has been proposed [cit] ."
"proof: the proof is provided in appendix e. from theorem 1 we know that y is a full rank matrix, which has no connection with the coherency of the equivalent sources. therefore, it is possible to estimate the signal and noise subspace correctly with y."
", which is composed of the powers of the actual sources. all elements in c are real values, and hence the rank of its autocorrelation matrix is one. the equivalent sources are considered fully coherent sources to the virtual array. the traditional subspace-based doa estimation algorithms, such as music and esprit, cannot be directly applied to the virtual array to estimate doas. various algorithms [cit] have been proposed to decorrelate the sources and to perform doa estimation. however, as we pointed out earlier in section i, the ss-music algorithm [cit] does not use all elements of the virtual array, which may degrade the performance of doa estimation. in addition, sparse signal reconstruction based algorithm [cit] usually requires a heavy computational workload because it is based on the signal spectra sparsity which is typically divided into numerous small grids to maintain the accuracy. to overcome these limitations, we propose a new doa estimation algorithm for the virtual array in this section."
"a string graph is an assembly graph where the vertices represent the input reads and the edges are maximal non-transitive overlaps between them [cit] . the set of reads is reduced to remove reads contained within other reads. a naive string graph implementation would take o(n 2 ) time to compare all pairs of reads to find overlaps, before removing contained reads and transitive edges. [cit] showed that it is possible to construct a string graph in linear time, by first generating an fm-index of the input reads r and an fm-index of their reverse r . alternatively a single index can be constructed containing r andr [cit] ."
"in this location set v of the virtual array, there are in total k 2 elements, but some locations may be repeated. more specifically, there may exist multiple pairs of"
"where diag(w) extracts the diagonal elements of w. equation (25) shows that there exist several ''1''s located at the same column, which correspond to the repeated elements in the location set v. the repeated elements are deleted when performing doa estimation with the ss-music algorithm [cit], which may lead to a reduction in snr. here, we use a reduced dimensional matrix to obtain the observation data of the dca using the data of all elements of the virtual array. the reduced dimensional matrix is constructed by"
"each k-mer stores which samples it was seen in. we refer to c as colour, a generic term that can mean a distinct individual, pooled population or a specific data set on a single individual (e.g. tumour/normal), depending on analysis context. we shall refer to this structure as a multi-colour de bruijn graph."
"we have presented a de novo assembly method that addresses the most important limitation of de bruijn graphs: the ability to leverage long-range connectivity information inherent in read data. while cutting reads into small k-mers has long been a useful way of simultaneously computing readto-read overlaps and overcoming high rates of sequencing error, increasing sequencing quality and read lengths have rendered de bruijn assembly methods less attractive. string graphs have been successful in incorporating long-range data into assemblies, but sacrifice desirable computational properties of de bruijn graphs. our solution, linked de bruijn graphs, combine the connectivity properties of string graphs with the rapid lookup of specific (multi-coloured) k-mers. due to the wide range of uses of dbgs in sequence analysis, we believe this offers a potential improvement to many existing algorithms. path encoding of reads has been suggested for read compression before [cit] . however we believe this is the first implementation to use it for multi-colour assembly that can scale up to large mammalian genomes on modern computer hardware."
"the rest of the paper is organized as follows. the signal model is presented in section ii. the proposed array geometry and its properties are developed in section iii, and the array characteristics of the gnsa with different array geometries are described in section iv. in section v, we use the nmra as the representative to compare its characteristics with the nested array and the coprime array. in section vi, the doa estimation algorithm is developed for the proposed array. section vii presents numerical examples. section viii concludes the paper."
"finally, we show that with links, we can use a panel of reference contigs derived from multiple sources to improve drug resistance locus characterisation in k. pneumoniae isolates. as the underlying graphs are considered immutable after construction, links derived from this panel cannot add k-mers to a sample. we hypothesised that the links panel could still provide valuable connectivity information where they were consistent with the graph without misleading the assembler in regions where they were divergent. we selected 21 k. pneumoniae isolates with known drug resistance status and that carry combinations of two alleles and two plasmid backgrounds at the k. pneumoniae carbapenemase (kpc) resistance locus, see table 1 . as references, we constructed links from a panel of four plasmid backgrounds carrying three different kpc alleles: pacbio sequences from two of the 21 isolates (carrying allele kpc-2), a kpc-harbouring plasmid from e. coli (carrying allele kpc-3), and a fourth k. pneumoniae plasmid known to harbour a resistance allele and background absent from the 21 isolates (carrying allele kpc-5). all accessions are described in supplementary table 2 . two assemblies were generated per isolate: one without links, and one with links."
"for which (v m − v l ) are the same. in this case, multiple virtual elements are associated with the same virtual sensor location. we refer to the repeated elements as redundancies. the number of redundancies in a virtual array is determined by the structure of the original array. a higher redundancy implies fewer distinct elements in v. the locations of a virtual array are symmetric due to the fact that for any"
"the proof is provided in appendix a. this proposition reveals the relation between the dca of a gnsa and the dcas of its components, subarray b and subarray a. according to proposition 1, the locations of the dca of a larger gnsa may be calculated using the locations of the dcas of the constituent two subarrays. proposition 1 also inspires us to compute the aperture length and the number of dof of the gnsa using parameters of the subarrays a and b. the following proposition specifies the gnsa."
"we have shown that read error correction and graph annotation can improve assembly performance of de bruijn graphs and that this can be seen with the recovery of large (12kbp) events in short read sequences. moreover, through application to real data we have shown that links can be generated from a wide range of sequencing technologies including data not used to construct the underlying dbg, and that this can be exploited to identify sequences of biological interest. ldbgs can also naturally represent paired-end connectivity information. we have proved that in the error-free setting, linked de bruijn graphs losslessly store the genome sequence, even when constructed from short reads and agnostic of k."
"in order to compare the estimation performance of our algorithm with the ss-music algorithm, we use monte carlo simulations to evaluate the average root-mean-square error (rmse) of the estimated doas as a function of snr. the"
"one mitigation of this inadequate reference problem is to augment the reference with known variation and alternative alleles to improve read mapping [cit] . such approaches commonly convert flat reference genomes into a graph structure, effectively mapping reads to all references simultaneously and choosing the path that best fits the data. [cit] found that mapping to a reference graph instead of flat contigs led to a 22% increase in the number of reads that map uniquely."
that intersects the peripheral zone around the cell. the practical use of this feature is described for the first time in this study when it will be used for automatic recognition purposes (see figure 4 ).
"granulometry is a texture feature that measures the particle size distribution in an image by mathematical morphology operations, such as dilation, erosion, opening, and closing. 26 these authors combined color granulometries and color histograms to achieve leukocyte a scoring system was proposed 33 to distinguish b-cell disorders based on the following quantitative features: nuclear shape, cellular area and shape, nucleus-cytoplasmic ratio, nuclear red/blue ratio, cytoplasmic green/blue ratio, and nucleolus detection. the study included 87 pb smears of b-and t-cell disorders and 6 healthy donors. for each pb smear, 30 images were analyzed demonstrating the usefulness of the scoring system, but it needed to be confirmed with a larger sampling."
"the segmentation step aims to obtain the cells separated from the other objects in the image and divided into regions of interest (roi). this step is critical to be successful in the feature extraction procedure. 15 in pb cells, 3 main roi are typically obtained as follows:"
"another example of a new geometric feature is the so-called rbc proximity, which is an indirect measure of the amount of cell cytoplasm that adheres to red blood cells (rbc). it is well known by pathologists that cytoplasm of reactive lymphocytes (rl) tends to adhere to neighboring (rbc). when a rl touches a rbc, there is a coincidence between the rl cytoplasm perimeter and the edge of the peripheral zone around the rl. based on this observation, the rbc proximity is defined as the proportion of the rl perimeter (in pixels)"
"a digital pb cell image is composed of a finite number of pixels with particular location and color values. color is a physical property very common in the visual characterization of blood cells. to explore the rich amount of color information in the malignant blood cells, several color spaces are used to obtain quantitative features. a color space is a way of describing a color by a number (3 or 4) of components. 15 in the rgb space, each color is decomposed into 3 bands corresponding to the basic colors: red (r), green (g), and blue (b). texture is related to spatial patterns of color or intensities, which can be visually detected. in digital image analysis, texture is quantitatively defined by uniformity, density, pixel tone, and their spatial relationships, among others. such analysis is not an easy task and is usually performed following 2 main approaches: (i) the so-called gray level co-occurrence matrix (glcm) and (ii) the granulometry."
"traditionally, the morphologic analysis of the pb smear has been performed using the manual microscope method. the close collaboration between cytologists, mathematicians, and engineers over the last few years has eased the development of automatic tools for digital image processing of normal blood cells. some equipment is capable to preclassify cells in different categories using neural networks, extracting a large number of measurements and parameters, which describe relevant cell morphological characteristics. 6 the use of digital microscopy systems in the clinical laboratories has been extended, and automated analyzers using digital images of leukocytes, erythrocytes, and thrombocytes have been integrated into the daily work. 7, 8 nevertheless, these automatic analyzers have some limitations related to abnormal or neoplastic cell detection. 9 new hematological analyzers are currently under development based on automatic morphology computer imaging of pb smears. 10 the topic of morphological analysis has received much attention with the increasing demands in both bioinformatics and biomedical applications. in the last 20 years, about 1,000 publications have reported the use of morphological cell analysis in biomedical research."
"geometric features have been previously used for quantitative characterization of lymphocytes in thyroid tissues, 4 such as nuclear diameter, area, or irregularity, useful for the discrimination between abnormal and reactive lymphocytes. in addition, the effect of albumin on cll lymphoid cell morphology on pb was analyzed using image analysis. 22 it was demonstrated that the addition of albumin to the blood smears resulted into changes in cytological features, decreasing not only cell, but also nuclear areas, while increasing the nucleus-cytoplasm ratio. in a later publication, image analysis was used to obtain geometric features to quantify some cytological parameters of the malignant lymphocytes in cll, mcl, and b-pll. 1 in that work, some quantitative descriptors, such as cellular area and diameter, nuclear area, and density, were found to morphologically discriminate among these 3 groups of abnormal lymphoid cells."
"for a grayscale digital image, the glcm is defined as the probability that pairs of neighboring pixels have similar intensities. 25 based on the glcm of a digital image, many measurements (second order statistical features) can be calculated providing more information about the texture of the roi, such as correlation, homogeneity, maximum probability, and others. 26, 27 by the way of example, figure 2 shows 2 original images corresponding to a lymphoblast (left) and a myeloblast (right) acquired with the dm96, as well as their corresponding magenta components as grayscale images. the intensity levels in these images rank from 1 (black) to 8 (white) and, therefore, the glcm are matrices with 8 rows and 8 columns, which are shown in the figure. as an example, figure 2 illustrates the meaning and value of the maximum probability feature. first, the most repeated pair of pixel intensities is identified. then, the ratio of the repetitions over the total possible pairs is defined as the maximum probability feature. for the lymphoblast case, the pair (8, 8), which is the brightest, has 8,178 repetitions, which means a ratio of .8682. this is in accordance with the visual observation, as the grayscale lymphoblast magenta component appears practically white. on the other hand, for the myeloblast, again (8, 8) is the most probable, but the feature value is .1332, much lower than in the previous case. this is because there are large numbers of pairs of pixels with different gray intensities as numerically shown in the glcm and visually observed in the grayscale image."
"as its introduction by haralick, the glcm has been widely used as a texture measurement in medical imaging, such as ultrasound images for solid neoplasms 28 and in bone marrow images to distinguish 4 types of erythrocyte precursor cells stages. 29 nevertheless, few studies involving glcm have been performed using pb images and mainly for differentiating among normal leukocytes 30 and blast lymphoid cells. 31 in other paper, the authors calculated 5 textural attributes based on the glcm (energy, entropy, among others) to differentiate between normal leukocytes (5 subtypes) and cll cells. 32"
"nucleus, whole cell, and peripheral zone around the cell. [cit] a fourth roi is obtained for the cytoplasm by the difference between the sets of pixels that belong to the whole cell and the nucleus region."
"from the segmented roi, the next step is to identify a set of quantitative descriptors, which are usually grouped in the following 3 categories: (i) geometric, (ii) color, and (iii) texture."
"the original images correspond to 2 blast cells from lymphoid and myeloid lineage, respectively, and they were acquired with the dm96. in their corresponding magenta component in gray scale, it can be observed that lymphoid blast displays mostly magenta component, which is represented by bright spots in the grayscale component. the intensity levels in the image rank from 1 (black) to 8 (white). the most probable pair of pixels for lymphoid blast is the brightest, where all intensity values are grouped. it resulted in a higher average of the maximum probability. in contrast, myeloid blast shows a large number of different pairs of gray pixels' intensities, resulting in a lower maximum value and a lower value for this descriptor cell texture and discriminative values of the features based on the granulometric curves among these groups. figure 4 shows an ex- as observed in figure 5, when a rl touches a rbc, there is a coincidence between the rl cytoplasm perimeter and the edge of the peripheral zone around the rl. as we explained in the section corresponding to geometric features, the rbc proximity is defined as the proportion of the rl perimeter (in pixels) that in addition, a morphologic-genetic correlation has been recognized for certain acute myeloid leukemia subtypes. as an example, there is a strong association between acute myelomonocytic and acute monocytic and myeloid leukemia with nucleophosmin (npm1) mutation. 37 some other correlations involving morphology and cytogenetic changes in abnormal lymphoid cells may show up using image analysis."
"in the future, the expanded use of current and new quantitative features for nuclear, cytoplasmic, and cell abnormalities may lead toward an objective morphological assessment of blood cells. crucial challenges for the laboratories in the implementation of this technology will include the standardization of the quality of pb smear staining to ensure appropriate, high-quality smears and to minimize the difference in images acquired from different sources. in addition, it would be a requirement for the laboratory professionals to increase their knowledge about image analysis and related topics, and to be involved in multidisciplinary collaborations with engineers and mathematicians, among others."
"more variability of the pixel intensity levels implies higher information variety, so that the entropy of the image is higher. in contrast, the low uniformity of the pixel intensity levels is translated into lower energy. conversely, the histogram corresponding to image 2 (in red) shows less dispersion and high uniformity in the pixel intensity values, which means low entropy and high energy of the image study. 19 in addition, quantitative features were extracted by image analysis to discriminate rl from blast cells and between myeloblasts and lymphoblasts. 34 a number of 20 quantitative features, which were relevant for the discrimination among 12 lymphoid cell types, were also analyzed. 23"
"quantitative features facilitate the last step of the image analysis process, which is the automatic cell classification. the reader may find more detailed discussions about image processing and machine learning tools in the morphological analysis of blood cells. it is interesting to mention that 10 of the 20 most relevant features for the classification into reactive or neoplastic pb cells were parameters obtained from the granulometric and pseudogranulometric curves (see figure 4) . in contrast, in a previous study reported 23 for the discrimination among 12 lymphoid cell groups, only 2 among the most relevant features were granulometric. the high number of relevant granulometric features contributing to the discrimination between reactive and neoplastic pb cells may be due to the differences in the large cytoplasm present in rl and some myeloblasts with respect to the low cytoplasm present in most of the abnormal lymphoid cells. this is translated into very different"
"the main steps in image analysis are the following: (i) image acquisition and preprocessing, (ii) segmentation, [cit] and (iii) extraction of features (quantitative descriptors), 18, 20, 21 which make possible the further classification of the different cell types."
"in general, geometric features are easy to interpret because they are the most intuitive and closely related to the visual patterns that are familiar to pathologists. most common geometric descriptors include parameters such as area, perimeter, circularity, diameter, eccentricity, elongation, roundness, convexity, nuclear size, cell size, nucleus-cytoplasmic ratio, nuclear eccentricity, and others. 1, 18, 20, 21 the nucleus eccentricity is calculated as the distance between the cell center and the nucleus center. 20 new geometry-based features may be proposed to quantify more complex characteristics. for instance, a novel descriptor was proposed in 17 to describe the cytoplasmic profile or hairiness in villous lymphocytes."
"although immunological, cytogenetic, and molecular tests are being increasingly used, the morphologic analysis of blood cells is still a crucial diagnostic aid. the information provided by the morphological analysis of the blood cells is relevant for the selection of additional techniques and follow-up of the patients with malignant blood diseases, including lymphoid neoplasms and leukemias. 1, 2 in the who classification system, tumor cell morphology, along with the immunophenotype and genetic changes, remains essential in defining disease entities. an early study about image analysis of small cell lymphoma of the thyroid gland, and the comparison of nuclear parameters of lymphocytes, demonstrated that the nuclear area was the optimum descriptor discriminating between small neoplastic lymphocytes in thyroidal lymphomas and reactive lymphocytes in hashimoto's thyroiditis. 4 image analysis was first developed using histological sections, and the measurements were performed over tissue parameters such as length, area, and cell count, being useful to supply information about the trabecular organization in the marrow space. 5 as blood cells are seen as individual entities in the smear, their cytological characteristics may be the targets in the definition of features to provide objective numerical scales and quantify them using image analysis."
"cll, mcl, b-pll) using 113 features (geometric and color/texture) was described in. 17 the same authors presented the most relevant features for the automatic classification of normal, hcl, cll, mcl, and b-pll. 18 in a further work, fl and rl were added to the previous f i g u r e 1 two original images corresponding to sézary cells acquired with the dm96. the 2-axis plot shows the histograms obtained from the magenta component in the grayscale images, in which the pixel count in the magenta intensity levels corresponding to images 1 and 2 was obtained using matlab. comparing the blue histogram from image 1 with the red one from image 2, it is possible to note that the first shows many pixels with different intensity levels, while the second has the pixels mainly in a short intensity range. in fact, image 1 displays more variability and contrast in the nucleus with respect to the cytoplasm, while image 2 shows bright dots practically in the whole cell."
"the goal of this study was to review how image analysis helps with respect to blast cell morphology, one of the major roles of the blood smear analysis is the discrimination between myeloid and lymphoid origin, especially for acute promyelocytic leukemia, in which a blood film is very helpful in the rapid diagnosis and treatment. 14 the majority of lymphoblasts lack granules and have a high nucleus-cytoplasmic ratio, a diffuse chromatin pattern and usually prominent nucleoli. myeloid blast cells may present a few azurophilic granules and a lower nucleus-cytoplasmic ratio."
the subjects should be able to relate and identify with the task the subjects should find the topic situation interesting the task situation should have an imaginative context so that subjects can relate to it.
this is the total mouse movement calculated by its x and y coordinates on the monitor. the count is incremented by one for x and y as the mouse hovers. mean mouse velocity (mmv) this is the total speed covered by the mouse on the monitor. number of mouse clicks (nmc) this is the total amount of mouse clicks on a page. the number of mouse click is incremented every time the mouse is clicked by a user.
"considerable research has been carried out to improve the quality of information retrieval systems by the use of relevance feedback. particularly the focus of research has been on implicit relevance feedback or implicit feedback. the implicit feedback approach uses implicit indicators to replace explicit rating for the development of recommender systems [cit] . it is used unobtrusively to infer the user's information needs based on their interest. although implicit feedback is widely available, it is considered a secondary option to explicit feedback [cit] and is noisy and less accurate compared to the explicit method [cit] . current research investigates the best way of replacing explicit feedback measures with implicit feedback approaches [cit] . for instance, in a controlled setting, mouse and scroll movements have been found to exhibit some correlation with explicit rating, but it is somewhat difficult to interpret that in the real world [cit] ). an advantage of the implicit approach is that a large amount of data can be collected ubiquitously without restricting a user to a particular place. the predictive strength of a number of implicit indicators has been investigated in the field of information retrieval. among the implicit indicators previously investigated include: time spent on a document (also called reading time or dwell time), mouse movement, mouse distance, mouse clicks, amount of scroll movement, copy and paste, printing, highlighting, emailing and bookmarking [cit] . unlike explicit rating which is intrusive, expensive and alters users' browsing behaviour, implicit measures remove the cognitive cost of rating and these are not intrusive [cit] . the next phase of the review will focus on commonly used implicit feedback measures (classical implicit indicators) and gaze-based feedback measures."
"and explained more variance in the regression model than the detection task, could lie in the 422 similarity of stimuli used for both tasks (bvmt and distractor task). both employ short non-423 speech syllables for which speakers have to be matched. however, task demands still differ 424"
"the data collection in user study 1 was based on capturing documents that were javascript enabled. pdf documents, images and video web resources were not captured. this is a limitation that will be addressed in our future work. another limitation in this work is that only selected documents were identified and used to examine the relationship between the predictive function model and gaze based measures. a better approach would be to capture the eye gaze measures and the classical implicit indicators simultaneously in a single study. the eye tracker used in this work was limited in the searching procedure. the tobii sdk software used for capturing gaze measures is only compactible with internet explorer browser while the javascript plugin designed to capture the classical indicators is specific to mozilla firefox browser. a crossbrowser plugin to capture classical indicators will be developed for future work."
this is the number of times text is copied to the clipboard from a document. it is incremented by one any time text from a particular document is copied. mouse duration count (dc) this is the total number of 100 ms intervals that occurs while the mouse is moved on the screen. time stamp this is the time and date in gmt when a document is loaded and when a document is closed. url this is the http address of any web document visited by a user. ip address (ip) this is the internet protocol address of a user. it represents the user's location.
"future work will include the development of a framework for a context-based recommender system to improve post-retrieval document relevancy. the framework will use vector space model (vsm) to index and rank documents based on query-document similarity, and the predictive model derived from users' previous interaction with the system will be used to re-rank the documents according to users' interest/perceived relevance."
"the participants were asked to perform search tasks and produce a short report of their findings. the study took 45 min for each of the group and the participants were given an option to either perform the experiment in a controlled environment or at home in a natural setting. the controlled study was performed in selected computer laboratories of the university. a consent form was given to them to complete which was followed by a short tutorial about the task. a zipped mozilla firefox portable with an embedded javascript plugin was uploaded to google drive for participants to download and extract to their computers. after the extraction, they were instructed to open the firefox browser in the folder and begin the search for answers to the given task. the participants were informed not to look at their clock while performing the task; this was to remove any anxiety and nervousness. participants either entered their own query in a search engine to find their required documents or they entered the url address of their required web documents directly. for every web document they visited, participants were prompted to enter their user id and then do the following:"
"accuracy in percentage correct, where reported, were calculated based on the corrected hit 301 and miss rates for detection and distractor task. these calculations followed the steps 302 pearson's correlations were used to determine the relationship between all three tasks. 305"
"distractor voices were controlled in a way that half of them showed high similarity to 439 the first target voice (t1) while the other half were markedly different. surprisingly, we did 440 not find an effect of distractor similarity on target identification, neither in the overall 441 performance (percentage correct) nor in the reaction time data. this is in line with the 442 [cit] who tested the resilience to distraction in both 443 face and voice perception and found that voice perception is more susceptible to distraction, 444 regardless of whether the distractor is similar or not. it is worth noting, though, that the 445 similarity manipulation in that study only matched speaker sex for target and distractor 446 voices (e.g. similar distractors being female speakers for female targets and different 447 distractors being male speakers for female targets). stevenage and colleagues argued that 448 voice recognition was vulnerable in itself due to the relative weakness of voice perception 449 pathways. as our design used a more stringent approach to what constitutes as a similar 450 distractor (smaller distance in voice space) rather than just speaker sex, our findings support 451 the notion of voice recognition pathways being vulnerable in general. 452"
the present challenge with implicit approach is that there are no standard and acceptable methods to determine how users' activities on the web relate to their interest. this study employs the use of an instrumented web browser to capture users' implicit and explicit data from client machines and store them in a central server for further processing. in this research we investigate the consistent and predictive indicators that are frequently used by a community of users with similar interests. the indicators can then be used to build a system which learns from users' behaviour and offers recommendations to them and future users with similar information needs while minimizing the time users spend finding relevant documents.
the remaining part of this paper is structured as follows: section 2 gives an overview of related work. section 3 presents the methodology and two user studies. section 4 presents the results and section 5 provides the conclusion and outlines our future work.
"considerably. each trial in the distractor task consisted of three voices, one played shortly 425 after the other (interval between each voice: 0.8 s). instructions then called for a 426 same/different decision regarding the first and the third voice. the bvmt, on the other 427 hand, is a task in which participants can replay the two voices per trial as often as they like 428 before making their same/different decision. as such, memory demands and time constraints 429 of both bvmt and distractor task differ considerably. in addition to that, the strength of the 430 correlation between bvmt and distractor task was only moderate to high (.57), suggesting order to fully address these issues in future research, an additional assessment of auditory 433 memory, as well as the inclusion of pre-ratings on all stimuli used (both in terms of physical 434 characteristics like f0, but also perceptual attributes like distinctiveness of sounds) could 435 prove helpful. additionally, introducing a time limit on the completion of the bvmt (e.g. time 436"
"participants' task was to listen to the three voices per trial, and then decide whether 276 the first and the third speaker were the same or not. decisions were made using the 'f' and 'j' 277 key for same or different voices (key assignment counterbalanced across participants). the 278 next trial started following a button press. during stimulus presentation, participants saw a 279 fixation cross in the centre of the screen. after the third voice (t2) had been played, the key 280 assignment was displayed on the upper half of the screen. completion of this task took 281 about 20 minutes. headphones (250 ω). up to 2 participants were tested at the same time. the order of the about the nature of the experiments, participants filled in a consent form before starting the 289 tasks. each task was introduced by the experimenter, and both spoken and written 290 instructions were provided. both voice detection task and distractor task included practice 291 blocks (8 trials/4 trials, respectively). stimuli presented in those practice trials were not used 292 in the main parts of the experiments. moreover, participants were encouraged to ask 293 questions in case of uncertainty about a task. after completion of all three tasks, participants 294 were debriefed and given contact details in case of further questions. 295 296"
"this is a visualization technique that separates different levels of fixation intensity, it show areas that are more fixated to be denser than areas that are less fixated."
"users of an information system present their intention/interest through the formulation of input queries; however this does not adequately capture their interest [cit] and the users have to go through several iteration to get accurate search results. therefore, there is a need to augment user query input with additional sources of information obtained explicitly or implicitly from their post-click interaction with the system in order to provide users with accurate search results based on current activity and context [cit] . with respect to relevance feedback, although the explicit approach (where users of a system state their opinion of the system) is commonly used for movie, music and product review, it is intrusive and alters user browsing pattern [cit] . explicit feedback can be replaced with non-intrusive approach that tries to infer users' interest implicitly. the inference usually takes a sequence of steps which include observing user browsing behaviour, selecting the appropriate set of implicit indicators and modelling them as source of evidence for implicit relevance feedback. these implicit indicators are obtained by observing user activity, for example through user mouse event, dwell time, keyboard event, eye tracking and psychological measures (e.g. facial expression). indicators from dwell time, mouse and key events are commonly used at low cost whilst eye tracking indicators are expensive and can be intrusive. eye gaze has been said to have a direct link with human cognition [cit], making it the most promising implicit indicator for predicting the user's perceived relevance of web documents. it has been shown to be useful in inferring cognitive states for personalisation [cit] . the cost, configuration and portability of an eye tracker mean that it is not easily applicable to real life applications. there is therefore a need to model low cost implicit indicators as a substitution for the eye gaze indicator."
"read the document for information relating to their task then close the current tab (web document). on closing the tab, participants were asked to explicitly rate the documents according to its relevance. the participants were required to visit and read a minimum of 7 web documents finally, participants were asked to prepare a 200 words report of the solution to the given task."
"the injected javascript plugin captured users' behavioural features (see table 1 ) and explicit ratings for relevance for each document visited. urls from google results pages, facebook and yahoo were prevented from being logged on to ensure that only documents related to the task at hand were recorded. the data collected were then sent to a central server, and then to a mysql database for storage. fig. 1 depicts how data were automatically captured and stored while the users were performing their tasks."
"references akuma, s. (2014) . investigating the effect of implicit browsing behaviour on students' performance in a task specific context. international journal of fig. 2 . the heat maps of documents with the highest mean fixation count and the lowest mean fixation count. table 7 comparison of study 1 and study 2 result."
"correlation coefficient 0.36. the relationship between the explicit ratings and the implicit indicators (dwell time and amount of copy) in the predictive model have a higher correlation as compared to other features examined and only these features are sufficient to derive the predictive model. considering that the relationship between the explicit and implicit indicators in the predictive model produced a correlation coefficient of 0.36, which is higher than that of the individual indicators examined, it suggests that when the features are aggregated, a higher degree of accuracy in prediction is obtained."
explicit ratings (user study 2) classical implicit indicators study eye gaze indicators study fig. 3 . graph showing the explicit ratings of users in study 1 and the mean explicit ratings of users in study 2.
3.1.1.2. simulated work task situation 1. gig software development company employed you as a consultant to provide a solution to the company's pressing problem of developing a customized software within a minimal time frame. some professional software developers achieved this by using the rational unified process while others used the waterfall model. 3.1.1.4. factual task (task 2). task 2 is considered factual because specific facts have to be sourced. it focuses on gathering information on a subject and participants have to find specific information which is explicitly measurable. the complexity for this task is high because at least 7 web documents have to be retrieved. the euclidean distance of mouse movement is calculated by its x and y coordinates on the monitor in every 100 ms.
"our aim for the current study was, on the one hand, to test two potentially separate 109 abilities that occur at different stages of voice perception. on the other hand, we also 110 wanted to explore their impact on a third, complex auditory task that has been used 111 previously and in more ecologically valid contexts. the aforementioned potentially separate 112 abilities are first, the ability to detect voices as a discrete class of sound objects (voice 113 detection ability), and, second, the ability to determine whether two utterances were spoken 114 by the same speaker or not (voice matching ability). to investigate whether both are suitable 115 to determine the accuracy on a more complex auditory task, we chose a distractor task 116 examining how vulnerable or susceptible someone is to the interference of a distracting 117 voice. this third task follows the example of voice perception tasks common in forensic 118 contexts (same/different decisions about a voice that one had previously been exposed to, 119 following interfering information). however, for the current study this takes place within a lab-120 based environment, allowing for stricter control of voice variables. for this reason, we also 121 wanted to revisit the issue of distractor similarity, i.e. whether distractors that are either 122 similar or different from the initial target voice affect the accuracy of one's same/different 123"
"apart from clinical contexts and the focus on general perception mechanisms, voice 82 identity perception has also received attention in non-clinical contexts, particularly in the field 83 of forensic psychology. [cit] point out, recognising an unfamiliar 84 person by voice alone is not a task we often encounter in natural settings, yet witnesses to a 85 crime might only be exposed to a perpetrator's voice. the reliability of witness testimony 86 therefore depends on a witness's ability to extract identity information from a typically 87 unfamiliar voice (i.e. process and compare the features of that voice to a stored 88 representation of average voices) and store this information for the newly heard voice. then, 89 at a later point, the witness needs to distinguish the initial target voice from other unfamiliar 90 voices (all of which require the same processing steps), and match it to its correct target at a 91 later voice line-up. in terms of belin and colleagues' more general model of possible distinct 92 modules, this forensic line-up task requires structural encoding of the perpetrator's voice 93 beyond just low-level auditory processing. ideally, identity-specific features of the target 94 voices also have to be accessible at a later time point to allow for correct identification of the 95 perpetrator. this process is, of course, prone to error ( to decide whether this test voice was identical to the initial target voice or not. accuracy on 105 this task was reduced as soon as any distractor voice was introduced. the detrimental effect 106 distractors had on overall task performance occurred both when the distracting voices were 107 similar (as defined by same speaker sex as target voice) or different (opposite speaker sex). 108"
"this task was inspired by a visual detection task for faces to investigate an individual with 134 severe face recognition impairments (prosopagnosia; duchaine, yovel, butterworth, & 135 [cit] ). our task was adapted to address the inherent differences between the 136 visual domain (faces) and the analysis of auditory information as it unfolds over time. while 137"
"the results in user study 2 show a significant correlation between the fixation count and the user perception of relevance. there was no significant correlation between fixation duration and user ratings. however, the explicit ratings for the documents identified in user study 1 were correlated with the mean explicit ratings of the documents in user study 2 and a strong correlation of there is a significant relationship reject null hypothesis the correlation between the explicit ratings of the predictive function model used for identifying and extracting the documents from the dataset is 0.36. the correlation between the total fixation count and the explicit rating is 0.32. considering the consistency in the ratings of the documents in the two studies, and the predictive model and fixation count producing similar correlation coefficient with the explicit ratings (as shown in table 7 ), we can infer that there is no significant difference between the predictive model based on implicit indicators and the eye gaze in the context employed. the relationship between the ratings is shown in fig. 3 . the predictive model derived can be used to estimate document relevance for the recommendation of relevant web documents to a set of users based on their previous interaction with the system."
"given by clicking on either of the response boxes. participants could listen to each item 243 multiple times if they wished. between trials, participants saw a centred fixation cross for 244 800 ms. on average, completion of the bvmt took less than 10 minutes. 245"
"although kreiman and sidtis' model does not indicate independent feature-specific 54 modules (e.g. for vocal affect perception) like belin and colleagues' model does, it 55 nevertheless posits the involvement of several distinct brain regions. tasks related to voice 56 perception therefore recruit the distributed areas that are relevant for solving a specific task. 57 the need for research on this topic, and indeed support for the existence of different 65 independent voice perception modules, becomes more apparent when surveying the 66 diversity of clinical symptoms reported for individuals with phonagnosia, or an impairment in 67 voice perception. for example, an extensive study of patients with brain lesions revealed 68 that while most patients with voice recognition deficits (in this case the recognition of famous 69 familiar voices) were still able to discriminate between two different unfamiliar voices, one of 70 the patients showed an impairment in both [cit] . however, in 71 this sample no further tests were reported to see whether other domains of voice perception 72 like the perception of gender or affect were selectively impaired as well. in recent years, 73 cases of individuals with developmental phonagnosia have emerged. to assess the extent 74 of their voice recognition deficits, these individuals often complete a number of voice 75 perception tests that target specific voice perception abilities. usually, only certain functions 76 of voice perception are impaired (e.g. identity perception), while others like gender 77 perception remain intact (see also the first reported case of developmental phonagnosia in 78 [cit] ). both acquired and developmental voice perception deficits underline 79 the need for a more in-depth assessment of possible singular processing stages in order to 80 establish the range of functions that can be selectively impaired. 81"
"nevertheless, we tried to keep the distribution of d-t2 distances comparable for male and 264 female trials with 13 small and 19 larger d-t2 distances each. all syllables uttered within an 265 item were different (e.g. abahedubu, and not abahed -aba), and t2 syllable type 266 (consonant-vowel-consonant or vowel-consonant-vowel) either matched only t1 syllable 267 type (13 items), d syllable type (13 items), both t1 and d (18 items), or was different to t1 268 and d (20 items). 269"
"most web pages are longer in length than the monitor height. when readers are interested in a page, they scroll the page. the scrolling is normally done by either clicking or dragging the scroll bar. any time a user clicks the scrollbar up or down, the count is incremented. number of keystrokes (nk): this is the total number of keystrokes on a document. this is incremented when the user strikes a key."
"to aggregate the most predictive indicators of interest for a function model, multiple linear regression analysis was used to estimate the user behavioural features that best represent the user explicit interest. a stepwise regression method was employed for this selection. the stepwise regression automatically selects the predictive variables through a sequence of t-test. the regression function is given as:"
"for the distractor task, each trial consisted of 3 voices: a first target voice (t1) 248 followed by a distractor voice (d) which, in turn, was followed by a second target voice (t2). 249"
"this work employed a task context approach to explore the relationship between implicit and explicit user parameters. our findings show that apart from keypress and mean mouse speed, other classical implicit indicators measured correlate with the explicit ratings. a predictive function model was derived from the relationship between implicit indicators and explicit ratings. the model consisted of two of the most predictive implicit indicators (dwell time and copy) and it had a higher correlation coefficient than the single indicators. a validation study based on eye gaze was used to confirm the predictive strength of the predictive model and it showed that there was no significant difference between the predictive model based on implicit indicators and the eye gaze in predicting perceived relevant documents within the context examined. these findings provide a cost effective method for understanding user behaviour in a task specific context through the use of implicit indicators which can be used in education-based recommender systems."
"as such, the detection of voices should be part of the earlier processing stream of vocal 131 sounds (as described in belin and colleagues' model). in our study we aimed to measure 132 participants' ability to detect voices in an ongoing stream of vocal and non-vocal sounds. 133"
"this is the actual rating of the web document by the user. the firefox plugin attaches a six scale rating button on each of the webpages. after reading a webpage, the user rates it by clicking on any of six scale buttons where 5 e means very relevant, 4 e means more relevant, 3 e means moderate relevant, 2 e means slightly relevant, 1e means very low relevance, 0 e means not relevant. fig. 1 . the experimental system used for capturing data. the purpose of this study is to correlate eye gaze measures with explicit relevance ratings on some web documents to validate the predictive model which is developed as result of user study 1. briefly, based on the predictive model, the documents perceived to be the most relevant and least relevant are identified from the pool of documents. these documents are then given to participants to read through with an eye tracker installed on the machine and rate them according to how relevant they are to the task under consideration. the rating categorisation is the same as that employed in user study 1. in this user study, 9 university students majoring in the area of computer science took part. a task brief explaining the procedure and a consent form was given to the participants to complete. this was followed by a short tutorial about the experiment. the eye tracker was calibrated on a five point calibration scale and each of the participants had to sequentially read through the 6 documents within 30 min and rate them."
"the goal of this study is to investigate the relationship between classical implicit indicators and explicit relevance ratings in task specific domain, and to identify the implicit indicators which have a stronger relationship with the explicit ratings to use in the predictive model. since there is no agreeable standard for sample size for an interactive information retrieval (iir) experiment and no linear relationship between sample and population [cit], we worked with a sample size that is consistent with previous research. the participants recruited for study 1 were 77 undergraduate students of computing. only participants above 18 years were allowed to participate. the participants recruited for this study had a high proficiency with the use of computers. remuneration was not given to the participants but they were informed of the overall benefit of the research to enhance student learning through the recommendation of documents by using relevance feedback approaches."
"we therefore predict that both the voice detection task and the bvmt should correlate with 161 the distractor task as they all rely on the analysis of a sound as a vocal object, but that the 162 correlation with the bvmt should be higher. in order to complete the distractor task 163 accurately, both an intact ability to detect voices and an intact ability to extract identity cues 164 from voices are necessary. we therefore also expect that performance in the voice detection 165 task and in the bvmt will both be predictors for the performance in the distractor task. 166 however, given the proposed similar, later processing stages necessary for the bvmt and 167 distractor task, we assume that the bvmt will be a better predictor. 168"
"two user studies were conducted. the first user study \"classical implicit indicators vs explicit ratings\" analyses 'commonly available' implicit feedback measures in relation to explicit relevance ratings for specific information retrieval tasks. it correlates implicit and explicit feedback parameters, and it uses multilinear regression to derive a predictive model that can estimate document relevance. the second user study \"eye gaze indicators vs explicit ratings\" validates the derived predictive model with the eye gaze measure."
"constraints on each trial) might help making both predictor tasks more comparable in future 437 studies, and therefore eliminate some of the variance introduced by mere task differences. 438"
"as the amount of information available through the internet and various intranets continues to grow, effective retrieval of relevant information has become a challenging task. the tremendous amount of available information leads to information overload [cit] . this phenomenon has led to research on personalised information retrieval and recommender systems. [cit] ) . hence, research in the area of personalisation has been found to be useful for addressing the problem of information overload by providing the users with relevant web documents based on their interest and current activity. this can enhance user search experience and improve efficiency. this can be achieved through relevance feedback based approaches. the process of gathering useful information about users in order to give them feedback or recommend documents based on their previous interaction with the system is called relevance feedback [cit] . relevance feedback is affected by contextual factors like task type [cit] . tasks are activities that people attempt to accomplish to meet a particular goal. identifying an accurate task stream is difficult because the demarcation of boundaries is not clear enough. however, common streams have classified different task along features like fact-finding vs information gathering [cit] . task type help to infer document usefulness and also influences the total time spent while performing the task [cit] . this work employs user task to examine the relationship between implicit parameters and explicit ratings."
"data was analysed using matlab [cit] ) and spss (version 22). performance in 298 detection and distractor tasks were calculated as sensitivity a', using signal detection theory, 299"
"null hypothesis indicates that there is no statistical significant relationship or association between two measured parameters. when the null hypothesis is rejected, it means there is evidence that there is a relationship between two parameters."
"among the nine features entered as inputs, the stepwise regression included only dwell time and amount of copy parameters. the predictive function below was obtained:"
"most studies of implicit indicators in the context of information retrieval have been based on search engine result page (serp) and very few studies have focussed on developing a predictive model through the aggregation of implicit indicators generated from web documents visited by users. this work uses a task based approach and it focuses on user post-click behaviour. it revisits some of the implicit indicators used in the studies above but instead of the multiple domain approach of data collection, information tasks are used to limit users' goals in a particular domain. the following implicit indicators are evaluated in this work: users' dwell time, mouse movement, mouse distance, mouse velocity, mouse clicks, amount of scroll, keystroke and amount of copy. a stepwise regression is utilised to extract and model the most predictive indicators. investigation is also carried out to examine whether the model can be used in place of eye gaze measures."
"following that, a hierarchical linear regression analysis was performed to understand 306 whether the general ability for voice matching (bvmt score) and performance in the 307 detection task predicted the performance in the distractor task. finally, paired t-tests on the 308 overall percentage correct in the distractor task were used to determine whether the 309 similarity of distractor voices influences the similarity decision for t1 and t2. 310"
"in this section, we discuss the tasks which were given to the students. the tasks employed for this research are simulated work situations which follow the recommendation by borlund [cit] . the tasks are designed to encourage the participants to search the web naturally. they are designed with the intention to be interesting and relatable for the participants. the tasks domain for the study is in the area of computer science. two simulated search tasks are composed of two parts e the simulated work task situation and indicative request. the simulated work task situation is a 'cover story' that has three characteristics:"
"in this paper, we investigate the relationship between implicit and explicit feedback parameters in different tasks. we derive a predictive model from implicit indicators that can be used to estimate document relevance. finally, we validate the predictive model with eye gaze tracker. the paper is focused on answering the following research questions: 1. can specific task situations be used to derive a predictive function model from classical implicit indicators that will signify that a web document is relevant? 2. can eye gaze predictive indicators be substituted by a predictive function based on classical implicit indicators?"
"in the main part of the experiment, participants listened to the 144 sounds described 222 above. these sounds were either presented to the right or left ear, to follow the structure of 223 participants had 2 seconds to react before the next sound was presented. during stimulus 229 presentation, participants saw a fixation cross centred on the screen as well as a reminder of 230"
"successful social interaction relies on our capacity to extract relevant information 23 from our surroundings and the people with whom we are interacting. while there is an 24 extensive amount of research into the perception of such cues from faces, the perception of 25 these cues from voices has been neglected until recently ( [cit] suggests that the recognition 37 process for voices relies simultaneously on the gestalt perception of the whole (pattern 38 recognition) and the analysis of specific auditory cues within the voice (feature analysis). the 39 degree to which both are engaged depends on the familiarity of the voices. recognition of 40 unfamiliar voices calls for the extraction of features more than for an overall pattern 41 recognition, possibly also involving comparison to a known \"average\" voice, and is more 42 stimulus-driven. familiar voice recognition is more top-down in that it relies heavily on the 43 overall voice pattern, with only voice-identity specific features becoming salient throughout 44 recognition. as such, recognising an unfamiliar voice is a question of discriminating and 45 matching two voice signals, and is therefore often described as the ability of voice 46 discrimination. recognising a familiar voice, in contrast, is the recognition of an overall vocal 47 pattern specific to a single person. the term \"voice recognition\" therefore often applies to the 48 recognition of voice identity for familiar speakers in particular (see also van lancker & 49 [cit] ) . furthermore, a recent neuroimaging study with lesion patients 50 [cit] has also found that different brain 51 structures are involved in the perception of newly-learnt unfamiliar vs. familiar voices, which 52 supports this distinction. 53"
"gaze data were captured with a tobii tx300 desk mounted eyetracker. it was paired with a 23 [cit] â 1080 pixels. the tracking frequency for the eye tracker was 300 hz and it gave room for subjects to move their heads. the accuracy was 0.4 degree of visual angle. the participants' fixation count, fixation duration and heat map as shown in table 3, were captured by tobii sdk software."
"3.1.1.1. mixed task (task 1). task 1 is considered a mixed product (decision and intellectual task) because it involves making a decision to solve a problem with the most efficient method (rup or waterfall model). it also focuses on 'how' a problem can be solved. it asks for the most important stage of the lifecycle, making it also an intellectual task. the goal of the task is specific because participants have to find a particular approach and it is also of high complexity because at least 7 documents have to be sourced."
"since the goal of this research is to obtain common consistent implicit indicators that can be used to represent users' interest, the consistency of the participants' explicit relevance ratings was examined. common documents visited by the users were extracted from the pool of documents visited. among these common documents extracted, the most viewed document had 21 hits while the least had 2 hits. an investigation was carried out to see if documents commonly visited are relevant. the mean of the explicit relevance ratings of the common documents visited was found to be 3.21. since '3' was labelled as 'relevant' on our 6 point subjective explicit rating scale, it therefore indicates that common documents visited by users are relevant across the users. it is therefore concluded that common consistent implicit indicators captured across all user populations can be used to infer relevance among a community of users. the findings of user study 1 led to the development of a predictive model."
"to conduct a study to validate the predictive model, an additional column was created for the predictive model in the dataset containing web documents and their related implicit generated indicators. the predictive function was used to compute a score for each of the documents. the computed score was sorted in a descending order and documents perceived to be highly relevant and least relevant were extracted from the pool of documents. this set of documents was then presented in a sequential order to participants in user study 2 to read before an eye tracker and rate their relevance."
"zemirli [cit] ) worked on post-retrieval documents and he developed a web browser (webcap) that uses 'examine and retention' indicators to infer users' interest in real time. he conducted an experiment on 6 users and found that webcap was able to capture 80% of relevant documents when compared with explicit user judgements. similar success was reported by shapira, taiebmaimon and moskowitz [cit] . balakrishnan and zhang [cit] examined the effect of some implicit indicators on post-retrieval document relevancy. they found that a combination of text selection, dwell time, click-through and page review post-click behaviour can improve the precision of relevance feedback."
"face perception research has tried to explore the different processing stages in face one limitation of our findings lies in the different characteristics of each task. of all 413 correlations, the ones with the voice detection task were the smallest, while bvmt and 414 distractor task showed the highest correlation. this could be due to the differences in 415 structure between all three tasks. arguably, the nature of the stimuli as well as the memory 416 demands of the voice detection task (rapid presentation of human vocalisations/animate and 417 inanimate environmental sounds) differed to those of both bvmt and the distractor task 418 (judgment of two/three vocalisations per trial without time limits). the variances introduced 419 by each specific method could therefore partly drive the strength of the correlations reported 420 here. similarly, the fact that the bvmt showed a higher correlation with the distractor task, 421"
the bangor voice matching test is a computerised voice matching test in which 234 participants make a same/different identity decision after hearing 2 different syllables per trial. syllables were either articulated by the same speaker (40 trials) or by two different 236 speakers (another 40 trials; for further details on item selection for the bangor voice 237
this section presents the results of user study 1 and user study 2. section 4.1 presents the results of study 1 and the results of study 2 presented in section 4.2.
"including the voice detection task as an additional predictor in the model led to a significant 382 change of variance explained, and although bvmt performance was descriptively a better 383 predictor than detection task performance, further analysis revealed that the difference 384 between both predictors was not significant. in terms of variance explained, though, bvmt 385 performance alone accounted for 31.8% of the variance (stage 1), whereas the inclusion of 386 detection task performance led to 36.4% of the variance explained in the full model. we 387 suggest that this is due to both voice matching (bvmt) and voice discrimination in the 388 distractor task occurring at later processing stages along the voice perception pathway 389"
"the goal of our proposed method can be formulated as follows. given a question representing a home service task, the proposed system can produce effective answers. the answers are not only instructive enough to teach how to perform the task, but also stable to resist perturbations from adversarial questions. the process of answer generation is illustrated in figure 1 : firstly, the tool inference layer takes questions as inputs and generates corresponding tool information. so the question representation is extended by integrating question terms and tool information."
"2) tool selection different tools are involved in performing home services. correct tool selection is the precondition for generating qualified answers, thus the selection of proper tools is chosen as a standard for estimating the comprehensiveness of produced answers. based on this point, we take tool information as a reference. when there is a match of tool names between tool information and corresponding answers, a positive score is produced. the reward relevant to tool selection is shown as follows:"
"the database system is made accessible but limited to relevant research participants. the testing process is used to monitor the performance of attributes, user requirements and database architecture and necessary adjustments are made where applicable based on an agile development approach."
"rock art is a repository of memory and a testament to over 30.000 years of human activity including interactions with peoples and the environment [cit] . sites are amongst the most vulnerable due to the impact of climate change, expansion of urban areas but also vandalism [cit] . it is becoming more and more important to develop conservation and preservation methods as more and more rock art sites are being lost due to natural and cultural threats. a centralized register could assist with the collection of rock art data and help improve understanding of rock art and preservation issues."
"as a sequence-to-sequence model, tool inference layer can produce tool information corresponding to the questions, which can be used to extend the question representation and guide the answer generation in reinforcement learning. tool information, including tool names and tool sequences, is a list of tool names aligned in a certain order. in this paper, tool names denote objects related to home services, and tool sequences mean the order the tool first appears. for example, the tool information corresponding to how to cook coffee can be represented as (water, coffee maker, teabag, cup). relevant objects are aligned with the order that they first appear, and we believe the order can reflect the execution order of subtasks. with tool inference layer, question representation is extended by integrating question vectors and hidden vectors that represent the tool information."
using an agile development approach in conjunction with the reference and domain model allowed for a more responsive development process. new user requests within each iteration frequently test the system's flexibility and allow to develop a deeper understanding of the correlation between user needs and the systems framework. the agile approach further assists with maintaining a quality framework through frequent revision processes.
in order to build a system that is flexible enough to allow for integration and sharing of data within a variety of international projects the radb system places and tests australian rock art within a national and global heritage context.
"from a technical perspective, a first draft of a prototype architecture for restassured has been developed. this architectural blueprint defines the functionality of all major system components, and defines the high-level interfaces between them."
"the radb management system is the deployed online portal of the radb using the radb system. the portal allows users to collect, manage, disseminate and discuss rock art information through specialized functionalities using rich media content and communication tools. this paper focuses on the 'tier 2: radb system', which is described in more detail in the following sections."
"the cidoc crm which provides a semantic reference model to allow flexibility, compatibility and integration of resources on a global scale within the greater heritage sector."
"sentences generated from qa model are regarded as actions in reinforcement learning. the action space, semantic space, is infinite since the length of sentences can be arbitrary."
the use of the cidoc crm gives the radb system a flexible structure that has proven to be easily customizable to cater for individual needs within a global system. problems still occur within the mapping process where it is often difficult to assign data to the appropriate entity or property within the crm as it is sometimes unclear which entity best to use for particular issues. producing charts and spreadsheets based on the dublin core approach has been helpful but has not fully solved all of these issues. to improve mapping techniques future implementations will investigate other methods such as the midas approach.
"inspired by observing the process in performing home services, we find that tools play an important role in home service. also, the qa system should have an inner cognition on tools when confronted with home service tasks. based on this point, the tool inference layer, a sequence-to-sequence model [cit], is constructed to generate tool information related to service tasks. the tool information denotes the inner cognition of the system. as is illustrated in figure 2, given the question terms ''how to cook coffee,'' the tool inference layer produces (water, coffee maker, teabag, cup) as tool information. tool information, containing tool names and tool sequences, can be adopted to extend the question representation, and works as a rule to estimate the accuracy of answers in reinforcement learning."
"the development of the radb system and the rarm document allowed for a better understanding of the radb management system requirements for heritage professionals and it professionals alike. while the cidoc crm provides the general oversight of global heritage problems allowing to break down larger problems into smaller manageable packages, the creation and use of the rock art specific domain model rarm allowed developing a greater understanding of individual needs within the specialised field."
the radb system offers a semantic rock art reference model (rarm) that can be implemented across international rock art projects while complying with the cidoc crm certificate requirements.
"as a major factor that affects the model performance, rewards are separated into immediate rewards and delayed rewards. immediate rewards are produced during the training process, as a way to optimize the answer quality, and delayed rewards are given at the end of the complete answer."
"rock art researchers around the world have been asking for centralized systems for the last few decades. while a number of systems such as the trust for african rock art (tara), the south african rock art digital archive (sarada) or the mirarr project have been developed for specific rock art projects and locations, a system that encompasses national and international sites is still missing."
"under the gdpr, personal data controllers and processors must assess risks to personal data, and employ security measures to appropriately manage identified risks, throughout the life of the system(s) storing and using the data. moreover, the data controller is responsible for proving the systems and processes used comply with the gdpr. it is no longer enough to implement recommended security measures based on a 'generic' risk analysis -one must analyze risks specific to each situation, design systems and processes to address risks to privacy, and continuously review and update the risk analysis and security measures as either the system or the threat landscape evolves."
"secure enclaves are offered by intel's sgx (software guard extensions) which is currently available in the marketplace, or amd's sme (secure memory encryption). while sgx and sme use different approaches, each with its advantages and disadvantages, the general idea is the same: a memory range is encrypted by the processor by a key which is generated at power-on, and not available to any running process. this means that all code and data within an enclave are protected from tamper and snooping, even by processes running at superuser level, or by dumping memory. restassured is creating a toolkit which will significantly simplify the work required by a developer to set up and use an sgx enclave (such as remote attestation, sealing, secret passing), allowing developers to focus on the development of their business logic. additionally, restassured is integrating the open source opaque project into its sgx toolkit. opaque is a spark sql engine that can work with encrypted data, leveraging intel sgx to protect the computation [cit] . users can run sql queries in a spark shell, or program the queries in high-level scala language. there is no need to develop sgx applications in c/c++ with the sgx sdk. however, opaque has some design and implementation limitations, related to attestation and data key passing. by integrating with the restassured toolkit, we enable efficient attestation of opaque enclaves, flexible data key passing and overall integration into the restassured platform."
"while this paper focuses on the development of the radb system described in stage 2 the conceptual model design, it is important to understand the radb system as part of an agile development cycle within these 5 project development stages which is frequently reviewed and improved."
"although the project is still in its first year, good progress has been made towards having a prototype implementation running two real-world use cases by project month 18. the first use case, highlighting social care services, shows how volunteer healthcare workers can be matched with suitable healthcare patients in a secure environment, preserving the data access rights specified by both parties. additionally, this use case demonstrates how restassured can enforce the security and privacy requirements for a workflow specifying the generation of summary reports by a third party only allowing them access to anonymized data. pivotal to this use case is the ability to integrate opaque into the restassured environment to support the database queries required to match caregivers with patients, as well as sticky policy enforcement across the whole workflow of the data."
"the rock art database system is the second tier within the radb universe. it describes the development phase of the rock art reference model document (rarm) that is used to inform the radb system. the radb system does not produce a physical database but rather seeks to inform such systems through the use of a semantic reference model as well as providing concrete examples of applied systems. the aim of integrating both semantic and concrete models is to improve understanding of the system by providing examples within a theoretical context. the examples can then further be used to improve and inform a rock art specific reference model through use case, user story within a feature-driven development process."
"the training process of reinforcement learning is illustrated in figure 3 . conditioned on the initial policy, sentences are generated as actions by the proposed model. then the produced sentences are sent to the semantic space for interaction. with the states after interaction, a reward related to accuracy and stability is given to optimize the model parameters for generating effective answers."
"researches demonstrate that the sequence-to-sequence model has been showing a tendency towards generating trivial responses that are short in length and irrelevant to topics [cit] . as the proposed model takes the sequenceto-sequence model as its backbone, measures have been taken to penalize semantic similarity between consecutive sentences from answers. both h p i and h p i+1 are hidden vectors representing consecutive sentences generated from the proposed model, so the reward is the negative log of the cosine similarity between two vectors:"
"mengyang zhang received the b.s. and m.s. [cit], respectively. he is currently pursuing the ph.d. degree in control theory and control engineering with shandong university. his research interests include intelligent space technology and service robot, reinforcement learning, and knowledge construction based on ontology."
"as in the previous use case, sticky policies, secure enclaves, and the restassured toolkit play a central role in simplifying what is required by developers to implement and deploy sgx-based applications."
"in our constructed datasets, wikihow is the main source, covering 85% of the datasets, and wikismall and wikianswers account for 15%. as is demonstrated in figure 4, we prefer the data representation in wikihow, in which the information is expressed in steps, and the first sentence in each paragraph is often the topic sentence. it facilitates the operation on handling complex sentences. compared with information in wikihow, the information representation in wikianswer or wikismall is colloquial and redundant, adding difficulties in the extraction of valuable information, so we have to extract and formalize the information with manual efforts."
"then, based on the extended questions, the answer generation layer is able to produce relevant answers. finally, answers are estimated with designed rules and parameters in the qa system are optimized during the process of reinforcement learning. the construction of the proposed nn-based qa system is described in three parts: tool inference layer, answer generation layer, and the process of reinforcement learning."
"sticky policies for data define access rights on the data and, as their name suggests, \"stick\" to the data, following it as it migrates across the cloud. in this way, sticky policies allow for decentralized data lifecycle management; i.e. access control can be enforced by decision points across the cloud, without the need for a centralized enforcement entity. sticky policies need not only support the rights of the data subject in accordance with gdpr requirements, but must also be able to support the security and privacy regulations which may be mandated by the enterprise which either owns or processes the data, as well as any regulations the data may be subject to, based on its physical storage location across the cloud."
"systems such as the australian heritage information management system (ahims) or state specific databases such as the queensland heritage register that deal with cultural, natural and indigenous heritage data are often too complicated [cit] . even though these systems deal with a variety of issues including indigenous land rights management or aural history, most are inadequate and disconnected from other national systems [cit] . in extreme cases such as the australian capital territory and tasmania heritage information is even managed in excel spreadsheets without any visible framework or use of comprehensive databases (see figure 1 ) [cit] ."
"the rock art database (radb) is a rock art heritage project at the place, evolution and rock art heritage unit (perahu) [cit] . the project aims to bring members of the global rock art community together in one centralized online platform to share and discuss rock art information. the deployed radb management system offers a tool to a wide range of users to collect, manage and disseminate rock art data through specialised functionalities in rich digital media formats."
the project development follows an agile development approach within a pimri cycle. agile software development describes a dynamic software development approach rather than a linear approach and assist with flexibility and improving system performance within a quality framework. the agile approach is characterised through undergoing a frequently repeating development cycle based on pimri (plan -implement -monitor -reviewimprove) (see figure 3) .
"the policy domain is concerned with the governance of the system. it describes set rules, conditions, terms and regulations within the radb system. [cit] . the rarm uses the dublin core method in two approaches [cit] ."
"in this part, the impacts of components in our model are further discussed. table 3 and table 4 demonstrate the effectiveness of different parts in our model."
results from stage 4 are formalized and made available for review to assist with the improvement of the system based on the agile development approach.
before we start looking deeper into the radb as a whole we first need to define its roles and functions. the radb as an organisation exists on multiple levels each producing different outputs in different stages. these levels are described in the radb universe tiers 1 -3 which have been adapted from the dl universe model [cit] :
"in order to reduce the complexity and the redundancy, each first sentence is picked and used to construct the corresponding answer for wikihow, and for other datasets, the topic sentences are selected and aligned with manual efforts. by filtering information of irrelevant topics, we obtained the training dataset, denoted as wikiservice. wikiservice is composed of 7182 service items with 59613 sentences. also, we extracted tool names corresponding to each service, and store them as a dataset, called wikitool, which is used as the training dataset of tool inference layer. the information in wikiservice and wikitool is illustrated in figure 5 ."
"the flexibility and dynamism of the cloud poses a big challenge for data protection. the applicability of traditional security mechanisms designed to keep the system in a stable secure state is limited. in particular, security-by-design methodologies are not sufficient, due to uncertainty at design time as to how the cloud and privacy requirements may evolve and change at run time."
the rock art reference model (rarm) aims to provide a document in form of a reference model for rock art documentation and management that can be used within a variety of applications across a wide range of platforms. in the radb project it is used to inform the design of the radb management system.
the rock art specific domain model has been particularly useful to clarify user roles and helped to better understand specific tasks assigned to particular roles within greater project responsibilities. the user domain is currently undergoing a review to optimize integration of user profiles for the implementation of social network functionalities to improve communication and crowd sourcing capabilities.
"the requirement for continuous and auditable management of risks is especially difficult in cloud-based applications, which may be subject to automatic adaptation at any time. one of the main goals of restassured is to provide the means to trace how such changes affect the level of risk, and where changes are made specifically to manage risks, e.g. by allocating sensitive processes to a secure enclave, or by introducing advanced encryption to block new risks when migrating workloads. the goal is to provide technologies that help data controllers to analyze risks and trace the measures used to address risks, making it much easier to comply with gdpr when using cloud-based applications. to achieve this, restassured integrates and extends two innovative approaches to information risk analysis:"
"the rest of this paper is organized as below: we give a description on related works in section 2. in section 3, the framework of the model constructed with the proposed method is described. section 4 elaborates the process of extending question representation. the reinforcement learning implementation is described in section 5. experimental results are shown and analyzed in section 6. finally, the paper provides a summary and recommendations for future work in section 7."
"each method is conditioned on the sequence-toarchitecture model. lstm uses the hidden vectors to represent questions, and bilstm takes hidden vectors from both directions into consideration. t denotes tool information generated from tool inference layer, and rl means reinforcement learning. our proposed method integrates bilstm, t and rl. in table 3, the performance of different components on comprehensiveness and logic is demonstrated. c denotes the comprehensiveness of answers, and l implys the logic. in the aspect of comprehensiveness, we can observe that the error rate of lstm and bilstm are 14.6% and 13.7% respectively. with tool information, lstm + t reduce the error rate by 6.1% (from 14.6% to 8.5%). compared with lstm + t, lstm + t + rl obtains an error rate of 6.3%. the proposed method gets the best result, indicating its produced answers are comprehensive. in the aspect of logic, lstm, bilstm and lstm +t obtained bad results (the error rates are 35.1%, 33.7% and 25.3% respectively). lstm + rl + t got a better result by decreasing the error rate to 9.5%, proving that it is necessary to regard the logic of tool sequence as a rule in producing effective answers. our method, bilstm +t +rl, achieves the best performance and reduces the error rate to 4.3%, as the contextual information is fully used. in table 4, the result of different components on wikiservice is demonstrated. lstm + t obtains a better result than lstm, increasing the f1 score from 32.5 points to 39.5 points, which indicates the importance of tool information. as we expected, the proposed method achieves the best performance, and improves the original lstm dramatically by 15.1 points (from 32.5 to 47.6). it indicates that extending question representation with tool information is beneficial for producing effective answers. also, the performance can be further improved by designing reward functions based on tool information."
"reinforcement learning can be used to model the long-term influence of essential elements by maximizing the reward value [cit] . owing to this advantage, models can be taught and guided by complying with reward functions, and the computation cost corresponding to action spaces can be alleviated [cit] . reinforcement learning has achieved some progress in a variety of domains, such as playing text-based games [cit], executing instructions for windows help [cit], understanding dialogues that give navigation directions [cit] or dialogue generation [cit] . in question answering, reinforcement learning has been applied to learn generation policy, including appointment scheduling [cit], recommendation on landscape [cit], and giving advises on restaurants [cit] . for more difficult problems, like negotiation policies [cit] and tutoring domains [cit], reinforcement learning can still obtain great consequences. thus reinforcement learning can be employed to integrate key elements into the generation of effective answers."
"due to the complexity and variability in human language, answers with only comprehensiveness and logic cannot guarantee the success in teaching how to perform home services. in this paper, attention has been paid to the relationship among operators, tools and objects, so other unimportant errors can be omitted. therefore, crowdsourced judges are employed to reevaluate the answers, changing answers with minor errors to correct ones."
"the final reward is composed of r 1, r 2, r 3, and r 4, where r 1 and r 2 are observed by the end of each sentence as immediate rewards, and r 3 and r 4 are delayed rewards observed after the whole answer is produced. the reward is designed on aspects of accuracy and stability. r 1, r 2, r 3 affect the accuracy on producing effective answers, and r 4 is designed to make the proposed model imperceptible to external perturbations. the final reward is as follows: (13) where n denotes the total number of sentences in an answer, and ω i implys the importance of rewards. in this paper, ω 1,ω 2, ω 3, ω 4 are set to 0.1, 0.2, 0.4 and 0.3 respectively. it is easy for the system to produce answers that satisfies the comprehensiveness, since the tool inference layer can be trained to reach a high accuracy. attention is paid to how to keep logic in produced answers, and how to make the system understand the importance of key question terms. if the weights corresponding to each part are changed, the performance can be influenced."
with no consistent rock art or national australian heritage framework to work with the radb uses the cidoc crm as a reference model for the database design. the system aims to comply with the cidoc crm certificate standards while introducing a flexible management system that allows for the integration of cidoc crm compliant and non-compliant projects through an agile development approach. the radb further produces a domain model with concrete data examples collected from a range of heritage bodies and rock art projects. the domain model provides real world examples while the cidoc crm provides the framework towards a cidoc crm certificate compliant system.
"the user domain consists of a range of possible functions a user can have within the system. it manages user profiles and associated privileges, restrictions and groups."
"the aim of the use of the reference model is not to tie it to any technologies or definite details such as data fields but to supply common semantics that provide a high level of flexibility that can be used in a variety of applications. the model describes entities and their relationships in order to clarify issues within international, national and local heritage documentation. it provides a flexible system that aids the process of finding potential solutions for a variety of research questions. the system further allows rephrasing research questions in order to be able to explore particular issues from different angles [cit] ."
"the radb aims to fill the gap of such a missing system. it aims to provide a tool that allows sharing and managing of rock art data on a local, national and global scale using australian rock art as a test model for its proof of concept design."
"tier 1: radb the rock art database (radb) is a virtual organization that aims to bring together professionals and members of the general public to collect, manage, disseminate and discuss rock art."
the agile approach as a result produces results within each pimri cycle in contrast to a linear start to finish approach. this allows for project development stages one to five to be revisited and redefined as part of the continuous development cycle.
"the answer generation layer is an encoder-decoder model that can generate corresponding answers based on the extended question representation. the encoder-decoder model, consisting of an encoder and a decoder, can learn a continuous space representation of sentences and preserve both semantic and syntactic structure. the encoder maps a variable-length sequence to a fixed-length vector, and the decoder maps the vector back to a variable-length sequence [cit] . in order to take advantage of the contextual information between sentences, both encoder and decoder in our model take the bidirectional long short time memory (bi-lstm) as their backbones."
"due to the complexity and variability, it is difficult to evaluate the quality of produced answers. although typical metrics, such as bleu [cit] or perplexity, have been adopted to sentence quality evaluation [cit], but debates on whether these metrics can succeed in reflecting the true quality of sentences still exist. in order to assure the proposed model is able to produce informative information, f1 score is employed as the main evaluation matric. f1 score is a measure of a test's accuracy, it considers both the precision p and the recall r of the test to compute the score. the equation is shown as below:"
the overall project takes an agile development approach to allow for flexibility and the ability to respond to emerging needs throughout the development of the radb system and management system. the development can be broken down into 5 stages:
"currently, there are two mainstream research directions for the task: semantic parsing-based (sp-based) methods [cit] and information retrieval-based (ir-based) methods [cit] . sp-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. ir-based methods usually search answers from the knowledge base according to the questions, where ranking techniques are often adopted to make correct selections from candidate answers."
"in this part, the ability of our proposed model is tested in several aspects. there are two stages for training the proposed model. in the first stage, both tool inference layer and answer generation layer are trained separately. the data used to train the tool inference layer covers 90% of wikitool, and 80% of data in wikiservice is used to train the answer generation layer. in the second stage, the proposed system is trained with the designed reward functions in reinforcement learning. the rest data is used for validation. one thing to be noticed, volume 7, 2019 the tool inference layer was trained independently to reach an accuracy of 95.4% for producing valid tool information, because precise tool information is the precondition of producing effective answers."
"an additional use case demonstrates a \"pay-as-you-drive\" scenario -where a driver's insurance rates depend on their monitored driving behavior. in this scenario, an application at the network edge (e.g. connected car) enables the data subject (driver) to identify and limit the transfer of personally identifiable information to the service provider for providing an agreed upon service (e.g. usage-based car insurance). this can be attained through the application of policies that match the intent of the data subject to the data, while also enabling the data subject to apply data minimization to certain data (e.g. sensitive data the data subject is not comfortable sharing, or data deemed not to be relevant for the purpose of service contextualization) prior to its transfer to the service provider. data subjects are able to opt-in/out of secondary/tertiary processing of the data beyond the original agreed-upon purpose and the transfer to third party organisations, as per their rights under the gdpr."
"the domain model which provides a collection of data fields, entities and attributes. the domain model in contrast to the cidoc crm provides concrete examples of data fields in a range of relevant existing projects."
"as the main source of the dataset, wikihow is a worldwide collaboration of thousands of people focused on one goal: teaching anyone in the world how to do anything. as a result, there is a large part of information teaching how to perform home services. another reason for selecting wikihow is its semi-structured representation. due to the uniform writing style, the representation facilitates information extraction for constructing datasets. wikismall is a parallel corpus which has been extensively used as a benchmark for evaluating text simplification systems [cit] . it contains aligned complex sentences from the ordinary english wikipedias. and wikianswers, a wikipedia-based dataset, consists of question-answer pairs in different fields."
"to make sound adaptation decisions at run time, one needs a model of the system, its requirements and environment. the model must be available at run time to enable online reasoning, hence it is called model@runtime [cit] . data protection concerns relate to all layers of the cloud stack, including secure hardware capabilities, colocation of different tenants on the same server, encryption of communication between application components, and data anonymization. all must be captured by the model@runtime. by monitoring the state of the system and its environment, updating the model, and comparing observed behavior to expected behavior, violations of data protection policies can be detected. if a violation is detected, further reasoning using the model@runtime can be used to automatically find and execute an appropriate adaptation action. this way, data protection issues can be mitigated or prevented automatically."
"in order to increase the accuracy and stability of the nn-based qa systems, an ameliorated nn-based method with reinforcement learning has been proposed to improve the performance of question answering on home services. in the proposed method, question representation is extended with tool information to represent proper information, tool information (including tool names and tool sequences) is regarded as crucial elements to promote the model performance, and question term attribution is also introduced as a rule to increase the stability. in order to verify the proposed method, three experiments are constructed to appraise the performance. firstly, our method is compared with several stateof-the-art methods in terms of effectiveness on producing qualified answers. the numerical results demonstrate that the proposed method can achieve better performance than other methods. in other words, the nn-based method with extended question representation and standards on accuracy and stability is effective. [cit] perturbation questions to verify the stability. the results prove that out method is able to provide competitive results and has merit among other nn-based methods in question answering. thirdly, impacts of components in the proposed model are analyzed thoroughly. the results indicate that the components based on tool information and question term attributions are the key elements to outperform other nn-based variants of question answering. therefore, the presented method is successful and suitable for producing answers oriented to home services."
"if we dig deeper into the decentralized australian heritage system we can find hundreds or possibly thousands of databases within university, research, parks, community and other public and private archives. while no apparent standardized structure is evident, no detailed analysis or effort has been taken to date to bring these archives together. [cit] a designated session proposed the assigning of a task group to focus on the issue but to date no formalized output has been generated [cit] . the use of reference and domain models in rock art applications: looking at the specialised field of rock art a similar picture emerges. while efforts have been undertaken for best practice research through eg the rock art stability index (rasi) no standards have been introduced within the field to date [cit] . while guides are written for fieldwork research the question of how this data gets integrated into a centralized national or global system remains."
"the use of an asset-based risk analysis approach supports compliance with information risk management standards like iso 27001 [cit], while the use of automation based on machine reasoning makes it possible to perform this analysis on a continuous basis in the loop of autonomic cloud application and infrastructure management processes."
"results demonstrate the good performance of the proposed method, thus we believe our method is more effective than others in the field of home services."
in order to develop the framework for the radb we need to take a closer look at existing systems on a local and international scale and ask the following questions.
"in this paper, we limit our research in the area of home services, because we believe that there exists valuable information in texts oriented to home services, which can be used to provide guidance on performing services. different from other areas where a topic related sentence can be regarded as appropriate, answers related to home services are logic and complicated, and can be observed as a sequential composition of subtasks. for instance, the answer to how to cook coffee should involve the operation of each subtask with correct orders, missing tools or reverse order can lead to failure of performing home services. unfamiliar service tasks can be performed with the assistance of correct answers, so the generation of effective answers is valuable and practical, which is the precondition of information utilization. based on this point, our goal is to construct a nn-based, home-service oriented qa system producing answers with accuracy and stability. there are three highlights in our method: firstly, tool information (tool names and tool sequences), as inner knowledge in the system, is applied to extend the question representation in qa system. secondly, the conception of integrated gradients (ig) is introduced as a standard to evaluate the attribution of question terms, in order to increase the model stability. the stability in this paper is the ability of a qa system on resisting external perturbations, which means the attacks from adversarial questions. thirdly, reinforcement learning is employed to integrate key elements that determine effective answers, and optimize the model parameters selectively. with the ability of reinforcement learning, the model can be taught to produce answers complying with the designed rules. to verify the performance of our proposed method, comprehensive experimental tests have been conducted. to begin with, our method is compared with a few state-of-the-art methods on producing effective answers in tasks of home services. then, by crafting a variety of adversarial questions, we make a comparison with other methods on the stability. finally, in order to verify the effectiveness of components in the proposed models, models with different component combinations are compared. experimental results demonstrate that the proposed method has better performance on generating effective answers, especially when adversarial questions are added. in a nutshell, our method can succeed in increasing the accuracy and stability of models."
"question answering (qa) is one of the most challenging problems in artificial intelligence, which contains language understanding, reasoning, and the utilization of common sense knowledge. given natural language questions, the goal of qa is to automatically return answers according to corresponding questions. with the process of deep learning, neural network-based (nn-based) methods have attracted more and more attention, and also have been introduced to the qa task [cit] . different from previous methods, nn-based methods represent both of the questions and the answers as semantic vectors. then the complex process of nn-based qa could be converted into a similarity matching process between the produced answers and the correct answers in a semantic space. the aim for a nn-based qa is to maximize the similarity score of the produced answers, when compared to the right answer."
the original idea for the radb grew out of a phd project that looked into the use of new technologies within australian rock art research. the project's research data was managed in a purpose build repository that was designed to allow for the management and presentation of a wide variety of digital data formats. as more and more data was accumulated the system needed to be redesigned and eventually lead to the idea for a global rock art database.
"both tool sequences from the tool list and tool information are parametrized, and transformed with z-score. the vector representing the tool list is denoted as a, and b represents the vector of tool information. len(.) is a function used to return the length of lists. it is comprehensive when the tool number in a equals to that of b, so tool sequence is the only factor to be considered. however, there are situations when tools are absent in a, thus the euclidean distance, a typical method for computing sequence similarity of equal length, is not suitable."
"secure cloud computing is key for business success and end user adoption of federated and decentralized cloud services, and as such, is essential to stimulating the growth of the european digital single market. and, while cloud-based data be kept secure, these data must also be made accessible to authorized users while ensuring privacy regulations such as the european union's general data protection regulation (gdpr) 2 . the restassured project aims to provide solutions to specific technical concerns of data protection in the cloud through four main areas of innovation (see fig. 1 ):"
1. use of cidoc crm 2. development of a rock art specific domain model 3. mapping the cidoc crm against the rock art specific domain model (see figure 4 ) figure 4 . the radb system
"the function domain consists of system and user defined functions. it encompasses relationships between information objects, metadata or users and is concerned with data interpretation and dissemination."
"different tools are involved in the process of performing home services. in the task of cleaning floor, there are brooms, dustbins, rags, and buckets, and each tool plays an crucial role in performing home services successfully. given this respect, correct tool selection is taken as metrics to evaluate the comprehensiveness of answers. by taking tool information as a benchmark, we compare tool names in tool information with that in answers. the produced answers are considered to be comprehensive when tool names in answers are consistent with that in tool information."
"besides correct tool selection, tool cooperation is also a key factor that determines the answer quality. the tool cooperation, which is reflected in tool sequence, can imply the logic of produce answers. in the task of cleaning floors, sweeping the floor should be prior to mopping, so the mop should be after the broom in tool sequence. it affects the result when the order is reversed. owing to this character in home services, tool names in tool information are extracted and aligned with the order they appear, as the correct tool sequence."
"there are limitations in our proposed method. the proposed system is designed specific to home service tasks, because tools are key factors in performing services, and can be used as standards to estimate the answers. but this system is inadequate for other areas, a heuristic method for extracting relevant features automatically is still necessary. also, manual efforts are involved in the process of designing rewards, which should be modified. in addition, the system has been trained with datasets containing only topic sentences, in order to reduce the conplexity of dealing with long sentences, so the ability of the system to handle complicated sentences should be further improved."
"in order to maximize the expected reward, the policy gradient algorithm is employed to optimize parameters of the agent. instead of q-learning, we choose policy gradient algorithm since the agent parameters need to be initialized with maximum likelihood estimation (mle) for producing plausible answers, while q-learning cannot satisfy this as it estimates future rewards from actions directly. during the training process, our model has been parallelized with a 4-gpu machine with the mini-batch size of 128, the learning rate is set to 0.001, and the training time takes 72 hours. the expected reward corresponding to an answer is defined as follows:"
"given the produced answers, parameters of the nn-based qa system are optimized with reinforcement learning. the composition of tool inference layer and answer generation layer acts as the agent to produce answers according to question terms. its parameters denote the policy for answer generation, sentences in answers are actions taken by the agent, the previous history of actions and the last sentence make up the current state, and rewards designed for both accuracy and stability are given conditioned on states. in the aspect of accuracy, tool names and tool sequences in tool information are regarded as standards to estimate the answer quality. more details are described in section 5. in the aspect of stability, integrated gradients (ig) is introduced to estimate the attribution of question terms in question answering. when question terms with high attribution are uninformative words, the corresponding reward is negative. and the reward is positive, when important question terms are denoted as high attribution. the policy gradient algorithm is employed during the training process."
"to cope with continuously changing data protection requirements in a continuously changing cloud environment, we apply methods from the field of self-adaptive systems [cit] . this way the system can adapt to changes in both the cloud and the data protection requirements, ensuring that requirements are met in the presence of changes, with minimal impact on performance and costs. adaptations may be made either fully automatically, or after approval from a human operator."
"in contrast to the cidoc crm the domain model informs the specific needs for the radb system including data fields, entities and attributes. the domain is broken down into constituent domains in which the radb system seeks to operate. for the purpose of the breakdown the model adapts the digital library domain model breakdown [cit] ."
the radb further seeks to find support within the national and international rock art community to assist with future development of the rock art database project with the aim to grow a sustainable platform free to use for rock art enthusiasts and professionals around the globe.
"another factor that determines the answer quality is the tool sequence in performing home services. we believe that subservices can be reflected in the tool sequence. for example, in the task of cleaning floor, we should sweep the floor first, and then mop it, so the tool sequence corresponding to this task should be (broom, mop). owing to this character, tool sequence is taken as a standard for estimating the logic of answers. tool names from generated answers are extracted sequentially, and stored as a tool list. then the sequence similarity between tool list and tool information is evaluated. the reward function for estimating the similarity is designed as below:"
the cidoc crm is used as the foundation for the reference model similar to the use of the crm in the digital library reference model [cit] . it provides a semantic framework within which the rock art reference model can operate.
"the idea of centralizing heritage data is not new. worldwide projects exist that aim to bring together heritage data from a variety of projects each with their respective aims and objectives. such projects include unesco's world heritage list, google maps, google earth, google cultural institute, the global heritage fund or cyark to name just a few. while all these projects focus on different forms of data centralization, data management or data presentation, rock art related information is scarce and the little information that does exist is often difficult to find [cit] . a system that focuses on identifying rock art related issues is still missing."
"tests within the last 6 months have primarily focused on the flexibility of the radb system regarding statistical evaluations concerned with the domain content, functionalities, users and policies. the system currently incorporates over 200 rock art projects with varying levels of data integration. further reporting tools have been generated and successfully deployed based on the state of the environment reports of the australian department of the environment as well as state departments."
"due to the progress made by deep learning, nn-based methods are used in constructing qa systems. in qa systems, the representation of questions can affect the system performance. owing to the ease and applicability, the method based on bag-of-words (bow) is widely used to represent a question into a single vector. however, there are two shortcomings in this kind of representation: first, the relationships between question terms are often ignored in bow methods, because the method considers only the frequency of each word. second, no enough attention has been paid to key question terms that determine answers. according to these problems, approaches with neural networks have been proposed [cit], the features of question can be extracted through a neural networks with multiply layers, and represented as a low dimensional vector. this kind of vector can capture semantic meanings and improve the representation, but the importance of key terms in questions stills lacks attention. due to the high abstract and ambiguity in words, external information is applied to modify the expression of word meanings. however, the selection and format of external information cost manual efforts. thus how to represent the question terms properly is promising."
"the heritage sector is no longer limited to classic heritage fields such as museum studies, anthropology or archaeology but overlaps with other fields such as computer sciences, natural sciences or engineering. new interdisciplinary collaborative networks help to improve our understanding of the world but also pose new challenges. different terminologies, discipline specific languages and methodologies can cause communication problems and ultimately interfere with the understanding and interpretation of shared data. it is becoming increasingly important to develop standards that allow us to share information in a common language and we need to reach for international standards through for example the international organisation for standardization (iso) or other authoritative bodies [cit] . complying with such a common language would not only mean that data could be understood by other professionals, it would also assist with establishing quality frameworks, ensuring compatibility, integration of larger systems and ultimately assist with a better understanding of the world as envisioned in gore's digital earth [cit] ."
"one of the greatest ongoing challenges to date within this project remains obtaining australian rock art and indigenous heritage specific information from national, state, community and local archives. this effected the optimization and streamlining process of data for the radb. the development of a rock art specific domain model has assisted with a better understanding of the urgent need to improve existing tools for managing heritage data. [cit] this paper proposes the forming of a task group to help improve australian heritage management through the development of a national reference model. the paper further proposes the forming of a rock art specific task group similar to unesco's world rock art archive group with focus on further developing the rock art reference model in conjunction with the ifrao."
"the outcome from this step is all the candidate architecture instantiations that meet area constraints. such a criterion can eliminate from design space solutions that lead to unacceptable overheads in device area due to excessive number of replica blocks. also, by allowing a designerdefined overhead in this metric, it is possible to explore and evaluate different architectural solutions. regarding our exploration, we set this area overhead to 35%, since otherwise our methodology leads to excessive area penalties."
"based on our observations and experience, we can classify existing trim command management strategies into two types: 1) block erasure in real-time, called e-trim, and 2) data invalidation based on address and prompt response, referred to as i-trim. as shown in figure 13, e-trim requires long processing times to erase physical blocks based on the target addresses specified by trim. for example, to process a single trim command covering a storage space of 2gb, ssd-l and ssd-z take 754 msec and 550 msec, respectively. note that the trim-latency increases linearly with the amount data that lbas in the trd frames aim to delete due to physical block erasures."
"in this paper, we successfully complete the optimization of conventional cordic algorithm, and resolve the problem of restrictive relationship of speed, area, precision in the design, break the limitation of angle coverage, provide a optimization for various functions by cordic algorithm, and accomplish the simulation of the optimized cordic algorithm with 16-bit on fpga. comparing the simulation results of optimized cordic algorithm and traditional one, we can get the conclusion that the optimized cordic algorithm reduce resource consumption, and increased the maximum operating frequency, the accuracy of the cordic algorithm is 10-5 as same as the data of traditional one."
"conventional storage interfaces hinder the scalability of modern ssds and make efficient ssd management difficult. to help with this, high-speed interfaces such as sata 6.0gbps and pci express are employed. further, the most recent version of sata provides ssd-specific command feature sets, which enable underlying ssds to expose their internal characteristics to the os. specifically, trim, one of these command feature sets, allows the os to invalidate data blocks that are no longer considered in use and delete the obsolete data at a system level. trim commands are expected to significantly reduce gc overheads and alleviate potential write degradation in many ssd applications. smart is another command feature set, which enables self-monitoring, analysis, and state-reporting. using it, os designers can effectively manage ssds by retrieving internal ssd information such as pe cycles and the number of channels and physical blocks."
"reliability is defined as the probability that a device will perform its required function under stated conditions for a specific period of time. predicting with some degree of confidence, strongly depends on defining a number of parameters."
"jim melton, editor of the iso sql standard, points out that the sql:1999 standard provides the capability to write a positive boolean expression, enclose it in parentheses, and append \"is false\" or \"is not true\" at the end, for example: where (some-boolean-expression) is false melton and alan simon document this capability in sql:1999. 18 they suggest that if a search condition \"sounds funny\" when read aloud, the phrase \"this test is false\" or \"this one is true\" could \"sound better.\" the implication is that such an exercise could help with comprehension. melton and simon acknowledge that the \"more complex a search condition is, the more likely you will want to consider using the is true, is false, or is unknown variations.\" melton and simon define negation in sql as \"removing rows\" as with except, intersect, distinct, not exists."
"to compare read performance with the performance of other types of operations, we executed different workloads composed of sequential accesses and random accesses with varying data transfer sizes (ranging from 1 sector to 128 sectors) on ssd-l, -c and -z; we observed that ssd-a exhibits similar performance characteristics to ssd-c, and ssd-p and -x achieve similar performance results to ssd-z in this test. performance comparison across our three ssds is plotted in figure 2 . specifically, figures 2a, 2c and 2e show variance in overall bandwidth, and figures 2b, 2d, and 2f plot variance in latency for ssd-l, -c and -z, respectively."
"in the clinical patient record, the medical narrative, as dictated by the health care professional, comprises an important element in the overall patient evaluation and treatment strategy. this narrative feature can be found in diagnostic observations, surgical notes, and discharge summaries -all of which are integral aspects of the patient record and which serve to augment laboratory test results. even objective diagnostic results, such as imaging (x-ray, ct, mri) outcomes, are subject to extensive narrative by the interpreting physician."
"copyright ⓒ 2015 sersc assuming the input angle was z0, the following four cases indicated the specific rules of conversion for 16-bit cordic algorithm, which is shown in table 5 ."
"the derived system's description is profiled in order to compute the power density of the functionalities described in xml file. for this purpose, input regarding power and area vlsi design 9 info, as it was already derived from post synthesis simulation, are employed."
"apart from arrhenius equation, we also evaluate the different architectures derived during our exploration, under the time-depended dielectric breakdown (tddb) [cit] . since oxide breakdown has already been of serious reliability concern in the semiconductor industry because of the continuous trek towards smaller device sizes, such kind of aging phenomenon should be carefully studied. defects occurred due to tddb are primarily caused due to the trapping of charges in the oxide that create an electric field, followed by charge flow through the oxide, resulting in a breakdown after sometime. the mttf due to tddb phenomenon is described by"
the meaning (semantics) of the natural language (english) query is often not correctly understood. thus the meaning (semantics) of the sql query implementation most frequently chosen by students does not match the meaning of the natural language query.
"more specifically, based on this figure we can conclude that a controllable area overhead (e.g., 20% increase as compared to the multiprocessor solution composed of leon3 components selected previously) leads to the reduction of the maximum temperature by almost 0.85x of the previous corresponding value."
"all the activities for handling read disturbance management, runtime bad block management, and ecc, referred collectively to as reliability management on reads (rmr), require additional i/o operations and compute cycles. these latency comparison between reads with reliability management (rmr) and ordinary reads (i.e., reads without rmr). when rmr is employed, the latency is at least 5 times higher than the latency of reads without rmr. overheads are not revealed to users, but can contribute to long latencies on normal operations. in this section, we examine the latency variation between reads with rmr and reads without rmr, which is veiled by most ssd manufactures. for our evaluation, we executed the random read-only workload, used in section 4.4, on three devices, ssd-l, -c, and -z, and the results are given in figure 7 ."
"usually there is a third clause, a where clause, which specifies particular conditions (restrictions) which apply to the query and the logical connections among tables in the database. this clause has the form of a condition, normally a compound condition using logical connectives (and, or) and may include negation. sql is based primarily on set theory and formal logic statements."
"the following conclusions can be derived from this figure. more specifically, as we increase the area of target architecture, the temperature values are reduced (almost monotonically). however, this temperature reduction in not constant since just replication of blocks does not guarantee alleviation of thermal stress (as we have already depicted in figure 3(c) ). hence, apart from the number of replica blocks, their properties (e.g., power density, area, power consumption, etc.) are also crucial for designing an efficient architecture."
"nand flash-based solid state disks (ssds) have recently become immensely popular and been employed in different types of environments ranging from embedded systems to personal computers to high performance computing (hpc) systems. moreover, various memory and storage systems have been proposed to take advantage of the performance benefits of ssds over conventional block devices. for example, to reap the benefits of high bandwidth on writes, prior hpc studies consider ssds as a burst buffer [cit], which can absorb heavy write traffic caused by check-pointing [cit] . there also exist many applications developed under the expectation that nand flash is biased toward reads in terms of performance and reliability. enterprise servers, for example, consider employing ssds for applications that exhibit many random reads [cit] or use them as read caches [cit], sitting between main memory and hard disk drive (hdd). similarly, ssds are also introduced as a main memory replacement, memory extension, and a part of existing virtual memory systems [cit] ."
"cordic algorithm is the best choice to achieve the functions of transcendental functions such as trigonometric, inverse trigonometric, exponential function, logarithmic function since that the cordic algorithm is provided with a simple structure, and characteristic of saving resources and high efficiency, as while as cordic algorithm is used widely for matrix operations, especially possess significance for development of transplant of highly complex operations in fpga, such as analysis algorithms for multidimensional data array [cit] . cordic algorithm is an iteration algorithm and commonly used to calculate basic arithmetic functions. the principle of the cordic is to use the deflection of the angles associated with the cardinal number, rather than get the desired angle. thus, the principle can be considered as a numerical approximation methods of calculation. because the fixed angle is related with cardinal number, the operations of computation include only shift and addition or subtraction. therefore, it will cost less fpga (field programmable gate array) resources than conventional calculation methods, such as multiplication and division. the conventional calculation methods are difficult to achieve or can not meet the designer's requirements. the appearance of cordic algorithm is to solve this problem, the cordic can greatly saving fpga resources to get better implement in hardware, which can achieve the requirements of the designer."
"in this paper we discuss how to implement the cordic algorithm with fpga, model it in verilog hardware description language, simulate it by eda tools, and analyze the"
"state-of-the-art ssds are composed of multiple cores, memory modules, data buses, and storage media. in the following, we provide a quick overview of ssds and nand flash, basic flash firmware features, storage interfaces, and reliability issues."
"these time periods are retrieved by incorporating info from application's simulation. the employed utilization ratios are averaged over hardware components of leon3 system in order to determine the active/idle time slots accurately. the output from utilization is fed as input to ptracegen tool in order to redistribute the activity of each component to a timing trace of several time slots. this is achieved by setting each of these components either as active or idle for every time slot. furthermore, the area derived in this stage may be different from the one computed during area filtering, due to additional free space inserted to design after floor plan (which is not occupied by any hardware block) in order to model the white space between hardware components."
"next, we depict that the criterion of power density is much more important than the corresponding one about power consumption. for this purpose, figures 3(b) and 3(c) give the thermal profiling as they derived with hotspot tool [cit] about a leon3 processor running sdr applications (e.g., filters, encoding/decoding, etc.), when the five and three most critical components retrieved with the previously mentioned analysis, respectively, are replicated two times. in order to perform this replication of hardware blocks, we incorporate the methodology introduced in this paper. we note here that all of these floor-plans were retrieved with hotfloorplan tool [cit] ."
"there is a class of exclusionary sql queries that the majority of students (in and undergraduate database management systems (or even in graduate-level course on databases) will incorrectly implement in sql. this is a practical teaching concern. these queries are called exclusionary queries because a portion of a set is excluded. here are examples from texts commonly used in our department, with negation underlined:"
"1. are ssds biased toward reads at a system level? why do random reads constitute a performance bottleneck in modern ssds? 2. for the sequential read accesses, could ssds support sustained performance? is there any performance degradation on reads? how could a system achieve a sustained read performance? 3. what is the relationship between read performance and internal ssd parallelism? can users characterize read performance by examining different i/o access patterns?"
"in order to show the importance of proper identification for hardware blocks that have to be replicated, figure 8 plots the temperature variation (in kelvin) versus the power density (w/cm 2 ) for each instantiation of the selected architecture with replica blocks. we choose to evaluate such a criterion, because power density for existing and upcoming devices becomes a major issue for architects. researchers have already identified this problem, whereas based on projections it is expected that power density regarding 14 nm nodes will be higher than 100 w/cm 2 [cit] ."
"our first observation is that the performance values with random read accesses (denoted using rnd-rd) are worse than other types of access patterns and operations, including even random write accesses, which is in direct contrast with the widely held expectation on read performance of ssds in the literature. specifically, the bandwidth values with random read accesses of ssd-l, -c, and -z are worse than the corresponding values with random writes by 59.7%, 39.4% and 23.7%, respectively. read latency characteristics are not much different from bandwidth; the latency values observed with random reads are worse than the latencies observed with sequential writes, random writes, and sequential reads by 41.3%, 35.2%, and 35.9%, on average, respectively."
"1. in theory, os and users can eliminate unnecessary gc operations through trims. how much of gc overheads can be eliminated using trim commands? 2. does trim command request pattern matter? 3. do trim commands themselves impose any overheads?"
"based on figure 9, the maximum temperature gradient occurs for architectures with smaller areas, whereas for architectures with area more than 66% of the maximum area is almost constant. if we take into consideration also the smaller delay overhead posed by devices consisted of fewer replica blocks (as it was already shown in figure 7), an additional filtering of derived architectural instantiations can be performed. more specifically, without affecting the generality of the proposed methodology, solutions that correspond to area overheads higher than 66% of the maximum area are assumed not to be desirable (due to additional delay), and hence they are eliminated from exploration space."
"even though bflush and bgc can recover their ssd performance unlike ssd-p, -x, -z and -a that do not have any background activity, we observed that the performance is not sustained or stable when considering the total execution time of write cliff. figures 15 and 16 plot performance characteristics of bflush and bgc, respectively, when we inject an idle period of 1 hour. in the case of bflush (figure 17a ), the recovered performance is sustained for 3 seconds, which provides stable performance for only 0.1% of the total execution time on the write cliff (1,711 seconds). as shown in figure 17b, the performance sustainability of bgc is not much different from that of bflush. even though bgc can keep the recovered performance much longer than bflush, it still sustains the recovered performance only about 120 seconds, which accounts for only 7% of the total execution time on the write cliff. we describe the difficulties behind the background tasks in the next section."
"deep learning has nonlinear mapping of the deep structure with the multilayer which has the benefits complex function can be expressed with fewer parameters. compared with surface learning, it can realize complex function approximation, and has strong ability for the massed learning of the essential characteristics of data set from a few samples. based on the above considerations, this paper proposes a hybrid malicious code detection model based on deep learning; reducing dimensionality of the data by using the autoencoder's space mapping ability of different dimensionality, then abstracting the main characteristics. based on this, setting dbn as the classifier for several times deep learnings. then improving the detection accuracy, and reducing the time complexity of the hybrid model. figure 5 depicts the process of mixing pre-trained detection algorithm. the hybrid detection algorithm is described as follows:"
(ii) we introduce of a novel methodology targeting to provide: (i) elimination of thermal hotspots at socs targeting sdr architectures and (ii) alleviation to the temperature gradients.
"as a reference point for this study, we use the thermal map for a leon3-based soc sdr architecture when no replica blocks are assumed. this map, shown in figure 3 (a), exhibits a temperature hotspot in region where the blocks with increased power densities (ahb controller, instruction unit and cache controller) are floor planned. this hotspot results in increased temperature value, as compared to the average onchip temperature, about 7%."
"we target on lightweight enhancements in original leon3's datapath to avoid extensive area, organization, and control overheads in respect to the original datapath. for this purpose, we apply selective replication in a coarse grained manner, that is, replicating at the level of instruction unit, rather than at the alu unit or the instruction fetch level. furthermore, we avoid replication of the actual memory components (i.e., register file, data cache, etc.), since their replication will require proper control mechanisms to establish data coherency among the various replicas."
"one concern that os designers might have is the potential overheads that can be experienced by an ssd in processing the trim command itself. in this section, we measure the latency incurred when processing a trim command, using our in-house ahci miniport driver and the lecroy protocol analyzer, sierra m6-1. to do this, we wrote data varying from 512b to 2gb, which has the same addressrange coverage as a trd frame, and sent a trim command by filling the corresponding lbas into the trd frame. we then captured the time duration from the data-set-management command submission to the end of the response of the following trd frames as trim-latency. since sierra m6-1 [cit] provides a detailed protocol-level timing model for the command issue and completion, we are able to capture trim-latency for each trim process. as shown in the previous section, since ssds do not get any benefit from rnd-trim, in this test, we focus on measuring trim-latency on seq-trim."
"in recent years, a substantial portion of these narrative reports have been integrated into computerized patient record databases not only for individual retrieval functions, but also for aggregate clinical research purposes. medical informatics researchers have been challenged in their efforts to implement exacting information retrieval search mechanisms due to several unique aspects of the traditional medical narrative. for example, as mutalik notes, \"for a medical document, the presence of a concept does not necessarily make the document relevant for that concept. the concept may refer to a finding that was looked for, but found to be absent or that occurred in the remote past.\" 12 simple automated concept indexing of medical narratives can also lead to many \"false-positive\" retrievals since health care personnel typically heavily record negatives in their narratives. for example, a simple search on the concept of \"glaucoma\" in a patient database would also yield a substantial number of reports stating \"no evidence of glaucoma\" or similar phrase. furthermore, the ambiguities of syntax and subtleties of natural language exert an additional impact on the identification of negation in medical narratives."
"in this paper, we propose the adoption of selective replication techniques in order to optimize the thermal behavior of the synthesized microprocessor systems targeting at an sdr system. we developed an automated exploration methodology that permits the thermal aware evaluation of various micro-architectural instantiations."
"2) we should be clear what set we are subtracting from: we need to include customers who did not place any order, not just those who placed an order, but on some other date. we should not assume that the according to the iso sql92 and sql:1999 standards 7, set difference could be expressed using the except operator:"
"the most remarkable performance characteristic shifts on modern ssds are observed in reads, since they are vulnerable to changes in internal ssd architecture. in this section, we first examine overall performance, comparing reads with other types of operations, and then analyze the challenges on reads in terms of performance sustainability and performance dependency on internal parallelism. lastly, we uncover reliability problems on read-intensive workloads, which is one of the most critical issues, in our opinion, for both the os and ssd designers."
"the optimization goal during this procedure is to reduce the thermal stress for each architecture in respect to the timing constraint. the alleviation of thermal stress is performed by spreading as much as possible the hardware blocks that contribute more to higher values of onchip temperature (e.g., modules with increased power densities). similarly, by minimizing the perimeter of bounding box that surrounds all the modules that are connected with a single bus, it is possible to improve the delay of this bus. hence, blocks that are connected through bus(ses) have to be floorplaned in spatially close locations."
"as indicated by these plots, the read latency with rmr is at least 5 times higher than the latency of reads without rmr, which would be unacceptable for latency-sensitive ssd applications. further, rmr overheads are more pronounced with small size random access patterns (1 sector ∼ 64 sectors), which is the dominant request size in many file systems. considering 8 sector (4kb) requests as an example, while the read latencies without rmr on ssd-l, -c, and -z are 75, 60, and 47 µsec, respectively, the increased read latencies with rmr are 685, 1787, and 4944 µsec, in the same order. even though the latency disparity between ordinary reads and reads with rmr tends to decrease as the transfer size increases, high rmr-induced latencies with large data sizes (1mb ∼ 32mb) still seem to be problematic."
"next, we will quantify the efficiency of the above solution when this is used in multiple instantiations of a multiprocessor architecture. figure 13 shows the variation of power density as we select solutions with higher area overheads. based on this figure, we can conclude that power density seems that it does not depend on silicon area. we have to mention that, during this analysis, the additional area mostly occurs due to the whitespace between hardware components, rather than the area of these replica components."
"unlike writes, reads require no erase operation or contentupdate on nand flash. consequently, many computing domains exploit ssds as a read cache or an intermediate layer when targeting read-intensive workloads to extend ssd lifetime and avoid heavy write penalties."
"during the last years, a number of different sdr-based architectures have been developed, whereas a typical instantiation is depicted in figure 1 . the front-end is responsible for converting the signal between the rf domain and an intermediate frequency, and the a/d and d/a components convert the signal between the analogue and the digital domain. in our analysis, the baseband functionality is carried out on software running on a system-on-chip based on leon3-embedded processor [cit] ."
"figures 6a and 6b give the variance in pe cycles on two different ssd-a instances under sequential and random access patterns, respectively. one can see from these plots that pe cycles increase in every evaluation round, in a direct contrast to what the current literature on systems exploiting ssds would lead one to believe. in sequential reads, the maximum pe cycles reach the half of pe cycles on writes, as shown in figure 6a . ironically, the maximum pe cycles with the random read-only workload are higher than that of writes by about 12x (figure 6b ). we believe that the reason behind this pe cycle increase on read-only workloads is the read disturbance and runtime bad block management. since these activities require erasing block(s) and live-data migration to the target block(s), read requests can shorten the ssd lifespan and significantly degrade overall performance. further, the disparity between the maximum and average pe cycles tell us another story; wear leveling strategies employed by current flash firmware mainly focus on writes, not on reads. while ssd-a firmware keeps reducing the gap between the maximum and average pe cycles on writes, the maximum pe cycles on reads is 247 times higher than the average pe cycles in each round, which makes certain blocks wear out faster and worsen ssd endurance characteristics."
"as demonstrated in the previous section, all the ssds tested generate their best performance on sequential read accesses. in this section, we further examine whether the sequential read performance can be sustained over time or not, which might have a significant impact on read-intensive ssd applications. for this set of experiments, we executed sequential read accesses with transfer sizes ranging from 1 to 16 sectors on two different ssd sets; \"pristine\" ssds and one can observe from these results that most of the read requests on all pristine ssds are served within 300 ∼ 400 µsec. however, when ssds get order, cdf curves shift from left to right exhibiting worse performance characteristics. the aged ssds take over 600 µsec for serving all the i/o requests, which is two to three times worse than our pristine ssds. one can conclude from this analysis that, sequential read performance characteristics get worse with aging and as i/o requests are being processed, which are unfortunately captured neither by nand flash data sheets or nor by ssd specifications. we believe that this performance degradation on reads is mainly caused by fragmented physical data layout and reliability management overheads on reads. this read performance degradation also implies that the read behavior of an ssd cannot be easily characterized by the os by examining only the current i/o request patterns despite recent works [cit] claiming that. we will provide a deeper evaluation and more evidence on this read performance characteristic in the following sections."
"we measured performance by writing data of different sizes trimmed using seq-trim and rnd-trim. in addition, we also evaluated the performance of our pristine-state ssds, denoted by pristine, and ssds that have no trim command management, called non-trim. the main insight from this evaluation is that, if ssds tested handle the trim commands appropriately, all the written data should be successfully deleted, irrespective of which trim pattern is employed. as a result, the trimmed ssds are expected to exhibit the same performance as our pristine state ssds. as shown in figure 11, seq-trim works very well for deleting data in both ssd-c and -z. as expected, the bandwidth of the ssds trimmed by seq-trim is similar to that of pristine. in contrast, rnd-trim shows no success in alle-(a) ssd-l."
"for this purpose, thermal management has recently received a lot of attention by design architects. the goal of thermal management is to meet maximum operating temperature constraints, while tracking timing specifications. moreover, thermal management can also achieve further temperature reduction in order to improve the reliability degradation of socs."
"however, the correction factor is not easy to determine in advance, and it is difficult to calculate it immediately. another approach called sub-quadrant method is taken in this paper. this method takes advantage symmetry of trigonometric, and converts all of the perspective of the whole cycle into the first quadrant. then it preserves the phase information according to the rules of transition, and puts it into the module of computation for cordic algorithm. after that, it restores to the original angle of the sine and cosine of the phase information."
"we also take a 16-bit cordic algorithm as an example, chose the same chip and selected the same angle. and the angles are indicated with 16-bit unsigned binary, the results of simulation are shown as 16-bit complement, and the msb is the sign bit, the remaining fifteen fits are decimal, the waveform diagram is shown in fig. 7 . the simulation results of sine and cosine function are shown in table 8 and table 9 . from the results, the optimized cordic have a high accuracy as while as enhance operating frequency."
"the sequences of 16-bit θ i taken part in symbolic computation are represented with two's complement in table 2 . in table 2, we can see that as the number of pipeline level increases, the capacity of the rom table grows exponentially, and the area of the system will also increase. as can be seen in fig. 2, it requires multiple iterations for once computation of cordic algorithm and the direction of iteration must be determined by the result of the last iteration. the more the number of iteration is, the more the number of direction determination must be required, which will undermine the speed of operation."
"apart from area, the maximum operation frequency also affects the onchip temperature values. based on figure 7, the alternative architectures lead to performance variations up to 14%, which mainly occur due to (i) additional replica blocks inserted into the design, (ii) the consequent different floorplans, and (iii) the increased wire-length for connecting these blocks."
"patricia wright states that instructions \"can also be difficult to understand if they involve negation.\" and that \"[c]omprehension problems seem inevitable when negation is combined with frequency information.\" 14 wright and bernard cite prior research showing \"that instructions phrased with a negative component are more difficult to understand than their affirmative counterparts\" and demonstrate that \"the phrasing of a decision rule can have a substantial effect on performance with numerical tables.\" wright and p. bernard speculate that the latency and accuracy differences of subjects in their research may have reflected internal \"recasting\" of a decision rule. 15 the presence of ambiguities related to negation is acknowledged in communications skills textbooks and is the focus of exercises. clark states that \"negation is found in vastly different guises\" and is \"an extremely heterogeneous phenomenon.\" 16 in addition to the extensive research on comprehension of negation in natural language, consideration should be given to the particular properties of the use of negation in sql. when negation is used in sql queries involving not exists or implied by a false result for a query involving exists, there is an implicit closed world assumption necessary for statements about (truth or falsehood of) existence to make sense. 17 the assertion that some statement is false means simply that the particular statement is not now stored in the database being used. in the treatment of sets a \"universe of discourse\" must first be posited before, for example, the concept of a complement makes sense."
"for supporting selective block replication, the processor microarchitecture has to be properly enhanced. we mention that in this paper, we focus mainly on the exploration methodology developed for the automatic evaluation of opportunities delivered through selective replication. thus, in this section, we briefly introduce some micro-architectural considerations that enable the design of processor architectures with replicated components. in the general case, the data flow and the control flow of the original processor architecture have to be modified towards two directions: (i) enabling mutual exclusiveness between the replicated units, and (ii) permitting run-time management of the replicated resources according to the run-time thermal state of the processor. we focus our analysis on the risc microarchitecture of leon3 [cit] embedded processor. the block diagram of a sdr system based on leon3 processor was already depicted in figure 1 ."
"specifically, we target at the development of an automated design space exploration framework that extracts and evaluates a large number of architectural solutions. every solution exploits selective block replication. based on the software-supported automatic exploration, we are able to compute higher thermal quality pareto curves, in contrast to many similar existing optimization approaches that retrieve only a single architecture [cit] . hence, architects can tradeoff between the desired level of temperature reduction at hotspots and the resulting timing/area/power overheads. furthermore, the supporting tool framework provides a considerable speedup at the exploration procedure."
"the output of ptracegen tool is a set of workload distributions based on employed applications, as well as accurate area estimation after floor plan. more specifically, the workload distribution is achieved by appropriately handling the power info per block of the target architecture (component of the selected level of granularity), as they were already derived from postsynthesis simulation. this approach guarantees that hardware blocks (replicated or not) dissipate power consumption only the time periods denoted by the application's functionality."
"even where the sql:1999 is false syntax is not available, a statement form of that type can be used to analyze the meaning of a query and should be recommended. statement forms using \"except\" can be used in analyzing meaning even when an sql set difference operator is not available in a given database product."
"the magnitude of performance gains with the trim commands significantly varies depending on the trim request pattern. while seq-trim can effectively eliminate gc overheads, rnd-trim has no positive impact on performance. in addition, ssds require quite long execution times to process trims, which can lead to unexpected performance degradation. to address this, a file system can consider new trim management strategies that are aware of the trim-process characteristics. trim buffer and scheduler. from the beginning of the trim process, the host can send trim commands by composing target addresses in an increasing order. similarly, a host module can buffer trims and merge the delete information under the file system in order to transform rnd-trim to seq-trim. in addition, it is better to utilize idle times to submit trim commands at a system level to avoid potentiol trim-latency overheads. a trim scheduler can blend legacy i/os with trims by utilizing system idle periods, thereby hiding the long trim execution times."
"the derived solutions are then evaluated in term of delay degradation, as compared to delay estimation retrieved from postsynthesis simulation. for this scope, we use the elmore delay model [cit] . since we are primarily interested to retrieve a thermal-aware solution, we already knew that a penalty in architecture's performance is affordable. for our study, the timing overhead is assumed to be 14% in order to avoid any mentionable delay overheads."
"autoencoder consists of three steps, which are pretraining, unrolling and fine-tuning process [cit], as shown in figure 1 . in the pretraining process, we set the output of each rbm hidden layer neuron as the input of the next rbm. rbm consists of the visible units and hidden units. we use the vectorv and h represent the visible units and the hidden unit state respectively. the structure is shown in figure 2 ."
"since embedded cores usually are designed with low power criterion, many researchers up to now pay effort to reduce maximal temperature values by identifying blocks that dissipate increased power budges. regarding the leon3 processor, the local data/instruction memories, the l1 data/instruction caches, as well as the register file are found to be the most power hungry blocks. more specifically, the average power consumption at these blocks, as compared to the total power dissipation, is 57%, 31%, and 8%, respectively. figure 2 (a) provides a first order metric about the components with increased power consumption, we show that it is not enough in order to retrieve conclusions about their importance regarding the thermal stress. this occurs since the power metric does not take into consideration the area of underline hardware block, which is especially crucial for thermal spreading. hence, a more representative metric should be employed in order to evaluate the importance of each core into the chip's temperature values."
"in order to study the correlation between areas occupied by target architectures and the maximum temperature values, figure 9 plots the corresponding diagram of these parameters. for the sake of completeness, we cluster alternative architectures into three groups based on their area (similar to previous figure), while for demonstration purposes we also compute the temperature gradient for each of these clusters."
"if we replace arctan(2-i) with 2-l from the m-level iteration, there is no effect on the calculation accuracy of the final results. we could use shift operation instead of process of checking the rom table, which speeding up the computation and reducing the occupancy of resources. the key of the theory is that the tolerance introducing by replacing arctan(2-i) with 2-l. for the computation of cordic algorithm, it is unnecessary to check arctan(2-i) from rom table, if replacing it with 2-i when the number of iteration is greater than or equal to m, which reduces the time accessing to rom, enhances the speed of computation, and reduces the rom resource by two-thirds [cit] ."
"the rest of the paper is organized as follows: section section 2 introduces the underline sdr architecture, whereas section 3 discusses motivational observations that guide us to propose selective insertion of replica blocks. section 4 describes in a brief manner the microarchitectural enhancements needed for applying selective block replication in existing microprocessor architectures. the proposed methodology is analyzed in detail in section 5, while a number of evaluation results are discussed in section 6. finally, section 7 concludes the paper."
fine-tuning process is the process that does the further adjustments to the initial weights after pretraining process to get optimal weights. we mainly use the multiclass cross-entropy error function [cit] for evaluation.
"in this paper, we examined widely held expectations and conceptions on modern ssds using six different commercial ssds and a series of experiments. our experimental results revealed many previously-unreported ssd characteristics from both performance and reliability angles. we also discussed what these characteristics mean to both ssd designers and system designers. our ongoing work includes designing and implementing system support that can take into account our newly-discovered facts on ssds, and evaluate this support using a diverse set of workloads drawn from embedded computing, enterprise computing and highperformance computing domains."
"at this procedure, an extra architectural parameter needs to be defined. more specifically, apart from the blocks that need to be replicated, we also need to clarify the maximum number for each of these blocks that can be replicated. since more replicas means better thermal management, in the expense of imposing overheads in area and delay, careful study should be applied. in this study, we evaluate solutions that correspond to maximum number of replica blocks up to five. this selection was based on our conclusion that architectures consisted of more replicas do not lead to additional temperature reduction (due to saturation effect). however, constraints posed by architecture specifications might reduce this number."
"in general in logic, as well as in computer-based implementation of logic programming languages such as prolog, it is recognized that the handling of complements presents certain challenges. only the minority of database students will have had a course on logic, in which prepositional and syllogistic logic have been treated systematically and thoroughly. results (from a group of 73) will be provided for student attempts to code four queries of a database consisting of two tables with the following schema:"
"based on the derived thermal profile, it is possible to evaluate the architecture instantiation in terms of different criteria tightly firmed to onchip temperature. for the scope of this paper, all the solutions that do not meet the selected thermal constraints (maximum temperature and the temperature gradient) are eliminated from exploration space, whereas typical packaging for embedded processors is assumed [cit] . thus, the output of the proposed methodology contains only the instantiations that correspond to architectural solutions that meet all the three constraints, namely, area, timing. and thermal."
"as figure 2 shown, rbm are mutually connected by the visible and the hidden layers. the connection matrix and the biases between the layers are get by unsupervised greedy algorithm. in specific training process, firstly, mapping the visual unit p v h , we derive the rbm weight update rule, as shown in formula (9): then fine tune the whole dbn from the back to the front by the supervised learning method which is similar to the traditional bp neural network. finally, we can establish the trained dbn model."
"previous works introduced the usage of parallelism in order to achieve power savings, which in turn lead to temperature reduction [cit] . a parallel implementation of a design essentially replicates component(s) of the design such that parallel branches process interleaved input samples. therefore, the inputs coming into each parallel branch can be effectively downsampled. an output multiplexer is needed to recombine the outputs, and produce a single data stream."
"we select such an embedded processor because it is widely used in numerous commercial and/or research products. however, apart from the selected target platform, the methodology we follow in this paper is also applicable to any other digital architecture."
"accelerated life testing employs a variety of high-stress test methods that shorten the life of a product, or quicken the degradation of the products performance. the goal of such testing is to efficiently obtain performance data that, when properly analyzed, provide reasonable estimates of the products life or performance under normal conditions. this induces early failures that would sometimes manifest themselves in the early years of a products life, and also allows issues related to design tolerances to be discovered before volume manufacturing. both the type of stressor and the time under test are used to determine the normal lifetime. regarding soc designs, usually the majority of these aging degradation mechanisms are tightly firmed to onchip temperature values."
"database education and training seeks to help information technology specialists (aspiring or current) learn how to use the syntax of sql correctly so that meaning of an sql query will accurately represent the corresponding natural language query of interest to an individual or organization. 1) in order to improve accuracy in the types of sql queries of interest here: a) within the scope of computer education and training, draw attention to problematic queries and help learners recognize these queries (this will include publicizing the nature of the problem within the discipline in educational and production settings) and describe actions to take to assure accuracy, such as: where such modules are in place, it may productive to show that the skills being acquired are of use not only in general discourse, but even in technical work environments, such as database programming."
"typical instantiation of this solution is the usage of dynamic voltage and frequency scaling (dvfs) [cit] . due to the approximately quadratic relation between supply voltage and power consumption, dvfs-based techniques achieve to provide mentionable savings in power consumption, but they impose slower operation frequency. moreover, these techniques cannot guarantee that temperature hotspots and/or temperature gradients will be reduced, since there are applied during runtime as a reaction to chip's thermal crisis."
"b) high school mathematics programs and college core curricula can include an introduction to logic (possibly within the context of a communication skills program) so that students become acquainted with the concepts of \"set difference\" and complements and with set operations in general. in some countries all students receive logic instruction covering sets, set operations, and predicate logic. c) students should be shown examples of \"false-positive\" retrievals in searches of text databases (in library orientation, web search training, or communication skills courses), so they will understand how retrieval searches can be affected by negative expressions."
"we believe that the main reason why ssds can experience opposite performance characteristics at a flash level (reads are much faster than writes at a memory cell level) is the lack of internal parallelism on random reads. note that, sequential accesses can be striped over multiple channels in a round-robin fashion, and the striped sub-requests can be interleaved across multiple flash dies in each channel. in contrast, random read accesses can potentially create a scenario where multiple requests end up contending for the same internal resources (e.g., channel, package, die, plane), referred to as resource conflicts. a request experiencing resource conflict has to wait for the completion of the other request(s) heading to the same resources. therefore, the resource conflict on random reads causes low parallelism and thus degrades both bandwidth and latency. unlike reads, flash firmware can easily forward the incoming write requests to a target sitting in idle by remapping addresses, which leads to low resource conflicts and high levels of parallelism."
"network data usually contains the normal data and the malicious data. malicious code detection is to differentiate between the normal data and malicious code data separately, so essentially it belongs to binary classification problems. to get a good performance of the malicious code detection model, there are two aspects of work need to be done: firstly, finding the essential characteristics of malicious code data; secondly, constructing a good performance of classifier model to accurately differentiate the malicious data from the normal data. in this paper, we make use of the advantages of deep learning, the organic integration of two deep learning methods, autoencoder and dbn. this hybrid model extracts the essence of malicious code data, reduces the complexity of the model, and improves the detection accuracy of malicious code."
"since we try to alleviate thermal stress mainly at hardware blocks with increased power densities, these are the blocks that should be replicated (as we have shown in section 3). for this purpose, hardware blocks of the design are sorted in descending order based on their power density values. from our exhaustive exploration study, we found that only a few of the total blocks exhibit increased power densities. hence, the architectures that contain all the possible combinations among replica blocks are evaluated. as a constraint to this procedure, we assume the maximum area overhead, as compared to the one retrieved when no replica blocks are assumed."
"based on this analysis, it is possible to make a decision regarding which of the architecture's hardware blocks have to be replicated. for this step, the power density for hardware blocks has to be measured. note that the total number of replication blocks is limited by area constraints posed by designer."
"a read operation may fail because of 1) read disturbance, 2) retention error (leakage problem), and 3) noise (e.g., at the power rails). we now explain what ssds do to address read failures. ecc recovery. to avoid failure on reads, error-correcting codes (ecc) are widely employed [cit] . ecc can correct certain bit errors but typically introduces extra computation cycles on both reads and writes. more specifically, while the encoding takes a few cycles (on writes), the decoding requires lots of cycles. this cycle disparity between encoding and decoding imposes extra overheads on reads, which in turn degrades system performance. since wider eccs are required as flash technology shrinks, ecc overheads become more pronounced in modern ssds. read disturbance management. read disturbance can occur when reading the same target cell multiple times without any erase operation. when reading data from a specific cell, v read (0 v) is applied to that cell, and all other cells are biased at vpass (4∼5 v), which makes them behave as pass-transistors. as a result, the cells on successive read"
"to study the pe cycle characteristics on reads, we executed iometer with two different read-only workloads, composed of sequential and random access patterns, about 200 rounds, each with a running time of 1 hour (total 200 hours). in each round, we sent a smart command using our inhouse ahci minport driver and measured pe cycles by decoding return codes based on the smart attribute table [cit] . to compare the pe cycles between reads and writes, we also measured the pe cycles on write-only workloads with the same access patterns and measurement method used in the read-only workload evaluations. we observed that unfortunately all the ssds tested provide insufficient information to understand ssd internal characteristics; in particular, all the data are normalized or provided as percentage based on their lifespan expectations, and some ssds do not even report their pe cycles on reads. therefore, we present the reliability evaluation results of a specific version of ssd-a, which is used in apple macbook air; unlike other ssds we tested, ssd-a provides absolute maximum/average pe cycles on both reads and writes."
"although the above methods have achieved certain results in the aspect of malicious code detection, there are still some problems. such as, feature-extraction is not appropriate, the detection rate and the detection accuracy are not high, and the complexity of the algorithm is high. this paper selects kddcup'99 data set as experimental data, and proposes a hybrid malicious code detection model based on deep learning; based on the autoencoder for data dimensionality reduction, this paper proposes to set dbn as a classifier. for the malicious code behavior, using multiple deep learning achieved better effects than surface learning model. finally, this method improves the malicious code detection rate and detection accuracy, and reduces the time complexity of the hybrid model."
"(b) ssd-z. figure 13 : e-trim overheads. since e-trim performs block erasure on demand and do not return control to the storage system, the host can be disabled until the trim process finishes. the latency observed with trim is 3x worse than response time of 4kb writes. viating the gc overheads in both ssd-c and -z. our latency evaluation results also show similar performance characteristics. to better understand the execution-time impact of trim, we also studied performance at a finer-level, focusing on small data transfer sizes, ranging from 1 sectors to 256 sectors, in figures 12a and 12b . as can be observed, the latencies of the ssds trimmed by rnd-trim are longer than those of the ssds trimmed by seq-trim by about 3x on average. one can conclude from this analysis that, ssds do not trim all the data, and their behavior is strongly related to the trim command submission patterns. in our evaluation, only seq-trim could successfully delete data, providing a similar latency to pristine ssds."
"based on this figure it is possible to select an architecture that better trades-off design constrains. a balanced design solution under the aforementioned criteria (area, maximum temperature, and delay) is the one that replicates four ahb controllers, three integer units, and two cache controllers. this architectural instantiation, mentioned as \"selected architecture\" in upcoming figures, belongs to solutions marked as valid during the area, timing, and thermal filtering. the selection of this architecture for further evaluation is performed since it belongs to the pareto front for reliability improvement, as it is discussed in more details in section 6.2."
"in this section, we will analysis limitations of cordic algorithm from the research object of rotation angle θ i, and put forward the corresponding optimization measures to solve these problems [cit] ."
"h.-j. klein describes concerns about the use of negation in sql due to a \"mixture of set based and logic based techniques used for sql semantics\" since three-valued logic is used to define the result of search conditions in where clauses and boolean logic in determining the value of a predicate including a subquery. he states, \"together with negation (not) or quantification (all) this may result in a critical loss of information.\" 20 klein makes recommendations on how to understand the meaning of a subquery or nested query. jalal kawash, points out that sql lacks a universal quantifier construct and therefore must express universal quantification through negating existential quantifiers and proposes a normal form for sql which eliminates \"undesired\" forms of negation and universal quantification by applying well-known logical rules. 21 kawash gives examples using a simple forestry database which he attributes to bradley. 22 rick chow covers set difference in conjunction with set membership, which he calls the \"membership test.\" in his example of a query involving set difference \"list the names of the faculty members who did not teach is320 at all,\" chow cautions that care must be used in expressing set difference as a subquery, because of partial information, and gives examples of incorrect and correct implementations of the query."
(b) worst-case latency. pattern) that covers the entire storage space of ssds. this makes ssds reclaim block(s) for new incoming requests so that we can easily capture the worst-case latency and system throughput imposed by gcs.
"to make our discussion easier to follow, let rnd-pdt denote the physical data layout resulting from random writes, and seq-pdt denote the physical data layout resulting from sequential writes. in figures 4 and 5, the dashed-lines and solid-lines indicate the read performance on rnd-pdt and seq-pdt, respectively. one can observe that, read performance significantly varies based on the physical data layout organization even though current i/o request access patterns are exactly the same. more specifically, bandwidth values for all the evaluations on rnd-pdt (figure 4 ) are under 80 mb/s, whereas bandwidth values on seq-pdt reach up to 220 mb/s. as the data transfer size used during the physical data layout construction increases, the performance gains are more pronounced since this allows the flash firmware to more easily build a physical data layout by sequentially writing data back-to-back. this performance impact is also observed in our latency characterization plotted in figure 5 . while the minimum latency with rnd-pdt is 210 µsec, the latency with seq-pdt is around 80 µsec."
"the majority of aging phenomena are tightly firmed to onchip temperature values. hence, higher maximum temperatures lead among others to devices having increased failure rates. for this reason, the first criterion employed in our methodology for selecting the architecture of target platform involves to study how temperature values are spatially distributed over the target device. figure 7 depicts the variation of maximum temperature (in kelvin) when different instantiations of the target architecture are considered. the axes of this figure denote the normalized operation frequency and the architecture's area, as compared to the corresponding maximum values found among all the candidate architectures, whereas the vertical axis gives the onchip temperature values. based on figure 7, it is evident that temperature values vary considerable among architectures with different selection of replica blocks. more specifically, regarding the leon3 architecture, temperature variations from 354 to 382 kelvin's, were reported."
"the uart controller is used to optimize the communication between the hardware of cordic and the serial device. the cache allocator is used to convert the two 8-bit datum to a 16-bit data for the initial value of the pre-processing unit. the pre-processing unit is used to convert initial angle into the first quadrant, trigger the iteration computation of the unit of optimized cordic. in the five modules, the unit of optimized cordic is the core, which determined the performance of system."
"communication has become one of the central uses of computing technology over the years. architectures that facilitate communication, such as mobile phones and wireless networks, have been primary factors in driving the evolution of microprocessors and computer systems. with the evolution of wireless mobile communications, the problem emphasis has shifted to networking protocols and signal processing that are required to sustain the necessary bandwidth of these applications. in recent years, we have seen the emergence of an increasing number of wireless protocols (e.g., 2g, 3g, gprs, wifi, etc.) that are applicable to different types of networks."
"where a f is the acceleration factor, e a is the activation energy in electron volts (its value is 0.5 ev for silicon defects), and k is boltzmann's constant, whereas t u and t t are the reference (kelvin) and the operation temperature during testing. based on the values depicted in this figure, we can conclude that the selected architectural instantiation achieves almost the minimum value for a f parameter among all the candidate solutions. furthermore, the conventional approach for designing leon3 architecture exhibits about 14% higher value for this parameter. this mainly occurs due to additional thermal stress introduced by architecture's components with increased power density. however, we have to mention that even this no-replica aware architecture is not the one with the maximum aging degradation, since there exist solutions that correspond to a f value up to 20% of the selected solution (this occurs since non all the possible combinations of blocks lead to alleviate the thermal stress)."
"the analysis concludes that \"because a large portion of all clinical findings mentioned in textual reports are negated, accurately identifying whether clinical observations are present or absent is critical to accurately extracting information from the reports\". 13 several medical informatics studies have demonstrated that the application of negation to various types of narrative medical reports requires an understanding of the ramifications of the concept, as well as the development of computerized negation detection algorithms which are sophisticated enough to flag variances in syntax, negation word placement, and semantics. while free-text medical documents present distinct challenges, the accurate identification of negation phrases is crucial to any automated retrieval system design in order to analyze these narrative reports for clinical and research purposes."
"the employed criterion allows blocks with increased power densities to be replicated more times as compared to blocks with smaller values of power density. this occurs because the area occupied from these blocks is usually smaller, and hence more of them are fit into the given (affordable) percentage of area overhead. since only one of the replica blocks is active at any time (based on approach discussed in section 4), such an aggressive replication of blocks with increased power densities lead to (i) minimize maximum temperature hotspots and (ii) spread more uniformly over the entire architecture."
"after the pretraining process of the training and testing data, we use autoencoder for data dimensionality reduction. through changing the iterations of the pretraining and fine-tuning, we could get different models, including autoencoder + dbn (pretraining iterations 5 times, fine-tuning5 times); autoencoder + dbn 10-10 (pre-training iterations 10 times, fine-tuning 10 times); autoencoder + dbn (pre-training iterations 10 times, fine-tuning five times). the detection results of malicious code as shown in table 1 . the experimental results show that with the increase in the number of iterations, in the respect of detection accuracy, the proposed method is superior to the method of single dbn, which was used in the first experiment. apparently, using autoencoder to achieve data dimension reduction is effective, it can improve the detection accuracy, for using autoencoder can capture the essential characteristics of date efficiently. meanwhile, the accuracy of detection (tp) is reduced. overall, in the respect of prediction accuracy, the mentioned method described in the paper is superior to the single dbn method. it can adapt to the complex environment, achieve effective detection of malicious code, moreover, it consumes less time. figures 6 and 7 show the error rate in the process of pretraining and fine-tuning. after two iterations, the error rate is maintained at a lower level stably. there are many parameters in the autoencoder, such as network structure, output dimension of data after dimensionality reduction, the number of iterations for pretraining and fine-tuning, etc. the output dimension of data after dimensionality reduction is one of the major parameters among them. this paper explores the impact of these parameters on these mentioned methods. figures 8 and 9 respectively show the effect on the detection accuracy and the time consumption of the method. in figure 8, detection accuracy increases with increasing number of iterations. in figure 9, with the increase of the number of iterations, cpu time consumption varies, but the dimension and training time consumption have no direct correlation, because autoencoder can restore data based on less information loss and error. figure 10 and figure 11 show the effect on the detection accuracy of the number of iterations and the time consuming. figure 10 show that when pretraining iterations increased to 10 times, the detection accuracy reached the highest point. figure 11 shows that when pretraining iterations increased to 10 times, most of the time consumption is maintained at a low level. fine-tuning process is to adjust the weights using back-propagation, for low-dimensional data, the network is over-learning. the iterations of fine-tuning do not affect two assessed value directly. autoencoder reduces the data dimensions and extracts the main features of data through the nonlinear mapping for complex multidimensional data; this makes the effectiveness of the experiment increased when applying dbn to classify. in short, for the detection of malicious code, the hybrid method mentioned in this paper is apparently superior to the single dbn method in the first experiment on the whole."
"the implementation of angle encoder cordic is shown in figure 2 . in figure 2, the algorithm requires an n-bit adder /subtracter, and a comparison unit for getting the minimum. since all of these operations are on the critical path of the each iteration, the time of iteration and consumption of area are greatly increased. angle encoder cordic algorithm could greatly reduce the number of iterations at the cost of increasing the latency of a single iteration, and skip some unnecessary rotation angle so that the scaling factor is no longer a constant [cit] ."
"to study the correlation between gc and the worst-case latencies, we prepared two sets of fully-utilized devices, each consisting of ssd-l and -c. we then executed our modified iometer with write-intensive workloads composed of 100% random accesses and sequential accesses for an hour on these ssds, and measured the latency and throughput values."
"we compare all the ssds tested and two types of enterprisescale hdds (7k rpm and 10k rpm) in terms of both the average latency and the worst-case latency. to quantify the average latency, we use pristine devices, and for the worstcase latency evaluation, we employ the fully-utilized devices for ssd and hdd. we run iometer with its enterprise openworkloads including streaming, workstation, database and filesever applications, with the fraction of writes being 99%, 20%, 33% and 20% (of total i/os), respectively."
"these telecommunication applications exhibit increased demand for bandwidth requirements. regarding our architecture, the components with higher power density values when such kind of applications is executed are the instruction unit, the cache and memory controller, the dsu (debug support unit), and the amba ahb/apb bus controllers."
"(i) adaptive differential pulse-code modulation (adpcm) is a variant of differential pulse-code modulation (dpcm) that varies the size of the quantization step, to allow further reduction of the required bandwidth for a given signal-to-noise ratio. (ii) cyclic redundancy check (crc) is an error-detecting code designed to detect accidental changes to raw computer data, and is commonly used in digital networks. (iii) fast fourier transform (fft) is an efficient algorithm to compute the discrete fourier transform (dft) and its inverse. (iv) gsm 06.10: gsm 06.10 is a digital speech coding standard used in the gsm digital mobile phone system."
"modern ssds are well optimized to hide gcs, but the throughput of writes significantly drops and the worse-case latency sharply increases when the write cliff is reached. further, the worst-case latency of ssds is much higher than hdds, which implies that it needs to be paid much more attention especially in the context of latency-sensitive applications. interestingly, the internal dram buffer would make the latency imposed by gcs on the write cliff much longer. this implies that the dram buffer management is in need of being aware of gcs in order to avoid making the worst-case latency even worse. background task scheduling. to alleviate the performance overheads on the write cliff, systems may utilize the background tasks by artificially injecting idle times. since the recovered performance by background tasks is not sustained, the scheduler needs to periodically inject idle periods even under the i/o congestion periods [cit] . in addition, considering the fact that the background tasks require long idle periods to fully recover the performance loss on the write cliff, the scheduler can inject idle times in an interleaving fashion and hide potential gc overheads over multiple ssd resources (e.g., flash array storage systems, and ssd raid systems). exposing ssd firmware api. a more promising approach to handle the background tasks would be exposing apis that allow a host explicitly to handle flash firmware tasks. for example, similar to the trim mechanism, a host can explicitly call gcs or flush data through the data-set management command on idle times so that the host cpu-burst time can be overlapped with the ssd internal tasks. based on our experiments, we believe that directly handling ssd internal tasks is much better approach to handle the write cliff than implicitly scheduling the background tasks."
"more specifically, the area and delay overheads for our selected replication-aware leon3 design are 15% and 7%, respectively, as compared to initial implementation (without considering any replica blocks). even though these penalties are not negligible for asic designs, we have to mention that they comes with considerable gains in term of maximum temperature value (about 17 kelvin or 8%), which in turn leads to higher reliability improvements. furthermore, the proposed methodology for selectively replication of blocks with increased power densities can also be applied to -core architectures, where the performance degradation is more affordable."
"modern ssds and nand flash chips employ several components to scale their performance under a given technology. as shown in figure 1, an ssd employs multiple internal resources described below. controllers. for physical layer (phy) management, ssds have two different controllers: 1) non-volatile memory host controller (nvmhc) and 2) flash channel controller (fcc). nvmhc manages the front-end phy layer to communicate with outside through a conventional thin interface/bus. fcc, in contrast, handles the back-end phy layer to control underlying flash packages and corresponding interfaces. multicore. while the controllers are responsible for handling the phy layer, the embedded processor is dedicated to running the flash firmware, which is composed of multiple software layers. since each layer of the firmware has a different goal and some of their functionalities can be parallelized, modern ssds employ multiple cores (or processors) in an attempt to minimize computation overheads [cit] . multiple channels. the flash package interconnection design is very important from a parallelism angle. in general, considering hardware complexity and signal integrity, several flash packages are connected to a single data bus, referred to as channel, and multiple channel are used in ssds. flash package. a flash die is composed of multiple planes, and multiple dies are stacked in a single flash package, which helps one improve storage capacity and flash-level parallelism. multiple requests can be interleaved through a limited number of ce (chip enable) and i/o pins. in addition, modern flash packages employ a queue and an ecc engine to offload system level overheads imposed by flash commands management and error code checking, respectively."
"the multiclass cross-entropy error function is the difference between the measurement of target probability distribution and the actual probability distribution, that the smaller, the two distributions are similar, and the better. autoencoder uses bp algorithm to adjust the weights of the multiclass cross-entropy error function, as shown in formula (3):"
"observation of student performance in college/university database courses led to the recognition that certain types of queries, involving negation and exclusion, were associated with very high rates of error. in reviewing retrieval searches, it was found that negative expressions incorporating a target term could affect search strategies. psycholinguists have been aware of particular problems with comprehension of statements involving negation. general comprehension challenges as well as the properties of database query interfaces contribute to errors with exclusionary queries and negation. errors can arise from a failure to comprehend the original naturallanguage statement of a query or a failure to comprehend the meaning of a particular database language statement intended to implement the query or from both in the same instance. after an exploration of how queries can be formulated incorrectly and correctly and of what tools are available for formulating queries, recommendations have been proposed, addressing every opportunity for beneficial intervention, from general language skill and logic instruction to database instruction and production approaches. given the possible adverse consequences of undetected errors in query implementation, this multifaceted approach seems prudent.  ___________________ the valuable assistance of those whose e-mail correspondence is documented in the references is gratefully acknowledged."
"although, selective replication in finer granularity than the proposed is a valid design option, we show that coarsegrained component replication can achieve significant temperature reduction and hotspot elimination, which in turn results among others in device improvement against aging phenomena. the proposed approach is not a restrictive one. as shown in figure 2 (b), except the register file which is excluded for replication, the rest of the maximum power densities inside a leon3 processor are distributed among the replicated components, specifically the instruction unit (iu), the cache controller (cache ctrl), and the ahb bus controller (ahb ctrl)."
"based on figure 11, a number of conclusions might be derived. among others, as we increase the maximum onchip temperature values, the mttf also increases. this is explained due to the tight correlation between aging phenomena and temperature values. additionally, the selected architecture achieves to improve the mttf parameter, as compared to initial instantiation of leon3 processor (without considering any replica blocks), about 14%. we have to notice that in case we increase the architecture's area, through inserting more replica blocks, the mttf parameter can be further increased. this is due to the fact that architectures with more replica blocks usually occupy more area, which in turn improve thermal spreading. however, as we have already mentioned, such an improvement in mttf parameter (from 0.86 to 0.81) leads to architectures with unacceptable area penalties (they have eliminated with area filtering as depicted in figure 9 )."
"another interesting conclusion might be derived from figure 7 . even thought higher operation frequencies usually result in higher temperatures, this seems to be alleviated when architectures with increased area are assumed. regarding the leon3 architecture, the additional area is dominated by blocks with low power density values (as we have already mentioned in section 2). hence, by introducing more replica blocks, it is possible to improve thermal spreading. however, since temperature values seem to be more agnostic about the maximum operation frequency, as compared to area overhead, we cannot make safely any conclusion about this."
"experimental test environment: the platform of intel core duo cpu 2.10ghz and 2.00g ram's, matlab v7.11. [cit] samples extracted from 10% samples in proportion which contain the 141 attacks recorded test data and additional 14 types of experiments. the experiment designed the autoencoder which consists of five layers. the numbers of neurons in the previous four-layer network are 41, 300, 150, 75, respectively. furthermore, the number of neurons in the last layer is variable, which determine the dimension of data number after dimensionality reduction."
"since state-of-the-art ssds employ many computational resources, they could perform ssd-internal tasks in the background. the background tasks are expected to allow ssds to expedite foreground tasks, which in turn is expected to achieve stable and sustainable performance."
"previous studies have shown that thermal stress is tightly firmed to reliability issues [cit] . for instance, thermal cycling can be modeled with the coffin-manson relation, which relates in an exponential way the number of cycles to failure to the magnitude of thermal cycling [cit] . existing approaches aim to perform thermal management with techniques that come from the power reduction domain."
"this subsection describes the results retrieved of applying the proposed methodology for designing chip multiprocessor architectures. for demonstration purposes, the target multiprocessor consists of four instances of leon3 (this number is parametric to our framework and can be appropriately tuned based on designer's requirements), while the replica modules among leon3 processors for a given instantiation of multiprocessor, are the same. as a reference to this study we employ a multiprocessor architecture consisted of the leon3 which was marked as \"selected\" in the previous figures. for the following figures, this solution is denoted as \"reference solution.\" figure 12 gives the thermal profiles for this architecture, as it was retrieved from hotspot tool. based on this figure, we can conclude that a number of modules per leon3 exhibit increased temperature values. note that these hotspots differ from those reported previously at figure 3, due to (i) the thermal diffusion effect, and (ii) the different floorplans for each leon3 processor."
"the framework of the optimized cordic algorithm based fpga is shown in fig. 4, which consisted of five modules, including an uart (universal asynchronous receiver/transmitter) controller, a cache allocator for the initial value, a pre-processing unit, an unit of optimized cordic and the post-processing unit."
"each textbook covering sql queries should describe the sql set difference operator and alternative syntax for expressing set difference when using sql products that do not include a minus or except operator. although [cit] sql standard, it is not commonly implemented and only explicitly referenced in a minority of database textbooks. 30 exercise sets should include exclusionary queries and the textbook databases should support query results which will clearly demonstrate the potential for error if an exclusionary query is not implemented correctly."
"the workload distribution, in conjunction to the packaging constraints, is fed to the hotspot tool [cit] to compute thermal profile of target architecture. for increased accuracy, we used the steady-state temperature of each hardware block as the initial temperature values. the default characteristics of hotspot version 5 were used for package, whereas the analytic model parameters are summarized in table 1 ."
"to quantify the performance impact of trim commands, we first wrote data over the entire storage space of ssd-c and -z using sequential and random access patterns, respectively. then, at a system level, we deleted all the data written using trim commands, which consists of two commandcomposition steps. the first step is to setup the trim field of the data-set-management command and send it to the ssd to let it know that the host wants to delete data, which is specified target addresses by following the trim request data (trd) frames. next, we need to configure the logical block address (lba) range entries in the trd frames and submit them to the ssd. using multiple trim commands and trd frames that cover the entire ssd address space, we wiped out all the written data in the previous step. if the order of the target lbas in the trd frames is ascending, we refer to the corresponding trim command pattern as seq-trim. on the other hand, if the order of the target addresses in the trd frames form a random access pattern, which has been used for writing data in the previous step, we denote this trim pattern as rnd-trim."
"in public domain, there exist several benchmark results published assuming bgc, and even ssd makers indicate that they alleviate gc overheads by utilizing idle times (the vendors of 4 of our ssds claim that they execute background tasks!). however, excluding the bflush impact, we observed that ssd-l is the only device that performs bgc."
"since computation of cordic algorithm needs calculating the arc tangent function, the usual process is to calculate the corresponding datum shown in table. 2 previously then store them in the rom, which took too much hardware resources and make the computation speed slow. if we reduce the number of iterations without affecting the computation accuracy, we can effectively improve the speed of operation. the proposes a feasible method by studying the rotation angle θ i [cit] ."
the output of this step is all those thermal-aware floorplanned solutions which their timing degradation meets the design specification (as it was derived after postsynthesis simulation without considering yet any replica block).
"the effect of these stressors can be mathematically determined. next we model aging acceleration due to thermal stress with the usage of arrhenius equation (1) . more specifically, this equation models how the age of a product is increased when it operates under higher temperature values, as compared to its normal operating temperature. figure 10 plots how this parameter varies for different architectural instantiations discussed in this paper. more specifically, the horizontal axis in this figure gives the average temperature, whereas the vertical axis shows how the onchip temperature affects aging degradation:"
"in this section, we analyze read performance of ssds by building 14 different physical layouts, in an attempt to reveal the relationship between read performance and previous writes accesses as well as internal ssd parallelism. the insight behind this evaluation is that the order of random writes can be transformed into a kind of sequential pattern by the underlying flash firmware, which may have performance impact on current writes as well as future reads. specifically, the address remapping process allows the flash firmware to easily strip the write requests over multiple internal resources irrespective of the access pattern, which leads to high levels of parallelism as well as improved performance on random writes. however, this physical data layout construction on writes may also introduce different performance behavior for future reads. unlike writes, for a read to occur, the data to be read should exist and it must reside in a particular location. as a result, the degree of parallelism and performance on reads depends highly on the underlying data layout, which is constructed during the previous writes."
"we believe that there are three main difficulties for modern ssds preventing them from performing bgc. 1) endurance characteristics, 2) block erasure acceleration, and 3) power consumption problem on idle state. first, as explained in section 6.2, lower flash technologies promise much less reliability, and endurance characteristics tend to get worse as the pe cycles increase. considering the fact that most modern ssds are using 2x nm technology now, and some of them are considering adopting 19 nm flash technology, it would be preferred to reduce the number of gc invocations even if they are going to be executed in the background. second, the flash firmware has to choose victim blocks to reclaim free pages, which are used for serving new i/o requests. if the reclaimed pages are not enough, the firmware has to perform gcs until it secures enough pages. since each of these processes has to erase the victim blocks, many flash firmware postpone gcs as much as they can, which leads to on-demand gc rather than bgc. lastly, bgc needs to consume power to perform gc even under the device idle state, and it is difficult to preserve data consistency in the case of power outage. most host vendors have their own power consumption specification, which regulates the maximum bound on power usage when the device is in the idle mode. therefore, bgc is inherently limited in page reclaiming during the idle periods. further, while performing bgc, the firmware has to preserve data consistency by journaling meta-data for the gc in an attempt to prepare for sudden power outages. this also introduces extra i/os at idle times."
"since the rotation angles of the cordic are known, the number of iterations is able to reduce using angle encoder method at least 50% with n-bit precision."
medical informatics researchers at both yale university school of medicine and at the university of pittsburgh center for biomedical informatics have developed and refined computer programs to execute negation detection algorithms on various types of narrative clinical medical reports.
"based on these evaluations, one can conclude that, since the virtual address space that the flash firmware provides on rnd-pdt has been constructed during previous writes, the order of sequential read accesses on the virtual address space are jumbled. consequently, the read accesses can suffer from multiple resource conflicts at a specific channel, chip and die, even though the access pattern itself is sequential. this in turn can degrade read performance due to low internal latencies with different physical data layouts. these latency comparison explains how the physical data layout is related to internal parallelism in two aspects. first, the read latency performed on rnd-pdt is 2.3 times higher than that of seq-pdt that induces lower resource conflicts. second, as the data movement size of reads increases, the magnitude of the latency improvement with seq-pdt is shorter than the improvement with rnd-pdt that has many resource conflicts potential."
"ssd parallelism. in contrast, sequential accesses on seq-pdt can be simultaneously served from multiple internal resources in different locations without any major resource conflict since there are no changes in the order of virtual addresses."
"in contrast, ssds with i-trim just mark flags into a mapping table indicating that the contents are not valid anymore and return response immediately. this trim strategy can alleviate the time consuming activities of gcs such as the live-data lookup and relocation, even though it does not erase actual blocks. although i-trim has some downsides (e.g., extra dram requirement, the possibility of losing inmemory trim data in the case of power failure), it is expected to improve ssd reliability to some extent. specifically, 2x nm technology flash chips are much less reliable than larger feature size (3x∼5x nm) flash chips, mainly because their memory array suffers considerably more from disturbance and has lower endurance characteristics. further, the industry observed that such disturbances occur even when erasing a block and the endurance gets worse as pe cycles increase [cit] . consequently, lower technology flash-equipped ssds employ i-trim rather than e-trim in an attempt to reduce the number of block erasures as much as possible and improve reliability. from a performance perspective, the latency of i-trim is much shorter than the latency of e-trim since the former requires only a few cycles to update the underlying mapping table, as shown in figure 14 . however, i-trim still takes 400 µs ∼10000 µs to handle individual trim commands. this i-trim latency is longer than an 8 sector write latency by 8 times ∼ 153 times. we believe that this is because the flash firmware cannot maintain all the mapping information in the internal dram, which leads to extra i/os on the flash medium to load/store the mapping table on demand. as a result, it takes some cycles to manage the mapping table and update the trim information in the appropriate entries of the table. one can conclude from this analysis that modern ssds require much longer latencies to trim data than normal i/os would take, which may put extra pressure on host systems."
"another technique for providing temperature hotspot elimination, especially at multicore architectures, is based on load balancing [cit] . even though these techniques have been studied for general purpose parallel computers, they targeted mainly the avoidance of performance bottlenecks rather than thermal issues."
"as an important technology of network security, intrusion detection discovers and recognizes intrusion behaviors or attempts in the system through the collection and analysis of key data in the network and computer system. efficient, accurate identification of malicious code can improve the efficiency of intrusion detection, therefore, malicious code analysis and detection is a key problem in intrusion detection technology."
"this section provides a number of experimental results derived from the proposed exploration methodology that prove the efficiency of our approach in term of reducing the consequences posed by thermal stress. for this purpose, a leon3-based design is employed [cit], and the introduced methodology is applied to identify the blocks that lead to higher temperature reduction, and hence they should be selectively replicated, as it was already discussed in section 5."
"note that for the sake of completeness, the temperature scaling is constant among for all the thermal maps depicted in figures 3(a), 3(b), and 3(c), in order to be clear that only careful replication of blocks with increased power densities can alleviate thermal stress."
"many schemes have been proposed by prior ssd research to reduce garbage collection (gc) overheads such as overprovisioning [cit], dram buffer [cit], and finer-granular address mappings [cit] . based on this, it is expected that long gc operations can be reordered and deferred, and therefore, they do not cause severe throughput degradation at a system level."
"according to previous analysis, we propose the adoption of a microarchitectural extension similar to the one depicted in figure 4 . in the examined case, we assume that each component has been replicated two times (this number is parametric in our methodology and its value is defined by the device architect). each replicated module is enhanced with operand isolation latches [cit] to lower redundant dynamic power by eliminating switching activity in time windows in which the component remains inactive. leakage optimization techniques, that is, power gating, can also be applied in an orthogonal manner. however, in the context of this paper, we only account for dynamic power operand isolation techniques, thus assuming that the inactive component leaks power during its idle time window."
"the inputs to this replicationaware thermal management methodology are the architecture description (in vhdl/verilog), as well as the selected cmos technology. initially, design is synthesized with synopsys design compiler [cit] and then we perform post synthesis simulation (with cadence incisive simulator [cit] ) in order to extract some metrics about the design. among others, these metrics include the area occupied by each component of the design, as well as its power consumption based on the switching activity of application executed onto the target architecture (the power consumption is retrieved with primetime px [cit] ). in addition to that, from the results of post synthesis simulation, it is feasible to have a first estimation regarding the timing of the design (maximum operation frequency) with the usage of elmore delay model [cit] . note that, both for power and delay study, we employ worst case test vectors as workload to architecture, in order to guarantee that the derived results, as well as the consequence thermal stress, will not be violated for any applications during runtime. the output of synthesis task is appropriately encoded into an xml format in order to be manipulated by the introduced tools of our framework. the granularity of system's description in this xml format is tunable, since higher detail means a more accurate thermal analysis, but it imposes the maximum computational effort. on the other hand, a more coarse grain approach leads to lower computational effort but it also imposes a penalty in term of hotspot elimination. even though our methodology is applicable at design time, and hence there is almost no performance degradation due to additional computational complexity, however, the increased number of functionalities inside an soc usually makes the selection of a fine grain description of target system a non desirable approach."
"a common drawback among techniques discussed up to now is that they do not incorporate any mechanism for handling thermal history of the cores. this feature provides useful guidelines about the future behavior of the system and can be exploited to improve the results of the migration. in addition to that, existing approaches mainly provide thermal aware application mapping onto soc devices based on exploration provided through simulation results. these approaches assume that target platform is fixed ignoring about potential improvements achieved through architecture-level optimizations [cit] . in this paper, we propose a new methodology, as well as the software supporting framework, for performing architectural and physical design under constraints posed by temperature hotspots. in particular, the motivated idea introduced in this paper exploits the selective replication of hardware blocks that exhibit increased power densities. then, by appropriately assigning tasks onto these replica blocks, it is feasible to alleviate the chip's thermal stress."
"apart from the selected architecture, any other architecture instantiation can be chosen without affecting the efficiency of our proposed methodology, if different constraints are applied. note that our framework mainly indents to enable the thermal improvement of architectures through inserting replica blocks."
"edward barkmeyer of the u.s. national institute of standards and technology indicated that the approach of object constraint language (ocl) was relevant. 28 jos warmer of klasse objecten commented that ocl provides \"select\" and \"reject\" operations for the handling of subsets of collections. 29 according to the ocl language description (section 2.6.1), the select operation, given a boolean expression, \"specifies a subset of a collection\" and \"results in a collection that contains all the elements from collection for which the booleanexpression evaluates to true\" while \"with reject we get the subset of all the elements of the collection for which the expression evaluates to false.\" the ocl language description in 2.6.1 further states that each \"reject can be restated as a select with the negated expression.\""
"dbn is a deep learning machine which consists of an unsupervised multi-layer rbm network and a supervised bp network. each layer unit captures highly relevant implicit correlations from the hidden units of the front layer. the adjacent layers of the dbn can be decomposed into a single limited rbm, shown as dbn training process is divided into two steps: the first step, train each layer of rbm separately by the unsupervised way; the second step, bp neural network in the last layer of dbn, we set the output vector of the last rbm as the input vector of bp neural network, then do the supervised training to entity relation classifier."
"specifically, the statistical characterization is performed in a window-based manner (denotes the temporal granularity for performing thermal analysis) by computing the statistical mean values of the primitive operations executed by the processor, that is, number and type of alu instructions, memory accesses, cache hits/misses, communication load, and so forth. for this purpose, power traces for all the replica units are generated. these traces describe the power activity for all the architecture's units (replicated or not) within a certain amount of time. using these statistics, the utilization ration per component is extracted in each examined window. the per component utilization ratios are correlated with accurate postsynthesis power measurements to generate the power traces. for the examined power traces, the thermal filtering tool extracts the pareto frontiers under various design objectives (i.e., power density of the chip, delay, area, max temperature, max thermal gradient, etc.)."
"depending on the specifics of the underlying hardware configuration, the role of flash firmware can be quite diverse. we now explain the common tasks of the firmware, which have an impact on system performance. parallelism. flash firmware can strip an i/o request over multiple channels, flash packages, and dies therein, in order to improve system performance in terms of both latency and throughput [cit] . since internal parallelism is key to boosting ssd performance, efficient parallelization of data accesses is one of the crucial tasks of the firmware. address mapping. since nand flash allows no in-place updates in a block, when a write arrives, flash firmware stores data in a temporal block (which is prepared in advance), and remaps the original address of the request (virtual address) and the actual location (physical address) of the corresponding data for future reads. in some cases, flash firmware can also remap the addresses in order to improve internal parallelism on writes [cit] . garbage collection. when the prepared blocks run out, flash firmware needs to reclaim physical block(s) so that it can serve an incoming update request. since this block reclaiming task, called garbage collection (gc), is basically a series of extra internal operations, which include reading/writing live data from the target blocks to new block, erasing the old blocks and updating the mapping information, it can introduce long latencies and degrade performance. to reduce gc overheads as much as possible, modern ssds employ more elaborate address mapping schemes, including some proposals that perform gc in the background. endurance. flash blocks have limits in terms of the number of program (write) and erase cycles, referred to as pe cycle. typically, a block that experiences higher pe cycles also experiences more errors and worse memory characteristics. further, once a block reaches its pe cycle limit, it is not available anymore for storage. since guaranteed pe cycles get smaller as technology shrinks, flash firmware needs to consider endurance related issues. wear leveling. since not all the information stored within the same location changes with the same frequency, it is important to keep the aging of each block as minimum and as uniform as possible. flash firmware is also responsible for ensuring that all physical blocks are evenly used (to the maximum extent possible) and keeping the aging under a reasonable value. these tasks are collectively referred to as wear leveling. disturbance. since a flash block is composed of multiple nand strings to which the memory cells are connected in series (in groups of 64, 128, or 256), a memory operation on a specific flash cell may influence the charge contents on a different cell. this is referred to as disturbance, which can occur on any flash operation and lead to errors in undesignated memory cells."
"the main differentiation of the proposed work, as compared to this approach, is that our solution does not assume that replica blocks of the same type are working in parallel. more specifically, in our methodology, only one of the available replica blocks is active at any time. the selection of this active block is based on its thermal condition, as it is described in upcoming sections. the contributions of this paper can be summarized as follows."
"we show that by using selective replication, we can deliver optimized architectural solutions with minimum thermal stress, affordable delay, and user-constrained area overheads. experimental results have shown a significant reduction of the maximum operating frequency, by 8%, which in turn leads to improvement at maximum on-chip temperature values. moreover, they have shown that our approach improves by 14% the aging phenomena. these two results show that our approach compares favorably to the conventional design techniques for soc-based sdr architectures."
"a relational database schema is a specification of the structure of a database and consists of, for each table in the database, the name of the table followed by a list of attributes in that table). a schema may also document database constraints."
"w. chapman and colleagues at the university of pittsburgh center for biomedical informatics utilized a negation algorithm called negex applied to umls phrases in 42,160 dictated clinical reports in the mars (medical archival system) database to analyze the effects of negation. ten types of narratives from mars, ranging from emergency department dictations to mammography reports to surgical pathology notes to history and physical exams, were included from reports logged by more than two thousand different physicians. while 60 negation phrases were triggered by negex, 90% were accounted for by only seven phrases (\"no\", \"denies\", \"without\", \"not\", \"no evidence\", \"with no\", and \"negative for\".) precision was 97% when negation identified by negex was compared with manual physician review of 813 umls phrases."
"at yale university, p. mutalik and colleagues have developed software called negfinder, which is designed to recognize negated concepts when documents are concept indexed using the national library of medicine\"s (nlm) unified medical language system (umls) metathesaurus. sixty documents containing 8,358 umls concepts were evaluated by negfinder with 544 negations detected, of which only 13 were incorrectly flagged and 27 were missed. a specificity rate of 97% was achieved with 92.5% of all negations found with just four words (\"no\", \"not\", \"without\", and \"denies/denied\"). a careful analysis of the errors and missed negations reveals some of the subtleties of current detection algorithms, such as the impact of double negatives, non-standard language usage, and intervening phrases beyond the three word limit threshold to invalidate a negation hit."
"the state transitions of the thermal aware runtime controller for a single type of unit, that is, the instruction unit, are depicted in figure 5 . the same control logic is applied to every type of replicated block, k. since each type of replicated block, k, is managed individually, the overall thermal controller is structured in a modular manner by several control paths like the one depicted in figure 5, each one dedicated to a specific type of block type. if the monitored temperature is lower than the defined threshold, . when the pipeline completes its execution, the controller issues the steering signals to the extra multiplexors and operand isolators and reconfigures the control and the data flow of the processor architecture to be performed by the new selected unit (the coolest candidate at the specific time window)."
"the last conclusion is very interesting since it shows that even architectures with increased power densities can achieve considerable onchip temperature reduction. this point verifies the argument discussed in section 3, that nonoptimal replication of blocks leads to similar (or higher) thermal profiles, as compared to initial architecture implementation (without replica blocks)."
"trim commands enable os to invalidate deleted systemlevel data contents, which can in turn reduce gc overheads significantly. motivated by this, many emerging ssd platforms (e.g., flash virtual memory, file system, database) are expected to send trim commands to underlying ssds as much as they can."
"finally, in the last task in the proposed methodology, the different architectural solutions are evaluated against thermal constraint. this task is automated with a new tool, named ptracegen, that generates proper power traces onto the examined processor architecture regarding the statistical behavior of the targeted application domain. for this purpose, the outcome from this tool provides detailed information about architecture's units that describe (i) the amount of time that a unit is active, (ii) the amount of time that a unit is inactive, and (iii) the switching activity of these units. the first two parameters (i.e., amount of active/inactive time) refer to the number of execution cycles for a timing window, whereas the third parameter corresponds to the number of transitions between operational states (active/inactive) during the architecture's execution phase."
"figures 9a (ssd-l) and 9c (ssd-c) plot the time series for both latency and system throughput along with gcs under the sequential access pattern. for both ssd-l and ssd-c, gcs are infrequently invoked, occasionally imposing long latencies but not impacting system throughput much. the execution in this case recovers performance immediately after the gc. in contrast, with the random write workloads, the worst-case latencies imposed by gcs significantly increase, which in turn dramatically drop the system throughput as shown in figures 9b and 9d . this performance characteristic caused by gcs under random write workloads is referred to as write cliff. specifically, once the write cliff begins, ssd latencies (bandwidth) become 11x (3x) worse than the normal case. further, more problematic challenge of modern ssds is that the performance degradation on the write cliff is not recovered even after many gcs are executed. we believe that this is because the range of random access addresses is not covered by the reclaimed block(s). consequently, block reclaims performed by gc are required for each access, which in turn leads to successive gc invocations. even though we focused on two ssds in this section (due to space concerns), we observed write cliffs in all the ssds tested."
"a candidate metric for this scope is power density, which denotes the ratio of power consumption for each hardware block per the area occupied by this block. figure 2 (b) gives the corresponding power density pie chart regarding the leon3 architecture. as we can conclude from this figure, the components with increased power densities are not those identified as critical based solely on the power consumption criterion. more specifically, the power density denotes that ahb controller, instruction unit, and cache controller are the blocks with increased impact on thermal stress. these blocks contribute to the total power density about 12%, 13% and 11%, respectively, whereas the five blocks already identified based on power consumption correspond to 5% of total power density. this occurs mainly since the blocks with increased power consumption have also considerable increased area (about 91% of the total architecture's area), which in turn leads to almost negligible power density values for these blocks."
"we take a 16-bit cordic algorithm as an example, chose the eplc4f400c6 chip of the cyclone series developed by altera company, and select some angles randomly in the cycle, such as 15°, 45°, 99°, 110°, 200°. the angles between 0° and 360° are indicated by 16-bit binary and the result of the traditional cordic is shown in fig. 6 . since the results are signed, the msb (most significant bit) is sign bit, and the remaining fifteen bits are the fractional part. the simulation results of sine and cosine function are shown in table 6 and table 7 . table 6 and table 7, when the input angles are less than 99.8°, the inaccuracy of simulation negligible, otherwise the inaccuracy are large, which verify the contents we discussed above, and indicate that the conversion of input angle is necessary."
"23 andrew pletch also makes practical recommendations to students on handling set difference correctly in sql queries. 24 gerd wagner covers a range of computational environments (programming languages, database query languages, modeling languages, production rule systems, and logic programming languages and shows that \"negation in all these computational systems is, from a logical point of view, not a clean concept, but combines classical (boolean) negation with negation-as-failure and the strong negation of three-valued logic (also called kleene negation).\" wagner distinguishes \"two kinds of negation: a weak negation expressing non-truth … and a strong negation expressing explicit falsity\" and in section 2.2, dealing with negation in sql, he gives examples of sql queries to show the behavior of negation in sql with regard to weak and strong negation. 25 production environment database specialists were asked for input in order to gain a sense of production practice. sharon croke-allsup confirmed negating an affirmative condition is a practical strategy in assuring comprehension and designing the query implementation: \"it's a lot easier for someone to envision a query that picks something, rather than excludes something. so, i tell them, write the query that picks what you don't want, and then use that to reject records from another query.\" 26 mary hoffman reported a strategy of using the outer join (or one-way or match-not-required join) of, for example, the set of all customers and the set of customers placing an order on the day in question. this can be a convenient way to isolate the complement in relational database systems that do not support the set difference operator. 27 crystal sloan and rita thissen recommended a similar approach."
"figures 10a and 10b illustrate the time series comparison between the cache-on and cache-off status devices, ssd-l and -c, respectively. the worst-case latencies of ssd-l are hidden by the dram buffer before the write cliff begins. however, once gcs start to be invoked in a series, the latency of the cache-on ssd-l becomes two times worse than that of the cache-off ssd-l. in the case of ssd-c, the performance disparity between the cache-on and cache-off status are more pronounced. while the dram buffer provides four times shorter latency compared to cache-off ssd-c before the write cliff begins, it introduces four times worse latencies when the write cliff kicks in. even though a system can react before the data is written into the actual flash device, the dram buffer needs to flush the in-memory data to the flash medium periodically. since target addresses of the buffered data are fully random, this flushing of data introduces a large number of random accesses, which can in turn accelerate gc invocations of ssds and introduce write cliffs."
"note that, unlike the other devices tested, ssd-l employs more reliable flash chips with 35 nm technology and the highest degree of over-provisioning. these two factors make the write amplification factor of ssd-l lower and thus make the device reliable [cit] even though bgc introduces more block erasures. also, ssd-l needs more power to perform bgc more than ssd-c and ssd-z, by about 270% and 78%, respectively, in idle periods."
"a well-known intrinsic characteristic of nand flash is that their read performance is tens to hundreds times better than their write performance [cit] . in addition, since ssds have no moving parts, they are expected to provide fast random read accesses [cit] . motivated by these, many platform designers consider ssds for the applications that contain mostly random reads."
"similar to previous conclusion, the floor-plan that leads to architecture instantiation where components with increased power consumption are replicated ( figure 3(b) ), also results in thermal stress. this occurs since the replicated components exhibit low power density, and hence they do not have considerable impact on thermal stress. however, we have to mention that this approach exhibits slightly reduced maximum temperature values, as compared to architecture without replica blocks, since it increases device area (the replicated blocks occupy about 91% of the total chip's area). on the other hand, the components with increased power densities still contribute to thermal stress, as it is shown at figure 3"
"in this paper, we conduct an extensive experimental evaluation with six state-of-the-art ssds carefully selected by considering different types of flash fabric technologies, manufacturers, cores, chips, and over-provisioning strategies. based on our empirical evaluations, we next perform a comprehensive data analysis and uncover critical ssd/flash characteristics, which are not reported, to the best of our knowledge, in the literature so far, and are opposite to the widely held expectations on ssds. our main goal is to correct common misconceptions on ssds using new data, which greatly effect performance as well as reliability of modern ssds, but have not been studied well in the past. we hope to motivate both academia and industry to rethink ssd system design, management and optimization based on our evaluations and data analysis. in this paper, we answer, either directly or indirectly, several questions described in the following subsections, and reveal some critical data regarding state-of-the-art ssds, which should be, in our opinion, taken into account by both os and ssd designers. the questions we want to address can be categorized into five groups."
"this methodology is shown graphically in figure 6 . the inputs to this methodology are the description of target architecture in vhdl/verilog, the technology constraints regarding the selected cmos technology, as well as the operating conditions, and the affordable area overhand (the higher area, the more replicas can be integrated into the design). even though this methodology is applied at design time, the upcoming sections also evaluate the onchip temperature variations during application execution."
"in the process of adjustment of weight training, firstly we update the state of the hidden layer neuron, and then update the state of the visible layer, thus get the adjusting weights. the weight updating rule as shown in formula (2): after the pre-training is completed, combining the current rbm output unit with the next rbm input unit as the independent layer. unrolling process is to connect these independent rbm into a multi-layered autoencoder, the unrolling process as shown in"
exclusionary queries should be highlighted in staff development so that staff will notice them and give them special attention. the meaning of the natural language query should be determined and reviewed before beginning implementation. a particular effort should be made to review exclusionary query syntax and subject it to testing. employees should be aware that errors in handling exclusionary queries are due to a convergence of comprehension and technical factors.
"after defining the type of replica blocks, as well as how many times they should be replicated, the next tool in our framework performs automatically this task by annotating appropriately the design's description. apart from the insertion of new (replica) blocks to the design, during this task we have to pay effort to provide the appropriate connectivity through routing infrastructure, as well as to insert the thermal-aware runtime controllers in xml format. moreover, during this annotation we keep the same connectivity among hardware blocks, while we have also to preserve that all the connections to (and from) replica blocks should be also replicated. this is an important differentiation of proposed solution, as compared to similar approaches found in relevant references [cit], since we do not aim at altering the functionality of underline architecture."
"egation poses certain challenges for the process of formulating queries and interpreting query results. herbert clark (stanford university, an expert on comprehension of negation) states in summarizing studies on negation, \"the main result of these studies can be summarized succinctly: negation is more difficult to comprehend than affirmation.\" 1 ) margaret donaldson (university of edinburgh, an expert on learning comprehension of negation) commented, \"few are likely to be aware of the effects that difficulties with negatives can have.\" 1) a select clause (in the form of a list of attributes and expressions) which specifies the output of the query."
"this conclusion is very important in order to find the amount of blocks that have to be replicated. in other words, based on figure 9, it is clear that only a few replicas of the blocks with increased power densities should be incorporated, in order to achieve the desired balance between temperature reduction and the consequences area and delay overheads."
"the paper [cit] believes that, in the typical dbn which has one hidden layer the relationship between visual layer v and hidden layer h can be expressed as formula (6):"
"in this paper, kddcup'99 dataset [cit] was used to detect malicious code data. they include five categories: probe, uzr (user to root), rzl (remote to local), dos (denial-of-service) as well as normal data. this paper adopted 10% of the samples of kddcup'99 as a dataset, containing a total of 494,021 training data and 311,029 testing data. in the dataset of kddcup'99, each data contains 41 properties. there are two types of data: numerical and character type. for numerical data, we can treat it directly as number; for the character of character data, we can achieve numeric in the standard method of keywords. to eliminate the effects caused by differences of the magnitude, and to reduce the excessive reliance on individual characteristics in the process of classification, we need to normalize data. firstly, each feature was standardized according to the formula (10)"
"since different architectures consist of different replicated blocks, their power densities also vary. as we can conclude from figure 9, there is not a straightforward correlation among the occupied area, the overall power density and maximum temperature. more specifically, regarding architectures shown in figure 9 that correspond to increased area (more than 66% of the maximum area), they seem to exhibit the maximal power densities but the lower temperature values. on the other hand, the architectures with smaller area (less than 66% of the maximum area) exhibit reduced power densities and increased temperatures values. this rather strange result is justified by the fact that the type of the replicated blocks is not considered in the previous analysis. thus, we can conclude that the type of the replication block has a great impact on the thermal behavior of the silicon."
"in figure 9, we have also marked the solutions which correspond to previously mentioned selected architecture, as well as to the original (without replica blocks) leon3. based on this, both those two solutions exhibit comparable power densities, but the selected one achieves to reduce maximum temperature value about 17 kelvin, or 8%."
"in contrast to our conclusion about power density, area has a great impact on the maximum onchip temperature values. this is also depicted at figure 14 ."
"software-defined radio (sdr) technology was created to improve interoperability between different wireless networks, field radios, and devices [cit] . sdr technology is comprised of both software and hardware that can be dynamically reconfigured to enable communication between a wide variety of changing communications standards, protocols, and radio links. with this latest sdr technology, system architects are able to create multimode, multiband, and multifunctional wireless devices and network equipment that can be dynamically reconfigured, enhanced, and upgraded through software updates and hardware reconfiguration."
"next, we proceed to the second criterion for evaluating the efficiency of derived architecture instantiations that affects the timing constraint. for this purpose, the solutions derived from area filtering are floorplaned with the usage of a thermal-aware floor planner [cit] ."
"the proposed approach aims at temperature optimization, while it can be considered as a proactive strategy that alleviates thermal stress at run-time. the introduced framework does not impose any architectural or compiler modification, whereas it is orthogonal to any other thermalaware methodology discussed above, since it is based on new architectural schemes to eliminate the consequences posed by temperature hotspots. thus, existing work on thermal aware application mapping and dynamic thermal management can be used in a modular manner to extend the proposed methodology."
"the file manager database system (often called va fileman), now in version 22, was developed for the health care system of the u.s. veterans affairs department and runs on platforms conforming to the iso 10756 standard for the m or mumps language. file manager is used in health care and biomedical research in a number of countries. the query interface of this system, unlike sql, is a dialog invoked from the \"search file entries\" option. some exclusionary queries have the property of dealing with a composite, such as an order with several lines possible. this example implements the following query:"
"the following display shows three sets: those who hadn\"t placed any order, those who placed an order on any day other than the 10 th, and those who placed an order on the 10 th . the rightmost three columns show the customers included in the result of a correct set difference implementation of the query and those included in the negative join version of the query (syntactically correct but semantically incorrect) and identify spurious extra rows or omitted rows for the incorrect query implementation. if we examine the relation customer ¨ invoice we note that set of customer rows involved in the relation customer is not equal to the source (the entire customer set), while the set of invoice rows involved in the relation is equal to the source (the entire invoice set). these rows will not participate in a join."
"in this section, we discuss the importance of different hardware blocks to be considered as critical for thermal stress. this problem becomes even more important regarding either high-end processor architectures, that is, superscalar organizations, or multicore soc designs, where multiple hardware components, each of which with different area and power values, are combined into a single device. hence, one of the challenges that architects are facing today is to identify the hardware components that higher affect thermal stress. in order to show how different hardware block thermal profiles affect the thermal stress of the entire ic, figure 2 (a) gives the power consumption for the components of the leon3 processor, when the sdr system is executed."
"the following recommendations are offered regarding database products incorporating sql, in order to assure the most current database language capabilities with regard to exclusionary queries:"
"solid state disks. today, there exist many different ssds on the market, with quite different performance characteristics based on the vendor and system configurations, in terms of the dram buffer size, the number of cores, and the number of flash chips. for our experiments, we chose six representative products shipped by five different ssd-makers. [cit], and their firmware are updated with the latest available version for our evaluations. since our goal is not to perform reverse engineering or make performance comparison across these commercial products, we refer to each of them using a different postfix character, instead of giving its full name. our ssds and their important characteristics are listed in table 1. since the runtime information provided by different vendors varies a lot, in each of our evaluations, we select an appropriate subset of our ssds and use them, and also mention the reason behind our selection. in general, ssd-l, -c, and -z are evaluated for all basic tests, and ssd-a, -x, -p are used for more specific evaluations. measurement and characterization tools. in order to uncover hidden performance characteristics and examine widely held expectations on our ssds, we need well-defined i/o access patterns, which can be controlled and reproduced irrespective of the underlying test platform. consequently, we use intel open source based storage tool, iometer [cit], as our default measurement and characterization tool. iometer can generate various i/o workloads parameterized in terms of read/write ratio, sequential access/random access ratio, request sizes and the number of queue entries. however, iometer reports performance results in terms of only average/min/max values at the end of the entire evaluation process. therefore, for some of our evaluations that require a more microscopic view with finer resolution than what iometer provides, we use a modified iometer, which captures the latency per individual i/o requests and performance characteristics on a second-basis without any underlying software intervention. lastly, to evaluate the phy level latencies, especially for the trim command overhead characterization, we use the commercial lecroy sata protocol analyzer (sierra m6-1 ) [cit] and double check the protocol status with this analyzer. protocol controls. to accurately evaluate different technologies employed by modern ssds, we also need a clean evaluation chamber under our control. for example, even though an application tool can mimic system idleness to evaluate background tasks by injecting artificial idle periods, the advanced host controller interface (ahci) driver/controller can periodically send commands like smart to examine the underlying system, which can make ssds continuously busy. to the best of our knowledge, there exist no public tool, which can generate a specific ata command, check its phy level latency, and directly handle the ahci. this is why, for some of our investigations that require the management of ata commands (e.g., trim, smart) and the control of the ahci, we needed an in-house driver. therefore, we also developed an ahci miniport driver, as a part of wdm (windows driver model, which can generate trim commands by filling target addresses for the deleted contents with different access patterns (random/sequential), handle smart commands to check the pe cycles of the ssds, and manually control specific power modes to examine background tasks. experimental system. our experimental system is equipped with an intel quad core i7 sandy bridge 2600 3.4 ghz processor and 4gb ddr3-1333mhz memory. intel z64 chipset is employed as the i/o controller hub in southbridge, and all ssds we tested are connected to z64 through the sata 6.0gbps interface. we execute all our scenarios in microsoft ntfs, store logs and output results into separate block devices in a full asynchronous fashion; and neither a system partition nor a file system is created on our ssd test-beds. this configuration allows each ssd test-bed to be completely separated from the evaluation scenarios and tools."
"to quantify this impact, we chose ssds that have no dram, to minimize any potential side effects of buffering on both read and writes. we then randomly wrote data into the entire address space of the dram-less ssds, ssd-p, -x and z, with seven different data transfer sizes ranging from 4 sectors to 256 sectors. as a control group, we also wrote data into the same type of ssds but different devices, using sequential access patterns composed of the same data transfer sizes used in the random writes. we then read the entire space of those ssds with varying data sizes ranging from 4kb to 32kb, and measured the bandwidth and latency. the results are plotted in figures 4 and 5 ."
"note that this classification with respect to area occupied by different architectures is also applied in upcoming figures (figures 9 and 10), since it can provide qualitative comparisons about the importance, as well as the efficiency, of the proposed methodology in term of alleviating thermal stress."
"against the problem of detecting malicious code, we propose a hybrid method of detecting malicious code based on deep learning, which combines the advantages of autoencoder and dbn respectively. firstly, the method used autoencoder for data dimensionality reduction to extract the main feature of data. then the method uses dbn to detect malicious code. finally, the experiment was verified by kddcup'99 dataset. experimental results show that compared with the detection method using single dbn, the proposed method improves detection accuracy, while reducing the time complexity of the model. however, in practical application, according to actual situation, the method proposed in this paper needs to have further improvements in order to improve its performance."
"experimental results prove the efficiency of the proposed methodology, showing that the selected architecture leads to temperature reduction about 8% (from 380 kelvin to 363 kelvin), with a controllable silicon area increase of 15%. as we show latter, such a temperature reduction apart from reduction in cooling cost also achieves mentionable improvement to the consequences posed by aging phenomena about 14%."
"one potential concern on this read-write comparison would be the impact of the internal dram buffer. since writes can be buffered in dram, if the internal dram does not flush the in-memory data to the flash medium, write performance would be much better than reads. however, as shown in figures 2e and 2f, we observed that dram-less ssds, namely ssd-p, -x, and -z, exhibit very similar performance characteristics to 128mb and 256mb dram-equipped ssds. further, the amount of data written into those ssds is over 200gb, which cannot be buffered by a small size dram; in fact, the dram capacity accounts for under 1% of the total amount of data we wrote in these experiments."
"unfortunately, most prior works study ssd behavior and performance characteristics based on limited information, or evaluate them based on select i/o access patterns to understand ssd-level parallelism and performance implications. in our opinion, these studies do not help os and system designers in understanding critical ssd features, and integrating ssds into existing storage stacks and efficiently optimizing them. further, a more problematic issue is that, even though ssd nand flash technology has changed dramatically over the last couple of years, many research groups still employ ssds based on assumptions that do no hold anymore."
"leon3 processor consists of the integer unit, the cache subsystem, the memory management system, and the amba interface. the instruction unit is fully compatible with the sparc v8 instruction set, whereas the pipeline consists of 7 stages. the integer unit has configurable separate instruction and data cache (harvard architecture), whereas the size for each of them is equals to 1 kbyte. furthermore, the integer unit includes a configurable register file with register window equals to 8. regarding the l1 caches, they are managed by a cache controller which is interfaced to the system's amba ahb bus. the communication to leon3 peripherals is performed with two bus controllers, referred as ahb (advanced high-performance bus) and apb (advanced peripheral bus) controller, respectively. the first of these controllers (ahb controller) is used for the connection of high speed components (i.e., integer unit, memory controller, etc.), whereas the second one (apb controller) provides control to the low-speed peripherals (i.e., uarts, i/os, etc.). finally, leon3 processor contains a configurable separate local data (2 kbyte) and instruction memory (2 kbyte)."
"regarding our methodology we allow all the blocks of the chip to be \"soft\" blocks, that is, their aspect ratio can change (in a controlled manner) in each annealing movement of hot floor plan tool, but their area is fixed."
"modern ssd firmware and architecture are well optimized to improve write performance, but the worst-case performance on writes is still a problematic challenge. in this section, we examine the worst-case write latencies, analyze their correlation with system throughput, and investigate the impact of the internal dram buffer on latency. to evaluate the worst-case write latency, we prepared a set of fully-utilized devices by writing data (with sequential access (a) average latency."
"since writes can be buffered using internal the dram, modern ssds are somewhat expected to hide gc overheads. in this section, to examine the dram impact on gcs, we setup two fully-utilized device sets and write data with a sequential access pattern. in these experiments, one of these two sets are evaluated under the disabled (dram) cache (cache-off), and the other set is evaluated under the cache (cache-on). to make device status cache-off, we submit cache-disabled command, which brings the force access unit (fau) tag of sata 3.0 [cit] for every i/o requests."
"furthermore, for each replicated component proper pairs of multiplexing and demultiplexing logic are added to the original datapath, regarding the lightweight control and data flow extension. specifically, the inputs of each replicated component are driven by the demultiplexer that properly guides the input data to the active module. accordingly, the output signals from each replicated component are multiplexed in order to propagate to the next level. we recognized two data-flow paths inside the processor datapath, namely, the memory-to-instruction unit and the instruction unit-tomemory data-flow paths. figure 4 depicts the combination of the two data-flow paths by traversing the architecture graph either in a top-down (memory-to-instruction unit) or in bottom-up (instruction unit-to-memory) manner. the original leon3 architecture is also enhanced with a thermal aware runtime controller module for distributing the workload to the available units during runtime. the thermal aware workload distribution is performed by properly issuing the selection signal to the added multiplexing and operand isolation logic. actually, the same signal configures both the aforementioned components. since only the selection signals to the extra logic are issued, the thermal aware controller works transparently from the control logic of the rest of the leon3 architecture. the controller makes the decision which replicated unit to be turned on/off according to the thermal state of the processor. it is assumed that runtime thermal data are available, that is, through thermal sensors."
"based on our experimental results and observations, we now provide a summary of our answers to the questions we raised at the beginning of this paper."
1) this is an example of set difference -we want to subtract the set of all customers who placed an order on january 10 from the set of customers we do not wish to include in the result. the set difference operation will assure that we do not include customers who placed an order on a different date and also on january 10.
"the value of k depends on the number of iterations n, and k was usually called focus constant or scaling factor. generally, we could calculate the scaling factor and the rotation operation separately. the rotation operation was carried out according to (7) ."
"the meaning of this query was agreed to be: the result should list all customers who did not place any order containing a line for a widget, no matter what else the order may have contained and no matter what other orders not containing a line for a widget they may have placed, and the customers who placed no order at all. notes on the search dialog: ' (apostrophe) means \"not\"; the colon (:) after salesorder is an instruction to navigate to/from another file referenced/referencing (or pointed to/pointing from) a field in the other file; in the a condition statement, 10 happens to be the unique id (ien or internal entry number) for part #a4726; successively pressing the enter key without any entry backs out of the search dialog and completes it; // means default to be accepted by pressing enter. george timson, the major designer of the file manager system, was consulted in reviewing file manager\"s capabilities for correct implementation of exclusionary queries. 9 what is important about this dialog-based approach is the protection offered by the questions as to when the search specification should be considered true and requiring the searcher to make a decision. this part of the dialog reflects whether the search is negative or not and whether the search involves multiples, since the hierarchical database model of file manager systems supports multiples which are similar to relation-valued attributes in non-1nf relational database systems. 10 even with this protection, if there are errors in comprehension, as discussed above with regard to sql, or errors in entering selections in the search dialog or in selecting the correct search specification conditions, the search will not produce correct results. in file manager, it is important to choose carefully the file in which to begin a search. jay smith emphasizes, with regard to database queries and modeling in general, that it is vital to consider focal point and direction to assure correctness."
"this section describes in detail the proposed methodology for reducing temperature hotspots through selective replication for some hardware modules of the target architecture. note that throughout this methodology we do not aim at redesigning the whole microarchitecture, but we focus only to critical components. the goals of this methodology are: (i) to provide a proactive thermal-aware approach targeting at micro-architecture designs and (ii) to support the rapid exploration/evaluation of different architectural selections in term of thermal stress. note that the architectural modifications applied with our methodology are transparent to the compilation flow (they do not affect existing tools), while they speed up the development of new products, since end-users (e.g., programmers) do not have to consider thermal issues)."
"73 students (in different sections, taught by different instructors) were assigned to write a set of queries as an in-class exercise. most had some prior experience with relational databases in a previous course or from work. some, particularly graduate students, had prior experience with sql. all 73 students had received at least three prior weeks of instruction in sql. they had prior practice with sql. there are two issues here:"
"we can complete most developments of digital devices by fpga, such as cpu (central processing unit), a circuit of 74 series. we can reduce design time and area of pcb (printed circuit board) and improve system reliability by using fpga to develop digital circuits. the process of fpga design is shown in fig. 3 . system engineers could connect the internal logic blocks in fpga together by editing of connection as needed, just like placing a test circuit board into a chip. the logic blocks and connection of a development of fpga could be edited by designer, so to complete the required logic functions [cit] ."
"furthermore, the temperature values for architectures with few replicas (less than 33% area overhead) are about 3x higher, as compared to the remaining solutions. since our methodology tries to alleviate the hotspots, such hightemperature variations usually result in increased cost for packaging and cooling, and hence they are not desirable. these solutions can also be eliminated from exploration space."
"to answer this question, we first wrote sequential data into the entire space of all our 6 ssds to observe the background task activities. we artificially introduced idle times after writing the data right until gcs begin. in order to avoid deceptive idle time, which means that even though application makes the underlying storage system idle, a logical block adapter (e.g., ahci, atapi) can periodically communicate with it by sending a system check command (e.g., smart), which in turn keeps the storage system busy, we disabled the block adapter by intercepting the device-generated check commands and discarding them.we found that, among the during idle periods, ssd-c flushes in-memory data into the flash medium, creating extra room, which can be used to buffer the new incoming i/o requests. this process is referred to as background flush, bflush. the bflush writes down all in-memory structures and data within 30 seconds and improves ssd-c bandwidth by three-fold over ssd-c without injecting any idle time, as shown in figure 15a . to ensure that this performance benefit is coming from bflush, not from any other background tasks, we also performed the same experiment on ssd-c with disabled cache. as shown in figure 15b, we observed no performance improvement on the cache-off ssd-c, irrespective of the amount of idle time introduced."
"since the majority of these systems exhibit highthroughput, low-power requirements, and short time-tomarket, previous studies proposed the usage of system-onchip (soc) architectures to support the efficient implementation of sdr [cit] . meeting the thermal constraints and reducing the temperature hotspots at these platforms are critical tasks in order to design reliable systems. furthermore, since chip's temperature has significant impact on performance, reliability, power consumption, as well as cooling and packaging costs, it should be carefully optimized at design 2 vlsi design time. thermal-aware design is difficult, whereas designing a chip and package for the worst-case power-consumption scenario may be prohibitively expensive."
(ii) those with area ranging between 33% and 66% of the maximum area among all the solutions; (iii) those with area higher than 66% of the maximum area among all the solutions.
"fpga is a semi-custom integrated circuit coming from asic (application specific integrated circuit, asic), which not only solve the defect of custom circuits, but also overcome the limitation of the number of the original gates of programmable devices."
"while many of these ssd applications and usage scenarios are proposed and developed based on common expectations from ssds, modern ssds and nand flash systems have undergone severe technology shift and architectural changes in the last couple of years. specifically, nand flash cells have shrunk from 5x nm to 2x nm in the past four years, and now fewer electrons are stored per floating gate. these cell-level characteristics make flash devices less reliable and introduce extra operations (e.g., multi-step i/o, verification, error correction processes) to successfully complete i/o requests, which in turn imposes longer latencies. state-of-theart nand flash packaging technologies employ an increased number of planes and dies within a single flash chip, a command queue, ecc engines, and faster data movement interfaces [cit] . these technological changes led in turn to modulations in ssd behavior and performance characteristics. in parallel, ssd internal architecture has dramatically changed; modern ssds now employ multiple internal resources such as flash chips, i/o buses, controllers and cores in an attempt to achieve high internal parallelism. in addition, to reduce performance variations and garbage collection overheads, flash firmware employs advanced strategies such as finer-granular address mappings, dram buffer and background tasks. finally, thin storage interfaces of mod-ern ssds define command feature sets, which provide a way to efficiently expose underlying ssd characteristics to operating systems (os). consequently, os can manage ssd internal resources more efficiently by utilizing system level information."
"figures 8a and 8b plot the average latency and worst-case latency of our ssds and hdds. we see that, the average latencies of all the ssds are better than hdds, irrespective of the workload type used. especially, compared to the 7k rpm hdds, ssds provide 2 ∼ 173 times shorter latency. however, the worst-case latencies on fully-utilized ssds are much worse than that of hdds, which is problematic for many write-intensive ssd applications. specifically, the worst-case latencies of all the ssds tested are 12 and 17 times worse than that of 10k rpm hdd-h and hdd-w, respectively, on an average. this is mainly because nand flash in ssds allows no in-place update with overwrites, which leads to gc invocations that contribute to overall latency."
"where x denotes the value of the original training sample, max (or min) denotes the maximum value for the sample data in the condition of same indicator (or minimum)."
"in comparison, ssd-l performs gcs in the background, referred to as background gc (bgc). as shown in figure 16a, when we introduced 30-second idle times as in the case of ssd-c, the bandwidth has been recovered to about 98% of that of the pristine state ssds in 12 seconds. this performance recovery property of bgc requires much longer time to provide stable performance, but it also exhibits more sustainable performance than bflush. in our evaluations, bgc needed 10 minutes to fully recover the performance loss on the write cliff. note that, unlike bflush, bgc of ssd-l can also be observed in the cache-off device. we further study the performance sustainability of these background tasks below."
"th m neighbouring active bs, respectively. for simplicity, the handover traffic is assumed to be equally distributed amongst the neighbouring active bs for homogeneous traffic conditions, so the handover traffic of each neighbouring active bs is approximated as:"
the new cell traffic load for each of the neighbouring active bs when bs m is switched to rs mode at t of f m is then:
we have performed a comprehensive assessment of a broad spectrum of state-of-the-art methods for subnetwork detection using up-to-date gene expression data specific for breast cancer. the key findings in this study can be summarized in the following three main points.
"to illustrate the differential module network inference algorithm, we applied it to 68 atherosclerotic (i.e. diseased) arterial wall (aaw) samples and 79 nonatherosclerotic (i.e. non-diseased) internal mammary artery (ima) samples from the stockholm atherosclerosis gene expression study [cit], using 1803 genes with variance greater than 0.5 in the concatenated data. the stage study was designed to study the effect of genetic variation on tissue-specific gene expression in cardiovascular disease [cit] . according to the systems genetics paradigm, genetic variants in regulatory regions affect nearby gene expression (\"cis-eqtl effects\"), which then causes variation in downstream gene networks (\"trans-eqtl effects\") and clinical phenotypes [cit] . we therefore considered as candidate regulators the tissue-specific sets of genes with significant eqtls [cit] and present in our filtered gene list (668 aaw and 964 ima genes, 267 in common), and ran the \"regulators\" task on each set of modules independently. as expected, independent clustering of the two data sets results in different numbers of modules, and an inability to map modules unambiguously across tissues (figure 2a ). in contrast, application of the differential module network optimization algorithm (section 4.2) results in a one-to-one mapping of modules, whose average overlap varies smoothly as a function of the reassignment threshold value (figure 2b) ."
"to ensure the minimum qos provision is upheld, the average achievable dl data rate in each cell for the bs-rs switching and bs sleep approaches were analysed across an entire day, with the corresponding results displayed in fig. 6 . in this context, the maximum transmit power was considered, irrespective of the distance between bs and ms. fig. 6 reveals the proposed bs-rs switching mode improved the average data rate in each cell, with the exception of centre cell #1, which is kept on all the time in both cases. the graph also confirms the data rates achieved with the new model are the same as those when all bs are active, so there is no compromising of the qos in securing significantly lower energy consumption."
"the clustering procedure should be repeated multiple times, using the same command, only changing the name of the output file. for instance we could generate 5 runs, named cluster1, cluster2, cluster3, cluster4 and cluster5, with the same command, just by changing the name of the output file."
"to examine modularity of the eight subnetworks, we used two different measures: global clustering coefficient (gcc) 9 and cut-based ratio (cbr). 10 gcc measures how close a subnetwork is to a completely connected graph. and cbr measures the degree to which a subnetwork consists of more edges between nodes within the subnetwork and fewer edges between nodes inside and outside the subnetwork. both modularity scores were scaled to the interval [cit] by dividing by the maximum quantities (fig. 3) . we can see that the optdis subnetwork has the highest gcc, probably because there are many small (3 to 5 genes) fully connected modules in the subnetwork. in contrast, the clustex subnetwork has the highest cbr, probably due to the hierarchical clustering step used before growing the subnetwork within the clusters. the subnetworks of jam.gr and degas have moderately high modularity scores; both methods search for subnetworks using greedy strategies."
"in a module network, only one conditional distribution needs to be parameterized per module, and hence it is clear that if k g, the number of model parameters in eq. (2) is much smaller than in eq. (1). moreover, data from genes belonging to the same module are effectively pooled, leading to more robust estimates of these model parameters. this is the main benefit of the module network model."
"methods data preprocessing. subnetwork detection usually requires two input data sets, a gene expression data set and a network data set. in this study, gene expression was measured by mrna sequencing (rna-seq), and were obtained from tcga breast invasive carcinoma category. 1 the expression data consist of raw counts, normalized median transcript lengths, and reads per kilobase of transcript per million mapped reads for 20,532 genes in 50 tumor samples, paired with 50 normal samples from the same patients. the network data set was downloaded from hprd. 6 after gene id matching using bionet, 7,369 nonredundant genes remained (supplementary table 2 ) and 28,571 interactions were recorded among the encoded proteins after removal of self-loops and isolated interactions (supplementary table 3 ). deseq 4 was used to normalize the raw counts and to detect differentially expressed genes between the tumor and normal samples based on a negative binomial model. the p-values were then adjusted for multiple testing with benjamini-hochberg procedure 27 (supplementary table 1) subnetwork detection methods. unless further specified, we used default setting of parameters for all eight models. the input expression and network data are summarized in table 1, and the gene and interaction lists of the eight subnetworks are in shown supplementary tables 2 and 3, respectively. jactivemodules 2,28 requires a weighted gene list with the weights ranging from 0 to 1. hence, we directly used the adjusted p-values from deseq as the weights. within jactivemodules, there are two different search strategies for subnetworks: simulated annealing and greedy search. for simulated annealing, we increased the default number of iterations from 2,500 to 10,000. default parameter settings were used for greedy search. for both kinds of searches, we set the maximum number of modules as 1. degas 29 has multiple optional algorithms, and we used the cusp (covering using shortest paths) heuristic algorithm to detect subnetworks. dysregulation direction was selected to be diff, and maximum number of modules was set to 1. the number of covered genes k was set to increase from 100 to 1,000 with a step size of 100. the other parameters were kept at their default values. bionet 30 requires the raw p-values (not adjusted for multiple testing) as the input from differential expression analysis by deseq. intrinsically, bionet first aggregates two lists of p-values from two pairs of comparisons (case 1 vs. control and case 2 vs. control) into one list. since we only had one comparison between tumor and normal samples, we input one more replicate list of p-values to meet the requirement. we set the false discovery rate (fdr) cutoff as 0.00001 other than the default value 0.001. a low fdr cutoff has effects on reducing the size of an output subnetwork. netbox 31 is provided with a preloaded human interaction network, and therefore, the only input data needed are a list of seed genes. we used only the genes with the adjusted p-value less than 0.0001 in the differential expression analysis as the seed gene set, which selected 1,063 (14.4%) out of 7,369 genes. the shortest path threshold was set to 2 rather than the default value 1."
"case i: zero-to-medium traffic period: during this period, certain bs will be switched off to conserve energy, while lowpowered rs are alternatively used to cover the area of switched off cell. bs m can be switched to rs mode provided neighbouring active bs are able to handle all the ms served by bs m . the approximate handover trafficρ(t of f m ) of this bs is distributed among the neighbouring active bs according to their own traffic profile, i.e., traffic loads ρ 1 (t"
"the \"cluster file\" is a simple text file, listing the location of all the individual cluster files generated at the \"ganesh\" step. by default, the tight clusters procedure is keeping only clusters that have a minimum of 10 genes (this can be easily changed by overriding a parameter in the command)."
"the biological assumption underpinning the differential module network model (section 4.1) is that each module represents a higher-level biological process, or set of processes, that is shared between conditions, whereas the differences in gene assignments reflect differences in molecular pathways that are affected by, or interact with, this higher-level process. to test whether the optimization algorithm accurately captures this biological picture, we first performed gene ontology enrichment (task \"go annotation\", section 3.7) using the go slim ontology. go slims give a broad overview of the ontology content without the detail of the specific fine-grained terms (http://www.geneontology.org/page/ go-slim-and-subset-guide). consistent with our biological assumption, matching modules in atherosclerotic and non-atherosclerotic tissue are often enriched for the same go slim categories (figure 3 )."
"despite cellular networks being one of the most fertile wireless communication areas, major concerns persist about future spectrum shortages and critical energy consumption issues due to its inexorable growth, which has far exceeded expectations 1 . the rise in wireless network energy consumption is reflected by increasing greenhouse gas emissions, which has been recognised as a serious threat to both environmental protection and sustainable development. vodafone's statistical results, for example, show the total energy consumption has reached 4,117 [cit] /11 base year, which equates to 2.29 millions of tonnes gross co 2 emissions, with 61% of this being generated by base stations (bs) [cit] . the outcome is that not only is this rapid growth in wireless networks impacting upon the economy, but also on the environment. strategies have being proposed by mobile operators to sustain capacity growth without environmental impact, by designing green communication networks with reduced operational energy consumption together with limits upon energy bills. the european commission has already acted seeking a 20% cut in co 2 emissions allied with a 20% [cit] . analogously, some uk mobile phone 1 gartner inc. [cit] . operators including vodafone and orange have announced co 2 emission reductions in the range 20% to 50% [cit] ."
"the goal of this task is to calculate the go (gene ontology) category enrichment for each module, using code from the bingo package [cit] . we have to specify two go annotation files that are describing the go codes associated with the genes (\"gene association.goa human\") and another file describing the go graph (\"gene ontology ext.obo\"). these files can be downloaded for various organisms from the go website (http://www.geneontology.org). we also specify the set of genes that should be used as the reference for the calculation of the statistics, in this case the list of all the genes that are present on the microarray chip (file \"all gene list\"). the results are stored in the output file \"go.txt\"."
"with the advent of high-throughput measurements in biotechnology, cancer biologists are able to dissect the complicated pathology of cancers from multiple directions. these measured molecular profiles include genetic mutations, copy number variance, messenger rna (mrna) expression, microrna expression, dna methylation, protein abundance, etc. 1 however, multidimensional data also bring a tremendous challenge to the computational biology community. what can these data tell us about cancer? differential analysis is a straightforward method in which differences in the molecular profiles of tumor and normal cells are identified. these analyses rely on a large number of samples and result in the identification of thousands of differences in molecular profiles. how to interpret these molecular variations as a whole is still under investigation."
"here, we are going to generate a single, robust clustering solution from all the individual solutions generated at the previous step, using a graph clustering algorithm [cit] . basically, we group together genes that frequently co-occur in all the solutions. genes that are not strongly associated to a given cluster will be eliminated."
"this paper investigates dynamic traffic-aware bs switching modes, where the bs can alter its operating modes between standard bs operations and switching to relay station (rs) mode, the so-called bs-rs switching model. depending on the traffic fluctuations, load profiles are divided into two categories; i) zero-to-medium traffic period -when a bs switches to the rs mode and turns off all its high-power consuming equipment; and ii) peak traffic period -when all bs are fully active. the rationale for switching from a bs to rs mode is to ensure those ms that would be served by the switched off cell and may suffer deep fading, are still be able to receive the same qos. furthermore, since the propagation distance has been shortened between the ms and serving bs via the backhaul connection, the required ms transmit power is concomitantly reduced compared with the bs sleep and cell zooming technique [cit] ."
"cross-model comparison and functional analysis of subnetworks. to investigate the similarity of the eight output subnetworks detected by the different methods, we first performed a pairwise comparison of the subnetworks using jaccard similarity, in terms of nodes (table 3 ) and interactions (table 4) . surprisingly, it was found that the subnetworks of bionet and netbox were the most similar even though they used different subnetwork detection strategies. methods using similar subnetwork detection algorithms have moderate similarities in their output subnetworks, such as jam.gr and degas. in contrast, methods with the same input expression and network data often detect very dissimilar subnetworks, for instance degas and optdis, and netbox and clustex. the pairwise similarities of the subnetworks suggest that the use of similar algorithms and/or similar input data do not guarantee a similar output. this is because the different methods use different objective functions to evaluate a subnetwork in optimization."
"next, we performed gene ontology enrichment using the complete, fine-grained ontology, and removed all enrichments that were shared between matching modules. the resulting tissue-specific module enrichments reflected biologically meaningful differences between healthy and diseased arteries (figure 4 ). for instance, clusters 3 and 7 present a strong enrichment in aaw for the regulation of natural killer (nk) cells that augment atherosclerosis by cytotoxic-dependent mechanisms [cit] . in ima, these clusters are predicted to be regulated by genetic variation in cd2, a cell adhesian molecule found on the surface of t and nk cells, whereas in aaw their predicted regulator is bcl2a1, an important cell death regulator and proinflammatory gene that is upregulated in coronary plaques compared to healthy controls [cit] . this suggests that misregulation of cytotoxic response processes plays a role in the disease, further supported by the overrepresentation in cluster 10 of genes associated with cell death that are a important trigger of plaque rupture [cit] . furthermore, variations in bcl2a1 are predicted to regulate other clusters exclusively in aaw too, with disease-relevant aaw enrichments. cluster 11 is associated with the regulation of b lymphocytes, which may attenuate the neointimal formation of atherosclerosis [cit], while cluster 26 is enriched for collagen production regulation. uncontrolled collagen accumulation leads to arterial stenosis, while excessive collagen breakdown combined with inadequate synthesis weakens plaques thereby making them prone to rupture [cit] . last, as expected, terms related with the heart, cardiac muscle and blood circulation are strongly enriched in aaw, in particular in cluster 36. in aaw, this cluster is regulated by gk5, which plays an important role in fatty acid metabolism and whose upregulation has previously been associated to the pathogenesis of atherosclerosis and cardiovascular disease in patients with auto-immune conditions [cit] . on the opposite side, cluster 36 in ima is regulated by gria2, a player in the ion transport pathway, which has been shown to be down-regulated in advanced atherosclerotic lesions [cit] ."
"finally, the gibbs sampling strategy for module identification was complemented with a probabilistic algorithm, based on a logistic regression of sample splits on candidate regulator expression levels, for sampling and ensemble averaging of regulatory programs, which resulted in more accurate regulator assignments [cit] ."
"once the list of candidate regulators is established, the assignment to the clusters can be made with a single command like this: java -jar lemontree.jar -task regulators -data_file data/expr_matrix.txt \\ -reg_file data/reg_list.txt \\ -cluster_file tight_clusters.txt \\ -output_file results/reg_tf"
"the \"cluster file.txt\" is a simple text clustering file, like the one obtained in \"tight clusters\" step, and \"reassign thr\" is the score gain threshold that must be reached to move a gene from one cluster to another. by default, this reassignment threshold is set to 0."
"complex systems composed of a large number of interacting components often display a high level of modularity, where independently functioning units can be observed at multiple organizational scales [cit] . in biology, a module is viewed as a discrete entity composed of many types of molecules and whose function is separable from that of other modules [cit] . the principle of modularity plays an essential role in understanding the structure, function and evolution of gene regulatory, metabolic, signaling and protein interaction networks [cit] . it is therefore not surpris-ing that functional modules also manifest themselves in genome-wide data. indeed, 2 module network inference: theory and algorithms"
"alternatively, molecular interaction data have shown powerful potential for connecting isolated molecular variations into a meaningful framework. these analyses usually start with differential analysis of molecular profiles, eg, differential gene expression, and score the extent of the difference for each gene. next, biological network data that indicate the association of genes are collected, and then the scores are overlaid on the network. now the task is to extract a subset of the network, ie, a subnetwork of the global network, such that the subnetwork is as small as possible while connecting as many highly scored genes as possible. this subnetwork enriched in differentially expressed genes can be used to discover, for example, that the upregulation of one gene is caused by the overexpression of its upstream regulator or dysfunction of its suppressor."
"lemon-tree is a software suite implementing all of the algorithms discussed in section 2.2. lemon-tree has been benchmarked using large-scale tumor datasets and shown to compare favorably with other module network inference softwares [cit] . its performance has been carefully assessed also in an independent study not involving the software authors [cit] . lemon-tree is self-contained, with no external program dependencies, and is entirely coded in the java tm programming language. users can download a pre-compiled version of the software, or alternatively they can download and compile the software from the source code, which is available on the github repository (https://github.com/eb00/lemon-tree). note that there is also a complete wiki on the lemon-tree github (https://github. com/eb00/lemon-tree/wiki), with detailed instruction on how to download, compile, use the software, what are the default parameters and an extended bibliography on the topic of module networks."
"select candidate regulator types (gene expression, microrna, copy-number profiles, epigene8c profiles, snps, etc.). preprocess input data. the purpose of the lemon-tree software package is to create a module network from different types of 'omics' data. the end result is a set of gene clusters (coexpressed genes), and their associated \"regulators\". the regulators can be of different types, for instance mrna expression, copy-number profiles, variants (such as single nucleotide variants) or even clinical parameter profiles can be used. there are three fundamental steps or tasks to build a module network with lemon-tree ( figure 1 ):"
"the first optimization strategy proposed to identify high-scoring module networks was a greedy hill-climbing algorithm [cit] . this algorithm starts from an initial assignment of genes to coexpression clusters (e.g. using k-means), followed by assigning a new regulator to each module by iteratively finding the best new regression tree split for the current set of leaf nodes, and reassigning genes between modules given the current regression tree, while preserving acyclicity throughout. the decomposition of the bayesian score [eq. (3)] as a sum of leaf scores of the different modules allows for efficient updating after every regulator addition or gene reassignment. an improvement to this algorithm was found, based on the observation that the bayesian score depends only on the assignment of samples to leaf nodes, and not on the actual regulators or tree structure that induce this assignment [cit] . hence, a decoupled greedy hill-climbing algorithm was developed, where first the bayesian score is optimized by two-way clustering of genes into modules and samples into leaves for each module, and then a regression tree is found for the converged set of modules by hierarchically merging the leave nodes and finding the best regulator to explain the split below the current merge. this algorithm achieved comparable score values as the original one, while being considerably faster [cit] ."
"we suggest that the definition of subnetwork needs to be refined to be something more than a simple subset of a global network. interactome data need to be dissected and reorganized using high-level structures, such as pathways and protein complexes. those interactome structures ensure that the output subnetworks are biologically meaningful and guide subnetwork detection methods to prune a global network without losing the important biological structures."
"to further examine the specificity and sensitivity of significant gene coverage of each method, we label each detected gene as a positive sample for each method and examined whether the expression p-values predict the eight (0.89), but there is an obvious kink point on the curve due to the selection of input seed genes based on p-values. the auc of optdis ranks the third, probably due to the small size of the subnetwork. jam.sa detects the largest subnetwork but does not perform well in covering high p-value genes since it accepts a low p-value gene with a specific probability at each iteration in order to avoid suboptimality. clustex figure 2 to show the predictability of the p-values for the eight subnetworks. from figure 2, we find that the top method is bionet since it achieves an area under the curve (auc) of 0.93, the highest auc for any method. this is particularly interesting since bionet does not depend on a seed gene set. netbox achieves comparably high auc does not perform as well as netbox, even though they use the same seed gene set and network data. this is because we only consider the largest subnetwork (210 seeds out of 801 genes) found by clustex as the output and discard the smaller subnetworks, which include 455 seeds."
"clustex 32 provides preloaded network data and also supports customized network uploading. for comparative purposes, we used the trimmed hprd network described above. number of methods detecting in subnetworks databases using kobas, and used them as the ground truth to evaluate the predictability of the eight subnetworks (see figs. 4, 5a, and 6). similarly, we combined the 462 breast cancer genes with 227 genes enriched in cancer pathways to query the hprd network and found 2,058 interactions (supplementary table 3 ) that connect the 689 genes in the querying list as a positive set of breast cancer pathways (see fig. 5b ). for the functional analysis of commonly detected genes by at least three methods, we input those genes in kobas and set the 7,369 genes to the background gene set (supplementary table 1 )."
"the regulators text files all have the same format: three columns representing respectively the regulator name, the module number and the score value. java -jar lemontree.jar \\ -task figures \\ -top_regulators reg_files.txt \\ -data_file data/all.txt \\ -reg_file data/all_reg_list.txt \\ -cluster_file tight_clusters.txt \\ -tree_file results/reg_tf.xml.gz note that the \"top regulators\" parameter is a simple text file listing the different top regulator files and their associated clusters. such a file could be for instance the file reg tf.topreg.txt mentionned in the previous paragraph. all figures are generated to the eps (encapsulated postcript) format, but it is relatively easy to convert this format to other common formats such as pdf."
"to examine functional enrichment of commonly detected genes, we used kobas to annotate the 553 genes detected by at least three methods (supplementary table 1 ). the top enriched kegg pathways of these genes are cell cycle (hsa04110), micrornas in cancer (hsa05206), and pathways in cancer (hsa05200), all with the corrected p-values less than 0.05. cancers are enriched as the topmost disease in kegg disease database with corrected p-values less than 0.1. and the top go terms enriched in this gene set are extracellular matrix (go:0031012), cell division (go:0051301), and their relevant terms. note that there is no breast cancer-specific term significantly enriched in terms of pathways, diseases, and functions."
"case ii: high traffic period: in this scenario, all bs actively provide services, with some likely to be more crowded than others. while a bs with a low traffic load is able to serve ms in a neighbouring cell which may have a high traffic load, by increasing its transmit power to reduce the blocking probability in the crowded cell [cit], this particular situation is not considered in this paper."
"this paper has presented an energy efficient cellular network model, which switches from a base station (bs) to a relay station (rs) mode during zero-to-medium traffic periods. the rationale for the bs-rs switching model is to guarantee undisrupted communications between the mobile station and bs and thereby ensure the same quality-of-service (qos) level as would be provided in normal bs mode. the simulation results confirm the proposed bs-rs switching model reduces energy consumption in both the up and downlinks while sustaining the requisite qos. in the current simulation scenario, it has been assumed a rs is deployed at the same bs position and has the same coverage. future work will focus on deploying more rs within a macrocell to keep the bs switched to rs mode for longer periods using dynamic switching, to secure further energy savings. the integration of cell zooming and load balancing strategies during peak macrocell periods will also be investigated."
"where p t,i and p r,i are respectively the transmitted and received signal powers of the i th ms at distance d i, and φ( * ) is the cumulative distribution function (cdf) of standard normal distribution."
"assessment of subnetwork quality. we assess the quality of subnetworks output by the eight methods from two aspects: coverage of significant genes and network modularity. first, we prepared volcano plots with log 2 (fold change) versus -log 10 (p-values) for each method and highlighted the found genes in the eight subnetworks in red, as shown in figure 1 . we find that jactivemodules using greedy search (jam. gr), bionet, and netbox cover most of the significant genes in their subnetworks, while excluding insignificant genes. in contrast, jactivemodules using simulated annealing (jam. sa), clustex, and netwalker cover a large number of genes regardless of their significance. degas covers more upregulated genes, whereas optdis covers more downregulated genes."
"lemon-tree is a command-line software, with no associated graphical user interface at the moment. the different steps for building the module network are done by launching commands with different input files that will generate different output files. all the command line examples below are taken from the lemon-tree tutorial, that users are encouraged to download and reproduce by themselves."
"in order to perform a fair assessment, we kept the input data of the eight models as similar as possible (see table 1 ). on one hand, we used the protein-protein interaction network from human protein reference database (hprd) 6 as model input if there is no preloaded network data in the models. on the other hand, if the models used their preloaded networks and output a subnetwork including genes not in the hprd network, we pruned them from the subnetwork. in terms of expression data, we first utilized deseq to normalize the raw counts of mrna sequencing from tcga breast carcinoma data set. then we performed differential expression analysis across the 50 case and 50 control samples and assigned each gene an adjusted p-value for its significance of differential expression. those p-values can be directly used as the input for subnetwork detection, be ranked to select a seed gene set, or be converted into a set of particular weights tailored to the requirement of the model (see table 1 and methods). next, we ran each program to detect subnetworks and tuned the parameters to control the size of subnetworks to be approximately 1,000 genes. finally, we obtained eight subnetworks from the models and performed an assessment of their coverage of significant genes, network modularity, hits of true breast cancer genes, and functional enrichment in kyoto encyclopedia of genes and genomes (kegg) 7 pathways and gene ontology 8 terms."
"further analysis of the greedy two-way clustering algorithm revealed the existence of multiple local optima, in particular for moderate to large data sets (∼1000 genes or more), where considerably different module assignments result in nearidentical scores. to address this issue, a gibbs sampler method was developed, based on the chinese restaurant process [cit], for sampling from the posterior distribution of two-way gene/sample clustering solutions [cit] . by sampling an ensemble of multiple, equally probable solutions, and extracting a core set of 'tight clusters' (groups of genes which consistenly cluster together), gene modules are identified that are more robust to fluctuations in the data and have higher functional enrichment compared to the greedy clustering strategies [cit] ."
"this task is aimed at maximizing the bayesian coexpression clustering score of an existing module network while preserving the initial number of clusters. a threshold can be specified to avoid that genes are reassigned if the score gain is below this threshold and allowing the systematic tracking of the conservation and divergence of modules with respect to the initial partition. this task can be used to optimize an existing module network obtained with a different clustering algorithm, or to optimize an existing module network for a different data matrix, e.g. a subset of samples as presented in section 4."
"the \"reg file\" option is a simple text list of candidate regulators that are present in the expression matrix. if the regulators are discrete, it is mandatory to add a second column in the text file, describing the type of the regulator (\"c\" for continuous or \"d\" for discrete). the profiles for co-expressed genes and for all the regulators must be included in the matrix indicated by the data file parameter."
"where p t and p t,i are the total power consumption in the dl and i th channel transmission power respectively. to attain a high ee while concurrently guaranteeing the qos for each user, the objective is to turn off certain bs and switch to lowpowered rs mode during zero-to-medium traffic periods. bs-rs switching decisions are based upon whether the traffic load is less than a prescribed threshold within a cell over a particular time interval, so the design problem can thus be accordingly formulated as:"
"we tested whether the detected subnetworks contain putative breast cancer genes. first, we collected 462 breast cancer genes from the kegg orthology based annotation system (kobas) version 2.0 11 functional enrichment list, which integrates online mendelian inheritance in man (omim), 12 kegg disease, 7 functional disease ontology (fundo), 13 genetic association database (gad), 14 calculated the precision and recall of each of the eight subnetworks (fig. 4) and found that the top subnetworks in identifying the true breast cancer genes are those produced by bionet, netwalker, netbox, and jam.gr. surprisingly, these four methods use totally different algorithms for subnetwork detection (see table 1 ). and netwalker displayed its potential for predicting true disease genes, even though its coverage of significantly differentially expressed genes was relatively poor; this may be due to its use of random walks to diffuse information through the whole network without any restriction to shortest paths and greedy search. then we used the list of true breast cancer genes to investigate if cancer-related genes are more likely to be detected by multiple methods. the distribution of all genes and the breast cancer genes is shown in figure 5a in terms of how many different methods detect genes in these classes. we can see in figure 5a that many genes are detected by only a few methods, whereas a small number of genes are detected by almost every method. surprisingly, the percentage of breast cancer genes in the reported subnetworks increases with the number of methods detecting those genes, suggesting that the genes detected by more methods are more likely to be a true breast cancer genes. and also it suggests that an ensemble method that integrates multiple methods may be a better way of detecting subnetworks covering more disease genes. similarly, we collected 2,058 interactions enriched in breast cancer pathways using kobas 2.0 from the kegg pathway, 7 pathway interaction database (pid), 16 biocarta, 17 reactome, 18 biocyc, 19 and protein analysis through evolutionary relationships (panther) 20 databases. the distribution of interactions in terms of the number of methods detecting those interactions is shown in figure 5b . we found that no interactions were commonly detected by more than six methods. the interactions commonly detected by more methods are slightly more likely to be enriched in pathways related to breast cancer."
java -jar lemontree.jar \ -task go_annotation \ -cluster_file tight_clusters.txt \ -go_annot_file gene_association.goa_human \ -go_ontology_file gene_ontology_ext.obo \ -go_ref_file all_gene_list \ -output_file go.txt 4 differential module network inference
"in data integration problems, we are often interested in explaining patterns in one data type (e.g. gene expression) by regulatory variables in another data type (e.g. transcription factor binding sites, single nucleotide or copy number variations, etc.). in this case, the causal graph is bipartite, and the acyclicity constraint is satisfied automatically."
"the goal of this task is to cluster genes from a matrix (rows) using a probabilistic algorithm (gibbs sampling) [cit] . this step is usually done on the mrna expression data only, although some other data type could be used, for instance proteomic expression profiles. we first select genes having non-flat profiles, by keeping genes having a standard deviation above a certain value (0.5 is often used as the cutoff score, but this value might depend on the dataset). the data is then centered and scaled (by row) to have a mean of 0 and a standard deviation of 1. to find one clustering solution, the following command can be used (the command is spread here over multiple lines, but should be entered on a single line without the backslash characters):"
"the remainder of the paper is organized as follows. in section ii, the bs-rs switching model is formally introduced. in addition, both the propagation loss and power consumption models are considered together with the supporting outage probability theory. section iii provides an analysis of the energy saving simulation results using the proposed energy minimisation strategy for both the uplink (ul) and downlink (dl), while section iv provides some concluding comments."
"in figure 2, we also define two more mso formulas, activei and errori, which are not part of the trail, where the first models the active nodes at step i, and the second expresses when an error occurs due to the dereferencing of a variable pointing to xnil, respectively."
"if s satisfiability-preservingly embeds in s, then clearly, when checking for satisfiability, we can ignore s if we check satisfiability for s. more generally, the satisfiability check can be done only for the minimal structures with respect to the partial-order (and well-order) defined by satisfiability-preserving embeddings."
"for all these examples, a set of partial correctness properties including both structural and data requirements is checked. for example, assuming a node with value k exists, we check if both sorted-list-search and bst-search return a node with value k. for sorted-list-insert, we assume that the inserted value does not exist, and check if the resulting list contains the inserted node, and the sortedness property continues to hold. in the program bst-insert, assuming the tree does not contain the inserted node in the beginning, we check whether the final tree contains the inserted node, and the binary-search-tree property continues to hold. in sorted-list-reverse, we check if the output list is a valid list that is reverse-sorted. the code for bubblesort is checked to see if it results in a sorted list. and the left-rotate and right-rotate codes are checked to see whether they maintain the property that maintain the binary search-tree property."
"there is a rich literature on heap analysis without data. since first-order logic over graphs is undecidable, decidable logics must either restrict the logic or the class of graphs. the closest work to ours in this realm is pale [cit], which restricts structures to be definable over tree-skeletons, similar to strand, and reduces problems to the mona system [cit] . several approximations of firstorder axiomatizations of reachability have been proposed: axioms capturing local properties [cit], a logic on regular patterns that is decidable [cit], among others."
"in this paper, we propose a new fundamental technique for deciding theories that combine heap structures and data, for fragments of a logic called strand."
"consider evaluating ψ over a particular model. after fixing a particular valuation of y, notice that the data-relations γi get all fixed, and evaluate to true or false. moreover, once the values of γi are fixed, the rest of the formula is purely structural in nature. now, if ψ is to hold in the model, then no matter how we choose to evaluate y over the nodes of the model, the γi relations must evaluate to true or false in such a way that ϕ holds."
"note that each boolean variable bi replaces an atomic relational formula γi, where γi places some data-constraint on the data-fields of some of the universally quantified variables. the following proposition is obvious; it says that if a universal strand formula ψ is satisfiable, then so is its structural abstraction b ψ. the proposition is true because the values for the boolean variables can be set in the structural abstraction precisely according to how the relational formulas γi evaluate in ψ: 2 the definition of structural abstractions can be strengthened in two ways."
"where lca(s, t, u) is an mso formula that checks whether u is the least-common ancestor of s and t in the tree; this expresses the requirements in definition 5.1."
"we also define a syntactic decidable fragment of strand, stranddec, which is a subfragment of the semantic class strand sem dec . in this fragment, we distinguish two kinds of binary relations in the heap, elastic and non-elastic relations. intuitively, a relation is elastic if for every model m and submodel m, the relation holds on a pair of nodes of m iff the relation holds for the corresponding pair of nodes in m . given a relation r, we show it is also decidable whether r is an elastic relation. stranddec formulas are then of the form ∃ x∀ yϕ( x, y), where (a) ϕ has no additional quantification, and (b) the atomic non-elastic structural relations in ϕ compare only variables in x. we show that stranddec formulas always have a finite number of minimal models with respect to satisfiability-preserving embeddings, and are hence decidable using the decision procedure for the semantic fragment strand sem dec . we report also on an implementation of the above decision procedures. for the structural phase, we use mona [cit], a powerful tool for deciding mso over trees which, despite its theoretical non-elementary worst-case complexity, works very efficiently on realistic examples, by combining a variety of techniques including tree-automata minimization, bdds, and guided tree automata. the quantifier-free data-logic we use is the quantifier-free logic of linear arithmetic, and we use the smt solver z3 to handle these constraints. we have proved several heap-manipulating programs correct including programs that search and insert into sorted lists, reverse sorted lists, and perform search, insertion, and rotation on binary-search trees."
"we define a notion called satisfiability-preserving embeddings that allows us to identify when a submodel s of t is such that, whenever t satisfies ψ under some interpretation of the data-logic, s can inherit values from t to satisfy ψ as well. this is consider-ably more complex and is the main technical contribution of the paper. we then build decision procedures to check the minimal models according to this embedding relation."
"we now introduce our logic strand (\"structure and data\"). strand is a two-sorted logic interpreted on program heaps with both locations and their carried data. given a first-order theory d of sort data, and given l, a monadic second-order (mso) theory over (lv, le)-labeled graphs, of sort loc, the syntax of strand is presented in figure 1 . strand is defined over the two- undecidability. strand is an expressive logic, as we will show below, but it is undecidable in general, even if both its underlying theories d and l are decidable. let d be linear integer arithmetic and l be the standard mso logic over lists. it is easy to model an execution of a 2-counter machine using a list with integers. each configuration is represented by two adjacent nodes, which are labeled by the current instruction. the data fields of the two nodes hold the value of the two registers, respectively. then a halting computation can be expressed by a strand formula. hence the satisfiability of the strand logic is undecidable, even though the underlying logics l and d are decidable."
"we omit the proof for lack of space; it's gist is as follows. when all relations are elastic, for any valid subset s, tailors(ϕ) holds on any valuation of variables over s iff ϕ holds on the same valuation over t (since the atomic relations are elastic). hence the submodel can always inherit the data-values from the model to satisfy the formula. the minimal models with respect to satisfiability-preserving embeddings are hence a subset of the minimal models with respect to the submodel-relation, which we can show is finite. when all relations are not elastic, the proof is much more complex, and relies on the fact that the non-elastic relations define only a finite number of equivalence classes of relationships over x."
"the bernays-schönfinkel-ramsey class: having motivated formulas with the ∃ * ∀ * quantification, it is worthwhile to examine this fragment in classical first-order logic (over arbitrary infinite universes), which is known as the bernays-schönfinkel-ramsey class, and is a classical decidable fragment of first-order logic [cit] ."
"instead of building an automaton representing the minimal models and then checking it for finiteness, we check the finiteness formula minmodel ψ using ws2s, supported by mona, which is a monadic second-order logic over infinite trees with setquantification restricted to finite sets. by quantifying over a finite universe u, and transforming all quantifications to be interpreted over u, we can interpret minmodel ψ over all finite trees. let us denote this emulation as minmodel u,ψ . the finiteness condition can now be checked by asking if there exists a finite set b such that any minimal model for ψ is contained within the nodes of b:"
"for the data-constraint solving phase, we first report the number of nodes of the tree (or string) that is an upper bound for all minimal models. the z3 formulas are typically large (but simple) as one can see from the size of the formulas in the table. we report whether z3 found the formula to be satisfiable or not (all cases were unsatisfiable, except sorted-list-insert-error, as the hoaretriples verified were correct), and the time it took to determine this."
"let us define the syntax of a basic programming language manipulating heaps and data; more complex constructs can be defined by combining these statements appropriately. let var be a countable set of pointer variables, f be a countable set of structural pointer fields, and data be a data field. a condition is defined as follows: (for technical reasons, negations are pushed all the way in):"
"trees are seen as graphs with σ-labeled vertices and edge relations ei(x, y) that define the i'th-child edges. monadic second-order logic over trees is the mso logic over these graphs."
"notice that the above theorem crucially depends on the formula being universal over data-variables. for example, if the formula was of the form ∀y1∃y2γ(y1, y2), then we would have no way of knowing which nodes are used for y2 in the data-extension of graph(t ) to satisfy the formula. without knowing the precise meaning of the data-predicates, we would not be able to declare that whenever a data-extension of graph(t ) is satisfiable, a dataextension of a strict submodel s is satisfiable (even over lists)."
we are now ready to define the mso formula on k-ary trees minmodel ψ that captures minimal models. let the structural abstraction of ψ be b
"in each of these cases, the verification conditions were always expressible in the syntactic fragment stranddec, and hence in the semantic decidable fragment strand sem dec, supporting our thesis that the decidable fragment is natural and useful."
"where leaf(x) is a subformula that checks if x is a leaf, and rightmostpath(x, y) (and leftmostpath(x, y)) is a formula that checks if y is in the right-most (left-most, respectively) path from x."
"the resulting formula is a pure data-logic formula without quantification that is satisfiable if and only if ψ is satisfiable over r. this is then decided using the decision procedure for the data-logic. theorem 5.8. given a sentence ∃ x∀ y ϕ( x, y) over r in strand sem dec, the problem of checking whether ψ is satisfiable reduces to the satisfiability of a quantifier-free formula in the datalogic. since the quantifier-free data-logic is decidable, the satisfiability of strand sem dec formulas is decidable."
"intuitively, if a model satisfies ψ, then it would satisfy b ψ too, as for every valuation of y, there is some way it would satisfy the atomic data-relations, and using this we can pull out a valuation for the boolean variables to satisfy b ψ (as in the proof of proposition 5.3 above). now, since the data-values in the submodel are inherited from the larger model, the atomic data-relations would hold in the same way as they do in the larger model. however, the submodel may not satisfy ψ if the conditions on the truth-and false-hood of these atomic relations demanded by ψ are not the same."
"note that despite its relative expressiveness in allowing quantification over nodes, strand formulas cannot express certain constraints such as those that constrain the length of a list of nodes (e.g., to express that the number of black nodes on all paths in a red-black tree are the same), nor express the multi-set of datavalues stored in a data-structure (e.g., to express that one list's data contents are the same as that of another list). we hope that future work will extend the results in this paper to handle such constraints. decidable fragments of strand: the primary contribution of this paper is in identifying decidable fragments of strand. we define two such fragments, one which is a semantic fragment strand sem dec that defines the largest class that can exploit our combination mechanism for decidability, and the other a smaller but syntactic fragment stranddec."
"we now define recursive data-structures using a formalism that defines the nodes and edges using mso formulas over a regular set of trees. intuitively, a set of data-structures is defined by taking a regular class of trees that acts as a skeleton over which the datastructure will be defined. the precise set of nodes of the tree that corresponds to the nodes of the data-structure, and the edges between these nodes (which model pointer fields) will be captured using mso formulas over these trees. we call such classes of datastructures recursively defined data-structures."
"the above formula when interpreted on a tree t says that there does not exists a set x that defines a non-empty valid strict subset of the nodes of t, which defines a model graph (subtree(t, x) ) that further satisfies the following: for every valuation of y over the nodes of graph(subtree(t, s)) and for every valuation of the boolean variables b such that the structural abstraction of ϕ holds in"
"recall that the design principle of the decidable fragment of strand is to examine the structural constraints in a formula ϕ, and enumerate a finite set of structures such that the formula is satisfiable iff it one of these structures can be populated with values to satisfy the formula. this strategy necessarily fails for the above formula ϕ2, as there is no class of finite structures that adequately captures all models of the formula, independent of the data-constraints. the sortedness formula ϕ1 in the first example is part of the decidable fragment of strand, while ϕ2 is outside of it. example 2.3. consider the formula:"
"it is now not hard to see that if s satisfiability-preservingly embeds into t wrt ψ, and graph(t ) satisfies ψ, then graph(subtree( t, s)) also necessarily satisfies ψ, which is the main theorem we seek."
"we define a new logic called strand (for \"structure and data\"), that combines a powerful heap-logic with an arbitrary data-logic. strand formulas are interpreted over a class of data-structures r, and are of the form ∃ x∀ yϕ ( x, y), where ϕ is a formula that combines a complete monadic second-order logic over the heap-structure (and can have additional quantification), and a data-logic that can constrain the data-fields of the nodes referred to by x and y."
"hence, we can decide whether a relation is elastic or not, by checking the validity of the above formula over all trees satisfying ψtr. the syntactic decidable fragment stranddec is defined as the class of all strand formulas of the form ∃ x∀ yϕ such that (a) ϕ has no quantification, (b) every occurrence of an atomic relation in ϕ is of the form r(z1, z2) where either r is an elastic relation or z1 and z2 are in x, or are constants. we can now show: theorem 5.9. over any class of recursively defined structures r, stranddec is decidable."
"the programs sorted-list-search and sorted-listinsert search and insert a node in a sorted singly-linked list, respectively, while sorted-list-insert-error is the insertion program with an intended error. the program sorted-list -reverse is a routine for in-place reversal of a sorted singlylinked list, which results in a reverse-sorted list, and bubblesort is the code for bubble-sort of a list. the routines bst-search and bst-insert search and insert a node in a binary search tree, respectively, while the programs left-rotate and right-rotate perform rotations (for balancing) in a binary search tree."
"for the structural solving phase, we report first whether the verification condition falls within our semantic decidable fragment strand sem dec . in fact, it turns out that all of our verification conditions can be written entirely in the syntactic decidable fragment stranddec! we also report the number of states, the bdd sizes to represent automata, and the time taken by mona to compute the minimal models. we report whether there were any models found; note that if the formula is unsatisfiable and there are no models, the z3 phase is skipped (these are denoted by \"-\" annotations in the table for z3)."
"one of the least understood classes of theories, however, are theories that combine heap-structures and the data they contain. analysis of programs that manipulate dynamically allocated memory and perform destructive pointer-updates while maintaining datastructure invariants (like a binary search tree), requires reasoning with heaps with an unbounded number of nodes with data stored in them. reasoning with heap structures and data poses fundamental challenges due to the unboundedness of the data-structures. first, for a logic to be useful, it must be able to place constraints on all parts of the structure (e.g. to say a list is sorted), and hence some form of universal quantification over the heap is absolutely necessary. this immediately rules out classical combinations of theories, like the nelson-oppen scheme [cit], which caters only to quantifierfree theories. intuitively, given a constraint on heap structures and data, there may be an infinite number of heaps that satisfy the structural constraints, and checking whether any of these heaps can be extended with data to satisfy the constraint cannot be stated over the data-logic (even if it has quantification)."
"as we show in this paper, the strand logic is well-suited to reasoning with programs. in particular, assume we are given a (straight-line) program p, a pre-condition on the data-structure expressed as a set of recursive structures r, and a pre-condition and a post-condition expressed in a sub-fragment of strand that allows boolean combinations of the existential and universal fragments. we show that checking the invalidity of the associated hoare-triple reduces to the satisfiability problem of strand over a new class of recursive structures rp ."
"for example, consider a list and a sublist of it. consider a formula that demands that for any two successor elements y1, y2 in the list, the data-value of y2 is the data-value of y1 incremented by 1 (as in the successor example in section 2):"
"given a straight-line program p that does destructive pointerupdates and data updates, we model a hoare-triple as a tuple (r, pre, p, post ), where the pre-condition is given by the datastructure constraint r with the strand ∃,∀ formula pre, and the post-condition is given by the strand ∃,∀ formula p ost (note that structural constraints on the data-structure for the post-condition are also expressed in post, using mso logic)."
"in this section, we show that given such a hoare-triple, we can reduce checking whether the hoare-triple is not valid can be reduced to a satisfiability problem of a strand formula over a class of recursively defined data-structures rp . this then allows us to use strand ∃,∀ to verify programs (where, of course, loopinvariants are given by the programmer, which breaks down verification of a program to verification of straight-line code). intuitively, this reduction augments the structures in r with extra nodes that could be created during the execution of p, and models the trail the program takes by logically defining the configuration of the program at each time instant. over this trail, we then express that the pre-condition holds and the post-condition fails to hold. we also construct formulas that check if there is any memory access violation during the run of p (e.g. free-ing locations twice, dereferencing a null pointer, etc.)."
"note that ψ bst has an existentially quantified variable z in gvar after the universal quantification of y1, y2. however, as z is a structural quantification (whose data-field cannot be referred to), this formula is in strand."
"note that the above is a pure mso formula on trees, and encodes the properties required of a minimal model with respect to satisfiability-preserving embeddings. using the classical logicautomaton connection [cit], we can transform the mso formula minmodel ψ ∧ ψtr ∧ b ψ to a tree automaton that accepts precisely those trees that define data-structures that satisfy the structural abstraction and are minimal models. since the finiteness of the language accepted by a tree automaton is decidable, we can check whether there are only a finite number of minimal models wrt satisfiability-preserving embeddings, and hence decide membership in the decidable fragment strand in fact, we develop, using the tool mona, the decision procedure above (see section 7)."
"the decision procedures work through a notion called satisfiability-preserving embeddings. intuitively, for two heap structures (without data) s and s, s satisfiability-preservingly embeds in s with respect to a strand formula ψ if there is an embedding of the nodes of s in s such that no matter how the data-logic constraints are interpreted, if s satisfies ψ, then so will the submodel s satisfy ψ, by inheriting the data-values. we define the notion of satisfiability-preserving embeddings so that it is entirely structural in nature, and is definable using mso on an underlying graph that simultaneously represents s, s, and the embedding of s in s ."
"in this section, we demonstrate the effectiveness and practicality of the decision procedures for strand by checking verification conditions generated in proving properties of several heapmanipulating programs. given pre-conditions, post-conditions and loop-invariants, each linear block of statements of a program yields a hoare-triple, which is manually translated into a strand formula ψ over trees and integer arithmetic, as a verification condition. the decision procedure for strand implements the decision procedure for the semantically defined fragment strand sem dec . given a strand formula, our procedure will first determine if it is in the semantic decidable fragment, and if not, will halt and report that satisfiability of the formula is not checkable. when given a formula in the syntactic fragment stranddec, this procedure will always succeed, and the decision procedure will determine satisfiability of the formula."
"consider first a purely relational vocabulary (assume there are no functions and even no constants). then, given a formula ∃ x∀ yϕ( x, y), let m be a model that satisfies this formula. let v be an interpretation for x such that m under v satisfies ∀ yϕ( x, y). then it is not hard to argue that the submodel obtained by picking only the elements used in the interpretation of x (i.e. v( x)), and projecting each relation to this smaller set, satisfies the formula ∃ x∀ yϕ( x, y) as well [cit] . hence a model of size at most k always exists that satisfies ϕ, if the formula is satisfiable, where k is the size of the vector of existentially quantified variables x. this bounded model property extends to when constants are present as well (the submodel should include all the constants) but fails when more than two functions are present. satisfiability hence reduces to propositional satisfiability, and this class is also called the effectively propositional class, and smt solving for this class exists [cit] ."
"in summary, we present a general decidability technique for combining heap structures and data, identify semantically a decidable fragment strand sem dec, demonstrate a syntactically-defined subfragment stranddec, and present experimental evaluation to show that the decidable combination is expressive and efficiently solvable. we believe that this work breaks new ground in combining heap structures and data, and the technique may also pave the way for defining decidable fragments of other logics, such as separation logic, that combine structures and data."
"the decidable fragment of strand is fashioned after a similar but more complex argument. given a subset of nodes of a model, the subset itself may not form a valid graph/data-structure. we define a notion of submodels that allows us to extract proper subgraphs that contain certain nodes of the model. however, the relations (edges) in the submodel will not be the projection of edges in the larger model. consequently, the submodel may not satisfy a formula, even though the larger model does."
"a fundamental component of analysis techniques for complex programs is logical reasoning. the advent of efficient smt solvers (satisfiability modulo theory solvers) have significantly advanced permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. popl '11, january 26-28, 2011 the techniques for the analysis of programs. smt solvers check satisfiability in particular theories (e.g. integers, arrays, theory of uninterpreted functions, etc.), and are often restricted to quantifierfree fragments of first-order logic, but support completely automated and efficient decision procedures for satisfiability. moreover, by using techniques that combine theories, larger decidable theories can be obtained. the nelson-oppen framework [cit] allows generic combinations of quantifier-free theories, and has been used in efficient implementations of combinations of theories using a sat solver that queries decision procedures of component theories."
"the goal of this section is to present an overview of the issues involved in finding decidable logics that combine heap structure and data, which sets the stage for defining the decidable fragments of the logic strand, and motivates the choices in our logic design using simple examples on lists."
"there have been a few breakthroughs in combining heap structures and data recently. for instance, havoc [cit] supports a logic that ensures decidability using a highly restrictive syntax, and csl [cit] extends the havoc logic mechanism to handle constraints on sizes of structures. however, both these logics have very awkward syntax that involve the domain being partially ordered with respect to sorts, and the logics are heavily curtailed so that the decision procedure can move down the sorted structures hierarchically and hence terminate. moreover, these logics cannot express even simple properties on trees of unbounded depth, like the property that a tree is a binary search tree. more importantly, the technique for deciding the logic is encoded in the syntax, which in turn narrowly aims for a fast reduction to the underlying data-logic, making it hard to extend or generalize."
"note that each program requires checking several verification conditions (typically for the linear block from the beginning of the program to a loop, for the loop invariant linear block, and for the block from the loop invariant to the end of the program)."
"the semantic decidable fragment strand sem dec is defined to be the class of all formulas for which the set of minimal structures with respect to satisfiability-preserving embeddings is finite, and where the quantifier-free theory of the underlying data-logic is decidable. though this fragment of strand is semantically defined, we show that it is syntactically checkable. given a strand formula ψ, we show that we can build a regular finite representation of all the minimal models with respect to satisfiability-preserving embeddings, even if it is an infinite set, using automata-theory. then, checking whether the number of minimal models is finite is decidable. if the set of minimal models is finite, we show how to enumerate the models, and reduce the problem of checking whether they admit a data-extension that satisfies ψ to a formula in the quantifier-free fragment of the underlying data-logic, which can then be decided."
"sem dec : a semantic decidable fragment of strand we are now ready to define strand sem dec, the most general decidable fragment of strand in this paper. this fragment is semantically defined (but syntactically checkable, as we show below), and intuitively contains all strand formulas that have a finite number of minimal models with respect to the partial-order defined by satisfiability-preserving embeddings."
reduction to satisfiability problem on the trail. it is easy to see that an error occurs during the execution of p on a graph defined through r that satisfies pre if the following strand formula is satisfiable on the trail rp :
"we utilize the semantically defined decidable class in the previous section to define a logic that has a simple syntactic restriction and is entirely decidable. the decidable fragment allows only formulas of the kind ∃ x∀ yϕ where ϕ has no further quantification. moreover, some of the structural edge relations r on the data-structure are classified as elastic relations. in ϕ, elastic relations are allowed to relate any pair of variables, while non-elastic relations are allowed only to relate existentially quantified variables in x."
"first, if γ i and γ j are of the same arity and over z and z, respectively, and further uniformly replacing z i with z i in γ i yields γ, then we can express the constraint (("
"the experimental results show that natural verification conditions tend to be expressible in the syntactic decidable fragment stranddec. moreover, the expressiveness of our logic allows us to write complex conditions involving structure and data, and yet are handled well by mona and z3. we believe that a full-fledged engineering of an smt solver for strand sem dec that answers queries involving heap structures and data is a promising future direction. towards this end, an efficient non-automata theoretic decision procedure (unlike mona) that uses search techniques (like sat) instead of representing the class of all models (like bdds and automata) may yield more efficient decision procedures."
"a relation r is elastic if, intuitively, for any model m and a submodel m of m, r holds on a pair of nodes of m iff r holds for the corresponding pair of nodes in m ."
"the decision procedure consists of a structural phase, where we determine whether the number of minimal models is finite, and if so, determine a bound on the size of the minimal models. this phase is effected by using mona [cit], a monadic second-order logic solver over (strings and) trees. in the second data-constraint solving phase, the finite set of minimal models, if any, are examined by the data-solver z3 [cit] to check if they can be extended with data-values to satisfy the formula."
"finally, separation logic [cit] has emerged as a convenient logic to express heap properties of programs, and a decidable fragment (without data) on lists is known [cit] . however, not many extensions of separation logics handle data constraints (see [cit] which combines this logic for linked lists with arithmetic)."
"we now show that we can effectively check if a strand formula belongs to the decidable fragment strand sem dec . the idea, intuitively, is to express that a model is a minimal model with respect to satisfiability-preserving embeddings, and then check, using automata theory, that the number of minimal models is finite."
"in this section we show how strand can be used to reason about the correctness of programs, in terms of verifying hoare-triples where the pre-and post-conditions express both the structure of the heap as well as the data contained in them. the pre-and post-conditions that we allow are strand formulas that consist of boolean combinations of the formulas with pure existential or pure universal quantification over the data-variables (i.e. boolean combinations of formulas of the form ∃ xϕ and ∀ yϕ); let us call this fragment strand ∃,∀ ."
"the satisfiability-preserving embedding relation can be seen as a partial order over trees (a tree t satisfiability-preservingly embeds into t if there is a subset s of t such that s satisfiabilitypreservingly embeds into t and subtree(t, s) is isomorphic to t ); it is easy to see that this relation is reflexive, anti-symmetric and transitive."
"intuitively, the formula ϕ2 refers to successive elements in the list, and hence a large model that satisfies the formula is not necessarily contractible to a smaller model. the formula ϕ1 in the sortedness example (example 2.1) refers to pairs of elements that were reachable, leading to contraction of large models to small ones."
"this formula has no free-variables, and hence either holds on the infinite tree or not, and can be checked by mona. this formula evaluates to true iff the formula is in strand sem dec . we also follow a slightly different procedure to synthesize the data-logic formula. instead of extracting each minimal model, and checking if there is a data-extension for it, we obtain a bound on the size of minimal models, and ask the data-solver to check for any model within that bound. this is often a much simpler formula to feed to the data-solver. in our current implementation, the mona constraints are encoded manually, and once the bound is obtained, we write a program that outputs the z3 constraints for the verification condition and the bound. the translation from strand to mona formulas and the translation from strand formulas to z3 formulas for any bound can be automated, and is a plan for the future. figure 3 presents the evaluation of our tools on checking a set of programs that manipulate sorted singly-linked lists and binary search trees. note that the binary search trees presented here are out of the scope of the logics havoc [cit] and csl [cit] ."
"we now show various examples to illustrate the expressiveness of strand. we sometimes use d() instead of data(), for brevity. example 4.1 (binary search tree). in strand, a binary search tree (bst) is interpreted as a binary tree data structure with an additional key field for each node. the keys in a bst are always stored in such a way as to satisfy the binary-search-tree property, expressed in strand as follows:"
", in the inner formula ϕ . second, if a constraint γ i involves only existentially quantified variables in x, then we can move the quantification of b i outside the universal quantification. doing these steps gives a more accurate structural abstraction, and in practice, restricts the number of models created. we use these more precise abstractions in the experiments, but use the less precise abstractions in the theoretical narrative. the proofs in this section, however, smoothly extend to the more precise abstractions."
we have introduced the calculus lk s which differs from standard sequent calculus by presenting a proof in a two-layered form: its abstract deductive structure on the one hand and a unifier which renders this structure a proof on the other hand. it has been shown that cut-elimination in lk s is equivalent to lk in the sense that the same set of normal forms is produced. the implicit term representation provided by lk s can be used for an implementation that provides an exponential compression of normal forms as well as for a fine-grained theoretical analysis of lk.
"for the conventional channel estimation in the time-invariant environment, the pilot tones are usually inserted between the subcarriers that contain the data signals, and the pilots and data signals coexist in the same ofdm symbol. however, when ici is introduced due to the time-varying channel response, the pilot signals suffer interference from the data signals of the neighboring subcarriers, and the accuracy of the channel estimation is degraded."
"in this work, a cs-based channel estimation method for a double selective channel is proposed for ofdm systems that use multiple transmission antennas. using the fitting polynomial assumption and a full-band training symbol, the proposed pf-omp algorithm is accomplished based on the iterative omp process. after exploring the advantages of the structured measurement matrix, the number of iterations is reduced and the computation complexity is consequently simplified. the proposed pf-omp algorithm can provide more accurate channel estimation results compared to the conventional ls-based method, and the improvement is more significant when the number of transmission antennas is increased."
"of the most significant taps (msts) in the channel response was provided. an optimized threshold is evaluated, and the tap coefficients that are less than the threshold are set to zero, and the noise introduced by the small coefficients can be eliminated. this method is designed for the ls channel estimation scheme, and requires an additional ifft."
"from an implementational perspective, we investigate the role of sharing in the context of first-order proofs from a novel point of view. previous work on proof normalisation with sharing treated the level of the proof, respectively the term calculus associated to it via a curry-howard correspondence: for example the work on optimal reduction for the lambda calculus, see [cit] for a survey, or deduction graphs [cit] which treat natural deduction directly. the present paper provides a complementary study of redundancy in the formulas of a proof, in particular the exponential compression described in section 5 cannot be obtained by the above-mentioned sharing mechanisms."
"comparing (19) and (18), the channel estimation process can be regarded as an observation of the channel response using pilot symbols. the vector c n t d contains all the parameters that will be estimated, and is modeled as the sparse vector it is possible to reduce the number of iterations from n t (q+1)l to n t l. when each delay path of the time-varying channel is approached using a fitting polynomial, the order-0 coefficient for the polynomial is usually much larger than the other higher order coefficients. since the omp algorithm is designed to identify the coefficients from the largest one, it can be expected that the order-0 coefficients from all polynomials are identified first with the highest probability. since the coefficients of different orders for the same polynomial are regularly allocated in the vector c n t d, once the index of the order-0 coefficient is identified, the location of the others can also be immediately indicated. for example, suppose that between the t-th transmission antenna and the receiver, the l-th path of the channel response is dominant, then the order-0 coefficient of the fitting polynomial is located at the (t −1)n cp (q+1)+l -th position in the vector c n t d . according to (14), (15), and (17), the order-1 coefficient is located at the (t − 1)n cp (q + 1) + n cp + l -th position, the order-2 coefficient is located at the (t − 1)n cp (q + 1) + 2n cp + l -th position, and so on. this means that when the order-0 coefficient is located at the (t − 1)n cp (q + 1) + l -th position in the vector c n t d, then the order-q coefficient is located at the (t − 1)n cp (q + 1) + qn cp + l -th position. therefore, the indication for the high-order coefficients can be avoided, and the complexity of the proposed algorithm is consequently simplified."
"in the present paper this separation is investigated from the point of view of cut-elimination. we introduce the calculus lk s, for first-order classical logic, which makes these two levels explicit: a proof contains formulas with free variables whose instantiation is specified independently. we define a cut-elimination procedure for lk s and show that it has the same set of normal forms as the standard sequent calculus. we describe two applications of this calculus: on the one hand we obtain an exponential compression of the size of normal forms which makes lk s a powerful mechanism for the implemention of lk. on the other hand the implicit representation of terms is used to give a considerably simplified proof of a characterisation of the form of witness terms obtainable by cut-elimination in terms of a regular tree grammar."
"with s n as set of substitutions. this lk s -proof in turn reduces to (ψ n+1, s n+1 ) in a constant number of reduction steps."
"suppose that the maximum number of channel paths is equal to n cp, and each path can be approached using an (7), the p-th subcarrier of the received symbol y m can be represented as"
"using the proposed polynomial-fitting omp (pf-omp) algorithm, the parameters for each channel path, i.e., the coefficients of the different orders of the fitting polynomial, can be more precisely evaluated, and more accurate estimation results compared to those from the ls algorithms are provided. the pf-omp algorithm can be directly applied to a multiple-input single-output (miso) channel where more transmission antennas are used. since the sparsity in the miso channels is more obvious, the advantage of the pf-omp algorithm is more significant."
"a technical aspect of cut-elimination is to keep track of the names of eigenvariables. the traditional solution of this problem is to work on regular proofs. an alternative would be to use additional constructs for local binding of these variables. in order to keep the object-level formalism as simple as possible, we opted for the first solution. the elimination of a contraction is the only reduction rule where this aspect has to be dealt with. let v be a set of variables. a substitution ρ is called fresh-variable renaming for"
"the remainder of this paper is organized as follows. section ii presents the model for the ofdm system in the time-varying environment. section iii shows the proposed channel estimation method, including the design of the training symbol and the method of adapting the omp scheme for the ofdm channel estimation. the numerical results are shown in section iv, before we finally conclude our work in section v."
"for the wireless time-varying channel between the t-th transmission antenna and the receive antenna, the n-th sample of the l-th path of the channel response can be denoted as g t m (n, l). the subscript m means that the channel is used by the symbol x t m . when disregarding the noise, and suppose that the maximum number of paths of the channel response is l m, the received signal transmitted from the t-th antenna can be volume 7, 2019 represented as"
(8) the ici patterns introduced from the transmitted symbol x t m after it passes through the channel with an order-q variation is represented as:
the c t m is a n cp (q+1)-by-1 vector that contains coefficients of all polynimials used to approximate the channel response between the n t transmission antennas and the receiver:
", ρ is injective and none of the α i occurs in v . we say that ρ is a fresh-variable renaming for an expression e if it is one for v(e). if an lk-proof π contains a subproof of the form"
"the channel response can be fitted after the coefficients of the polynomials are estimated. conventional least-square (ls) algorithms can be applied to evaluate the polynomial coefficients. however, when the required order of the polynomials increases for a faster time-varying channel, and more coefficients are awaiting to be identified, the conventional ls algorithms cannot provide a satisfactory estimation results. fortunately, the channel responses in the real world are usually dominated by a few high-gain paths, and about 95% of the energy is concentrated in these dominant paths [cit], and it is the inherent sparsity of the double selective channels [cit] . this implies that the estimation can be focused on these dominant paths and a more accurate approximation can be provided."
for the reader's convenience we first recall the reduction of a universal quantifier in lk. let π be an lk-proof. if it contains a subproof of the form
"an ofdm system with n subcarriers and n t transmission antennas is considered in this work. the transmitted signals are modulated using the inverse fast fourier transform (ifft), the n-th sample in the m-th ofdm symbol transmitted from the t-th antenna can be expressed as"
"when the channel estimation is applied to h t m, there are only n diagonal values waiting to be estimated in the time-invariant environment. in contrast, when the channel response changes within an ofdm symbol, ici is introduced, meaning that h t m will no longer be a diagonal matrix. it is difficult to directly estimate total unknown n 2 values in the n -by-n matrix h t m . fortunately, for the continuously fading channel in the real world, the variation of the time-domain channel response can be approached using polynomials, then the number of unknowns can be significantly reduced. this means that sparsity can be observed in h t m . moreover, the number of channel paths may be fewer than the length of cp, and only a small number of high-gain paths dominate the channel response, and further increase the sparsity."
in this section we will show that lk and lk s have the same normal forms. in order to compare the normal forms of lk s with those of lk we consider the set (nf lk s (π))
for the reduction of a contraction in lk s we have to extend the renaming to all variables which depend on eigenvariables of the duplicated proof.
"so lk s is equivalent to lk from an extensional point of view. it is however different from an intensional point of view, a property that will be exploited in the next two sections to demonstrate that lk s is an advantageous mechanism for implementing cut-elimination and a useful tool for carrying out a more finegrained analysis of lk."
"we can now reduce an lk-proof π using either the standard lk-reductions to obtain a cut-free lk-proof π * or the lk s -reductions to obtain a proof (ψ, s) where ψ is cut-free. for a regular lk-proof π we define"
"following the previous idea, the modified omp process applied to determine the solution for (18) is represented in algorithm 1, where the measurement matrix can be arranged as"
"where w is the noise introduced during the observation. the omp process contains two major operations in each iteration. the index of a non-zero coefficient is first identified, then the values of the currently collected non-zero coefficients are evaluated. these two operations are iteratively processed until all nonzero element in x are evaluated, and the maximum number of iteration is equal to the number of rows in ."
"because of its high spectral efficiency and robustness against multi-path frequency-selective channel [cit], orthogonal frequency division multiplexing (ofdm) is widely used in many communication systems. however, ofdm systems are vulnerable in time-varying environments, especially when the channel response changes within the duration of a single ofdm symbol. the orthogonality between subcarriers can be destroyed and inter-carrier interference (ici) is introduced. before removing the ici effect, the channel responses in double-selective environments, i.e., the multi-path time-varying channels, must be accurately estimated. recently, estimation schemes for double-selective channels [cit] have been extensively studied [cit] in high mobility wireless communication environments."
"the above result shows that lk s is an advantageous mechanism for implementing cut-elimination because it avoids to unfold terms as long as possible. in addition, it should be noted that due to the simplicity of the used data structure -a set -implementing lk s does not require more effort than implementing lk. the global substitution which is explicitly computed by lk s represents the full cut-elimination in a concise way and can also be applied to structures derived from the original proof π with cuts: for example to a short tautology [cit] read off from π to obtain a herbrand-disjunction or to the characteristic clause set [cit] to obtain a propositionally refutable clause set."
"without the interference from the data carriers, the ici in the full-band training symbol forms a predictable pattern and it can then be used for time-varying channel estimation. in addition, rather than using a randomly constructed measurement matrix as with the conventional cs techniques, a structured measurement matrix is adopted in the proposed omp algorithm, which consists of the prediction for the ici patterns that are derived from the cooperation of the full-band training symbols and the polynomial-approaching time-varying channel model."
the author is convinced that lk s will be a useful tool for obtaining stronger results of the above kind which is left to future work.
"can produce rich and diverse knowledge resources [cit] . the specificity of web forums is that they share a common layout. in particular, the posts are presented in chronological order and organized within threads. this well-organized structure is very useful for targeting specific data within forums. in general, data extraction from web forums involves retrieving the links that lead to threads or posts and obtaining the actual data objects of those threads and posts. a data object can be any information related to user participation in the forum, such as the publication date, author pseudonyms and the post title or content."
"in this paper, we present vigi4med scraper, a framework that extracts data objects from web forums and represents them in a semantic structure while maintaining the user's privacy. the vigi4med scraper framework consists of three main blocks: data extraction from web forums, semantic data representation and anonymization. whereas each one of these functionalities corresponds to an active research field, we combine them in a highly configurable solution. our system generates anonymized semantic graphs from any forum-like website according to a user-determined configuration file. with this configuration file, the user can freely specify the desired segments of the forum to extract and denote the correspondence between these segments and the desired semantic components. this flexibility in choosing the segments of data to extract allows vigi4med scraper to handle any forum-like website, which positions it as a generic solution for data extraction from web forums."
"in the example of fig 6, the anonymization process generates a new graph where the nodes \"post_7280\", \"thread_42\", \"forum_15\", \"site_xyz\", \"user_marie\" and \"user_mimi76\" are replaced by the anonymization keys obtained by sha1."
"these elements generate triples in the rdf graph, where the subject is the extracted url, which is used as an identifier, the predicate is the rdf relation \"type\", and the value is the corresponding semantic vocabulary describing a thread or a post. the generated identifiers are also used as subjects for the semantic attributes defined by the user in the remaining thread and post sections. to define a semantic attribute, the user states the desired predicate as well as the xpath that leads to its value for threads or posts. this is achieved by using the pattern described in listing 4."
"as mentioned earlier, vigi4med scraper consists of three main functionalities (fig 1) : data extraction from web forums, semantic data representation and anonymization. each of these functionalities is described in detail in the following subsections."
"the extraction of useful information from websites, referred to as scraping [cit], is a significant challenging task on several levels due to the large amount of information available on the internet. first, a scraping system must efficiently access web pages by avoiding non-informative data and duplicate pages. then, only useful data should be detected and extracted. the extracted data should be represented in an exploitable structure to facilitate data analysis. privacy is another major concern when manipulating web data [cit] . protecting the identity and private life of the user should be taken into consideration [cit], particularly in sensitive domains such as health [cit] because an increasing number of users today are sharing their personal information on social media such as web forums."
"the project involves several partners; each partner is responsible for one specific task in the project. vigi4med scraper was designed to complete the tasks of data extraction, semantic representation and anonymization. the anonymized graph generated by the first partner using the framework is further processed by another project partner to realize the annotation task. the pharmacovigilance end users are represented by two regional centers acting as partners in the project. these users are in charge of comparing case reports in the french spontaneous reporting system with potential adrs identified within patients' posts after the annotation step."
3. a message is sent to the forum's owners and administrators to inform them about the motivation of the project and their data crawling policy. this message also verifies our commitment to refrain from distributing or republishing the collected data.
"the application of our framework in this project involved an auxiliary script to extract a list of forums for each site chosen by the pharmacovigilance experts. this list was filtered manually to exclude non medical-related forums. for example, the forum \"fashion\" was eliminated from the forum list of the website \"www.doctissimo.fr\". this list was the starting point of the extraction process, along with its parameterization in the vigi4med scraper configuration file. because we hypothesized that a non-related medical discussion within a medical forum could indirectly lead to information about adverse drug reactions, no specific selection was made to filter the threads and posts. in addition, the extracted content was annotated (by the following partner) to discard irrelevant posts. regarding anonymization, we considered that pseudonymization was suitable for this project as none of the partners has access to both the anonymized data and the anonymization keys. as a result, 55 websites were selected by the pharmacovigilance experts. among these websites, 22 [cit] . the scraping within this period was not continuous, and it always respected the specified delays between sequential requests. over 60 million posts, 2.5 million threads, and 5.4 million pages corresponding to more than 200 gigabytes of data were collected. fig 8 shows the size of the generated rdf graph, the number of pages, and the number of threads and posts of each scraped website in this experiment. the privacy protocol was strictly followed. none of the involved website owners objected to the crawling process. the anonymized semantic graphs were delivered to the vigi4med partner in charge of adr annotation."
"we have presented vigi4med scraper, a generic tool to extract structured information from web forums. vigi4med scraper is part of the vigi4med project for detecting adverse drug reactions in social networks. all the scraping and anonymization scripts were implemented in php. the proxy is a perl program that is connected to a database (berkeleydb [cit] ) for caching. to run the system, a php server and perl installation are required. the complete source code is verbosely commented and publicly available under the gnu open source license. it can be accessed at the following address: https://github.com/bissana/vigi4med-scraper. full documentation (in english and french) regarding the code and configuration file is also provided at this url. although the configuration file for vigi4med scraper guarantees the maximum flexibility of the application, preparing such a file is a sensitive step requiring special attention. a complementary tool that helps the expert initialise the configuration file would be helpful for the preparation phase. in addition, a user-friendly interface that controls the grammar of the free parameters and proposes default configuration settings would prevent errors and increase efficiency. because the framework does not currently handle client-side generated scripts, authenticated access, or encrypted pages, adding a specific module to handle these cases would be an interesting extension of our work. finally, the maintenance of dom-based approaches is a critical issue in the literature of web data extraction because the structure of online pages is unstable and can be modified repeatedly. although this was not a problem for the project (which was a one-shot process to extract retrospective data), analyzing the state of the art on this issue would help us gain an important perspective for improving our framework."
"vigi4med scraper offers a freely available open source framework to retrieve data objects from web forums. vigi4med scraper employs a forum crawling strategy based on natural navigation and a data extraction approach based on dom structure. the framework is highly configurable and can be adapted to any forum-like website. privacy is handled by explicitly anonymizing any data objects that can potentially reveal a person's identity. the semantic representation in an rdf graph offers a harmonized structure that allows for straightforward manipulation by data analysis algorithms. with this representation, integrating the collected data with an existing semantic resource can be directly achieved. in other words, the resulted rdf graph follows the standard syntax and serialization format defined by the world wide web consortium (w3c); thus, it is straightforward to link it to other existing rdf graphs or extend it with new concepts and semantic relations. the valid conceptual representation in vigi4med scraper acknowledges the nature of forums, as the organizational structure of the forums and the page flipping aspect are naturally represented in the rdf graph. furthermore, vigi4med scraper is extremely selective; it will not blindly explore all the available links and data in a page but instead utilizes the specific \"next page\" link, which also allows it to maintain the logical connection between posts (or threads) across several pages. this selective behavior has the advantage of avoiding non-informative data. for example, advertising posts, which generally do not have the same structural characteristics as normal posts, are invisible to our algorithm. duplicate pages will only be scraped once because our solution keeps track of previously accessed links, and the proxy ensures that no additional requests are sent to previously accessed pages. the proxy also guarantees a minimal delay between sequential requests to avoid network and server overload. although the requirement of having a trained user fill out the configuration file can be considered as a potential limitation of the framework, it guarantees accurate and efficient data extraction. in addition, such user intervention can be facilitated by the dom inspection tools of several internet browsers. for all these reasons, vigi4med scraper was the adopted solution for extracting posts from several medical-related forums within the vigi4med project."
"approaches that extract structured data from web pages have been extensively studied. the procedures implemented to achieve structured data extraction are called wrappers [cit] . several techniques, such as regular expressions and tree-based methods, can be used to generate a wrapper. the document object model (dom) is commonly used to extract data from web pages. a dom tree represents the pages' information in a structure that can be exploited by special queries (xpath queries). although manual approaches allow one to specify the data of interest, they rely heavily on users with the appropriate technical expertise. automatic approaches were introduced to lower the amount of user effort required for this task. the majority of these approaches still require human intervention to label training examples (e.g., [cit] ). fully automatic approaches try to detect nested or repeated patterns to target interesting contents (e.g., [cit] ), but they suffer from a higher risk of extracting non-informative data and are difficult to customize."
"3.1.3 data extraction summary. as we see from fig 1, the data extraction step can be summarized as follows. the user fills out one configuration file per site. in this file, she provides the list of forums that she wants to scrape. for each url on this list, the scraping function generates another list that contains the urls of all the available threads in the current forum. this list is then used to scrape the posts in each thread prior to scraping the threads of the subsequent forum. the scraping function is identical for both threads and posts, and the configuration file tells the function which objects to extract in each case. before requesting an url, the scraping function starts by checking the logs. if the url was previously scraped, it will be ignored; otherwise, it is sent to the proxy. the proxy will either retrieve the corresponding html page from its cache or fetch the page from the destination website. when the html page is ready, a dom tree is generated, and the xpath queries in the configuration file are executed against the dom tree to obtain structured data records. for example, if the user defined the xpaths of the posts' titles, creators and publication dates in the configuration file, the algorithm will produce structured data records with the values of these fields for each post in each scraped html page. vigi4med scraper will keep navigating (in thread or post pages) until no match is retrieved for the xpath defined in the element \"nextpage\". this naturally happens when we reach the last page of threads (or posts), where the link that leads to the next page is absent."
"vigi4med scraper was used within a pharmacovigilance project. pharmacovigilance is defined by the world health organization as \"the science and activities relating to the detection, assessment, understanding and prevention of adverse effects or any other drug-related problem\" [cit] . in this domain, analyzing web forums is an appropriate way to generate new knowledge about adverse drug reactions (adrs) [cit] . the task imposes two strict requirements for data extraction policy: protecting the privacy of forum users and preserving the performance of the targeted sites. protecting user privacy is extremely critical when handling personal health data; however, most of the existing web crawling and data extraction approaches blindly gather all types of information without any consideration of privacy. in addition, medical forums are exceedingly popular and have large-scale usage. thus, the basic requirement of preserving the performance of the crawled websites should be strictly fulfilled, and a particularly respectful attitude towards the hosting server of medical forums should be considered."
"scraping web forums implies a significant number of connections to the websites from which we want to extract data. in order to minimize the network load and avoid overwhelming the destination servers, it is important to avoid duplicate requests and have a delay between sequential connections. in web forums, a web page can be scraped several times if a thread belongs to several sub forums or if the scraping process is restarted for any reason. to handle the first case, a log file keeps track of all the visited urls in a website. before requesting a new url, the scraping script checks if the url has already been visited, in which case, it will be ignored. to handle unexpected issues relating to retrieving a url that was already visited, a proxy with a cache database is proposed. before opening a new connection with the destination server, the scraping function checks if its database already contains the desired html page. if so, the html page is sent back to the scraping function; otherwise, a connection is established with the destination server, and a copy of the received html page is saved to the cache. moreover, the proxy maintains a minimum delay of 0.330 seconds between two sequential requests. this delay is extended to 2 seconds during working hours at our university to preserve the performance of the network."
"this paper is organized as follows. we start by presenting an overview of related work in section 2. the overall structure of the vigi4med scraper is described in detail in section 3. section 4 shows how the framework was applied to the vigi4med project. a discussion comparing our system with previous work is proposed in section 5. finally, the availability of vigi4med scraper and future directions are presented in section 6."
"adverse drug reactions are often mentioned in medical-related discussions. the extraction of knowledge from web forums has recently received attention in the scientific community to exploit this complementary data source. indeed, clinical trials are essential for identifying adrs; however, they are expensive and time consuming and cannot detect all possible reactions, particularly uncommon reactions. this is because a limited number of patients are enrolled in these trials, and children and pregnant women are often not considered in them. moreover, during the post marketing phase, patients and health professionals do not report every adr to safety agencies or the pharmaceutical industry, even when such reporting is mandatory for health professionals. adrs are a common cause of morbidity. for example, in france, the estimated annual number of adr-related hospitalizations is greater than 140 000 [cit], which explains why new strategies to address the problem of under-reporting are being embraced. patient feedback through online social networks is a non-negligible resource for potentially reducing the number of deaths and hospitalizations due to adverse drug reactions [cit] . the efficient detection of adrs from such resources could finalize and/or confirm the results of clinical trials and post-marketing reports. it also allows for the expedited detection of potential adr signals that have gone unnoticed in previous sources that might emerge at a later time. in this context, the french drug safety agency ansm (french acronym of agence nationale de sécurité du médicament et des produits de santé) founded the vigi4med [cit] . the main goal of ansm is to detect adrs from posts in social networks, particularly medical-related web forums. to achieve this goal, the partners of the vigi4med project have agreed upon the following protocol:"
"1. a declaration about extracting medical information from web forums is sent to the cnil [cit], the national organization of data protection in france."
"it is important to note that to break the anonymization, we can simply concatenate both the anonymized data and the corresponding anonymization key files. recognizing this concatenation, an rdf parser can find the connection between the anonymized triples and their keys in the same file. one may argue that anonymization should be irreversible. however, we hypothesized that the retrieval of original identifiers should be allowed in specific cases. for example, in pharmacovigilance, the detection of a dangerous case of drug exposure might necessitate notifying the patient to contact her physician. nevertheless, our framework is designed to generate a separate graph of anonymization keys, which should be kept in a safe place during vigi4med scraper: structured data extraction and semantic representation of web forum normal usage. in addition, these keys should not be used by the team that processes the anonymized data."
"in this pattern, the name of the predicate generated in the n-triple file is specified by \"rdf-predicate-name\". the xpath that leads to the desired data value is \"xpath-address\", which corresponds to the object of the triple. the user is free to specify the semantic vocabulary used to define the predicates. the most commonly adopted standard to describe forums elements is sioc [cit] . for example, the property \"sioc:num_replies\" defines the number of replies within a thread. other vocabularies can be used to describe generic (not forum specific) properties, such as \"dc:date\" from dublin core [cit], which has broad and generic elements to describe a wide range of resources, or \"nie:htmlcontent\" from nepomuk information element ontology [cit], which describes native resources available on the desktop. \"rdftype\" is optional; if it is declared, the type will be added to the value of the generated triple. in the example of fig 5, the xpath leading to the number of thread views is specified in the configuration file \"configfilea\". the pattern in this example specifies that this object corresponds to the semantic predicate \"sioc:num_views\" of type \"integer\". using this configuration, the corresponding triple is generated in the output file \"temp-filea.n3\". to demonstrate rdf graph generation, fig 6 shows an example of a generated sub graph for a post. in this example, the post \"post_7280\" has the type \"sioc:post, and it is associated with the author, text and date by the semantic relations \"sioc:creator\"', \"sioc:content\", and \"dc:date\", respectively. the \"post_7280\" belongs to the thread \"thread_42\" (of type sioc:thread), which is associated with the title, creator, and the number of replies via the semantic relations \"dc:title\"', \"sioc:creator\", \"sioc:num_replies\", respectively. finally, this thread appears in the forum \"forum_15\" (of type sioc:forum), which belongs to the website \"site_xyz\" (of type sioc:site)."
"pseudonymization is applied in the current version of the framework. we defined authors' pseudonyms, profile information, and the urls of posts and threads as the identifiers to anonymize. each identifier is replaced by a key generated by the cryptographic hash function sha1 (secure hash algorithm 1). the semantic representation is preserved for both the anonymized data and the anonymization keys. in other words, we generate two n-triple graphs, one for the anonymized objects and the other for the anonymization keys. for example, in fig 7, the file \"rdffilea.n3\" has a triple regarding the number of views of a thread. the identifier of this thread (its url) is anonymized in the file \"anonymfilea.n3\", while the anonymization key is kept in another file \"anonymkeysa.n3\"."
"3.1.1 configuration file. vigi4med scraper requires a configuration file that contains the xpaths of the objects the user wants to extract from the forums of a website. as shown in the listing example 1, these xpaths are organized into two sections, one for threads \"threadsinfo\" and the other for posts \"messages-info\". the number and the nature of the objects to be extracted are not pre-defined. the only constraint is the specification of the elements \"sioc: thread\" and \"sioc:post\" (details in section 3.2), which contain the xpath of the identifier of a thread or a post, respectively, and the element \"nextpage\" (for threads and posts) to provide the xpath of the next page's url. apart from these particular elements, the algorithm will take into account any object described in the configuration file as long as it matches a pattern that the algorithm can recognize. we describe this pattern later in listing 4. in addition to these two sections, the configuration file contains proxy information and a regular expression that describes the format of the \"date\" objects (e.g., post publication date) in the scraped website. this regular expression is optional; it is used by the algorithm to standardize the date representations in the output rdf [cit] file. the section \"files info\" contains the input and output file names. the input file is the list of forums' urls to scrape, and the output files are the log and the generated rdf graph."
"in order to represent the collected data in a flexible and efficient structure, we use a rdf graph with n-triples syntax [cit] . each line in the generated n-triples file is a sequence of subject, predicate and object separated by whitespace and terminated with a \".\" after each triple. vigi4med scraper does not presume a unique inner-structure for threads and posts, as these data can differ from one site to another and depend on the requirements defined by the user for a specific task. nevertheless, the system is designed to scrape web forums. three elements are thus mandatory: navigation through pages for both threads and posts, the identification of a thread, and the identification of a post. the special element \"nextpage\" should be used to declare the xpath corresponding to the \"nextpage\" navigation link; the extracted link will only be used for crawling and will not appear in the resulting rdf graph. the elements \"sioc: thread\" and \"sioc:post\" are used to declare the xpaths to the urls of the threads and the posts as shown in the examples presented in listing 2 and listing 3, respectively."
"we have proposed an mber beamforming receiver for multi-user sdma based qam systems. the ber formula has been derived explicitly, and the optimal mber solution has been obtained by minimising the ber cost function using an efficient simplified conjugate gradient algorithm. although the bit decision procedure is inherently more complicated than the symbol decision making process, it is interesting to see that the ber calculation turns out to have a similar complexity as the ser calculation, at least for the 16-qam case. therefore, the mber beamforming solution requires a similar computational complexity to that of the mser solution. the simulation results obtained have also confirmed that the mber beamforming receiver attains the same ber performance as the mser beamforming receiver, and the both solutions significantly outperform the standard mmse-based beamforming receiver."
"the approach adopted in this study, however, can be extended to higher-order qam schemes. without loss of generality, user 1 is assumed to be the desired user and the rest of the sources are the interfering users. a linear beamformer is employed, whose output is given by"
"to the best of our knowledge, however, no direct mber solution has been derived for qam systems to date. for a high-order qam scheme, multiple bits that form a symbol point have different bit error probabilities and the bit decision procedure is a complicated one [cit] . by contrast, making a symbol decision is easier, and it would appear that the symbol error ratio (ser) calculation therefore would be computationally less complex. this was partly the \"reason\" why the mser criterion was considered in our previous work [cit], when studying high-order qam systems. in this contribution, we explicitly derive the ber expression as the function of the beamformer's weight vector, and formulate the mber beamforming for qam systems as the solution of the resulting optimisation problem that minimises the mber criterion. we adopt a computationally attractive simplified conjugate gradient algorithm, which has previously demonstrated its effectiveness in solving the mser optimisation problem [cit], to solve this mber optimisation for sdma based qam beamforming systems. a surprising result of this study is that the complexity of the mber optimisation turns out to be similar to that of the mser optimisation. moreover, our simulation results confirm that both the mber and mser systems have the same achievable ber performance, and they significantly outperform the standard minimum mse (mmse) based solution."
"the ever-increasing demand for mobile communication capacity has motivated the employment of space-division multiple access (sdma) for the sake of improving the achievable spectral efficiency. a particular approach that has shown real promise in achieving substantial capacity enhancements is the use of adaptive beamforming receiver with antenna arrays [cit] . classically, beamforming design is based on minimising the mean square error (mse) criterion. since for a communication system, it is the achievable bit error ratio (ber), not the mse performance, that really matters, the minimum ber (mber) beamforming has been derived for binary phase shift keying (bpsk) systems [cit] and quadrature phase shift keying (qpsk) systems [cit] . note that a qpsk system can be viewed as consisting of two bpsk systems. quadrature amplitude modulation (qam) schemes [cit] have become popular in numerous wireless standards by virtue of providing a high throughput. minimum symbol error ratio (mser) beamforming receiver has been conceived for qam systems [cit] ."
"the decision rules for the quadrature c1 and c2 bits are given similarly based on y i (k). it is seen that this bit decision procedure is more complicated than making a symbol decision. classically, the beamformer's weight vector is determined by minimising the mse metric of"
"while the second summary term in (27) is much smaller than any other summation term in the ber expressions (19), (26) and (27). thus, p"
"are 11, 10, 00, 01, and the two in-phase bits that form the real part of 16-qam symbol are known as the in-phase class 1 (c1) and class 2 (c2) bits, respectively [cit] . the decision for the in-phase c1 bit is given by"
"for notational simplicity, we assume 16-qam modulation. therefore, the kth transmitted symbol of user i, denoted as b i (k), takes the value from the symbol set"
", are all the legitimate equiprobable sequences of b(k). thus, the noisefree part of the beamformer output only takes values from the finite set given byȳ"
" after manual investigation, it appears that many of the urls are bad/incomplete from the moment they are posted. for example, a url might end with .ocm instead of .com. we suspect simple typos. without analyzing all of the urls, we can't put a percentage on the number of typographical error related failures across the entire data set, but an evaluation of 1000 non processable url's revealed that roughly 20% were due to typographical error."
"image collection was done using a combination of two tools, cutycapt [cit] and xvfb [cit] . the first of which, cutycapt, takes screen-shots of a fully loaded web-pages by launching a webkit based gui-less web-browser. xvfb was used to provide cutycapt with a virtual frame buffer, since the server this entire processes resided on did not have an x server installed. the problems involving these two pieces of software are as follows:"
"the main contributions of this paper include: first, the time-varying characteristics of parameters of the pmsm model is analyzed, and a simplified multi-step prediction model of tracking errors of torque and stator flux is derived. second, a method for reducing the candidate switching sequences by using band constraints is proposed, and how to properly configure the band size is analyzed. third, different control modes are designed in transient and steady state respectively, which can ensure that the system has good dynamic and steady-state performance at low switching frequency. she is currently a professor with the college of electrical engineering, zhejiang university. her current research interests include electrical machines and their control systems, power electronics, and electric drives."
"another issue that needs to be considered is the influence of having teachers as \"friends\" on facebook. although [cit] mention, some students have been affected by what they wrote on facebook after adding their teachers as friends. another problem is that the self-disclosure of instructors on facebook could affect their credibility and the perception that students have of the teacher. that is why many authors suggest educators to create a different facebook account just for professional and educational purposes [cit] . [cit] also analyze another challenge of using facebook in the classroom. generally, instructors are interested in the use of technology to augment student learning; however, at times, they have difficulty maintaining students' attention on class activities when facebook activities are initiated. conflicting evidence exists on the impact of facebook on dedicated study time since some authors suggest that facebook users spend less time studying and achieve lower academic results compared to facebook non-users [cit], it was possible to observe that facebook users had lower gpas and spent fewer hours per week studying than nonusers. [cit], this usually happens because if a teenager is trying to have a conversation on an e-mail chat line while doing homework, he will suffer a decrease in efficiency, compared to if he just thought about the homework until he was done. fortunately, this context is a little different in english classrooms because in efl classrooms, the knowledge is constructed precisely by interaction and communication. actually, students can practice english as they talk to their friends about topics that are interesting for them. the only thing that teachers need to consider is to give students the opportunity to have access to english native speakers in order to promote productive conversations in english."
"in order to identify when two web sites look the same, we render the site, take a screen shot using cutycapt and compute a hash of the resulting image. we compare website images by calculating the hamming distance [cit] between their hash values. hamming distance detects the similarity of two equal strings that the exact same length. the resultant number is the minimum set of substitutions that must occur from one string to the other to make them match. for example, when comparing two 32bit md5sum's, the possible hamming value would be in the range of 0 to 32 where 0 means they are identical and 32 means that every single bit is different. we experimented with a variety of hash calculation methods including cryptographic hashes like md5, sha512 and phash (perceptual hash). phash is a hashing algorithm specifically designed for fingerprinting multimedia files such as images that relies on discrete cosine transformation (dct) to reduce sampling frequency in the file [cit] . in table 1, [cit] . we introduced a series of minor changes to the site and computed the hamming difference relative to the original site for each hash algorithm. our first change was minor modifications in the page text. specifically, we removed all cases of the word \"the\" from the page. relative to the original image, the hamming distance for the md5 hashes was 3, while the sha512sum resulted in a distance of 0 and the phash distance was 0. after completely replacing a few sentences, we began to see larger distances with the sha512sums as well. however, even with a significant portion of the text changed and the color of the paypal logo altered slightly we still received a hamming distance of 0 using phash. it was not until we changed the color of multiple items on the page and rearranged things that we started to see higher distance values. this means that a phishing site would need to significantly different from the original page that it was trying to spoof for it to go undetected with the phash technique."
as we continue this work we intend on implementing functions for dealing with inconsistent page load times. currently we stop trying to process a page after 3000ms however in some select cases pages may need more time to load. we intend to implement a system whereby pages that have not loaded after 3000ms will be handed off to a child process that will continue to wait an additional period of time. this will stop a single long loading page from slowing down the rest of our analytic process.
"google has developed and deployed a hybrid approach. their system relies on both google's own page ranking system and an email spam filtering system to per-identify potentially malicious pages before analysis [cit] . they also apply a machine learning classifier which considers the characteristics of the url and the website or message content. while this method works well in google's own environment, it is unreasonable to assume that an individual institution, or even government would be able to deploy their own version of it given google's unique place in the internet infrastructure."
the selected technical requirement to be addressed was the sso; a common requirement in the three use cases. the planned objectives of the study have been reached; open labyrinth has been integrated into the openedx platform as an example of integrating vp systems with mooc platforms.
"simply adopting a technology and not truly understanding its potential will not suffice. if teachers do not try to learn more about the multiple uses of all these online tools like facebook, it won't be possible to engage and motivate students because they are ahead of teachers in the use of technology [cit] . however, this is not the only challenge that educators face. a second challenge is the distinction between entertainment and truly intellectual engagement. the nature of multimedia can captive students easily, but this visual engagement does not necessarily represent intellectual engagement. in fact, too much multimedia stimulation can interfere with the deeper cognitive processing that is critical to learning [cit] . that is why teachers need to make sure that the activities they plan to develop using facebook really help students to learn."
we are also working on cluster analysis software that will identify the closest matches in the whole data set without requiring a legitimate site to match against.
" cutycapt can only process one url at a time and launches a new instance of its self every time it does so. on a headless system, this requires xvfb to launch a virtual x session every time cutycapt processes a url. in conjunction with the additional time required to launch, there is an associated tear down time for that x session. if a delay is not included at the end of the cutycapt url parsing loop, then an x session collision occurs and will keep occurring for every link read into cutycapt until the x session fully closes. to deal with the previous issue of x session build up and tear down time, we attempted to run multiple instances of xvfb and cutycapt simultaneously on different groups of the urls. the basic concept of this was to split the url input file into equal pieces and when requesting the image capturing script we would then increment the virtual display number in xvfb. each instance could then append their results to the output csv file. however this proved unreliable since xvfb uses such a high percentage of system resources when starting up. more then two instances of xvfb running at one time slowed the quad-core test system down to a crawl."
"the sso was achieved by the use of the blti standard. the implementation was verified by the use of two test cases that were created with the aim of demonstrating a transparent authorization process for the two different types of users. in particular, the users logged into the openedx platform may access the content in the integrated open labyrinth without repeated entry of their credentials. the instructors from openedx get automatically authorized by open labyrinth and may view, edit, delete, or author a new vp by acquiring the corresponding administrative user rights to the open labyrinth vp system. the learners from openedx get authorized in open labyrinth and may view the vp cases."
"from the previously established technical requirements [cit] ) a central one was selected for implementation: a transparent authentication of the learner enabled by an identity management mechanism. the feasibility study evaluates a type of single sign-on technique integrating a virtual patient system and a mooc platform that has not been demonstrated previously. it enables a user logged into the mooc platform to access vp cases without requiring a repeated manual entry of the credentials. from the user perspective, the moment of entering a second system is unnoticeable, which saves time and improves user satisfaction with the learning experience. this feature is useful both for instructors and learners of a mooc course, and is a prerequisite for the implementation of more advanced functions."
"considering that the cost function of ptc contains the tracking error terms of torque and stator flux amplitude, and the band constraints are set according to the limits of tracking errors in this paper, the prediction model of tracking errors of torque and stator flux amplitude is directly derived in this section."
"in this paper, a band-based multi-step predictive torque control strategy for pmsm drives is proposed. based on the simplified multi-step prediction model of tracking errors and the method of reducing candidate switching sequences by using band constraints, the proposed strategy can achieve three-step prediction within 50µs. compared with the c-ptc, the proposed strategy can reduce the switching frequency by more than 37%, and keep good dynamic and steady-state performance at the same time. therefore, the proposed strategy is feasible and effective in achieving multi-step prediction and reducing switching frequency."
"in the future, we would like to explore modifying cutycapt to add multi-threaded support and x session reuse to the code base. by reusing a single x session we would reduce the multiple seconds of latency that is incurred each time that build up and tear down occurs. adding multi-threaded support that incorporates virtual x session reuse would be even more beneficial. we would be able to increase load time wait in cutycapt to support slow sites, without waiting for one site to complete, before processing another."
"according to the analysis in the section iii, the reasonable setting of the band size is crucial to the implementation of the proposed strategy. the method to determine the appropriate band size by simulation analysis is introduced in this section. the principle is as follows: first, remove the switch term in the cost function (18), and take into account only the tracking errors of the torque and stator flux amplitude. it can be sure that the optimal sequence obtained by exhausting all switching sequences must be able to minimize the torque and stator flux ripples. then, gradually reduce the band size, and the remaining switching sequences after band screening will be less and less, until the torque and stator flux ripples show an upward trend, indicating that the band size is too small at this time, thus the optimal sequence may be excluded. the band size corresponding to the critical point at which the torque and stator flux ripples increase can be used as a reasonable set value."
"to identify the standards apt for implementing the integration, we reviewed the databases of scopus, eric and pubmed. [cit], to discard outdated technologies. the review was performed using the following queries:"
"it should be noted that the optimal switching sequence u opt (k) obtained by multi-step predictive optimization, only the first element, i.e. s opt (k), is applied to the inverter at kt s moment, and the above process is repeated in the next period to obtain the new optimal switching sequence u opt (k + 1), namely receding horizon optimization strategy."
"facebook also provides teachers access to valuable educational apps [cit] . for instance, \"booktag\" can be used to share books in english and ask students to comment on them; \"knighthood\" is a game app on facebook that promotes reading skills in english; \"language exchange\" is an app that helps students to get connected with foreign language practice; and finally, \"flashcardlet\" is an application used to create flash cards that students can study on facebook to learn vocabulary words in english [cit] . in general, facebook has many features that teachers can take advantage of. actually, there are several studies that explain how teachers all around the world are using this network for educational purposes, especially in english language classrooms. [cit] mention that they successfully used facebook in an esl (english as a second language) environment to post announcements and give students a place to connect with each other and chat in english. they also reported that students were so engaged with this online resource that they continuously posted educational links they wanted to promote [cit] . actually, facebook has many possible uses such as asking and answering questions, posting information, and engaging students in reading and writing activities which are helpful to improve their english skills [cit]"
"the size of the rectangular region is determined by e max t and e max ψ, so whether the torque and stator flux amplitude exceed the band constraints can be judged directly according to the tracking errors. at steady state, the rectangular region rotates synchronously with the reference stator flux vector. in order to ensure good steady-state performance, the stator flux vector must always be in the rectangular region. therefore, at steady state, the tracking errors of torque and stator flux amplitude should satisfy the following constraints:"
"facebook also provides students meaningful learning experiences that give them the opportunity to practice their language skills in a more incidental and informal manner [cit] . this helps teachers to make connections between instructional and real-life tasks. in that way, students can use the skills learned in the classroom and apply them to real situations outside of class [cit] noticed that using facebook to connect the theories and concepts learned in class to real life contexts is a good way to enhance students' understanding. in their study, conducted with 535 adolescents of different efl classrooms in malaysia, they found out that facebook was a powerful pedagogical tool that helped students to improve their english language skills. although this study was conducted in an esl environment, the results are also practical for efl classrooms."
"in this paper we explored the possibility of integrating moocs with vps. adding virtual patients to moocs provides the learner with possibilities for active, exploratory acquisition of competencies such as clinical reasoning. integrating moocs with vps was discussed in the past in the context of investigating the potential educational benefits of three previously proposed educational scenarios [cit] . however, a practical implementation of selected technical elements was reported for the first time in this study. the challenge involved the selection of a suitable standard and extending the source code of a spacious open-source project in several places. this implementation enables further research on the educational benefits of the integration as well as their evaluation."
"when the changes of parameters k and e need to be considered, the predictive values of tracking errors of torque and stator flux amplitude at (k + n)t s moment can be obtained by iterative calculation of (10) to (13) . however, if n is relatively large, it will take a lot of extra calculation time."
this study aims to investigate how to technically integrate virtual patient systems with mooc platforms. such knowledge will inform the feasibility of further educational research and evaluation of the potential educational benefits. bearing in mind the vast topic we predict that the implementation will follow an iterative step-wise approach starting with the most fundamental requirements. the objective of the study is to select such a technical requirement and implement it as a prototype.
"this first differentiation of mooc types resulted in a wave of new terms to label variants of open online courses in respect to scale, pedagogical model or target audience. vocational open online courses are delivered for wide but targeted audiences with the aim to foster career progression and leverage specific work-related skills [cit] . small private online courses (spocs) support flipped classroom learning; the online lectures are delivered to a limited number of participants of a college class. the instructors may use the actual class time to provide the remaining, more student-tailored, learning experience [cit] ."
"according to (3), the stator flux amplitude ψ s and torque t e can be controlled by the x-axis component and y-axis component of the applied voltage vector u s respectively. essentially, the voltage vector u s controls stator flux amplitude and torque by changing the position of the stator flux vector ψ s . as shown in fig. 2, when the stator flux vector ψ s is located in the rectangular region centered on the reference stator flux vector ψ ref s, the torque and stator flux amplitude can be guaranteed to meet the band constraints. therefore, at steady state, only the voltage vectors that can leave stator flux vector in the rectangular region are selected as the candidate voltage vectors, which are represented by the red solid arrows in fig. 2, thus realizing the selection of candidate switching sequences. obviously, the larger the rectangular area is, the more candidate voltage vectors need to be selected for each step of prediction. on the other hand, if the rectangular area is too small, the number of candidate voltage vectors may be zero, leading to no solution for optimization. therefore, it is necessary to reasonably set the band size through simulation analysis, as shown in section v-a."
"in order to verify the feasibility and effectiveness of the bm-ptc strategy, this paper conducts an experimental study on a 6-kw pmsm. the parameters of the pmsm are shown in table 2 . the experimental setup is shown in fig. 8 . the load is supplied by an 11.2-kw induction motor controlled by the sinamics s120. the control system employs a tms320 f28335 dsp and an ep3c40q240c8n fpga."
"in this paper we investigated the technical perspective of integrating vps in moocs, aiming to set a base for future investigation of the topic; the pilot implementation provides evidence about the potential of integrating vp systems with mooc platforms on the example of a transparent authentication mechanism, inviting further research for a complete integration and implementation of the suggested use cases."
"although there is evidence to support the effectiveness of training clinical reasoning skills using vps [cit], they \"play only one part in the development of skilled health professionals\" and coordination with other instructional activities is suggested [cit] . positive effects have been reported when vps are used as an additional resource or as an alternative to traditional methods [cit] ."
"the mpc strategies can be divided into continuous control set model predictive control (ccs-mpc) and finite control the associate editor coordinating the review of this manuscript and approving it for publication was xiaodong sun . set model predictive control (fcs-mpc). the former needs to obtain continuous input variables with the help of space vector pulse width modulation (svpwm) technology. its switching frequency is constant, but it will increase switching loss because of unnecessary switching actions [cit] . the latter directly takes the discrete switching states as the input, without complex space vector modulation process [cit] . the fcs-mpc strategies can be divided into single-step mpc and multi-step mpc according to the prediction horizon. single-step mpc can only obtain the local optimal solution in one control period, while multi-step mpc can obtain the global optimal solution in a longer prediction horizon [cit] . however, the optimization problem of fcs-mpc is a typical integer programming problem, which is generally solved by the exhaustive method. with the increase of the prediction horizon, the calculation burden will increase exponentially [cit] . therefore, the conventional multi-step mpc algorithm is difficult to realize in a short sampling period, volume 7, 2019 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ which makes it difficult to apply to motor drives. in order to solve the problem of high computational complexity of multistep mpc algorithm, many multi-step mpc strategies have been proposed successively [cit] ."
"both edx and openedx platforms conform to the blti standard, meaning that they can act as a tool consumer. open labyrinth however required adjustments to function as a tool provider. the blti makes use of the oauth protocol signing approach (oauth) to secure the message interactions between the consumer and the provider, which requires a set of credentials: a key and a secret."
"substitute (19) into (2), the average values of x-axis component and y-axis component of each voltage vector in the prediction horizon can be obtained as"
"virtual patients (vps) are defined as \"interactive computer simulations of real-life clinical scenarios for the purpose of healthcare and medical training, education or assessment\" [cit] . this definition distinguishes the vps from devices, human standardized patients, part task trainers and high fidelity manikins [cit] . vps have become an established tool in healthcare teaching and assessment. in particular, vps are suggested as the key technology that can develop the fundamental skill of clinical reasoning amongst students, allowing students to develop these skills to a similar level as that achieved whilst training with real patients [cit] ."
" website load time was the last major contributing factor to the limited number of urls processed for image comparison. we put in a hard limit of 3000ms for a page to load before cutycapt would simply stop and move onto the next, this proved insufficient for many pages. there is of course a trade-off between the timeout value and our ability to process all urls in real-time."
"the emergence of mooc technology provides new opportunities to support the learning process. however, their current form is limited to the passive transmission of knowledge, based mainly on video-based lectures combined with self-assessment questions. moreover, their application in healthcare education is still in early stages of investigation. this study demonstrated that extending moocs in order to support healthcare education can be achieved by integrating domain specific software."
"where, u x and u y are x-axis and y-axis components of stator voltage vector u s respectively; ψ s is the amplitude of stator flux; t e is electromagnetic torque; p is the number of pole pairs; l d and l q are the direct and quadrature stator inductances respectively. ψ f is the amplitude of permanent magnet flux; δ is the displacement angle between stator and rotor flux, i.e. the load angle; ω r is the rotor flux electrical angular velocity. the k and e are time-varying parameters, varying with ψ s, δ, and ω r . the derivative model of δ and ω r can be expressed as"
"the spatial location of the output voltage vectors corresponding to the 8 switching states in the α-β complex plane is shown in fig. 1(b) . where, u 1 -u 6 are active vectors, and u 0 and u 7 are zero vectors. the x-y rotating coordinate system is shown in fig. 1(b), the stator flux vector ψ s is aligned on the x-axis. the 6 active vectors u 1 -u 6 in the x-y rotating coordinate system can be expressed as"
"since parameters k and e are time-varying, the predictive tracking errors of torque and stator flux amplitude at (k+2)t s moment can be obtained as"
"nevertheless, educators also need to consider the pitfalls and challenges of this tool like the loss of privacy, bullying, harming contacts, and more [cit] . it is not possible to ignore these dangers. that is why teachers need to address these concerns through planning. when teachers plan appropriately and wisely, they can maximize the benefits and minimize the pitfalls of this tool in order to transform the classroom into a safe, rich, and interactive environment [cit] . additionally, it is necessary to train students on both the benefits and risks associated with social networking in order to successfully incorporate facebook into their academic lives because regardless of the pitfalls, social networking is still an essential component in the lives of youths [cit] . therefore, teachers have to make the effort to overcome the challenges and take advantage of the benefits that this technological tool provides. nevertheless, it is still unclear if there are other technological tools that provide the same benefits as facebook, but with less risk. continued investigation of online education may provide further insight into these matters."
"the medical and healthcare education community is actively investigating the potential for adapting moocs and other forms of online education to fit their own needs, predicting that online medical courses will be commonplace within the next five years [cit] . moocs may support undergraduate, graduate and continuing medical education (cme) by having the potential to address some of the current challenges of the healthcare education [cit] . they assert, however, that lecture-based courses form only part of the educational experience which should be provided. in particular, the technological infrastructure of moocs may foster learner communication and interaction to some degree, but not necessarily to the extent that healthcare education requires [cit] ."
"we performed additional validation of our method using known phishing sites. specifically, we identified 5 fraudulent sites found on phishtank.com [cit] and matched them with their legitimate counterparts. we deliberately chose sites that"
"based on the two-level voltage source inverter (2l-vsi) fed pmsm drive system, the proposed strategy is studied in this paper. in this section, the physical models of 2l-vsi and pmsm are introduced, and the prediction model of tracking errors of torque and stator flux amplitude is derived."
"in order to explore the viable ways of integrating a vp system into a mooc platform a literature review was conducted to identify the potential standards which were suitable for the integration. the collected results were examined to address the selected technical requirement. to construct the prototype a trial course was created in the openedx platform. open labyrinth was modified accordingly to conform to the selected e-learning standard. a vp case was imported into open labyrinth for the purpose of providing test educational content. finally, two test cases were performed to verify the implementation. the process followed in this study is visualized in fig. 1 ."
"on receiving the user's details by the blti.php file, the user-handler class first looks in the database to identify whether the user's entry already exists and if not, creates a new one to register the user. then, by using the log-in function, it allows access to the user and returns to the index class. the user-handler class matches the user's role acquired by the blti class to the corresponding one in open labyrinth in order to provide the appropriate user rights. moreover, it includes the function to encrypt the user's password that will be maintained in the open labyrinth's database."
"from the identified technical requirements we isolated and implemented just one for the prototyping. the approach to addressing this requirement was guided by a selected e-learning standard. the verification of the pilot implementation was based on two proposed test cases, the nature of which was determined by the aim of the study. in particular, the test cases were based on a pilot course in openedx and a single inserted vp case. hence, evaluating the user experience was not included in the objectives of this study. future studies may investigate more advanced tests while exploring the user experience."
"in tables 1 and 2 we present the test cases used to verify the pilot implementation. the selection of the test cases was informed by the aims of the implementation to provide a transparent authentication and access rights mechanism for the users with the appropriate credentials. figure 4 depicts the integrated open labyrinth in the openedx platform from the perspective of the instructor after the automatic authorization step. the instructor is provided with the user rights imposed by the corresponding administrator's role of open labyrinth. by that, the instructor may access, edit or delete the content in open labyrinth or author a new vp case."
"had not yet been taken down so that we would be able to run our analysis tools on them. table 2 shows the results of our page component parsing script on these 5 fraudulent sites and their legitimate counterparts. we matched 4 of 5 phishing pages to their legitimate counterparts using only the structural statistics we recorded. adding the comparison of the screen-shot captures allows us to identify all 5. other tools for used for phishing website identification report success rates in the range on average between 20% and 60% effective with only 1 tool in our literature review reaching nearly 90%. in addition the average false-positive rate for these tools is 42% [cit] . our experience with these five sites illustrates the importance of using both the easy to compute structural characteristics and the image comparison via hash value. four of the five fraudulent sites can be detected based on structural characteristics alone. due to the nature of how some sites are copied and used for phishing, we won't always have exact matches based purely on page markup characteristics. to illustrate this we look at the analysis of paypal and its cloned site in table 2 . in this case the fraudulent site was of very poor quality as set against phishing standards. the site did not replicate any of the functionality of the page it was spoofing other then the login form. as such, none of the other page characteristics matched up. yet, when we look at the output of our image capture as shown in figure 2, we can see that the page still pulls off the \"authentic\" look part of the site 1 ."
"3) since the stator and rotor flux rotate synchronously, the load angle δ is approximately constant. from (4) and (5), at steady state, the parameters k and e are approximately constant. at this time, it can be derived from (9) with a recursive method that the simplified multistep prediction model of the tracking errors of torque and stator flux amplitude in steady state is as follows 14) or is expressed recursively as follows"
"where, i α, i β, u α, u β, ψ α, and ψ β are the α-axis component and β-axis component of the stator current vector i s, stator voltage vector u s, and stator flux vector ψ s, respectively; r s is the stator resistance. generally, γ can be set within 0.1 to 0.5 [cit] . the torque can be calculated as follows the proposed ptc algorithm can be realized through the following procedure:"
"in particular, the logged in trial openedx course users, should be able to get authorized in open labyrinth and access the vp case through the platform, without requiring a repeated entry of their credentials."
" the remaining 60% of non-processed manually examined urls were simply unreachable in one way or another, some had unresolvable dns entries and some returned \"page not found\" errors. as we discuss later, we used a page load time of 3000 ms and suspect that some tuning of that value may be necessary. in future work, we plan to explore a wider range of time out values and investigate the cause of truly unresolvable urls in more detail."
"in this paper, a band-based multi-step predictive torque control (bm-ptc) strategy is proposed. this strategy can significantly reduce the computational burden of the multistep mpc algorithm, and reduce the switching frequency on the premise of ensuring the system has good dynamic and steady-state performance. the main research contents include: first, a simplified multi-step prediction model of tracking errors of torque and stator flux amplitude is derived, which can significantly reduce the computational complexity in the prediction stage. second, the band constraints on torque and stator flux amplitude are set with the limits of tracking errors. then, the fast optimization algorithm based on band constraints is proposed, the candidate switching sequences can be significantly reduced. meanwhile, the system can maintain good steady-state performance at low switching frequency. third, different control modes are designed in steady state and transient state respectively to avoid the influence of low switching frequency on dynamic performance. the symbols used in this paper are detailed in table 1 ."
"open labyrinth was set up on a virtual lamp server, launched through amazon elastic compute cloud (ec2) in order to prepare and finalize the adjustments required for the integration (amazon ec2). the advantage of this solution is that ec2 includes an auto-scale option which allows the instance to meet potential increased load."
"teachers can obtain many benefits from using facebook. for example, it can help teachers to engage students outside the classroom [cit] . through this tool, instructors can also learn about trends and issues in efl education as well as to obtain ideas for classroom practice. in fact, although facebook is widely known as a digital means of communication, it is also a means for gathering information since it presents powerful professional resources for efl educators [cit] . nevertheless, these are not the only benefits that facebook provides teachers."
"mooc platforms are currently at different levels of development but share common features. the content is delivered online by a set of tools forming the infrastructure of the course [cit] . they are decentralized, networked and based on cloud computing technologies [cit] . a central vle unit for moocs has diminished in importance and constitutes usually only one component in the mooc's network. it is used mainly for administrative purposes, such as students' registrations, and for hosting of discussion boards. the remaining parts are external tools where the students' activities concentrate. this includes in particular web applications, which are used to host lectures in forms of videos, support the learners' interaction, collaboration, evaluation and selfassessment [cit] . the course instructions consist of descriptions with links to tools and are distributed to the participants in the form of newsletters [cit] ."
it is worth noting that the values of torque and flux band size are related to motor parameters and control parameters. it is necessary to adjust the band size offline. and the method to adjust the band size by simulation analysis introduced in this paper is applicable to other motors.
"some other methods for detecting phishing websites that are related to our own website-feature-based and visual-cluebased. c. ludl, et. al. presented evaluation results of a number of anti-phishing tools and determined that the best relied on both blacklist sites that used manual reviewers and those that did some sort of analysis using specific page characteristics. we validate our choices of page characteristics in our own method against those stated in their work [cit] . we further validate our method by considering the effectiveness of methods such as the use of page color histogram comparison matching and color vector distance calculations which have been proven to be very effective at detecting matches between known phishing and legitimate sites [cit] . the main issue with both of these methods being the load that they put on a system which restricts them from scaling. comparisons done using these systems range from .02 to 11.2 seconds per page while using up to 512 mb of ram. our method is able to process each page on average within 4 seconds using less then 32mb of ram. additionally our design easily facilitates simultaneous processing of pages."
"for the sake of simplicity of the implementation, we extended manually open labyrinth's database, in the particular context of open labyrinth and openedx systems, to include the tool consumer's (openedx) id and credentials. other tool consumers may be added manually in the database to allow the integration of open labyrinth. however, this functionality could be automatized by enabling tool consumers to add appropriate credentials (key, secret) using a dedicated graphical user interface. this would require a careful design to ensure the security of the consumers' credentials and the users' information during the control process. moreover, open labyrinth should be modified in order to accept and store potential extra parameters transmitted by the launch messages, since the set of parameters may differ between the consumers."
"at steady state, since the candidate switching sequences that satisfy the band constraints must be able to limit the tracking errors of the torque and stator flux amplitude to a certain range, it can be considered to add the switch item into the cost function to reduce the switching frequency. even if the weight coefficient of the switch item is large, the steadystate performance will not be excessively deteriorated."
"1) detect the stator current i s, and estimate the torque t e and stator flux ψ s at kt s moment. 2) calculate the tracking errors e t and e ψ at kt s moment via (7), and calculate the parameters k and e via (4) and (5). 3) judge e t (k) and e ψ (k) whether meet the constraint (17) to select control mode. if meet (17), select steady state mode and skip to step 4, otherwise select transient state mode and skip to step 6. 4) predict the tracking errors e t (k + n) and e ψ (k + n) under different switching sequences u(k) via multistep prediction model (15), at the same time, select the candidate switching sequences that satisfy the band constraints (17) . this process is based on the nested loop algorithm as shown in fig. 3 . 5) choose the optimal switching sequence u opt (k), which minimizes the cost function (18), from the candidate switching sequences left after screening, and apply its first element s opt (k) to the inverter. this is the end of the steady state optimization. 6) predict the tracking errors e t (k + 1) and e ψ (k + 1) under different switching states s i via (9) . choose the optimal switching state s opt (k) which minimizes the cost function (22) and apply it to the inverter. this is the end of the transient state optimization."
"our work takes inspiration from this manual process of finding pages that \"look the same\". specifically, we automate some of what the human does to recognize duplication of an original page. to do this we analyze a web page based on some of its structural characteristics and based on the way it looks visually. specifically, we record a number of page markup characteristics including the page title text and number of links, images, forms, iframes and metatags. throughout the rest of this paper we will refer to this 5 tuple as a pages structural fingerprint. the image analysis portion of this work is at it's simplest is done by setting a fixed dimension and quality setting for rendering a page within a headless browser and then taking a page screen-shot. we compute a hash of the resulting image and compare the hash values using the hamming distance equation. this process takes on average 4 seconds a page including software buildup/tear-down time. to speed up the process further, we could prioritize image analysis for pages that with matches in the most easily parsed/computed items from the pages markup. figure 1 shows a high-level overview of the full process. the first step is the real-time gathering of url's from social media sites like twitter. we used a modification of the get_tweets.php script provided by the 140dev twitter tool [cit] to fetching the raw json version of each tweet and store it in a mysql table. next, we parse the tweets looking for url's. since twitter requires all url's to begin with the standard \"http://\" to be hot-linked it was as simple as parsing the json data for that expression. we fetch the page content for each url using php's loadhtml and dom object walk through functions to visit the site, load the html into memory and finally count up each of the specific dom objects. each time a page has been parsed it is then logged in the master url_stats.csv file for later analysis. for each new url, we also use xvfb and cutycapt to render the resulting page and capture a screenshot. finally, we compute a variety of hash values on the resulting image."
"a significant barrier that medical faculties often encounter in integrating vps in their curriculum is the timely, costly and complex process of producing and authoring vps. vp systems have been extended in the past in order to support content transfer and by that to enable the technical sharing of the vp cases among institutions. that was achieved by applying the medbiquitous virtual patient standard (mvp) ."
"besides their technical innovation, xmoocs are based on the theoretical presentation of the learning context, supplemented by interactive tasks and discussion boards activities: \"cmoocs focus on knowledge creation and generation whereas xmoocs focus on knowledge duplication\" [cit] . xmoocs' learning objectives are predefined by the courses' instructors, while the participants' communication is limited [cit] . for this reason, xmoocs are criticized for replicating traditional pedagogical models."
"similar to the hysteresis bands in direct torque control strategy, the upper and lower bounds are set for the torque and stator flux amplitude respectively as follows in this paper, the band constraints are set for three purposes: first, reduce candidate switching sequences in multi-step prediction, so as to reduce computational complexity; second, ensure that the system has good steady-state performance at low switching frequency; third, judge the state of the system to select control mode."
"evidently, one of the main advantages of facebook is that it allows people to share information, knowledge, and resources [cit] . in consequence, it can be used to import and share class blogs, post words and definitions for vocabulary review, share resources and materials for class projects, share ideas about class discussions in english, or post students' journals and share them with the class via a classroom page or group [cit] . facebook can also be used as a space for collaboration and discussion [cit] . for example, it is possible to involve students in writing workshops with peer review and instructor oversight, encourage students to communicate through a facebook discussion wall during class time, connect the classroom with speakers around the world in order to improve their foreign language skills, bring quiet students out of their shell by asking them to participate in facebook discussions, create study groups to easily connect with each other within their own facebook groups, track down old students or professionals that could come to the classroom as guest speakers, connect with classes all around the world, and discuss classroom ideas with other teachers on facebook. all these ideas can contribute to improve students' english language skills."
now we can look at the analysis of our known fake/non-fake ebay pages as outlined in table 4 . ebay is a great example for this work since it at the moment it is one of the top spoofed sites. [cit] table 3 shows significant correlation of both the structural site characteristics and image hashes between the site characteristics between the fake pages pointed to in our twitter data set and the legitimate ebay site. table 4 shows the page screen-shots as well as phash values of the real and fake ebay login pages. notice that the hashes are identical for the legitimate and fraudulent sites.
"permanent magnet synchronous motor (pmsm) has the advantages of simple structure, high power density, high reliability and so on. it has been widely applied in elevator driving, electric vehicles, rail transit and other fields [cit] . to improve motor control performance, many advanced control strategies are proposed, such as the neural network control (nnc) [cit], the internal model control (imc) [cit], the state feedback control (sfc) [cit] and so on. in recent years, model predictive control (mpc) has been widely studied and applied in the control of pmsm drives due to its advantages of intuitive concept, flexible design and fast dynamic response [cit] ."
"the algorithm implementation of the proposed strategy requires offline tuning of some parameters, such as the band size and various weight coefficients. the tuning method of the weight coefficient used to balance the torque and stator flux is very mature in the conventional ptc (c-ptc) strategy [cit] . therefore, the tuning method of the band size and the weight coefficient of switch term is mainly analyzed by simulation in this section. the pmsm used in the simulation is the same as that used in the subsequent experiments. the relevant parameters are shown in table 2 ."
"we designed our research as a feasibility study to verify the idea of integrating virtual patient systems and mooc platforms. this includes building a \"proof-of-concept\" prototype to evaluate if the system design can be implemented and will provide reasonable output [cit] . in this section we describe preparations for an implementation and our approach to verifying the resulting prototype."
"this paper has discussed the groundwork for a method and tool that can detect phishing sites. we have promising initial results comparing known phishing sites and their legitimate counterparts. we were able to find a number of phishing sites from a live dataset and we also gained critical insight into how to effectively and efficiently gather, format, and analyze, this social data."
"as this work has matured, we have continued building on our general social media analytic collection and analysis process. to date, we have improved our collection process and storage method to the point where we are consuming and storing upwards of 70 million messages a day. in our current work includes the creation of routines that automatically find correlations between potential phishing pages and known trusted sites. specifically, we use cluster analysis to identify clusters of pages with similar characteristics."
"in order to provide educational content in the course for test purposes we manually imported a vp case from the evip project repository. the selected case refers to bronchogenic carcinoma which is an important topic in medical education, since it is the most common cause of cancer-related deaths worldwide [cit] ."
"in transient processes, it usually takes a long time for stator flux vector to return to the rectangular region shown in fig. 2 . therefore, band constraints cannot be used to select the candidate switching sequences, resulting in difficulty in realizing multi-step prediction. in addition, the dynamic response capability must be sacrificed if reducing switching frequency is considered. therefore, at transient state, the control objective is to quickly reduce the tracking errors, and the single-step prediction is adopted. the cost function can be designed as"
"the total size of captured and stored data from the twitter api is even larger than the raw feed after the json stream is decoded. we tried a number of popular database systems like mysql, hadoop and cassandra. with each system tried, we ran into substantial problems in achieving fast access to million+ entry tables. in this prototype, we used mysql and divided logical database tables into roughly 9 hour increments to allow for smaller table sizes with a limited number of entries in each. after decoding and importing the 81.9 gb of json data into mysql, this data was slimmed down to 27.3 gb of pure content, which shows how much overhead is included in each tweet. after parsing the mysql database and creating a new table of just url's, we ended up with 193.5 mb of usable links."
"teachers who follow efl educational pages through facebook obtain several benefits. for instance, they can get effective classroom resources for free, develop and maintain friendships and collegial or professional relationships for mutual benefit, and receive valuable educational information like notifications of journal publications, tips for teachers, and new books and e-books available for educators [cit] . therefore, facebook provides many advantages for teachers all around the world, but they are not the only ones who obtain benefits from this platform."
"in the evolving process of online education, virtual learning environments (vles) have undergone considerable changes [cit] . currently vles are being prepared to be used at large scale in massive open online courses (moocs). moocs are a new form of learning activities on the internet providing free access to elite universities' courses for an unlimited number of participants [cit] ."
"particular, demographic changes and the growing population demand an increase in trainees' required competencies and call for better training of higher order skills [cit] . the learning opportunities provided to medical students for observing the treatment process in hospitals are diminished . at the same time, the limited access to public medical education, the technological innovations entering the field of healthcare [cit] and the rapid expansion of new medical knowledge generated by clinical research [cit] highlight the need for massive and continuing healthcare education."
"besides the technical sharing of the vps, the cases require meeting ethnic, language and socio-economic aspects of the institutions in which are used [cit] . the process of adapting the vp cases to meet these requirements is known as \"repurposing\". the electronic virtual patient project (evip) [cit] ) ."
"in the section ii and section iii, the establishment of tracking error prediction model for torque and stator flux amplitude and the application of band constraints are introduced respectively. on this basis, a band-based multi-step predictive torque control (bm-ptc) strategy is proposed in this section."
"many educational organizations like the international reading association (ira), reading rockets (rr), and the national education association (nea) create and publish pages on facebook to promote their organizations and share useful resources for educators [cit] . these pages usually include information about current trends and issues in efl education, classroom resources, opportunities for professional development, and much more. moreover, they give teachers the opportunity to collaborate with other efl educators. following educational organizations through facebook is therefore \"a way to access valuable information and resources at no expense and through an online medium that is likely being used anyway\" [cit], p.28) ."
"combined with the multi-step prediction model (15), the tracking errors of torque and stator flux amplitude under different switching sequences can be easily calculated iteratively. considering that the candidate switching sequences need to satisfy the band constraints (17), the optimization problem at steady state is as follows"
"we captured 81.9 gb of raw json data representing roughly 4 days of twitter traffic including a total of 19,624,335 tweets. table 3 shows the details of the dataset. in our initial experiments, we were able to capture 2,813,476 unique url's. however, in practice we were only able to process 1,829,531 via our page characteristic technique. this was due to a large number of url's being un-resolvable by the time we processed them. it is difficult to know in all cases why a url was unresolvable, but we can identify a number of factors besides phishing sites that have been taken down by attackers. in section 3.2, we elaborate on these other reasons urls were unresolvable including typos and unavailable url shortening services. at this time we have not had the opportunity to sufficiently analyze the captured url dataset from twitter. in the future we are interested in measuring the frequency at which legitimate url's versus phishing url's are posted, and how many urls come from individual users and measuring the number of intentional versus accidental posting of phishing urls. in the following sections, we elaborate on some of the challenges we faced in processing and characterizing links found in twitter data."
"next, by using the oauth signing mechanism the signature is re-computed and compared with the one received from the lti launch request to verify the credentials of the sender. the set of values received are additionally checked for their appropriateness according to the protocol. if the values are not appropriate the blti class will reject the connection. the connection and queries to the database are managed through the homonymous file. if the signatures' comparison is successful, the user-handler class is called to manage the user."
1the page title field of the fraudulent ebay page is formatted differently then the legitimate ebay sign-in page. this is easily overcome by removing the special html encoding characters.
"the optimization problem (21) can be solved by the nested loop algorithm as shown in fig. 3 . only the switching sequences satisfying the band constraints can enter the smaller cycle from the larger cycle, rather than completing all the cycles and substituting into the cost function for every switching sequence, so the calculation burden is limited."
"facebook provides many advantages for students. for instance, it promotes human interaction and social interchange between participants [cit] ). therefore, this network has positive effects on students since it makes english language teaching and learning more practical, interactive, and holistic. [cit] conducted a study with several university students from france and germany. during the research, they observed that using facebook to promote cooperative learning in efl classrooms increased students' learning."
"after verification of techniques we moved on to the larger \"wild\" data set of unknown urls. specifically, we look for urls in twitter posts. previous studies have shown that 12% of all url's posted on twitter may be malicious. [cit] a real-time source of candidate phishing sites is increasingly important as attackers shorten the time a given phishing site is active before the content is moved onto a new compromised host. we also chose twitter for the collection of urls because it allowed for real-time interaction. our work aims to shorten that life-cycle by providing near real-time feedback and thus needs a way to bring url's in that are relevant and live at the time of processing. traditional crawlers are not up to the task and we do not have the resources of companies like google at hand. with their real-time api, twitter offers one of the best means of potentially malicious near-real-time urls."
"at steady state, the proposed strategy can realize multistep prediction by using the simplified multi-step prediction model (15) of tracking errors and the method of reducing switching sequences via the band constraints (17) . at this time, as shown in the analysis in section iii, the cost function can accommodate both the tracking error term and the switch term, taking into account the requirements of low switching frequency and good steady-state performance simultaneously. it is worth noting that if the cost function only contains the switch item, the normal operation of the motor can still be maintained and the switching frequency can be minimized due to the existence of band constraints. however, in order to make the proposed strategy more general and suitable for high performance occasions, this paper chooses to retain the tracking error item. therefore, the cost function of the proposed strategy at steady state can be designed as"
"based on the information presented before, it is possible to conclude that facebook can help teachers to blend online instruction with conventional face-to-face teaching successfully [cit] . the advancement of the internet has created a borderless world. thus, teaching and learning are no longer confined to the four walls of a classroom [cit] ). therefore, efl teachers need to be creative and up-to-date with the current technology in order to keep up with the fast-paced society. thus, with careful planning, efl teachers can utilize facebook as a powerful instructional tool to engage students in meaningful language-based activities and to enhance the development of the communicative competence in language learners [cit] ). actually, if planned appropriately, facebook can even facilitate the development of an online community of english language learners where students can practice their language skills through videoconferences and chats with english native speakers [cit] ."
"in addition, it should be noted that the band size of bm-ptc is small in order to reduce the number of the candidate switching sequences at steady state. therefore, it is not guaranteed that the number of candidate sequences will not be 0 in the multi-step optimization. in this paper, a safeguard measure is set in the algorithm in experiment. when the number of candidate sequences is 0, it will automatically jump to the transient-state control mode."
"the tracking errors of stator flux amplitude and torque are defined as (7) where, e t and e ψ are the tracking errors of torque and stator flux amplitude, respectively; ψ ref s and t ref e are the reference values of stator flux amplitude and torque, respectively."
"another challenge that teachers face is that the preferred language used by most students to interact on facebook is \"spanglish\" (the combination of spanish and english), or any other combination of languages according to each country. very few students actually use standard english to interact with each other. thus, the quality of english used in the online interactions is very poor [cit] . that is why, in order to control this situation, teachers have to recommend and remind students that they have to revise their work before posting it on facebook. it is also useful to require students to check their choice of words, spelling and sentence structures during their interactions on facebook [cit] ."
"our implementation focused on the sso mechanism connecting the two selected systems: openedx and open labyrinth. for the purpose of prototyping, we created a sample course in openedx. the course was not publicly released and was used only for the purpose of this study."
"anti-phishing research has been done for a number of years and falls into a number of categories. a.p.e rosiello, et. al. define these categories as email-based, blacklist-based, visual-clue-based, website-feature-based, and informationflow-based. the method employed in their work could be best categorized as website feature based and consists of a string search technique which identifies the dom objects within a page and builds a tree of those objects. this tree is directly compared to those of known legitimate pages. when a page claims to be a specific site, all dom trees are compared with a known good sample to look for dissimilarities [cit] . this approach works as a client side browser plugin and thus is useful on an individual user basis but is not a conducive method for broader analysis."
"the system that we have build currently allows for exact match searching, that is, results that have an exact characteristic match and/or exact distance score. in addition we have the ability to search for other combinations of characteristic measurements and specific distance scores. this allows us to search on less precise matches. we used this to search for exact matches of the top 5 most phished sites and table 5 lists the number of exact matches for each. this could also be used by the administrator of any web site to search for matches to their site. table 6 shows, when comparing the structural characteristics of two distinctly different pages we can not distinguish any difference. however when adding in the phash value of the pages and using them to calculate hamming distance we can see that indeed the pages do not match."
"1) the rotor flux electrical angular velocity ω r is approximately constant. 2) since the ripple of stator flux amplitude is very small, the variation of parameters k and e caused by the change of ψ s can be ignored."
"although most teachers use facebook for trivial activities like sending information to students who were absent or posting class announcements, this social network can be a practical tool for many other educational activities. for example, facebook allows efl teachers to start online discussions, post articles to develop a reading comprehension exercise, start online chats between students and english native speakers, and many other activities [cit] an article with one hundred ideas on how to use facebook for educational purposes. for instance, it mentioned that facebook can be used to attend remote lectures and presentations from all around the world, play educational games like puzzles and crosswords, and participate in challenges posted by educational outlets. additionally, this platform provides teachers effective tools to manage class projects and assignments [cit] . for example, it can be used to write reviews and reports of books assigned in class, follow journalists on social networks in order to gather past and current news clips relevant to classroom discussions in english, or host a regular english book club with the students."
"whilst the features of moocs offer the potential to enhance the learning process by promoting interactivity and self-directed learning, their contemporary form is limited to passive transmission of knowledge, based on the presentation of videos. moreover, from the technical and pedagogical perspective their application in the healthcare education is still in the early stages of investigation [cit] ."
the following pictures depict the learner's perspective while accessing the content by getting authorized in open labyrinth (fig. 5) and trying the vp case (fig. 6) .
"the educational possibilities of extending moocs with vps, with the goal of fostering medical competencies such as clinical reasoning, were discussed by us theoretically in the past [cit] . in particular, we proposed three educational scenarios taking advantage of vp features, augmented by the distributed knowledge base provided by participants and the mass customization features of moocs. however, the technical feasibility for implementing the suggested use cases has not yet been examined."
"by emulating the role of the healthcare professional, the learner is provided with training opportunities to identify relevant information from a set of anonymous patient-related data, conduct physical exams, laboratory tests and make diagnostic and therapeutic decisions [cit] without any real world repercussions. vps are reported to be a response to some of the current challenges in medical education [cit] ) such as the limited learning opportunities for observing the treatment process. moreover, vps \"fill gaps in clerkships by exposing students to diseases that they would not otherwise experience because of short clinical rotations and limited ambulatory care experiences\" [cit] ."
"the implementation required modifying open labyrinth in order to function as a tool provider integrated in the openedx platform. the openedx user, by selecting the open labyrinth link to access the content, issues a blti launch request, where a http post message transmits a set of data elements required to authorize the user. this is imposed both by the oauth standard and the lti specification. in order to implement the blti interface we programmed the elementary framework classes of basic lti (files blti.php and oauth.php) as indicated by the ims-lti specification. we also created two new files named user-handler.php and database.php and we modified the index.php page of open labyrinth. in the database of open labyrinth we added a new entry to maintain the credentials (key, secret)."
"the massiveness of the mooc audience and the diversity in their background implies challenges regarding the development and integration of open services, the scalability in the infrastructure and the automation of tasks. at the same time, the integration of open services may support the educational process and increase the interactivity of the learning environment [cit] ."
"where, t s is the sampling period; containing (k) represents the value of the variable at the kt s moment; containing superscript p represents the predictive value of the variable. by default, the reference values of torque and stator flux amplitude remain constant in the prediction horizon. from (7) and (8), the predictive tracking errors of torque and stator flux amplitude at (k + 1)t s moment can be obtained as"
"then we modified the landing page of open labyrinth to intercept the data that are passed on by a blti launch request. the code in index.php file receives the data and transmits them to the blti and oauth classes in order to be verified. the blti class firstly confirms that a minimum set of values to meet the protocol requirements has been received and then, using the obtained key looks up the corresponding value of the secret in the database."
"we are in the process of completing the missing pieces of overall analysis architecture to enable full scale, real-time analysis of the twitter data feed. specifically, we are completing the automated characteristic comparison routines as well as the alerting functionality as illustrated in figure 1 . in addition we plan to implement a true multi-threaded design for simultaneous processing of pages which will replace our current parallel page processing instance launching code."
"the integration demonstrated in the current study was based on the example of a single vp system and a mooc platform: even though there are no reasons to suspect that the selected platforms were non-representative, future studies may investigate the integration strategies in a wider perspective including different vp and mooc systems as well as achieving a tighter integration of the system by addressing the remaining identified requirements."
" the next largest contributor was expired or non-working shortening services. a great example of this was goo.gl which is fairly new at the time of this research. the goo.gl shortening service is a google labs program that has been plagued by outages. at the time of the 1000 url manual analysis, failed expand url shortening caused roughly 20% of all the non-processed pages."
"the open labyrinth virtual patient system is a project developed and maintained by an international consortium of universities. open labyrinth is a web application for authoring and displaying vp cases. it is currently the most advanced, freely available, open-source virtual patient system. [cit] ) it may be regarded as representative for the class of virtual patient systems."
we verified the technical implementation by performing test cases. the test cases were designed in order to evaluate the system's response to different input requests. the following distinct test cases were developed to test the transparent authentication mechanism:
"is the park transformation matrix; u x,i and u y,i are the x-axis component and y-axis component of u i, respectively; u dc is the dc-bus voltage; θ s is the stator flux angle."
"in the transient processes such as load abruptly changing, the rectangular region may move along the y-axis sharply, causing the stator flux vector to jump out of the rectangular region. therefore, the voltage vector which can reduce the tracking errors must be selected first so that the stator flux vector can return to the rectangular region as soon as possible. at this time, whether the tracking errors of torque and stator flux amplitude satisfy the constraints (17) can be used to judge the state of the system, and then an appropriate control mode can be selected."
"the block diagram of the proposed ptc strategy is shown in fig. 4 . in this strategy, the stator flux vector ψ s can be observed by the stator flux estimator with a low-pass filter [cit], as shown in fig. 5 . where, ω c is the cutoff frequency of the low-pass filter, which can be calculated as follows:"
"from table 3, we note that the number of urls that were processed for page characteristics was inconsistent with the number of unique urls gathered. in this section, we discuss some of the reasons for this difference:"
"on the other hand, there are also concerns that social networking \"increases the likelihood of new risks to the self, such as the loss of privacy, bullying, harming contacts and more\" [cit], uninformed students and teachers can put themselves at risk by sharing the most innocent piece of information. once information is released into cyberspace, it becomes a part of a global network. persistence and search ability of content, replication and manipulation of content create a framework in which underage children are at risk [cit] . online bullying has gone global as well. that is why some schools prohibit the use of facebook and internet on school. in this case the solution is professional development. professional development is essential to training classroom teachers and students on both the benefits and risks associated with social networking [cit] . furthermore, collaboration and inclusion of all stakeholders is vital in order to develop appropriate activities and lessons, especially when facebook will be used as an educational tool [cit] ."
"students also have positive views and opinions regarding the use of facebook as an educational tool to facilitate english language learning. for example, learners consider that facebook provides opportunity for authentic interaction and communication that they have not experienced before. they also think that it increases confidence in language acquisition and sense of connectedness among themselves ( that students enjoy using facebook because it give them the opportunity to practice english with native speakers in a more natural and friendly environment. [cit] says that, when students have to talk to english native speakers, they feel forced to improve their language skills because they need to communicate with them in english. therefore, their language abilities increase simply because they write, read, listen, and speak in english more. [cit] points out that students can learn new vocabulary words through friends in facebook because when their friends post comments using words that they don't understand, they feel motivated to refer to a dictionary and look up the meaning of those words. similarly, those students who play games on facebook feel forced to improve their english skills because those games require students to understand instructions in english and to interact with fellow gamers in english. all this demonstrates that people learn better in social settings like facebook because they expose learners to authentic and relevant social interactions. however, although facebook provides many benefits for students and teachers, it also presents several challenges and dangers for those who do not use it wisely and responsibly. thus, the next section will discuss several challenges implied in the use of facebook."
"finally, it is also important to consider the learning preferences of students. those learners who have a more rigid, stronger, traditional view of teaching and learning do not consider that facebook can be an effective online environment to learn. for example, according to some students, to grasp and enhance the english language, it is necessary to have a more academic and formal structure [cit] . that is why teachers need to be balanced and try to adapt the classes to the needs and preferences of all students. they also have to be careful not to overuse facebook or consider that this tool will replace face-to-face instruction and practice."
"the edx initiative was launched by massachusetts institute of technology (mit) and harvard and offers not-for-profit online and in the classroom education. the edx platform is hosting moocs of global partner institutions and organizations. the open-source release of the edx platform is named openedx. the selected mooc platform is comparable with other available ones such as coursera or udacity, therefore the choice of platform does not influence the generalizability of the study. [cit] ."
"to date the most widely deployed approaches for protecting users from phishing attacks are blacklist-based. blacklistbased tools that enable them to be warned away from sites that are known to be malicious but have yet to be taken down. these tools rely on known blacklisting services such as phishtank.com that rely on the submission of suspicious urls for analysis. after a suspicious url is submitted, a variety of techniques are employed to triage the link including suspicious dns reputation, suspicious url format, url containment of other domain names in a directory field, and actual matching of known url terms using lexical analysis [cit] . these methods reduce but do not eliminate the manual work required. most have high false positive detection rates and all methods get the url's that they are analyzing from other services which feed them primarily malicious or at least suspicious url's to analyze. they have not been shown to be effective at finding malicious urls in a a large live data set of predominately legitimate urls."
"secondly, expected results could not be obtained with the ultrasonic system developed to determine the fruit quality using the correlation between ultrasound parameters and the properties of the fruit. according to our results, tt mode ultrasonic measurement is not possible, although it is mentioned in the literature that the quality of fruits can be determined with the same method. erefore, both pe mode ultrasonic system and machine vision system can be more efficient tool in determining the fruit quality. erefore, this study may lead to more intensive study and development of pe mode applications of ultrasonic systems in order to determine the fruit quality."
"the following subsection discusses the feasibility of using rdf literals to embed geometry descriptions of construction elements, according to the practical requirements and the relevant w3c specifications. each paragraph discusses if and how each requirement can be fulfilled."
"the fog ontology is thus populated with a taxonomy of datatype properties that refer to existing geometry formats. fog extends omg, as the root datatype property is omg:hassimplegeometrydescription. the taxonomy then defines fog subproperties per geometry format (e.g. fog:asdwg), which splits per version of the geometry format (e.g. fog:asdwg_v2018). some geometry formats can also consist of different (related or unrelated) files, so an additional set of subproperties is defined in those cases (e.g. fog:asgltf_v2.0-gltf and fog:asobj_v2.0-glb)."
"if a building object has multiple geometry descriptions of which at least one has associated files, an additional intermediate node between the building component and each geometry description is necessary. the omg ontology provides the necessary terminology to do so and can be used together with fog as shown in figure 5 for a level 2 geometry pattern (one intermediate node). the content of associated files and their original file names have to be stored in an unambiguous manner along the main geometry description. each omg:geometry instance node is connected via the fog:hasreferencedcontent property to a corresponding fog:referencedcontent instance node, if applicable. this node then connects to the file name via an rdfs:label and the actual content of the file via the applicable fog datatype property as defined in dq2. when geometry descriptions are not embedded in the graph, but links to external geometry files are defined instead, the correct referenced file name of associated files can be available in the reference string itself. an advantage of using omg level 2 is the option to add metadata related to individual geometry descriptions as demonstrated in figure 6 . terminology to define metadata has to be provided by an existing or a new ontology, but is covered by neither fog nor omg."
"since continuous smart city data is often based on seasonal patterns and appears as noisy time-based datasets the ability to separate trend, seasonality and irregular components allows an interpretation of the current situation. for example, when there are no cars measured on a small street at night, it does not typically mean that there is a traffic jam. therefore the composite monitoring applies a decomposition of the time series into additive seasonal, and irregular components using moving averages. the trend of the time series is a useful indicator for the needed length of the analysed data stream period but is not utilised in the for an incident evaluation."
"the irregular component at time t, that describes random, irregular influences, which counter regular patterns and therefore have the most significance for current regular and irregular sensor situations. it represents the residuals or remainder of the time series after the other components have been removed."
"fog does not define rdfs:range restrictions on the datatype properties as the same property can be used to link to rdf literals with different datatypes depending on the situation. for example the property fog:asgltf_v2.0-glb can be used to link to a literal with a xsd:base64binary, xsd:hexbinary, xsd:anyuri or even a custom datatype for other binary-totext encodings. besides the fog datatype properties, used to link to geometry descriptions stored or referenced in rdf literals, fog also defines specific object properties (e.g. fog:asgeomontology) as subproperties of omg:hascomplexgeometrydescription to link to rdfbased geometry descriptions that use dedicated ontologies such as geom, ontobrep and ontostep."
"for a basic evaluation of the proposed approach the datasets, described in table i, have been used. open data aarhus (odaa) 1 is an open data portal for the aarhus municipality and allows access to various datasets, e.g., traffic and cultural event data. the traffic data consists of 449 deployed traffic sensors in the city, which report information about the vehicle count and average speed on the defined sensor location. only a subset of driving cars is measured since these traffic sensors only measure activated wifi/bluetooth communication devices. during the citypulse project [cit] this data has been collected for more than two years and is available as a semantically annotated dataset 2 . the second used data source tomtom 3 allows access to a set of developer apis that deliver traffic information. the traffic flow api provides information about the current average vehicle speed and the typical vehicle speed stationary traffic, 4 closed road. this traffic information is generated from movement of mobile devices (navigation apps, in-dash navigation, phones [cit] ). both data sources are measuring just a subset of the cars that are driving on the road of interest. therefore, they are not able to deliver distinct and precise information of all vehicles in the city. this paper shows that a cross-evaluation of both datasets, which are generated out of two independent sensor sets, is a useful measure to determine their reliability."
"this paper discusses measures for ensuring and increasing the reliability of smart city applications by employing a monitoring approach for correlating data streams. during run-time, the monitoring evaluates the plausibility of sensor observations. composite monitoring, can be triggered by detected incidents or suspicious sensor readings in data streams. it provides a correlation dependency model-based approach integrating new evaluation schemes. this approach supports appropriate spatiotemporal distance measures to achieve an efficient monitoring of relevant correlating data. in conclusion, the suggested framework provides methods to cope plausibility analysis for heterogeneous data sources in smart city applications, and in addition considers that the sensors become unreliable over time by approaching a continuous time series evaluation. as a countermeasure, several actions to identify and react on varying information qualities have been investigated and integrated into the citypulse framework. in the future, we plan to investigate an open platform for a drag and drop evaluation approach, which allows to apply the concept to different domains and allows easy utilisation of city infrastructure knowledge."
"the building topology ontology (bot) is developed within the w3c linked building data (lbd) group as a central ontology that can be extended with other modular ontologies [cit] . the current version of bot (v0.3.0) (w3c [cit] ) allows to connect any bot:element or bot:zone instance directly to a geometry description stored in an rdf literal using the generic datatype property bot:hassimple3dmodel. the bot ontology does not define format specific datatypes or datatype properties for geometry descriptions stored in rdf literals. besides the above datatype property, bot also contains the object property bot:has3dmodel to link building elements or zones to rdf-based geometry descriptions or an external geometry files. the property names and definitions suggests that they should only be used to connect to 3d geometry descriptions, thus ignoring 2d geometry."
"). in this case, quality determination of the whole fruit by the ultrasound method in the tt mode is not very logical. although ultrasound is a safe tool for the measurement of the fruit in the tt mode, the experimental results are problematic in terms of reliability. in this study, the machine vision system developed in our previous study was used, and successful results were obtained in determining the size of the fruit [cit] . is system is practical and easy to use in determining the physical properties (surface area/volume) of fruits even though it is not sufficient to determine other quality characteristics such as firmness."
"an initial method could be the definition of custom datatypes in fog (see figure 2), similarly as has been done in strdf / stsparql and geosparql. however, as stated in the w3c specifications, an rdf literal has exactly one datatype and in dq1 it was already proposed to use the datatype to store other information. additionally, it is not possible to create a hierarchy of custom datatypes that can be used by reasoners, as there is no such thing as a 'subdatatype'."
"e tt mode ultrasonic system has been developed for fruit quality determination. in addition to the ultrasonic system, image processing based on the machine vision system was used to determine the quality of the fruit."
"the content of binary geometry files can be stored in rdf literals by encoding them in unicode characters. the base64 encoding is recommended, as its usage is widespread and encoders / decoders are available in almost every programming language. with this methodology the content of any file (text / binary and open / closed formats) can be embedded in an rdf graph. in the case of text encoded geometry files, text lines are important and the text can contain quotes. this means that each new line and quotation sign should be escaped correctly, depending on the rdf serialisation. with the omg terminology, both two or three dimensional geometry descriptions can be included."
"two sample abox datasets are published in the github repository of the fog ontology: the first only contains the graph structure and dummy content for the rdf literals, while the second contains the same graph structure with actual geometry descriptions. the first dataset is used in the publicly available sparql-visualizer demo8 to demonstrate how it can be queried, while the second dataset is used by the demo web application. see figure 8 for an overview of the graph structure of the first dataset. figure 9 shows the visualised geometry and the geometry descriptions that can be downloaded in the web application. geometry descriptions using geometry schemas such as step, the rdf-based geom ontology and the e57 format, cannot be visualised by three.js as there is no loader available."
"nondestructive ultrasonic method itself is not enough to determine the fruit quality using internal parameters. it is essential to use a proper method to obtain the physical characteristics of fruits in terms of size, shape, and volume as well. nondestructive custom-designed ultrasonic system combined with a noncontact physical measurement unit can provide superior quality assessment of fruits."
"journal of food qualitytested by contacting each other, and the received signal was observed on the oscilloscope. however, when the measurements were repeated on the fruits, the same result was not observed as shown in figure 6 . e attenuated signal cannot be measured in either case. in the literature [21, [cit], the most commonly used frequency value is 50 khz for tt mode measurements. however, even 55 khz frequency used in this study did not provide similar results mentioned in the literature. although the signals produced were also applied with horns, there was no change in the measured results. if very thin slice of fruit is used instead of whole fruit, the results in the literature may be obtained. because, as it is known, the absorbed distance varies exponentially (a � a o e −αx ) and consequently the attenuated signal amplitude (a) is inversely proportional to absorption (e"
"e receiving probe was connected to the oscilloscope (model: [cit] a 100 mhz), and the received signal was attenuated around 10 db (model: keysight 8496a 110 db) to make more clear signal visibility on the oscilloscope."
developed ultrasonic system performance was tested before measurements. experimental setup as shown in figure 1 was used to confirm system reliability and accuracy. two ultrasonic probes with 1 mhz were used for transmission and reception of ultrasonic signals. one of the probes was transmitter and the other was receiver.
previously reported results of our image vision system were used to support this study and are available at doi: 10.1109/ebbt.2018.8391460. is prior study was cited at relevant places within the text as reference [cit] . figures used to support the findings of the ultrasonic nondestructive measurement in this study are included within the article.
"the coins project [cit] provides a methodology and vocabulary to annotate and link any group of construction related files using linked data technology. the result is called a coins container, consisting of a number of files and an rdf annotation of these files and referenced files not included in the container. in the coins approach, the separate files are linked to the rdf annotation graph using rdf literals containing the location of the file combined with the fixed datatype xsd:anyuri. this literal is connected with a generic property to an intermediate cbim:uriproperty instance node, which is again connected to a cbim:documentreference instance node. in a similar way, users can link any string literal to a cbim:stringproperty instance node that is connected to the cbim:documentreference instance node, to add information about the document type and the document media type (mime type) of the annotated file. this method allows users to define custom text descriptions at will, making it hard for software developers to query for specific files as several slightly different descriptions of the same file format might exist. major parts of the coins project are incorporated in the information container for data drop (icdd) which is now under review for standardisation as iso 21597."
"intensive researches have been conducted to design and build computer-aided machine vision system for a variety of fruit samples, and promising results of machine vision system for fruit quality have been shown [cit] . for this purpose, image processing techniques, which are intelligent, feasible, cost-effective, and easy-to-use machine vision system, are mostly used for sorting and grading of fruits based on size, volume, and shape. our computer-aided machine vision system has basically three steps for fruit sorting and grading: (1) firstly, images of fruits from different angles are captured by high-resolution cameras; (2) secondly, required features are extracted from the captured images by image processing techniques, and (3) finally, volume is estimated using extracted features."
"both text and encoded binary geometry descriptions can be embedded in an rdf literal, while a literal can also store a reference to an external geometry file. first, the content of a literal can be a url referring to an external geometry file that can be downloaded from a weblocation or is stored locally. a parser that recognises this, will know that it should look for an external file. secondly, if a parser can recognise that the geometry description is embedded in an rdf literal and is text-based, it can be configured to unescape newline and quotation signs. finally, if an encoded binary geometry description is embedded in an rdf literal, the type of encoding should be known -ideally without having to access the lexical form of the literal -so that the right decoder can be called by a parser. the type of encoding (e.g. base64, base122, hexadecimal, etc.) is unrelated to the binary geometry format used. software applications can be prepared to deal with the above situations, as long as this information can be retrieved for each geometry description. as a result, stakeholders do not need to manually interpret this kind of information every time geometry is exchanged."
"exchanged building geometry can be 2d or 3d, it can be a point cloud of an existing construction recorded by a laser scanner, a detailed geometry of a building product or a conceptual design of a new construction. looking at current practice in the construction industry, it can be concluded that a wide variety of software and related geometry formats are used in different projects, during different lifecycle phases and by different actors [cit] . the following minimal practical requirements for the use and exchange of geometry descriptions using rdf literals are derived from this everyday reality:"
"since the available odaa data, which only measures a subset of bypassing cars every 5 minutes, shows a high noise between different consequent measurements a smoothing filter function was applied to get data that is more comparable. the discrete cosine transform (dct) expresses a finite sequence of data points as a sum of cosine waves with different frequencies and amplitudes. the implemented discrete cosine transform filter depicted in figure 5 uses the dct to convert a signal (irregular) to an ordered sequence of frequencies and associated amplitudes (dct-a). in the frequency domain, the sequence is multiplied with a low pass (dct-b) to remove high frequencies (dct-c). the inverse dct transforms the low pass filtered sequence back to the time domain resulting in a smoothened signal (irregular-dct)."
"relations between different versions of existing geometry schemas can be established via subproperties or subclasses. however, as established earlier, it is not possible to create subdatatypes. the advantage of subproperties and subclasses is that this information can be used during a reasoning process to infer the superproperties, respectively superclasses. alternatively, this information can also be included as metadata of the respective classes, properties or datatypes inside the ontology."
"e system consisted of two ultrasonic transducers, fruit samples (apple, tomato, apricot, and peach), an ultrasonic pulser unit, an oscilloscope [cit] a), and a personal computer (intel core i7). in this study, a special ultrasonic pulser unit (us 100) was designed and implemented [cit] . is customized remote pulser unit has four main components: (1) power supply unit, (2) microcontroller unit (mcu), (3) interface unit, and (4) mosfet unit. e remote pulser can be controlled by a pc over the usb port. variety ultrasonic parameters are easily tuned in this system. depending on the applications, frequency, pulse repetition frequency (prf), number of pulses in a burst, starting pulse (positive or negative), and triggering option (internal or external) all can be set by the user. 55 khz low-frequency ultrasonic probes were used for ultrasonic wave transmission and reception in addition to high penetration of ultrasonic waves from the fruit's surface (shell) into the fruit. rough transmission (tt) technique was used for measurement of ultrasonic velocities and attenuation of transmitted signals. fruits were placed between two transducers; the transmitting transducer was excited with the pulser unit, and it generated ultrasonic longitudinal waves passing through the whole fruits. voltage, frequency, prf, and starting pulse of the pulser were setup as 180 v pp, 55 khz, external trigger, and positive starting pulse, respectively."
"e received signal amplitudes were 10.7 v pp, 18.7 v pp, and 23.1 v pp, respectively. ese results are shown in figure 2 . according to these results, it is said that the systems are working properly. figure 3 shows the experimental system we used for measuring velocity and attenuation of ultrasonic signals for fruit quality measurements."
"when a building component is connected to multiple geometry descriptions of which some have one or more associated files, at least one intermediate node between the building element and each geometry description is necessary to know which files are related to the same geometry description. the earlier mentioned omg ontology provides vocabulary to do exactly this, which would in this case result in a level 2 (one intermediate node) or level 3 (two intermediate nodes) geometry pattern. as the referenced files (e.g. one or even multiple material files (.mtl)) are listed in the main file (e.g. the .obj file) via their file names, these names have to be stored in the graph as well."
"alternatively, specific datatype properties that point to the rdf literal with the geometry description can be used (figure 4) . this method allows to use the datatypes for the purposes defined in dq1 and at the same time, the graph is less verbose than the method involving specific classes. owl allows to define subproperties, similarly as subclasses, that can be used during a reasoning process. based on the above reasons, the method with specific datatype properties was selected to fulfil this design question."
"e system developed in this study has the capacity to give more detailed information about the physical (external dimension) and internal (inner sugar content, acidity, etc.) properties of the fruit. erefore, fruit sorting and grading with high accuracy based on predetermined features, ultrasonic and visual, of fruits can be achieved using machine learning methods using our customized systems."
the remainder is structured as follows: section 2 presents the state of the art whereas section 3 describes a model for the definition of data stream correlations and available datasets. section 4 describes the evaluation concept of the monitoring approach whereas a performance evaluation is discussed in section 5. the paper concludes in section 6.
"a list of five minimal, practical requirements for the use and exchange of geometry descriptions within rdf graphs was presented. it was concluded -based on the relevant w3c specifications and existing related ontologies -that rdf literals have the potential to fulfil these requirements. however, a clear and uniform modelling method and related ontology was still missing. the presented fog ontology and related modelling patterns make it possible to embed geometry of any existing format in rdf literals. the geometry descriptions can contain 2d or 3d geometry, and the geometry schema can be open or proprietary. the suggested method can also be used to link to external geometry files and rdf-based geometry descriptions. where rdf-based geometry descriptions demand specific applications to interpret them, the presented approach allows to reuse existing geometry formats and a wide variety of related toolsets. thus, it is better suited for the exchange of geometry between a diverse group of stakeholders. the fog ontology is modular as it extends the more generic omg ontology to define geometry schema specific relations between building elements and their connected geometry descriptions. this is useful, for example in the case of an existing building, as an individual component can be represented in a 2d as-built drawing, a (part of a) point cloud and a 3d brep model at the same time. additionally, multiple geometry schemas can be used simultaneously during the design of new buildings. omg and fog can be used together with other linked building data (lbd) ontologies such as bot (building topology), product (classification of building components) and props / opm (properties of building elements or zones), depending on the needs of a building project at a certain time. the proposed fog ontology and modelling patterns were used in a proof of concept web application and applied when creating a set of sample triples and queries that was shared online using the sparql-visualizer tool. the demo application visualises the geometry descriptions coming from a connected rdf triplestore and allows to download any geometry description to a file. the decoder module of the application uses the datatype of each literal to know if and how the content should be decoded, or if an external geometry file has to be loaded. the geometry is automatically loaded and visualised by the correct loader based on the defined fog properties identifying the used geometry schema. before the large scale application of geometry descriptions embedded in rdf literals, the potential influence of literal size on the query performance has to be evaluated. minimal metadata related to each individual geometry description such as units / scale, up-axis, georeference, etc. is essential if an application needs to deal with geometry descriptions of any geometry schema coming from different modelling applications. additionally, it is also useful to add metadata related to the modelling accuracy if the geometry description was modelled based on survey data such as point clouds. the approach presented in this paper allows to add such metadata if at least one intermediate node is used between the building object and the geometry description. the terminology to specify geometry-related metadata has yet to be defined in an ontology as existing metadata ontologies such as dublin core and prov-o are rather generic. in contrast to geosparql and stsparql for wkt and gml geometry descriptions and bimsparql for wkt geometry descriptions, it is not yet possible to execute spatial queries on a dataset constructed with fog and the related modelling method. future research should study how this can be implemented for other, frequently used geometry formats in the construction industry besides the wkt and gml formats. finally, this approach of embedding geometry descriptions in rdf literals has to be compared in detail with other approaches to figure 9 : screenshot of the demo web application include geometry descriptions in linked data graphs such as linking to external geometry files and rdf-based geometries. this will help users to select the right approach for their specific use case."
"the proposed modelling method together with the designed fog ontology, allow users to define specific relations between building components and geometry descriptions. this makes it possible to query for geometry descriptions of specific geometry schemas, or all geometry descriptions independent of the geometry schema. the defined datatype properties from fog can be used to connect to rdf literals containing either an embedded figure 6 : adding metadata to individual geometry descriptions geometry description or a reference to an external geometry file. similarly, object properties are defined in fog to link to rdf-based geometry descriptions using dedicated geometry ontologies such as geom, ontobrep, etc. the resulting ontology is thus flexible enough to be used in a wide variety of cases involving geometry descriptions in linked data. metadata about each geometry schema is added to the property definition in the ontology. this includes -if applicable -the file extension, links to associated geometry schemas defined in fog (dependencies), online specifications, iana media (mime) types and links to related entries in dbpedia and / or wikidata. it is expected that the current version of the ontology will be extended over time to include missing and new geometry schemas, and this should be a community effort. as the ontology is published on github6, users can easily propose new subproperties for the existing ontology. besides the raw ontology on github, a human readable html documentation page is provided via a http redirect from its base uri7."
"the assessment of quality of data can basically be evaluated in 5 common dimensions: completeness, correctness, concordance, plausibility and currency. [cit] provides a table of different terms used to describe one of the dimensions of data quality. furthermore, they provide a mapping between data quality dimensions and data quality assessment methods. [cit] introduced sieve, a framework to flexibly express quality assessment methods and fusion methods. this module leveraged user-selected meta-data as quality indicators to produce quality assessment scores through userconfigured scoring functions. in their work they use the \"fit for use\" definition, which is a subjective way of looking at data quality, since it depends on the data consumer. in this paper, we describe a method of assessing the plausibility of data in an objective way for better comparability. the star-city project [cit] describes a system for semantic traffic analytics. based on various heterogeneous data sources (e.g., dublin bus activity, events in dublin city) their system is able to predict future traffic conditions with the goal to make traffic management easier and to support urban planning."
"to do that, ultrasonic horns made of aluminum are designed and manufactured. e inside portions of these horns are empty; therefore, they are filled with ultrasonic gel during the application to reduce signal loss. measurements were repeated with the horns mounted at the ends of the receiver/transmitter probes. after assembly, the probes were attenuator ultrasonic pulser (us100) oscilloscope figure 1 : experimental setup to show ultrasonic system reliability."
"the rdf 1.1 w3c specification3 determines how rdf literals should be used in linked data graphs and refers to other related w3c standards. while each rdf triple consists of a subject, predicate and object, an rdf literal can only be used in the object position of a triple. thus, a literal can have an incoming relation but no outgoing links. the three essential parts of an rdf literal are (1) the lexical form, (2) the datatype uri and (3) an optional language tag. the lexical form has to be a single collection of unicode characters that corresponds to a certain literal value (value space). the datatype uri refers to a datatype that in turn defines what content is valid (the lexical space), the value space and the mapping between the lexical space and the value space. each literal has exactly one datatype. predefined datatype uris in the w3c specifications have a fixed referent. the web ontology language or owl 2 specification4 for example contains the fixed datatypes owl:real, owl:rational, rdfs:literal, rdf:plainliteral, rdf:xmlliteral and a selection of xsd datatypes (xml schema definition language) documented in the w3c xsd 1.1 specification part 25 (e.g. xsd:integer, xsd:string, etc.). additionally, custom datatypes can be defined in an ontology by extending a fixed datatype or creating separate new datatypes. datatypes cannot be formally defined as a 'subdatatype' from another datatype, as each rdf literal has exactly one datatype assigned. this distinguishes them from classes 3https://www.w3.org/tr/rdf11-concepts/ 4https://www.w3.org/tr/2012/rec-owl2 [cit] 1211 5https://www.w3.org/tr/2012/rec-xmlschema11-2 [cit] 0405/ and properties in rdf, which can have respectively a 'subclass' or a 'subproperty'."
"after the discussion of event-based sensor readings in the last section, this section deals with measurements on a continuous scale. therefore tomtom traffic flow data has been evaluated against odaa traffic flow measurements. for every tomtom measurement that showed a severe slowdown of more than 15km/h compared to the free flow speed an evaluation has been triggered: six weeks time series data for up to 10 sensors within a maximum distance of 200 meters have been analysed and their minimum values of the irregular component have compared to the tomtom measurement. figure 8 the two data sources. 95.5% of odaa comparisons also have an irregular component value i t  0km/h, which is indicating a decrease in speed. these measurements confirm the plausibility of the tomtom flow data, although there is no direct correlation between measured values."
"the class of an additional intermediate node could denote the used geometry format in the content of a connected rdf literal. by defining subclasses, reasoners can be used to infer the class hierarchy of related geometry formats (see figure 3 ). however, to group rdf literals that have to be treated together (see feasibility analysis of req. 4), another additional node is needed between the building element and this node indicating the geometry format. as a consequence, this method tends to result in a relative verbose graph structure."
"two geospatial ontologies that allow to store existing geometry descriptions in rdf literals are discussed: the geosparql and the strdf ontology. the ogc geosparql specification [cit] describes an ontology with the same name, and a sparql extension for 2d spatial querying. the ontology provides terminology to connect any object via an intermediate node to a well known text (wkt) or geography markup language (gml) geometry description embedded in an rdf literal. the datatype property between the intermediate node and the rdf literal is specific regarding the geometry format (geosparql:aswkt or geosparql:asgml). additionally, the literal has a custom datatype assigned (geosparql:wktliteral, resp. geosparql:gmlliteral). while each gml file can reference a standardised coordination reference system (crs) by design, the geosparql specification allows to mention the crs inside the lexical form of the literal in case of wkt geometry description."
"researchers in both academia and industry are actively investigating the benefits of web technologies to improve the exchange of structured building data, including geometric data . the semantic web technology stack is standardised by the world wide web consortium (w3c) and uses resource description framework (rdf) triples as an elementary building block to create graphs of linkable entities. different methods to include rdf-based geometry descriptions in linked data graphs already exist. they have their own dedicated vocabularies, e.g. the ontobrep1 or geom2 ontologies, and related toolsets [cit] . creating geometry representations in rdf according to the approach mentioned above can be achieved by using a dedicated linked data geometry modelling tool. alternatively, geometry coming from regular cad applications can be converted into rdf-based geometry descriptions. when reusing such geometry descriptions stored as an rdf graph, the geometric data has to be transformed into a readable format for geometry kernels of typical non-rdf applications used in practice. the construction industry, however, also demands a more straightforward methodnot related to specific rdf-based geometry toolsets -to 1https://github.com/ontobrep/ontobrep 2https://github.com/w3c-geom-cg/geom easily transfer geometric data. such data can flow bidirectionally between semantic graphs and the stakeholders' existing geometry processing tools, ideally with as little conversions as possible to minimise conversion errors. consequently, methods to introduce already existing and widely used geometry formats (e.g. step, obj, dwg, etc.) in rdf graphs have to be considered. two possible alternatives for rdf-based geometry emerge: linking rdf entities of building elements to (1) external geometry files by storing their file location references in rdf literals or (2) rdf literals containing the entire content of such geometry files. this paper focuses primarily on embedding geometry descriptions in rdf literals, but also considers rdf literals referencing external geometry files. the remainder of this paper contains four sections. the section 'rdf literals and geometric content' covers an analysis of the relevant w3c specifications, existing related implementations and practical requirements to enable the adoption of rdf literals for geometric data. the final part of this first section discusses the feasibility of using rdf literals to store geometry descriptions. shortcomings of the existing implementations for rdf literals and geometry are addressed in the following section. a new ontology is proposed and validated together with an accompanying modelling method. section three, 'proof of concept application', demonstrates how the above can be implemented in applications, while the final section contains the conclusion and addresses future work. all uniform resource identifier (uri) prefixes mentioned in the remaining of this paper are assembled in listing 1."
"however, in contrary to the literature, the tt mode ultrasonic system is not a reliable method in determining the quality of fruit due to the aforementioned reasons. in the future studies, the pe mode ultrasonic method will be used as an alternative to tt mode ultrasonic measurement. ultrasonic waves reflected from the fruit shell will be measured, and the quality of the fruit will be obtained by designing a t (transmission)/r (reception) switch for the pe system."
"rdf literals -as defined in the current w3c specifications -can store the content of every kind of geometry format, making it possible to embed any geometry description directly in an rdf graph. as discussed in the related work subsection, most existing ontologies for describing such rdf literals either focus on a limited amount of specific geometry formats (e.g. geosparql, strdf / stsparql and bimsparql) or exclude 2d geometry descriptions (bot). omg on the other hand makes it possible to link to any geometry description stored in an rdf literal and is not geometry format specific. to make the approach more practical in real world situations, the following three issues have to be addressed. first, there should be a method for applications to unambiguously distinguish between rdf literals referencing external geometry files, literals embedding text-based and encoded binary geometry descriptions. secondly, the specific geometry schema of each geometry description has to be made explicit and they should be related to other geometry schemas. finally, content of associated files has to be connected properly to the content of its main file, including the file names as referenced in the main file. in the following section, these three issues are addressed by the new file ontology for geometry formats (fog) and its related modelling patterns."
"the composite monitoring [cit] evaluates the plausibility of individual events, thus evaluating the data streams causing the event. this is realised in the domain of large sensor networks, e.g., those deployed in a smart city, where no ground truth is available. the monitoring employs modelbased analysis of different spatially and temporally related sensor values. the model-based approach allows detecting outliers in sensor readings that are caused by defect sensors and cannot be explained by similar information of related sensors. for example, a traffic jam can be detected by traffic sensors reporting extensively slower traffic speeds. this can be validated by an analysis of consecutive traffic sensors on a road. therefore, the composite monitoring inspects historic time series of various dependent sensor streams."
the estimated correlation model between the described data streams is illustrated in figure 1 . it visualises the expected impact of odaa cultural-events and tomtom traffic flow/incident events on the odaa traffic data. the following section describes the applied methodology for the evaluation.
"erefore, in recent years, the application of nondestructive, noninvasive, and noncontact methods and designing new instruments for food quality determination have been the focus of interest by researchers. ese techniques are becoming more favored and practical compared to destructive techniques as nondestructive methods allow the measurement and analysis of individual fruit, reduce waste, and permit repeated measurements on the same item [cit] . different quality parameters have been determined in several agricultural products by a variety of nondestructive methods. ese methods are based on optical, mechanical, electrical, and electromagnetic measurements [cit] . nuclear magnetic resonance imaging (nmri) [cit], raman imaging [cit], ultraviolet (uv), near-infrared (nir), mid-infrared (mir), e-nose [cit], ultrasonic technique [cit], and machine vision [cit] are some widely used nondestructive methods. quality can be determined by evaluation of external and internal parameters of fruits. firmness measurement is one of the most common methods used to determine maturity and/or ripeness. evaluation of fruit ripeness or maturity is essential for timing in finding optimal harvesting, transportation-marketing, and consumption [cit] . due to the fact that ripe fruit is very sensitive to mechanical damage, microbiological decay, and physiological deterioration, ultrasonic fruit quality evaluation can be a potential solution as a nondestructive system [cit] . ultrasonic technique has become a more common modality among other nondestructive methods for evaluation of fruits because of the advantages in firmness evaluation which are cost-effectiveness, robustness, reliability, and fruit safety [cit] . e basic principle of ultrasonic measurement is considered as changes in attenuation and velocity of ultrasonic waves, namely, sound wave with a frequency higher than human hearing limit, by absorption and scattering of waves when interacting with a matter [cit] . according to the literature, acidity, viscosity, and sugar content of fruits can be evaluated using the ultrasonic method by correlation of ultrasonic parameters with fruit inner factors. velocity and attenuation are dependent on the physical parameters of fruits. attenuation is also dependent on the frequency of sound propagation inside fruits [cit] . although the ultrasonic method is admitted as providing fast, accurate, and nondestructive fruit quality evaluation, currently developed ultrasonic systems are not suitable for nonlaboratory and field applications. moreover, each fruit has a unique ultrasonic response but traditional ultrasonic systems are designed for specific fruit species. our ultrasonic system gives the user the ability to change frequency, amplitude, and pulse repetition frequency as well as the number of pulses in a burst. moreover, the system we used in this experiment differs from other devices used in the market by producing positive, negative, and bipolar pulses and changing the frequency settings from 10 khz to 10 mhz. e devices in the market produce frequencies starting from the lower limit of 500 khz. is frequency is too high for fruit quality measurement as frequency increases, penetration decreases, and general losses increase. erefore, the custom-designed ultrasonic system is crucial to monitor the quality of different types of fruits in picking, storing, and packaging site in addition to laboratory-based applications."
"two different schemes have been designed for tt mode ultrasonic experimentation. fruits with and without shell have been exposed to the ultrasonic signal. according to the experimental results, the ultrasonic signal is strongly reduced due to absorption while the signal passes through the fruit. as it can be seen in figures 5(a) and 5(b), no signal measurement was observed by the receiving probe. even under the signal which has a high signal amplitude (e.g., 120 v), the signal measurement that can pass through the fruit was not possible."
"the application of rdf literals for storing building geometry descriptions or references to geometry files is not new, but research documented online and in literature focuses on rather specific cases or domains. the shortcomings of six existing ontologies are analysed in this subsection. first, two frequently used geospatial ontologies are discussed, followed by three ontologies from the construction domain. lastly, a more generic and flexible ontology, recently published by the authors of this article, is proposed."
"the seasonal component at time t reflects seasonally repeated variation. it exists when a time series is influenced by seasonal factors for a fixed and known period (e.g., daily traffic pattern like morning and evening rush hours)."
"to evaluate the plausibility of a reported event, in the first step, nearby data streams are identified. based on the category (e.g., temperature, parking, traffic) of these sources further sensors, which are located nearby are selected. according to the used model, the event should also affect neighbouring sensors and cause a similar behaviour. real world events exhibit a typical spatial propagation that can be modelled. for example, traffic propagates along the roads whereas noise propagates in every direction. figure 2 shows the voronoi cells for odaa traffic flow sensors and illustrates that the nearest traffic sensor is unlikely to represent the current flow of the illustrated street segments. figure 3 describes the evaluation process of the composite monitoring. the goal is to determine a correctness value c e for the event e. a set of correlating data streams s e is used as validation source. a set of sensor specific validation functions v s is used to compute the plausibility of the event e. the figure shows the four phases of the evaluation process: 1) determine relevant sensors in the set of all streams s. find spatially correlated streams by using a suitable distance model m d, which describes the means of propagation of the event (e.g. air or street) 2) determine the temporal distance by analysing the direction d of expansion, propagation velocity v, and range r of the impact as function of m d 3) compute the correlations between streams and the event e by applying a) v s as the set of validation functions for event e and each stream s 2 s e b) ⌧ s as the set of temporal direction (defines if the change in s is a result of e or the cause for e) 4) analyse partial correlation values to evaluate the correctness by using a set of weights w s for each stream s 2 s e and a combination function p, e.g. min, mean. as a result, we get the tuple"
"first of all, the outer characteristics (size and volume) of the fruits were determined by the machine vision system. is technique consists of multiple cameras which is much more practical, unlike the conventional method in which images are captured from a single camera and fruits have to be rotated by the hand. more accurate results may be obtained by reducing unwanted noise (shadows) in the images."
"in this section, the potential of rdf literals for representing and exchanging geometric content is studied. the first subsection introduces how rdf literals are defined in the relevant web standards, while the second part discusses how other researchers and organisations apply rdf literals for geometric content. based on current practice in the construction industry, five practical requirements for the use and exchange of geometric data are listed in the third subsection. finally, the feasibility of rdf literals to store geometry descriptions is discussed."
"the quality of experience using smart city applications highly depends on the availability of appropriate, accurate, and trustworthy data. this includes the availability of necessary data sources as well as proving their plausibility. the reliability of the extracted sensor information of external data sources has to be monitored permanently during runtime to obtain satisfying application functionality and user experience. smart city infrastructures are using a variety of different information sources facing a divergent trustworthiness of information providers and their sensor equipment. in contrast to common simplified iot sensor infrastructures, cities often utilise aggregated and reasoned information. traffic data, for example, often rests upon only a subset of cars that are measured and is interpolated to offer a view of the whole situation. due to a frequent unavailability of precise sensor data and a missing ground truth, there is a high need for evaluating data source reliability and determining the trustworthiness. therefore, this work proposes a correlation model -based monitoring approach to evaluate smart city data sources."
"in this study, two methods, including ultrasonic measurement and image processing-based machine vision measurement, were used to evaluate the fruit quality based on external and internal parameters of fruits. ese methods are explained in the following sections."
"evaluation of sensor data is a time critical task since its usefulness and validity is not unlimited. furthermore, the number of sensor sources is growing rapidly with the result that future analysis has to be scalable and support a huge number of sensors in parallel. figure 9 shows the performance measurement of the previously presented time series decomposition and comparison/correlation calculation to a given value or event. it shows boxplots for 10 repetitions of the experiments. the boxplot whiskers show the lowest duration still within 1.5 inter-quartile range (iqr) of the lower quartile and the highest duration still within 1.5 iqr of the upper quartile. outliers are drawn as a dot. the xaxis shows the numbers of sensors that were used for the evaluation. the y-axis shows the duration of the evaluation. the colour of the box illustrates 3 different time-spans that were used for the time series decomposition that has been used for the evaluation. the figure shows a basically linear time increase for a rising number of sensors. however, the graphs are not consistently linear since the algorithm has been parallelized in 11 threads for multiple sensors 4 . outliers mainly depend on active system tasks that have been running in parallel. the performance evaluation shows that e.g, an evaluation of 10 correlating sensors, regarding 12 weeks of historic data and decomposing it for a correlation analysis can be approached in usually less than 5sec. this leads to a feasible utilisation in a live environment for the evaluation of suspicious data. furthermore a caching of time series (e.g., once a week) allows the hardware setup to substitute processing power with memory."
"a proof of concept web application was implemented to visualise geometry stored in an rdf graph. the application, conceptually demonstrated in figure 7, communicates via sparql queries with an rdf triplestore that contains the fog ontology (tbox) and abox rdf triples following the modelling principles defined in the previous section. the triplestore answers the query with a json response containing at least the three variables: (1) value (the lexical form of the literal), (2) datatype and (3) property (referring to the geometry schema). the decoder module of the application uses the returned datatype of each result to know if and how it should decode the returned value variable, or if it should load an external geometry file. with a switch operator each property variable is evalu-ated and the correct three.js geometry loader -if available -is called to visualise the geometry description in the 3d web viewer. this last module uses the open source three.js library to create a webgl-based visualisation. the geometry is coloured differently per geometry schema as indicated in a user interface legend. additionally, the switch also creates and activates a download-to-file button for each geometry description, including the ones that cannot be visualised by three.js."
"the analysis of isolated data streams is not a feasible solution in determining the correctness of outliers and suspicious stream behaviour. this paper proposes a correlation model between data streams to determine data plausibility with the dependency of relevant data streams. the model describes expected relations between individual information sources and their mutual impact. thereby, the model does not necessarily describe inevitable effects between data streams. therefore, when a traffic sensor a reports an average car count of 0 vehicles per minute, a data source b that reports traffic jams out of different sensor data can be used to verify that the sensor a is working correctly."
relevant metadata related to the individual geometry descriptions can be added only when using at least one intermediate node between the building component and the rdf literal. the vocabulary for these metadata properties has to be documented in an ontology.
"the strdf ontology was developed together with a sparql extension named stsparql to allow 2d spatiotemporal querying [cit] . similar as geosparql, strdf allows to link any object via an intermediate node to a wkt or gml geometry description stored in an rdf literal. in contrast to geosparql, only the (custom) datatypes are specific: strdf:wkt or strdf:gml. the strdf / stsparql implementation also allows to include the used crs inside the lexical form of the literal in case of wkt."
"a natural way to denote the used geometry formats, is to define custom datatypes in an ontology as has been done in geosparql and stsparql for both wkt and gml literals. other options are the definition of specific classes in combination with additional intermediate nodes or the definition of specific datatype properties. in order to be able to query for any geometry description stored in rdf literals, the generic omg datatype property omg:hassimplegeometrydescription could be used to link to an rdf literal containing a geometry description."
"1. geometry descriptions stored in rdf literals can be of any existing geometry format, including text and binary files from open and proprietary formats. the rdf literals can contain 2d and / or 3d geometry descriptions. the content of text encoded geometry files can contain single and double quotation marks and is typically read line by line."
"the recently published ontology for managing geometry (omg) [cit] by the authors of this paper provides a datatype property named omg:hassimplegeometrydescription to link to a geometry description (2d or 3d) of any geometry format stored in an rdf literal. similar to bot:hassimple3dmodel, this relation is not specific, but its scope is broadened to include both 2d and 3d geometry descriptions. omg does not define any specific datatype properties and custom datatypes, as its scope is to arrange the management of geometry in general, independent of the actual geometry formats used. inspired by the ontology for property management (opm) [cit], the omg can be used to create level 1, 2 or 3 relations between a building element and its geometry descriptions [cit] . this gives users the flexibility to use a more complex level 3 pattern (three relations between the building object and the geometry description) that allows version control, or a very simple level 1 pattern (direct link between an element and its geometry description) for easy querying."
"e aim of this study is to introduce a nondestructive system integrated with ultrasonic and machine vision methods for fruit quality evaluation. is system is eligible to monitor the maturity of tomato, peach, apple, and apricots and evaluate volumes. although the ultrasonic system integrated with automatic machine vision could provide more accurate fruit quality assessment, our results showed that quality assessment of fruits using ultrasonic methods using through transmission (tt mode) technique has drawback in terms of reliability. experimental results of this study conflict with the results in the literatures."
e captured images were segmented from the background using a simple thresholding method combined with morphological operations in each image. feature extraction process (major and minor axis lengths) was accomplished following segmenting the fruit sample from the background. e volume of the fruit was calculated by considering the fruit as an axisymmetric object. detailed description of volume estimation used in this study was explained in our previous study [cit] .
"fikret yildiz, ahmet turanözdemir, and selman uluışık proposed the research topic, supervised the study, and prepared the initial draft of the manuscript together. fikret yildiz contributed significantly to the writing and organization of the manuscript. ahmet turanözdemir mostly coordinated the experimental work and analyzed the experimental data in ultrasonar defense and aviation technologies inc."
"this section discusses the design of the file ontology for geometry formats (fog) and the accompanying linked data modelling patterns. fog is designed as an owl ontology that can be used together with the ontology for managing geometry (omg). therefore, defining generic properties for linking building components to geometry descriptions is out of scope for fog, as it is already covered by omg. additionally, terminology for properties to add metadata to individual geometry descriptions is not included in fog. the design decisions for the fog ontology and related abox modelling conventions are explained via three design questions (dq) derived from the feasibility analysis in the previous section."
"in the proposed modelling pattern, the datatype of each literal is used to add information about the encoding of its lexical form as depicted in figure 1(a) and 1(b) . the fixed datatype xsd:string can be used for a text-based geometry description while the fixed datatypes xsd:base64binary and xsd:hexbinary can be used for encoded binary data. alternatively, it is also possible for developers to define other binary encodings as custom datatypes in an ontology. finally, datatypes can also be used to distinguish between embedded geometry descriptions (text or binary encoded), and references to geometry files outside the rdf graph (figure 1(c) ). in this case, the fixed datatype xsd:anyuri can be used when the lexical form of the literal contains the location of a file, similarly as has been proposed in coins. accordingly, no vocabulary has to be introduced in fog to address this first design question as the fixed datatypes suffice in most cases."
"e raising awareness of consumers in high quality of foods directs the producers to a reliable, rapid, nondestructive, and noninvasive technique for maturity determination, especially during harvesting and packaging processes."
"the following figure 6 illustrates the appearance of a tomtom incident report in the city of aarhus combined with nearby odaa traffic data streams. the x-axis and z-axis show the longitude and latitude of the information source. the y-axis shows the time of measurement. at the origin of the z-axis, a grey plot of the street segments [cit] the analysis shows a significant speed drop during the tomtom event on several sensors that measure the same direction inside the street-crossing area."
"2. the actual geometry file format of the content stored in the rdf literal should be defined explicitly. as a result, software applications can query for the used geometry file formats, without having to analyse the content of each individual literal. doing so not only helps applications to deal with the wide variety of geometry formats that can be available in an rdf graph, but also prevents the exchange of geometric data that is not supported by a certain application. the last three properties are very relevant if the software application of a stakeholder has to use geometry from different geometry formats coming from other geometry modelling applications."
"destructive methods and professionally trained panels are widely used to determine the quality of fruits and vegetables. ese methods are obviously more reliable; however, they are time-consuming and cost intensive and require specialized sample preparation. erefore, the methods in this class are not suitable for industries such as packaging industry as it ruptures the fruit tissue and evaluation of whole lot cannot be done [cit] . trained panels are in another way abundantly used to evaluate quality of fruits and vegetables. although welltrained panels make the evaluation, the scores can fall into a large variability and may drift over time. moreover, the panel is generally limited to 6-8 objects per session, the procedure is slow, and the cost will be high [cit] ."
e transducers are coupled to fruits with an ultrasonic gel in order to increase the amount of ultrasonic power that penetrates the fruit. e receiver probe was used to collect back-attenuated signals that passed through the inspected fruit and connected to the oscilloscope in order to visualize and measure the attenuated signals. e first experimental result of the fruit quality measurement using the customized remote pulser unit has been found [cit] .
"e quality (texture, color, shape, size, sugar content, and nutritional value) of agricultural products is highly important in terms of consumers, determining market acceptance, and thus, directly or indirectly a ects storage and postharvest processing operations (such as transportation and conditions for storage) [cit] . erefore, these agro-food products should be harvested at optimum ripening stage, classi ed into di erent classes based on their quality parameters ( rmness, color, size, and shape), and immediately transported to the market for consumers, as most of the vegetables and fruits are very sensitive agricultural products in over-ripening stages."
"stakeholders should be able to focus more on the content of the exchanged geometry instead of the formats. if the geometry format of a geometry description can be uniquely identified with fog terminology, applications can unambiguously request geometry in their supported or even preferred formats. additionally, it should be possible for applications to retrieve the geometry format of each geometry description in an rdf graph, without having to access the lexical form of each such rdf literal. as a consequence, fog has to provide the necessary terminology to assert the used geometry format of each geometry description. this type of information can be modelled in several ways using custom datatypes, specific classes or specific datatype properties."
"an alternative to approximating execution paths using a cfg is symbolic execution [cit], which relies on a constraint solver to explore feasible paths. symbolic execution has been explored before in the context of wcet analysis [cit], but neither presents a tool and the timing models are not used for the analysis of other temporal properties. however, [cit] presents an integrated path and timing analysis method for c programs based on cycle-level symbolic execution using instruction-level simulation techniques. in six out of seven benchmarks, the method yields precise timings. [cit] presents a tool for wcet squeezing, which combines symbolic execution with ipet for computing a precise wcet bound. wcet squeezing is an iterative postprocess implemented in the r-tubound [cit] toolchain. it works by mapping the result of the ipet analysis to an execution trace in the program, which is then symbolically executed for determining the feasibility of the path. if it is infeasible, the ilp problem of ipet is extended with additional constraints that exclude that path from being considered again. the new ilp problem is then solved and the process starts over again. thus, on each iteration, the wcet bound is made tighter."
"researches on data streams are motivated by emerging applications involving continuous massive data sets such as customer click streams, e-commerce, wireless sensor network, network monitor, telecommunication system, stock market and meteorological data. for the data stream applications, the volume of data is usually too large to be stored or scanned more than once. furthermore, the data objects can be only sequentially accessed in the data streams, random data access techniques are not practical, so data stream clustering is a challenging area of research that attempts to extract useful information from continuously arriving data. a well designed data stream approach allows the processing of potentially infinite amounts of data. it scans the stream ideally in a single pass, keeping just the necessary data in the main memory. the elimination of random access is the great benefit that allows even gigantic amounts of data to be processed. however, a specialized data stream algorithm is necessary. the aim of our work is to develop an algorithm that can handle with data stream clustering effectively."
"a network of timed automata (nta) is the parallel composition of a set of tas. the tas of an nta proceed concurrently and communicate through synchronization channels and shared variables. actions and co-actions on channels are denoted ! and ?, respectively."
"1 build r*-tree for the new data in basic window and find corresponding distance when k-dist curve turns steep to smooth, then determine the reasonable value of eps; 2 select any point p from data set and query it in basic window; 3 if p is the core point, then find all the point directly density-reachable from p and form a cluster contains p; 4 otherwise, label p as noise point; 5 if there is not any marked point in the data set of basic window, then randomly select a point and repeat the operation mentioned above; 6 otherwise, get initial clusters; 7 get synopsis data;"
the jpf-symbc-rt component of symrt conducts a post-translation of the symbolic execution tree to the nta formalism using a newly developed symbolic execution tree capability available in jpf-symbc v7 [cit] .
the program is written in matlab under the matlab 7.9 [cit] . the tests were performed on a core(tm) i7 2.67ghz with 4 gmb memory and 500gb hard disk. the experimental data is derived from the network intrusive data stream of kdd-cup'99 which is widely used in many data stream clustering literatures.
"we are working on applying symrt for the performance analysis of different components of the nasa tactical layer solution for planes, t-tsafe, currently focusing on the conflict detection and conflict resolution algorithms."
"all analyses are expressed in the specification language of uppaal, which extends tctl. in essence, this means that the timing analyses supported by symrt are viewed as model checking problems. for example, the schedulability analysis is expressed as a reachability problem: the property of a non-schedulable system is expressed in tctl and if the model, which is derived from the actual system, satisfies it, then we conclude also the system non-schedulable. the details on how the analyses are formulated are described later."
"both building r*-tree and drawing k-dist graph are very time-consuming, especially of large-scale database. besides, users need trial and error to select appropriate kdist value, so we should preprocess the data stream before clustering."
"generating timing models for each task using this per-task analysis significantly reduces the complexity of symbolic execution because any behavior of the system outside the local task needs not to be considered. the composition of all timing models generated from the tasks from the per-task analysis yields a complete system model which is also safe, because the composed model over-approximates the possible interference between the tasks."
"it is perhaps surprising that the combination of symbolic execution and model checking works so well as both have issues with scalability due to the large number of paths respectively states to explore. however, by using a \"per task\" symbolic execution leveraging the safety critical java (scj) programming model that groups code into missions consisting of relatively short tasks, the analysis is tractable. furthermore, when using timing models of the target execution environment, the generated ta of the program is at basic block level, which significantly reduces the state space size."
"the program model is built modularly. the timing behavior of the execution environment is obtained from environment models capturing the timing behavior of the jvm and hardware of the target platform. based on a configuration, our technique generates the complete timing model as a network of timed automata (nta) amenable to model checking using the uppaal [cit] model checker. the nta model can readily be used for estimating wcet and bcet but also generalizes to verification of properties expressible in timed computation tree logic (tctl). in contrast to previous tools [cit], which provide limited feedback, our technique can also generate witness traces that expose the reported behavior, useful for debugging and program understanding."
"capturing the temporal behavior of the execution paths of the real-time tasks is done at java bytecode instruction level; for each java bytecode of the execution path, a location is created in the ta and connected by outgoing edges to successor locations corresponding to the control flow. for branching instructions, such as ifeq and fcmpl, there can be up to two and three outgoing edges, respectively, if the path conditions of the branches are satisfiable (or if the decision procedure is inconclusive). for all other instructions, there will be only one outgoing edge."
"for a complete timing model, the nta is further extended with tas from tetasarts capturing the scheduling policy and for controlling and monitoring the state of associated real-time tasks. a controller for periodic tasks is shown in fig. 9 . controllers for sporadic tasks are similar except that their eligibility for being scheduled is determined by the previous firing of the associated event. the task tas are modified for making the association to a corresponding controller resulting in the ta shown in fig. 10 for our running example."
"we have used two configurations of the execution environment; (1) jop and (2) hvm 2 [cit] running on the avr atmega2560. the timing model of the latter is a generated timing scheme. we use the same configuration of the execution environment across the tools, e.g., symrt and wca have been configured with read and write wait cycles set to 1 and 2 (and cache configuration is the same). also, to give indications of the precision of symrt, we compared with measurements of bcet and wcet obtained by using inputs yielding the best and worst case behavior (e.g., for bubble sort, a sorted and unsorted list). we used the jop simulator to read the cycle count before the first instruction is executed of the target method and after the return instruction. for hvm+avr, the measurements have been obtained in a similar way by using the debugging facilities of atmel studio 6. for this set of experiments, we used a laptop with an intel core i7-2620m cpu @ 2.70 ghz with 8 gb of ram. the peak memory consumption for symbolic execution is 500-700 mb for all examples. uppaal peaks at 50-200 mb during model checking. table 1 shows the results for the comparison on jop."
"use ssq(sum of square distance) to compare the clustering qualities of d-dstream, stream and clustream on the basis of real dataset. ssq is a method to compare k-partition clustering quality, which measures the quality of k-partition of algorithm by calculating the distance between every point and its clustering center, smaller the value of ssq is, better the clustering quality of algorithm is. for d-dstream, ssq is the quadratic sum of the distance between the point and corresponding cure object. then we can know from figure 5 that the clustering quality of d-dstream is better than stream and clustream which rely too much on historical data and neglect new data, while d-dstream introduces synopsis data to solve the problem. this paper proposes a data stream clustering algorithm (d-dstream) based on wavelet network and two-phase density. in essence, wavelet network applies the technology of wavelet transformation to compress data which flows into time window, also it has been strictly proved and is able to get good results, while tdbscan is improved on the basis of density-based spatial clustering (dbscan).because stream is controlled by historical data and clustream is difficult to describe nonspherical cluster or eliminate \"old data\" on line, d-dstream introduces method of wavelet network and two-phase density to solve these problems, meanwhile, it gets clustering results with high quality and consumes little time and memory. this single-scan, incremental update algorithm can process noise and old data by adjusting the parameter of density, and is demonstrated effective and accurate on the basis of real dataset. for a future work, this paper suggests studies on how to accurately design the varying synopsis data in sliding window and simplify the structure of d-dstream algorithm."
"our technique is implemented as a tool, called jpf-symbc-rt, which can be used standalone for above purposes. the tool has also been integrated with the timing analysis tool tetasarts-we will refer to this combination as the symrt tool. symrt is used for automatically constructing a complete timing model of java real-time systems written in a variant of the scj profile. this model is similarly built on a per-task basis using jpf-symbc-rt, taking into account possible interference from other tasks. symrt allows to reason about additional timing properties such as the schedulability of periodically and sporadically released task, worst case response time (wcrt), worst case blocking time (wcbt), and processor utilization under the specified scheduling policy. symrt allows direct interaction with jpf-symbc-rt for conducting wcet and bcet analysis. to the best of our knowledge, symrt, and more specifically, the jpf-symbc-rt sub-component, is the first tool supporting bcet analysis of java bytecode. both tools are open source; the project website http://people.cs.aau.dk/~luckow/symrt/ contains details on how to obtain the source code and usage instructions."
"the net weight between hidden layer and output layer is denoted as w 1,w 2,…,w k, and single neuron in output layer is used to conduct summation operator on the signals through the transformation of hidden layer neurons. this is because the signal formulation based on wavelet neural network is achieved through the linear superposition of selected wavelet basis. set output signal as s(t), then come up the formula"
"this paper is organized as follows. in section 2, the related works of clustering algorithm are given. in section 3, the novel dynamic data stream clustering algorithm is presented. the experimental results of comparing the algorithm proposed in this paper with other algorithms are also presented in section 4. finally, our work of this paper is summarized in the last section."
"the timing model of a multi-tasking real-time system is built modularly, based on the symbolic execution of each individual task. whenever an instance or a static variable is read (via getfield or getstatic bytecodes), the per-task analysis checks to see if the variable is possibly shared among multiple tasks. a variable is shared if it is referenced in a chain of references from a static field or from a task (thread) object. symrt also relies on additional checks in jpf for determining sharedness, e.g., if the field is immutable. we have implemented a procedure that propagates the sharedness information along reference chains whenever an update happens (via put-field and putstatic) to a variable that was marked as shared. furthermore, the lazy initialization used in handling symbolic references has also been modified to mark all the newly created objects as shared (if the symbolic fields belong to a shared object)."
"this paper presents symrt in terms of its design and capabilities and is organized as follows: in section 2, we provide the preliminaries followed by section 3, which discusses related work. section 4 presents the overall design of symrt and its capabilities. we also show how the timing model is constructed using symbolic execution and real-time model checking in a task local approach and present optimizations for mitigating state space size. in section 5, we present the usage of the tool along with experimental results. section 6 contains conclusive remarks."
"in recent years, several analysis tools for scj have been put forward including wca [cit], sarts [cit], tetaj [cit], and tetasarts [cit] . characteristic for these is that they rely on reconstruction of the control-flow graph (cfg), which characterizes all possible execution paths of the program. this representation over-approximates possible program behavior since it also includes infeasible execution paths, i.e., execution paths for which there does not exist inputs that lead to that path being exercised. in turn, temporal behavior is over-approximated, thus likely yielding very pessimistic results of execution times that potentially affects the conclusions that can be made about the temporal correctness of the system-a system may be deemed unschedulable on a given platform although in reality it is schedulable."
"the oeta module has similar major counterparts but does not reason on system level when generating the program nta. using this module, it is possible to extract a trace leading to the wcet (and bcet) and visualize the respective execution paths. we will demonstrate this facility later in the paper."
"in this paper, we present a combination of symbolic execution [cit] and real-time model checking that generates a precise control-flow model from the symbolic execution trees obtained with a symbolic execution of the program. each tree characterizes the set of feasible execution paths (up to some bound) of the analyzed task and yields a precise timing model. both symbolic execution and model checking have issues with scalability due to the large number of paths respectively states to explore. we address this by using a \"per task\" symbolic execution leveraging the scj programming model that groups code into missions consisting of relatively short tasks. we will show experimentally using representative examples of real-time systems that our analysis approach is tractable."
"in which, w k, a k, b k are net weight, wavelet expansion factor and shift factor, k is the number of wavelet basis, that is, the number of hidden layer neurons. the structure of single layer network with an input and output node is shown in figure 2 ."
"divide the data stream sequence fragment of window w into m basic windows at t, apply technology of wavelet transformation on the data of these m data fragments and transform the high-dimensional data structure into other low-dimensional one. then apply one-phase tdbscan algorithm on the processed data stream and get the synopsis data. at last, take the generated synopsis data as static clustering data source, and then conduct the secondary destiny clustering method and update clusters, that is, produce k clusters as requirement. the design of d-dstream algorithm embeds dwt algorithm and tdbscan algorithm, the description of tdbscan is as follows:"
"there is a need in the real-time java community for techniques and tools for temporal verification. analyzing timing properties for java programs is challenging primarily due to the fact that java is usually translated to java bytecode, which is then interpreted by a java virtual machine (jvm). this level of indirection complicates formal analysis as both program and jvm have to be taken into account for a given hardware platform; some of this complexity can, however, be reduced by a hardware implementation of the jvm such as jop."
"here, the combination of the fetch channel and asm_inst establishes the communication link with the jvm nta. the jvm nta synchronizes on the channel whenever a machine code instruction (as provided by the asm_inst variable) is to be simulated the product regardless of whether translation approach 1 or 2 is used is the complete timing model with the analyses specified as uppaal specifications. the output of model checking is the analysis result, e.g., a yes/no answer for tctl properties, and a number for wcet and bcet analysis. in addition, for execution time analysis, a witnessing trace, i.e., a sequence of symbolic states leading to the violating state, can be used for debugging, visualization, and program understanding."
"a high-level overview of symrt is shown in fig. 3 . symrt takes as input the java class files constituting the real-time system and a configuration which, among others, specifies the analyses and the model generation technique. symrt allows the generation of two different timing models. the configuration specifies which model to generate, which can either be a complete timing model or an execution time analysis optimized model:"
"we compare the schedulability analysis of symrt with tetasarts using the minepump control system [cit] (0.5 kloc), the real-time sorting machine (rtsm) [cit] (0.3 kloc) and a variant of md5scj [cit] with multiple tasks (0.4 kloc). we also conducted the analysis on the mer arbiter (3.6 kloc) that models a flight software component for the mars exploration rover (mer) developed at nasa jpl [cit] . it was not written for realtime java, but it features two users that use a number of resources; we made a minor revision associating to each user a periodic handler. finally, we also analyzed a version of the lift real-time system from jembench [cit] with 18 tasks. tetasarts has not been able to construct the models for mer and lift. even though mer is relatively simple, the dependency extent computed in tetasarts for generating the cfg is too large."
"the model generator component in the tetasarts module constructs the program nta by using either the original cfg nta generator or the real-time symbolic pathfinder component. the former is based on reconstructing the cfg, while the latter is based on symbolic execution and works on the symbolic execution tree. the resulting program nta is then combined with the ntas of the execution environment, i.e., the jvm nta and the hardware nta, using the model combiner component. during this process, optimizations are also performed, e.g., for tailoring the jvm nta to the hosting program. for details on the execution environment ntas and optimizations, see [cit] . the product of the combiner is the complete timing model, which is amenable to model checking using uppaal, and the output will be the analysis result: for schedulability, it will be a yes/no answer, and for wcet a number. in addition, a witnessing trace, i.e., a sequence of symbolic states leading to the violating state, can be used for visualization and debugging."
"if the variable is considered shared, a safe modular generation of the timing model for the task must account for values written to that variable from other tasks. this is necessary, because the values may alter the control flow of the logic in the task and possibly make feasible other execution paths that may yield a higher wcet (or lower bcet) thus rendering the analysis unsafe if these are not considered. to account for this, the per-task analysis automatically introduces fresh symbolic variables for the potentially shared variables. hence, these accounts for all possible values that can be assigned to the shared variables and therefore captures all possible thread interferences. this yields a safe local analysis of the tasks because the symbolic variables over-approximate the values of shared variables and thus the feasible execution paths."
"to compare wcet tools, the mälardalen wcet benchmarks [cit] suite has been put forward and now includes a comprehensive overview of many of the above mentioned tools and their properties."
"faced with these shortcomings, this paper proposes a clustering model called d-dstream based on wavelet network and two-phase density clustering. d-dstream uses sliding window to handle with data, then uses wavelet network to compress data stream in basic window and applies density clustering to process data stream, after that, conducts secondary clustering and updates clusters on the basis of synopsis data. through the analysis of theory and experiment, we know that ddstream model has effective improvement in execution time, memory space, clustering quality and so on."
"most of the mentioned tools analyze binaries, however, the sweet [cit] tool distinguishes itself by translating to alf (artist flow analysis language) and performs flow analysis on this representation. the result of the flow analysis is flow facts containing information about loop bounds and infeasible paths in the program. such flow facts can be used as input for wcet tools such as rapitime and ait wcet analyzer or the legacy lowsweet tool."
"during the process of combining the program nta generated by jpf-symbc-rt with the jvm nta and the hardware nta, optimizations are performed, e.g., for tailoring the jvm nta to the hosting program. for details on the execution environment ntas and optimizations, see [cit] ."
"the connection is created using the run[tid] channel; tid is a task identifier. note that the values of offset, deadline, and period are extracted from the task instantiations in the source code. clock variables corresponding to the target analyses are generated as part of the controller ta, hence enabling wcet, wcrt, and wcbt analyses in this example using the same timing model. the controller ta will enter the executingthread location when the associated task is executing. the clock releasedtime is used to track the relative time from the release point. if it exceeds the deadline of the task, the deadlineoverrun location is entered, otherwise execdone is entered and the process continues when the period is met."
"d-dstream executes incremental processing on data stream, that is, adds new generated synopsis data, removes old data and continuously updates clusters along with the increase or deletion of data stream. this paper uses two-phase density clustering algorithm to cluster data streams, so the building of synopsis data is different from others. it applies wavelet transformation on the data in basic windows and conducts one-phase density clustering to build synopsis data. synopsis data is a structure that can continuously update a representative dataset feature in memory which is much smaller than data scale, d-dstream is used for the approximate processing and online analysis of the synopsis data. the description of the algorithm is as follows:"
", in which c denotes the amount of data and s denotes the area around cure object o. then make comparison of the d-dstream, stream and clustream. the clustering result gotten by stream may be controlled by historical data, while d-dstream algorithm uses sliding window to process data stream, applies wavelet network to compress data stream and builds a much smaller synopsis data structure to save main characteristics of data stream, then clusters with twophase density clustering algorithm, which has a good impact on real-time update of data, solves the shortcoming of stream. as clustream algorithm doesn't eliminate \"old data\" on line and causes the increasing cost of process, while in the process of secondary clustering, d-dstream algorithm processes on the basis of synopsis data and gets new clusters, compare the clusters with origins, the number of data won't increase, on the contrary, d-dstream algorithm may cause the smaller of density threshold and merge some clusters far away, thus will reduce the number of data and get better result."
"the product of translation approach 2, shown in fig. 5, relies on simulating the execution time based on the state of jvm and the hardware. separate ntas are used for capturing the timing behavior of the program, the jvm and the hardware, denoted the program nta, the jvm nta, and the hardware nta, respectively. the interactions among them capture the dependencies between the temporal behavior, e.g., the temporal properties of an arbitrary java bytecode modelled in the jvm nta is dependent on the pipeline and cache state of the processor in the hardware nta. the interactions are well-defined using dedicated (co-)actions and shared variables, making it possible to replace nta components to configure the analysis to the desired execution environment."
"we have presented symrt, a timing analysis tool that uses a combination of symbolic execution and model checking to achieve flexible and tight verification of timing properties of real-time java systems. we have elaborated on the translation of the symbolic execution tree to the nta modeling formalism of uppaal enabling a modular and configurable system timing model."
"symrt is built to accommodate the configuration of the execution environment in terms of hardware and jvm implementation. the latter can be a traditional, software implementation, such as the hvm, or a hardware implementation, such as the jop. parameters related to the temporal behavior of the system are configurable as well, such as the clock frequency of the hardware. the enabling technology for allowing timing analyses is model checking using uppaal. symrt and jpf-symbc-rt are open source and available from https://bitbucket.org/luckow/ symrt/ and https://bitbucket.org/luckow/jpf-symbc-rt."
"there are now several academic implementations of scj, including the ovm [cit], hvm [cit], and java optimized processor (jop) [cit] . the potential is also manifested in commercially available implementations [cit] that have been used in industrial applications from the avionics domain, such as unmanned aircraft control systems [cit], in the modernization of the navy's aegis defense system [cit], and in robot control [cit] ."
"a timed automaton (ta) is a finite state machine extended with real-valued clocks and constraints on clocks. all clocks progress synchronously. a ta can be viewed as a graph; vertices are called locations and the arcs are called edges. locations and edges have an associated set of clock constraints called invariants and guards, respectively. invariants must hold when being in the location and time elapses. guards must be satisfied to proceed to the next location. edges are fired instantaneously and can reset a set of clocks."
"the java programming language has recently received attention in the domain of real-time systems due to desirable characteristics, such as reduced development costs attributed to higher maintainability and productivity when compared to the c programming language, which for long has been the preferred choice in this domain. however, java in its traditional form is unsuitable for real-time systems for many reasons notably due to the inclusion of a (usually time unpredictable) garbage collector. to address this, the java community, through jsr 302, has made tremendous progress and an important step has been taken with the upcoming safety critical java (scj) profile [cit] making java a viable choice for development of embedded real-time systems. the scj profile introduces important real-time concepts such as high-resolution clocks and timers, but it also affects the programming model, e.g., by the introduction of a scoped memory model to avoid the need for a garbage collector. in addition, scj has a sufficiently tight thread semantics and a programming model based on tasks grouped in missions, all contributing to facilitating verification of real-time properties."
"many model-based timing analysis tools such as meta-moc [cit] for executables, and wca [cit], tetaj [cit], sarts [cit], and tetasarts [cit] for java rely on processing the program to build a cfg and enumerating the execution paths over the cfg. the paths are then translated to a modeling formalism, e.g., timed automata, and timing analysis is then formulated as a reachability problem using an appropriate logic such as tctl. the temporal behavior of complex hardware features such as caches and pipelines can be modelled accurately as well. while these tools provide a safe approach, they are overly conservative, which is mostly a consequence of disregarding data in the reconstruction process."
"virtual method invocations also complicate cfg-based approaches since all possible callees must be considered unless the reconstruction is complemented with additional analyses such as rapid type analysis [cit] that attempts to reduce the set of possible callees. when using symbolic execution, symrt does not need to be complemented with such analyses, since the stack is part of the symbolic state, hence the callee is known at every call site of every execution path."
"as c is the predominant programming language, most tools target this or binaries produced from c, e.g., the ait wcet analyzer. ait is an abstract interpretation based state-of-the art industrial strength tool. the tool performs a per-task wcet analysis of binaries for a given execution platform taking the intrinsic cache and pipeline behavior into account. ait also takes into account user annotations such as upper bounds on loop iteration counts, recursion depth, targets of indirect function calls, etc. the resulting wcet can be used as input for traditional schedulability analysis, however, as the ait tool assumes that a task is executed sequentially and uninterrupted, it more or less prescribes a cyclic-executive schedule, as the tool assumes no interference."
"the ts subscript denotes that a timing scheme with fixed execution times for all the java bytecodes has been used instead of modeling their behavior as an nta. for this set of experiments, we used an application server with an intel xeon x5670 @ 2.93 ghz cpu and 32 gb of ram. the results are shown in table 3 . in all cases, the systems have been deemed schedulable, and the results show that the analysis times and memory consumptions are lower when using symrt. we also tried, e.g., rtsm with hvm+avr, but the complexity of the resulting models, regardless of the tool used is too big, which can be attributed the jvm nta, which largely dominates the complexity. in this case, uppaal runs out of memory."
"where whales are at risk: a mariner's 'cone of concern' figure 6 depicts the results of simple vector analysis demonstrating how a whale's swimming speed and a ship's transit speed influences the width of the cone of concern. figures 6a,b depicts ships traveling at 10 knots (5.14 m/s) and 19 knots (9.77 m/s) on a collision course (toward poc) with a humpback whale swimming at a typical speed (1.23 m/s; 6a) or at a fast swimming speed (2.47 m/s; 6b) perpendicular to, and toward, the ship's path. for both scenarios, the opposite angle in the right triangle (defined by the ratio of the whale's swimming speed and the ship's travel speed) is maximized because the whale is in a 'crossing' situation; i.e., it is headed directly toward the ship's path resulting in the shortest time for potential collision. for the fast ship/typical whale (6a) scenario, the cone of concern would be approximately 14 degrees (7.2 degrees on either side of the ship) and encapsulate a search area of nearly 0.8 km 2 . for the slow ship/fast whale scenario, the cone of concern has a nearly equal search surface area (0.84 km 2 ) even though the search area is much wider (∼51.2 degrees)."
"the maneuver process is the time it takes for the ship, once commands have been ordered, to achieve the desired operational state. the maneuver process is also best considered in the context of a time lag because the new commanded operational state does not occur instantaneously. the maneuver process can vary dramatically among ships although approximate generalizations are appropriate for estimation and/or simulation scenarios. similar to other large ships, safe maneuvering of large cruise ships encapsulate a range of turning and slowing options based on the interaction between ship type, existing operational state, and environmental conditions. our experience, based on informal sampling of whale avoidance maneuvers during the past several summers in best-case scenarios, has been that the maneuver process can vary from 25 to 180 s depending upon operating conditions and the type of maneuver ordered."
"in the search node formalism, the start and end times of each operator instance in actset are the start and end times of the unifying primitive task plus. all parent task pluses of the generated operator instances and task pluses in taskplusset constitute the hierarchical task decomposition structure associated with the current search node. this structure can be extracted from the common enhanced hierarchical task network. the extended stn will be introduced in section 5."
"first, when simulations of whale encounters were conducted in the full-mission ship simulator, pilots never attempted to slow the ship in response to the sighted whale, instead preferring slight changes in course. in the de-briefs that followed, pilots communicated that, while change in speed may influence the dynamics of a whale -ship encounter, the distance necessary to slow the ship to speeds necessary for effective avoidance based on speed change alone (mariner body of knowledge relative to vessel avoidance actions) tended to exceed the sighted distance to the whale owing to potential unsafe results of rapid speed changes. given the absence of this response, we did not produce simulations that contrasted the efficacy of slowing the ship vs. [cit] simply to provide context with regards to the magnitude of space/distance needed to slow (and recognizing that the target speeds listed in nash were arbitrary relative to whale avoidance). however we feel a brief description as to why rapid changes in speed are not regularly practiced is necessary owing to its prominent relevance in ship strike dynamics and context for understanding the estimates in table 1 ."
"to date, most management efforts aimed at reducing ship strike risk have focused either on modifying shipping lanes, which can reduce the relative and absolute risk of strikes by reducing spatial and temporal overlap between ships and whales [cit], and/or reducing ship speed, which may reduce the probability of a collision [cit] or the likelihood of mortality should a collision occur [cit] . yet each of these approaches has limitations. modifying shipping lanes will only be as effective as the spatial persistence of whale aggregations, can require considerable regulatory effort, or may be impractical in narrow straits or for ships arriving into ports of call [cit] . speed restrictions can generate resistance from the shipping industry owing to economic implications of the additional at-sea time that results from lower speeds, particularly when applied over large areas, which may be one reason voluntary reductions in speeds tend to have low compliance [cit] . regardless, whales can be notably unresponsive to approaching ships [cit], and thus any action that facilitates the avoidance of whales by mariner training and active avoidance techniques (lowering the reliance on whales to avoid ships) are important to develop."
"the development of a whale avoidance module in a fullmission ship simulator can also advance whale avoidance by training mariners, through repetition and experimentation, who have less experience with conditions where whale avoidance may be effective, avoidance techniques, and range of maneuvers that may be possible. training modules can also lead to improved transit planning by scripting exercises within a specific operating area where whales are likely to be encountered while also accounting for local environmental conditions (e.g., wind, current, sea state), traffic situations, and other navigation hazards commonly experienced (e.g., ice)."
"according to the abovementioned concurrent controlling mechanism for ensuring parallelism of multiple planned activities, all the cause-effect relationships are discovered and encoded by the stn when primitive tasks are applied. as a result, the generated actions are arranged properly and do not interfere with one another."
"the command and compliance lag is the time needed for the pdmv to articulate the avoidance decision into a specific command and for the bridge team to comply. for example, the pdmv may command the helmsman to initiate a new heading. for some shipping entities, the bridge procedures require 'closed loop communication' whereby the command cannot be executed until the initial order is first acknowledged (by the quartermaster 3 https://www.navcen.uscg.gov/pdf/navrules/cg_nrhb_20151231.pdf for course changes, or by the deck officer for speed changes), and then confirmed by the pdmv. this lag is generally 5-7 s in situations where all involved understand and are in agreement, but can be longer (upwards of 60 s) if the command is misunderstood, not heard, not acknowledged, or there is disagreement on the appropriate avoidance action."
"the detection lag represents the time between when a bridge team member detects an object in the water, confirms its identity, and formulates their report to the pdmv. based on anecdotal observations from marine pilots aboard large cruise ships in alaska, this lag was estimated to vary from 1-2 s, as when a whale spout is immediately recognizable, or as much 5-10 s if the nature of the perceived object is not readily apparent (e.g., a whale lying motionless on the surface or a floating log?). what's more, many lookouts (personnel assigned to view the waters forward of the ship) are trained to simply make a report of an \"object in the water, \" if they cannot readily identify what it is, and then continue to observe the object to develop clarifying information."
"understanding the opportunities for, and feasibility of, active whale avoidance also serves to benefit mariners by clarifying conditions and actions that may facilitate effective whale avoidance. for example, large ship operators undergo years of training, including frequent maneuver testing in full-mission bridge simulators, which are often focused on collision avoidance with objects including reefs, shoals, and other vessels. yet we know of no simulator modules for whale avoidance, which would provide opportunities for mariners to learn from others and test new ideas for maneuvering, particularly if they incorporated state-of-the-science information pertaining to whale behavior."
"our simulations were limited in number as was our bridge team size, which would normally include a dedicated lookout and one or more deck officers. thus, we did not draw inferences on detection probability from the simulator. additional limitations existed due to the lack of fidelity of the simulated whale/cues which are the subject of further refinement and improvement. nevertheless, the descriptive data on timeto-command and archive of commonalities that influenced decision-making were appropriate as full-mission simulations are regularly used to describe processes that occur on the ship's bridge, and can serve as realistic proxies for evaluating risk and commanding new operational states [cit] ."
"our conceptual model lists a series of steps that we have termed the command and maneuver processes. the command process consists of detection, reporting, assessment, decision, command, and compliance actions best described as time lags because any time that elapses after a whale is detected reduces the ship-to-whale distance (as the ship moves toward the whale in the scenarios modeled) and decreases the options for an avoidance maneuver to occur. the maneuver process represents the time it takes for the ship, once commands have been executed, to achieve the desired new operational state. we describe these steps in more detail below."
"in this part, the key procedures involved in the main planning algorithm are presented, and the processes of providing specific functions by the presented decision-making model are introduced."
"in its simplest terms, the process of active whale avoidance can be described as occurring in five sequential events (1) a whale surfaces somewhere forward of the ship where a collision with the vessel is possible; (2) bridge personnel tasked with ship navigational decisions detect the whale; (3) the pdmv evaluates the situation and decides that an avoidance maneuver is necessary, feasible, and safe; (3) the pdmv decides upon and commands a new operational state such as a change in course, speed or both; and (5) the ship obtains a new operational state resulting in a lower risk of a collision. our conceptual model (figure 1) includes observational (whale surfacing behavior, detection) and operational processes (commands and maneuvering) that are structured sequentially. each of these components are described in more detail."
"the decision lag represents the time needed for the pdmv to consider the available safe avoidance options based on competing risks. the decision by marine pilots (serving in the capacity of pdmv) is founded on the principle of do-no-harm, firstly to people, secondly to the ship, and thirdly to the environment. in practice, this results in a rapid and dynamic calculation of the trade-offs in the risk of whale collision with the risks of harm to people, the ship, or the environment (or some combination such as a collision with another ship, a shoal, or even another whale). consequently, critical factors in the decision lag are based on the situational awareness of the pdmv to the proximity to these hazards and the operational state of the ship; i.e., what is in the realm of possibility based on its speed, sea state, etc. based on opportunistic assessments, decision lags can vary from a few to 20 s, based on complexity and competing risks."
"despite the larger scale of the underling stn in our model, our decision-making model performs better than the xeplanner in planning time. the main reason for the reduction in planning time is the high performance of incremental temporal management method, which propagates time constraints on semi-stn introduced in section 5. the method is more efficient than the pc-2 embedded in xeplanner when propagating temporal constraints. this finding is also verified in section 7.4. the plan metric values of produced action plans are the same, because the similar state-based forwarding and non-backtracking algorithm is used to expand the search space in these two planning paradigms. in fact, our decision-making model is developed by extending xeplanner to provide temporal flexibility and higher computational performance. the experimental results demonstrate that our decision-making model is practical to solve real-world emergency response planning problems in such short time. that is very important during emergency command operation, where decisions should be made quickly."
"once a decision is made on an appropriate avoidance maneuver (maintaining existing operations may also be an active decision; see section discussion), the rapidity by which the new operational state is achieved can vary dramatically among ship types (e.g., bulk carriers vs. tankers vs. passenger vessels) and within similar-type ships based on technical features such as hull shape and maneuvering systems (e.g., [cit] . further, variation can occur based upon environmental conditions (e.g., [cit] and/or the existing operational state of the ship (wave height, wind, ship's existing speed, acceleration/deceleration, whether or not the ship is already engaged in a turn, etc.; [cit] ) . consequently, determining how quickly a ship can achieve an avoidance maneuver is well beyond the scope of this paper (although our simulations were insightful for which factors should be prioritized for further development)."
"  is a set of task pluses called child task pluses used to achieve the given task i task with the variable array k varbindarray . otherwise, the variable records the operator to achieve the primitive task plus with this variable array. according to the abovementioned definition, the enhanced hierarchical task network records the traversed search track by decomposing the initial incident objectives. obviously, the track can be used to extract the abovementioned emergency response plan. an example of the enhanced hierarchical task network is shown in figure 2 to describe its structure. as shown, the icons in this figure represent the variables in definition 6 and 7. cognitive control [cit], which challenges the decision-making capabilities of emergency managers. the planning process is a computational model for providing cognitive-level support to the development process of emergency response plan, which involves a sequence of decision points as shown in section 2.1. this section introduces the main algorithm and key procedures of the planning process."
"lethal collisions between large ships and large whales (ship strikes) are a recurring and common threat to whale populations across the globe [cit] . in some cases, such as with the critically endangered north atlantic right whales [cit], and an important sub-population of sperm whales in the canary islands [cit], ship strikes have direct implications for population persistence and biodiversity. in other cases, such as with the population of blue whales in the eastern north pacific, ship strikes do not appear to regulate population dynamics given the frequency of (known) ship strike mortalities [cit], although the number of detected collisions may be an underestimate of the true number that occur [cit] . regardless, management agencies and the general public value large cetaceans and seek effective ways to reduce ship strikes, even when population persistence is not at stake [cit] ."
the role of the abovementioned structure maintains an instance of htn exploration for the current planning problem and provides guidance for emergency managers to direct and supervise the response efforts.
"the emergency response business process represents the execution flow of the emergency response plan. in the processs, the planned activities represent the execution process of all operation instances in the generated search node (section 2.3). the order relationships between the activities are encoded by the semi-structure stn. the process of extracting elements of the emergency response business process is introduced in this section. the order relationships between tasks are defined by the semi-structure stn if their start and end time points are in the same time point cluster. that is, one task is ordered to another task if the end time point of the former task is before the start time of the latter one. the ordered relationships between tasks are also defined in different levels. if one task is ordered to another task, all planned activities achieving the former task are before those achieving the latter one."
"(2) the duration of the operator cannot be pre-determined. instead, the enhanced operator provides terms to represent the start and end times of the response activities, which cannot be estimated exactly because of the uncertainty in the emergency response process."
"responding to large-scale emergencies is typically characterized as dynamic, highly time-dependent, and subject to considerable uncertainty. these characteristics determine the context and constraints in developing and deploying emergency response plans. these plans should also consider the enduring characteristics of the application domain to provide suitable support for the emergency command operation. based on the analysis of planning for emergency response, the following requirements are necessary."
"temporal constraints should be considered while developing emergency response plans. first, incident objectives with deadlines must be achieved before emergencies result in disastrous effects. second, partial order relationships between response tasks without detailed information on durations [cit] are represented and handled explicitly to synchronize and coordinate the response work of multiple responding organizations. complex synchronization, such as deadlines or temporal landmarks of response tasks and other synchronization schemas between them, also defines temporal constraints that should be taken into account while planning for emergency response. by contrast, causal-effect dependencies between response tasks carried out by multiple responding organizations imply temporal constraints between them. consequently, multiple types of temporal constraints underlying htn should be encoded and handled explicitly."
"an instance of a temporal enhanced operator is shown in figure 1 . this knowledge formalism describes an activity which drives a vehicle ?t from location ?loc-from to location ?loc-to. unlike the operator in pddl 2.1 [cit], the duration of this operator is not pre-defined. in addition, the effects occurs at any time during the execution interval."
"(1) the extended task node in the temporal refining task network is defined as therefore, all elements in the temporal refining task network can be extracted from the generated search node curnode and the enhanced hierarchical task network enhietasknet in the presented decision-making model."
htn planning and temporal constraint propagation provide the theoretical basis for the designing of the presented decision-making model in this paper. this section discusses literatures relevant to this research.
"in a simulated emergency situation, the typhoon will arrive in 12 hours, the water level at station 1 in location loc 22 is 23 meters, and the water level at station 2 in loc 16 is 28 meters. emergency managers in eoc locating in loc 7 identify two incident objectives by assessing the emergency situation. one objective with the priority \"1\" describes that residents in com-a should be evacuated to the assigned shelter within 18 hours as the deadline. the other objective with priority \"2\" describes that residents in com-f should be evacuated to the assigned shelter within 24 hours. the emergency response plan automatically generated by the presented decision-making model is shown in figure 6 and figure 7, and achieves the identified incident objectives in the given emergency situation. the temporal refining task network of the plan is presented in figure 6, where the objective decomposition structure is described clearly. the yellow rectangles represent compound tasks, the red ones represent primitive tasks, and the blue ones represent planned activities. all the planned activities and order relationships are represented by the emergency response business process as shown in figure 7 ."
"our decision-making model has outperformed sapa in almost all problems, in both the planning time and the plan metric value of generated action plans. compared with sapa, our model provides high expressiveness and temporal flexibility for emergency response domain. despite the additional computation overload of our model for handling time constraints, the planning time is short in almost all problems except problem 2. according to the literature, sapa is capable of handling the multi-objective nature of metric temporal planning to generate optimal tang p., shen g.q.p. (2015) action plans [cit] . however, the plan metric values of produced action plans generated by our planner are more optimal in all problems than that of sapa. these results demonstrate that our planner can handle preferences effectively during planning process."
"the immediate previous action plus set . two virtual action plus start and end, which represent the source and sink nodes respectively, are added to the business process. this addition is inspired by the workflow definition. the variable . start prev is initialized by an empty set. the variable . start succ is initialized by all the first planned activities. moreover, the virtual action plus start is added to the immediate previous action plus set of all the first action pluses. the variable . end succ is also initialized by an empty set. the variable . end prev is initialized by all the last planned activities. moreover, the virtual action plus end is added to the immediate successor action plus set of all last action pluses. hence, the execution process emergency response business process starts execution from the virtual action plus start and terminates at the virtual action plus end ."
(1) the instantaneous precondition of operators can be satisfied before the responding activity starts or at any time during execution. the invariant conditions in operators are also formalized explicitly and describe the conditions that should be satisfied in any interval of the execution process.
"(1) expressing decomposition structure of incident objectives based on the planning procedure for emergency response, the identified incident objectives are decomposed step by step, until all obtained tasks can be disseminated to the responding organizations for execution. tasks generated during the planning process are either performed by one or multiple task forces from geographically dispersed responding organizations. each task represents an essential element to achieve the incident objectives with available resources in the current situation, possibly via the collaborative execution of several sets of subtasks by more than one responding organizations. therefore, the hierarchies are clearly the structural characteristics necessary to understand and conceptualize the emergency response plan. related to levels in the emergency response organizational structure, emergency response plans should reflect alternative means to achieve high levels of tasks. the hierarchical task structure is suitable for expressing the means to achieve the joint intentions of emergency managers; these intentions should be achieved by multiple responding task forces in different incident locations [cit] . if the task can be represented, identified, and analyzed properly, then developing coordination mechanisms to manipulate non-local tasks based on the task features is possible [cit] . this point of view on emergency response plans defines the following requirement."
"the key procedures in the presented model, such as expanding the search space, discovering the cause-effect relationships between tasks, and concurrent controlling mechanism of multiple planned activities, are novel and will be introduced in the following section, and are different form those in xeplanner [cit] ."
"we utilized the navigation software seaiq, a commonly used platform by pilots across the u.s. for understanding ship maneuverability, and to focus on a simple and achievable question: for a typical large cruise ship traveling at 10 vs. 19 knots, how far in advance must a turn be initiated to achieve a cpa of at least 100 m with a stationary whale while remaining within defined safety parameters? we used a stationary whale because it simplified the vectors and isolated the focus on the maneuvering capacity of the ship. the 100 m cpa was also for simplicity purposes and should be viewed simply as a means to estimate maneuverability, not as a recommended cpa for mariners. we did not introduce confounding factors, instead simplifying the simulation to reflect 'best case scenarios' including unlimited visibility, calm water, no wind or current, deep water maneuvering, no other vessel traffic or whales, and the ship was initially traveling in straight line. our defined safety parameters were guided by our working history of the ship's safety parameters and a generalized pilot card describing the ship's sensitivity to heading changes (e.g., maximum rate-of-turn; rot) at varying speeds, as well as limitations in stopping distances. the turn, based on non-emergency safety parameters, conservatively did not exceed a 10-degree rate-of-turn and did not factor in progressively higher rates of turn."
"time constraints are an enduring characteristic that should be addressed during the planning for emergency response in the emergency response domain. the domain knowledge can be applied to speed up temporal constraint propagation with structural information and prune unnecessary propagation effort. lin proposes a new algorithm called stp , which triangulates stn and propagates time constraints, to achieve high performance of computing minimal network of stn [cit] . this algorithm provides a new perspective on temporal problems, which are composed by a set of triangles, two of which are connected if they have one common edge. therefore, constraint propagations can be performed according to this new graph of triangles. inspired by this algorithm, we design our temporal constraints propagation algorithm based on triangles in stn. yorke-smith first exploited the structure of an htn plan in performing temporal propagation on an underling stn [cit] . the introduced algorithm, sibling-restricted propagation (sr-pc), transverses a tree of sub-stns that corresponds to the expansions in the hierarchical task network. the considered stns, encoding constraints between parent and child task nodes and between sibling ones, are smaller compared with the stns corresponding to the entire plan. it demonstrates an order of magnitude improvement. however, it doesn't operate on general temporal networks generated by planning process and cannot propagate time constraints incrementally. instead, in the sr-pc algorithm, all time constraints are given at the beginning. as a result, the sr-pc algorithm cannot be embedded in htn planning paradigms to assume time propagation function directive. fusun presented temporal milestones in hierarchical task network to enable the complex synchronization of tasks and introduced an efficient temporal reasoning algorithm called the d-pc [cit] . this algorithm propagates temporal constraints incrementally without re-computing the stns induced by htn and is a generalized version of sr-pc. however, this algorithm cannot handle temporal propagation in htn planning process. planken proposed ippc algorithm to solve sparse stns incrementally by enforcing partial path consistency [cit] . however, the ippc algorithm needs to triangulate the stn before propagating time constraints and is designed to handle general sparse stn without domain knowledge. finally, the prop-stp algorithm applies variable elimination to exploit the tree-decomposition method in which messages are represented compactly as sub-stns and an efficient message passing scheme is designed to compute the minimal constraints of sub-stns [cit] . however, this algorithm requires the stn to be triangulated and operates over the set of maximal cliques of the triangulated constraint graph. the time complexity of prop-stp is"
"( t, then the negative timed predicate should occur after this invariant condition terminates. that is to say, the time constraint t is ordered to time point 1 t .)"
"the input of the planning algorithm is the initial planning state representing the emergency situation at the beginning of the emergency response and all the identified incident objectives to be achieved. similar to shop2, this planning process is a state-based forward and proceeds as the enhanced hierarchical task network is expanded or the planning state is refreshed. the planning process is also advanced to expand a common search space consisting of search nodes with a special structure. the detailed definition of a search node is listed as below."
"active whale avoidance has been developed and successfully practiced for decades by marine pilots in alaska (and possibly elsewhere) and is not new in the maritime community. however, a more formal exploration will help clarify (1) the development and application of these techniques by other mariners, (2) the regulatory language that makes implicit or explicit assumptions about a ship's ability to avoid whales, and (3) important research questions with regard to the efficacy and effectiveness of different maneuvers under varying operational and environmental conditions. for example, the u.s. code of federal regulations (50 cfr §224.103) states that it is illegal to approach [north atlantic] right whales closer than 500 yards (457 m) with some exceptions for vessels 'restricted in her ability to maneuver.' in alaska, federal regulation dictates that all vessels must operate at a 'slow, safe speed when near a humpback whale' (50 cfr §223.214) which assumes that the ship can take proper and effective action to avoid collision when near a humpback whale or that ship operators have advance knowledge of where whales are located. 36 cfr §13.1170 stipulates that a vessel in glacier bay inadvertently positioned within 1/4 nautical mile of a whale must \"immediately slow the vessel to ten knots or less without shifting into reverse\", and \"direct or maintain the vessel on as steady a course as possible away from the whale until at least 1/4 nautical mile of separation is established\" -requirements that were largely established pertaining to smaller craft and may be unattainable by large ships."
"the first step in this process is dictated by whale behavior because whales need to be available for detection at the surface in order to be avoided. the availability and detection processes have been well studied owing to its relevance for abundance estimation (via distance sampling), and we refer to these studies for describing factors that influence cue frequency and behavior [cit] . gray, blue, and humpback whales (among many others) regularly embark on a cycle of surface intervals, consisting of several shallow submergences between respiration/surfacing events, punctuated by longer deep dives (e.g., [cit] . consequently, whales are infrequently but regularly available to be detected. in general the most frequent cue available during a surfacing event takes the form of the appearance of the whale's body in concert with a vertical spout/blow, which is composed primarily of water vapor, air, and lung mucosa, that may extend to several meters above the water and persist for several seconds."
"in classical htn paradigms, the planning state is represented by a set of predicates that assume that these propositions are true, while propositions not in the classical planning state are false. in this section, an enhanced planning state is proposed for recording the time when the predicates are generated or triggered. therefore, the temporal enhanced predicate is also called the temporal predicate. the positive and negative predicates are all recorded in the enhanced planning state. if the enhanced predicate is a positive one, then the predicate p is added to the world state after being triggered. otherwise, the predicate p is deleted from the world state. the abovementioned analysis shows that the temporal predicate has the following form. records the time when the predicate is generated. during the planning process, temporal predicates are given by the initial planning state or generated by the execution effects when the operators are applied to achieve the primitive tasks. therefore, if a predicate is specified in the initial state, then the variable tp is initialized by tr representing the reference time of the emergency response process. otherwise, the variable tp is specified by the formula t delta  . if the symbol \"  \" is given, then variable t represents the start time of an operator; otherwise, it represents the end time. the variable delta is the time interval relative to the start or end time when the execution effects are generated. as a result, the temporal enhanced planning state consists of a set of temporal enhanced predicates. unlike traditional planning paradigms [cit], the abovementioned planning state assumes that the properties of the world change over time and records the time when these properties change. instead of a single \"global time,\" multiple \"local times,\" which are the time stamps of each predicate, are recorded in the enhanced planning state. they provide basis for discovering the cause-effect relationships between the planned activities and the design of concurrent controlling mechanism to ensure parallelism of multiple planned activities. the details will be introduced in section 4."
"obviously, unlike the causal links between two operators in pocl [cit], the presented cause-effect relationships are temporal and are defined on two time points. as a result, the temporal causal-effect relationships are more precise because the time points can be any time point during the execution interval of operators. the relationships provide an expressive representation and exploit the greatest level of concurrency between planned activities. moreover, unlike siadex [cit], which only defines causal-effect links between planned activities at the lowest level, the abovementioned cause-effect relationship formalism represents the causal structure between the tasks in different abstraction levels. in addition, when a cause-effect relationship"
"as part of the simulations conducted in the full-mission ship bridge simulator at avtec, the average elapsed time, resulting from the aggregate of the command time lags, i.e., from detection of the simulated whale spout to initial compliance with an ordered avoidance action, was 23 s. [cit] . during the debriefing meetings, we found that, following initial detection, uncertainty in the whale's direction of travel and swim speed were common factors that contributed to the delay in a command; the pdmv needed sufficient confidence in the information (making it 'actionable'), particularly on whether the whale was swimming toward or away from the ship's heading. consequently, the pdmv regularly communicated with the lookout and mate and, absent good information, waited for a subsequent surfacing event before deciding on an appropriate avoidance action."
"throughout our effort we consistently contrasted scenarios involving fast (19 knots) and slow ships (10 knots) to explore how speed may influence the constituent processes in active avoidance of a single whale surfacing near the ship. while our objectives were not to rigorously test the role of ship speed in these processes, nor were they to identify an optimal speed that balances whale avoidance vs. transit efficiency (should one exist), we highlight some insights based on our results that warrant discussion and further development."
"finally training can assist in communicating the value of whale avoidance to other members of the bridge team, such as the ship captain/mates. pilots in southeast alaska have found that, upon boarding the ship, communicating with the bridge team that whales may be encountered, emphasizing the importance of whale avoidance, and discussion of avoidance techniques has increased situational awareness of whales while in transit (similar to communicating local knowledge of navigational hazards) and, importantly, often reduced resistance to implementing proactive avoidance maneuvers or temporary reductions in ship speed. a recent study in the st. lawrence estuary demonstrated the value that marine pilots can have in implementing strike-risk reduction efforts, in part, through elevating its importance for the larger bridge team [cit] ."
"when all the time constraints mentioned above are added to the stn and remain consistent, the current plan-refining steps can be executed. the stn is a framework widely used for checking temporal consistency and deriving the minimal network [cit] . aiming at improving the effectiveness of time management, the traditional stn is extended to encode all time constraints and hierarchical decomposition structures induced by the htn planning process. first, the time point cluster structure is recorded to divide the underlying stn into multiple smaller subnets. second, the cause-effect relationships defining on two time points representing the interdependencies between different tasks are represented explicitly. the definition of the extended stn is stated as below."
"tang p., shen g.q.p. (2015) are generated and added to the extended stn when a compound task is decomposed. third, the time constraints of p-type1 and p-type2 are generated and added to the extended stn when a primitive task is applied."
"(1) decomposing compounded tasks. when a compounded task t with no predecessors is selected to be refined, the unifying methods are used to decompose it. first, the preconditions of each branch are evaluated individually on whether they are satisfied by the current planning state. if they are satisfied with a variable binding array, then the generated cause-effect relationships are added to the variable . exstn caueftset, which records all cause-effect relationships between these two time points. all subtasks in this branch are added to the variable enhietasknet . their time points and the underlying time constraints are also added to the extended stn exstn ."
"the abovementioned emergency response plan also describes the incident objective decomposition structure generated by the emergency response plan developing process. second, the plan structure represents decision contexts explicitly. this representation enables emergency managers to associate monitors to different abstraction levels of tasks and detect situations that may invalidate the current plan. third, the start and end times of the planned activities are not pre-determined until execution to enhance the temporal flexibility of the entire plan. the following content introduces the design of a decision-making model that will generate this plan structure based on integrating htn planning and scheduling technologies."
"finally, clarifying research needs and models derived from active whale avoidance will help scientists prioritize and/or refine existing efforts that will have tangible conservation outcomes and assist mariners in applications of these concepts. for example, a suite of efforts currently exist to facilitate mariners sharing information on whale sightings yet it's unclear how well these sightings equate to changes in maritime operations and, ultimately, whether certain factors, such as the way the information is transmitted or when its received by the operator, equates to a reduction in ship strikes."
"the study takes full advantage of the hierarchical decomposition structure to propagate time constraints on the underling stn. a new time propagation algorithm, called prop-h-stn, is proposed and triggered once the time points and their underling time constraints are added incrementally. the prop-h-stn algorithm is triggered in three cases during the planning process. first, time constraints of c-type1 are generated and added to the extended stn when the precondition in the method is checked. second, the time constraints of c-type2, c-type3, and c-type4"
"larger-scale emergencies are highly volatile. they change quickly and can have unpredictable consequences. these enduring characteristics determine that effective emergency management requires inter-organizational collaboration and needs to adapt quickly to dynamic situations [cit] . fast and dynamic decision making is necessary when making decisions within deadlines. therefore, planning for emergency response is ubiquitous in nature. the action plans evolve as situations change and new information is received during emergencies. in our future work, we intend to design an integrated planning and execution model [cit] based on this novel emergency response plans structure as support for the development, deployment, and repair of plans. it aims to provide support for emergency command operation. this approach will allow quick and effective response to changes in the environment, which has been advocated by many researchers."
"for large cruise ships (and likely other large ships) power management plays a major role in operational decision-making (e.g., [cit] ), not just in the context of managing fuel costs and optimal fuel efficiency (and resulting levels of air pollution; [cit] ), but also for safety reasons. for large cruise ships, power needs are met using multiple engines that are variably configured for two different power loads including the propulsion load, which is typically about 80% of total load while transiting, and the hotel load, which is the electrical energy needed to power the ship's lights, heating/air conditioning, galley, etc. rapid changes in power use can negatively affect emissions, damage the generators (engines) and, in a worst-case scenario, cause a 'blackout' (total loss of power). to help guard against these negative outcomes, large cruise ships typically have some form of power management system, such as a 'load control program' that limits dramatic fluctuations in power use. given that propulsion is the primary power requirement, and that propulsion is a function of the propeller's rotations per minute (rpm), as a general rule, when a large cruise ship is in load control the propeller rpms are generally not reduced by more than 2-3 rpms per minute. consequently, gradual changes in speed represent best load management practices and the gradual change may not meet the more immediate change in operational state necessary for avoiding a whale."
"this paper is organized as follows. section 2 introduces a novel structure for an emergency response plan that accounts for the emergency command operation requirements. section 3 introduces the knowledge formalism of the decision-making model. section 4 presents the planning algorithm. section 5 lists an incremental temporal management method embedded in the proposed model. section 6 presents the method for extracting the emergency response plans. section 7 introduces an empirical study in a typhoon evacuation domain, as well as the relevant experimental results. section 8 discusses the related work on htn planning and temporal constraints propagation. finally, section 9 concludes this study with a discussion of its contributions and future work."
"this section proposes the knowledge formalism of our decision-making model based on ai planning technologies, such as the temporal enhanced operator, temporal planning state, and enhanced hierarchical task network. they provide basis for the design of a planning algorithm that will produce the abovementioned emergency response plans. the other knowledge formalisms are similar to the existing htn planning paradigm xeplanner [cit] ."
"prop-incremental-pc2( edstn, tightentcset, i c ); 8: end if 9: end for each 10: end function in each iteration, a triangle is selected and computed (line 10). in line 11, one edge of triangle  is tightened and updated similar to pc2 algorithm, or all three edges are tightened at once compared to the stp  algorithm. given that each tightened edge represents a time constraint, their associated triangles are added to the variable q . they can be added to the front, the end, or any position in the queue. the manner in which the triangles are inserted in the queue also affects the performance of this temporal propagation algorithm. the experiments will be introduced in section 7. finally, the algorithm terminates, and a minimum sub-stn is obtained once the variable q is null."
"traditional action plans in existing htn planning paradigms cannot satisfy the aforementioned design requirements. that is the impetus of the current study to extend the action plans with enriched syntax and semantics to represent the emergency response plan. in this section, the hierarchical task network generated during the planning process is recorded to express the structure of the incident objective decomposition. the conditions are attached to htn to encode the decision contexts of multiple decision points during the emergency response plans developing process. finally, unlike action plans with determined start and end times, stn is integrated into the action plans to represent synchronization among the planned activities and encode all the underlying time constraints to provide temporal flexibility. the extended emergency response plan structure is listed as follows."
we formalize this idea using simple vector analysis and a trigonometric representation of a whale crossing a ship's path at a 90-degree angle. we contrasted ships traveling at 10 knots (5.14 m/s) and 19 knots (9.77 m/s) with whales swimming at an average speed of 1.23 m/s (2.4 knots) and at fast swimming speeds of 2.46 m/s (4.8 knots) to explore how these parameters influence the size of the cone of concern.
"the third potential way to facilitate the effectiveness of active whale avoidance is by reducing the time identified in the command process. pilots in alaska are regularly conveyed unnecessary or incomplete information by members of the bridge team following a whale sighting. if the information is incomplete, the pdmv may have to wait for another surfacing event before having information of sufficient quality to be 'actionable.' this may equate to the ship traveling several hundred meters closer to the whale (based on average submergence data and typical transit speeds in alaska) before the pdmv can confirm the whale's location and direction of travel. training bridge personnel with regards to what information is desirable and protocols for communicating that information (e.g., ' [cit] m three points to starboard, moving away from the ship') can make a significant difference in time available for pdmv to assess the situation and implement the maneuver without further increasing the risk of harm to the people, the ship, or other components of the marine environment. a simple suggestion of utilizing the same training used for reporting of a man-overboard to continuously point to the person (whale) promotes the effectiveness of the pdmv's detection and decision process significantly."
"an operator describes the responding activities performed by task forces, such as searching for victims, firefighting, transporting evacuees, and other tasks. according to the design requirements in section 2.2, the responding activity is executed by a complex process carried out by responding organizations. hence, for adapting to the characteristics, a temporal enhanced operator is proposed by extending operator formalism in the existing htn planner, such as xeplanner [cit] and shop2 [cit] . the extensions are listed as the following:"
"our goal is to present a conceptual model of active whale avoidance derived by coupling perspectives from biologists, focused on the science of whale behavior, with the expertise of ship operators. to that end, our research team included alaska marine pilots with over 90 years of combined experience developing and practicing active whale avoidance while piloting large ships. as proof of concept, we collected and applied data to our conceptual model focused on avoidance of humpback whales by large cruise ships transiting waters in alaska. data informing our conceptual model originated from (1) [cit] focused on quantifying surfacing behavior of humpback whales around the ships and the ability of mariners to detect them; and (2) data collected during trial simulations in a fullmission bridge (ship) simulator to identify and quantify the practices that occur on the ship's bridge during active whale avoidance. large ship maneuvering capabilities were further explored using seaiq, a navigation software commonly used by marine pilots to navigate and assess maneuvering possibilities 1 . although our work is focused on a specific type of ship (large cruise ships) and single species of whale (humpback), variations of the components of our conceptual model can be applied to whale avoidance by other types of ships and other types of whales."
"note that, based on our estimates of the command and maneuver lags (see above), both the slow and fast ship would have limited (if any) opportunities to avoid the whale if it went undetected during the first surfacing interval (green shaded areas) because 40 s to collision (when the whale surfaced from its dive) exceeded the aggregate time to implement these processes."
"large-scale emergencies constantly result in disastrous consequences. the emergency response process is beyond specific organizational boundaries and involves efforts from various functional departments, such as the police and fire departments, medical corps, military, civil organizations, and multiple jurisdictions [cit] . responding organizations focus on individual efforts, and may be unaware of the response activities taken by others. the allocation of resources to certain tasks may result in these resources becoming unavailable for other tasks. therefore, disastrous situations present complex interdependencies and conflicts among the response tasks. this case is complicated by various factors, such as high uncertainty, considerable time pressure and urgency, severe resource shortage, and multi-authority and massive personnel involvement [cit] . effective coordination is an essential element of emergency response management. the crux for coordination in emergency response is that tang p., shen g.q.p. (2015) response activities with variable intervals, and providing temporal flexibility to adapt to the uncertainty and dynamic nature of the response process. the proposed decision-making model presents several valuable extensions in addition to existing htn planning paradigms to provide better decision support to the emergency command operation and to generate the emergency response plans of this novel structure. first, an enhanced htn is designed to record the htn exploration space, in which the objective decomposition structure and execution monitoring conditions are extracted. second, a new concurrent controlling mechanism based on temporally enhanced planning state and controlling rules is proposed to ensure parallelism of the response activities with the variable intervals. finally, this plan structure allows handling of extensive temporal knowledge, such as temporal causal dependencies, deadlines, temporal landmarks or synchronization schema, and represents the starting and ending times of all the tasks explicitly in the hierarchical task network to provide temporal flexibility by integrating the simple temporal network (stn) [cit] with the task network. stn is used extensively to encode and reason quantitative temporal constraints over variables. in particular, a dedicated stn solver taking full advantage of the task decomposition structure induced by the htn planning process is proposed and embedded in the decision-making model to propagate generated time constraints on the underlying stn incrementally."
"a practical case of typhoon evacuation and the experimental results are presented in this section to demonstrate the applicability of the presented decision-making model for providing support to emergency command operations in eoc. first, a typhoon evacuation domain that reflects the characteristics of emergency response is introduced. an emergency response plan generated by a decision-making model for coping with an emergency situation case is presented. the decision-making model is also compared with existing planning paradigms. finally, a set of experiments are performed to show the performance of prop-h-stn algorithm based on the semi-structured stns embedded in the model."
"during emergencies, planning products should be disseminated to multiple geographically dispersed responding task forces on scene based on the chain of command. responders undertake specific orders by implementing complex processes that must satisfy the given constraints and produce the execution effects based on the operation procedures and local emergency situation. therefore, the specific start and end time of planned activities in the emergency response plan cannot be pre-determined during the emergency command operation. instead, the exact execution time is determined by field responders in actual local situations."
"the abovementioned decision-making model that aims to support the development process of emergency response plans provides excellent expressing capability to represent qualitative and quantitative time constraints. when the compounded tasks are decomposed or when primitive tasks are applied to expand the search space, the time constraints are generated and added to the search nodes to encode interdependencies between the tasks. obviously, temporal conflicts may occur. in the presented decision-making model, the start and end times of all the tasks are represented explicitly. consequently, the entire stn is larger compared to existing planning paradigms, and more planning time is required to check the temporal consistency of the underlying stn. this section proposes a temporal constraint propagation algorithm called prop-pc2-stp, which takes full advantage of the hierarchical decomposition structure induced by the htn planning process. this algorithm is embedded in the planning process for propagating time constraints."
where l is the number of all the tasks) is a set of time points representing the start or end times of all tasks in the hierarchical task network and the reference time point of the planning process.
"(2) emergency response business process the emergency response business process consists of a set of planned activities with order and synchronization relationships. the planned activities achieving the primitive task nodes in the temporal refining task network describe the response actions carried out by geographically dispersed responding task forces, such as firefighting, searching for victims, transporting evacuee, setting up tents, and so on. according to the analysis, the emergency response business process is defined as the follows. the planned activity is also represented by an operator instance as shown in section 3.1, which describes the detailed execution process of a specific primitive task node. its start and end times are exactly the same as those of the associated primitive task node. moreover, the assumed execution effects representing response efforts describe the way the emergency situation will change during the execution interval. the assumed execution effects also set the objectives for task forces performing this activity and provides the criteria for monitoring the execution progress for emergency managers. as a result, the task forces will carry out the responding activities according to the local situation to produce the defined effects. finally, the sequence and parallel relationships are also defined tang p., shen g.q.p. (2015 explicitly to provide support for emergency managers in eoc to issue the incident orders."
"comparing with the existing state-based forwarding htn planners, the presented decision-making model expresses the objective decomposition structure, record all decision nodes and their context to monitor plan execution process, provide temporal flexibility for adapting to the uncertain and dynamic nature of the response process, and handle the interdependencies and synchronization of response activities with a variable interval. it also represents the start and end times of each task explicitly in the hierarchical task network. depite the associated stn in our model is of large scale and requires more planning time to check temporal consistency, a new time reasoning algorithm based on semi-stn is embedded to overcome the obstacle effectively."
"planned activities can be conducted simultaneously and independently if one activity does not affect the others because the response task forces are also geographically dispersed. consequently, improving synchronization among the response activities in emergency response plans can save considerable response time, thereby resulting in better performance. by contrast, if the execution of an activity affects or is affected by another one, then interdependencies exist between these activities. in the first case, one planned activity generates the execution effects, which provides the precondition to execute another one. therefore, a cause-effect relationship exists between these activities. in the other case, if more than one planned activity requires common resources, such that rescuing the victims under debris in different locations requires the only available search-and-rescue team and equipment, then they should be carried out one after the other depending on the scheduling rules. based on the analysis, the requirement is detailed as follows."
"this section introduces the method for extracting the emergency response plans of the new structure from the generated planning information once the planning process terminates. the generated planning information, which is recorded by the data structure of the enhanced hierarchical task network, as well as the generated search node, is introduced in above sections. the emergency response plan consists of two elements. the first element keeps a trace of the selected decompositions during the htn exploration, which records a sequence of decision points while planning for emergency response. the second element corresponds to the action plan, which consists of a set of planned activities in the lowest level of temporal refining task network. the extraction of the two elements is introduced in this section."
"a ship's bridge represents a classic example of a socio-technical work environment because operational tasks, such as changing course or speed, must be achieved by a team requiring joint efforts of 'human and technological interlocutors' [cit] . to that end, full-mission ship simulators are appropriate for understanding the decision-making process by coupling the human element with technology. to better understand the elements of decision-making and time lags related to active whale avoidance, we conducted familiarization and feasibility exercises during 2 [cit] using the kongsberg full-mission bridge simulator (figure 2c) at the alaska vocational technical center (avtec) in seward, alaska 2 . the full-mission simulator at avtec is regularly used for training alaska's marine pilots in the maneuvering of large ships as part of (re)certification and continuing education, and mirrors the platforms used by marine pilots at other training centers around the united states."
"joint emergency response plans are an effective method for coordinating multi-organizational response during emergencies. the formation of a joint emergency response plan is a real-world problem faced by emergency managers during emergency command operation. enormous efforts are invested in designing decision support tools to develop emergency response plans. this research concentrates mainly on what emergency response plans are required during emergency command operations and how to generate them by integrating htn planning and scheduling technologies. in this paper, an extended emergency response plan structure that accounts for the requirements of emergency command operation is proposed. the extended structure expresses the decomposition structure of the incident objective explicitly, records decision contexts, and provides temporal flexibility. a decision-making model is developed to generate this plan structure intelligently. this model presents several valuable extensions comparing with other existing state-based forwarding htn planners. first, an enhanced hierarchical task network is designed to record traversed htn exploration space of initial sets of incident objectives. second, the generated plan is temporally flexible, in which the start and end times of actions are not pre-determined. that is important for handling temporal uncertainty in emergency response domain. a new concurrent controlling mechanism to ensure parallelism of response activities with variable interval is also proposed. moreover, it enables representation of the dynamic emergency situation. third, the start and end times of all tasks in the hierarchical task structure are represented explicitly. all the time constraints defined on them are encoded and handled by a semi-stn attached to the task network. the study also proposes a dedicated stn solver that takes full advantage of the decomposition structure induced by the htn planning process to propagate on the underling semi-stn incrementally. despite the scale of induced stn of our decision-making model is larger comparing with existing htn planners, such as xeplanner, its computation efficiency is much higher. that enables our model to overcome the obstacles faced by other temporal htn planners, such as siadex and xeplanner, which cost much more planning time when checking the consistency of stn. empirical research on typhoon evacuation demonstrates that the model is suitable for solving real-world problems when planning for emergency response in practical application. therefore, our decision-making model makes an actual contribution to htn planning by extending the existing planning framework and temporal propagation algorithm."
"seven simulations were conducted whereby a team of two pilots, one serving as the pilot, the other as the helmsman, operated the bridge of a ship, which had operational parameters similar to that of the m/s diamond princess, a 115,875 gross tonnage, 288 m cruise ship that is representative of the large cruise ships calling in alaska during the summer. also on the bridge was an observer who recorded the time of events including (1) the start of simulation, (2) the first detected surfacing event of a simulated humpback whale spout (the first actual surfacing event -detected or not -was known only to the simulator operator and scenario coordinator who were located in a different room; figure 2c), (3) the communications that occurred between the pilot and helmsman, (4) when a command was initiated and (5) the end of the simulation, once the ship had passed the whale. following each simulation, a de-brief discussion was held to review the events and clarify the reasoning related to the decision-making process. during the de-brief, the elapsed time between first detection and the time of the ordered command was quantified, and the common elements related to the decision-making process were identified."
"the conceptual model of active whale avoidance was derived primarily from the collective experience of pilots in southeast alaska who have 'learned by doing, ' which has required significant time on the water. we thus submit a number of ideas for priority research and training development to hasten the adoption, applicability, and effectiveness of active whale avoidance."
"here we describe active whale avoidance by mariners aboard large ships which serves as a complementary, but comparatively underexplored, means to reduce whale strike risk. active whale avoidance is defined here as a mariner making operational decisions, such as a course change or speed reduction, with the goal of reducing the chance of a collision with a sighted whale. active avoidance differs from more 'passive' regulatory approaches in that the risk-reducing action is primarily initiated by the mariner upon sighting of a whale surfacing forward of the ship as opposed, for example, to a ship entering a mandatory speed reduction area which requires a change in operational state independent of whether a whale is present in the area and/or at risk of collision."
"an important function of emergency response plans is to discover and manage interdependencies between response tasks and activities. one of the principles of this plan is coordinating multiple responding task forces based on unified action plans during emergency command operation. the basic interdependencies between tasks are cause-effect relationships [cit] that show that the producing task provides the execution conditions of the consuming task. during the search space expanding process, two basic plan-refining steps discover and generate cause-effect relationships among the tasks, such as decomposing compound tasks and applying primitive ones. when a compounded task is decomposed, the instantaneous precondition list for each branch of the unifying methods is evaluated to check the satisfiability. for each literal in an instantaneous precondition list with time point i t that matches a positive temporal predicate with time point j t in the planning state, a cause-effect relationship is generated to record the causal structure between the tasks and is defined on these time points. the same procedure is applied for a primitive task. from the abovementioned analysis, the knowledge formalism of cause-effect relationships is listed as below. logicalexp is a disjunction, negation, assignment expression, or call expression, then no cause-effect relationship is generated."
"refining estimates of detection probability, particularly as it applies to the area with in the cone of concern also represents an important research thread. the instantaneous detection estimates of the radial ship-to-whale distances we utilized [cit] were derived based on detecting whales across the 180-degree arc (beam-to-beam) forward of the ship. we assume that the probability of detection will be much higher at a given distance, or much father at a given detection probability, if similar detection functions were derived based on search effort solely in the cone of concern. we acknowledge that some of the 'gains' in detection from focused search in the cone would be offset somewhat if mariners tasked with sighting whales are also tasked with other duties (e.g., monitoring radar or responding to radio communications). however, updating estimates of detection probability based on a lookout's focused search within the cone of concern would provide more reliable estimates and produce a more realistic range of feasible options of avoidance maneuvers."
3) the variable actset is a set of planned activities describing the execution processes of associated primitive tasks. the detailed paradigm will be introduced in section 3.1. each planned activity achieves a primitive task in the hierarchical task network.
"the reporting lag represents the time it takes for the person making the observation to vocalize the observation which, from our experience, may vary from 2 to 10 or more seconds depending upon: (1) the volume or quality of the initial sighting information (which may require dialogue with the pdmv); (2) the observer's ability to articulate the relative bearing, distance, direction of travel, or other relevant information;"
"the time elapsed between surfacing events was also variable, although the length of most submergences were centered in groups of 10-15 and 15-20 s (figure 3b) . we feel confident that, once a surfacing event was observed, detection probability of subsequent events was very high as observers (and bridge teams) focus on small area where whales are likely to resurface to gain as much quality information as necessary to evaluate collision risk. together, the data suggest that mariners engaged in active (humpback) whale avoidance in alaska generally have about three opportunities for detecting the whale during its surface interval, with an average of around 20 s between events."
"based on data from hundreds of surfacing events of humpback whales by observers, we demonstrate that mariners aboard large ships in alaska typically have about three opportunities, each separated by about 20 s, to detect the whale and make a decision about whether an avoidance maneuver is necessary, possible, and safe. while these estimates were largely consistent with other studies of humpback whales in alaska [cit], we highlight that data on surfacing frequency was not corrected for any negative biases owing to the observer's chance of missing the initial (or several) surfacing events, particularly at large shipto-whale distances or limited to the mariner's cone-of-concern. while we initially sought to minimize the chance for this distance bias by using only information from whales surfacing close to the ship, ultimately we decided against subsetting the data (1) because surfacing intervals that began close to the ship were often still continuing when the ship passed abeam (when observers terminated their observations of that whale) which would also underestimate the number of surfacing events per interval, and (2) to avoid biasing the inferences if, in fact, whales alter their surfacing behavior as a function of distance. note that slower (initial) speeds require less power (load) and thus fewer engines are needed to meet those power load demands for propulsion. [cit] ."
records a set of plan-refining steps to achieve task i task . these steps are generated when all preconditions of the methods unifying the current task are evaluated to determine whether they are satisfied in the current planning state.
"recognizing these biases, however, helps identify possible ways in which the effectiveness of whale avoidance can be increased. for example, a key finding of our conceptual model is that processes that occur on the ship's bridge such as reporting a whale sighting, assessment of the risk, and compliance to commands, couple with maneuvering constraints to produce a variable, yet important time lag between detection and achieving a new operational state (that reduces collision risk). this aggregate lag contributes to the inverse relationship between time available to make an avoidance maneuver and the range of maneuver options available. any activity or operation that increases the chance of detection when a whale is first available to be detected thus increases the options for avoiding the whale and the odds of successful avoidance. we identified three factors that may help pdmvs detect a whale and obtain sufficient information to actively avoid it."
"our goal for this paper was to present (1) a conceptual model of active whale avoidance, and (2) provide a proof of concept by utilizing empirical data of humpback whale surfacing behavior collected from the bow of cruise ships and 1 http://seaiq.com/ from simulations of large cruise ship operations in a fullmission bridge simulator and via commonly used pilotage software. [cit] between a team of state of alaska marine pilots from the southeast alaska pilots' association (seapa), [cit] s. the conceptual model is presented first (figure 1) by describing each of the constituent processes, and factors that influence them. components include availability and detection processes, reflecting how often and how long whales are available to be detected, and the ability of mariners to detect them once available; and command and maneuver processes, reflecting the procedures that occur on the bridge once a whale is detected, and the ability of a ship to achieve a new operational state commanded by the mariner that reduces collision risk. these components are based upon existing literature (e.g., availability and detection processes) and the collective experience of marine pilots (command and maneuver). to that end, the 'results' of the conceptual model include narrative describing how and why certain factors are important, particularly as it relates to ship operations and maneuvering, including events that transpire on the ship's bridge when a whale surfaces and is detected forward of the ship. for our proof of concept, data collection procedures are organized according to the different components of the conceptual model. while more details on the fieldbased methods can be found elsewhere [cit] they are described briefly below."
"a typhoon is a typical large-scale disaster in china's south-east coastal areas. given the strong winds and rainstorms, collaboration and coordination between multiple responding organizations are required to achieve identified incident objectives [cit] . such objectives include evacuating and settling residents in low-lying communities, controlling floods, and patrolling water conservancy facilities. in this section, a typhoon evacuation domain is designed to test the decision-making model by investigating a local jurisdiction of shenzhen in the southeast coastal region of china. when a typhoon or rainstorm comes, emergency managers in eoc in loc 7 should assess and identify the communities in danger and evacuate residents in these low-lying areas to specific shelters before the disaster happens. once incident orders are received, task forces from multiple responding organizations, such as police, fire control, medical, civil administration, and transport departments, respond and carry out specific tasks to evacuate and settle residents. close collaboration and cooperation among the responding organizations are essential to achieve a coherent response to typhoon disasters."
"this experiment is performed to compare the performance of our decision making model with that of existing state-of-the-art planners. the selecting planner is sapa [cit], which is a domain-independent heuristic forward chaining planner which can handle durative actions, metric resource constraints, and deadline goals. because sapa cannot express the characteristics of typhoon evacuation domain, zenotraveltime domain is selected in this experiment, which is the benchmark of the third international planning competition [cit] . a set of planning problems are generated randomly and are solved by sapa and our decision-making model. the plan metric value of produced action plans generated by sapa is quadruple make-span of the generated action plan similar to xeplanner. the plan metric value of action plans generated by our decision-making model is defined as the previous experiment. the experimental results are shown in table 2 ."
"our results also demonstrate how, in some scenarios, slower ships may have increased opportunities for whale avoidance acting through both the maneuver and detection processes. faster ships, by definition, travel further distances compared to slower ships during set time periods, such as when whales are submerged between surfacing events (averaged 20 s in our study), on deep dives (324 s modeled based on literature), or during time lags related to decision-making and communications on the bridge following detection. for example, if the command processes takes the same amount of time on fast and slow ships, and total time elapsed following detection to the point the ship begins to change course is approximately 115 s (figure 7), we demonstrate that the faster ship achieving a 'near miss' of 100 m from a whale would need to detect the whale over 1100 m from the point of collision as opposed to just over 700 m for a slow ship, simply because the faster ship moves further over the same time period given the conservative safe maneuvering limitations imposed on the initial test scenarios. an alternative way of interpreting those results is that, had the slow ship and fast ship begun the command and maneuver process at the same distance from the whale (as opposed to the same time), the slower ship could have achieved a greater cpa because it would have had a longer time period to continue its turn."
"in the context of active whale avoidance, the relevant inference is the probability the mariner detects at least one of the available surfacing events in a surfacing interval because whales often engage in multiple surfacing events (per surfacing interval) and mariners generally need only to detect one of the events to begin evaluating whether a whale avoidance maneuver is necessary and feasible. [cit] estimates to calculate the cumulative probability of detecting one of the events in a series of surfacing events, i.e., the first or second surfacing event in a 2-surfacing interval, the first or second or third surfacing event in a 3-surfacing interval, and so on."
"based on our findings and observations, we conclude that active whale avoidance is feasible and, in most cases, can be practiced without creating an increase in competing risks. what's more the practice can complement existing efforts that increase situational awareness of whales (e.g., whale alert) even in areas where other risk-reduction measures, such as operating at slower speeds, are in place. most importantly, continuing collaboration between professional mariners, scientists, and natural resource managers is vital to reaching mutually beneficial reductions in whale strikes."
"another productive avenue of research is a more rigorous examination of competing risks related to whale avoidance. during our simulations, once pilots confirmed that a whale was forward of the ship at some risk of collision, a primary consideration was how to achieve a new (avoidance) heading while not increasing other risks, such as collision with other vessels, reefs, or shoals. for obvious reasons, ship operators will rarely increase the risk of deleterious impacts to passengers or damage to the electrical system that accompanies dramatic and unsafe operations [e.g., a 'crash stop' [cit] or rapid turn], unless those maneuvers are offset by reduction in risk to more consequential events such as a grounding (for example). the risk of negative impacts from dramatic changes in course or speed to avoid a whale will thus always be weighed against the potential benefits of whale avoidance. in all simulations where a course change was needed, the pilot evaluated the efficacy of the course change by considering the needed time to incrementally 'build up' to the desired rate-of-turn to minimize impacts to passengers, the ship, and the environment, thereby avoiding excess 'heel.' larger heel angles aboard cruise ships increase the chance that furniture will begin to slide and passengers will be injured from falls/by falling objects, and swim pools will spill, etc. as previously discussed, the electrical or propulsions systems can be negatively impacted in extreme instances of abrupt speed changes. we note that parameters identified as \"safe\" often represent general guidance and can be modified depending upon the pdmv's experience and the situation (e.g., commencing an initial rate-of-turn, within defined parameters, and then after the ship has stabilized at that rate-of-turn, incrementally increasing, and stabilizing at greater rates-of-turn, while maintaining the ship within safe heel angles)."
"in the existing temporal htn planner, the added time constraints are propagated individually on the entire underling stn directive. all generated time constraints in prop-h-stn are instead added to the initial affected sub-stn and are propagated simultaneously. the time point cluster structure changes according to rules 1 and 2 in section 5.1. the detailed procedures of prop-h-stn algorithm is shown in figure 3 . if ( tightentcset is not null)"
"ship speed can also influence whale avoidance by influencing detection probability. to be clear our results do not indicate that mariners on slower ships are able to detect whales any better compared to mariners on faster ships -there is no logical reason why detection probability would differ for a surfacing whale at a set distance (e.g., 2000 m from the ship) for mariners aboard a fast or slow moving ship. however, if we held the time to a collision constant, as in the scenario in figure 5, then, by definition, the faster ship will be further from the point of collision than a slower ship at the same time to collision. thus, a surfacing event critical for detection would occur closer to the slower ship influencing the cumulative probability of detection, providing more time for a maneuver."
"the abovementioned planning algorithm is a state-based forward planning paradigm similar to shop2 and xeplanner, in which the generation process of planned activities is the same as the execution process. despite the actions are generated one by one during the planning process, they are partially ordered, and their execution intervals may overlap. this section introduces a concurrent controlling mechanism of multiple operators with variable intervals. logicalexp . in addition, all the following conditions should also be satisfied."
"once large-scale emergencies break out and are reported to the emergency operation center (eoc), emergency managers will collect and maintain information on the current and forecast situations, as well as the resources tang p., shen g.q.p. (2015) assigned to the incident on scene. the complexity and unpredictability of the scenarios have resulted in emergency managers not being unable to develop a detailed action plan ahead of time. achieving the identified incident objectives is also beyond the capabilities of any organization. therefore, emergency managers develop emergency response plans by breaking down incident objectives into more specific response tasks implemented by task forces from multiple organizations with responsibilities. this process involves a sequence of decision-making points to determine the tactical direction and specific resources, reserves, and support requirements to implement the selected strategies [cit] . emergency response plans represent the intention of emergency managers, which define the work of involved responders, to cope with disastrous situations. they also need to ensure that all responding activities undertaken are defined and time conscious. the involved sequence of decisions are dependent on the on-going incidents and response efforts throughout the emergency response plan development process. finally, emergency managers disseminate incident orders, which are precisely the planned activities in the emergency response plan, and direct responding task forces to achieve the incident objectives collaboratively. while the individual task forces are performing specific response activities based on local situations, emergency managers monitor on-site response efforts and emergency situations, and compare the planned progress with the actual process to evaluate the validity of the current plan."
"our decision-making model and xeplanner [cit] are state-based forward planners applied in generating action plans for emergency response and can encode multiple types of domain knowledge in emergency management. comparing with our decision-making model, stn only defines tasks in the lowest level of the hierarchical task network in xeplanner. therefore, the stn underling the hierarchical task network has a significantly smaller scale in this planner. given that xeplanner and our decision-making model are developed for the same purpose and applied in planning for emergency response, a set of planning problems are selected randomly in typhoon evacuation domain to test their performance. the plan metric value of generated action plans is quadruple make-span of the produced action plan in xeplanner. the make-span of a generated action plan is defined as the earliest end time of the emergency response business process in the generated emergency response plan in our decision-making model, because the generated action plans are temporal flexible. the planning time of solving all the problems and the plan metric values of produced action plans are listed in table 1 ."
"(2) formalizing decisions in a context-sensitive manner developing emergency response plans involves a sequence of decision points to select the proper methods to achieve complex tasks. these decisions are also formed based on the incident status, responding efforts, and other constraints. the decision contexts describe specific conditions under which tasks in the emergency response plans can be achieved. however, the highly dynamic environment changes constantly and unpredictably while planning for emergency response. the difficulty of developing an accurate and precise model on how the environment tang p., shen g.q.p. (2015) evolves over time may result in the developed emergency response plans not being incapable of adapting to the existing situation. the uncertainty from the execution process of the response activities also results in unexpected effects. consequently, discrepancies between the assumed and actual emergency situations occurs frequently during the emergency response plan development and deployment processes. therefore, emergency managers should evaluate whether the current executing plan remains valid during an emergency command operation. the design requirement based on the analysis is listed as follows."
"during the htn planning process, the plan-refining steps include decomposing a compounded task and applying a primitive one. the enhanced hierarchical task network is designed to record the entire decomposition structure of the incident objectives generated during the entire planning process. its formalism is listed below."
"the precise start and end time of the response task in emergency response plans should satisfy all defined temporal constraints, and cannot be pre-determined before execution."
", where k is the number of cliques and w is the induced tree width. therefore, prop-stn for stns with known and bounded tree width achieves linear time complexity, indicating a substantial improvement over the use of pc-2 algorithm. however, the cluster tree structure is constructed by triangulating the stn in prop-stp algorithm. in addition, time constraints are not added and propagated incrementally. inspired by temporal propagation algorithms, such as sr-pc and prop-stp, the entire stn is divided into a number of sub-stns to improve efficiency in the incremental temporal management method introdueced in section 5. the time point clusters in this temporal propagation algorithm are not constructed by triangulating the entire stn, or defined by parent and child tasks relationships. the temporal propagation algorithm divides the entire stn directive by considering the hierarchical task network and underling cause-effect relationships in our decision-making model. the presented time constraint propagation algorithm also handles multiple time constraints simultaneously in each sub-stns and demonstrates a much higher performance comparing with pc-2."
"(1) comparison of pc-2 algorithm and prop-h-stn algorithm pc-2 is the most popular incremental temporal propagation algorithm and is embedded in the existing htn planners with temporal management function, such as siadex [cit], xeplanner [cit], and sipe-2 [cit] . our decision-making model represents all start and end times of each task in the hierarchical task network, and the stn is of large scale and consumes more time to propagate generated time constraints. hence, a more efficient time propagation algorithm is needed in our decision-making model. prop-h-stn algorithm is proposed in section 5 to propagate generated time constraints dynamically based on the semi-structure stn. a number of planning problems are generated randomly in the typhoon evacuation domain to evaluate the performance of the new temporal propagation algorithm. moreover, the pc-2 and prop-h-stn algorithms are embedded in the presented decision-making model to assume temporal management function. the experimental results are listed in table 3 . as shown, the prop-h-stn algorithm sharply decreases the planning time compared with pc-2 algorithm. (2) comparison of the three methods for managing the queue of triangles in propagating sub-stn the updated edges in the prop-h-stn algorithm can be added to the queue in three method, such as the front, the end, and any position in the queue. ten sets of planning problems are generated randomly in the typhoon domain to evaluate these methods for managing the queue of triangles in our temporal propagation algorithm. the experimental results are shown in figure 8 . the addition of the associated triangles of tightened edges to the end of the queue resulted in the algorithm outperforming the other two method when the temporal consistency is checked. the algorithm displayed the worst performance when the associated triangles were added to the front of the queue. moderate performance was achieved when the associated triangles were added to any position of the queue. hence, propagating the time constraints as early as possible across the time constraints graph is more effective. the propagation also requires less time to checek temporal consistency when the associated triangles of updated edges are added to the front of the queue. [cit], all the edges representing time constraints in selected triangle can be computed and updated as a whole during the planning process. ten sets of planning problems are designed in the typhoon evacuation domain to evaluate the effectiveness of these two methods. the experimental results in figure 9 demonstrate significant improvements when our decision-making model simultaneously computes all edges in the selected triangle. the results indicate excellent performance in terms of cpu time when the presented decision-making model checks temporal consistency of all edges in a triangle simultaneously."
"during post-simulation de-briefs several common themes were discovered. first, marine pilots rarely command a speed reduction in response to a single sighted whale owing to their familiarity with the time it takes to achieve the new speed [cit], reproduced in part in table 1 ) and that a course change alone is most often more effective and efficient than a potential speed reduction. second, and perhaps more importantly, the pilot's maneuvering decisions were ubiquitously based on the evaluation of competing risks. for instance, once the pilots confirmed that a whale was within the cone of concern, a primary consideration was how a change in course would influence other risks, such as the risk of collision with other navigation hazards including, but not limited to, other vessels, reefs, or shoals. likewise, in all simulations where the pilot decided that a course change was needed to reduce collision risk with a whale, an evaluation occurred whereby the efficacy of the course change was considered relative to the time needed to safely 'build up' to the required rate-of-turn. the rate-ofturn required to avoid the whale was then considered relative to that particular ship's safe rate-of-turn guidelines and heel angles to mitigate the risk of deleterious impacts to the vessel and its passengers."
"(3) existing bridge communications; and (4) language or cultural communication issues. for example, the lookout may spot a whale spout and report, \"whale two points to starboard\" with no additional information on distance, direction of travel, or speed. at that point, the pdmv will look in the indicated direction and engage the lookout for information needed to make an assessment which may result in an additional 10-20 s depending upon the length of the submergence between surfacing events or other ongoing action by the pdmv (e.g., communicating on the radio with other traffic, establishing and monitoring navigational parameters, etc.). in the meantime, the initial cue is often no longer available. the elapsed time associated with detection and reporting lags will be minimized if the pdmv makes the observation him/herself (with a high degree of certainty) and immediately articulates the observation to the bridge team. in these instances the total time elapsed may be as short as 5 s, but more frequently it will be closer to 15 s."
"the ultimate objective is the application of htn planning in real-world domains. we are interested mainly in the process of addressing the problems of developing emergency response plans by using htn planning. it requires more expressive representation and capability than those provided by traditional htn planning paradigms, which tang p., shen g.q.p. (2015) produce a sequence of actions and cannot satisfy the requirements of emergency command operation. moreover, these planners cannot handle time constraints and actions explicitly executed in variable intervals."
the extended emergency response plan should be sufficiently elaborate to describe the interdependencies and synchronization between planned activities to define coordination across multiple responding task force units.
"a key characteristic of our decision-making model is that a semi-structure stn is embedded in it. the semi-structure stn encodes and handles time constraints defined on the underling the hierarchical task network. an incremental temporal management method is also proposed to propagate the generated time constraints incrementally during the planning process. in this section, we design a set of experiments in typhoon evacuation domain to evaluate the performance of the time management method based on the semi-structure stn introduced in section 5."
requirement 2: decision contexts should be formalized explicitly by defining attached conditions and constraints in the emergency response plans to monitor plan validity during emergency response.
"as a result, the time point cluster j c is called the affected time point cluster, and the sub-stn underlying j c is called the initially affected sub-stn, to which the generated time constraints are added. from the abovementioned analysis, the entire stn is divided into multiple sub-nets underlying each time point cluster. each sub-net encodes all the time constraints underlying the time points in the associated time point cluster."
"when the time constraints of c-type2, c-type3, and c-type4 are generated, given that the start and end time of the compounded task are found in time point cluster i c, a new time point cluster j c is created and initialized by tr . moreover, the start and end time of the compound task to be decomposed and those of all its subtasks are added to time point cluster j c . the separator between time point clusters i c and j c is initialized by the start and end times of this compounded task and tr."
a method of constructing the temporal refining task network (section 2.3) is proposed according to the information elements generated by the planning process. the detailed process of extracting each element in the temporal refining task network is listed as follows:
"given the serious consequences of large-scale emergencies, generating emergency response plans should be implemented by human-machine cooperation to improve fidelity of action plans. emergency managers should be supported to enable them to make good decisions on how to plan and respond to disastrous situation. however, computers cannot assume the role of commanders to evaluate incident status and make critical decisions on the appropriate response. thus, the decisions involved in the development process of emergency response plans should be implemented collaboratively by decision support paradigms and emergency managers. one of our next research objectives is to design human computer interaction interfaces for providing cognitive-level support to emergency managers. this approach combines both htn planning and mixed-initiative approaches. emergency managers interact with the decision reasoning process of computational models to advance the development process of emergency response plans. thus, not only the explicitly readable domain knowledge recorded by computer, but also implicit experiences in the mind of emergency managers are all integrated to make good decisions. moreover, the reasoning capability of computers and evaluation capability of emergency managers are all applied to complement each other with advantages, which is the ultimate objective of human and computer collaboration process."
"comparative protein modeling of a target protein based on sequence similarity to a protein with known structure is widely used to provide structural models of proteins. frequently, the quality of the target-template sequence alignment is non-uniform along the sequence: parts can be modeled with a high confidence, whereas other parts differ strongly from the template. in principle, molecular dynamics (md) simulations can be used to refine protein model structures and also to model loops in homology modeled protein structures, but it is limited by the currently accessible simulation time scales. in the current work we have used a recently developed biasing potential replica exchange (bp-rex) md [cit] method to refine and to model loops in homology modeled protein structure at atomic resolution including explicit solvent. in standard rex md [cit] simulations several replicas of a system are run in parallel at different temperatures allowing exchanges at preset time intervals. in a bp-rex md simulation replicas are controlled by various levels of a biasing potential to reduce the energy barriers associated with peptide backbone dihedral transitions. the method requires much fewer replicas for efficient sampling compared with standard temperature rex md. starting from incorrect loop conformations this bp-rex md method samples the correct loop conformations as dominant conformations in all the cases. application of bp-rex md to several protein loops indicates improved conformational sampling of backbone dihedral angle of loop residues compared to conventional md simulations. bp-rex md refinement simulations on several test cases starting from decoy structures deviating significantly from the native structure resulted in final structures in much closer agreement with experiment compared to conventional md simulations [cit] ."
"our decision-making model provides an expressive representation of time constraints between tasks. during the planning process, the generated time constraints, which are either pre-defined beforehand or induced by the plan-refining process, are added incrementally to the stn for encoding the independencies between tasks in a search node. on the one hand, when a compounded task is decomposed, four types of time constraints are generated and should be encoded by the stn. the details include the following: c-type1: time constraints produced by checking the satisfaction of the preconditions in the method, as shown in 4.2.2, c-type2: time constraints encoding the start time of the parent task is ordered to the start time of all its child tasks, and those representing the end time of all the child tasks are before the end time of the parent task, c-type3: time constraints encoding the start time of all the child tasks are ordered to their end time, c-type4: qualitative and quantitative time constraints between the child tasks, which are defined in the method formalism. on the other hand, when a primitive task is applied, two types of time constraints are generated, and are listed as follows: p-type1: time constraints produced by checking the satisfaction of the preconditions for the operator unifying the primitive task, as shown in 4.2.2, and p-type2: time constraints produced by applying the concurrent controlling rules."
"pre-emptive (planned) reductions in speed are, however, regularly used by pilots in alaska as a strike risk reduction strategy. pre-emptive speed reductions are those initiated in anticipation of, rather than in response to, a whale aggregation, and are utilized in two general scenarios. the first is when mariners are informed of a whale aggregation recently detected along the ship's route and communicated to the ship personnel. the second general scenario is when the ship is approaching a narrow navigational area that also historically has supported whale aggregations. for example, with the cooperation of cruise ship masters, pilots regularly slow cruise ships to 14 knots in snow passage, alaska, because avoidance options are limited and the area is often characterized by small to large whale aggregations. pilots have found that these pre-emptive speed reductions tend to produce less resistance from other bridge personnel when (1) they can be accounted for in transit planning and (2) they do not adversely affect port arrival times."
"we emphasize that our goal is to generate a conceptual foundation upon which specific processes, such as the relationship between whale surfacing distance and appropriate maneuver response, can be subject to more rigorous testing and replication. to that end, our findings (at this stage) are not intended to prescribe what mariners should (or shouldn't) do when in the vicinity of surfacing whales. instead, we draw some more general but important inferences from our conceptual model and related data including the role of ship operations (e.g., speed and heading variables) in active whale avoidance. ultimately we hope these ideas will help advance the development and application of active whale avoidance techniques on a global scale."
"to illustrate the cumulative chance that a mariner detects a whale that initially surfaces at different distances, we then plotted the cumulative probability of detecting at least one of the surfacing events for a whale engaged in an average surfacing interval of 3 surfacing events each separated by 20 s submergences (from our data below) initially surfacing at distances of 4000, 3000, 2000, or 1000 m from a ship. note that because the speed of the ship is relevant to the changes in ship-to-whale distances among surfacings, we modeled these probabilities based on a ship traveling 19 knots."
"second, assigning a designated lookout tasked solely with searching for whales in the cone of concern could also enhance detection probability and thus opportunities for whale avoidance. while we did not test whether different configurations of personnel (pilot, pilot + designated observer, etc.) produced different detection functions, experiments aboard large fast ferries have demonstrated that a dedicated whale 'spotter' vastly improved detection probability and the distance at which whales were detected [cit] . research based on line transect theory and distance sampling also demonstrate that detection probability increases when additional observers are utilized [cit] including with whales [cit] . while we recognize that transiting at night or in heavy seas may reduce or eliminate detection, we also highlight that technology continues to reduce barriers to detection in some of these conditions [cit] and application of the cone of concern may help inform development of the technology to maximize effectiveness."
"finally, in typical ship operations, while only one person has ultimate 'command' authority while on the bridge, the person directing the movement of the vessel may vary depending upon time and duties, and may be the pilot, captain or deck officer. for simplicity, hereafter, we refer to this person collectively as the person directing the movement of the vessel (pdmv)."
"unanticipated events, such as traffic congestion that delays the arrival of fire rescue personnel, changes in weather conditions, and bad weather that prevents the necessary equipments from arriving on site, may affect the execution process to complete the response tasks. this situation demonstrates that the precise start and end time of response tasks cannot be determined during the development process of the plan. therefore, emergency response plans should provide temporal flexibility to adapt to the uncertain and changing environment. based on the tang p., shen g.q.p. (2015) preceding analysis, the following designing requirement is provided."
"the assessment lag represents time needed for the pdmv to verify the information and subsequently assess if a collision is possible. in the determination of collision risk, mariners are trained not to make assumptions on the basis of \"scanty\" information (see us coast guard rule 7 risk of collision, international rules of the road 3 ), highlighting the need for quality information before taking action. if, for example, direction and travel speed of the whale are not available, the process may cycle back to the detection lag, awaiting another surfacing event upon which to formulate an avoidance decision. consequently, a simple report of a whale at a relative bearing and distance may not provide sufficient information upon which to base an avoidance action, even for a whale sighted directly ahead. consequently, the assessment lag, as with the other lags, may be relatively quick (3-5 s) for the \"obvious\" situations or it may take longer if inconsistent or incomplete information is reported."
"(2) applying primitive tasks. when a primitive task t with no predecessors is selected to be refined, it is applied by the unifying operators. first, the preconditions of this operator are evaluated as decomposing the compounded tasks. if they are satisfied by the current planning state, then the enhanced stn is updated in the same manner. the operator instance is also added to the variable enhietasknet and the search node. as a result, the planning state is updated by the execution effects of this operator instance."
"thus, all the order relationships between each pairs of planned activities are generated. in this paper, these relationships are represented by a boolean matrix called adjacent matrix. the ith planned activity should be executed before the jth one if the element line i and row j are true. the ith planned activity is called first planned activity if all the elements in the ith line are false. that is, the ith planned activity should be executed first. the jth planned activity is called the last planned activity if all the elements in the jth row are false. that is, no other planed activities should be executed after the jth planned activity."
"we note that we did not use seaiq to estimate how much time (and the total distance) it would take for the ship to slow down (e.g., from 19 to 10 knots) because during the full mission bridge simulations, pilots were found to avoid slowing speed in response to a single sighted whale, reflecting their normal practice. during de-briefings it was noted that while a moderate change in heading can be achieved in a relatively short time period (following whale detection), it takes much longer to achieve a moderate change in speed, reducing the effectiveness of speed reduction as a reactive response for whale avoidance, particularly avoidance of a single observed animal. moreover, pilots never practice 'crash stops, ' i.e., a rapid stopping of the ship to avoid a collision with a whale owing to the deleterious impacts it could have on the infrastructure of the ship. [cit] and re-visit the role of speed reduction as a pre-emptive avoidance maneuver in the section \"discussion.\""
"first, marine pilots in alaska, based on decades of experience encountering and avoiding (primarily humpback) whales surfacing near large cruise ships, have developed a searching pattern 'cone of concern' based on familiarity with approximate travel speeds of humpback whales relative to the ships' transit speeds. in doing so, pilots and other bridge personnel narrow their search efforts (by over 80% based on our simple vectors and geometry; figure 6 ) by delineating the 'population' of surfacing whales at risk of collision vs. those that are not. this practice could easily be standardized by integrating the concept into transit planning and/or regular communications with the bridge team to focus on parameters of, and need to search within, the cone of concern."
"this paper investigates the use of hierarchical clustering techniques to improve the measurement of cohesion of copyright © 2012 scires. jsea classes in oo systems. existing cohesion metrics are directly or indirectly based on observations of the attributes referenced by the methods. the measurement of cohesion of classes is based on the similarity between their methods. so, we used clustering to better identify clusters of related methods. the proposed approach has been evaluated using three particular case studies taken from (maintenance, restructuring and aspect mining) literature. the achieved results show clearly that the new approach better reflects the structure and quality of the design of the evaluated classes than the selected traditional structural cohesion metrics. the approach was effectively able to detect the disparity between the roles implemented by the evaluated classes. it allows, in fact, better differentiating and classifying methods of a class into groups of related and cohesive methods. this capacity is mainly due to the potential of separation into cohesive groups offered by clustering techniques. according to the evaluated case studies and the obtained results, the new approach appears to better detect design problems, such as assigning disparate roles to a class, than traditional structural cohesion metrics."
"this implementation causes a code tangling because the class has been assigned a second responsibility (handling of received messages and giving them to the following participant objects in the chain in case the current object could not handle the message). otherwise, the class would be reduced to a very simple code just for the handling of color. table 3 gives the values of the selected structural cohesion metrics. metrics lcom* and tcc indicate (is the case of this example also) a perfect cohesion. in contrary, the metric lcom indicates that the class presents a lack of cohesion."
"in this study, we proposed a hierarchical deep learning-based framework rpiter, which contains two sequence coding parts as inputs and involves two basic cnn and sae network architectures to generate comprehensive prediction result."
"comprehensive experiments were performed to compare the performance of different sequence coding methods and rpi prediction methods. we employed six metrics to compare the method performance, namely accuracy (acc), sensitivity (sn), specificity (sp), precision (pre), matthews correlation coefficient (mcc), and auc (the area under the receiver operating characteristic curve (roc))."
"to input rna and protein sequences into deep learning or conventional machine learning models, the sequence data must first be transformed into numerical representations. because the rna and protein sequence length vary in a large range (0-4000) in our datasets, the commonly used fixed length sequence encoding methods for deep learning models, such as one hot encoding, do not fit our problem. therefore, we adopted and improved the ctf method, which counts k-mer frequency in sequence to form fixed length vector representation."
"existing cohesion metrics are directly or indirectly based on observations of the attributes referenced by the methods. the assessment of cohesion of classes is based on the notion of similarity between methods. clustering (or unsupervised classification) methods are concerned with grouping objects based on their interrelationships or similarity [cit] . clustering is a data mining activity that aims to partition a given set of objects into groups (or clusters) such that objects within a group would have high similarity to each other and low similarity to objects in other groups [cit] . the inferring process is carried out with respect to a set of relevant characteristics of the analyzed objects [cit] . clustering techniques have been widely used to support various software reengineering activities such as system partitioning [cit], architecture recovery [cit] and program restructuring [15, [cit] . clustering techniques have also been used more recently in the area of aspect mining [18, [cit] ."
"in this study, we evaluated the performance of rpiter and other methods by six metrics, acc, sn, sp, pre, mcc, and auc. the formulas of the first five measurements are as follows:"
"the rest of the paper is organized as follows: section 2 presents a brief survey on major class cohesion metrics. section 3 introduces the new approach we propose for the measurement of cohesion of classes in oo systems. section 4 gives a simple example of application of our approach. section 5 presents the case studies we used to evaluate our approach. finally, section 6 presents some conclusions and future research directions."
"from figure 3 we can clearly see the limits of the object paradigm due to the duplication and tangling of code that were inevitable. indeed, the class point (and other classes not mentioned here) must integrate all the code fragment relating to the maintenance and reporting of observed objects in their implementations (pieces of code figure 3) . furthermore, we note the duplication resulting from calling the method informobservers() in order to report the change in an attribute to the observer object (piece of code labelled b). moreover, we note the tangling of code in the same class (point), between its main functionality (labelled a) mixed with the code for the notification mechanism of the observer we have described above (statements labelled b, c, d). this means that the class point has been assigned more than one role [cit] . table 1 gives the values of the selected structural cohesion metrics. according to these values, the class is not cohesive."
"without the n cl metric, it's only by reviewing the code that we will be able to determine it. the metric n cl (as an indicator of the disparity of the concepts implemented by the class) reveals in an explicit way this problem. together, the two metrics reflect in several situations some design problems (weaknesses in the design). the case studies presented in section 5 illustrate this dimension. the coh cl metric indicates the cohesion degree of the class. the n cl (taken with coh cl ) helps in interpreting the results."
where p(x) and p(y) are the properties of the objects x and y respectively. the distance measure used for discriminating objects express the dissimilarity between them. a possible measure of distance can be defined as follows:
"the study performed in this paper should, however, be replicated using a large number of oo systems in order to draw more general conclusions. indeed, the findings in this paper should be viewed as exploratory and indicative rather than conclusive. as future work, we plan to: extend the study by introducing other oo cohesion metrics, explore the use of the approach to support aspect-mining activities and finally replicate the study on data collected from a large number of oo software systems to be able to give generalized results."
"moreover, we can see by the decomposition of clusters k 1 and k 2 that each cluster contains only methods related to the same role. cluster k 2 includes elements of code b (figure 11), while cluster k 1 contains only elements of base code of the class colorimage, which is the method getcolor() (it means that there is no tangling of code in the same cluster). once again, the values of the two metrics coh cl and n cl, taken together, better illustrate the structure of the class."
"where tp and tn mean the number of correctly predicted positive and negative samples, respectively; fp and fn denote the number of wrongly predicted positive and negative samples, respectively; and p and n represent the total number of positive and negative samples, respectively. all these evaluation metrics were calculated by five-fold cv and no overlap between training and testing data."
"the last ensemble module concatenates the prediction results of former two cnn modules and two sae modules as its input tensor and generates a more comprehensive prediction result for a given rna-protein pair. the ensemble module is designed as a three-layer architecture, dense-16-8-2, with three fully-connected layers in which the neuron numbers are 16, 8, and 2, respectively."
"in module conjoint-sae and conjoint-struct-sae, first, two similar sequence embedding parts using sae analyze the rna and protein input vectors separately and generate two sequence embeddings. then, a three-layer fully-connected part concatenates the two sequence embeddings as input and makes the interaction prediction. each sequence embedding part with sae has three fully-connected layers with neuron numbers being 256, 128 and 64, thus is denoted as dense-256-128-64. after dimension reduction and high-level feature abstraction by the two three-layer sae parts, the sequence embedding representations of rna and protein are output by the last layers of two dense-384-256-128 parts. finally, a three-layer fully-connected part dense-128-64-2 concatenates the previous two sequence embeddings as input for its first layer and makes the interaction prediction for a specific rna-protein pair at the third layer. similar to previous cnn-based modules, the prediction results of the two sae-based modules are further integrated by the later ensemble module."
"in module conjoint-cnn and conjoint-struct-cnn, first, two similar sequence embedding parts using cnn analyze the rna and protein input vectors separately and form two sequence embeddings. then, a three-layer fully-connected part concatenates the two sequence embeddings as input and make the interaction prediction. each sequence embedding part with cnn has three convolution layers with filter numbers being 45, 64 and 45, thus is denoted as conv-45-64-45. between two convolution layers, max-pooling layer is used to reduce the representation dimension and introduce an invariance to noises. after the last convolution layer in conv-45-64-45, the output two-dimensional tensor is flattened and further serves as the input of a fully connection layer with 128 neurons, denoted as dense-128. then, the two sequence embedding representations of rna and protein are output by the two dense-128 layers separately. finally, a three-layer fully-connected part with 128, 64 and 2 neurons in each layer, dense-128-64-2, takes the previous two sequence embeddings and makes the interaction prediction. in dense-128-64-2, the input of the first dense layer is the concatenation of the protein and rna embeddings, and the output of the last dense layer is the prediction result, which is further integrated by the later ensemble module."
"for sequence coding, we adjusted the ctf coding methods by adding more primary sequence information and sequence structure information into the coding vectors. according to the performance comparison on dataset rpi2241, our improved ctf and improved struct ctf showed advantages over the previous ctf coding method. besides, we tried three commonly used deep learning sequence coding methods of one hot, word2vec and doc2vec, but they performed worse than ctf form coding methods in this rpi prediction problem and were abandoned in this study. the sequence structure used in our coding method was predicted by the software, thus might only contain limited information. we noticed that rpi-pred achieved 93% acc using the experimentally validated structure but only 83% acc using the predicted sequence structure on dataset rpi1807 [cit] . thus, more experimentally validated sequence structure data would contribute to more effective rna-protein interaction prediction."
"we improved the sequence coding method ctf by complementing more primary sequence information and sequence structure information, noted as improved ctf and improved struct ctf (described in detail in section 4.2). the performances of three sequence coding methods in five-fold cross validation (cv) on dataset rpi2241 are shown in table 1 and figure 2a . ctf yielded an acc of 0.848, sn of 0.826, sp of 0.869, pre of 0.864, mcc of 0.697 and auc of 0.929. improved ctf and improved struct ctf achieved the same acc of 0.852, 0.004 higher than the 0.848 obtained by ctf. for sn, sp, pre, mcc and auc, improved ctf and improved struct ctf also achieved similar performances and showed small advantages over ctf. as shown in figure 2a, the roc curves of three methods were very close but the curves of improved ctf (orange) and improved struct ctf (green) were slightly higher than the curve of ctf (blue). moreover, we tried three commonly used deep learning sequence coding methods: one hot, word2vec and doc2vec [cit] . one hot coding encodes each element in sequence into a binary vector and then concatenates all binary vectors successively to form the final sequence coding representation. one hot coding is extensively used for sequence coding in the deep learning-based models for biological sequence analysis [cit] . word2vec coding views k-mer in biological sequence as word in natural language and encodes the biological sequence by concatenating the k-mer representation vectors successively. doc2vec coding directly transforms a biological sequence into a fix length vector of its embedding representation. the sequence structure information has also been considered in these three kinds of coding methods (the coding methods with \"struct\" in name in figure 2b ). the roc and auc of these coding methods on dataset rpi2241 in five-fold cv are shown in figure 2b . on rpi2241, one hot coding is superior to word2vec coding for having higher auc, and word2vec coding is better than doc2vec coding. the sequence structure information notably improved the performance of doc2vec coding. the highest auc achieved by these three coding methods is 0.872 (one hot coding), but the ctf, improved ctf and improved struct ctf coding methods obtained obviously better auc of 0.929, 0.934 and 0.931, respectively."
"this paper aims at exploring the use of clustering techniques to improve the measurement of cohesion of classes in oo systems. we believe, indeed, that using clustering will better reflect the structure and the design quality of classes. clustering provides, in fact, a natural way for identifying clusters of related methods based on their similarity. the paper proposes a new approach to measure the cohesion of individual classes within an oo system based on hierarchical clustering. the proposed approach has been evaluated using three particular case studies taken from the literature. we also used in our study three well-known structural cohesion metrics: lcom [cit], lcom* [cit] and tcc [cit] . the achieved results show that the new approach appears to better reflect the cohesion (and structure) of classes than traditional structural cohesion metrics."
"the coh cl metric gives, in fact, the degree of relatedness between the methods of the class. a low value of coh cl indicates that the methods of the class are poorly related, in spite of the fact that they may constitute a single group of related members. however, it may also indicate (in an implicit way) the existence of several (two or more) groups of connected methods. in fact, these different groups may reflect, in some cases, the disparateness of the roles (more than one concept) assigned to a class. in this case, we will be able to determine it only by reviewing the code. a low value of coh cl may be interpreted in different ways and reveals, in fact, various situations: 1) the methods of the class constitute a single group of connected methods but are however weakly related; 2) the roles assigned to the class are disparate; and 3) possibly both. in practice, we may have two classes with comparable values of cohesion (let us assume 0.50): in the case of the first class, the methods are weakly related but constitute a single group of connected methods, and in the case of the second class the roles assigned to the class are unrelated which will be reflected in its implementation."
"we designed a deep-learning framework, rpiter (figure 1), to tackle the rna-protein interaction problem. our two sequence coding methods are used in the two sequence coding parts: improved ctf coding and improved struct ctf coding. after each coding part, two different network architectures, cnn and sae, are employed in two modules to extract features from the input and form high-level representations. in each module, the protein and rna coding vectors are analyzed by two similar parts separately to form respective sequence embedding representations. finally, an ensemble module integrates the outputs from the four basic modules (conjoint-cnn, conjoint-sae, conjoint-struct-cnn, and conjoint-struct-sae) to form the whole architecture of rpiter. the source code of rpiter can be freely download from https://github.com/pengeace/rpiter."
"as shown in figure 5, on datasets rpi369 and rpi2241, rpiter had a notable accuracy increase over all other methods, and the standard deviations of accuracy in five-fold cv (gray lines on the top of accuracy columns) were obviously smaller than ipminer and rpiseq-rf. on datasets rpi488 and npinter, rpiter also performed better than rpiseq-rf and lncpro and had similar prediction accuracy as ipminer. on dataset rpi1807, rpiter, ipminer and rpiseq-rf achieved similar accuracy and all outperformed lncpro. moreover, on datasets rpi369, rpi1807, rpi2241 and npinter, rpiter yielded the highest sn and auc among all methods. it should be noted that the dataset used to train the lncpro model overlapped with rpi488 [cit] . thus, lncpro performed well on rpi488 but showed poor performances on other datasets. ipminer employed complex stacked ensembling [cit] technique to increase the performance over its basic predictors including rpiseq-rf. in implementation, each of the three basic predictors of ipminer was trained three times by a three-fold cross validation to generate the training and testing data for the lr-based ensemble part. as shown in figure 5, this complicated ensemble manner brought about noteworthy acc increase over rpiseq-rf on datasets rpi488 (0.010), rpi2241 (0.010) and npinter (0.014). nonetheless, the ensemble manner was too time-consuming for deep learning-based models. in contrast, rpiter directly trained each basic module one time, and then concatenated the outputs of four basic modules and trained the whole architecture one time again to make rpi predictions. the boldface indicates the highest metric performance among the compared methods on specific dataset."
"the designed deep learning model rpiter could integrate the advantages of four different basic predict modules and provide a comprehensive rpi prediction result. based on experiments on five benchmark datasets, the proposed rpiter showed good performance in predicting rpi compared with the previous methods."
"the ep matrix is provided as input for the hierarchical clustering algorithm of the xlstat tool in order to obtain multiple nested partitions of the entities of the class as a hierarchical tree. we used in our approach the jaccard coefficient as similarity measure. the jaccard metric seems to be the most intuitive for software entities. moreover, as mentioned above, we used in our approach the bottom-up hierarchical clustering algorithm of xlstat. given a set of n entities, the bottom-up algorithm begins with n singletons (sets with one method), merging them until a single cluster is obtained. at each step, the most similar two clusters are chosen for merging."
"the observer design pattern defines a \"one to many\" dependency between a subject and several observers [cit] . when the subject object changes its state, all observers objects will automatically be notified and updated accordingly [cit] . figure 3 gives an oo implementation of this pattern [cit] . it models a simple system of graphic figures elements in which a class point share the same interface figureelement with other classes. if a call to any of the methods prefixed by \"set\" on an object of type figureelement is triggered, the observer object must be alerted in order to reflect the changes at the graphical representation. according to the object approach [cit], the implementation usually requires that the subject (the observed object) must define a field (list) to provide mechanical registration and deregistration of interested observers (i.e. methods: attachobserver() and detachobserver()) and a notification method (i.e. informobserver ())."
"class cohesion is considered as one of the most important object-oriented (oo) software attributes. it is used to assess the design quality of classes. class cohesion (more specifically, functional cohesion) is defined as the degree of relatedness between members of a class. in oo systems, a class should represent a single logical concept, and not to be a collection of miscellaneous features. oo analysis and design methods promote a modular design by creating high cohesive classes. however, improper assignment of responsibilities during the design phase can produce low cohesive classes with unrelated members. the reasoning is that such (poorly designed) classes will be difficult to understand, to test and to maintain."
"for model design, we found cnn architecture has a powerful fitting ability for the k-mer features of protein and rna sequence compared with sae architecture and machine learning models rf and svm. we infer that our cnn-based modules using convolution neural networks are more complicated than our sae-based modules using simple fully-connected layers, thus the cnn-based modules have larger sample demands for effective training than the sae-based modules. nonetheless, when sufficient samples are available, our cnn-based architecture can perform better in feature extraction and high-level feature representation than the sae-based architecture. [cit] employed sae architecture to process the conjoint triad features of protein sequence for protein-protein interaction prediction. their model would probably have a performance increase if the sae architecture were replaced by the cnn architecture, since cnn is more effective."
"in this study, we proposed a fully deep learning-based hierarchical model, rpiter, which utilizes the sequence and structure information of rna and protein to predict the ncrna-protein interactions. the whole proposed model consists of four modules for analyzing the inputs from two sequence coding parts and an ensemble module for integrating the outputs from basic modules to make a comprehensive prediction result. the whole architecture is shown in figure 1 . first, apply two sequence encoding methods to code the rna and protein sequences with or without structure information; second, use cnn and sae-based modules to form high-level feature representations and output prediction results. third, rely on ensemble module to combine the previous prediction results of four basic modules and further improve the prediction performance."
"our whole model rpiter consist of four basic prediction modules, namely conjoint-cnn, conjoint-sae, conjoint-struct-cnn, and conjoint-struct-sae (described in detail in section 4.4). the prediction accuracy of our four basic modules and whole architecture rpiter on five benchmark datasets by five-fold cv are shown in comparing the performance of modules using different architectures of cnn and sae but the same sequence coding inputs, it was found that the cnn-based modules have advantages over the sae-based modules when the training samples are sufficient. on the two smallest datasets rpi369 and rpi488, the acc of conjoint-cnn and conjoint-struct-cnn were slightly inferior to conjoint-sae and conjoint-struct-sae. on the two medium size datasets rpi1807 and rpi2241 and the largest dataset npinter, the acc of conjoint-cnn and conjoint-struct-cnn had advantages over conjoint-sae and conjoint-struct-sae. on rpi2241, conjoint-cnn and conjoint-struct-cnn obtained 0.885 and 0.879 acc, respectively, which notably better than conjoint-sae (0.859) and conjoint-struct-sae (0.857). on npinter, conjoint-cnn (0.953) and conjoint-struct-cnn (0.953) both yielded a 0.011 higher acc than conjoint-sae (0.942) and conjoint-struct-sae (0.942)."
"clustering aims to differentiate groups inside a given set of objects. the resulting groups (clusters), distinct and non-empty, are to be built so that the objects within each cluster are more closely related to one another than objects assigned to different clusters. the clustering process is based on the notion of similarity (or dissimilarity) between the objects. the similarity between two objects x and y can be derived from the following measure [cit] :"
"the generic concept of similarity (and dissimilarity) is presented in bunge's ontology [cit] . these concepts were first used in the area of oo software measurement by chidamber & kemerer [cit] . in this paper, we are focussing only on hierarchical clustering. the hierarchical clustering methods represent a major class of clustering techniques [cit] . there are two styles of hierarchical clustering algorithms: bottom-up and top-down. in our work, we use as a first attempt the bottom-up approach. given a set of n objects, the bottom-up methods begin with n singletons (sets with one element), merging them until a single cluster is obtained. at each step, the most similar two clusters are chosen for merging [cit] . the hierarchical clustering algorithm is implemented in several statistical analysis tools. in our case, we used the one integrated in xlstat 1, a software for statistical and data analysis for microsoft excel."
"many metrics have been proposed in order to measure class cohesion in oo systems. the argument over the most meaningful of those metrics continues to be debated [cit] . major of proposed cohesion metrics are based on the notion of similarity between methods, and usually capture cohesion in terms of connections between members of a class. based on the underlying information used to measure the cohesion of a class, there exist different classes of cohesion metrics [cit] : structural metrics [cit], semantic metrics [cit], information entropy-based metrics [cit], slice-based metrics [cit], metrics based on data mining [cit] and metrics for specific types of applications [cit] . the class of structural cohesion metrics is the most investigated category of cohesion metrics. structural cohesion metrics measure cohesion on structural information extracted from the source code. these metrics present, however, some differences in the definition of the relationships between members of a class. a class is more cohesive when a larger number of its instance variables are referenced by a method (lcom* [cit], coh [cit] ), or a larger number of methods pairs share instance variables (lcom1, lcom2 [cit], lcom3 [cit], lcom4 [cit], co [cit], tcc and lcc [cit], dc d and dc i [cit] ). several studies using the principal component analysis technique have been conducted in order to understand the underlying orthogonal dimensions captured by some of these metrics [cit] developed a unified framework for cohesion measurement in oo systems that classifies and discusses several cohesion metrics. development of metrics for class cohesion assessment still continues [3, [cit] 37 ]."
"no individual module can always surpass all other modules on all datasets. the architecture of our proposed rpiter combines the advantages of four basic modules with different architectures and sequence coding methods, which can provide a more comprehensive rpi prediction result."
"structure. in order to realize steering either manually or automatically, a direction proportional electrohydraulic valve is installed in parallel with the existing hand pump. the automatic steering system consists of steering control section and steering mechanism section. the system structure is shown in figure 1 . the control commands from the host computer are sent to the steering computer via rs-232 bus communication unit. the wheel turning angle is measured by the angle sensor in real time. the commands of desired steering angle from the host computer and the signal of wheel turning angle are received and processed by the steering computer; then a controlling continuous voltage raging from −10 v to +10 v is sent to the steering mechanism to adjust the proportional electrohydraulic valve to control the guide wheel. the signal of oil pressure is used to confirm the steering status. the steering computer will switch to the manual steering model if the signal for the pressure sensor is a high-level signal or a pulse signal."
"where the dot line denotes the sinusoidal signal, the solid line denotes the nominal system response to the sinusoidal input. similarly, to show the performance of robust stability of the system, now suppose that there exist 20% errors for estimating the gain and process time delay such that both of them are actually 20% larger. the perturbed system response is denoted using the dot line in figure 7 . supposing the gain and process time delay are actually 20% smaller, the perturbed system response is denoted using the solid line in figure 7 ."
"but because the controller ( ) is obtained using the approximate method, the filter can be adjusted near 2 and 1 in practice to obtain the excellent servo performance."
"cache blocking [cit] improves performance by improving the locality of accesses to the x vector, which is also the goal of classical bandwidth reduction techniques [cit] . cache blocking could be applied to the individual submatrices on which pbr operates as well, although likely with smaller benefits."
"we computed the experimental structural connectivity matrix in accordance with a previous work [cit] . briefly, the seed and target rois used for fiber-tracking were obtained by freesurfer. the participants' motions were corrected by the fmrib software library (fsl). fractional anisotropy images were then calculated from the corrected images and used for registering the diffusion-space to the t1-space by a non-linear registration tool (fnirt) in fsl. the local model of the fiber orientations was the fiber orientation distribution (fod), reconstructed at each voxel by constrained spherical deconvolution [cit] with six-dimensional spherical harmonics for the response function. based on the reconstructed fod, fibers were probabilistically tracked by mrtrix. the fiber tracks were generated 105 times from each roi. we calculated the structural connectivity strength as the number of fibers within each roi pair ft divided by the total number of fibers generated from seed roi fs with voxel size normalization: (ft/vt)/(fs/vs), where vt and vs are the number of voxels in the target and the seed roi. since the direction of the structural connectivity strength was not determined by a measurement principle, the structural connectivity matrix was symmetrized by assigning the higher strength to both directions. all parameters were determined as done in a previous work [cit] . the representative structural connectivity matrix was obtained by averaging the structural connectivity matrices of all participants with respect to the participants' common cortical roi (table s1 )."
the spatial correlation between the empirical and simulated rsfcs was obtained by averaging the cross-correlation coefficients of the lower triangular components between the empirical and simulated rsfcs for each parameter set.
"as a control system, the output of the steering system can be regarded the angular position of front wheels corresponding to the steering control input voltage . the steering system can be modeled as"
"because we cannot reduce the number of nonzero values that must be read from main memory, our approach focuses on reducing the size of the index data structure. we identify blocks of recurring patterns and generate custom code for those patterns. as a result, fewer indices are needed since a pair of coordinates expresses the coordinates of each block, rather than needing indices for each nonzero within a block. the microstructure of each block is expressed in the machine code of the inner loop that iterates over all blocks of identical structure within a matrix."
"our evaluation contains multiple parts: first, we demonstrate how pbr-based smvm shortens time to solution, or overall runtime, in both sequential and parallel environments, relative to readily available alternatives. second, we demonstrate the relative impact of the prefetching and vectorization optimizations we applied to pbr. finally, we evaluate the runtime costs associated with the pbr implementation in comparison to benefits by pbr. for all experiments, we used the matrix set in table 1, which was introduced in sect. 2.3."
"the larter-breakspear model is a phenomenological scheme that describes the electrophysiological neuronal dynamics in each region based on structural connectivity. this model consists of the mean membrane potential of the excitatory neurons (v) and the inhibitory neurons (z), and the average number of open potassium ion channels (w). the mean firing rate for excitatory and inhibitory populations are described by q v and q z . the voltage-dependent fractions of open ion channels are described by m ion . these sigmoidal functions describe averaging over a population of ion channels and cell firing rates under gaussian distribution. excitatory interactions between region i and j are described by q v i . simulations were performed using ode23, which automatically chooses the step size, maintains a specified accuracy, and solves ordinary differential equations in matlab. we repeated simulations for each parameter set 10 times with different initial values to reduce the influence of initial values. the simulation length was set to 10 min. we discarded the first 2 min to eliminate the influence of the initial values. all simulation parameters in table 1 were determined based on a previous work [cit] . in the parameter search, the balance of intra-and inter-regional excitatory synaptic connection strength was changed by the global coupling strength c, and the oscillation of the excitatory and inhibitory neural populations was changed by the variance of the excitatory and inhibitory threshold. it took about 2 days to complete 10 min of simulation for a particular parameter set and an initial value using our high performance computer server."
"resting-state brain activities have been extensively investigated to understand the macro-scale network architecture of the human brain using non-invasive imaging methods such as fmri, eeg, and meg. previous studies revealed a mechanistic origin of resting-state networks (rsns) using the connectome dynamics modeling approach, where the neural mass dynamics model constrained by the structural connectivity is simulated to replicate the resting-state networks measured with fmri and/or fast synchronization transitions with eeg/meg. however, there is still little understanding of the relationship between the slow fluctuations measured with fmri and the fast synchronization transitions with eeg/meg. in this study, as a first step toward evaluating experimental evidence of resting state activity at two different time scales but in a unified way, we investigate connectome dynamics models that simultaneously explain resting-state functional connectivity (rsfc) and eeg microstates. here, we introduce empirical rsfc and microstates as evaluation criteria of simulated neuronal dynamics obtained by the larter-breakspear model in one cortical region connected with those in other cortical regions based on structural connectivity. we optimized the global coupling strength and the local gain parameter (variance of the excitatory and inhibitory threshold) of the simulated neuronal dynamics by fitting both rsfc and microstate spatial patterns to those of experimental ones. as a result, we found that simulated neuronal dynamics in a narrow optimal parameter range simultaneously reproduced empirical rsfc and microstates. two parameter groups had different inter-regional interdependence. one type of dynamics was synchronized across the whole brain region, and the other type was synchronized between brain regions with strong structural connectivity. in other words, both fast synchronization transitions and slow bold fluctuation changed based on structural connectivity in the two parameter groups. empirical microstates were similar to simulated microstates in the two parameter groups. thus, fast synchronization transitions correlated with slow bold fluctuation based on structural connectivity yielded characteristics of microstates. our results demonstrate that a bottom-up approach,"
"to predict which block size would yield the best performance, we developed a simple multiple linear regression model that estimates the performance for a decomposition resulting from each potential choice of block size. for sufficiently large matrices, we expect that pbr's execution time is dominated by memory accesses, therefore our model includes three variables that capture the memory accesses performed during the multiplication. the values of these input variables can be derived from statistics collected during the analysis step."
"in the larter-breakspear model, the numbers of excitatory, and inhibitory neurons are the same in each roi, and only the structural connectivity differs between the rois. for this reason, the high-dimensional lead field was averaged in each roi to remove the area's influence in each roi. if the rois were divided into smaller sections and the lead field was not averaged for each roi, the simulated microstates might differ. we note that the contribution rate of the singular value decomposition did not affect the simulated microstates because the cross-correlation coefficient was 0.9995 between the lead fields by singular value decomposition and the mean value."
it can be observed from (3) that the characteristic equation does not contain any time delay. so direct synthesis method is used to design the controller.
"design. automatic steering control system consists of a freescale mc9s12xs128 single-chip controller, rs23 bus unit, power convert unit, output unit, overload signal input unit, angle signal input unit, and pressure signal input unit. the control system structure is shown in figure 2 ."
"to calculate the simulated rsfc, the mean membrane potential of the excitatory neurons was converted into simulated bold signals by the balloon-windkessel hemodynamic model. in this paper, neuronal activity was given by the absolute value of the time derivative of the mean excitatory membrane potential within each brain region. for the ith region, neuronal activity z i increased vasodilatory signal s i, which is subject to autoregulatory feedback. inflow f i responds in proportion to this signal with concomitant changes in blood volume v i and deoxyhemoglobin content q i . the following are the related equations:"
"where ( ) is the desired closed-loop trajectory for set point changes. considering the implementation and system performance, the desired set point tracking transfer function is proposed"
"register blocking [cit] identifies dense substructures and fills in zeros to obtain dense subblocks, which can then be stored using more efficient indices. by contrast, pbr achieves the same index reduction without storing (or computing with) zeros. as such, it will include blocks that register blocking heuristics would reject because they would require too much zero-filling. others have proposed preprocessing [cit] and matrix permutation to find or create dense substructures [cit] ."
"harpertown pbr shows how many iterations would be needed to compensate for pbr's overhead. since the bep depends on the achieved speedup for a given matrix, we also report as a second point of comparison how many csr iterations could be performed in the time it takes to analyze the matrix (analysis), convert to pbr (structure) and link to kernel objects (dynlink) in the same table. these numbers provide a cutoff value below which pbr cannot be amortized. the best block size (bbs) column depicts the block sizes that yielded the best performance. this table excludes the code generation/compilation cost, which is shown separately in table 7 . matrices in this table are sorted by their size, with smaller matrices in the top half. since small matrices benefit less from pbr, their break-even point is generally higher. the break-even point reduces significantly with increasing matrix size."
"the resting-state eegs were recorded for 5 min. the four participants who took part in the fmri experiment fixated on the cross, let their mind wander, and avoided focusing on any one thing. all of the four participants gave informed written consent. their eegs were recorded with a whole-head 63-channel system (brainamp; brain products gmbh, germany). the sampling frequency was 1 khz. electrooculogram (eog) signals were simultaneously recorded and then stored in the eeg."
"the development of biomarkers for psychiatric disorders using microstates and rsfc has been investigated [cit] . one example is autism spectrum disorder (asd). asd patient's rsfc and microstates have different spatial and temporal characteristics compared to healthy individuals. our results could not reproduce enough spatiotemporal characteristics of rsfc and microstates to compare asd patients and healthy individuals. however, our results will give hints to solve these problems and enable making hypotheses about the dependence of slow and fast resting-state brain activity on neuronal network parameters."
"sparse matrix vector multiply (smvm) has long been recognized as an extremely challenging numerical kernel. its speed is limited by available memory bandwidth, particularly on modern architectures. we proposed a novel method to reduce its memory bandwidth requirements by exploiting a memory-efficient index structure that identifies and exploits repeating patterns, which are captured in generated code. unlike previous techniques, our method is agnostic with respect to structure. perhaps counterintuitively, the lion's share of the structure of many matrices that arise in scientific problems can be captured using a relatively small number of frequently recurring patterns. our technique can benefit from features that are available on dominant modern architectures, such as prefetching and vectorization, and it is easily parallelizable."
"agricultural modernization has been realized in most field operations such as tillage, harvesting, weeding, land preparation [cit] . automatic navigation technology of agricultural machinery is one of the most important support technologies for modern agricultural equipment. its application in precision agriculture is regarded as one of the main issues noteworthy for efficiency improvement [cit] and lower cost [cit] . it would desirable to offer the promise of higher efficiency, better and more consistent performance, and reduced labor costs and release the operator from driving to concentrate on implementing other functions. the need for tractor has been reinforced because of its advance in application technology [cit] ."
"our code cache is designed to hold modules for different target architectures and with different optimization options (e.g., sse) simultaneously, thus allowing it to be shared by multiple machines on a network. if needed, the size of the code cache repository could be managed by a periodically scheduled cronjob that deletes files that have not been accessed for a certain time period, which is similar to the strategy used by many unix distributions to purge stale files from the /tmp directory."
"as with any optimization method, pbr's overhead must be evaluated relative to the benefits it provides. we relate pbr's overhead to its performance benefits over the csr method in the context of iterative solvers. the break-even column (bep) in table 5 table 4 average of parallel speedups given in fig. 12 2 core 4 core 8 core"
"it can be seen from (2) that the electrohydraulic steering system is an integrating plant. the presence of integrating factor leads to an excessive overshoot and long settling time in some cases and gives rise to consequence that the balance between the input and output may be easily destroyed by a load disturbance. as for this system, the performance and robustness of the controllers designed based on conventional pid methods with unity feedback control structure cannot be fully achieved. the main advantage of the proposed method is that it is simple with two controllers. it can be seen from the design procedure that the performance of servo and load disturbance rejection response are decoupled completely and can be monotonically tuned to meet a good performance by controllers ( ) and ( ), respectively. what is more, the two controllers are designed with pid form, and the performances can be obtained by the tuning parameters and, respectively. from the responses illustrated in figures 4 and 6, it can be observed that the proposed method for electrohydraulic steering system provides better performance for both set point tracking and disturbance rejection. figures 5 and 7 show that the designed control system holds the robust stability of the load disturbance response well in the presence of the severe process uncertainty."
"the objective of this investigation is the development of electrohydraulic steering control system for autonomously 6 journal of electrical and computer engineering guided agriculture tractor. firstly, a steering control system has been developed based on single-chip computer, which controlled bipolar electrohydraulic steering valve by controlling continuous output voltage from −10 v to +10 v according to electronic commands sent from the host computer and real time data obtained from sensors. secondly, as for the openloop transfer function of the electrohydraulic steering system model with the control voltage signal as the control input and the guide wheel angular position as the control output, a modified smith predictor scheme was proposed based on a two-degree-of-freedom control structure. servo controller and disturbance rejection controller were all designed in the form of pid and could be monotonically tuned by a single adjustable parameter, respectively. lastly, simulation results demonstrated the validity of the proposed control scheme."
"the relative contribution of prefetching and vectorization is shown as zero if prefetching or vectorization did not improve performance. for a few matrices, these optimizations yielded slight slowdown. on harpertown, small matrices benefit particularly from vectorization, whereas the benefit of prefetching is more pronounced on the opteron."
"pbr faces a number of limitations that affect its applicability and performance. first, there is the inherent requirement that a block size exists for which there is significant nonzero coverage. second, pbr may decrease the locality of the right-hand side x vector. third, whereas smvm using csr writes each element of the left-hand side vector y only once, pbr may require multiple reads and writes. if the x or y vectors do not fit into the cache, such as for very sparse problems, the performance impact of these limitations may reduce pbr's effectiveness."
"we optimized our baseline implementation by applying explicit prefetching for the matrix elements. although the streaming pattern of these accesses would ordinarily prevent gains from prefetching, prior work on smvm optimizations has shown [cit] that explicit prefetching is beneficial on sse-enabled x86-based architectures, which are used by 91% [cit] top-500 list [cit] . prefetching can place data directly into the l1 cache and labels cache entries with the correct temporal locality, thus allowing eviction in preference to other data such as elements of the x and y vectors, which may be reused. explicit prefetching is implemented via the gcc __builtin_prefetch() compiler intrinsic, which results in the emission of prefetch* sse instructions. our code allows varying the prefetch distance at runtime, which enables dynamic tuning of the prefetch distance on a per matrix basis."
"the structure conversion step creates the block indices and arranges the nonzero values in pbr format for the selected blocksize. we keep the (row, col) block indices and the matrix nonzero values in two one-dimensional contiguous arrays. as depicted in fig. 4, the nonzeros and indices of all blocks that belong to the same pattern are stored in contiguous slices of these arrays, which guarantees spatial locality in the inner loop of each kernel. because the number of blocks for each recurring pattern has been determined during the analysis step, the start and end locations of these slices can be precomputed in a single pass over all patterns. thus, nonzeros and block indices can be copied directly to their target destinations by maintaining pointers to the current nonzero and block index offsets within each pattern's slices."
"our model correctly selected the best-performing block size for 12 matrices. for the remaining 17 matrices, the performance loss due to mispredicted block size was less than 3%, with a maximum slowdown of 12% for the finan512 matrix."
"in this study, we investigated whether there is a model that simultaneously explains two experimentally observed phenomena in the resting state: slow fluctuation manifested by resting-state functional connectivity (rsfc) and fast transient dynamics manifested by eeg microstates. we simulated a neural mass model using the larter-breakspear model constrained by the structural connectivity and optimized the model parameters [the global coupling parameter and the local gain parameter (variance of excitatory and inhibitory threshold)] by fitting the simulated rsfc to the experimental rsfc and the simulated microstates to the experimental microstates. as a result, we obtained three key findings: first, the parameter sets with high fitting performance to rsfc overlapped with those with high fitting performance to the microstate; second, two distinct parameter sets were identified within the overlapped parameter region; third, the overlapped parameter sets are much narrower than the parameter sets obtained by fitting only rsfc. in other words, based on these three findings, both fast synchronization transitions and slow bold fluctuation changed based on structural connectivity in the overlapped parameter regions. empirical microstates were similar to simulated microstates in these regions. thus, fast synchronization transitions correlated with slow bold fluctuation based on structural connectivity yielded microstates. these results suggest that adding the microstates as fitting criteria is important for significantly reducing the uncertainty of good model parameters. the maximal cross-correlation coefficient of the rsfc was about 0.45, which is almost the same value as that in previous research [cit] ). the maximal cross-correlation coefficient of the microstates was about 0.7, and the mean transition times of the simulated microstates were about 1.5 or 2.0 times longer than those of the empirical microstates."
"as can be seen from (5), this closed-loop transfer function zero introduces an overshoot for the servo response for the electrohydraulic process. also, whenever there are step changes in the set point, there is possibility that there exists set point kick. so in order to improve the servo performance and reduce the overshoot, a set point filer is designed"
"concerning the empirical microstates, ms2 accounted for the highest proportion, and their mean transition times were about 100 ms, as in the previous research ( figure 3a) ."
"for matrices with 80% or more nonzero coverage, which account for at least 30 of the 53 matrices on both architectures, pbr provides a maximum speedup of 3.41 with an average of 1.53 on the harpertown and a maximum speedup of 2.32 with an average of 1.64 on the opteron."
"we computed the functional connectivity matrix using parcellation defined by anatomical automatic labeling (aal) for each participant. we extracted a representative time course in each region by averaging the time courses of the voxels therein. a band-pass filter (transmission range, 0.008-0.1 hz) was applied to these sets of time courses prior to the following regression procedure. the filtered time courses were linearly regressed by the temporal fluctuations of the white matter, the cerebrospinal fluid, and the entire brain. here, the fluctuation in each tissue class was determined from the average time course of the voxels within a mask created by the segmentation procedure of the t1 image. these extracted time courses were bandpass-filtered (transmission range, 0.008-0.1 hz) before the linear regression, as was done for the regional time courses. all parameters were determined as done in a previous work [cit] . then a representative rsfc was obtained by calculating the cross-correlation coefficients among the bold signals for each participant and averaging the rsfc of each participant with respect to the common cortical roi in all participants."
"the pbr library provides a conversion routine that implements the matrix analysis, structure conversion, and any necessary code generation. users provide the input matrix in csr format, thus making pbr drop-in compatible for any codes exploiting this widely used format. the conversion routine returns an opaque handle that is used in subsequent smvm operations. the handle refers to an internal data structure that encapsulates matrix-specific information (such as dimension and sparsity), pbr-specific information (such as blocksize, number of recurring patters, nonzero coverage and identifiers for generated code and compiled object files), the data structures to hold the converted matrix itself (including nonzero values, block indices, list of patterns and their occurrence) and the remainder matrix in csr format. an analyzed matrix structure can be saved to and restored from disk, allowing re-use of the analysis results for the same or matrix or other structurally identical matrices. the library is callable from c code, but it is implemented in portable c++ using several high-performance container and utility classes provided by the stl [cit] and boost [cit] libraries. the custom smvm kernels that implement the multiplication step are generated as c code and compiled with an optimizing c compiler. the coverage, index overhead savings, and number of patterns are based on the block size that yielded the best performance on the intel harpertown (hpt) and amd opteron (opt) architectures table 1 . values are tabulated for the block size that yielded the highest performance on each of the architectures shown table 2 frequency with which each block size yielded maximum performance for matrix set shown in table 1 2 pbr conversion involves the following steps: analyzing the matrix structure to find recurring patterns, selection of a block size, structural conversion, code generation, compilation (if necessary), and loading. the following sections describe these steps."
"our model must also predict the performance of the remainder matrix, which is kept in csr format. we modeled csr's performance with a separate multiple regression model that is based on matrix dimension and number of nonzeros. section 4 discusses the computation of parameter estimates and the fit of these models for our training set and the architectures we considered."
"these charts also contain the results we obtained using oski 1.0.1h [cit], a widely used optimization library. we used oski's aggressive tuning option without providing structural hints. oski is consistently slower than pbr, and even provides little or no speedup compared to naïve csr on the architectures considered, which is consistent with earlier results obtained by others [cit] . we built a basic thread pool implementation on top of linux's pthreads api. to avoid the potentially random placement of threads onto cores by the os scheduler, we used the portable linux processor affinity (plpa) interface by the openmpi project [cit] to explicitly set cpu affinity for each thread, which forces a fixed mapping of threads to cores. when using 2 threads, we placed threads on neighboring cores that share the same l2 cache on harpertown and the same l3 cache on opteron. when using 4 threads, we placed them onto the same socket, thus sharing a single front-side memory bus."
"sparse matrix vector multiply (smvm) is a numerical kernel that dominates the runtime of iterative solvers for systems of linear equations, which form the core of many scientific codes. compressed representations of sparse matrices lead to a low ratio of floating point operations to memory accesses. for instance, if a matrix is represented in the commonly used compressed sparse row (csr) format, each floating point multiply operation is accompanied by at least two memory accesses that trigger compulsory cache misses: one to retrieve the matrix element and a second one to retrieve its column index. since accesses to main memory proceed at only a fraction of cpu speed [cit], sparse solvers achieve only a small fraction of the processor's peak performance."
"where cov denotes the nonzero coverage by pbr. we saw that for 25 matrices, the total error was less than 25% for both the predicted block size as well as the block size that performed best. therefore, the cancelation of model errors is not significant for most matrices, and the high model accuracy is due to the good fit of the model."
"we presented a library that performs matrix conversions on the fly, allowing our method to be used as a drop-in replacement for existing methods. we evaluated pbr by demonstrating its performance advantage for sequential and parallel implementations and by quantifying its overheads relative to its benefits. we described a performance predictor for choosing a blocksize that achieves an optimal or near-optimal performance. for many practical scenarios in which matrix structures are reused sufficiently often, pattern-based representation can augment the arsenal of tuning methods for sparse matrix-vector multiplication."
"since the structural connectivity in this study had stronger interhemispheric connectivity ( figure s1a ) than in the previous research, the cross-correlation coefficients of rsfc were about 0.05 higher than in the previous research [cit] ."
"the results of a comparative evaluation study of techniques for improving smvm performance [cit] concur with our emphasis on reducing memory bandwidth usage. another recent study [cit] considered the impact of several known optimization techniques for smvm on emerging multicore architectures. these techniques included thread blocking, cache and register blocking, prefetching, sse-based simdization of dense substructures, and others. we included the matrices used in this study in ours and found that many significantly benefit from pbr. we did not compare pbr to the optimizations implemented in that study because the implementation is not readily available."
"here, ( ) is designed using the method of unit feedback based on internal mode control theory [cit] . − . in order to ensure that the system is internally stable, the filter is designed as"
"he and oy: conceptualization, empirical data analysis, simulation programs, and simulation data analysis. oy: funding acquisition. nh: empirical data acquisition. he, nh, and oy: writing the paper."
"the result of the analysis step provides, for each of the considered block sizes, the number and kind of distinct recurring patterns. no information regarding block locations in the matrix are collected at this point. we exclude patterns for which the product of pattern frequency and number of bits contained in the pattern does not meet our threshold, and we exclude patterns with less than three nonzeros."
"blocking [cit] reduces this memory overhead if a matrix contains dense substructures. instead of recording one index per matrix element, blocked representations record one index per block. however, blocking may require zero-filling, which introduces unnecessary memory and floating point operations, and it cannot be applied if a matrix does not contain dense substructures or if those structures cannot be identified."
"the code generation step generates custom c functions for all qualifying patterns, stores them to disk, and invokes the c compiler to create shared .so object files. each .so file is dynamically linked into the process's address space. a pointer to the c function it contains is added to an array of function pointers. the outer loop of the smvm routine iterates over this array and invokes the generated block multiplication functions."
"the ms7 and ms11 in each simulated case have a larger occupation ratio than the ms3 in the experiment (figure 3) because the structural connectivity strength between the occipital regions is weak ( figure s1a) . we assume that the interhemispheric structural connectivity was important for reproducing ms4, considering that ms4 has an equal distribution for the distant interhemispheric positions. thus, the constraints based on structural connectivity are more important than the signal noise and the conduction delay proportional to the inter-regional distance. signal noise probably affects temporal transition and the conduction delay proportional to the interregional distance probably results in rois close to each other being synchronized."
"the occupation ratio of the empirical and simulated microstates differed and the simulated microstate did not reproduce the specific spatial patterns of the empirical microstates. the restingstate alpha waves in the cerebral cortex are empirically affected by the thalamus [cit] . however, the larter-breakspear model in this study does not include thalamic dynamics, conduction delay proportional to inter-regional distance, signal noise, or serotonin receptor density. therefore, incorporating these effects in this model might more fully reveal how well the simulated rsfc and microstates reflect the empirical rsfc and microstates in realistic conditions."
"because the tractor can only provide voltage 12 v dc, voltage stabilization module sdw50-12s24 is selected to convert 12 v dc into 24 v dc, and voltage stabilization module sdw50-12s5 is selected to convert 12 v dc into 5 v dc. as for the output unit, the control signal from the controller is demanded between −10 v and +10 v, dac0832 chip is selected to output continuous voltage from −10 v to +10 v, and at the same time wd12-5d15b1 unit is selected to provide standard external voltage between −15 v and +15 v for amplifier, choosing ad581 unit to provide 10 v reference voltage for dac0832 chip. as for the overload signal input unit, iso-a4-p1-o4 module is selected to transfer the input of 4-20 ma standard signal into 5 v isolated signal for ensuring safety for the hydraulic valve [cit] . as for the angle signal input unit, we select a linear potentiometer attached on the hydraulic steering cylinder as an angle sensor."
"to compare the empirical and simulated microstates, we converted the mean membrane potential of the excitatory neurons into simulated eeg signals by the lead field. first, the simulated eeg signals were calculated by multiplying the mean membrane potential of the excitatory neurons by the lead field. second, a common average reference was applied for simulated eeg signals and the simulated eeg signals were bandpass-filtered between 10 and 15 hz. third, the bandpassfiltered signals were downsampled from 1,000 to 100 hz. finally, the simulated microstates were acquired by applying the basic nmicrostate algorithm and the segmentation smoothing algorithm to the obtained signals."
"figures 9 and 10 compare the performance of unoptimized (plain) sequential pbr to the performance of pbr with prefetching and pbr with vectorization on harpertown and opteron, respectively. speedup results are shown relative to the naïve csr performance."
"rsns are characterized by an rsfc based on slow fluctuations observed by fmri and microstates based on fast synchronization transitions observed by eeg. simulated rsfcs and microstates, which were obtained by the larter-breakspear model that integrated the empirical structural connectivity, were compared with the empirical rsfcs and the microstates. regarding slow fluctuation, the mean excitatory membrane potentials were converted into blood-oxygen-level dependent (bold) signals by the balloon-windkessel model. next, after global fluctuations were regressed out of the bold signals, simulated rsfcs were obtained by calculating the cross-correlation coefficients among the bold signals. the empirical and simulated rsfcs were evaluated for their spatial pattern similarity. regarding the fast synchronization transitions, a simulated eeg was obtained by multiplying the lead field and transformed into microstates by applying modified k-means clustering. we evaluated the empirical and simulated microstates for their spatial pattern similarity and non-stationary switching of microstates (figure 1 )."
"the phase-locking matrix quantitatively visualizes the interregional interdependence of the non-linear simulated neuronal dynamics. first, the 10-15 hz bandpass filter was applied to the mean membrane potentials. second, these signals were hilberttransformed, extracting the phase θ . finally, we calculated the phase-locking values (plvs) between regions p and q by the following formula:"
"we compared our parallel implementation of pbr to a parallel implementation of naïve csr. we parallelized csr in a manner that partitions the matrices' rows such that each thread operates on roughly the same number of nonzero elements. we used the same thread pool implementation as for pbr. figure 11 shows the overall runtime of parallelized pbr relative to parallelized csr for 2, 4, and 8 threads on intel harpertown (top) and amd opteron (bottom). for this discussion, we consider only the subset of matrices that provided more than 80% coverage and that yielded at least 1.1 sequential speedup. a value greater than 1.0 indicates that the use of pbr is beneficial for a given number of threads. these results indicate that pbr retains its performance advantage over csr in parallel as well, providing a maximum speedup of 3.8 on harpertown and 5.0 on opteron using 8 cores. figure 12 depicts parallel speedup by csr and pbr relative to their respective sequential implementation for the same matrix set used in fig. 11 . the averages of the speedups in this figure are summarized in table 4 . both methods exhibit superlinear speedup with 8 threads for medium sized matrices due to doubled memory bandwidth and cache capacity by using two sockets. these results are in line with csr superlinear speedups that have been observed on similar architectures by others [cit] ."
"in this study, to evaluate experimental evidence of restingstate activity on two different time scales but in a unified way, we investigated a connectome dynamics model that explains both experimental rsfc and microstates. we used the larterbreakspear model, in which the inhibitory and excitatory neurons in one region are connected with those in other regions based on a connectome measured with diffusion mri. we optimized the global coupling strength and the local gain parameter (variance of the excitatory and inhibitory threshold) of the simulated neuronal dynamics by fitting both rsfc and microstate spatial patterns to those of the experimental ones. as a result, we found that fast synchronization transitions correlated with slow bold fluctuation based on structural connectivity yielded characteristics of empirical microstates. in detail, we found that the parameter sets with high fitting performance to rsfc overlapped with those with high fitting performance to microstates and that the optimal parameter range was greatly reduced by adding microstates as evaluation criteria compared with not adding them as in a previous work [cit] ). we found two parameter regions where both rsfc and microstate spatial patterns were reproduced with moderately high accuracy: one had a high local gain (high variance of the excitatory and inhibitory threshold) and weak global coupling strength, while the other had a low local gain (low variance of the excitatory and inhibitory threshold) and strong global coupling strength. in investigating the neural mass dynamics generated from these two parameter sets, the former showed highly periodic and synchronized activation; the latter showed fewer synchronized and periodic activations. the temporal transition of the simulated microstates for the former parameters persisted for about 200 ms, and that for the latter parameters persisted for 150 ms. both resulted in longer durations than the experimental data. our results demonstrate that a bottom-up approach, which extends microscopic models of single-neuron dynamics based on empirical studies [cit] into a mesoscopic neural mass dynamics model [cit] and integrates macroscopic structural connectivity, can effectively reveal both macroscopic fast and slow resting-state network dynamics that are observed in human neuroimaging measurements."
"using the identified electrohydraulic steering model, the simulation is programmed using the matlab simulink toolbox. for illustration, two groups of simulation test are made for the electrohydraulic steering system with these controllers' settings."
"the remainder of this paper is structured as follows: sect. 2 describes our method in detail. section 3 describes our implementation, including matrix structure analysis, blocksize detection and code generation. section 4 provides an extensive performance analysis and discusses limitations. section 5 compares pbr to related approaches and sect. 6 concludes."
"overall, these results show that the block sizes selected by our predictor model lead to optimal or near-optimal speedup. we note that block size prediction is an optimization; if the optimal block size is desired and a matrix structure is sufficiently often reused, it is possible to perform a trial structure conversion using each block size and choose the optimal block size via benchmarking."
"since empirical eeg and rsfc were obtained by two studies aiming at different purposes, the number of participants of eeg and rsfc differ. empirical rsfc was obtained from 13 participants, sufficient for statistical purposes. empirical eeg was obtained from only four participants. however, the four microstates identified from this empirical eeg are similar to the four microstates identified by other studies [cit] b) . we presume the four microstates will be similar to the microstates identified from 13 participants."
"the studies involving human participants were reviewed and approved by the ethics committee of the advanced telecommunication research institute international, japan. the patients/participants provided their written informed consent to participate in this study."
"in this paper, we seek to design an electrohydraulic steering control system for autonomously guided agriculture tractors. a control method is proposed in order to improve the steering performance. the paper is organized as follows. the automatic steering control system is designed in section 2. the mathematical model of the electrohydraulic steering used on the tractor navigation platform is proposed in section 3. the form of the steering controller and design specifications are given in section 4. simulations are carried out to demonstrate the validity of the proposed control program in section 5."
"because our prediction includes two components, the pbr and the remainder csr parts, prediction errors with opposite signs may cancel each other out. to validate that the high accuracy of our model was not due to chance, we computed the weighted sum of the absolute values of the relative error η for both the pbr and the csr remainder parts:"
"the eeg data were preprocessed with a low-pass fir filter with a cutoff frequency of 50 hz, downsampled at 100 hz, and passed through a high-pass fir filter with a cutoff frequency of 0.5 hz. after a common average reference, the eog artifacts were removed by generating a multiple linear regression model to predict the eye-movement-related components in the eeg data using the eog data. cardiac artifacts and sensor noise were removed by ica. all of the eeg data were converted into empirical microstates."
"to realize high device speed and rf operation of devices, the nms must be doped. the above discussion and all prior work that uses self-sustained strain relate to undoped nms. when we attempted to dope the strained trilayer nm prior to release, using the doping approach that has been successfully applied to unstrained si nms 29, 30, the strained nm curled upon release and became very rough. figure 2a briefly illustrates the application of the nm prerelease selective ion implantation doping approach, but being applied to the strained trilayer nm. a detailed doping process flow for unstrained si nms is shown in figure s1 . figure 2b shows a 3d microscopic image of a doped and annealed trilayer nm after release from the handling substrate. as can be seen, the topology of the released trilayer nms becomes unsuitable for further processing, such as transfer and patterning. we believe that the si/sige/si epilayer structure has been damaged through atomic mixing and crystalline defects created by the implanted ions. the subsequent annealing procedure intended for recrystallization failed to restore the ion implanted damaged layers and interfaces back to their original order. instead, significant and unbalanced stress was built in the nm after performing the doping processes, which caused severe curling of the released nms."
"in this study, 90% of the training set was randomly selected to optimize the parameters of the proposed two-branch neural network. the remaining 10% of the training samples were used as the validation set to justify the performance of the network during training process. as for the testing set, it was only used for calculating the final overall accuracy and the confusion matrix after the network was well-trained."
"in addition, the confusion matrix indicates that most classification errors occurred among the following land-use types: highway, railway, road, and parking lot 1. this is mainly because all those land-use categories belong to impervious surfaces that share similar spectral properties. highway, railway, and road also share similar shape features, which could increase the difficulty in separating between them using a patch-based cnn model, since the cnn takes spatial contextual information into consideration when classifying each pixel. other errors occurred where several healthy-grass and stressed-grass pixels were misclassified as the railway. this is uncommon in remote-sensing image classification, since grass and railway share different spectral characteristics. however, when checking the classification map, it is in the eastern cloud-covered regions that several grass areas were misclassified as railway. this is mainly because the existence of heavy cloud distorted the spectral curves of the grass, which led to the uncommon classification errors."
"in order to assess the performance of the proposed two-branch network for urban land-use classification, both visual evaluation and a confusion matrix were adopted in this study. visual inspection was used to check the visual effects, while the confusion matrix, derived from the testing samples, was used to quantitatively assess the classification accuracy of the proposed method. [cit] ieee grss data fusion contest, which are the same as in reference [cit] ."
each column in the original matrix x is a linear combination of the columns of u weighted by the components of the corresponding column in v. therefore u can be regarded as containing a basis that is optimized for the linear approximation of the data in x [cit] . it is proven by lee and seung that the objective function o in (1) is nonincreasing under the update rules (2) and (3).
"where tp is true positive, fp is false positive, n is the number of data points, k is the number of clusters, c is the number of classes, c i is the i th cluster, l j is the j th class, i(x;y) is the mutual information between two random variables x (the cluster ) and y (the class)."
"the remainder of this paper is organized as follows. section 2 presents the related work on clustering using nmf. section 3 derives the proposed method along with the formal proofs. section 4 presents the experimental results, followed by section 5 featuring some concluding remarks."
"dc and rf characterizations of devices. dc characteristics were measured with an agilent 4155 semiconductor parameter analyzer in a dark environment. for rf characteristics, scattering (s) parameter measurement was taken using an agilent e8364a network analyzer. the ''open'' and ''short'' features were used for a deembedding procedure to obtain the intrinsic rf characteristics of device. the deembedding procedure follows the equation;"
"in summary, we have demonstrated a simple and viable approach to realizing strained-mono-crystalline-si rf transistors on flexible plastic substrates. this technique has great potential in low-power and high-speed flexible-electronics applications, and could be used to replace a number of rigid counterparts for use in mechanically bendable and non-planar conformal surfaces where rigid devices cannot be easily used. one can foresee as a consequence manufacturable large-area applications of such flexible high-speed thin-film transistor technology."
the proposed two-branch network was trained with the tensorflow library [cit] on ubuntu 16.04 operation system with an intel core i7-7800 @ 3.5 ghz cpu and an nvidia gtx titanx gpu with 12 gb memory.
"the numbers of training and testing samples together with colors for each class are shown in table 1 . as can been seen, the number of training samples is quite limited, which makes it very difficult to achieve high classification accuracy."
"clustering has been an active area of research in data mining and machine learning due to the rapidly growing data in different domains such as biology and clinical medicine. in biology, for instance, there is an avalanche of data from novel high throughput and imaging technologies. when applied to cancer images, clustering has been effective in identifying malignant and normal breast images [cit] . biomedical publications often present the results of biological experiments in figures and graphs that feature detailed, explanatory footnotes and captions. this annotation comprises a simple, textual representation of the images. in the clinical literature, a new semantic representation has evolved as a result of mapping the words in physicians' clinical notes to the corresponding semantic descriptors in the unified medical language system (umls). each representation of the data e.g. images, captions and semantic descriptors, is a unique data modality generated by a particular process wherein the objects have different features, structure and dimensionality. differential encoding of the features of each modality causes variability in the obtained partitions when clustering around the individual data modality. in this discussion we explore alternative methods of building clusters around the complementary data modalities of a particular dataset to obtain more cohesive clusters. unlike current algorithms which cluster on a single data modality, our proposed approach creates clusters by extracting information from completely different domains of information that describe the same data."
"the gui interface is implemented with a cross-platform gui toolkit. the interface provides a straightforward way for users to input matlab files, set moccasin options such as the type of output (sbml or xpp), view the resulting output and save the converted file."
"to demonstrate the effectiveness of the self-sustained straining approach, figure 1c shows an x-ray diffraction (xrd) off-axis reciprocal-space map (rsm) around the (044) reflection for the as-grown trilayer structure. the rsm indicates that the sige layer is strained to the si lattice constant; the si and sige peaks lie along the same vertical line. this result confirms that there is no plastic relaxation of the mismatch strain in the alloy before release of the trilayer (sige layer thickness is below kinetic critical thickness for dislocation formation). figure 1c also illustrates the strain sharing results in the si/sige/si layer (an undoped sample is shown). on-axis xrd lines scans around the (004) reflection allow us to measure the outof-plane lattice constant change that occurs during strain sharing. in the as-grown state the sige layer is compressively strained to the si lattice constant; an in-plane compressive strain translates to an outof-plane expansion (smaller bragg angle). after release, the sige becomes less compressively strained and the si layers become tensile strained. the expansion in the in-plane lattice constant leads to a reduction in the out-of-plane lattice constant of both layers. both the sige and si peaks shift to higher bragg angles (10.1 degree), indicating that elastic strain sharing occurs between sige and si after release; the relaxation of compressive strain in the sige layer is equal to the tensile strain transferred to the si layers. the strain in the trilayer nm is now self sustained in the released and freestanding nms."
"the structure of the se block is described as follows. for any given features u, they are first passed through a global average pooling (gap) layer to produce a channel descriptor, which embeds the global distribution of channel-wise feature responses. this is followed by two fc layers and a sigmoid layer, in which the channel-specific weight can be learned through a self-gating mechanism based on channel dependence. the output features of the se block were already recalibrated or reweighted, leading to adaptively emphasizing the informative features and suppressing the less useful ones. compared with the traditional feature-stacking fusion method, the se module in this paper can provide a more effective and rational way for feature-level fusion of multisource data."
"to further justify the performance of the proposed approach, it should be compared with other widely used machine-learning methods, such as random forest (rf) [cit], support vector machines (svm) [cit], and state-of-the-art methods."
"meanwhile, the hsi branch yields a better classification map with fewer errors than that of the lidar branch. however, due to the large spectral variance of different urban land-use types, hyperspectral data alone could also result in inaccurate classification results. for instance, the eastern area of the image is covered by some clouds, which leads to the spectral distortion of certain landuse types, resulting in more classification errors. the lidar data alone also do not contain enough information to differentiate complicated urban objects, especially for different objects with the same or similar elevation. nonetheless, the fusion of hsi and lidar data can avoid the above demerits and benefit from both the spectral characteristics of the hsi image and the geometric information of the lidar data, which could lead to a better classification map."
"the clustering results are evaluated by comparing to gold standard annotations of images and radiology reports. we use three measures to evaluate the quality of the clusters: micro-averaged precision, purity and normalized mutual information (nmi). microaveraged precision is an average over data points, which by default gives higher weight to those classes with many data points. nmi measures the amount of information by which our knowledge about the classes increases upon definition of the clusters."
"matlab is difficult to parse fully [cit] : the language is complex and idiosyncratic, and there is no published definition of its syntax rules. we did not attempt to develop a complete parser for matlab; instead, we leveraged the fact that moccasin's input is already expected to be syntactically valid matlab (because users are converting working code), and thus moccasin's parser can be simpler and make more assumptions. the parser creates an internal representation that is essentially an embellished abstract syntax tree (ast)."
"meanwhile, we selected xu's model [cit] as a strong baseline since it first utilized a two-branch cnn for hsi and lidar data fusion, and achieved an oa of 87.98%, [cit] ieee grss data fusion contest testing dataset. all the above methods were trained and tested with the same training and testing samples as the proposed method to maintain fairness. the accuracy comparison results are listed in table 7 . table 7 indicates that our proposed modified two-branch cnn outperformed both rf and svm with an oa improvement of 7.90% and 7.71%, respectively. this was expected since, when compared with traditional machine-learning methods, the cnn could learn high-level spatial features of complicated and fragmented urban land-use types, which led to a more robust and accurate classification result."
"in order to quantitatively evaluate the proposed approach in the study, the confusion matrix, together with the overall accuracy (oa) and kappa coefficient, were calculated based on the testing samples. results are shown in the table 4 . the proposed two-branch network shows good performance, with an oa of 91.87% and a kappa of 0.9117. however, the highway class had the lowest producer accuracy (pa) with 80.89%, while all the other classes had a higher pa of more than 83%. this could be due to the spectral mixture between highway and other impervious surface types, such as railway and commercial areas, since they all consist of concrete materials. it should also be noted that all highway training samples are outside the cloud-covered regions, while nearly half of the highway testing samples are from the cloudcovered regions. this could cause the spectral inconsistency between training and testing samples of the highway class, which could result in relatively lower classification accuracy."
"since the proposed approach used a pixel-centered patch as input to the networks, more comparative experiments should be done to compare the performance between the pixel-based and patch-based classification, and to investigate the effect of the pca and non-pca approaches. therefore, a series of ablation experiments were performed, and the comparison results are shown in the following table. specifically, since the input was changed to 1d pixel vectors, all the 2d convolutional layers of the original patch-based two-branch cnn were replaced by 1d convolutional layers in the pixelbased cnn, while all parameters remained the same. as can be seen from table 6, the patch-based models outperformed the pixel-based ones with an accuracy increase of 7.89% and 5.82% for the non-pca and pca-based approaches, respectively. this is mainly because, when compared with the patch-based model, the pixel-based model only considers the spectral characteristics of the land object. however, the patch-based model can take both the spectral and spatial contextual information into account, leading to more discriminative and representative features which are essential for classification. when compared with non-pca approaches, the usage of pca has a positive effect on classification, leading to an accuracy increase of 4.56% and 2.49% for the pixel-based and patch-basedmodels, respectively. this is due to the fact that pca can effectively reduce the data redundancy of the original hyperspectral imagery, which can reduce the overfitting risk of the convolutional neural network and, thus, improve its generalization ability when predicting on new datasets."
"thus ( ) ( ). we conclude the proof of theorem 1 by checking that (21) corresponds to (15). indeed, given (22) and (26), we can get by solving ( )"
"in addition, all the above studies are based on shallow architectures and hand-crafted feature descriptors, which cannot obtain the fine and abstract high-level features of a complex urban landscape. deep learning, on the other hand, is capable of modeling high-level feature representations through a hierarchical learning framework [cit] . abstract and invariant features, together with the classifiers, can be simultaneously learned with a multilayer cascaded deep neural network, which outperforms hand-crafted shallow features in computer-vision tasks, such as image classification [cit], object detection [cit], and landmark detection [cit] . deep-learning methods have also been a hot topic in remote sensing [cit], and have been successfully applied to building and road extraction [cit], wetland mapping [cit], cloud detection [cit], and land-cover classification [cit] ."
"where is a quadratic function that depends only on, the generic term of the matrix v. we need to show that the function is non-increasing under the update rule (15), or equivalently find an auxiliary function for such that the update rule (15) corresponds to (21). we compute the first and second order derivatives of . one can easily check that:"
"to further evaluate the performance of the proposed method, a series of ablation experiments were done including: (a) only the hsi branch, (b) only the lidar branch, and (c) feature-stacking (i.e., using stacked or concatenated features of hsi and lidar instead of adaptive fusion for classification). the results of class-level classification accuracy are illustrated in table 5 including all the above three cases together with the proposed two-branch network. table 5 indicates that the lidar branch alone achieved the lowest classification accuracy with an oa of 53.42% and a kappa of 0.4967. this is mainly due to the fact that height information alone can hardly separate different land-use types in complicated urban regions. meanwhile, the hsi branch alone achieved much higher accuracy than that of only the lidar branch, with an oa of 83.83% and a kappa of 0.8244. the reason why the hsi branch alone outperformed the lidar branch alone is that hsi images can provide much more abundant spectral and spatial information of land surfaces than lidar-derived dsms, leading to a higher capability of differentiating complex urban land-use types. table 5 also indicates that, when compared with single-source data alone, the integration of multisource hsi and lidar data leads to a significant improvement of classification accuracy for almost each urban land-use class. this is reasonable since the separability of urban objects could increase if we simultaneously integrate multiple spectral and elevation features. compared with hyperspectral data alone, the integration of lidar data improved oa by 4.42% and 8.04% through feature-stacking and the proposed two-branch network, respectively. in terms of class-level accuracy, the main contribution of lidar data was in the following classes: synthetic grass, tree, soil, water, commercial, railway, and parking lot 1 and 2. this is due to some of the classes (e.g., grass and tree) sharing very similar spectral characteristics but having different height values; therefore, the inclusion of lidar-derived dsms could significantly improve the separability between these classes."
"as is known, training a deep cnn model needs a large quantity of labeled data. however, for remote-sensing applications, it is laborious and time-consuming to obtain enough labeled data. to address this issue, data augmentation was utilized in this study. original training patches were rotated 90°, 180° and 270°, flipped left and right, up, and down to increase the number of training samples. furthermore, classes with fewer training samples were oversampled to tackle the problem of class imbalance."
moccasin features a modular architecture comprised of (i) a module that parses matlab files; (ii) a module that extracts the ode-based model and produces a model with explicit odes; (iii) a module that infers the biochemical reactions implied by the odes and produces sbml output with biochemical reactions for kinetics; (iv) a command line interface and (v) a graphical user interface. python developers can use as few or as many modules as they desire.
"the matlab input currently must conform to certain simple forms and make limited use of matlab features. future enhancements will (i) expand the set of matlab constructs that can be interpreted; (ii) support models spread over several matlab input files; (iii) generate sed-ml (simulation experiment description markup language; [cit] ) files, to encode procedural aspects that cannot be expressed in sbml and (iv) directly implement the biocham reaction inference algorithm [cit], to streamline the translation process."
"matlab is a general-purpose numerical computing environment whose powerful features have attracted many researchers. it has been the substrate for countless models as well as software tools written in its object-oriented programming language. despite its popularity, there are reasons why matlab programs are not themselves a desirable format for exchanging, publishing or archiving computational models in biology. these include the lack of biological semantics in matlab programs, which makes clear interpretation of programs as models of biological processes more difficult; the fact that matlab is proprietary and expensive, which makes it unsuitable as a universal format for open scientific exchange; and the fact that model details are often intertwined with program implementation details, which makes it difficult to determine which parts constitute the essence of a model."
"our algorithm extends nmf using two modalities of the data. we argue that each modality covers certain aspects of the data, therefore utilizing two modalities maximizes the gained benefit and potentially improves the clusters. the two data modalities are represented by the matrices a and b. let a r (mxn), b r (pxn), r (mxk),"
"it should be noted that the proposed two-branch network, which uses adaptive feature fusion, outperforms the traditional feature-stacking method by improving oa from 88.25% to 91.87%, with an increase of 3.62%. this is because, when simply stacking all features together, the values of each feature can be significantly unbalanced, and the information carried by each feature may not be equally represented. therefore, we introduced the adaptive squeeze-and-excitation module to automatically assign a weight to each feature according to its importance, which could integrate multiple features in a more natural and reasonable way, resulting in the accuracy improvement of 3.62%."
"this paper proposed a modified two-branch convolutional neural network for urban land-use mapping using multisource hyperspectral and lidar data. the proposed two-branch network consists of an hsi branch and a lidar branch, both of which share the same network structure in order to reduce the burden and time cost of network design. within the hsi and lidar branches, a hierarchical, parallel, and multiscale residual block was utilized, which could simultaneously increase the receptive field size and improve gradient flow. an adaptive feature-fusion module based on a squeeze-and-excitation net was proposed to fuse the hsi and lidar features, which could integrate multisource features in a natural and reasonable way. experiment results showed that the proposed two-branch network had good performance, with an oa of almost 92% [cit] ieee grss data fusion contest dataset. when compared with hyperspectral data alone, the introduction of lidar data increased oa from almost 84% to 92%, which indicates that the integration of multisource data could improve classification accuracy in complicated urban landscapes. the proposed adaptive fusion method increased accuracy by more than 3% when compared with the traditional feature-stacking method, which justifies its usefulness in multisource data fusion. the two-branch cnn in this paper also outperformed traditional machine-learning methods, such as random forest and support vector machines. this paper demonstrates that the modified two-branch network can effectively integrate multisource features from hyperspectral and lidar data, showing good performance in urban landuse mapping. future work should be carried out on more datasets to further justify the performance of the proposed method."
"(a) hsi branch only, i.e., using only hsi data and the hsi branch for classification; (b) lidar branch only, i.e., using only lidar data and the lidar branch for classification; (c) the proposed two-branch cnn. it is evident that the synthetic use of hsi and lidar data leads to a classification map with a better visual effect and higher quality when compared with the results of only the hsi branch and only the lidar branch."
"in this paper, we demonstrate an enhanced data clustering approach whose innovation is its exploitation of multiple data modalities called bi-nmf. our proposed method is a bimodal clustering algorithm based on non negative matrix factorization. it utilizes two modalities of the data to improve clustering. we applied the method on two biomedical datasets and demonstrated enhanced performance relative to ensemble clustering and nmf based on single and merged data modalities, on three standard metrics. given our results, we conclude that bi-nmf is advantageous for enhanced biomedical data clustering and potentially useful for data from other domains."
"the study area was the university of houston campus and its neighboring urban areas, which are located in the southeast of texas, united states. [cit] ieee (institute of electrical and electronic engineers) grss (geoscience and remote sensing society) data fusion contest [cit] . specifically, the hyperspectral imagery was acquired on 23 [cit], which consisted of 144 spectral bands ranging from 380 to 1050 nm, with a spectral resolution of 4.8 nm. the spatial resolution was 2.5 m, while the height and width were 349 [cit] m, respectively."
"transferrable mono-crystalline si nanomembranes (nms) are suitable for active material of fast flexible electronics when comparing with any other materials for flexible electronics owing to their material uniformity 12, mechanical flexibility 13 and durability 14, electrical properties equivalent to their bulk counterparts 15, easy handling [cit] and processing, and low cost. si has modest mobility values in comparison to most iii-v and other materials 19, 20, but can be easily down scaled for performance improvement. as large critical device dimensions are generally preferred for flexible-electronics applications where cost and large area are often of the most concern, improving device speed using approaches other than dimension downscaling is preferred. among them, strain engineering is one of the most effective ones 21, 22 . in contrast to bulk si, where strain in the active device layer can be easily sustained/held by a rigid substrate 23, in transferrable si nanomembranes strain needs to be self-sustained. here we describe a combined strain compatible effective doping and strain engineering approach that is especially suited for transferrable si nms. strain compatible device fabrication techniques then lead us to desired higher device speed on flexible substrates."
"where m i is the biaxial modulus and t i is the thickness of the respective layers. the mismatch strain, e m, is the amount of strain in the sige before release from the handling substrate."
"doping of unstrained si nanomembrane. effective doping of the sinm is needed to reduce contact resistance in source and drain regions of rf transistors. commercially available soi (soitec usa, 2 centennial drive, peabody, ma 01960, usa) with 200 nm si (001) template and 145 nm buried oxide (box) layers is used as the starting material. the si (001) template is lightly doped with boron. phosphorus ion implantation is used on the soi substrate with a dose of 2 3 10 15 cm 22 and an energy of 20 kev. following the ion implantation, the sample is annealed in a high-temperature furnace at 950uc for 30 mins in n 2 ambient to re-crystallize the si template and activate the dopants. tem cross section images of si nms (unstrained) before and after anneal are shown in figure s1 . the simulated and characterized doping profiles using secondary ion mass sepectroscopy (sims) before and after anneal are shown in figure s2 ."
"focal loss [cit] was utilized as the loss function in this study instead of traditional cross-entropy loss. this is mainly because focal loss has the merits of downweighing the loss assigned to wellclassified examples, which prevents the vast number of easy examples from overwhelming the classifier during training [cit] ."
"all the parameters of the proposed two-branch network need to be trained to generate the best model for urban land-use classification. in this study, a two-step training strategy was used to train the whole network. firstly, the hsi and lidar branches were separately trained with a larger initial learning rate of 10 −4 . secondly, the pretrained hsi and lidar branches were merged through the adaptive feature-fusion module, and the whole network was fine-tuned with a smaller initial learning rate of 10 −5 . the adam optimizer [cit] was used due to its capability of automatically adjusting the learning rate, which could result in a faster and more stable training procedure."
"fast flexible electronics operating at radio frequencies (.1 ghz) are more attractive than traditional flexible electronics because of their versatile capabilities, dramatic power savings when operating at reduced speed and broader spectrum of applications. transferrable single-crystalline si nanomembranes (sinms) are preferred to other materials for flexible electronics owing to their unique advantages. further improvement of si-based device speed implies significant technical and economic advantages. while the mobility of bulk si can be enhanced using strain techniques, implementing these techniques into transferrable single-crystalline sinms has been challenging and not demonstrated. the past approach presents severe challenges to achieve effective doping and desired material topology. here we demonstrate the combination of strained-nm-compatible doping techniques with self-sustained-strain sharing by applying a strain-sharing scheme between si and sige multiple epitaxial layers, to create strained print-transferrable sinms. we demonstrate a new speed record of si-based flexible electronics without using aggressively scaled critical device dimensions. f lexible electronics have been mainly addressing electronic applications operating at low or moderate speed [cit] . for these applications, form factors such as bendability and large area are of more importance than speed. organic semiconductors 4, 5, and amorphous 6 or polycrystalline si 7, which can be processed at relatively low temperature and with low cost, often suffice to address them. on the other hand, there is a wider spectrum of electronics applications where higher speed and mechanical flexibility are simultaneously needed, such as high-speed and wireless communications, remote sensing and airborne/space surveillance 8 . we entitle the special category of such flexible electronics as fast flexible electronics. a number of applications even require the operating speed (frequency) be beyond 1 ghz and hence these applications can be further termed as radio frequency (rf) flexible electronics. fast flexible electronics provide superior performance and application advantages. as is known, high-speed devices consume much less power if they are operated at a reduced speed [cit] 25, which dramatically benefits battery powered devices. wirelessly connected devices enabled only by high operation speed/frequency are more convenient to use than wired devices 1 and a high-frequency wireless system is also generally more compact than a low-frequency one."
"thinning down si nanomembrane. after the ion implantation, the si template is treated in an rie chamber (unaxis 790, 30 w) with sf 6 /o 2 for 40 sec. a dry thermal oxidation at 1050uc for 10 min is applied to the sample to further reduce the si template to the desired thickness in the si/sige/si tri-layer structure. the final thickness of the si template is 48 nm, as verified by xrd measurement. [cit] 20 sec in 10% hf (with a 5-min di water rinse between each step) before putting the sample directly into the high-vacuum growth chamber. we resistively heat the substrate to 475uc during pseudomorphic growth of the alloy and used a growth rate,3 nm/ min. h/2h lines scans around the (004) reflection (figure 1c and 2e) were fit to simulations to extract the ge composition of the sige layer and each of the layer thicknesses before release from the initial growth substrate: 46 nm si/80 nm si 0.795 ge 0.205 /48 nm si. in the as-grown heterostructure, the main peak at lower bragg angles is from the sige layer and the broad peak modulated by the thickness fringes is from the two si layers."
"to realize selectively doped trilayer si nms for rf device fabrication, the selectively doped unstrained si template layer was thinned down followed by epitaxial growth of sige and si layers as performed in the undoped trilayer nm case. figure 2(d) shows images of trilayer nms (in the form of strips) at the different stages of processing. the doped regions for the source/drain/source, with 1.5 mm gaps between them (a two-gate finger device), are clearly identifiable. it is noted that after finishing the growth, only the bottom si layer is heavily and selectively doped, with little dopants diffusing back to the sige or to the top si layers. the doping and thinning down procedures have some effect on the crystallinity of the as-grown si/sige/si trilayers (as indicated by a general broadening of the diffraction peaks in figure 2e (ii)). the sige layer, however, is still strained to the si lattice constant so strain sharing is expected to occur as predicted upon release of these trilayers (e si 5 0.34% 6 0.01%). with this amount of biaxial tensile strain in si, we can expect a,47% increase in the electron mobility 28 . figure 3a provides the illustration of the device (field effect thinfilm transistor, tft) fabrication procedures using the transferred trilayer nm, which is bonded to a polyethylene terephthalate (pet) substrate. using a flip transfer procedure, the selectively doped source and drain regions on the bottom si layer of the trilayer nm becomes the top surface. following the procedures described previously 31, the device fabrication is finished. figures 3b and 3c show the cross section, dimensions, and optical image of a finished device on a pet substrate. figures 3d and 3e show the reference device fabrication and information on the unstrained si nm. the gate length and channel length, which are identically applied to both the strained and the unstrained nm, are 2.5 and 1.5 mm, respectively."
"pubmed images dataset. it consists of 3000 images extracted from articles of pubmed central. images with no captions were dropped and 2607 were retained. the images in the dataset were classified into 5 different categories by domain expert annotators. discrepancies among the annotators were resolved by assigning the image to the category with the majority of votes. the list of annotations is: 564 images were assigned to the experimental category, 1131 images to the graph category, 645 images to the diagrams category, 86 images to the clinical category, and 181 images were assigned to the others category. we generated two modalities for the images. in one modality the images were represented using the pictorial and textural features computed using the haralick method [cit] . the other modality is a bag of words bow representation generated using captions."
"to realize effective doping while maintaining a flat topology of the flexible, strain-shared trilayer nm for fabrication of rf devices, we designed an alternative approach to strain-compatible effective doping processes (figure 2c ). instead of applying the ion implantation and anneal processes directly on a trilayer nm, we first applied them to an unstrained si layer on soi. typically, low-energy phosphorus ion implantation is used to heavily dope the top portion of a si template layer, leaving the bottom portion barely damaged by the implanted ions. an annealing process follows to recrystallize the damage and simultaneously drive the implanted dopants to diffuse until they reach the bottom surface of the template layer, generating a high-level and nearly uniform doping profile across the entire si template layer. leaving the bottom portion of the template layer undamaged during implantation is critical, as the bottom portion serves as the seed layer for recrystallization of the entire layer during the annealing process. detail of ion implantation conditions and results can be found in the method section and figure s2 . after finishing these processes, very low sheet resistance is obtained, which is needed to achieve high-frequency device operation (see the method section). for unstrained-device fabrication, used as reference devices in this work, the selectively doped si template layer is ready for release and transfer to a flexible substrate."
"meanwhile, due to the availability of diverse remote sensors, researchers began to integrate multisource and multisensor data for better characterization of the land surface [cit] . since then, the combined use of hsi and light detection and ranging (lidar) data has been an active topic [cit] . the addition of lidar data can provide detailed height and shape information of the scene, which can improve classification accuracy when compared with the use of hyperspectral data alone. for instance, roofs and roads that are both made of concrete are difficult to distinguish in hyperspectral images, but they can easily be separated using lidar-derived height information due to the significant difference in altitude. based on the above points, researchers investigated the fusion methods of multisource hyperspectral and lidar data. [cit] highlighted two methods for hyperspectral and lidar data fusion, including a combined unsupervised and supervised classification scheme, and a graph-based method for the fusion of spectral, spatial, and elevation information. [cit] discussed both the pixel-and feature-level fusion of hyperspectral and lidar data for urban land-use classification and showed that the combination of pixel-and objectbased classifiers can increase the classification precision. moreover, the fusion of hyperspectral and lidar data has also been applied in many other fields, such as forest monitoring [cit], volcano mapping [cit], and crop-species classification [cit] ."
"nevertheless, the above studies that use a two-branch network have two drawbacks that could be improved. firstly, the data-fusion method of simply stacking or concatenating different features does not consider the importance or contribution of each feature to the final classification task, which could be improved by assigning a specific weight to each feature. secondly, the backbone of the network is conventional, e.g., alexnet [cit], which can be replaced by other recent network structures."
"it is obvious that ( ) ( ) we only need to show ( ) ( ). since is a quadratic form, consider the following taylor series for :"
"is an open format for representing models in systems biology [cit] . designed to resolve incompatibilities between systems that use different formats to describe models, sbml is neutral with respect to modeling framework and computational platform. this helps make models portable across tools, and ensures that models as research products can persist regardless of changes to any particular software tool or operating system. unfortunately, translating models from matlab to sbml is not straightforward. some matlab toolboxes [cit] offer sbml capabilities; however, they have limited utility for translating legacy models, lack support for the latest sbml releases, and must be used from the start of a modeling project to have an effect."
"recall that in the nmf_merged method the matrices a and b pertaining to both data modalities are first combined and nmf is subsequently applied to the combined matrix after normalization. the performance of the two methods depends on their respective emphases on forming the bi-nmf clusters from various modalities versus combining different features of the data modalities prior to the formation of clusters. the quality of the clusters obtained by bi-nmf was superior compared to that of combined ensemble clustering and nmf_merged for both datasets in terms of all reported measures. on radiology reports, compared to combined ensemble clustering, bi-nmf achieved a relative improvement of the order of 33%, 18% and 60% in terms of micro averaged precision, purity and nmi, respectively. similarly, it outperformed nmf_merged and yield a better clustering solution with a difference of 32%, 13% and 38% in terms of micro averaged precision, purity and nmi, respectively. bi-nmf also outperformed combined ensemble clustering and nmf_merged for pubmed images as shown in table 2 . the micro averaged precision reported for bi-nmf was .551 compared to .483 for combined ensemble clustering. likewise, purity and nmi showed a relative improvement of 3% and 5%, respectively. superior performance is also observed for the proposed method compared to nmf_merged, it yield a relative improvement of 50%, 21% and 68% in terms of micro averaged precision, purity and nmi, respectively."
"the lidar data were acquired on 22 [cit] and had already been co-registered with hyperspectral imagery. the spatial resolution of the lidar-derived dsm (digital surface model) was also 2.5 m. figure 1 shows a true-color composite representation of the hyperspectral imagery and the corresponding lidar-derived dsm. all the training and testing samples are from the data fusion contest. the spatial distribution of training and testing samples are depicted in figure 1c and d, respectively. there are 15 classes of interest in this study: grass-healthy, grass-stressed, grass-synthetic, tree, soil, water, residential, commercial, road, highway, railway, parking lot 1, parking lot 2, tennis court, and running track. it should be noted that parking lot 1 includes parking garages at both the ground level and elevated areas, while parking lot 2 corresponds to parked vehicles."
"when compared with xu's state-of-the-art model, the proposed method in this study improved oa from 87.98% to 91.87%, with an increase of 3.89%. however, when using feature-stacking, the modified two-branch cnn in this study only achieved a slight accuracy increase of 0.27% compared to xu's model. this indicates that, when compared with the modification of the network structure, the introduction of the feature-fusion module contributed more to the increase of classification accuracy. this is because the feature-fusion module could learn the importance of each feature, which can emphasize more effective features while suppressing the less informative ones, leading to a more reasonable and robust fusion strategy for multisource remote-sensing data. the backbone of the modified two-branch cnn is less effective than the feature-fusion strategy and does not show superior performance to that of xu's model."
"the architecture of the proposed modified two-branch neural network is depicted in figure 2, which consists of the hyperspectral branch for the spatial-spectral feature extraction and the lidar branch for height-relevant feature extraction. the feature-fusion module was utilized to adaptively fuse the features from each branch, and the class label was determined after the fully connected (fc) layer and the softmax classifier."
"these issues led us to develop model ode converter for creating automated sbml interoperability (moccasin), a standalone tool that can take ode models written in matlab and export them as sbml files. moccasin is written in python and does not require access to matlab. to develop it, we drew on recent advances in the inference of biochemical reaction networks [cit] . the result allows for richer sbml that can also be used for qualitative analyses where knowledge of the reaction network behind a system of odes is required."
"results figure 1a illustrates the previously developed 21, 24 strain sharing techniques used to create self-sustained strain in si nms. first, 80 nm of undoped si 0.795 ge 0.205 is epitaxially grown on silicon-on-insulator (soi) with a 48 nm si template layer. a nearly symmetrical trilayer structure (si/sige/si) is formed by growing 46 nm of undoped si on top of the sige. because of the lattice mismatch between si and sige, the sige layer in the trilayer structure is compressively strained to the si in-plane lattice constant (mismatch strain, e m 5 20.77% for si 0.795 ge 0.205 ). the trilayer nm is released by selective removal of the sio 2 (box) layer of the soi. during the release, strain sharing occurs between the sige and si layers; some of the compressive strain in the sige layer transfers as tensile strain to the outer si layers. it is important to have a balanced trilayer structure (top and bottom layers approximately equal thickness) to prevent the heterostructure from curling during release from the handling substrate 26 . of more importance, the tensile strain in si layers is self-sustained by a balance of forces between the si and sige layers in the freestanding trilayer nm 21, 24 . the amount of strain transferred to the si layers is determined by the si/sige thickness ratio and ge composition of the alloy layer, which controls the mismatch strain (e m ) in the trilayer 24 . the thickness ratio determines the fractional amount of the mismatch strain in the sige alloy that is transferred to the si layers. the magnitude of strain sharing between the layers increases with increasing sige thickness and decreasing si thickness. the thickness of the sige, however, must be kept below the kinetic critical thickness for dislocation formation (figure 1b) 27 because in the as-grown state, the sige layer accommodates all the strain in the trilayer. note that the kinetic critical thickness is different from (i.e., larger than) the thermodynamic critical thickness, because of kinetic barriers to dislocation formation. as the growth temperature is lowered the kinetic critical thickness increases. thinning the si in soi mentioned above allows maximum strain sharing. figure 1b shows the expected strain transfer to the si layers as a function of ge composition in the alloy layer and total si layer thickness (t si 5 thickness of top 1 bottom layers), assuming the alloy layer thickness is equal to the kinetic critical thickness for growth at 550uc. from knowledge of the layer thicknesses and ge composition of the initial heterostructure, we expect,0.35% biaxial tensile strain in the si layers after strain sharing. mobility enhancement is expected from the tensilely strained si 28 ."
"for rf, the gini coefficient was used as the index for feature selection. for svm, the radial basis function (rbf) was used as the kernel function. as for the determination of the hyperparameters of rf and svm, we utilized the grid-search method to find the optimal values. specifically, the ranges of used parameters for rf are as follows. the number of trees ranged from 50 to 500 with a step of 10, while the max depth had a range of 5 to 15 with a step of 2. for svm, gamma ranged from 0.001 to 0.1 with a step of 0.001, while punishment coefficient c had a range of 10 to 200 with a step of 10. after the procedure of grid search, rf achieved the best overall accuracy of 83.97% when the number of trees was 200 and the max depth was 13. meanwhile, svm achieved the best accuracy of 84.16% with a gamma of 0.01 and a c of 100."
"the rest of the paper is organized as follows. section 2 introduces the study area and dataset. section 3 presents the detailed architecture of the modified two-branch network. section 4 shows the experimental results and discussion, and section 5 provides the main concluding remarks."
"encoding a model's ode equations in a one-to-one fashion using sbml's 'rate equations' is sufficient to ensure simulation reproducibility, but the translated model is not ideal if the original system of odes actually represents a biochemical reaction network. reconstructing this network captures the underlying model more productively and enables subsequent application of analyses that require biochemical reactions [cit] . to export sbml models with fully resolved reaction networks, moccasin sends the converter's output via web services to biocham, a modeling environment that incorporates a state-of-the-art algorithm for reconstructing and inferring the complete reaction model from a given set of odes [cit] . due limitations in the xpp format and the biocham service, the result lacks some components present in the original model. moccasin therefore post-processes the output from biocham to add initial assignments, references to the time variable (if used in the original model) and other pieces. all components of the initial matlab ode model are thus captured, and each reaction in the sbml output is fully characterized with well-identified reactants, products and modifiers."
"strain calculation of si in si/sige/si. the ge composition will determine the mismatch strain, and the thickness ratio controls the fractional amount of that mismatch strain that is transferred to the si layers. the strain transferred to the si layers is:"
"it consists of radiology reports collected from clinical records of patients for research purposes. the radiology reports were annotated by domain experts and classified into four categories. the categories and the counts of their content reports are: 35 abdominal mri reports, 486 abdominal ct reports, 248 abdominal ultrasound reports and 500 non-abdominal radiology reports. for simplicity, we will call these mri, ct, ultrasound, and non-abdominal, respectively. the reports are represented using two data modalities: textual features bow and bag of concepts (boc). in the bow modality, the reports are represented using the original words that appear in the clinical narratives and weighted using their tfidf score. in the boc modality, the vectors are indexed by semantic concepts derived from ctakes [cit], a natural language processing tool that maps text to concepts from the umls ontology."
"we compare the clustering solutions produced by bi-nmf which draws information from different data modalities with the output clusters obtained using single data modality in order to demonstrate the benefit of leveraging multiple representations of the data. we show the performance of regular nmf on single modalities, along with comparable approaches such as k-means and ensemble clustering [cit] . in ensemble clustering, a number of clustering solutions are aggregated in a co-association matrix that measures the number of times each pair of data points are placed into the same cluster. kmeans is applied to the co-association matrix to get the final clusters. table 1 shows a comparison between the performances of several clustering methods on single data modalities: k-means clusters of each data modality, the cluster ensembles of each data modality and nmf applied to each individual data modality. for the sake of clarity, the method descriptor has two parts: the applied method and the data modality used. for radiology reports, we observed that the ensemble clustering method applied to one data modality performed poorly when compared to nmf of single data modality, while outperforming single-modality k-means. with the exceptions discussed below, bi-nmf clusters were significantly better than single modality nmf, single modality ensemble clusters and k-means clusters as shown in table 1 vs table 2 . it is important to mention that for the pubmed images data, the clusters of k-means for the captions modality yield comparative clusters to bi-nmf based on purity as shown in table 1 . nevertheless, nmi and micro averaged precision measures suggest that bi-nmf clusters are better than kmeans clusters. to further assure this result, we computed the average of 100 bi-nmf runs and got consistent results. this result strongly emphasizes the benefit of our method that draws information from two data modalities."
"urban land-use mapping is of great significance for various urban applications, such as urban planning and designing, urban-environment monitoring, and urban-land surveys [cit] . traditional methods for urban land-use mapping are based on the visual interpretation of high-resolution optical remote-sensing imagery and field surveys, which can be quite time-consuming and laborious. therefore, it is very important to investigate the automatic classification methods for fragmented and complex urban land-use types."
"to tackle these problems, this paper modified the original two-branch neural network [cit] to adaptively fuse hyperspectral and lidar data for urban land-use classification. the proposed model mainly consists of three parts, i.e., the hyperspectral-imagery (hsi) branch for spatial-spectral feature extraction, the lidar branch for height-relevant feature extraction, and a fusion module for the adaptive feature fusion of the two branches. specifically, the hsi branch and lidar branch share the same network structure, which is based on the cascade of a new multiscale residual block in order to reduce the burden of network design. during the training procedure, each branch is first separately trained, and the whole network is then fine-tuned based on each trained branch."
"in order to evaluate the performance of the proposed two-branch neural network for urban landuse mapping, a series of classification maps are depicted in figure 7 including the following cases:"
