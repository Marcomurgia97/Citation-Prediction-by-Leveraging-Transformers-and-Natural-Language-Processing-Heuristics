text
"in this section, we assess the performance of the mxl algorithm in practical wireless networks by means of numerical simulations. specifically, driven by current design requirements for 5g mobile networks that target a dramatic decrease in energy-per-bit consumption [cit] through the use of multiantenna transceivers [cit], we focus here on the problem of energy efficiency (ee) maximization in mu-mimo networks."
"in order to maintain the status of the link, each node records the time and received signal strength indicator (rssi) whenever it receives valid frame from its neighbor. this recorded information is stored in rssi table which contains the status of the previous communication over that link and also the node can view its neighbor status using the stored information."
each communication node in the applications of mobile sensors will be in motion and mobility ranges alter based on time duration. most of the existing protocols do not build new connections in a faster manner which results in the network performance degradation. thus it is necessary to maintain the quality of service (qos) in order to save energy. [cit] the mobile nature of the sensor networks creates the following issues 1) consideration of space and time during data collection. 2) user and event mobility are to be considered during data processing. 3) location of sensor need to be measured during the on-demand configuration. 4) necessity of the effectual and adaptable positioning system 5) sensor network needs multi-modal and multiquerying capability.
"the emergence of massively large heterogeneous networks operating in random, dynamic environments is putting existing system design methodologies under enormous strain and has intensified the need for distributed resource management protocols that remain robust in the presence of randomness and uncertainty. to mention but an example, fifth generation (5g) mobile systems -the wireless backbone of the emerging internet of things (iot) paradigm [cit] -envision millions of connected devices interacting in randomly-varying environments, typically with very stringent quality of service (qos) targets that must be met in a reliable, distributed manner [cit] . as such, the fusion of game theory, learning and stochastic optimization has been identified as one of the most promising theoretical frameworks for the design of efficient resource allocation policies in large, networked systems [cit] . in view of the above, this paper aims to provide a distributed learning algorithm for a broad class of concave games and distributed optimization problems that arise in signal processing and wireless communication networks. specifically, the proposed learning scheme has been designed with the following operating properties in mind: (i) distributedness: player updates should be based only on local information and measurements; (ii) robustness: feedback and measurements may be subject to random errors and noise; (iii) statelessness: players should not be required to observe the state (or interaction structure) of the system; and (iv) flexibility: players can employ the algorithm in both static and stochastic environments."
"next, supplementary figure s9 shows the results for the sample dataset generated by the garvan institute from the coriell cell repository na12878 reference cell line. these data were sequenced on a single lane of an illumina hiseq x machine. the coverage of this dataset is 49â."
"regarding the computational performance of each of the methods, calq uses the least memory of all of them while maintaining a reasonable processing time, operating on average at 0.5 mb/s during encoding and 3.3 mb/s during decoding, respectively. the calq encoder yields an average peak memory usage of approximately 90 mb and the calq decoder even uses only about 20 mb of ram. qvz 2 and crumble (in all their modes) exhibit average peak memory usages of approximately 3.7 gb and 54 mb, respectively. p-and r-block use around 700 mb of ram. however, due to its memoryexpensive algorithm, quartz (run in its low-memory mode) yields an average peak memory usage of 26 gb which hampers its application on most personal computers and embedded devices. we refer to section 3 of the supplementary material for a deeper analysis of the performances of the used compressors, including calq."
"in this work, we propose a novel lossy compressor, calq (coverage-adaptive lossy quality value compression), for the compression of the quality values in sam files. specifically, calq exploits the alignment information of the reads included in the sam file to measure the uncertainty of the genotype at each locus of the genome. hence, the raw reads must be aligned prior to compression. since most of the generated raw sequencing data is subsequently aligned, we do not believe that this requirement is significant. the proposed algorithm further uses the alignment information to determine the acceptable level of distortion for the quality values such that subsequent downstream analyses such as variant calling are presumably not affected. finally, the quality values are quantized accordingly. to the knowledge of the authors, calq is the first compressor of its type, which exploits alignment information to minimize (or even eliminate) the effect that lossy compression has in downstream applications. thus, high compression is achieved with a negligible impact on downstream analyses."
"the outputs of all pipelines are analyzed using the hap.py benchmarking tool as proposed by illumina and adopted by the global alliance for genomics and health (ga4gh, https://github.com/ ga4gh/benchmarking-tools)."
"algorithm 1 shows the set of instructions performed by the proposed algorithm. in words, for every locus l, the encoder gathers the sequences of reads and quality values covering that locus, the read mapping positions, the cigar strings and optionally the reference sequence(s), as defined in the sam file format specification [cit] . for each read in the sam file, its associated cigar string is the set of instructions that need to be performed to align the read to the reference, excluding substitutions. then, calq computes the genotype uncertainty at that locus l which is then used to select a specific quantizer with k levels from a previously computed set of quantizers. the chosen quantizer is used to quantize all quality values associated with locus l. finally, entropy encoders are used to compress the quantized quality values and the indexes k. supplementary figure s2 shows the block diagram of the calq codec, together with the necessary side information needed for compression and decompression."
"with the release of the latest next-generation sequencing (ngs) machines the cost of human whole genome sequencing (wgs) has dropped to merely us $1000. this milestone in sequencing cost has opened the doors to personalized medicine, where the dna of the patient will be sequenced and analyzed as part of a standard procedure. in this scenario, massive amounts of ngs data are expected to be generated. furthermore, in the course of the next decade the amount of genomic data is expected to surpass astronomical data in volume [cit] ."
"although [cit] an argument was made to perform the lossy compression at the beginning of the pipeline, we believe that placing it just before the variant calling is a more realistic position since it is the point where all processing of the sam file has been done. we believe that if one would need to store a sam file, it would be the one at this point, after all processing, and not the ones before, which have been subject to less processing."
"this idea is depicted in the example of figure 1, which shows at the top the alignment of four reads with their respective quality values. quality values represented by special characters or digits correspond to lower qualities, and quality values represented by capital letters correspond to higher qualities. for simplicity, let us assume that these reads were sequenced from a haploid organism. below the reads, the figure shows with colors the 'genotype uncertainty' for each genomic locus, where green and red indicate a low uncertainty and a high uncertainty, respectively. after the genotype uncertainty for each locus has been computed, the algorithm uses this metric to estimate the manageable level of quantization for the quality values at that locus, i.e. the index k. thus, each of the numbers in the colored row represents the number of quantization levels k that will be used at that genomic locus. a large number is associated with a high 'genotype uncertainty' and vice versa. the bottom of the figure shows the quantized quality values after a quantization with k levels has been applied to the original quality values. note that the number of different quantized quality values in each locus are up to k."
"however, for a few loci deep narrow valleys are observed. these represent those loci where the genotype uncertainty is high (low values in the figure) . hence, the quality values will play an important role in the computation of the genotype at these loci. in these cases, by applying finer quantization, the quality values will remain very close to the original ones."
"the proof is relatively involved so, due to space limitations, we only sketch here the main steps thereof; for a detailed treatment, see [cit] . the first step is to consider a deterministic, \"mean field\" approximation of (mxl) in continuous time, namelẏ"
"after each run of the pipeline, a set of variants is obtained which are stored in the well-known vcf file format. note that when using the gatk þ vqsr pipeline, in the last step, a filter level must be specified such that all variants below that filter value are discarded. [cit], we have chosen to use four different levels of filtering, namely (and ordered from more to less restricting) 90, 99, 99.9 and 100. for more information about the meaning of this filter values and the specific filters of the other pipelines we refer the reader to the supplementary material."
"note that bam files can be up to twice the size of the compressed raw data as they include both the raw data (from the fastq file) and all the information generated by the aligner. moreover, since all the information contained in the raw data is also contained in the bam file, these files have become the baseline for performing further analysis on the sequencing data. one example of this trend is the new genomic data commons data portal by the national institute of health (nih) where the baseline data are stored as bam files."
"nevertheless, none of the above solutions focuses primarily on the compression of the quality values. it has been shown that quality values can take up to 80% of the lossless compressed size [cit] . to further reduce the file sizes, illumina proposed a binning method to reduce the number of different quality values from 42 to 8. with this proposal, illumina opened the doors for allowing lossy compression of the quality values. the drawback is that downstream analyses could be affected by the loss incurred with this type of compression. [cit] showed that quality values compressed with more advanced methods could achieve not only a better performance in downstream analyses than illumina-binned quality values, but even better performance than the original quality values in some cases because these methods remove noise from the data."
"the diversity filtering strategy is likely more useful when investigators choose to ensemble results from a larger number of individual methods, both in terms of clustering performance and computational costs. our extensive evaluations demonstrate that same-clustering provides robust and accurate clusters for scrnaseq data. batch effect can potentially heavily influence clustering results. although same has demonstrated satisfactory performances across real datasets, it is prudent and highly recommended to perform batch effect correction using customized batch effect correction methods (37) before running individual clustering methods and subsequently same. in addition, implementation of our same method provides a general and statistically rigorous framework for ensemble clustering using the mixture model-based method. we therefore expect same-clustering to be a helpful tool not only for single-cell clustering, but for other datasets that benefit from ensemble clustering approaches."
"the technique by which the each node measures the load (l) depending on the capacity utilization is termed as load estimation. since network traffic keeps varying, sensor nodes has a necessity to compute l over appropriate time interval t. in order to perform the computation, the load update interval (t l ) is utilized."
"in this paper an optimal load and mobility aware mac protocol for wsn is proposed. each node measures the rssi from the receiver by monitoring the acknowledgement packets and based on the rss value, the link quality and mobility of the node can be predicted. if link quality is less and rssi is more, the node tends to move which initiates the relay node selection (rns) phase. when the neighbor nodes receive the rns request message from the source, it recovers from sleep state and computes the current load. when more than one rns reply is received, the node with minimum load and best rssi is chosen as relay node. then, source starts to transmit the packets to relay node. by simulation results, it has been shown that the proposed approach minimizes the energy consumption. the advantage of the approach is that it reduces packet drop due to overloading and weak link quality."
"due to the movement that occurs in the medium like water or air or movement that occurs in the mobile hardware, the sensor nodes actually shift from their location. this simultaneous occurrence of node links or failures as well as movement of nodes results in strong mobility. [cit]"
"for the first set of simulations we used the paired-end run err174324 of the na12878 individual. [cit] system as part of their platinum genomes project. the coverage of this dataset is 14â. [cit] we consider chromosomes 11 and 20. in addition to using chromosomes 11 and 20, to broaden our test set, we also considered chromosome 3. figure 4 shows the bits per quality value versus the average recall and precision over the considered variant calling pipeline achieved by the proposed algorithm calq, and by crumble (for two different modes) (https://github.com/jkbonfield/crumble), illumina binning (performed with dsrc) [cit], p-block (for two different modes) (cá [cit] ), quartz [cit], qvz 2 (for five different compression modes) [cit] and r-block (for two different modes) (cá [cit] ) . we refer the reader to the supplementary material for brief descriptions of these tools. the values shown in the figure are the result of averaging over the four vqsr filtering values mentioned above as well as over all three chromosomes. for the individual values for each filter and chromosome we refer to the supplementary material."
"in this paper, we investigated the convergence properties of a distributed matrix exponential learning (mxl) scheme for a general class of concave games with noisy/imperfect feedback. to this end, we introduced a novel stability concept which generalizes rosen's diagonal strict concavity condition [cit] . our theoretical analysis reveals that mxl converges to globally stable states from any initialization, and under feedback imperfections of arbitrary magnitude. in view of this, the proposed mxl algorithm exhibits several desirable properties for large-scale resource allocation problems in networks: it is distributed, robust to feedback imperfections, requires only local information on the state of the system, and can be applied in both static and ergodic environments \"as is\". to validate our analysis in practical scenarios, we applied the proposed method to the problem of energy efficiency maximization in mu-mimo interference network. our numerical results confirm that users quickly reach a stable solution, attaining gains between 100% and 400% in energy efficiency even under very high uncertainty."
"broadly described, the proposed method calq first infers the 'genotype uncertainty' for each genomic locus l from the observable data using a statistical model. given the sequencing depth n at locus l, the immediate observable data are the read-out nucleotides (and their corresponding alignment information) and the associated quality values of all reads overlapping locus l. the genotype uncertainty can be regarded as a metric that measures the likeliness that a unique genotype is the correct one. this metric is then used to determine the level of quantization to be applied to the quality scores at the corresponding locus. the level of quantization is parametrized by an index k representing a quantizer with k quantization levels. specifically, if our method believes that two or more different genotypes are likely to be true, then the genotype uncertainty will be high and hence, k will be high. however, if there is enough evidence in the data that a particular genotype is likely the correct one, then the genotype uncertainty will be low, and therefore, k will be low. thus, the compressibility of the quality values associated to each locus l will be driven by the uncertainty on the genotype at that particular locus."
rssi is inversely proportional to the square of the physical distance between sender and receiver and it is represented as a function of distance and transmission power as per following equation. the speed of a mobile node is calculated using the equation (1) by applying the rssi values.
"in the second experiment, two sets of sources are taken which will send data to the sink through a set of relay nodes. the node speed is varied as 5 to 25m/s for cbr traffic. figure 5 gives the average end-to-end delay when the speed is increased. it shows that the proposed lma-mac protocol has lower delay when compared to 802.11. figure 6 gives the packet delivery ratio when the speed is increased. it shows that the proposed lma-mac protocol achieves good delivery ratio when compared to 802.11. figure 7 gives the energy consumption when the speed is increased. it shows that the proposed lma-mac protocol utilizes lower energy when compared to 802.11."
"ngs machines produce a multitude of readouts-reads in short-of fragments of dna material [cit] . during the sequencing process, a quality value (qv), also known as quality score in the literature, is assigned to each nucleotide in a read. these quality values express the confidence that the corresponding nucleotide has been read out correctly [cit] . the reads, the quality values and the associated read identifiers are commonly stored in the fastq format [cit] ."
"in the initial experiment, one set of source is taken which will send data to the sink, through a set of relay nodes. the node speed is varied as 5 to 25m/s for cbr traffic. figure 2 gives the average end-to-end delay when the speed is increased. it shows that the proposed lma-mac protocol has lower delay when compared to 802.11. figure 3 gives the packet delivery ratio when the speed is increased. it shows that the proposed lma-mac protocol achieves good delivery ratio when compared to 802.11. figure 4 gives the energy consumption when the speed is increased. it shows that the proposed lma-mac protocol utilizes lower energy when compared to 802.11."
"at the decoder, the quality values have to be reconstructed using the quantization indexes i (in correspondence with the correct quantization tables k). the representative quality values are placed at the mid-points of the corresponding bins."
"for the low-compression modes of qvz 2 (i.e. t1, t2 and t4), for p-block, r-block and for crumble (both modes) the results are quite consistent (low variance) and very similar to those of the original data. in these cases, the compression ratios vary from 23.2% (qvz 2 t2) to 35.0% (crumble -1)."
"unlike sc3 and seurat, cidr, t-sne+k-means, and simlr do not perform gene filtering. in order to test the potential impact of gene filtering on clustering performance, we adopted the method used by sc3. specifically, we assessed the impact of different levels (ranging 0-10%) of filtering on individual methods (supplementary table 6 ) and the extent to which gene filtering affects same ensemble clustering (supplementary table 7 ). our results suggest that gene filtering tends to improve clustering and we therefore conducted all our analysis with the 10% filtering criterion. more details on the implementation of these individual methods can be found in the supplementary data."
"in this paper, an optimal load and mobility aware mac protocol for wireless sensor network is proposed. each node measures the received signal strength (rss) from the receiver by monitoring the acknowledgement (ack) packet and based on the rss value, the link quality and mobility of the node can be predicted. if link quality is less and rss is more, the node tends to move. hence it initiates the relay node selection (rns). initially the source node broadcasts the rns request message to all its neighbor nodes at predefined time interval t 1 . upon receiving the request message, the neighbor nodes wake up from sleep state and compute the current load. the neighbor node sends the rns reply message to the source node which includes the computed load. when more than one rns reply is received, the node with minimum load and maximum rss is chosen as relay node. when no rns reply message is received within next predefined time interval t 2, source node enters into sleep state in order to avoid inefficient communication and to save energy. then, source starts to transmit the packets to relay node."
the noises in the network fluctuates the link quality. but the quality of link becomes worse due to the movement of mobile node to certain direction at the constant speed before it changes its direction.
"if link quality is less and rssi is more, the node tends to move. hence it initiates the relay node selection. let th min be the minimum threshold value for link quality and rssi pmax is the maximum value of rssi p."
"each node measures the rssi p (using equation 1) from the receiver by monitoring the ack packets and based on the p(rssi) value, the link quality is predicted. the link quality (l q ) is predicted using the following formulae"
"finally, we further validate previous work that showed that the binning performed by illumina is far from being the best approach for reducing the burden that the quality values have on the size of sam/bam files."
"moreover, after the raw data (i.e. the set of reads stored in fastq files) have been generated, one of the most common subsequent processing steps is the reference-based alignment of the reads [cit] . during the alignment process, additional information is generated for each read such as the mapping positions or the set of operations needed to be performed on a read so that it aligns perfectly at that position. as a result of the alignment, the data of the fastq file are further extended to include all the information generated during the alignment process. these data are usually stored in the form of bam files [cit] ."
"since calq is a compressor where the reconstructed (i.e. decompressed) quality values can be different from the original ones, it is of uttermost importance to assess the effect that these changes in the quality values have on downstream applications. in the scope of this paper, we choose variant calling as it is crucial for clinical decision making and thus widely used."
"this section describes the computations performed by calq to infer the genotype uncertainty (line 5 of algorithm 1). as mentioned above, the genotype uncertainty is computed from the observable data using a statistical model. in this paper, we use a bayesian model similar to the one used by the tool unifiedgenotyper from the genome analysis toolkit (gatk) [cit] . however, in other implementations, the model could be controlled or selected depending on the targeted application or depending on general preferences of the user."
"if there were no constraints for the players' actions, y i (n) would define an admissible sequence of play and, ceteris paribus, player i would tend to increase his payoff along this sequence. however, this simple ascent scheme does not suffice in our constrained framework, so y i (n) is first exponentiated and subsequently normalized in order to meet the feasibility constraints (2). 2 of course, the outcome of the players' gradient tracking process depends crucially on the quality of the gradient feedbackv i (n) that is available to them. to that end, we do not assume that players can observe each other's actions; instead, we only posit that each player has access to a \"black box\" feedback mechanism -an oracle -that returns an estimate of their individual gradient at a given action profile. 3 with this in mind, we will consider the following sources of uncertainty:"
"interestingly, the results for recall, which are shown in figure 7, are quite different in terms of results variance and performance. here, all compressors yield similar variances with quartz being the one with the highest variance, and the high-compression modes of qvz 2 and p-block and r-block being the ones with the lowest variance. regarding the variant calling performance, almost all methods in nearly all cases outperform the original data, with quartz being the one achieving an overall best recall. the proposed method performs very similarly to the other methods while achieving higher compression gains. again, the illumina binning is the worst performing method of all, achieving an overall poor performance."
"from the figure we can observe that calq achieves the best performance in terms of both recall and precision. moreover, calq achieves a considerably higher average recall than that of the lossless case. this means that the variant caller identifies more true positives with calq quality values than with the original ones. note that this is also true for some of the other lossy compressors, although in a minor scale. this seems to indicate that by applying a lossy compressor, more true positives are discovered. [cit] ."
"recent technological advances in single-cell rna sequencing (scrna-seq) have allowed researchers to catalog the transcriptomes across a large number of individual cells, empowering us to systematically study the heterogeneity at the cellular level. scrna-seq has transformed the paradigm of genomic studies by investigating biology down to the single-cell resolution, which unveils information masked from the commonly used bulk rna sequencing (rna-seq). scrna-seq analysis has led to, among others, the identification of existing and novel cell types, characterization of cells, prediction of cell fate, classification of tumor subpopulations, and investigation of cellular heterogeneity (1) (2) (3) . single cell clustering is a crucialstep to achieve above-mentioned utilities (4) . for example, only after clustering the single cells, the following analyses can be meaningfully and conveniently carried out: identification and examination of cell type specific gene expression signatures, adjustment of cell type compositions for differential expression, and deconvolution of bulk rna-seq expression data. due to its importance, it is not surprising to find many existing scrna-seq clustering methods (5) (6) (7) (8) (9) (10) . unforunately, we find that clustering results from different methods are rather dissimilar (supplementary figure 1), which is consistent with literature (11, 12) and not surprising because different methods employ different strategies for essential components of clustering (including choice of distance metric, dimension reduction, clustering approach and estimation of number of clusters) (supplementary table 1 ). each scrna-seq clustering approach has its own strengths and limitations. thus, the use of two or more clustering methods is recommended for more accurate and comprehensive overview of cell clustering. however, when true (\"gold-standard\") cluster labels are not available, it is difficult to select the best method(s), either before or after clustering analysis."
"afterwards, each set of variants (stored in the output vcf file) is compared against the consensus set of variants released by the nist. this comparison is performed using the benchmarking tool hap.py. note that a bed file is also used to restrict the comparison to the high-confidence regions of the consensus set."
the illumina binning is the worst performing method of all. it achieves the worst compression rate while yielding a high variance and overall poor results.
"to partially address this issue, several specialized compression methods have been proposed in the literature. currently, cram [cit], [cit], is the one seeing the broadest acceptance. however, newer methods like cbc [cit] or deez [cit] have shown to achieve better compression ratios than cram [cit] . in addition, other algorithms have been proposed to address or improve further important features like random access [cit] or scalability (cá [cit] ."
"interestingly, same-clustering demonstrates its capability to identify novel clusters. nkg7 and gnly, which are known nk cell marker genes (21), are highly expressed in nk cells (as expected) but not in the new cluster of cells identified by same ( figure 5 ). although most cells in the novel cluster are annotated as nk cells, the low expression of the known marker genes suggests that these cells have transcriptomic profiles deviating from the \"gold-standard\" nk cells. the remaining three clusters correspond to b cells, t cells, and nk cells. we feed cluster labels from same into seurat to find the top ten cell type marker genes that are expressed in at least 70% of cells of the corresponding cell type. figure 6 shows that, when sampling cells that have concordant labelling between the a priori annotation and same clustering, there is an apparent separation of b, t, and nk cells ( figure additionally, our same-clustering discovers limitations in \"gold-standard\" cell type annotations. we will illustrate with one example in the darmanis (18) dataset. we combined top 40 cell-type markers of astrocytes, oligodendrocytes, and neurons from an independent study (35) and intersected them with"
"we use these metrics to assess the performance of calq, as well as the performances of previously proposed algorithms. we start by assessing the performance of the proposed algorithm using the gatk best practices pipeline as proposed by the broad institute, which includes vqsr as the variant filtering step."
"we can observe from the table that the coverage is correlated with the compression ratio. this is to be expected since generally a higher sequencing depth at a given locus yields a higher genotype certainty for that locus. calq exploits this effect by quantizing the quality values at these loci using quantizers with very few levels. furthermore, the outputs of these quantizers are highly compressible yielding superior compression ratios. for example, the best compression ratio from table 1 is 0.15 bits per quality value for the dataset err174324. this result suggests that this particular dataset contains few errors since high genotyping certainty values are computed by calq for almost all loci."
"the datasets used for this analysis pertain to the same individual, namely na12878. the reason behind this choice is that the national institute of standards and technology (nist) has released a consensus set of variants for this individual [cit] . with the publication of this consensus set we have the means for analyzing, in a more concise manner, the effect that lossy compression of quality values has on variant calling. note that similar analyses were conducted in other works [cit] . figure 3 shows the scheme of the used pipelines for the analysis. the box shown in the middle of the figure is where each of the analyzed lossy compressors will transform the quality values. to generate the metrics that will be used as baseline (i.e. the lossless results) the same pipeline is used but with the green box removed."
"to comprehend the volume of data that is represented, stored and transmitted in bam files, current sequencing machines are capable of delivering over 18 000 whole human genomes a year, which accounts for almost five pb of new data per year. therefore, efficient storage and transmission of these prohibitively large files is becoming of uttermost importance for the advancement towards routine sequencing."
"however, these characteristics of individual cluster solutions are unknown a priori. therefore, it is hard to gauge whether to include additional sets of cluster results. our results (supplementary figure 10) suggest that diversity filtering of individual methods before ensemble leads to slightly improved ensemble clustering."
"the difference ðp ml à p 2 þ can be interpreted as the genotype certainty (and hence, one minus the genotype certainty yields the genotype uncertainty). note that any other metric, such as the entropy or the kullback-leibler divergence, could also be used. specifically, we chose the metric m as defined in equation 6 over the entropy and the kullback-leibler divergence because it yields more meaningful results in the case that the genotype likelihood pðnjg; qþ consists of few approximately equally likely genotypes. finally, by using the likelihood (without the prior) to compute the genotype uncertainty levels, the proposed scheme is able to operate without a reference. however, the posterior could be used instead of the likelihood if a reference is available."
the two-hop neighborhood information at each node is contradictory for longer duration which influences the accuracy of the protocol in schedule based mac protocols. the above problems can be handled by the dynamic frame time which is inversely proportional to the mobility range. [cit]
"all the previously proposed lossy compressors for quality values are primarily focused on the fastq file format; and even if those compressors could be easily applied to sam files (sam files are the uncompressed version of bam files), they do not exploit the extra alignment information stored in these files. to the knowledge of the authors, no specialized compressors for the quality values in sam files have been proposed so far in the literature. therefore, it is of primary importance to propose specialized compressors for aligned data, given the size of these files and its impact on the future of personalized medicine."
"we propose and implement same-clustering, a mixture model based probabilistic framework, that performs ensemble clustering for scrna-seq data. results across 15 real datasets show that same-clustering tables 4 and 5 ). our method is flexible and can easily accommodate additional sets of clustering solutions, as new clustering methods continue to be proposed (36) . supplementary figure 10 shows that adding one more set of cluster results may improve ensemble results. we hypothesize that quality and added diversity of the additional contributing solution(s) influence whether the ultimate ensemble solution improves."
"regarding the proposed algorithm calq, the recall performance is comparable to the best modes of the different compressors. however, in terms of precision, calq yields a significant improvement over the rest of the algorithms, including the original dataset. finally, note that the illumina binning as implemented by dsrc performs very poorly in both recall and precision."
"the wireless sensor networks (wsns) include the enormous count of sensor nodes which is deployed in a compact manner. the sensor nodes are small sized and they function in restricted processing speed with reduced power and minimum bandwidth. the process of monitoring and gathering data or identification of events as well as localization and communication are performed by each sensor. then the node swaps information among neighbors and does some operation to find the way of routing data as per required applications. [cit] the wireless sensor network finds its application in defense and scientific fields, habitat monitoring, climate control and disaster management [cit]"
"finally, in addition to the three human datasets, we tested the compression performance of calq on an e.coli dh10b strain dataset, sequenced with illumina (i.e. short-read paired-end) technology. this dataset was sequenced at a coverage of 422â. furthermore, we tested calq on a fifth dataset, which is a d. melanogaster genome sequenced with pacific biosciences (i.e. long-read) technology at a coverage of 100â. the complete compression results in bits per quality value for all five datasets are shown in table 1 ."
let rns req and rns rep represent relay node selection request and reply messages respectively. rns rep includes the estimated load value along with the status of the link.
"the quantization indexes i outputted by the quantizers are encoded with seven order-0 arithmetic encoders [cit], selected by the corresponding quantizer index k. in other words, the seven arithmetic encoders model the seven conditional probability distributions pðijkþ. the quantizer indexes k are encoded with another order-0 arithmetic encoder, which models the probability distribution p(k). all arithmetic encoders work nonadaptively, i.e. they accumulate the individual symbol probabilities of a block of data before performing the actual compression. the outputs (including the accumulated probabilities) of all entropy coders are then multiplexed and written to the output file. note that to reconstruct the quality values at the decoder, the mapping positions, the cigar strings and the reference sequence(s) are required by the decoder. these can be provided by an existing sam/bam compressor [for example cram [cit] or tsc [cit] ]."
"next, we show the results for the srr1238539 run on the na12878 individual for which an ion torrent sequencing machine was used. the coverage of this dataset is 10â. as before, chromosomes 3, 11 and 20 were considered. again, the results shown are the results of averaging over the same four filter values and both chromosomes. figure 5 shows the results for the ion torrent dataset. as before, we run the analysis using 14 different compressors (including the different modes for some lossy compressors) and computed the recall and precision."
"furthermore, we show that calq performs as good as or better than the state-of-the-art lossy compressors in terms of recall and precision for most of the analyzed datasets."
"obviously, if x * satisfies (vs), it is the unique nash equilibrium of the game; also, (vs) is implied by (dsc) but the converse is not true [cit] . in fact, as we show in the next section, the variational stability condition (vs) plays a key role not only in characterizing the structure of the game's nash set, but also for determining the convergence properties of the proposed learning scheme."
"availability same-clustering, including source codes and tutorial, is available at https://yunliweb.its.unc.edu/same/ and https://github.com/yycunc/sameclustering. similarity between estimated and \"gold-standard\" cluster labels is measured through adjusted rand index (ari), for 15 benchmark datasets."
"one cannot analytically solve the maximum likelihood function in . 3, when all the parameters ( 's and 's) are unknown. fortunately, however, we can optimize . 1 via the em algorithm. specifically, we introduce hidden data z, the distribution of which should be consistent with the observed values y:"
"the proposed algorithm calq achieves a variance similar to that of quartz, but in almost all instances the performance of calqcompressed data outperforms that of the original data, being the only method to boost performance. in terms of compression ratio, calq achieves an average of about one order of magnitude (10.6%) reduction in size with respect to bam compressed data. furthermore, its performance is considerably better than that of the two other methods (qvz 2 t8 and t16) that achieve better compression ratios."
"the statistical hypotheses (h1) and (h2) above are fairly mild and allow for a broad range of estimation scenarios. in more detail, the zero-mean hypothesis (h1) is a minimal requirement for feedback-driven systems, simply positing that there is no systematic bias in the players' information. likewise, (h2) is a bare-bones assumption for the variance of the players' feedback, and it is satisfied by most common error processes -such as uniform, gaussian, log-normal, and all subexponential distributions."
"to simplify the problem, we assume conditional independence among individual clustering methods with each method weighted equally, so the conditional probability of / can be represented as the following:"
the high network dynamics are contained in the sensor networks. there may be possibility that hardware malfunctioning and energy consumption cause node failure and further the new nodes get attached to the network. this will affect the network topology and this changes results in weak mobility.
"as seen in the figure, for the case of recall, quartz is the bestperforming algorithm. however, it performs among the worst in terms of precision. this is probably due to the fact that, on average, more calls are made on quartz compressed data than on the original data. however, these extra calls are also filled with false positives, as shown by the performance drop on precision incurred by the quartz compressed data."
"the proposed load and mobility aware mac (lma-mac) protocol is evaluated through ns2 [cit] simulation. a random network of 100 sensor nodes deployed in an area of 500 x 500m is considered. two sink nodes are assumed to be situated 100 meters away from the above specified area. in the simulation, the channel capacity of mobile hosts is set to the same value: 2 mbps. the simulated traffic is cbr with udp source and sink. the speed of the mobile sensor node is varied from 5m/s to 25m/s. the number of relay nodes is selected as 4 for two different scenarios."
"regarding the precision, calq also achieves the best results, yielding a performance marginally above the lossless case. this improvement with respect to the lossless case is also observed for 7 out of the other 13 compressors. however, all incur in more bits per quality value. in this regard, calq achieves a compressed size of less than 0.2 bits per quality value which is an order of magnitude less than the state-of-the-art lossless compressors."
"we analyze the performance of several lossy compressors for quality values in terms of trade-off between the achieved compressed size (in bits per quality value) and the precision and recall achieved after running three different variant calling pipelines over sequencing data of the well-known individual na12878. by compressing and reconstructing quality values with calq, we observe a better average variant calling performance than with the original data. at the same time, with respect to the state-of-the-art lossless compressors, calq achieves a size reduction of about one order of magnitude, yielding only 0.37 bits per quality value on average. this is approximately a 10-fold improvement of the compression factor with respect to bam. the better variant calling performance is not a surprising fact as it has been previously validated for other lossy compressors in the past. however, previously proposed lossy compressors fail to achieve an order of magnitude improvement in compression."
"we benchmarked our same-clustering method and the five individual methods on 15 published datasets that represent a wide variety of sequencing technologies, tissue of origins, data units, numbers of single cells and numbers of cell types (supplementary table 2 ). figure 2 summarizes clustering results, gauged by ari. among the 15 attempted datasets, same-clustering produces the best results in eight datasets (darmanis, baron_human1, baron_human3, baron_human4, baron_mouse1, goolam, zeisel, and the challenging case), and the second best in four datasets (biase, baron_human2, li, yan). additionally, same-clustering outperforms at least three individual methods in all 15 datasets. to further support consistency of same in producing reliable results we rank each method from 1 st to 6 th for all datasets, where ties are replaced by their mean rank. figure 3 clearly shows that same-clustering outperforms all other methods rank-wise. the worst rank of 5.5 came from the biase dataset, where only one cell was misplaced by same, leading to a high ari but didn't perform well rank-wise because there were 3 methods that achieved perfect clustering when compared to the \"gold-standard\". we also compare our results to our previously published safe method (12), which performed overall second best and remains an attractive alternative (33), particularly when analyzing large datasets to save computation time."
"finally, due to the heuristic nature of the variant calling pipelines, it is hard to separate the effect that the different confounding factors have on the final results. this hampers considerably the ability for a deeper analysis on which particular conditions yield an improvement on variant calling. [cit], we believe that the proposed lossy compressor produces partially denoised quality values that, under most conditions, improve the accuracy of the variant calling pipeline."
"in this paper, we propose the first lossy compressor for quality values that exploits the alignment information contained in the sam/ bam files. specifically, calq computes a genotype certainty level per genomic locus to determine the acceptable coarseness of quality value quantization for all the quality values associated to that locus."
"by analyzing previous papers [cit] related to mobility aware mac protocol, it was found that the relay nodes are selected randomly. but there may be a situation that randomly selected relay node may be overloaded or it can be moving, leading to data loss and repeated retransmissions. so it is necessary to develop an efficient technique for relay node selection."
"finally, given the genotype likelihood pðnjg; qþ, the genotype uncertainty is calculated by applying the metric mðpðnjg; qþþ over the genotype likelihood. specifically, we choose m to be one minus the difference between the maximum likelihood p ml and the second largest likelihood p 2, i.e."
"to address the challenging issue of selecting the optimal method(s) when true cell types are unknown, combining information from multiple individual methods becomes an appealing alternative. we present single-cell aggregated clustering via mixture model ensemble clustering (same-clustering), a well-grounded statistical model to solve the problem of consensus clustering. we use a cluster ensemble method because it is known to provide robust and improved quality solutions (13) . moreover, the multinomial mixture model cluster ensemble approach underlying same-clustering accommodates varying numbers of clusters from individual solutions, addresses the issue regarding correspondence of cluster labels across different solutions, and solves the issue of missing labels from some solution(s) (13, 14) . furthermore, mixture model is a maximum likelihood-based approach where we can conveniently leverage model selection criterion to determine the optimal number of clusters for the final ensemble solution."
"existing literature has pointed out the importance of diversity in partitions from individual methods to enhance the performance of ensemble solution (29) (30) (31) (32) . to assess the diversity of the five individual clustering methods, we calculate pairwise ari's to quantify similarity between any two individual clustering solutions. note that a low pairwise similarity represents a higher diversity. after attaining all the pairwise similarities, which can be represented as a heatmap, we calculate the variance for the vector of similarities with each method, including the similarity of value 1 for the method to itself. due to the inclusion of this value 1 (self-similarity), this method-specific variance-based statistic tends to be larger for methods that are dissimilar to others. in comparison, the method with the lowest variance is most similar to other methods, with evidence aggregated from all pairwise comparisons. we, therefore, removed the method with the lowest variance, since the method would contribute the least in terms of diversity. we observed that this diversity-filtering approach results in improved performance in six datasets (darmanis, deng, li, baron_human1, baron_human2, and baron_human 3), same performance in two datasets (ting and yan), and impaired performance in five datasets (supplementary figure 10) . the average increase in ari for the six datasets (0.048) is slightly higher than the average decrease in the five datasets (0.038). taking the reduced computational costs also into consideration, we proceeded with removing the method that contributes the least diversity."
"since the p3p algorithm randomly selects three matched features from an image, all possible combinations must be calculated (i.e., c n 3 combinations). as a result, the computation depends on the number of feature combinations. hence, the more features are matched, the longer the program takes. fortunately, the derivation of each camera model is independent of each other. considering the advantage of using gpus volume 7, 2019 with a large number of threads, we can have the p3p and ransac modules implemented in parallel structures."
"should be the same at this point. however, due to cumulative errors and noisy sensory observations, the landmarks seen figure 7. landmarks observed by the previous and current camera poses. volume 7, 2019 by the camera at k are somewhat offset from the landmarks matched at the previous moment. this paper uses such offsets and the previously observed landmarks l k−1 i to rectify x k . note that this method does not consider dynamic objects. e k i is defined as the euclidean error between two matched landmarks, as shown in (3) ."
the number of times a block is required for computing the matching module is therefore determined by the number of threads per block s and the multiplication of the number of features p and the dimensionality of a feature descriptor r.
"the main purpose of a visual odometry (vo) is to assist robots in navigation or autonomous localization. a visual odometry system obtains movement distances of the robot by repeatedly performing ''robot movement and localization'' and ''visual landmark detection and positioning.'' first, the visual odometry system captures an image via a camera. after that, feature detection algorithms are applied to the image to find significant and robust features or landmarks. when the robot moves, the distance is calculated by matching the same visual landmarks. basically, visual odometry can"
"the matching process includes two major parts. the first one is to calculate the differences of descriptors; the second one is to find the minimal difference of matched feature descriptors. since the number of landmarks is fluctuating depending on the environment, the amount of computation time varies. to achieve a highly parallel computation, multiple blocks with multiple threads are introduced, where the number of blocks is designed based on the number of landmarks q."
"to verify the accuracy of pose estimation by the proposed approach, ground truths are used to investigate the localization errors. performances of the gpu-implemented surf, les, and camera pose estimation module are also investigated by comparing with their cpu-implemented counterparts. four sets of experiments are conducted, described as follows."
journal of electrical and computer engineering 3 substitute (4a) and (4b) into (5a) and (5b); the coefficients and can be obtained and they are shown in the following equations:
"the normal components of the magnetic flux densities and the tangential components of the electric field intensities in two media are continuous at the interface. from the boundary conditions [cit], the relationship between 0 and 1 is obtained as the following equations:"
"cross-spectral iris recognition employs the additional information contained in both nir and vis spectrums and wavelengths [cit] . therefore, the identification and authentication become more optimal [cit] . figure 1 depicts the differences between the vis and the nir iris images. the iris patterns are clearly apparent in vis imaging. for light-pigmented irises, the iris textures are highlighted using vis imaging. however, the iris recognition under vis imaging is affected by reflections from the cornea. meanwhile, the iris recognition in nir imaging is more robust compared to vis imaging because of fewer reflections; however, the nir imaging dismisses almost all of the prominent pattern or patterns of the pigment melanin in the iris. in this case, cross-spectral iris matching schemes are considered as some of the most accurate recognition frameworks [cit] . in crossspectral iris matching, the information captured at each nir and vis wavelengths is different. therefore, proposing the same representation of the iris in both nir and vis imaging is a great challenge [cit] ."
"to evaluate the performances of the proposed vo on a heterogeneous computing platform, we conduct the experiment in an indoor environment, where a pioneer 3-at mobile robot is used as shown in fig. 20 . the robot carries a tx2, a zed stereo camera, a router, a udoo single board camera, and batteries, and moves along a 7-meter l-shape path (the yellow line) as shown in fig. 21 (a) and (b) . the zed stereo camera used in the experiment is capable of providing images of a depth ranging from 0.5 to 20 meters at 60 fps. a laptop is used to enable the tx2 via wireless connection in the beginning of the experiments, record the estimation results during the experiment, and display the estimation trajectory after the experiment is completed. the udoo single board computer, which is also enabled by the laptop via the router, is employed for controlling the pioneer 3-at mobile robot. a marker is positioned on the tail of the robot so that as the robot moves, its actual trajectory is drawn on the floor."
"the main difference between mmas and oa is the way the algorithms update the pheromone matrix. in oa, a constant pheromone matrix τ 0 with τ"
"this paper proposes a novel parallel indirect vo system, incorporating the parallelism of surf, les, p3p and ransac. in addition, we also design a map management method and a key-frame selection mechanism to provide stable landmarks as well as to filter out similar images. furthermore, a camera correction model is proposed to reduce cumulative errors, hence improving the estimation accuracy. finally, we implement the proposed indirect vo on a cpu/gpu heterogeneous parallel computing platform. experimental results have shown that, compared to a conventional vo, the proposed indirect vo system is approximately 80 times faster when 80 matched features appear in the environment. chiang-heng chien received the b.s. and m.s. [cit], respectively, where he is currently a postgraduate research assistant with the computational intelligence laboratory and is involved in the fpga design of a visual slam algorithm. he has been studying the problems of robot localization, evolutionary computation, and slam for more than four years. his expertise includes premature convergence problems in robot localization and visual slam algorithms."
"thus far, all studies [cit] assessed cross-spectral iris recognition performance in order to improve matching accuracy. since the development of large-scale national identity program has increased in line with the demand for more accurate and robust biometric system. hence, an accurate cross-spectral iris recognition approach with a lower eer is required."
"1) the number of new features in the previous image is larger than t n of current matched features. in other words, a current image that is quite different from the previous one is treated as a new frame. 2) the euclidean distance between the ray vector of the current matched features and the ray vector of previous features is greater than a threshold t r, and the number of features that meet the threshold t r is greater than the threshold t f . the first rule refers to the scenario in which a camera visits a place with many new scenes. the second rule indicates that the camera is observing landmarks from different angles."
"to avoid excessive errors due to mismatched landmarks or dynamic objects, we adopted the idea of successive least square approximation [cit] to eliminate outliers. the steps to eliminate outliers are described as follows."
"finally, what we are concerned about is the voltage received by the wire antenna on the ground, and it can be obtained by the following formula about :"
"the key-frame selection mechanism is similar to a filter. after capturing and matching the features of the current image, an image that has a high overlap rate with the previous image is filtered out. otherwise, the image is regarded as a key-frame responsible for estimating the camera pose. in the proposed vo system, the current image is treated as a new key frame if any one of the following two rules is satisfied."
"clearly, a significant variation of the parameters used in section 4.1 did not alter considerably the behavior of the algorithm. using a greater value of o, the behavior is almost identical, while a smaller value shows a figure 4 : comparison between the mean behavior of the proposed oa and the mmas using different ranges, allowing a detailed observation of the convergence characteristics slight fall in its performance. this is due to the diminished strength of the search for nearby good solutions. using a larger m, the search was slower at the beginning but resulted in a slightly better solution at the end. a smaller m did the opposite. this can be seen as if the increase of the number of individuals delays the search, but ensures a good final search zone. by increasing the parameter k, a slower initial progress was observed, while decreasing it did the opposite. this can be understood because the more frequent update of the pheromone matrix allows searching for nearby better solutions in advance."
"to solve the current distribution on the antenna surface is the key work of the whole subject. from maxwell's equations, the relationship expression between the electric field e and the vector potential a is shown in"
"as a further study, the influence of the frequency and stratum parameters on current distribution of the antenna and the electric field generated by the antenna is discussed on the basis of the above study."
"unstable landmarks are likely to influence estimation accuracy of the camera pose. also, accumulated landmarks observed from previous key-frames increase computational burden. therefore, unreliable landmarks have to be deleted. a matched landmark is regarded as stable when it appears for more than 7 times in 10 consecutive images, and it will not be deleted from images. on the other hand, a landmark is regarded as lost and will be deleted if it is mismatched for more than 35 times in 50 consecutive images."
"after these encouraging preliminary results, the authors are working on the comparison of oa for tsplib problems to other aco algorithms as p-aco and mmas with pheromone trail smooth and local search. future work may concentrate on a deeper theoretical study of oa and its application to other combinatorial optimization problems in comparison with other world-class metaheuristics. preliminary results with different real world engineering problems confirm the advantages of using oa as will be soon published."
"finally, its conceptual simplicity and the fact that it uses the same foundations as aco allow a deeper study of the reasons of aco's success."
"similar population-based aco algorithms (p-aco) [cit] were designed by guntsch and middendorf for dynamic combinatorial optimization problems. the main difference between the oa and the quality strategy of p-aco is that oa does not allow identical individuals in its population. also, oa updates τ every k iterations, while p-aco updates τ every iteration."
"ant colony optimization (aco) is a metaheuristic inspired by the behavior of ant colonies [cit] . in the last few years, aco has received increased attention by the scientific community as can be seen by the growing number of publications and the different fields of application [cit] . even though there exist several aco variants, what can be considered a standard approach is next presented [cit] ."
"the changes of the electric field magnitude with the radial distance are shown in figure 3 . the curves of figure 3(a) show the variation of the axial component of the electric field magnitude with the radial distance on the ground at 10 hz and 50 hz, respectively. from that we can see the field magnitude at 50 hz is higher than that at 10 hz and both the variation trends are the same with the radial distance. with the increasing radial distance, the magnitude gets smaller and smaller. curves in figure 3 (b) represent the variation of the radial component of the electric field magnitude with the radial distance on the ground at 10 hz and 50 hz, respectively. the trends of the field magnitude with the radial distance both at 10 hz and 50 hz are also the same, but the magnitude at 50 hz is smaller than that at 10 hz, opposite to the axial component of the electric field magnitude. the radial component of the electric field magnitude becomes larger with the increasing distance first and then becomes smaller after a maximum value. compared with the radial component of the electric field magnitude, the axial component of the electric field magnitude is several orders smaller, so it only has a slight effect on the total electric field."
"the iris recognition performance is evaluated using the eer and a receiver operating characteristic (roc) curve. the eer represents the error rate at which the far equals the frr. the best recognition system results in a lower eer. meanwhile, the roc curve represents the trade-off between the gar and the far. a roc curve from a good iris recognition system will be located near the top of the graph (high gar) for most far [cit] ."
"finally, it may be noticed that oa completely forgets old solutions that are not members of the population, while mmas evaporates the pheromone very slowly at the arcs of these old bad solutions, slowing down its convergence."
"in fig. 21 (a) and (b), the stars shown on the path represent nine anchor points where estimation errors are investigated as the robot moves. each anchor point is 1 meter from the neighboring points. note that there are two anchor points, d and d 1, at the turning point because the orientations and the positions of the robot at those two points are not identical. moreover, the turning point provides a scenario for a pure-rotation estimation, which poses a huge challenge to track features on the image in real-time and estimate the camera pose subsequently. to verify the performances of the cpu/gpu heterogeneous implementation of the proposed vo, comparisons are made with its counterpart using only a cpu. note that the cpu used in the experiment refers to the one on the tx2 for fair comparisons. the robot moves at two constant velocities including 6.7 cm/s and 20 cm/s to challenge the capabilities of real-time operations. fig. 22 and fig. 23 respectively present the experimental results at two different velocities. since the cpu does not perform efficiently, it is easy to get lost when new features are observed before the previous camera pose is estimated. as a result, the current pose cannot be reliably estimated. this problem is even worse when the velocity is 20 cm/s. fig. 23 (a) shows the estimation diverges before the experiment is completed using only a cpu. on the other hand, the heterogeneous computing allows the system to perform efficiently so that the camera poses are satisfactorily estimated. note that it is particularly obvious to see the reliability at the turning point because the pure rotation scenario makes the system difficult to track features in real-time due to heavy computational burden. hence, implementing on the cpu/gpu heterogeneous computing platform, the proposed vo is capable of estimating the camera pose effectively and efficiently. table 2 to table 5 show the average translational and rotational estimation errors for the implementations on cpu and cpu/gpu of the tx2 platform. the errors are presented as the form of mean ± standard deviation. as can be seen from the tables, the proposed vo running on the cpu/gpu heterogeneous computing platform at both velocities are superior because features can be extracted and matched in real-time, allowing the camera pose to be correctly and accurately estimated."
"in general, visual sensors have high frame rates that can be up to 20 frames per second. since the visual sensor does not necessarily move quickly every time, each frame contains a large proportion of overlapping scenes. if the camera moves very slowly, the estimations of camera poses can be very similar. therefore, in visual localization, a key-frame selection mechanism is required to avoid having every image captured by the camera as a key-frame."
"as shown in fig. 13, the traditional sequential cpu approach requires three loops for the p3p and ransac to estimate camera poses. because the execution of p3p and ransac is independent in this three-layer loop, the three layers can be flattened such that each camera model can be derived using p3p and ransac algorithms in an individual and independent thread. as shown in fig. 14, multiple gpu threads are issued to perform p3p and ransac."
"to explain the reasons why oa outperforms mmas, one of the best-known aco algorithms [cit], it is useful to remember two main reasons why aco is a good algorithm for a tsp with globally convex structure [2, 8, 9 ]."
"for the following experimental results, a 2 ghz computer with 256 mb of ram was used. the programming language chosen was c and the operating system was linux. first, a comparative study between mmas and oa is presented. then, the behavior of oa as a function of its parameters is studied."
"the matching performance is based on the all matching results from the grf, the bsif, and the dog. the limitation of one method will be covered by another method. thus, a combination of these methods can boost matching results. in addition, we explore the integration of the 1d log-gabor filter with this method. the gabor filter is known to be robust to illumination variation, imaging contrast, and camera gain, by using the phase information rather than amplitude."
"to compare the efficiency of the heterogeneous implementation performing different sizes of data, the value of the hessian threshold of the surf algorithm is adjusted to generate various sets of features in different numbers, i.e., fewer features are detected with lower hessian thresholds. table 9 shows the performance improvements of heterogeneous computing (cpu/gpu) over cpu. when the number of landmarks to be matched exceeds 10,000, the proposed parallel method can be 18 times faster than a cpu implementation, as shown in table 9 . table 10 shows the computational efficiency of modules for estimating camera poses. computation time increases as the number of features n increases. the traditional sequential approach requires more time as the number of features increases. on the other hand, if 200 features are required for estimating the camera pose, the proposed approach can accelerate up to 400 times faster than the conventional sequential approach."
"the matching is a process of comparing the testing iris template against the training iris templates using hamming distance (hd) in order to obtain the similarity score. the similarity score is a single numerical value that represents the degree of similarity between two iris templates [cit] . the hd is a measurement metrics that used to determine the bit similarity between two iris templates. two iris patterns are from different iris patterns if the hd value is above the threshold. otherwise, if the hd value is below the threshold, the two iris patterns are identic. the threshold represents a reference value to determine whether the matching score is genuine or an impostor. the genuine score is a matching score between two iris templates originated from the same user. meanwhile, a matching score between two iris templates originated from the different user is known as an impostor. the threshold can be expressed as follows [cit] : (15) in the above equation,"
"in the proposed vo system, an image of a key-frame is divided into three areas: left, center, and right. in each area, a minimum number of landmarks is required to be triangulated from the image such that landmarks are evenly established in the environments. if matched landmarks exceed t a in any area, then the newly observed landmarks are ignored. on the other hand, additional landmarks that are newly detected will be established. the value of t a is determined by the scale of the experimental environment and the image size."
"in figure 3 we observe the mean in 25 runs of the evolution of the best length of both algorithms for a larger problem, the well-known kroa100 [cit] . in figure 4 the ranges are modified, as in figure 2, to show the advantage of oa over mmas, considering convergence. once more, considering mean behavior in 25 runs, oa outperforms mmas."
"while p3p provides multiple sets of camera poses, the pose estimations are not fully reliable because of the instability of the landmarks. therefore, ransac is widely used for regression analysis. in general, the accuracy of a mathematical regression model is influenced by outliers, which can be filtered out by the ransac algorithm. basically, a ransac algorithm consists of four steps."
"it can be observed from table 1 that the integration of grf decreases the eer about 6.9% for the bsif descriptors and 19.4% for the lbp descriptors. in addition, the descriptor that integrated with grf results in large gar values. among all the comparisons, the combination of grf and bsif resulted in the best cross-spectral recognition performance with 1.69 % eer at 96.92% gar."
"in the proposed heterogeneous architecture, surf, feature matching, and camera pose estimation are implemented using gpus, with surf accounting for the majority of time used. in other words, surf dominates the overall system performance as shown in fig. 24 ."
"earlier works demonstrated that, the 1d log-gabor filters are often integrated with several feature descriptors due to the gabor filter have good theoretical properties. therefore, we explore to concatenate the gabor filter with the grf methods to accomplish a robust cross-spectral iris recognition. furthermore, the phase information of the gabor filter is robust against illumination variations, camera gain, and imaging contrast. hence, we evaluated the concatenation of the 1d log-gabor filter with the aforementioned methods. figure. 9 shows the performance of the concatenation gabor filter with grf approach. by using the gabor filter, the grf results in a higher recognition performance. a gabor filter improves the accuracy of the grf approach by about 10%. the most accurate cross spectral iris recognition is achieved using the combination of gabor grf, gabor bsif, and gabor dog, with an eer of 1.02% at a gar of 98.97% as shown in table 2 ."
"however, the proposed grf failed in images that have non-linear intensity variations caused by specular reflections, such as in the 10 th image. the recognition error was caused by specular reflection which was misrecognized by grf as an edge of iris texture. therefore, the system cannot find the gradient correlation between nir and vis iris images."
"the implementation of the surf algorithm using gpus in the proposed vo system employs opencv. as for the les algorithm, it is time consuming because features and landmarks are matched one at a time. consequently, it is highly dependent on the number of features detected within the current image. if p features have to be compared to q landmarks independently, the computational complexity is o (p q ). however, exactly because the matching process between features and landmarks can be performed independently, it is extremely beneficial to implement les on gpus."
"1. given a population of good enough solutions, better solutions may be found with larger probability closer (considering the distance concept explained in section 2) to good solutions than to bad ones. all aco algorithms, including oa and mmas are based on this known property."
"in this section, the implementation of grf into the crossspectral iris recognition domain is evaluated using the bsif and lbp descriptors. the comparison of grf with another photometric normalization was conducted using multi scale weberface (msw) normalization. in addition, we also investigate the integrated grf with 1d log-gabor filters in order to boost recognition performance."
"already knowing the current distribution, the vector potential a concerned can be calculated by (13) . then the axial and radial components of the electric field can be obtained from the following expressions which are derived from (14):"
"this method is the first in the literature that implements the grf to reduce illumination variation in cross-spectral iris recognition. the contributions of this paper are: 1. a novel feature extraction method for cross-spectral iris matching that is illumination invariant and representative in both nir and vis imaging, based on integrated gradientface-based normalization and the texture descriptors. 2. an improvement of cross-spectral iris recognition performance with integrated features using decision fusion. the organization of the rest of this paper is as follows. section 2 describes the related works. section 3 explains the proposed framework in this study. results and discussion are given in section 4, and the conclusion is summarized in section 5."
"a. the perspective-3-point algorithm fig. 4 shows the method to estimate the position of the perspective center point (cp), denoted as v . a, b, and c represent three known landmarks, while a, b, and c denote the distances between v and a, b, and c, respectively. θ ab, θ ac, θ bc denote the angles between v and a, b, and c, respectively. the lengths of a, b, and c can be obtained from the ray vector of points a, b, and c, while θ ab, θ ac, θ bc can be obtained according to the cosine theorem. after obtaining the distances a, b, and c, the three planes p 1, p 2, and p 3 are constructed by vector projection. as shown in fig. 5, three planes p 1, p 2, and p 3 intersect at point r. the length between v and r can be obtained by the volume 7, 2019 figure 5. planes p 1, p 2, and p 3 intersect at point r."
"the above two rules show that the three thresholds t n, t r, and t f play an important role in the key-frame selection mechanism. t n and t f can be determined by the scale of the experimental environment and the size of the image resolution. t r can be determined according to the speed at which the camera moves in the environment and to the frame rate of the visual sensor used by the vo system. for instance, if a camera is moving at a low speed in a high frame rate, t r can be set slightly higher."
"to provide robust landmarks for estimating the camera pose, an effective map management method is proposed to filter out unstable landmarks. the method is composed of three mechanisms, including landmark updates, landmark deletions and landmark additions."
"step 1. calculate the euclidean distance and their direction of matched landmarks with respect to the current camera pose, and calculate the standard deviation as well as average of all landmark errors."
"2. because the existence of local optimal solutions, it is better to search within a region defined by a whole population of good solutions than only close to the best-known one. even though all aco algorithms use this property in one way or another, oa is the one that best exploits this property in an explicit way, looking for good solutions mainly in the subspace spanned by the best m known solutions, without concentrating its search mainly around the best current solution which usually is only a local optimum."
"the parameters used in the experiments are described as follows. in the map management method, t a is designed as 7 and 60 in ground truth experiments (i) and (ii), respectively. in the key-frame selection mechanism, t n and t r are set as 0.2 and 15, respectively. as for t f, it is designed as 0.3 of the number of matched features, and the inlier ratio γ is 0.7."
"oa was initially inspired by mmas, an aco currently considered among the best performing acos for the tsp [cit] . it is based on the hypothesis that it is convenient to search for nearby good solutions [cit] ."
"a. cross-spectral iris recognition using grf an ideal biometric system results in a lower eer with a higher gar. figure 6 demonstrates the performance of cross-spectral iris matching using grf and without grf. both standalone descriptors, i.e. bsif and lbp, resulted in a higher eer and a lower gar. however, when the grf was integrated with these descriptors, it resulted in better recognition performances. the proposed grf yields small eer and large gar values for both descriptors. the gradient-based normalization works better with local texture descriptors such as lbp and bsif. figure 7 describes the effect of applying grf to the vis image and nir image. grf used to provide the edge present in vis and nir images. grf detect the smaller and liner edges present in the texture of the iris. moreover, the shadow caused by specular reflection can be minimized through grf because grf extracts the illumination insensitive from the gradient domain. combining the grf with the spatial texture of lbp and bsif features result in an edge image containing the more exceptional edge present in the iris texture. consequently, the iris pattern is well recognized. in this work we concern with recognition performance rather than the image itself. therefore, we evaluate the impact of iris recognition performance by applying grf as feature enhancement not as preprocessing."
"in this paper, we proposed a model of a vertical electrically small antenna located underground with elf or slf. to get the expressions of the field generated by underground antenna, quasistatic approximation has been used for overcoming the calculation difficulties caused by sommerfeld integral. numerical results including the current distribution, electric field, and voltage received have been obtained by using the method of moments (mom). further study has been done to get the variation curves of the current and electric field with the frequency or stratum parameters."
"we conclude that the integration of the grf with bsif and lbp descriptors can boost the recognition performance. the best combination is achieved when the grf is integrated with the bsif descriptor, resulting in an eer of 1.69%. figure 8 illustrates the performance of grf compared to another photometric normalization that is illumination invariant, namely msw. the msw, bsif, and dog combination method generates a large far and small gar. this is because the msw is a threshold based photometric normalization. msw computes the salient pattern of an image by calculating the ratio between the relative intensity difference of the current pixel to its neighbour and the intensity of the current pixel. otherwise, the combination of grf, bsif, and dog methods resulted in larger gar and small far. the grf is a gradient-based orientation which considers the relationship of neighbouring pixel points and is therefore, suitable to be combined with texture features such as bsif, which extract the statistical features from nir and vis images."
"however, new landmarks should be unique and are not too concentrated in any specific areas. to avoid over-concentration of landmarks, a threshold is set to restrict the number of landmarks. a new feature is included as a landmark when its euclidean distance to any existing landmark is greater than this threshold. in other words, features that are too similar or too close to existing landmarks are ignored in the proposed system."
"to analyze the overall performance of the heterogeneous architecture and the cpu, various sets of features in different numbers are used for the p3p algorithm. table 11 shows the experimental results. to find camera poses using p3p, we took a number of features, ranging from 20 to 80, out of a total of 80 features. in a heterogeneous computing architecture, more features lead to significant speedups, and it can be up to 80 times faster compared to conventional sequential approaches. based on the proposed method, the heterogeneous computing architecture not only reduces computation time but also improves the accuracy of pose estimation."
"in recent decades, iris recognition has received considerable interest in human identification [cit] . iris recognition is considered as one of the most accurate biometric identification systems [cit] . a human iris is highly distinctive, because each person has different spatial patterns in the iris. therefore, the iris is appropriate for personal identification. previously, iris recognition utilized an iris image captured on the visual light spectrum (vis) at 400-750 nm wavelength or on the near-infrared spectrum (nir) at 750-1400 nm wavelength [cit] . nowadays, iris recognition is explored in the cross-spectral domain, to boost recognition performance [cit] . cross-spectral iris recognition is defined as the ability to match images acquired in different electromagnetic spectrums [cit] ."
"the current distribution of the antenna is calculated by mom; figure 2 shows the current distribution of the wire antenna in the case of the frequency is 10 hz. from figure 2 we can see that the current distribution is approximate triangular distribution, well matching the current distribution of the electrically small antenna."
"in search of new aco analytical tools, a simple algorithm preserving certain characteristics of aco was developed. this is how the omicron aco (oa) was conceived and its name comes from the main parameter used, which is omicron (o). oa was motivated by the ideas behind aco, i.e. the search for nearby good solutions. oa was first designed with theoretical motivations to study convergence properties [cit], but proved to be very useful also in practical applications. therefore, this paper compares oa with respect to one of the best-known aco algorithms, the max -min ant system (mmas) proposed by stützle and hoos [cit] . oa is a more straightforward utilization of the successful reasons of aco. as a consequence, oa outperforms mmas in the preliminary experimental results shown in this work. besides, its simplicity and decreased sensibility to input parameters make it easy to configure. this paper is organized as follows. in section 2 the test problem and the definitions are presented. the standard aco approach, the ideas that motivated oa and its pseudocode are given in section 3. in section 4, a performance comparison between mmas and oa, and a simple study of the oa as a function of its parameters are made. experimental results are explained in section 5. finally, the conclusions and future work are presented in section 6."
"fig . 3 shows the flow chart of a conventional visual odometry system. first, the visual odometry system uses a vision sensor to capture images and subsequently uses a feature extraction algorithm to extract distinct and robust features from images. after that, the features are triangulated as landmarks. when the visual sensor moves to the next position, the distance of the movement is estimated using the positional relationship between the matched landmarks and the visual sensor."
"in expressions (4a) and (4b), and are undetermined coefficients that are determined by the boundary conditions, and 0 is the zero-order bessel function of the first kind. here some variables are given by"
"the computational platform used in the experiment is an asus laptop with an intel i7-6700hq cpu at 2.6 ghz, and the sensor is a pro live rgb-d camera. because the xtion rgb-d camera is only capable of providing depth images within 0.8 ∼ 3.5 meters, the experiment is therefore conducted in an indoor small-scale environment. the laptop is responsible for receiving images captured via the xtion rgbd camera and performs the conventional vo as well as the proposed one. in the experiments, the camera always faces the same scene as shown in fig. 17, so that features can be easily extracted. the camera moves counterclockwise four times around a square with a circumference of four meters, as shown in fig. 18 . table 1 shows the average errors of pose estimations at the four corners during the first and the fourth turns. the root mean square error (rmse) at the four corners of the four turns has been reduced to 4.065 cm using the proposed vo in comparison to 9.45 cm using the conventional vo. moreover, the results show that when the camera moves away from the initial position, the pose estimations of a conventional vo system gradually diverge because of the accumulated errors. on the other hand, the proposed vo avoids such problems because of the map management method and the camera pose correction model. table 1 and fig. 19, it is clear that the proposed vo system outperforms the traditional vo system."
"as can be seen from the above, heterogeneous computing is an advantageous option for accelerating vo computation. however, to the best of our knowledge, there are still no publications that implement indirect vo on a cpu/gpu heterogeneous computing platform. therefore, in this paper, we propose a novel indirect vo system based on a heterogeneous computing architecture for execution in gps-denied environments. since critical components including surf, les, p3p, and ransac are suitable for implementation in parallel, the gpu is therefore in charge of accelerating these algorithms, while the cpu is responsible for the rest of the algorithms to achieve real-time operation. the platform we use is nvidia jason tx2 [cit], which features a gpu with 256 cuda cores and a cpu with 4 arm cores to improve overall system efficiency. the main contributions of this paper are listed as follows:"
"step 2. if the standard deviation is greater than a threshold t e, the euclidean distance of the error farthest from the mean is removed. then, recalculate the standard deviation and the average until it is less than a threshold t s ."
"1. generate an edge map from iris image i by using canny edge detection. 2. compute the iris boundary by biasing the gradient of i in the vertical direction. 3. compute the pupil boundary of i by weighting the vertical and horizontal gradients equally. 4. set the value of the iris and pupil radius range manually. for the polyu database, the pupil radius ranges from 28 to 75 pixels and the iris radius ranges from 90 to 150 pixels. 5. calculate the centre coordinates of i, the centre coordinates of the pupil, the radius of the iris, and the radius of the pupil by performing the hough transform for the iris boundary first, and then for the pupil boundary within the iris region as follows:"
"comprehensive study demonstrates that a standalone descriptor is not sufficient for cross-spectral iris recognition. the modality factors such as different wavelengths, illumination variations, and different sensors, cause the variations in iris appearance. therefore, the iris recognition performance is decreased [cit] . in this study, we integrated the photometric normalization technique with the feature descriptor and the filtering. we explored the grf to remove the illumination factor. the bsif is used to classify the iris texture in the nir and vis images. furthermore, the dog is used as a bandpass filter which can suppress frequency aliasing and noise caused by the difference of nir and vis wavelengths."
"to tackle such a problem, we propose a correction model that linearly modifies the current camera pose and subsequently builds landmarks using that pose. as shown in fig. 7, suppose the camera pose derived at the previous moment is x k−1, and the landmarks created by"
"the proposed framework is illustrates in figure 2 . nir imaging is used for testing and vis imaging is used for training. for the vis image, the experiment was carried out under the red channel, because the red channel provides more textural information of the iris pattern than other channels [cit] ."
"therefore, the main reason oa outperforms mmas is its ability to search explicitly in a whole subspace ω spanned by m good solutions, instead of mainly increasing the amount of pheromone of only one good solution. at the beginning, this property makes oa converge faster to a good region, given that it uses more information of each cycle (up to m pheromone updates, instead of just one). at the end, oa searches for the best solution close to a whole subspace ω, without giving more importance to any of the m individuals of population p . on the contrary, other acos as mmas look for new solutions mainly near the best-known solution, which usually is only a local optimum."
"when the antenna radius and the wavelength meet the condition ≪, the assumption that current is uniformly distributed along the surface is established. then the effect of the current along the antenna surface is equivalent to a line current along -axis. combined with (14), the following expression is obtained:"
"6. perform linear hough transform using radon transform to isolate the eyelids. 7. isolate the eyelashes using a simple thresholding method, since the eyelashes are quite darker than other portions. 8. obtain the segmented iris image (i s )."
"where γ and n are the inlier ratio and the number of matched landmarks, respectively. because landmarks are located relative to the camera pose, a camera pose is estimated offset to the left when landmarks are observed to be offset to the right. therefore, a minus sign should be prefixed to the amount of correction for e, and a corrected camera pose can be obtained accordingly as shown in (5)."
"in the proposed vo system, the processes of surf feature detection, les, p3p and ransac are time-consuming, critically affecting the efficiency of the overall vo system. as such, we propose a parallel architecture to accelerate these algorithms using gpus with power of parallel computing over massive data. fig. 8 shows the flow chart of the proposed vo algorithm based on parallel computing, where functional blocks indicated by red dashed boxes are implemented on gpus, including a ''surf and matching module'' and a ''camera pose estimation module.''"
the cnn-based features are generally sparse and result in compact iris template representations. experimental results demonstrated that the cnn-based method resulted in crossspectral iris recognition with an eer of 5.39%.
"the formulas of green's function are obtained, but the equations can not be solved directly. the difficulties like oscillations will appear in solving this kind of equations because of the sommerfeld integrals [cit] . to avoid the trouble in computation, a quasistatic situation can be considered under the condition of elf or slf. in this case, wait [cit] has shown that 0"
"in this paper, an illumination invariant cross-spectral iris matching framework is proposed using the integration of grf. the best framework is achieved using the combination of a gabor grf fused with bsif descriptors and dog filtering with an eer of 1.02% at gar 98.97%. the experimental results showed that the grf is appropriate to be combined with the local texture descriptor, the bandpass filter, and the gabor filter. therefore, it is reasonable to conclude that the proposed grf can perform very well, when it is integrated with the different descriptors. however, further research is required to conduct a combination of grf with global descriptors, to obtain a more discriminant feature in cross-spectral iris recognition."
"the purpose of the normalization step is to prepare a segmented image for the feature extraction steps [cit] . the normalization transforms the shape of the iris into fixed dimensions [cit] . the successful segmented images result in the doughnut shape iris. therefore, the normalization step is performed by unwrapping the circular region into a rectangular block of constant dimensions, in order to allow comparisons. daugman's rubber sheet normalization technique was employed in this study. figure 4 represents the daugman's rubber sheet normalization. daugman's normalization method converts a segmented iris texture within the iris region from cartesian to polar coordinates (r, θ) using [cit] :"
"to overcome the illumination factor, photometric normalization techniques are applied [cit] . photometric normalization aims to reduce illumination variation in order to increase the performance of iris recognition [cit] . photometric normalization has been found to be useful to improve robustness in face recognition [cit] . photometric normalization was previously used as a pre-processing step in face recognition [cit], and when photometric normalization is combined with suitable descriptors, this combination achieved promising results [cit] . the photometric normalization enhances the features with the same statistical properties. therefore, the successful cross-spectral iris recognition depends on the best combination of photometric normalization techniques and the descriptors."
"2. calculate the gradient of the image i n by convolving smoothed image i with the derivative of the gaussian kernel function in the x, y directions."
"the oa results are promising considering two reasons. first, the minimum tuning work made in the algorithm parameters and second, the fact that partial results show an increased robustness, since using exactly the same parameters similar results are observed in the 100 cities problem kroa100."
"in a vo system, correcting cumulative errors is an important issue because the further the camera moves, the larger the localization error it has. when a new landmark is created using an inaccurate camera pose, errors of other landmarks being developed later also increase explicitly."
"an effective model was established to solve the problem of signal transmission through the stratum. in this mathematical model, the expressions of the electric field excited by an underground antenna were derived, which were in forms can be easily solved. numerical method was used here to obtain the current distribution and the electric field. in these numerical results, the variation of the axial and radial components of the electric field with the radial distance was investigated, and the voltage received on the ground was obtained. from the received voltage signal, we could get the information transferred from underground. the study of the field variation with the frequency and stratum parameters was done and made us know the characters of the antenna underground communications more clearly. higher frequency will cause larger attenuation on the radial component of the electric field and the total electric field, so slf and elf can be the appropriate frequency to apply. the conductivity has a relatively large impact on current and electric field, and the radial component of the electric field magnitude concerned gets smaller when the conductivity gets larger. the relative permittivity only has a slight impact on current and electric field, and there is a negative correlation between the radial component of the electric field magnitude and the relative permittivity."
"note that best results were obtained and bigger instances were solved using mmas with local search [cit] . in these preliminary tests, only little instances of the tsp were solved (because no local search was implemented) to make a comparison between both algorithms without any interference."
". parameters α and β define the relative influence between the heuristic information and the pheromone levels. while visiting city i, n i represents the set of cities not yet visited. the probability of choosing a city j at city i is defined as"
", where i represents the index of the landmark observed in k-1 frames. then, the camera moves a meter forward to a pose denoted as x k . at this pose, the camera observes landmarks"
"before establishing landmarks and estimating camera poses in an unknown environment, the system has to investigate whether the detected features can be matched with known landmarks in the map. if so, and if the matched landmarks are also observed in previous frames, the descriptors of the corresponding features are assigned to these landmarks. this process can be regarded as ''updating'' the matched landmarks."
"in the above, t 1, t 2, represent the bit wise of template 1 and template 2, respectively. n is the number of bits in the template; mask 1, mask 2 are corresponding noise masks for each template, respectively. the matching performance is evaluated using false acceptance rate (far), false rejection rate (frr), and genuine acceptance rate (gar). the far is an impostor score falling below the threshold (wrongly accept). otherwise, frr is defined as a genuine score that exceeds the threshold (wrongly rejected). the genuine score falling below the threshold is called gar. the computation of far, frr, and recognition rate are shown in eqs. (17), (18)"
"several parameters can be used to evaluate the system performance such as, spectral efficiency, delay, fairness and system throughput. the variety of parameters results on the creation of multiple scheduling algorithm and strategies."
"we must know that equity or fairness does not mean equality. the main objective of this category is to reach fairness and equity between users. generally, these algorithms have an insufficiency in term of spectral efficiency. several works have treated the fairness between users like, round robin, max-min and game theory based algorithms."
"unlike downlink scheduling, uplink scheduling side is much more complicated for several reasons, first, the ue sends the data to the enodeb and we know very well that the ue has a limited energy source; secondly, it is very difficult to predict the number of radio resources that the ue needs to exchange data with the base station. depending on the objective function considered and the traffic classes that pass over the radio channels, we have three different categories of schedulers: those dealing with best-effort flows (best effort schedulers), whose take into account the qos and those optimizing the power transmission. in this section we will try to turn on the main families of algorithms for resource allocation in lte uplink."
"the radio resource allocation is made in the enb by the ps, this task is too complex, as it requires taking into account several factors at the same time, plus it must be immediate (real time). the objective of this paper is to present a state of the art on the radio resource allocation in lte. in this work, we tried to go about the existed approaches in the literature in both downlink and uplink directions, we also mentioned some algorithms, we have shown the advantages and disadvantages of each category and algorithms, thereafter it would be wiser to focus on one type of traffic, trying to improve performance, and without doubt it will be the multimedia flows."
"in addition to comparing puf signatures across different fpga chips, we can also implement a puf multiple times on a single chip -each time in a different region of the chip. naturally, we expect that any two fpga chips should differ more (from the variations standpoint) than any two regions on a single chip. consequently, if puf signatures for different regions on a single chip are substantially unique, we have strong evidence that signatures between chips will be at least as unique. following this reasoning, in addition to comparing puf signatures across six different virtex-5 chips, for each virtex-5 chip, we investigated six puf implementations, one implementation in each of the six regions shown in fig. 8 . each region spans half the die horizontally and a third of the die vertically. puf placement was constrained to regions using range constraints provided to the xilinx synthesis tool. with six pufs per chip and six chips in total, we have 36 puf implementations. the methodology of using multiple unique implementations of the same circuit on a single chip can only be applied for reconfigurable platforms such as fpgas. we believe the same methodology may prove useful in researching other aspects of process variations."
"an artifact of state-of-the-art sub-100 nm ic manufacturing is that random variations in doping concentrations, line widths, or other properties cause unpredictable variations in transistor speed and interconnect. most puf designs compute unique signatures by exploiting such delay differences. at a high level, the approach taken in puf design is to incorporate multiple identical copies of a particular combinational path into an ic design. the copies must be perfectly matched from the logic, placement and routing perspectives, and ideally, also from the perspective of their physical surroundings. since the copies are identical, delay differences between the copies are due to random variations that are inherent to the manufacturing process, and cannot be controlled or cloned. puf circuitry measures the delay differences between path copies to generate the unique puf signature. puf circuits bring a useful purpose to the same variations which have a deleterious effect on other ic metrics, notably timing and yield."
"the puf designs described above are not specifically targeted to fpgas; they can equally be implemented within custom chips. moreover, several challenges arise when they are implemented on fpgas. first, both types of pufs in fig. 1 require that identical logic and routing be used along certain combinational paths, thereby guaranteeing that delay differences along such paths are due to process variations. in practice, identical implementation of paths requires the use of hard macros, which incorporate fixed placement and routing. the use of hard macros complicates the design flow and requires that a designer work at a lower-level of abstraction than rtl. in fact, it may be necessary to manually route the above puf circuits to ensure that specific fpga interconnect resources are used along certain paths. manual routing is tedious and error- prone. furthermore, the locked routing in the puf presents an \"obstacle\" to other design signals in the routing stage of the flow, potentially increasing design congestion and reducing circuit performance. finally, the prior puf designs consume considerable silicon area per puf bit. in contrast, ease-of-use within the fpga cad flow and low silicon area are key benefits offered by the proposed puf design."
"in the matrix creating process, the cd paradigm considers the channel state information (csi), so, users whom have larger csi values will have the opportunity to allocate more resources, this approach reaches high throughput values but suffers from starvation problem. meanwhile, the pf paradigm use the ratio of csi and data rate of each user, so fairness is proportional on csi values. this approach achieved good throughputs and at the same time, it solves the starvation problem."
one of the main features in lte systems is the multi-user scheduling because it is in charge of satisfying the qos of all active users.
"note that lut a's initialization bitstring is the complement of lut b's bitstring. the shift register implemented in lut a will produce the sequence 0101... and so on. whereas, the shift register implemented in lut b will generate the sequence 1010.... the shift register inputs, in, are assigned to allow the same sequences to continue beyond the initial 16 cycles. importantly, the shift register ou t pins drive the select input pins on carry chain multiplexers. both carry multiplexers have their \"0\" data input tied to logic-0. the bottom carry chain multiplexer has its \"1\" data input tied to logic-1. the output of the bottom multiplexer drives the \"1\" data input of the top multiplexer."
"unlike the previously mentioned types, data needs to be delivered in a time interval, but this type of traffic emphasizes the rate of loss of data (packet error rate)."
"in most applications, luts are used to implement combinational logic functions, in which case the lut's sram cell contents is programmed during device configuration and remains unchanged thereafter. an alternative application of a lut is to use it as a memory. each 6-lut in virtex-5 contains 64 sram cells and can therefore be used as a 64x1 ram. moreover, the luts in a slice can be combined with one another to implement rams with different aspect ratios. besides using luts as small rams, the virtex-5 architecture allows the internal sram cells within a lut to be chained together serially, thereby allowing the lut to function as a shift register."
"all these parameters can be summarized in one term, the consideration of flow's qos. trying to satisfy all these parameters is impossible, simply because the scheduling and resource allocation is an np-hard problem, because of this; different scheduling strategies have been developed. an important parameter in the design of schedulers is the support for qos. this forced the lte network to differentiate between the data streams and therefore can be distinguished:"
"the ac sub-task is responsible for accepting and rejecting new requests, in fact, the decision to accept or reject a request depends on the network capacity to deliver the qos required by the request (application) while ensuring the qos asked by the already admitted users in the system. the ps meanwhile, performs the radio resource allocation to the users already accepted by the ac, i.e., performing the ue-rb mapping by selecting ues who will use the channel affecting their radios resources rbs that permit them to maximize system performance."
"the pf scheduler is often used in 3g networks, as the rate of this type of network is limited. for beyond 3g networks, a key factor must be taken in the mind, the maximum delay of multimedia flows that represents the type of the most important traffic in the b3g networks. unfortunately this factor is not considered by this algorithm, consequently, for the non-real time flows it works fine but for the real time traffic it is not a smart choice."
"this approach considers flows classes when the treatment is different for each class rt and nrt. this type algorithm favors real time flows compared to not real time ones, which makes it the most suitable and more effective scheduling in lte networks, but equity is not really considered."
"consider the dynamic clocked behavior of the circuit in fig. 3 . initially, the ou t pin of lut a is at logic-0, and therefore signal n 2 is at logic-0. the ou t pin of lut b is logic-1, setting signal n 1 to be logic-1. at the rising clock edge, the ou t pin of lut a will transition from logic-0 to logic-1, and 1 every second clb in virtex-5 contains one slicem and one regular slice. the ou t pin of lut b will transition from logic-1 to logic-0. although lut a and the multiplexer it drives should be identical to lut b and its multiplexer, the two pieces of circuitry in fact experience different delays due to random process variations. we exploit this property for puf signature generation. there are two cases worth highlighting. first, consider the case wherein lut b and the multiplexer it drives are faster than lut a and its multiplexer. in this case, when lut b transitions from logic-1 to logic-0, signal n 1 also transitions from logic-1 to logic-0. following that, the slower lut a transitions from logic-0 to logic-1, and signal n 2 is held constant at logic-0 throughout the process. the second case is the opposite one where lut a and its multiplexer are the faster ones. in this case, lut a's ou t pin transitions from logic-0 to logic-1 and net n 1 has not yet transitioned from logic-1 to logic-0. a short positive spike (a glitch) will appear on n 2 for the period before n 1 transitions to logic-0. the presense or absense of a positive spike on n 2, and the length of the spike pulse, are due to process variations that impact the relative delays of luts a and b and the carry chain multiplexers."
"as we illustrate in the next section, our puf design configures luts as shift registers. 25% of the luts in virtex-5 can be used as memories/shift registers. such luts reside in slicem blocks, and are spread at regular intervals throughout the array 1 . slicem blocks are variants of slice blocks, where the \"m\" indicates the luts can be used as memories. luts in altera's stratix-iii fpgas can also be used as memories [cit] ."
"concerning exp-pf, the parameters w (t) and a define the required qos level by the flow. these parameters try to give more importance to applications with a higher level of qos. when the exponential part of the formula is equal to one, the exp-pf algorithm performs like the pf algorithm. this scenario is possible if the flows have almost the same delay for different users. the rr algorithms does not take into account the qos, because the flow does not have same needs (voip, streaming etc.), also allocate the same amount of resources is not really fair, because users have not necessarily the same channel conditions, nor same types of flows etc. beyond 3g networks, lte specifically focuses on qos of real time flows, so, using rr is really not the good choice."
"in this section, we present an art's state on existing scheduling algorithms for both directions downlink and uplink. these algorithms are based on the mathematical formulations mentioned above, try performing the efficiency radio resource allocation to the system's users."
"the radio resource allocation algorithms, aims to improve system performance by increasing the network spectral efficiency and fairness. it is therefore essential to find a compromise between efficiency (rate increasing) and fairness among users. several families and categories of algorithm exist in literature; each family usually contains a set of algorithms that have a common characteristics."
conversational class: this is the most sensitive to delay; it includes video conferencing and telephony. it does not tolerate delays because it assumes that in the two ends of the connection is a human.
"the uplink scheduling algorithms take in input a matrix with k rows (number of active ues) and m columns (number of rbs). m, is a matrix value associated to the couple ( − ). following the paradigm used, this value correspond to the csi for each rb on each ue (channel dependent), or the ratio between csi and he data date of each user. as we said, the value in the matrix represents the association between ues and rbs, thereafter, these values will be used by the scheduler."
"physically unclonable functions are circuits that leverage process variations to compute a unique signature for a fabricated ic. pufs have varied applications, including anticounterfeiting, hardware security, and cryptography. in this paper, we proposed the first fpga-specific puf design -one that takes advantage of the fpga logic and routing architecture. compared with prior work, our design consumes little area and is easy to implement and incorporate into a surrounding design. measured results on 65 nm virtex-5 fpgas demonstrate the puf signature uniqueness and its reliability at high temperature. future work will involve the development of fpga flows that employ puf signatures for ip protection/licensing and anti-counterfeiting."
"we instantiate 128 instances of the design described in section iii to generate a 128-bit signature. we evaluate the design using six virtex-5 fpgas on xilinx xupv5-lx110t development boards. the xilinx lx110 virtex-5 fpga on each board has about 69,000 luts, of which about 18,000 may be used as rams. our 128-bit puf uses less than 2% of such ram luts. the board has a serial rs-232 output which we use to communicate the puf signature to a connected pc. we clock the puf using the 27 mhz clock signal available on the board."
the general architecture of lte mainly contains the evolved packet system eps which includes: the evolved packet core epc and the radio part of the core network.
"long term evolution (lte) or 3.9g systems is an important technology originally designed to achieve a significant data rates (50mbit/s in the uplink and 100mbit/s in the downlink in a system bandwidth 20 mhz), while allowing the minimizing of the latency and providing a flexible deployment of the bandwidth. lte offers several main benefits for the subscribers as well as to the service providers. it significantly satisfies the user's requirement by targeting the broadband mobile applications with enhanced mobility. it is designated as the successor networks 3g. it allows an efficient execution of internet services emerging in recent years. it uses the packet switching process as well as 3g networks, the difference is the using of time division multiplexing (td) and frequency division multiplexing (fd) at the same time which is not the case of high speed packet access hspa networks, which performs only the time division multiplexing, this allows us to have a throughput gain (in spectral efficiency) concerning 40 %. [cit] orthogonal frequency division multiple access ofdma is the multiple access method used in the downlink direction. it combines time division multiple access tdma and frequency division multiple access fdma. it is derived from ofdm multiplexing, but it allows the multiple access of the radio resources shared among multiple users. the ofdma technology divides the available bandwidth into many narrow-band subcarriers and allocates a group of subcarriers to a user based on: its requirements, current system load and system configuration, this process helps to fight the inter symbol interference isi problem or the channel frequencyselective, as well as, it allows for the same bandwidth a higher spectral efficiency (number of bits transmitted per hertz) and it has the ability to maintain high throughput even in unfavorable environments with echoes and multipath radio waves."
"this type of algorithm tries to maximize the objective function that represents the data rate, this approach treats the real time flows and non real-time, the resource allocation depends on the size of the queue of each user. exp rule, max -weight are examples of this category."
"for the fme, the algorithm starts with searching the ue having the highest metric value, once found, it expands the allocation process in the left or in the right (it compares the value of rb in the left with right value for the same ue and chooses the highest), until the algorithm finds no more rb whose having highest metric for the same user selected above."
"as a conclusion, the radio resource allocation is feasible (several algorithms and approaches exist), but the diversity of flows (qos) and radio conditions affect the performance of the algorithms. the resource allocation is an np-hard problem, since the algorithm tries to maximize and/or minimize several parameters simultaneously. for this reason, each approach or algorithm tries to optimize the maximum parameters that could."
"for our experimental study, we found that the configuration shown in fig. 6 produced the best results in terms of achieving a roughly equal balance of puf bits being logic-0 and logic-1. lut a is in the top-most position of a first slice, and lut b is placed in the third position in a slice immediately below. note that the slice-to-slice carry connection is made through a dedicated fast wire between slices, and not through general purpose interconnect. the intermediate multiplexers along the carry chain between luts a and b have their select inputs tied to logic-1. observe that the flip-flop receiving the potentially \"glitchy\" signal (n 2) is placed within the top slice (utilizing an extra slice is not needed for this flipflop)."
"r (u) : average throughput of user u at tti t r * (u, c) : estimated throughput of user u at resource chunk c at tti t. resource chunk rc is a set of continues rbs."
"b. xilinx virtex-5 fpgas fig. 2 (a) depicts a virtex-5 logic block, called a configurable logic block (clb). a clb comprises two slices, as shown in fig. 2(b) . a slice contains four 6-input lookup-tables (luts), four flip-flops, and other arithmetic circuitry (not shown in fig. 2(b) ). 6-input luts are small memories that are capable of implementing any logic function of up to 6 variables. in particular, each lut contains static ram cells that hold the truth table of the logic function implemented by the lut. clbs are arranged in a two-dimensional array on the fpga and can be connected to one another through a programmable interconnection matrix. fig. 2(c) shows additional slice details that are used in our puf design. observe that each lut output connects to the select input of a 2-to-1 multiplexer. each multiplexer receives one of its data inputs from the multiplexer below it, and the second data input can be received from outside the slice. the dashed lines indicate that the lut outputs also drive other circuitry, not shown in the figure. the vertical chain of multiplexers is called the carry chain and it is intended for implement-ing fast arithmetic operations. we use the carry multiplexers in our puf design implementation. note that the carry chain is not unique to virtex-5; similar structures are present in fpgas from other vendors, such as altera's stratix-iii [cit] ."
"four main classes of puf have appeared in recent literature: 1) ring oscillator (ro)-based pufs [cit], 2) arbiter pufs [7, 8, 9, 10], 3) memory-based pufs [cit], and 4) glitch count-based pufs [cit] . of these, the ro-based and arbiter puf are the most well-studied. fig. 1(a) shows a simple ro-based puf with two oscillators. the oscillators (shown in the dashed boxes) must have an identical layout on the ic, such that any frequency differences between the two are owing strictly to process variations. the two oscillators drive counters, and following a period of oscillation, the counter values are compared, producing a single bit of the puf signature. ro-based pufs contain many ros and the pairs of ros to compare may either be pre-selected, or a multiplexer may be added just before the comparators to allow user-selection of the ros to compare. in this way, the puf can function in a challenge/response paradigm wherein user-supplied input vectors are paired with puf responses. the challenge input from the user indicates the ros to compare; the response is the 0/1 indication produced by the comparator. the challenge/response notion is stronger from the security perspective than having one unique signature. in challenge/response, a unique signature response is produced for each unique user input challenge. fig. 1(b) shows an arbiter puf, comprising two parallel nstage multiplexer chains feeding a flip-flop. a step input is applied, along with challenge input bits x(0 : n). at stage i of the multiplexer chain, the step input signal is either fed forward along the same chain, or it is interchanged with the opposite chain, as controlled by x(i). depending on the challenge bits and the delay differences between the top and bottom mux chains, the step input will either arrive at the flip-flop's d or clock input first, causing either a logic-1 or a 0 to be latched, respectively. the latched value constitutes one bit of puf signature. while fig. 1 illustrates the basic design concepts, numerous improvements have been proposed to strengthen their resilience to attacks [cit] . the third type of puf, a memory-based puf, uses the random initial state of memory bits on device start-up as the puf signature [cit] . the fourth and final type of puf counts variability-dependent glitches on the output of a combinational multiplier [cit] ."
"for the non elastic flows (best effort flows), the hol packets delay is similar for all users (do not differ a lot), the exponential term is closed to 1, and the exp-pf perform as the pf algorithm."
"field-programmable gate arrays (fpgas) are programmable chips that can be configured by the end-user to implement any digital circuit. fpgas are used in a broad range of computing and embedded applications because of their reconfigurability, improving physical qualities (i.e. speed, area and power), and their steadily decreasing cost. the economics of semiconductor scaling is such that as feature sizes shrink, the costs associated with building a custom asic escalate rapidly. this trend has made fpgas the technology of choice for many applications today, even those with tens to hundreds of thousands of units in volume."
"the rest of the paper is as follows, in section 2 will be presented the mathematical modeling of the radio resource allocation problem; in section 3, a state of art of the radio resource allocation strategies and a detailed study of several scheduling algorithms proposed for lte (uplink and downlink) is made, we will present the scheduling algorithms existing in the literature and evaluate the performance of these algorithms with some criticism in section 4, then a conclusion and perspectives will be presented in section 5."
"with the goal of managing pulse width in mind, we note that the position of lut b in the slice can be tailored to create a meaningful puf. as mentioned previously, each virtex-5 slice contains four luts, shown in fig 5. lut a will reside in lut slot position 0. lut b can be placed in any of slots 1, 2 or 3 to modulate the pulse width. increasing the slot location number of lut b will tend to increase pulse width, as transitions from b's output will take longer to propagate to the top-most mux in the carry chain. furthermore, the bottommost carry chain multiplexer in the slice can be driven by the top-most multiplexer in the slice immediately below. the purpose of this connectivity is to enable the creation of longer carry chains (i.e. longer than 4 bits). the carry connectivity between slices permits longer pulse widths to be produced within the proposed puf, with the puf circuit spanning multiple slices arranged vertically in the placement."
"we propose an fpga puf circuit that, based on random process variations, will produce either a logic-0 or logic-1. multiple instances of our circuit will be instantiated to create a multi-bit puf sigunature. fig. 3 shows the core of our puf design. two luts, a and b, within a virtex-5 slice are used in 16-bit shift register mode. the shift register contents are pre-initialized as follows:"
"we use the presense/absense of a positive spike on n 2 to determine a puf signature bit. n 2 is connected to the asynchronous preset input of a flip-flop, as shown in fig. 4 . the flip-flop is initialized to logic-0 and has its output q fed back to its d input. in the event that a glitch on signal n 2 reaches the preset, the flip-flop output becomes logic-1 and the puf signature bit is logic-1. otherwise, the puf signature bit is logic-0. the flip-flop of fig. 4 can be located in the same slice as the circuitry of fig. 3 because they can use the same clock signal. the virtex-5 slice has an architectural restriction that only a single clock signal may be used in any given slice. we used relative location (rloc) constraints (a xilinx physical packing constraint) in our vhdl to locate the flip-flop in the same slice as the circuitry of fig. 3 ."
"this type of algorithm focuses -on the spectral efficiency of real time or non-elastic flows (video and voip), indeed, it tries to maximize the objective function that represents the data rate, this approach treats the real time flows and for the non real-time flows, it considers that they do not deserve any priority. the radio resource allocation process depends on the size of the queue of each user."
"circuit speed is influenced by die temperature and therefore, puf signatures are sensitive to temperature. ideally, signature variation with temperature would be small or non-existent. for the data given above, room temperature fpgas were configured and a puf signature measurement was taken immediately. we compared those \"cool\" signatures with \"hot\" signatures gathered at high temperature to gauge puf reliability. the virtex-5 fpga has a built-in system monitor that can measure die temperature within 4"
"although the delay differences due to process variations may trigger a short pulse on signal n 2 as shown in fig. 3, the pulse width may be so short that it is \"filtered out\" on its way along the routing path to the flip-flop preset input. fpga routing contains buffered switches and metal wire segments. in essence, the resistive and capacitive loading on the routing path can be viewed as a low-pass filter, potentially damping out the highfrequency pulse, thereby causing the puf bit to be logic-0 with high probability. in fact, the rc nature of the routing is also impacted by process variations. conversely, if the pulse is always generated and its width is too wide, it is more likely to reach the flip-flop preset input, making puf bits logic-1 with high probability. hence, it is desirable if the pulse \"resolution\" can be tuned to maximize the puf utility."
"streaming class: similar to the previous class, but it assumes that only one person is at the end of the connection, therefore, it is less demanding in terms of time and delays. eg: video streaming interactive class: examples of this class are: web browsing, access to databases ... etc."
"hol and delay of packet flows are the fundamental parameters of this kind of schedulers. this type treats the non elastic flows, when a packet exceeds its hol, it will be removed from the queue. m-lwdf is a delay based algorithm and in the same time an opportunistic one."
"an important element of the lte architecture is the enodeb, which has an interesting task, the rrm consisting mainly of two sub-tasks: the ac and the ps."
"the principle of greedy algorithm is that the rbs are grouped into rcs, with each rc containing a set of contiguous rbs. after that each rc gets allocated to the ue having the highest metric in the matrix, the rc and ue will be removed from the available rc list and ue schedulable list. the algorithm aims to maximize the fairness in resource allocation among ues."
"trying to satisfy all users in the mmf algorithm, gives the advantage to users with low requirements, so they will often served. in the other side, users who require more resources are penalized. this approach does not take into account the multiuser diversity and the fact that streams have different qos requirements i.e. fairness does not represent equality. to summarize, this algorithm is really not the right choice for scheduling in lte."
"background class: also known as best effort class, no qos is applied; it tolerates delays, packet loss. examples of this class: ftp, e-mails etc. [cit] two other parameters influence the design of scheduling algorithms in lte uplink. the later are imposed by the sc-fdma access method, which are: the minimization of the transmit power (up to maximize the lifetime of ues batteries), as well, the rbs allocated to a single ue must be contiguous. this makes the radio resource allocation for lte uplink more difficult than for the downlink."
"for the lte uplink radio resource allocation, the scheduler has in input a ue-rb association matrix in order to give the best results that improve the system performances."
"f is defining as : the first one, named single channel scheduling algorithm sc-sa assigns one rb to each ue at a given tti. in case that number of active users is lesser than the number of rbs, the algorithm distributes the rbs proportionally between users according to . otherwise i.e. if the number of schedulable users is higher than the total number of available rbs, it assigns rbs to users experiencing the poorest conditions (eg, users that the maximum delay is almost reached). the main objective of this algorithm is to allocate resources to ues with severe qos constraints. the second is called multiple channel scheduling algorithm (mc-sa). it is similar to the first one; the main difference is the possibility to allocate more than one rb to the users that are not meeting the throughput target. these algorithms have the same behavior in case the number of ues is smaller than the number of available rbs in the system. in the case where the number ue is higher than that of available rbs, it allocates the taking into account the (19) equation."
"the need to ascribe a unique binary signature to an integrated circuit (ic) has applications in digital design and embedded systems, ranging from digital rights management, ip protection, cryptographic key generation, device authentication, and ic counterfeit detection/prevention. [cit], when the fbi announced that counterfeit cisco networking products had unknowingly been purchased and used by the u.s. government [cit] . more recently, over the past few months cloned cellphones and the industry that produces them have garnered considerable media attention [cit] . in addition to copies of specific products, a legitimate concern for fabless semiconductor companies is that counterfeit versions of their chips can easily be made and sold by malicious individuals in the same fabs to which they outsource their fabrication. to counter such threats, recent work has considered methodologies for ending ic piracy that requires a fabricated chip to generate its own unique signature [cit] . a physically unclonable function (puf) is a new concept in hardware security, and a promising candidate for ic signature generation."
"concerning uplink side, it is much more complicated for the two sc-fdma constraints added, the rbs allocated to a single user must be continuous, and the signal power transmitted constraint. algorithms dealing with qos are best suited and most respondents because they treat the most important factor in lte networks, which is the qos flows. but also the best effort schedulers have a good performance and there are most used because of their low complexity."
"epc consists of a set of control elements: mobility management entity mme, home subscriber server hss, serving gateway and packet-data s-gw and p-gw. the epc is responsible for connecting with other 3gpp and non-3gpp networks. the radio part of the network is composed of enodeb and ue."
"opportunistic scheduling considers user where queues are continuously backlogged (this full buffer setting is typically used to model elastic or best effort flows). the main objective of this type of algorithm is to maximize the overall system throughput. several algorithms use this approach such as: pf (proportional fair), exp-pf (proportional fair exponential) and the m-lwdf (maximum largest weighted delay first) scheduler is an opportunistic scheduler and also a delay based one, so we will describe it later in the delay based algorithms section."
it starts with bad conditions one; it first looks for all the rb that maximizes data rate and then looks at the left and right of this rb to the allocation of remaining n rbs.
"for the uplink direction, single carrier-frequency division multiple access sc-fdma method is used, it is a variant of ofdma, they have the same performance (throughput, efficiency ... etc.), but sc-fdma transmits sub bands sequentially to minimize the peak -to-average power ratio papr (ofdma has a huge papr), this is the reason of choosing sc-fdma in the uplink side, to deal with the limited power budget (the use of battery by the ue) to minimizing the papr."
"the central objective of lte scheduling is to satisfy quality of service qos requirements of all users by trying to reach an optimal trade-off between efficiency and fairness. this goal is very challenging, especially in the presence of real-time multimedia applications, which are characterized by strict constraints on packet delay and jitter."
"in the other hand, the rme scheduler starts similarly to fme (it searches for the couple (ue-rb) having the highest metric value), then it expands the allocation process both in the left and the right until there will be no more users whose maximum metrics belong to the same user. the mad algorithm is a search-tree based; its problem is having a higher computational complexity."
"(4), represents the objective function that is designed to maximize the data rate. (5), means the constraint that aims to guarantee the minimal data rate for each user. (6), is constraint assuring that one sb is assigned to one and a single user. (7), all sbs owed to a user employ the same msc (it is an lte networks constraint)."
"for team management: as observation 2 states, hybrid repositories do not show the same trends as non hybrid ones. adopting new tools and new technology is only part of the change and by no means enough or complete. tools that bring a new vision to how software is developed should be followed by a shift in policy and project culture as well. one cannot hope to improve the development process by only improving the tools."
"we envision a new generation of tools that can use the average size of a commit as a quality metric. when a developer has uncommitted code larger than a threshold, the tool could suggest that it's time to split changes and commit."
"the tool was tested on single-trial lfp responses recorded from rat barrel cortex evoked by whisker deflection under a mixture of til-xyl or urethane anesthesia. as evidentiated in the subsequent sections, the tool successfully removed stimulus artifacts, detected different features of the evoked response and accurately estimated the signal shape characteristics. also the tool has an option to perform batch processing of multiple experiments. (fig. 6, inset a) which was then used in detecting the artifact's start and end more precisely and at last removed from the signal. it is worth mentioning that the single-trial lfp response shown in fig. 6 was with only one stimulation artifact present, but the algorithm is capable of successfully removing multiple stimulation artifacts from single-trial lfps in one run."
"rq 6: does team size affect the choice of vcs? table 10 shows that most teams use the distributed model, regardless of the team size. however, the presence of the centralized version control systems increases once the team size increases. § ¦ ¤ ¥ observation 10: teams of all sizes predominantly prefer dvcs interpretation: since 53% of the survey projects are less than two years old, it is likely that they were developed during the rising popularity of the dvcs. as for the popularity of cvcs at larger teams, this can be interpreted as inertia of larger teams to use new paradigms and tools. however, once the team size crosses 100, the overhead of merging changes pushes the team against their inertia."
"one of the main threats is the practice of squashing commits. as we have shown in rq 3, squashing is a used practice among software developers. for dvcs, roughly 36% of developers squash their commits. because squashing rewrites history, it is impossible to detect squashing activity. the main effect is that commits gets larger, because squashing combines two or more commits into a single commit. the result is an increased commit size. thus, the average commit size for dvcs might be even smaller than the ones we report. observation 1 would still stand even in the case of heavy squashing practices."
"stimulus evoked lfps in the barrel cortex are thought to mainly derive from activity of neuronal populations in the vicinity of the recording electrode and can therefore be considered as fingerprints of local network activity [cit] . for this reason, it is a general practice to repeatedly record single-trial lfps (i.e., responses to a given stimulus) and then obtain a stimulus-locked averaged signal to be characterized under the assumption that the network response to the same input is similar across trials. however, this may be a too strong assumption. previous studies have demonstrated that relevant information provided by singletrial lfps may be lost in the averaged signal [cit] . consequently, extraction of single-trial lfp parameters can be important to truly characterize the network dynamics (e.g., by determining the cortical layer activation order [cit] ) and to reconstruct neuronal activity (e.g., by current source density analysis). in this context, signal shape characterization also plays a fundamental role [cit] and methods are required to extract essential shape features and to cluster single-trial lfps accordingly. whereas a number of methods are available to perform a detailed characterization of neuronal spikes [cit], methods for lfps analysis are lagging behind. recent efforts have been made contributing to fill the gap [cit] including denoising [cit], artifact removal [cit] and shape-based classification of lfps [cit] . efficient automated methods for single-trial lfps characterization, however, are still lacking. this represents a challenge given the typical variability of shapes that can be found among single-trial lfps when recording in the barrel region [cit] as well as by the various possible contaminations from stimulus artifacts."
"we found that developers' behavior is influenced by the vcs type. when using dvcs, developers make commits 32% smaller and they organize their changes in several commits. depending on the vcs type, the reasons why developers find the commit process more natural are different. we found that developers using dvcs include issue tracking labels more often in commit messages. also, the commit size decreases as the project matures."
"under til-xyl, the evoked responses recorded from the superficial layers (i.e., layers ii and iii) showed less variability than those recorded from deeper layers (i.e., layers iv and va). this was highlighted in the distributions of the rol (figs. 9 and 11a, in gray) and peak latencies (figs. 10 and 11b, in gray): in particular, the distribution of the rol was wider in the deeper layers than those in the superficial ones."
"results from the survey show that only 30% of the developers squash their commits. the results for the distributed and centralized repositories are shown in table 6 . table 6 shows that squashing happens twice more often in distributed repositories than in centralized ones 2 . this probably has to do with the fact that it is easier to manipulate commits in dvcs. developers who practice squashing mention two main reasons (table 7) : (i) to group several changes together and; (ii) they do not care about the path they took to a solution as long as it's finished and it works. © observation 6: squashing does not occur often in practice. if it does occur, it's a practice mainly associated with dvcs rq 4: why do developers prefer one version control system over another?"
we hope that our work inspires future research not only into the impact that centralized and distributed vcs tools have on software development but also on how general properties of vcs tools enable developers to manage and express change.
"in our initial manual investigation of commits we have discovered that many commits do not represent actual programming changes carried out by a developer (e.g., adding features, bug fixing, refactoring, etc.), but are the result of applying tools such as code formatters. such commits are extremely large, i.e., they affect thousands of loc. since these commits would bias our analysis, we decided to filter them out. our analysis filters out any commit that:"
"after craniotomy, contralateral whiskers were trimmed at about 10 mm from the mystacial pad. the principal whisker was then identified, as the one which provided the maximum response amplitude when deflected, from a subset of whiskers corresponding topologically to the electrode's position in the cortex. the principal whisker was deflected repeatedly by rapidly displacing a 25g hypodermic needle (bd plastipak, madrid, spain) which was attached to a piezoelectric bender (p-871.122, physik instrumente, karlsruhe, germany) using double-sided tape. the bender was driven by a waveform generator (agilent 33250a 80 mhz, agilent technologies inc., colorado, usa) by providing square stimuli at 1 khz which was triggered by a custom labview program (www.ni.com/labview/)."
"individual operators produced 1850 tracks (113807 annotations) which were merged into the 728 tracks of the consensus tracking ground truth. the performances of each operator with respect to the consensus ground truth is reported in (table 7) . to this end, the tra 15 measure was computed. this measure includes a complete comparison of tracks represented as an acyclic oriented graph 16 . in order to estimate this measure, the ground truth and the individual tracks were converted in the format described in 15 and evaluated using the trameasure software provided along. however, that software and methodology matches a cell in the ground-truth with a cell in the track to be evaluated, when they overlap more than 50% in space. being our dataset centroid-based a difference of 1 voxel would made the matching not possible. hence, considering the typical cell diameter, we approximated a sphere around each of the centroids. the tolerance radius of the spheres was at maximum of 10um and was truncated in case of two centroids closer than 10um. the script ltdb2ctc.m was used to export the ltdb tracks in the acyclic oriented graph format described in 14, 15 ."
"such mechanisms could be more complex branching and merging models. [cit] show that as projects grow in size and activity, the complexity of the branching model increases."
our finding that developers from hybrid repositories use the same habits after switching to dvcs as when they used cvcs suggests the need for tools to help educate developers on how to effectively change their habits.
"for tool builders: practically all large teams use issue tracking systems in order to track work items. however, in the current state of practice developers track code and issues by inserting issue references inside commit messages. this is tedious and imprecise since developers have to manually group changes by issue."
"for developers: smaller commits make code reviews easier. having a tool that enables small, fine grained commits allows users to separate and document each change individually. one participant mentioned that they split their commits because \"[changes] should be logically separated, to easily allow [the] commit message to drive [the] review\". consider reviewing a new feature that has been added. instead of going through thousands of changes, the reviewer can go through one change at a time, each explained by the commit message."
"squashing is a process by which history can be altered or completely lost. to prevent history loss, vcs tools could support features such as hierarchical commits: the ability to create a virtual commit that holds other real commits. instead of loosing history through squashing, developers could group commits into larger, composite commits."
"the distributions of the rpa were less informative (fig. 12), showing wide and flat distributions; however, comparing the mean values of the response onset amplitude (roa) and the rpa (fig. 13), it was noticed that the evoked responses elicited by the whisker stimulation were larger in amplitude in animals anesthetized with urethane. further experiments are required to understand whether these phenomena are associated with diverse effect of the anesthetics on the synaptic plasticity."
"for detecting cell populations visible in more than one channel (table 2 (available online only) and table 3 ) we encourage the usage of a co-localization method based on supervised machine learning such as ilastik 23 or trainable weka segmentation 24 . for discriminative machine learning models, it is worth noticing that all the cells of the videos ltdb001 to ltdb020 which are expected to be visible in the indicated channels were tracked. other objects such as background, cell debris or additional cell populations were not tracked."
"for tool builders: although git enables finer grained changes, it is still the developers' task to disentangle these changes. this is a manual, tedious, and time consuming process. vcs tools could keep track of different change intents and then offer to commit them separately. [cit] show a technique by which this can be achieved. they devised a heuristic untangling algorithm that splits tangled changes according to different source code criteria (e.g., the distance between two changed ast nodes)."
"artifact detection (ad) this subfunction provided with the number of artifacts present in a single-trial lfp signal and marked the probable end of the artifacts, which was then sent to the next module for removal. to detect an artifact each data point of the signal starting from the stimulus onset was checked for threshold crossing and its direction with respect to the previous data point (up or down). when a pair of threshold crossing data points with same directions was detected in the signal, the second point in the pair was considered as a probable end of an artifact. the pseudocode is listed in algorithm 1."
"centroids of cells were manually annotated and linked over time, using the \"spots drawing\" tool from imaris (bitplane). this process was performed by a group of three operators who tracked all the cells"
"using a dvcs can offer developers more power when it comes to choosing what to commit. dvcs tools like git allow the splitting of commits at line level, which helps when changes with multiple intents are interleaved in a single file. this kind of separation is not possible when using svn. a participant mentioned that he preferred git because \"it gives useful tools for splitting or merging commits\"."
"despite the existence of specialized imaging software packages such as imaris (bitplane), volocity (perkinelmer) and fiji 11, the automatic analysis of immune cell migration 10 in mp-ivm data is problematic. challenges are introduced at each stage of the previously described pipeline and arise both from the complex biomechanical properties of leukocytes and from technical artifacts of in vivo imaging (fig. 1, table 1 and fig. 2a ). more specifically, high plasticity of cell shape, sustained speed and frequent contacts, set a limit on the capacity of detecting and tracking cells for long time periods 12 . additionally, technical artifacts such as the variation and non-uniform diffraction of the light emitted by fluorescentlytagged cells or the physiological movement of the sample due to peristalsis, breathing or pulsing of blood vessels, further challenge the automatic analysis. therefore, additional steps such as image pre-processing, tuning of software parameters and manual curation of tracks, are required to improve tracking results. as a consequence, usability of imaging software is reduced 13, bias introduced and the reproducibility of the results is compromised. an example is provided in (fig. 2b) where the track speed mean, directionality, track length and track duration were computed for the entry ltdb017a (data citation 1). these values exhibited highly significant differences (p 0:0001) between automatically-generated vs. manuallygenerated tracks."
"multiple independent annotations and tracks were merged into the consensus ground truth provided along with the dataset using a majority-voting scheme. this process was performed manually by a fourth expert using the \"unify\" functionality of imaris. the matlab script ltdbcheck.m was used to facilitate a video belongs to a collection, depicts one or more problematic and includes an image series. the image entity is double-framed because it is a weak entity, which depends on the video entity. a cell has one type and one unique identifier. one video tracks one (or more) cell, every cell being depicted by the track association at a given timestamp (t) and in a spatial position (x, y, z) of that video. the visiblein association further describes the channel of the video in which a cell is visible. the logical database is derived from the conceptual model and then optimized for read-access. the cells_positions track matching, detecting common errors and highlighting conflictive situations. two tracks were said likely to \"match\" (i.e. referring to the same cell) if their annotations were closer than 10um for at least n time instants. n was defined as the minimum between the track duration and 10. conflictive situations were detected as tracks matching for only certain time instants but not for the entire track duration. these include a) tracks with an annotation in a far position by mistake, b) a longer track matching with one or multiple shorter tracks, c) two tracks matching for n instants but having different initial and/or final positions (i.e. track switches for closely interacting cells) amongst others. tracks with a duration shorter than 4 time instants were also inspected manually. due to the high plasticity of cells these criteria were used only to facilitate the work of the fourth expert who had to manually merge multiple tracks as follows: if at least two operators agreed on the direction of a cell, the track was included in the dataset (i.e. two matching tracks having the same duration and detected in the same frames). if two operators tracked a cell, but the track duration was different, the points annotated only by one operator were evaluated, confirmed or discarded by the fourth expert. when two operators could not agree on the direction of a cell, the following method was applied. if the fourth expert or the matlab script identified an evident tracking error (i.e. cells not annotated by mistake, unrealistic jumps or broken tracks) the error was corrected and the tracks were merged. for real conflictive situations (i.e. track switching for closely interacting cells) the experts were asked to meet and discuss the most appropriate solution. if still the majority consensus could not be reached, and only in this case, tracks were interrupted. finally, the position of cell centroids included in the ground truth was not averaged but selected as the centroid closer to the mean. although this choice may produce less smooth tracks, it avoids to position a centroid outside non-convex cells. these criteria together with the manual merging of tracks and re-evaluation of tracking conflicts, allowed to include the maximum number of tracks for the longest possible period of time."
"in this type of client-server architecture user interface, functional process logic ('business logic'), and data storage and access are maintained as independent modules to promote software modularity allowing any of the layers to be upgraded or replaced independently with changing requirements [cit] ."
"on the other hand, large teams tend to squash more often which would result in bigger commits with more entangled changes. therefore, researchers may have to trade off some traits over other ones when choosing to study repositories from small or large teams."
"multi-photon intravital video microscopy (mp-ivm), in combination with image-based systems biology 1, represent a key methodology for studying the interplay of cells in organs and tissues of living animals 2 . indeed, recent analyses of leukocyte migration in mp-ivm data, highlighted unprecedented cell-to-cell interaction patterns such as antigen capturing 3 and presentation 4, hostpathogen interaction 5, 6, tumor immune surveillance 7 and cell activation 8 amongst others. the advantage of mp-ivm with respect to other optical methods relies on the usage of multiple infrared photons. the low energy of the photons allows a deep and point-wise excitation of the sample which reduces light scattering and limits photo-damage. these properties make mp-ivm suitable to capture 4d data with remarkable resolution, depth and prolonged periods of observation 9 . the most common image acquisition and analysis pipeline of mp-ivm data is depicted in (fig. 1) . initially, an animal having fluorescent cells, is anaesthetized, and prepared for imaging by immobilization and surgical exposition of the organ of interest (fig. 1a left) . then, 4d data, composed of parallel image planes at different depths, are acquired for several time instants (fig. 1a right) . after acquisition, data are analysed by detecting cells (fig. 1b left), tracking their position over time (fig. 1b right) and finally quantifying cell migration 10 . the described pipeline was used to generate all the entries proposed in the current work (fig. 1d) ."
"the expected usage of ltdb is to serve as a ground truth for the validation of tracking algorithms (fig. 4a) . differences with respect to the ground truth can be evaluated using, for instance, a metric that accounts for complete tracking graph comparison 16 . ltdb further aims at being a training dataset for supervised machine learning methods. indeed, in light of the recent application of deep learning for object detection and tracking in highly variable scenarios [cit], ltdb can provide the large number of images-tracks pairs required for the training of predictive models (fig. 4b) . in this case, broad imaging conditions may support the generalization capabilities of these methods."
"we found that the use of cvcs and dvcs have observable effects on developers, teams and processes. the most surprising findings are that (i) the size of commits in dvcs was smaller than in cvcs, (ii) developers split commits (group changes by intent) more often in dvcs, and (iii) dvcs commits are more likely to reference issue tracking labels. these show that dvcs contain higher quality commits compared to cvcs due to their smaller size, cohesive changes and the presence of issue tracking labels."
"according to the survey, we have found two main reasons why developers find a commit process more natural. the first is the presence of a killer feature. it usually helps developers achieve higher productivity by allowing a workflow that is more comfortable for them. the second is habit. developers get used to a certain tool. therefore, they will find the tool natural to use from the habits they have acquired while using it on a daily basis. table 8 summarizes the complete results."
interpretation: the fact that we see a larger number of developers committing changes belonging to two or more issues per commit for cvcs might be an indication of a higher difficulty in selecting the changes to be committed. the difference in the granularity of change selection between the tools could explain these results.
"are developers truly taking advantage of these dvcs features or are they simply paying the steep learning price without benefiting from them? despite the large scale adoption of dvcs, we know little about the state of the practice in using this new paradigm. without such knowledge, developers and managers are left in the dark when deciding whether it is worth to invest time and effort to transition to these new tools. also, researchers are in danger of making errors when mining repositories, due to confounding effects imposed by dvcs. finally, tool builders can build the wrong tools if they are not aware of developers' habits."
another possible cause that enables small commits in git is that each developer commits to his own local repository without the need to synchronize with everybody else. this means that there is no risk of conflicts upon every commit.
"interpretation: this decreasing trend can be explained by different types of changes that happen during projects' life. in the early stages of development, commits tend to be larger because developers are adding features from scratch. as the project matures, development switches from adding new features to performing corrective changes, like bug fixes. corrective changes are usually smaller in size."
"to the best of our knowledge, our paper is the first study to compare the impact of cvcs and dvcs on the practice of committing changes."
"in 46% of the cases developers prefer dvcs because of a killer feature. by looking at individual replies we have found that one of the features mentioned is the possibility to commit to the local copy of the repository. also, we can see that the main reason for preferring cvcs is the ease of 2 see internal threats to validity (section 4)"
"however, the data shows that developers use the same intuition for splitting changes, regardless of team size. this could hint that large teams must use other mechanisms to control the complexity of software changes."
"in order to evaluate tracking performances we provide a matlab script ltdb2ctc.m to export ltdb tracks as the acyclic oriented graph representation 16 used in the cell tracking challenge described in 14, 15 . this allows the usage of the accurate methodology and software provided by the aforementioned authors to compare computed tracks vs. ground truth."
"dvcs brings a whole set of novel capabilities. using dvcs, developers (i) can work in isolation on local copies of the repositories enabling them to work offline while still retaining full project history, (ii) they can cheaply create and merge branches, and (iii) they can commit individual changed lines in a file, as opposed to being forced to commit a whole file like in cvcs."
rq 1: does the type of vcs affect the size of commits? rq 2: do developers split their commits into logical units of change? how do they do it? rq 3: how often and why do developers squash their commits?
"signals recorded by single neurons are not sufficient to understand brain circuits functions, and population approaches are needed. as single-trial lfps represent fingerprints of a local neuronal population activity, their analysis will help in revealing the mechanism of information processing by neural networks. the high variability among single-trial lfps poses a challenge in developing general purpose and efficient automatic analysis tools. moreover, single-trial lfp responses are often contaminated by complex stimulation artifacts that must be removed. therefore, the algorithms proposed in this work were designed to perform automatic characterization of evoked single-trial lfps, first by removing the stimulus artifact and then accurately characterizing the various features by quantifying related parameters. as highlighted in the above sections, the tool is efficient and accurate and capable of capturing small differences in the single-trial evoked responses. finally, the tool's purposefulness is demonstrated by its application to the study of the effect of anesthesia on the variability of single-trial lfps from rat barrel cortex."
"rq 1: does the type of vcs affect the size of commits? table 3 shows the commit size, both in lines of code and in number of files, made by individual authors. this data is grouped by vcs type."
"accordingly, as it has been reported earlier [cit], three corresponding parameters are used to characterize the evoked responses: response onset latency (rol), response peak latency (rpl) and response peak amplitude (rpa). however, to be able to compute these parameters precisely, accurate detection of the related features is very important and a custom algorithm was developed to the purpose."
"after detection of the response onset the rest of the signal was divided into chunks and scanned for derivative change to find the other features. care was taken in case of the sp, which is rarely present in a signal and if present, may have either positive or negative direction. in case of the positive direction, a threshold of 10 lv was set to make sure that it indeed was a feature and not just background spontaneous brain activity. if the signal was found to be going down, then the maximum negative peak was found and from that peak a time window ae5 ms was selected and scanned for occurrence of yet another negative peak. if a second negative peak was detected, the first negative peak was set as feature 1 and the second peak as feature 2 (rp); otherwise, the positive peak was absent and the feature 2 (rp) was set as the maximum negative peak."
"for team management: because commit size tends to become smaller as projects get older, it is reasonable to assume that developers tend to spend more time analyzing existing source code instead of adding new code. therefore developers' productivity should be measured not only by the amount of code, but also by the complexity and importance of their changes."
all the videos and tracks are made available as individual files or as a spatio-temporal database ( fig. 3a) which was optimized for faster access to data and metadata (fig. 3b) .
"in our corpus of open-source repositories, 83% of the projects were developed in java, and the remaining 16% used c/c++, javascript and python. moreover, the pure git repositories consist of 98% java projects whereas the pure svn repositories consist of 80% java projects. while we have no reasons to believe that programming language affects the culture of committing changes, in the future we plan to diversify our corpus and explore change variation in programming languages."
"we conducted a survey where we asked 20 questions about developer commit practices. 820 respondents answered our survey. the participants are developers recruited by promoting the survey on social media channels specific to the development community, i.e., twitter and google+ feeds that are mainly read by developers. table 1 shows the demographics of the respondents. most are experienced developers working on industrial projects. the data shows that git is widely used by developers (52%), followed by svn (20%). classification of open-ended questions."
"in git, the trend seems to be opposite. this suggests that git commits do not get larger in size when they reference several itl. rather, git commits could contain a change common to all referenced issues."
"while designing our survey we aimed at keeping it short. however, in doing so, some of the participants may have misunderstood our questions. for example, when we asked the question \"do you squash your commits?\" we were aiming to find if developers are using the squash command from git or similar tools. this command collapses together commits after they were committed. however, respondents might have interpreted the question as squashing multiple changes before committing. this can explain why 18% of developers using cvcs reported that they use squashing, even though this is not possible in cvcs tools. while we did run a pilot [cit] of our survey, there is always the possibility that we have miscommunicated our intent."
"the changes that a developer makes might belong to one or more logical units of change. do developers split these changes and commit separately? or do they just group everything and generate one large commit? the answers in the survey give us the picture depicted in table 4 . one explanation for this fact is that in dvcs, the commit process is easier and cheaper than in centralized ones. there is no risk of conflict with each new local commit. moreover, the smallest atomic unit of change in dvcs is the line, not the file (as it is in cvcs). all these make committing easier, so developers are willing to take the time to split and commit each logical change separately. in a recent study conducted in parallel with ours, [cit] have also discovered that the ability to commit locally and independently allows developers to work incrementally."
"evoked extracellular lfps generated throughout the layers of the barrel cortex were recorded and analyzed. in these experiments, recordings were performed at 100-lm resolution by means of conventional ag/agcl electrodes inserted into borosilicate glass micropipettes (gb150t-10, science products gmbh, hofheim, germany) with tip diameter $ 1 lm and resistance $ 1 mx, starting from layer ii (320 lm) down to layer va (920 lm). the micropipette was inserted into a holder connected to a patchstar micromanipulator (scientifica, east sussex, uk) for fine control of the electrode during the experiment."
"the algorithm first identifies the response onset as the starting point of the evoked response. for the accurate identification of response onset, each point from the stimulus onset was checked for a threshold crossing. the standard deviation of the signal's steady-state part (i.e., the signal part before stimulus onset) was taken as threshold. signals of 10-ms duration after the stimulus onset were divided into small parts of 0.5 ms, and derivatives of those parts were obtained. the response onset was identified as the time instance of the signal when the small parts derivative was found to exceed aethreshold value."
"to facilitate the usage of ltdb, the following matlab code is provided, under the gpl v3 open source licence, at http://www.ltdb.info/downloads/ or via git-hub at https://github.com/irb-ltdb/."
"for each commit we collect the following metrics. commit id, for identifying commits. commit date, for sorting commits chronologically. the author of the commit, for grouping by authors. number of loc changed by the commit, for determining the size of commits. for each commit we compute loc added, deleted, or modified as reported by the standard diff tool."
"we can see that commits to svn and hybrid repositories tend to be larger when more issues appear in the commit message. the correlation is strong and positive, at 0.68 for svn and 0.81 for hybrid repositories. for git repositories this trend does not hold. there is a slight tendency for commit size to decrease when the number of issue tracking labels increases. there is a weak negative correlation, at -0.38). table 15 shows the detailed results. interpretation: the strong correlations for svn and hybrid repositories reinforce the idea that in these repositories developers tend to group different change intents (issues) together."
"to explore the statistical significance of various sample differences, we applied the wilcoxon rank-sum test. we chose this test since none of the data fit a normal distribution."
observations 3 and 5 show that developers using dvcs split their commits more often and that their commits contain finer scoped changes. this hints to the idea that they might carve out the common piece of code that contributes to solving both issues.
"the culture of the project takes a longer time to change when a new tool is introduced. thus, in long lasting projects, it seems that old habits die hard."
"continuing on the idea of metrics, the field of software design flaws can be applied to repositories as well. researchers have identified many software design flaws [cit] . marinescu [cit] presents detection strategies for these flaws, allowing tools to identify, report and offer suggestions for improvement. by following this approach researchers can devise design flaws for repositories and then metric based detection strategies for these flaws would allow tools to measure the health of a repository."
"the consensus tracking ground truth provided with ltdb includes 728 unique tracks composed of 44722 instantaneous annotations. on average, each track is composed by 61 annotations. this varying with the track duration. the total observation time included in ltdb amounts to the equivalent of 260 hours for a single cell. common tracking errors (i.e. cells not annotated by mistake, broken tracks or jumps in the z-axis) as well as conflicts produced by multiple operators were detected by executing the matlab script ltdbcheck.m provided in the code availability section."
dvcs commits contain more itl than cvcs. this suggests that dvcs repositories are better candidates for research projects studying links between commits and issue tracking systems.
"therefore, tool builders should create tools that (i) keep track of code written for a particular task, (ii) automatically group code changes by issue, and (iii) incorporate issue tracking inside version control systems."
"one of the reasons why developers do not put issue numbers into commits could be the extra work it involves. building upon the suggestion of section 13 for tool builders, there could be a better integration between vcs and issue tracking systems."
no image processing was applied to the provided videos. raw images were also used for manual tracking. cropping of large 4d volumes in space and/or time was performed for the entries of the case study collection to focus on the area of interest.
"reliability: can others replicate our results? the list of repositories we used for our analysis is available online [cit] . also, the infrastructure we used for the analysis is available open source as a github repository [cit] ."
"the sources for our repositories are github and sourceforge. this means that we only looked at projects that used git or svn. we did not study other vcs tools for the distributed or the centralized paradigm. however, as the data from our survey indicates, git and svn are the predominant systems used today. they are the most widely used in their class, thus we think they are representative."
"imaging data were captured from organs of living animals using either the splenic or the popliteal lymph node surgical models (fig. 1a and table 4 ) which are typical for mp-ivm investigations of the immune system 2 . cells involved in both innate and adaptive responses were included in the dataset. videos 12, 13, 14 (data citation 1) come from recently published mp-ivm studies 5, 7, 21 . to represent data generated by multiple laboratories in different experimental settings 22, ltdb includes videos with different size, resolution, sampling rate and challenges for the automatic analysis (table 2 (available online only)), acquired by three different microscopy platforms (table 5) . moreover, cells were labelled with different fluorescent tags and detected by multiple channels (table 3) ."
"in this work, we present an automated and efficient tool to characterize single-trial lfps recorded in the barrel cortex upon whisker deflection. the characterization is mainly done in two passes: (1) in the first pass, the individual signal files are preprocessed (e.g., scanned for the presence of any stimulus artifacts and, if found, removed automatically); (2) in the second pass, the evoked responses are identified and characterized (e.g., by detecting important shape features and calculating relevant parameters of the responses). the identified features and estimated parameters include: the response onset (start of the evoked response), the response onset latency (time delay between the stimulus onset and the start of the response), the response peak (apex of the response), the response peak latency (time delay between the stimulus onset and the apex of the response), the event amplitude at the response peak and the positive rebound (the apex of the positive rebound after the response peak). at the end, this information is automatically written in an american standard code for information interchange (ascii)-coded text file, and necessary figures are generated and reported to the user."
"where dv min and dv min are minimum and maximum amplitudes of the response peak identified from a single cortical depth for each rat. figures 9, 10, 11, 12, 13 show the estimated parameters and depict the variability of single-trial lfps caused by the different types of anesthesia."
"images resulting from mp-ivm are contained in two zip archives with name tiffs_ltdb001_ltdb020.zip for the videos with id 001 to 020, and in tiffs_cs001_cs018.zip for the videos in the case study collection with id 001 to 018. in these archives, a folder for each video contains 4d images as tiff files."
"for github we selected the top ranked repositories, i.e., repositories that have been marked as favorites by developers and/or have been forked the most. for sourceforge we used its own internal ranking metric to select the top ranked repositories. we queried the sourceforge projects through the notre dame sourceforge research archive [cit], which serves as a mirror designed specifically for researchers. by choosing the top repositories we ensure that we collect mature projects with a rich history."
"artifact removal (ar) once the information on the number of artifacts and their probable end points were determined, this module was implemented to determine the exact start and end of those artifacts. to calculate the exact area covered by the artifacts, the probable end point received from ad subfunction was taken as reference. the signal was then scanned backward from the reference point, and the local minimum was computed. signal derivatives were calculated for the signal portion starting from the detected local minimum until the stimulus onset. in this portion where the signal derivative approached zero was taken as the starting point of the artifact. for the exact end of the artifact it was sufficient to detect the point nearest in value to the starting point. thus, to calculate the end point: from the reference point the signal was scanned forward, the difference in derivative between the starting point and current point was calculated, and the process continued until this difference crosses a predefined error value (defined as 10 % of the signal standard deviation). once the starting and ending points were detected, interpolation between them was performed, and the artifact area was replaced with the interpolated points. the pseudocode is listed in algorithm 2."
"however, under urethane, the evoked single-trial lfp responses showed different behavior: the distributions of rol (figs. 9 and 11a, in black) and rpl (figs. 10 and 11b, in black) were slightly narrower in the deeper layers than those in the superficial ones, with the widest distribution in correspondence to layer iii. this feature is likely to be associated with different streams of information along the cortical column (modulated by the type and the level of anesthesia) and due to inter-layer connectivity in the barrel network. the distributions of the rpl were narrower, and the rp of the evoked lfps occurred earlier in the case of urethane than the ones recorded under til-xyl anesthesia. this was confirmed by comparing the mean values of the rol (fig. 11a ) and the rpl (fig. 11b) ."
"in the context of big-data analysis, (fig. 3c) ltdb represents a resource to compare the biological properties of tracks (i.e. speed, directionality) amongst different experimental conditions. a review of the possible measures that could be computed from the tracks is provided in 10 . the sql database ltdb.sql can be installed optionally and for instance using the mysql database management system. queries to retrieve videos of interest (i.e. associated to a specific challenge, type of cell or site of imaging) can be addressed to the locally installed database. additionally, a web interface was set up to facilitate search, preview and download of videos and it is accessible at http://www.ltdb.info/"
"in this paper we present the first large-scale study that answers in-depth questions about the extent to which dvcs influences the practice of managing changes. to this end, we designed and launched a survey. we recruited 820 participants, 85% of them being developers from industry. 56% have ten or more years of programming experience. 51% work in teams larger than 6 developers."
"changes are more granular in dvcses and usually have only one issue reference. vcs tool builders could include new abstractions that represent features. for example, cherry picking could be done at feature level."
"to get further insights into how dvcs affects code changes, also we analyzed 409m lines of code changes from 358300 commits, made by 5890 developers, in 132 repositories containing a total of 73m loc. our corpus contains both pure and hybrid repositories. pure repositories use the same vcs throughout their lifecycle. hybrid repositories started in the centralized paradigm and switched to the distributed paradigm. the hybrid repositories can reveal if changing the version control system influences developers' practices."
"to assess the overall performances of a cell tracking algorithm, we direct the user of ltdb towards the entries ltdb001 to ltdb020. to test the behaviour of an algorithm on specific cases instead, we recommend the user with the videos in the case study collection cs001 to cs018 that facilitates manual investigation and debugging having a reduced number of cells."
"using the survey results, we selected svn and git as being representative for the centralized and distributed categories, respectively. we collected svn repositories from sourceforge as and git repositories from github. these repositories span several programming languages: java, c, c++, javascript, and python."
"dvcs allow users to change history before they make it public or available to others. one participant stated he squashed commits because he \"committed more often locally while working. that need not be seen in the final push, because it usually only adds noise\". this is a threat when mining repositories. the repository that is publicly available might not be the one that developers had when they committed their changes. squashing is just one of the ways in which developers can change history. research on such repositories should take this threat into account."
"another question is whether developers working in larger teams squash more frequently than developers working in smaller teams. table 13 shows that this could be the case. while teams of two to five developers squash only 27% of the time, teams with over 100 developers squash 57% of the time. this is more than double the rate compared to smaller teams. the reasons for this are twofold:"
"there is a lot of noise when studying different types of software changes introduced by commits. as seen in section 2.2.2, there are many types of commits and individual changes that do not constitute acts of development. researchers should clearly define what types of changes they are studying and then take the appropriate actions to filter undesired commits. by not paying attention to different types of commits, researchers risk biasing or confounding their results."
"for the centralized paradigm we chose svn as the best representative. for the distributed paradigm we chose git. using the data from our survey and our mining of repositories, we answer 12 research questions organized in three overarching themes:"
"prior to imaging, mice were anesthetized with a cocktail of ketamine (100 mg/kg) and xylazine (10 mg/kg) as previously described 4 . all animals were maintained in specific pathogen-free facilities at the institute for research in biomedicine (bellinzona, ch), theodor kocher institute (bern, ch) and massachusetts general hospital (boston, ma). all the experiments were performed according with the rules and regulations of the local authorities and approved by the institutional animal committees: swiss federal veterinary office, research animal care of the massachusetts general hospital, mgh institutional animal care and use committee (iacuc)."
ltdbcheck.m and estimatedsmeasures.m make use of the following libraries: imarisreader (https://github.com/peterbeemiller/imarisreader) to read imaris files and bwdistsc 20 to efficiently estimate the distance of each voxel from the closest centroid.
"the feature detection algorithm performed accurate detection of the important features defining the evoked responses in the single-trial lfps. figure 7 illustrates an example of the algorithm's single run on a representative dataset. from 500 single-trial lfps recorded and analyzed for each cortical depth, one trace per depth was randomly selected for demonstration purpose. it is also shown in fig. 7 that the features defining the evoked responses in the single-trial lfps were detected precisely by the feature detection algorithm. figure 8 highlights the accuracy of the feature detection in the single-trial lfp evoked responses under two anesthetics (a mixture of til-xyl, and urethane). it can be seen that the responses recorded under tiletamine-based anesthetic were more similar to the usual evoked response's signature ( $ 99:06 % of recorded single-trial lfps were detected) in comparison with the ones under urethanebased anesthetic ( $ 93:69 % of recorded single-trial lfps were detected)."
"experiments were performed by four research groups using three customized two-photon microscopy platforms (table 5 ). either the splenic or the lymph-node surgical models were used for acquisition (fig. 1a) . videos were acquired from 26 unique experiments, to observe the interplay of neutrophils, b cells, t cells and natural killer cells in innate or adaptive immune responses (table 4) . data pre-processing"
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org."
"whisking is a dominant contributor of sensory information in rodents. through whisking, rodents are capable of localizing objects, detecting textures and differentiating shapes very precisely [cit] . whiskers in the rodent snout are represented in a cortical region called 'barrel cortex' following a simple one-to-one topographic mapping where single columns correspond to single whiskers. as such, the somatosensory pathway related to whisking is a valuable prototype system to study information processing of sensory inputs in rodents [cit] . yet, understanding how neuronal networks in the barrel cortex represent information on whisker deflection remains a largely unmet challenge [cit] . intracellular single neuron recording (e.g., patch clamp) can only partially contribute to our understanding of the network and must therefore be complemented by experimental strategies to investigate neuronal populations. thus, among other available techniques, the recording of population local field potentials (lfps) is of primary importance [cit] . while it is recognized that network function can be in part inferred from lfps, not much has been done for developing software tools for their automated analysis, which is still mainly delegated to manual operation [cit] ."
"number of issues referenced in the commit message, to determine the cohesiveness of changes. the issues refer to programming tasks, such as features or bugs, managed with external systems such as bugzilla, jira [cit], etc. in order to detect them, we used an approach similar to the one described by bird at al. [cit], which employs searching for specific text patterns in the commit message."
"by splitting changes into multiple and smaller commits developers can cherry-pick changes. cherry-picking refers to the operation of selecting one commit from a branch and applying it to another one. this way, developers can migrate changes from one branch to the other without the need to merge all changes. this has maximum benefits when commits carry only one intent, as noted by one respondent who splits his commits because of \"the ability to easily cherry pick or revert [commits]\"."
"for researchers: we found that the average commit size slightly decreases over time. this could be explained by a variation of change practices during software development stages (development, testing, support, etc). a more detailed investigation on how software development stages influence change practices is needed."
"construct: are we asking the right questions? we are interested in assessing the state of the practice for version control systems usage. thus, we think that our research questions have high potential to provide a unique insight and value for different stakeholders: developers, researchers, tool builders and team management."
"number of files impacted by the commit, for determining the commit size. while loc tells us how much software editing has been performed in a commit, the number of impacted files tells us how spread the change is within the system."
"we are answering this question using two data sources, the survey and the repository analysis. table 11 summarizes results collected from the survey. overall, 93.87% of the participants reported using an its. while the value is lower for one-person projects (63%), it is over 90% in all other cases. more interestingly, all participants working in projects with over 100 developers report using an its. the analysis of the repositories shows that 91.6% of the projects contain commit messages that refer to issues in an its. this correlates with the survey. we measured the correlation coefficient between team size and commit size (measured in lines of code and number of changed files). table 12 shows no linear relation between team size and the size of commits. this holds for both loc and number of files. the fact that all but one coefficient are negative could indicate a weak downhill trend for commit size as team size increases. observation 12: there is no discernible relationship between the team size and the size of commits other than a weak tendency for commit size to decrease as team size increases."
"rq 5: does the vcs influence the frequency with which developers commit? table 9 shows results we obtained from the survey. developers commit several times a day regardless of the version control they use. the data for each vcs type shows a slightly different picture. developers using dvcs commit once an hour more often (19.66%) than developers using cvcs (4.10%). also, when using cvcs developers are more likely to commit once a day (14.75%) than when using dvcs (7.19%) interpretation: the fact that developers commit once an hour more often when using dvcs than when using cvcs suggests that they find it easier to commit. results from the previous research questions also lead to this conclusion. one interesting results is that 14.75% of developers using cvcs commit once a day. this suggest a pattern of committing once the work day is over."
"developers can remove mistakes and clean a project's history by squashing their commits. several respondents mentioned that they squash to \"to correct a previous commit\" or \"to make it easier for people reading the log to understand what's been changed\". however, we see in observation 6 that it is not widely used. this is because, sometimes, squashing leads to a loss of historical data. this information might be useful in the future when debugging or trying to understand the origin of some changes."
to answer our research questions we needed to collect repositories that are representative of the centralized and distributed paradigms. we also collected hybrid repositories that started in a centralized paradigm and switched to the distributed one. our assumption is that differences in metrics taken from these 3 kinds of repositories provide valuable insights on how they influence source code management.
"providing the scientific community with datasets interpreted by experts is essential to foster the development of data science methods. to this end, international cell tracking challenges on public datasets 14, 15 allowed to highlight the properties amongst different algorithms. however, the provided datasets did not include leukocytes observed by intravital imaging. for this reason, it is necessary to develop an extended dataset of mp-ivm videos, where a significant number of leukocytes are tracked. here we present a leukocyte tracking database, namely \"ltdb\", that includes mp-ivm videos of immune cells, together with their relative tracks which were manually annotated by experts. each video contains one or more challenges for the automatic analysis (table 2 (available online only)), and captured the behaviour of one or more cell populations (table 3) in response to different stimuli (table 4) ."
where the functions time() and amplitude() denote the instantaneous time and amplitude of the signal at a given point. the detected features and estimated parameters were saved in a file for further processing.
"hybrid repositories on the other hand do not seem to experience smaller commits after switching to git, as observation 2 shows. our assumption is that in such cases a certain commit policy is formed within the team while the project is under svn. this commit policy is then intuitively carried over after switching to git, leading to the same observed commit size."
"from observation 7 we learn that developers like dvcs because of some of their killer features. one that was mentioned often was the ability to commit locally: \"you get to commit to a local repository and make your changes public only when they are ready\". learning how to use these features takes time and effort. using the same tools allows developers to keep their level of productivity in the short run. however, the initial effort and loss of productivity caused by learning a new version control system or paradigm may pay off in the long run. one participant reported that he \"tried git but its too similar yet just different enough to confuse the hell out of me and slow us all down\". another \"[...] was not happy about this [using git] to start off with, and it took me about two years to learn and love git\". the advantages of switching would be overall increased productivity, compared to using a cvcs, and better history and management of software changes."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. also, our own survey of 820 developers shows that 65% use dvcs and 35% use cvcs."
"tool builders should abstract away low level vcs concepts in order to ease the learning curve of version control systems. instead of using implementation level details (branch, rebase, hash, etc) as their interface to the user, vcses could use a high level vocabulary from the domain of software processes (feature, bug, review, etc.)."
"for team management: very large teams that struggle with aggregating changes should consider investing the extra effort and switch to distributed tools (git, hg, etc) and more involved branching models (specialized branches, deeper forking tree, etc) [cit] ."
"in order to investigate how the commit size varies in time, we averaged the commit size for monthly intervals. we then calculated correlation coefficients for the monthly values of average commit size. table 16 shows that the commit size tends to become smaller as projects get older. the average age of a typical repository (time between the first and the last commit) is 55 months. for svn repositories it is 54 months, for git repositories it is 30 months and for hybrid ones it is 94 months. the average commit size usually decreases by approximately 15-20% during the lifetime of a repository. overall correlation between commit size and time of commit for all types of repository is usually negligible (-0.11) and in most cases appears to be negative."
"interpretation: we expected to see that commit size decreases as team size increases. small teams can be very agile and quickly grow the code base because everybody knows what everybody else is doing. large teams have less knowledge of the overall task distribution, and they must exercise more care when accepting and integrating changes from many sources. therefore, we assumed, large teams would perform smaller commits to better express changes."
"for tool builders: as long as changes tend to become smaller as projects mature, it becomes worthwhile to rebuild smaller parts of applications as changes occur. thus, we encourage tool builders to provide more support for intra file incremental builds."
"the evoked response evoked responses from the barrel cortex follow a specific laminal morphology which has been reported earlier [cit] . usually in upper cortical layers (i, ii) the response rarely has a small positive peak (sp) followed by the main negative peak, i.e., the response peak (rp) and then a positive rebound (pr). in the midlayers (iii, iv and v) the sp disappears and the signals start with the rp followed by the pr (see fig. 5 ). in deep cortical layers (vi), the main rp becomes smaller and usually gets divided into two smaller negative peaks followed by the pr. these features of the signals can be exploited to automatically detect the evoked responses in the recorded signals. overall, heuristically, the evoked responses are characterized by three features (see fig. 5 ): the start of the response (response onset or feature 1), the rp (feature 2) and the pr (feature 3). figure 5 shows an annotated example of a single-trial lfp response recorded from layer iv (720 lm) where the features are highlighted [cit] . it should be noted that in cases where the sp was detected, the response onset or feature 1 was considered as the maxima of sp. in the remaining text, the terms feature 1 and response onset are used interchangeably."
"the survey provided valuable information on why developers prefer one paradigm versus the other. dvcs are preferred because of killer features, such as the ability of committing locally. in contrast cvcs are preferred for their ease of use and faster learning curve."
"as shown in algorithm 3 each single-trial lfp was lowpass-filtered (250 hz, butterworth low-pass filter) and translated (on the y-axis) by setting the signal amplitude at the stimulus onset to zero. this translation helped to avoid the slow deviation of signal that might obscure the real amplitude of the features. then the second derivative of the signal was obtained to facilitate the detection of features representing the evoked response by evaluating the signal shape change in the response area. large fluctuations in the derivative were identified, which correspond to the signal shape change and thus were used in detecting the features."
"the survey contained both multiple choice and open-ended questions 1 . we hand-coded the answers to the open-ended questions using qualitative thematic coding [cit] . we developed a set of codes that we validated by achieving an inter-rater agreement of over 80% for 20% of the data. two coders, the first and the third authors, developed the categories which were not known apriori. for measuring the agreement we used the jaccard coefficient."
"the artifact removal algorithm was divided into two main subfunctions: artifact detection (ad) and artifact removal (ar). the ad subfunction scanned each signal for occurrence of artifacts and, if found, marked the probable end of the detected artifacts. the ar subfunction calculated the artifacts' exact initiation and end points, removed the artifacts and interpolated the artifact region. once the artifact was removed, the clean signal (see fig. 6 ) was stored for further processing and analysis. the artifact removal process is shown as a flowchart in fig. 4 ."
"we report that the automatic feature detection and parameter estimation yielded similar results in comparison with manual calculation performed by hand. finally, as an example of the tool's usability, we report a preliminary characterization of single-trial lfps variability depending on two different types of anesthesia."
"respondents identified features as an important factor for using dvcs. some mentioned that certain features were an integral part of their workflow. paying attention to these workflows and creating the tools to support them will pay off in the future. the payoff will increase productivity on the developers side, and bring a larger user base on the tool builder's side, since developers will prefer a tool that best fits their work style."
"one other threat is the possibility of age bias in our repositories. since svn has been available for a longer period of time, svn repositories might contain older, more mature projects than git repositories."
"implementation details refer to how was a change carried out (e.g., change field type, add new branch to a switch statement, etc). intent of change splits changes by expressing the what part of the change carried out (e.g., add a feature, fix a bug). policy splits changes based on a criteria that is externally imposed (management practices, development process, etc). other represent reasons that do not fit in the above criteria. table 5 tabulates the reasons for splitting commits. we observe that in the case of dvcs, developers split their changes based on implementation details more frequently than they do in cvcs. this will inevitably result in more commits. as is the case with observation 3, we can attribute this to an easier commit process. as we have seen in observations 3 and 5, dvcs users split their changes in several commits more often and they do it with a finer-grained scope in mind. one participant reported: \"each commit is one cohesive change that might fix a bug, add new functionality, alter existing functionality ([...] like \"sphere class can now calculate its own volume\" -user level features usually take many commits)\". this corroborates with the findings about the influence of version control systems on commit size (rq 1). being able to more easily split the commits and the commit process being simpler as well, will result in smaller commit size."
"while we analyzed 132 repositories from the open source community, we cannot guarantee that these results will be the same for proprietary (closed source) software. however, given the overall agreement between the survey, which was filled in mostly by developers working with proprietary software, and the data we acquired by analyzing the repositories, we can assume that the general trends that we found will be true for proprietary software as well."
"the expected use case scenario of ltdb is the evaluation of results produced by a cell tracking algorithm (fig. 3a) . considering a generic cell tracking algorithm as an input-output system that reads an image sequence and outputs the tracks, ltdb can be used both as a source of images and as a ground truth for comparing the output."
researchers must be careful when collecting software repository related metrics. we have found that old repositories that migrated through several vcs tools present a different behavior than pure repositories. it may be the case that the culture formed in the era of the first vcs shadows the subsequent ones. there might other phenomena that influence a repository's structure. by not paying attention to different phenomena that affect repositories researchers risk biasing or confounding their results.
"commit size tends to decrease more in git then it does in svn. 71% of the analysed git repositories show decreasing trends in average commit size over time. for svn only 60% showed this trend, while for hybrid repositories this number constitutes only 57%. § ¦ ¤ ¥ observation 17: commit size tends to become smaller as projects get older."
"one participant stated that \"git promotes the idea that your commit space is not inflicting pain on anyone else, so frequent commit and experimentation is encouraged. by design it promotes small, frequent commits that serve a specific purpose rather than the '5pm commit.\"' resolving conflicts becomes a task that is consciously entered into when deciding to synchronize changes with other team members. it is not something that must happen with every commit."
"p30 à p50 rats were anesthetized with a mixture of tiletamine and xylazine (til-xyl) or urethane. the anesthesia level was monitored throughout the experiment by checking reflexes, respiration and the absence of whiskers' spontaneous movements. additional doses of anesthetics were administered to maintain a constant anesthesia level. during the surgery and recording section, animals were kept on a stereotaxic apparatus under a stereomicroscope and fixed by teeth and ear bars. the body temperature was monitored with a rectal probe and maintained at 37 c fig. 1 acquisition of evoked lfp responses from rat barrel cortex (s1) upon whisker deflection. the arrow on the hypodermic needle, attached to the piezoelectric bender, shows the direction of its movement. the stimulus is shown at the bottom which is used in driving the piezoelectric bender for dorsoventral movement of the whisker inserted in the needle. the indicated 10-v trigger impulse produced by the waveform generator was translated to 60 v and applied to the piezoelectric bender for the required amount of displacement using a heating pad. an incision was made along the medial line of the head, starting from the eyeline and ending at the neck. thus, the skull over the right hemisphere was drilled to open a window in correspondence of the somatosensory cortex, s1 (à1 to à4 ap, þ4 to þ8 ml) [cit] . meninges were then carefully cut by means of forceps at coordinates à2:5 ap, þ6 lm for the insertion of the recording micropipette. throughout all surgical operations and recordings, the brain was bathed by standard krebs solution kept at 37 c."
"recent advances in intravital video microscopy have allowed the visualization of leukocyte behavior in vivo, revealing unprecedented spatiotemporal dynamics of immune cell interaction. however, state-of-the-art software and methods for automatically measuring cell migration exhibit limitations in tracking the position of leukocytes over time. challenges arise both from the complex migration patterns of these cells and from the experimental artifacts introduced during image acquisition. additionally, the development of novel tracking tools is hampered by the lack of a sound ground truth for algorithm validation and benchmarking. therefore, the objective of this work was to create a database, namely ltdb, with a significant number of manually tracked leukocytes. broad experimental conditions, sites of imaging, types of immune cells and challenging case studies were included to foster the development of robust computer vision techniques for imaging-based immunological research. lastly, ltdb represents a step towards the unravelling of biological mechanisms by video data mining in systems biology."
"ltdbexamplequery.m provides an example for querying the locally installed database. xtltdbimporttracks.m imports tracks as spots in imaris. ltdb2c2c.m exports ltdb tracks in the format used for the cell tracking challenge described in 14, 15, mainly for evaluation with the methods proposed in 15 . ltdbcheck.m checks for common tracking errors (i.e. annotations deleted by mistakes, broken tracks and overlapping tracks)."
"the lfp responses evoked after whisker deflection were acquired by the recording electrode with reference to a ground electrode (see fig. 1 ). both electrodes were connected to an extracellular preamplifier or head stage (sec-ext, npi electronic gmbh, tamm, germany) having 10x gain and high-pass filter with a corner frequency of 1 hz. the preamplified waveforms were then 10x amplified and low-pass-filtered with a cutoff frequency of 33 khz using an amplifier (sec-10l, npi electronic gmbh, tamm, germany) operating in bridge mode. the amplified raw signals were stored by an acquisition pc at 50-khz sampling rate using ni pci-6071e data acquisition board (national instruments inc, texas, usa) [cit] terminal block (national instruments inc, texas, usa). at each cortical depth 500 single-trial lfp responses were recorded in response to the mechanical deflection of the principal whisker. a temporal delay of 2 s was set between subsequent traces to avoid any phenomenon related to adaptation. a depth profile resulting from an experiment is shown in fig. 2 ."
"we have built an analysis platform to gather several commit metrics. we used git as the canonical representation for all repositories. this is possible since the git object storage model is a superset of the centralized model. for example, the linear history of cvcs can be easily represented in git's directed acyclic graph branching model. thus, we converted all svn repositories to git, using the svn2git tool [cit] ."
"to select hybrid repositories, we searched for internet posts about migrating repositories from svn to git. in addition, while collecting git repositories, some of them proved to have actually started in svn. thus we classified them as hybrid. we distinguish the two stages of hybrid repositories as hybridsv n stage and hybridgitstage."
"we took extra care to ensure the integrity of repositories, i.e., git repositories did not originate in svn, by searching for keywords in commit messages. table 2 shows the corpus of repositories. for each repository kind, we tabulate the number of individual repositories, commits, and authors that contributed. the last column shows the total number of lines of code that have been changed by all commits."
"based on these findings, we propose several actionable implications for four audiences. researchers can better align their research questions with the type of repositories they mine. for example, for questions that rely on a discrete and precise software changes (e.g., bug prediction etc.) they should mine distributed repositories. developers can give more precise meaning to their changes when they use dvcs. tool builders can further build up on the strengths provided by dvcs such as the ability to better group changes and express their intent. managers can make more informed decisions when choosing tools for their projects. this paper makes the following contributions: 1. research questions. we designed and answered 12 novel research questions to understand the extent in which dvcs help developers manage software changes. 2. survey. we designed and launched a survey to provide insights into the practice of using dvcs. we recruited 820 participants. 3. mining repositories. we developed tools to collect metrics and analyze centralized and distributed repositories. we applied these tools on 132 repositories. 4. implications. we present implications of our findings from the perspective of four audiences: researchers, developers, tool builders, and team managers. the tools, summary of survey responses, and corpus are publicly available at:"
"the overall low number of commits with itl can be attributed to a relaxed commit policy. as the repositories are gathered from open source projects, it may be difficult to enforce a strict commit policy. to the best of our knowledge, none of the analyzed repositories enforced a practice of mandatory itl inclusion in commit messages."
in this paper we present the first in-depth study to measure the impact of dvcs on software change. to this end we ran a survey with 820 participants and analyzed a corpus of 132 repositories.
"a small number of commits are labelled with itl. nevertheless, issue tracking labels appear more frequently in dvcs commit messages than in cvcs commit messages."
"this could affect metrics that rely on team size for repositories analysis. even though we have encountered few cases of aliases usage while analyzing repository data, we also noticed that it is a very rare and exceptional practice."
"although ltdb was provided to primarily enhance tracking algorithms, the database embeds biomedical knowledge. to this end, data-mining and image-based systems biology methods can be applied to correlate images, tracks and metadata for investigating properties of the immune system in health and disease (fig. 4c )."
"for researchers: researchers mining software repositories and studying discrete changes should focus on dvcs because they allow smaller atomic units of change. for refactoring researchers, the smaller git commits could better define individual changes. for researchers who tie different software artifacts to code, such as bugs, git commits are more precise therefore they may have fewer false mappings."
"distributed version control systems (dvcs) like git [cit] or mercurial [cit] are widely used today. over the last couple of years github [cit], which is the most popular repository hosting service for git projects, has taken the open source community by storm [cit], github hosted over 4.6m repositories. compare this with the previous paradigm, cvcs, epitomized by svn [cit] and cvs [cit] . sourceforge [cit], the primary repository hosting service for svn had about 300k [cit] ."
"in this case, both grids exist, and the grid-interfacing converters has to synchronize with the grid. the control of the hv side chb converter is focused on balancing the cells capacitors (i.e. controlling v cell) while the control of the lv dc/ac 3p4l converter is dedicated to maintaining the lv dc link capacitor at the target value (i.e. controlling vdclv). finally, the dab dc/dc converter control is responsible of controlling the power transfer between the two ports given that the voltage at both dc links is controlled at the required constant voltage."
"in summary, ed/svd (or pca) and spcp separate dynamic blood flow from static structure by assuming that the static signals between the repeated frames is highly correlated. ed/ svd employ linear-regression filter and thus, is sensitive to outliers (tail-noise and motion artifacts). while spcp is robust against outliers, this technique seems to enhance unwanted results albeit small in intensity. lastly, rpca (or spcp in specific) is still computationally expensive, however, reader should keep in mind that rpca is still under rapid development for efficiency and robustness at the time of writing. zoom-in visualization of the locations marked as \"1\", \"2\" and \"3\" in (a) respectively. octa, optical coherence tomography-angiography;"
"in order to compare between pca and rpca, all of our resultant data matrices were normalized in log scale, using log1p function of matlab. the top-down, or x-y plane, projection was a maximum intensity projection with the depth (of pixel with maximum intensity) color-coded."
"perhaps too sensitive to the changes in signal between the repeated frames, spcp introduces significantly more speckles artifacts, which are not 'true flow'. in the example of nail fold, this unwanted result resembles desmosomes of spinous cell, and is questionable to whether this observation could be useful additional information."
"what specially distinguish the energization and start-up of the modular three-stage ssts from other sst structures is the isolation requirements. it not only requires isolation between its two ports but also between series stacked modules/cells at the hv side ac/dc converter stage which can be several kv over the lv side reference [cit] . another important fact for practical sst application is the possibility of starting-up the sst with both grids available or with just one of them (i.e. grid-forming at one port) independent of the start-up port. in other words, depending on the availability of the grid, the sst has to be able to startup from the hv side or from the lv side without any compromise in the behavior."
"the auxiliary variablesp andq are introduced to the network side, and the auxiliary variablesp andq are introduced to the pv power plant side. then, the opf model can be rearranged as"
"to solve the non-convex problem of the opf model under the distribution network, the original model was transformed into a convex form based on the branch flow equation and on piecewise linearization and second-order cone relaxation [cit] . the drawbacks of such schemes are that it requires additional computing resources to satisfy constraints accurately when there are large load variations and complex matrix computations. authors also applied semi-definite convex relaxation to the optimal scheduling of pv inverters and verified its effectiveness in small community rooftop pv systems [cit] . however, such schemes are not suitable for high-voltage distribution and complex pv systems."
"the limitation of this work is that it is not deployed on other scenarios having dynamic conditions. a distributed state estimation and control method considering packet losses is proposed by developing a distributed consensus estimator based on the mean square error [cit] . the drawback of this method is that it does not address different observation matrices and no algorithm is used for state estimation. a similar work adaptively minimizes the estimation error covariance matrix of the local estimators [cit] . however, it does not consider the linearization error of convergence analysis and the delay and packet losses between the microgrid and state estimator. a global state estimation scheme which uses the locally computed state estimates, covariance matrices, and corresponding weighting factors are also proposed [cit] . the computational complexity is reduced by deploying the trace of the estimated local covariance matrices. however, it does not address the channel fading effects on the system performance and no channel encoding scheme is considered."
"result compared with the results of traditional optimization design and conventional design is shown in table ii . ix. conclusion optimization design software for straight bevel gear is developed in the paper to achieve the automatic optimization by using vb and matlab, in which the genetic algorithm is selected and the augmented penalty function and integer serial number encoding is used, so the global optimal solution can be obtained, design efficiency greatly improved, weight of bevel gear reduced, the production cycle shortened."
the objective function combines the linear equations (2) and (3) of the node voltages and aims to minimize the sum of the network active power loss costs and the pv power plant's disposal penalty fee by encouraging the absorption of pv power generation as
"saddle-point dynamics (spd) transforms the process of optimization into the asymptotic stability process of dynamic systems from the perspective of dynamic system control. authors have applied spd to general communication networks [cit] and in solving the non-convex opf problem on the ieee 14-node system [cit] . however, spd does not guarantee globally optimal stability, it requires a large amount of information exchange between the units, and the convergence speed is slow. a discontinuous spd model for linear programming problems was established that confirmed that a dynamic system can converge to a globally optimal solution even under certain interference [cit] ."
"moreover, the obtained experimental waveforms (fig. 12 ) are compared to the simulation waveforms (fig. 6 ). the cell voltage in both cases is compared in fig. 13 . the noticeable agreement between the simulation and the experimental results validates the reliability of the developed simulation. this is a key point to the validation of the method on the full sst through simulation models."
"the above methods are centralized optimization methods: that is, the scheduling center collects and processes the global information before optimizing the solution and executes the centralized calculation to issue control commands to each controller. however, with the development of smart grids, more and more controllable devices are expected to be connected to the grid, making centralized collection and control challenges. in addition, when the controllable units participating in the optimized scheduling belong to different entities, collecting global information can also increase the risk of exposing private parameters. decentralized power optimization in smart grid systems has emerged as the solution to this problem, and it can also support the plug-and-play feature."
"step 3: after the network and each rooftop pv receive the new coupling variable value, they update the variables related to themselves in parallel as described in step 1. this process repeats until the trajectory of each variable is progressively stable to the optimal solution."
"in pca, the repeated a-scans form a data matrix that consists of n z (axial pixels) observations with n t (repeated frames) variables. our objective is to find a set of linear combinations of temporal frames (variables), and decompose this set into the principal components. in simple case, the first pca component (labelled as '1 st pc' in figure 1 ) reconstructs exactly the static signal, since the frame are strongly correlated across spatial/depth pixels (observations). blood flow and noise signal can then be reconstructed from the other components, i.e., second to n t -th principal components, as they are mostly uncorrelated between frames. an example of blood flow analysis using pca is shown in figure 1, where n z -pixel intensity between two repeated frames are plotted in log-scale for visual presentation. from our assumption about signal correlation across repeated frames, pixel containing static signal can be seen as data points adjacent to the first principal component (the 1 st pc); and pixel containing blood flow signal can be seen as data points with high variance on second principal component (the 2 nd pc). this analysis can also be generalized to higher dimensions, i.e., n t repeated frames."
"the basic idea of the proposed procedure is the synchronization of the energization/charging of the primary and secondary dc links, where the primary is the gridfeeding port and the secondary is the grid-forming port."
"to further verify the accuracy and feasibility of the proposed algorithm, dspd is applied to solve the 33-node distribution network, as shown in fig. 7 (the dotted line is the closed loop, and the circle indicates that the node has pv power plant access). among the 15 pv power plants, the predicted output of plant 14 (named after the plant's node number) is 950 kw, and the predicted output of plants 3, 7, 17, 23, and 31 is 50 kw. the predicted output of the remaining pv power plants is 220kw, and pv forecast output accounted for 85.6% of the total load. the remaining parameters are designed to be consistent with the rooftop pv system in section iv.b.1 fig . 13 shows the evolution of the coupling variables for the open loop and closed loop of the 33-node distribution network; it is noted that the coupling variables can gradually converge to a stable value in both network states. table 3 shows a comparison of the objective function results for the 33-node distribution network under the cplex and dspd algorithms. the findings in the table show that the objective functions of the two algorithms are completely consistent, and the convergence time of the closed-loop network is shorter than that of the radiated network. this is because the coupling relationship between the nodes in the power network is tighter in the closed-loop state, which is more conducive to the variable trajectory. under the radiation network, due to the certain abandoned light in the pv power stations of nodes 12, 14, and 17, it is beneficial to reduce the network loss. therefore, these pv power stations have different degrees of light abandonment."
"in this case, the lv grid, vaclv, is existing while the hv grid, vachv, is formed by the sst. the chb converter control is in charge of forming the hv grid while the balancing of the cell capacitor voltage is carried out by the dabs. each dab controls its vcell independently. on the other hand, the lv side converter controls vdclv."
"the aim of this section is, first, to clarify the challenges related to the energization and start-up of the selected sst topology based on the selected aps structure and second, to apply the proposed start-up technique step-by-step on all sst operation modes."
"in the standard straight bevel gear design, the independent design variables are three: m, z1, fr, where m and z1 are discrete, and fr is continuous. if fr is kept two fractions, it is changed into discrete variable. population size is an important parameter in genetic algorithm, which specifies how many individuals there are in each generation. with a large population size, the genetic algorithm searches the solution space more thoroughly, thereby reducing the chance that the algorithm will return a local minimum that is not a global minimum. however, a large population size also causes the algorithm to run more slowly. so it can not be too large or too small. the range of population size is 10-160. it is determined to be 50 by trying."
"the analysis starts with the complex signal, following the fast fourier transform (fft) of spectral signal from fourier-domain oct. in octa/omag imaging protocol, it is required to scan the same tissue location repeatedly over time (i.e., repeated a-scans, or m scan) in order to build a time series of a-scan oct signals, from which the signal fluctuation is extracted to represent the blood flow signal. thus, the resulting complex oct signal is a 2d matrix of a depth m-scan. since the oct probe beam is highly (randomly) scattered inside the blood vessels, blood flow signal can be extracted from the complex m-scan signal via an appropriate filter."
"first, to charge the primary dc link from the rectifier connected to the grid, a pre-charge circuit is used to limit the inrush current. two pre-charge circuits are essential at both ports of the sst (see fig. 2 ) to be able to start-up in any operation mode (see fig. 3 ). the value of the pre-charge resistors are selected based on the required time constant and the value of the dc link capacitor. the time constant is selected to be in the range of 600 ms. resistor values are consequently calculated to be 1 k for hv side and 450 for lv side."
"in addition to selecting the number of filtered principal components in ed/svd analysis, choosing the number of frames in an ensemble is also important. the larger number of frames in one ensemble would yield higher blood flow snr, providing that the tissue motion is not significant. a comparison between 8-frame ensemble and 5-frame ensemble, with the same filtered principal components, is shown in figure 4 ."
"separate into static and blood flow signal by rank: in this paper, we implement a technique in rpca, also called stable principal component pursuit (spcp) (27), in separating the static structure and dynamic blood flow oct signals. amongst other rpca techniques, spcp requires less processing power whilst providing adequate dynamic blood-flow signal filter. in addition, this technique also seeks for an explicit noise component within the rpca decomposition."
the theoretical method is validated using simulation results performed in simulink matlab® and applied to one module of the sst. the results are shown in fig. 6 .
"the scanning protocol is also an important factor in ed analysis, especially the parameters of the repeated (slow-time) scan. a higher number of repeated scans and faster repeated scan rate would improve the blood flow snr, assuming that the static signals amongst the scans are stationary. however, perfectly stationary sample rarely occurs and 'motion artifacts' are often introduced in ed/ svd analysis. the phenomenon of 'motion artifacts' can be seen as a significant reduced blood flow snr in one particular filtered b-frame image. equivalently, motion artifacts can be seen as higher intensity stripes across the projected x-y images (figure 4) . motion artifacts are caused by decorrelation of static signals between certain number of frames, which skew our ed/svd analysis. this sensitivity due to outliers (out-of-plane observation) is well known in ed/svd (pca) analysis (30) . in order to improve image quality in ed/svd analysis, the number of ensemble in slow-time axis (or number of repeated b-frames) should be as large as possible, but only up to certain limit that static signals start to de-correlates between the frames in the ensemble. this requirement dictates how scanning protocols is selected for optimal results in various cases. for example, sample under dynamic motion is typically scanned at 400 hz with 8 repeated frames; whilst stable sample could be scanned at 100 hz with 4 repeated frames."
"in this study, pca and rpca are performed on the same sets of oct complex data, which was acquired using a dedicated oct system. the first set of oct data was obtained by imaging a microfluidic-channel phantom. the second set of data was collected from human nail-fold tissue in vivo."
"different from transmission networks, the line resistance of low-and medium-voltage distribution networks is difficult to ignore. the traditional dc optimal power flow algorithm cannot be applied in distribution network research because opf must be based on ac to make the node voltage and branch circuit. variables such as power are considered."
"the control of the sst is implemented in a partial distributed structure [cit] . each sst module has an independent control unit, this is called the slave unit. slave units are responsible for measurement and control tasks within the module, an example is generating gate signals for the chb full-bridge, measuring input/output currents and voltages and performing the dab control. however, there is a master control unit where the top level control is implemented, dealing with balancing control of the chb, measurement of line voltages and currents, grid synchronization, etc. all slave control units of one phase are communicated together and to the master control unit using a ring communication protocol based on tosnet. each unit represents a node in the ring. this communication structure simplifies and reduces the optical fiber count [cit] ."
"the three main characteristics of the proposed method for energizing and starting up the addressed sst topology are the ability to mitigate undesired transients without any additional device or circuitry, validation for all operation modes and simplicity."
"iv. experimental results fig. 11 shows the developed full-scale sst module. as shown in the corresponding schematic in fig. 2, it is composed of the full bridge of the chb, which is developed using 1.7-kv non-commercial si igbts with a sic antiparallel diode from infineon. the dab bridges are developed using 1.2-kv commercial sic mosfets from rohm and two-channel commercial cree driver boards. the control slave unit is designed based on a xilinx spartan 3e fpga and including all ad converters and the required optical fibers for sending/receiving signals from all the module devices. the hft is designed for 24-kv of galvanic isolation based on a uu-core and a separate winding structure [cit] . the utilized apss are commercial 60-w sources. the front panel board is designed to show the activity of all important components inside one module. through the front panel it is possible to visualize any fault happening in the module as well as indicating the failing component. fig. 12 shows the experimental results when testing the proposed start-up procedure in the case of lv port operating in grid-feeding, and the hv port operating in grid-forming ( fig. 4 and fig. 5 ). the test is carried out on the sst module (fig. 11) at the target voltages. the results verifies the validity of the proposed start-up method."
"the sst can operate in many operation modes depending on the availability of the grid at each of its ports. if the grid is existing at a port, then this port is working in gridfeeding. on the other hand, if the grid does not exist at a port, the port is working in grid-forming and is required to create this grid. since the three-stage chb-based sst topology has two ports, three operation modes are possible. each operation mode imposes a distinctive control structure for each of the converter stages of the sst. each of these modes as well as the control function of each converter are explained in this section."
"bevel gear drive, characterized by changing direction, high coincidence and smooth transmission, etc., is widely used in the aerospace, automotive and large mechanical transmission system. so its design quality not only affect its own transmission performance, size and weight but also have some impact on the machine's performance. in practical engineering design, involving many parameters, consuming much calculation time, prone to error, and repeated calculation, query and drawing are needed for series of product design, resulting in substantial duplication of effort, so the development of bevel gear design software finding the optimal design is of great significance [cit] ."
"in gear design, the tooth shape coefficient y fa and stress correction coefficient y sa are the most complicated to determine, affected by the gear module, helix angle, equivalent number of teeth, variable coefficient, pressure angle, tooth root fillet radius and other factors. ref. [cit] gives the calculating formula in detail, in which a transcendental equation is needed to solve."
"based on spd, this paper proposes a decentralization method for solving the optimal power flow of high penetration in a distributed pv network. the accuracy and the effectiveness of the method in comparison with the centralized method are verified under two different scenarios. in summary, the proposed scheme has several important features. first, from the perspective of dynamic system control, it provides a new perspective for solving convex optimization problems. second, the linearized power flow equation used has high precision, and the constructed quadratic convex programming opf model can ensure that the algorithm is stable in the global optimal solution. third, the network unit and the pv unit only exchange node injection power information, and the pv units are completely confidential with respect to each other. finally, there is no need for global coordination, and the proposed scheme supports the plug-and-play feature. the proposed algorithm does have some limitations. the input has to be feasible under the dynamic reactive power limits and common coupling. as there are variations in practices, a sensitivity analysis could be carried out to assess the robustness of the proposed scheme."
"the most recent development in omag complex signal analysis relies on eigen decomposition (ed) (18, 19), which requires the formation of covariance matrix (xx t ) from 2d space-time complex matrix signal, x. since the formation of covariance matrix, xx t, can cause loss of precision [läuchli matrix as an example (20) ], singular value decomposition (svd) is employed as an alternative technique in dynamic signal filtering (21) . furthermore, traditional pca (including svd and ed) techniques are sensitive to outliers, i.e., the principal components can easily be skewed by a few corrupted frames, due to severe sample motion that causes decorrelation between frames, and by tail-noise effect where speckle intensity varies greatly between frames."
"this paper revisits current pca techniques, and implement spcp to improve the complex blood flow analysis. spcp decomposes the observation matrix x, an ensemble of repeated a-scan lines, into static (low-rank), dynamic blood flow (sparse) and noise. since the observations are often corrupted by noise, motion, and highly scattered speckles, adding a noise term seems to be a natural choice. the resulting blood flow matrix from spcp analysis shows less motion and tail-noise artefacts. while the examples illustrate benefits of spcp in comparison to ed/svd, the disadvantages of spcp should also be taken into account in order to avoid unwanted results."
"however, charging of secondary dc link is only possible after the energization of the corresponding aps (i.e. charging its feeding capacitor to the aps threshold voltage). consequently, total synchronization (i.e. pre-charging both dc links at the same time) is not possible [cit] . in this case, in order to avoid the inrush current resulting from the voltage difference applied on the transformer terminals, a soft starting technique is implemented as explained in detail in this section."
"in omag, the low-rank and sparse matrix can be thought of as static structural and dynamic blood flow signals, respectively. the analysis starts with a given noisycomplex data matrix x. spcp decompose x into a sum of low-rank matrix, l 0, and a sparse matrix, s, with an added noise term, z 0 :"
"in gear design process, involving a large number of graph and table data, how to invert artificial seeking into automatically calculation is a basic problem in the process of cad operations."
a simple step-by-step procedure to solve these challenges is presented in this work. the procedure is explained in details for all the sst operation modes. it is clearly seen that no additional circuitry nor complicated control is required.
"the main challenge in this sst topology is the high isolation between hv and lv side grids, which is seen at the module level. while the proposed distributed aps structure provides the optimum solution for modular sst designs, providing simpler and more robust auxiliary circuitry energization technique, it implicates a more challenging start-up. the complication comes from two facts. the first one is that the aps has a threshold input voltage that should be achieved before it can feed any circuitry, implicating sst connection to the grid without any feedback or monetarization from the modules. the second one is that in a grid-forming situation, the forming port of the sst would be energized through the feeding port. this situation clearly leads to an uncontrolled energization, which may mean transient events such as inrush current and voltage drops/sags. these transients can lead to the possible breakdown of devices or drivers, saturation of hft or switching off of the apss."
"according to sources, line graph can be divided into two kinds: one kind is represented by the parameters of the graph between which have calculation formula, just because the formula is so complex that drawn into graph to manually search; one is that the parameters of the graph can't be found calculation formula. for the first type, it's needed to find the original formula, and incorporated into the program; for the second, it's needed to discrete the graph into table, and treat it as table or seek fitting formulae by curve fitting method and incorporated into the program. fig. 6 shows the dynamic load coefficient kv [cit] . in the paper, each curve is respectively programmed by curve fitting. the programs are respectively named as kv5, kv6, kv7, kv8, kv9, kv10, kv11 and kv12."
"this operation mode takes place when the sst is supposed to establish the hvac grid voltage while fed from the lvac grid. fig. 4 shows the detailed step-by-step proposed procedure for this particular operation mode. since the grid is available at the lv side, it is obvious that the energization is done from this port. the blocks highlighted in blue are actions taking place at the lv side, while in red are those taking place at the hv side and in green are actions taking place simultaneously at both sides of the sst."
"in order to eliminate confusion in terminology, it should be made clear that author would use the term pca (achieved by ed, svd) and rpca (achieved by spcp) when discussing techniques in a broader sense."
"software design flow chart is shown as fig. 5 iv. mathematical model for optimization design optimization objects for straight tooth bevel gear are various. if user requirements needed, the consumption of material is the measurement of design. therefore the optimization object is the volume of frustum of cone of the bevel gear pair [cit] ."
"solid state transformers (sst) are the replacement for conventional line frequency transformers (lft). they are recently gaining research attention due to their wide range of functionalities in comparison to the lfts [cit] . this is due to the fact that these smart transformers are formed by power electronic converters functioning at a relatively high switching frequencies. this enable them to provide many advantages like dc low voltage (lv) ports, controlled bidirectional power flow, protections, fault ride-through, imbalances and harmonics compensation [cit] . these are appealing features for applications like smart grids to efficiently integrate distributed resources and storage systems. ssts also provide a relevantly higher power density, which is of special interest for applications where volume and weight are critical. good examples are traction, offshore and subsea applications [cit] . on the other hand, there is always the question of reliability and robustness of the sst in comparison to the lft and it is obvious that, in this aspect, the lft is superior. however, recent achievements in the field of semiconductor devices are opening a way to a possible change of this. the faster switching speeds and higher blocking voltages of semiconductor devices can allow a possible relevant enhancement to the sst reliability [cit] ."
"the independent design parameters of volume of straight bevel gear drive include big end module m, tooth number of pinion z1, face width coefficient φ r, so the design variables are."
"one of the widely used sst topologies is the modular three-stage topology where one stage is a modular ac/dc converter to interface with the medium/high voltage (mv/hv) grid, the second is a dc/dc converter and finally a dc/ac conventional lv converter to connect to the lv grid. many works in literature have studied this sst structure focusing on the different aspects of modelling, control techniques, efficiency improvements, protections and fault tolerance [cit] . however, few ideas are presented related to the energization and the start-up of this specific sst structure. in literature, this topic is mostly discussed for non-modular topologies. moreover, to the best of the authors' knowledge no experimental results have been provided dealing with the first stages of the sst connection to the grid, including issues like pre-charging dc links, energizing auxiliary electronics in each of the sst modules, dealing with the possibility of operating in grid forming or grid feeding at any of the sst ports, etc."
"various examples of different sst topologies are available in literature [cit] . the differences are mainly the number of stages and the modularity [cit] . three-stage topologies are preferable as they provide more flexibility, simpler control and commutation as well as optimization of the full sst by dividing it into three separate stages."
"in the decentralized opf model established in section ii.c, the objective function components c u and c c,l on the network side and the pv power plant side are quadratic convex functions, and their constraints are linear. therefore, the decentralized opf model is a convex quadratic model. in the constraint, except for the coupling constraints of equations (15) and (16), the variables on the network side and the variables on the pv plant side are completely independent; thus, the coefficient matrix a of the constraint is a sparse matrix. accordingly, the decentralized opf model can be simplified as follows:"
"taking together, there are undoubtedly many techniques to achieve the separation between static and blood flow signals in complex signal analysis, but we will only discuss the development of rpca for octa and compare rpca with traditional pca (which includes ed and svd) in this paper. although different techniques exist for different purposes in the field of omag, they all aim to recover a highly correlated signal (static or slow-moving signal) from uncorrelated signal (i.e., dynamic blood flow). pca and rpca analysis are discussed in each section below. a summary of each technique is shown in table 1 ."
"keeping in view the above state-of-the-art proposed methods and their limitations, this paper proposes a robust scheme based on the high-precision linearized tidal current model of the distribution network, spd is applied to a distributed pv network to solve the opf problem. the proposed decentralized opf method provides a novel approach, transforming the optimization problem into a stability process of dynamic systems expressed as a feedback control system. the proposed method is also theoretically proven to guarantee a stable global optimal solution. moreover, considering the distribution network and each pv inverter as independent control units, the proposed scheme can be realized with only a small number of information exchanges between the distributed control units. the main contributions of the paper are summarized as follows:"
"to verify that the proposed algorithm can still operate effectively with failures, pv12 is assumed to suddenly disconnects during operation. as can be seen by the trajectory change of the coupled variable as shown in fig. 14, even after pv12 disconnects from the network, the remaining variables quickly stabilize to the new optimal solution after only a short period of fluctuation. fig. 15 shows the comparison of pv network loss of the proposed dspd scheme with other existing methods. it can be observed that the results are the same before 10:15 and after 14:18. this is because the pv power output is less than the load power during this period. it is beneficial for the inverter to track the maximum active power of the pv power supply to reduce the light and reduce the network loss. abandoning the light slightly increases its penalty value, but it is more beneficial to reduce the network loss. moreover, the proposed scheme has less network loss as compared with the other state-of-the-art methods [cit] which makes it more suitable for deployment. fig. 16 shows the comparison of the total generation cost of the proposed dspd scheme with the existing cplex scheme. the proposed scheme has a fixed cost of generation while the traditional cplex scheme has more cost requirement for many iterations and approaches the proposed dspd scheme after iteration 16. this clearly indicates that the proposed scheme is economical and cost-effective than the cplex scheme. moreover, due to the stable nature of the curve, the calculation time is also smaller than the cplex scheme which has an obvious impact on the computational complexity of the system."
"bevel gear volume are the sum of pinion volume and gear volume, while each bevel gear volume is similar to the volume of frustum of cone between the big end and small end of the pitch circle. therefore, according to the volume formula of frustum of cone, volume calculation formula of straight tooth bevel gear pair can be expressed as:"
"the pca, by definition, is a statistical procedure that converts a set of observations on (possibly correlated) variables into a set of linearly uncorrelated new variables. each new variable (described by the eigen vector), called principal component, is a linear combination of the original (and possibly correlated) variables. this idea is important in omag, or octa in general, since most pixels containing static signal are highly correlated over a small-time window, while however is not the case for blood flow signal and noise."
"in the paper, integer serial number encoding method is used, in which each chromosome represents the number of a variable value in discrete collection. the advantages of integer serial number encoding relative to the binary are smaller seek space and simple, intuitive operation. for example, a design variable has 10 optional discrete values, 4-bit binary code is needed (16 kinds of solutions can be expressed), then will result in 6 invalid or duplicate combination solutions. however, integer serial number encoding can fully express combination solutions with only numbers from 0 to 9, so the genetic algorithm does not produce an invalid solution or repeated solution, but also reduces the code string length and improves the encoding and decoding efficiency [cit] ."
"establishing mathematical model is prime step for gear design, in which the design variables, objective function manuscript received january 1, 2010; revised december 10, 2010; accepted january 8, 2011 corresponding author: zhang xiaoqin and constraints are determined through studying gear design theory."
"according to equation (33), the dynamic equation of each variable on the network side and the pv power station side under the opf model can be obtained aṡ"
"the raw oct data holds most information about a sample of interests, including static structure and dynamic blood flow information. however, since our data is often large (typically 5 to 50 gb of memory) and not all information is required for diagnosis of certain disease. therefore, raw data is usually processed and filtered to reveal the information of dynamic blood flow signal, which provides more clinical relevance about a particular disease. the reduction of information in our analysis achieves two things: (i) reduce the amount of information for further analysis; and (ii) projecting our data onto new sets of bases which reveal most variance between frames (i.e., dynamic blood flow signal, speckles and noise). ed/svd and spcp analysis works on the same principal, where static and dynamic blood flow signals are filtered by comparing between the repeated frames within an ensemble. ed/svd, as one of the techniques in traditional pca, constructs the rank k subspace approximation to the n tdimension-n z observations that is optimal in a least-square sense (26) . however, least-square techniques are not robust in a context where outlying measurements can arbitrarily skew the solution from the desired solution (24) . a few examples are tail-noise (strong scatters) and motion artefacts (low coherence of static signal between frames in an ensemble). both examples produce bright stripes across the filtered data matrix, which represents a significantly lower blood flow snr, in comparison to other regions not being affected."
"tables used in the mechanical design are divided into simple lists and list functions according to having function relations or not between data. in simple lists various data are independent, having no clear relationship, which can be stored in one-dimensional array, two-dimensional array or three-dimensional array, retrieved by using look-up, interpolation method, and so on. function relation is existed between function data and variables, but cannot be expressed by clear expression in list function, which is usually treated by interpolation or curve fitting methods. table i shows using coefficient ka [cit] . it is a simple table. when programming, the data are restored into twodimensional array. matlab program is as follows:"
"from equations (31) and (32), it can be found that for each variable, the information involved in its dynamic process includes (1) the derivative of the objective function with respect to the variable itself; (2) the constraint equations involved in the variable, including the coefficient of the equation and the current value of the coupled variable; and (3) the lagrangian multiplier corresponding to the constraint equation in which the self-variable participates. this natural distributed property of the dynamic equation provides the basis for the next step to decentralizing the optimal power flow in the distribution network."
"modularity is mainly advantageous for achieving redundancy pushing the sst a step forward towards reliability. the importance of selecting a modular design is directly related to the required voltage levels that the sst has to interface as well as the rated power transfer that it has to process [cit] . non-modular multilevel topologies can be employed for high voltage grid levels, however, it compromises the redundancy and fault tolerance of the sst."
"the software can accomplish strength-calculation and optimal design of straight tooth bevel gear based on gb/t calculation methods of load capacity for bevel gear. moreover friendly user interface is developed using vb language shown in fig. 1 . input the known data, click on the button \"optimization calculation\", the user can get the optimization results of straight tooth bevel gear. click on the button \"gear materials and fatigue limit query\", the user can get the corresponding fatigue limit of gear materials, shown in fig. 2 . click on the button \"coefficient calculation\", various coefficients can be calculated, shown in fig. 3 . for example, select yfsa and click on the button \"calculate coefficient\", then the interface for calculating yfsa appear as shown in fig. 4 . input the known data and click on the calculating button, the results of recombination tooth coefficient for pinion and gear are gained."
"in omag, pca separates static and slow-moving signal data (highly correlated across the time frames), from uncorrelated signal such as blood flow, speckles and noise signal via linear-regression filter (18, 19) . this can be done by assuming that most of the signal comes from static scatters, and secondly, by assuming that the pixels containing static signal are highly linearly correlated (or at least has higher degree of correlation in comparison to the variations in blood flow and noise) within the observing time-window."
"genetic algorithm (ga) is referred to as a search method of optimal solution to simulating darwin's genetic selection and biological evolution process. genetic algorithm is a series of random iterations and evolutionary computations simulating the process of selection, crossover and mutation occurred in natural selection and population genetic, in according to the survival of the fittest, through crossover and mutation, good quality gradually maintained and combined, while continually producing better individuals and out of bad individuals. through the generational produce and optimizing the individual, the whole group evolves forward and constantly approaches to the optimal solution."
"the pv plant's abandoned light quantity needs to meet the non-negative constraint. this is expressed in equations (39) and (40) where the subscript h is associated with the pv inverter h. it can be found from equations (34)-(44) that the update of the variables of the network unit and the pv unit is only related to their own variables. also, from the auxiliary variables -that is, the dynamic unit of the network unit and the pv unit in the progressive convergence -only the node injection power information at the pv access point needs to transmit to each other. it is worth noting that the entire optimization process does not require the participation of a central coordinator, and the pv units are completely independent of each other and completely confidential. in addition, when converting the spd model to the dspd model, only the sparsity of the coefficient matrix a matters and does not depend on any other conditions. therefore, in this case, the spd model and the dspd model are completely equivalent, and the dspd algorithm is guaranteed to gradually converge to the optimal solution of the original quadratic programming problem. the proposed dspd algorithm is calculated as follows:"
"another alternative to achieve pca is the svd, which can be applied directly onto our complex data matrix, x, without constructing an estimated covariance matrix, c, as in comparison to ed technique. svd also has higher numerical accuracy, i.e. numerical precision by computing the quadratic term x* t, x, especially when the values of entries in our complex data matrix is discretely small. in svd technique, the data matrix, x, is decomposed into:"
"this work provides a discussion on the challenges and solutions to energize and start-up a three-stage chb-based modular multilevel sst. a distributed aps scheme is proposed to self-feed each sst module from its local lv dc links. this solution simplifies the design of the power source required to feed the auxiliary electronics, which may be complex due to the required high galvanic isolation between primary and secondary of one module. on the other hand, this aps structure provides a problematic energization in cases of grid-forming in one sst port. the fact that apss are not operative till the dc links are pre-charged produces an uncontrolled initial start-up stage where transients are neither monitored nor controlled."
1) the algorithm begins by creating a random initial population. e) produces children from the parents. children are produced either by making random changes to a single parent -mutation -or by combining the vector entries of a pair of parents -crossover.
"where n z is the index for discrete depth axis, and n t is the index of discrete time axis. for simplicity, we further assume that static signal is invariant over the observing timewindow, but blood flow and noise. this assumption ensures that, over time, pixels containing static signals would have low variance across repeated frames, whilst pixels containing blood flow signals will appear sparse."
"in most cases, the general assumption holds that pixels (observations) containing static signals are dominant and are highly correlated across the slow-time (repeated n t frame). however, in real case scenario, static signals are not perfectly stationary across repeated frames due to random scattering event (speckle artifact) and tissue motion (motion artifact). these artifacts are generally considered to be outliers (see figure 1 ), which can skew the principal components and produce unwanted results. for example, the first pc would attempt to reduce the variance from the outliers, thus making static signals appear to be dynamic. rpca is therefore introduced in order to solve this problem regarding outliers."
"with the recent development in linear algebra and computing, principal component analysis (pca) [ed (22) and svd] has been adopted on ultrasound angiography; robust principal component has been adopted in magnetic resonance, and x-ray computed tomography (21) . this paper introduces readers to pca methods for omag, and presents an improved algorithm, called robust pca (rpca). this rpca technique is capable of extracting blood flow signal in capillaries with higher blood flow signal-noise ratio, and significantly reduces blood vessels' tail-noise and motion-induced artifacts."
"since eigen and svd decompositions yield similar results in most cases, the data is thus analyzed using ed algorithm in all following experiments. it is also important to distinguish the difference between strategy and technique in the context of oct blood flow analysis. in this paper, strategy refers to different methods of organizing the complex data matrix before applying statistical analysis (i.e., eigen, svd, spcp); and technique refers to different methods of statistical analysis. although algorithm for each technique remains the same for most of our analysis, strategy does require adaptation upon the nature of the complex data matrix. for example, data from a still object can be arranged into casorati matrix (one b-frame rearranged into a vector, and each vector represent a repeated frame); whilst the same strategy (re-arranging data into casorati matrix) would not work well on an in vivo sample where sampling volume is under constant motion. in fact, the term 'motion artifact' arises to describe the diminishing blood-flow signal to noise ratio (b-snr) when data signals de-correlates between slow-time frames due to motion."
"in this case, only the hv grid, vachv, is existing while the lv grid, vaclv, is required to be formed by the sst. similar to the previous case, the control task of the chb converter is balancing the cells capacitors. however, the control of the lv dc link capacitor voltage, vdclv, has to be carried out by the dab. it can no longer be performed through the dc/ac converter because its control will now be focused on creating/forming the lv grid. since the dabs are all connected in parallel to the same clv dc link capacitor, the voltage control cannot be implemented in each dab independently, instead, the outer voltage control loop is implemented in the master control unit and the current reference is sent to all dabs. grid-forming (fig. 3c ):"
"the distribution network opf model constructed by equations (2) and (4)- (11) is centralized. to facilitate the distributed solution, the network is regarded as an independent unit, and each pv inverter is a mutually separate unit, as shown in fig. 1 ."
"ed filter reveals adequate blood-flow snr, but suffers from motion and tail-noise artifacts. since the number of repeated frames in slow-time scan is small, the number of (filtered) principal components becomes important."
"the selected sst topology is a three-phase chb-based modular three-stage topology. it is schematically shown in fig. 1 [cit] . the topology is composed of three converters. the first stage is an ac/dc converter acting as the sst front-end converter connecting it to the hvac grid. this stage is implemented using a chb converter, whose cells are composed of a full-bridge (fb) and a dc link capacitor. second stage is a dc/dc converter acting as the intermediate stage with the function of maintaining a required isolation level between both sst ports as well as bidirectional power flow. this stage uses a dual active bridge (dab) converter. the outputs of all the dabs of the three phases are connected in parallel creating a lv and high current dc link. third stage is a conventional highpower dc/ac inverter which uses a 3-phase 4-line (3p4l) topology. table i shows the specification of the sst. the ac/dc converter and the dc/dc converter stages have a modular structure. each cell of the chb is connected to one dab converter forming one module of the sst (see fig. 2 ). modules are identical and stacked. each module must be able to handle all the isolation required between the two sst ports. this is achieved by the dab high frequency transformer (hft)."
"in gear design manuals, the relationships between many coefficients and other parameters are expressed by table or line graph. in the traditional design, we can determine the values of relevant coefficients by consulting manual, while in optimization design software, all designs and calculations are fulfilled through the computer automatically. therefore programming of chart data is needed."
"to transform the convex optimization problem into a stability process of dynamic systems, equations (28)- (30) can be expressed as a feedback control system as shown in fig. 3 . here, the orange dashed box presents the gradient dynamic process of the original variable, and the process in the green dashed box represents the propagation of the lagrangian multipliers. the integral dynamic process eliminates the deviation caused by the equality constraint ax − b. the blue virtual frame part couples the dynamic process of the original variable and the lagrangian multiplier to realize the system's feedback control. this provides us with a new perspective that the process of convex optimization can be transformed into the stability process of dynamic systems."
"introduction on the observations of complex signal fluctuations, which are hypothesized to be related to moving erythrocytes within functional blood vessels (12) (13) (14) (15) (16) (17) . due to the fact that scattering elements behave differently in static and moving (blood flow) regions, appropriate digital filtering of the oct signals can distinguish blood flow signal from static tissue background, achieving blood flow mapping for microcirculatory tissue beds. there are many different algorithms to contrast blood flow signals from the oct measurements, such as speckle intensity variance, phase variance, and complex signal analysis (8) . this paper focuses on the omag complex signal analysis, which is also the main imaging modality throughout the discussion of this paper."
"the model and the decentralized optimization algorithm proposed in this paper are verified under two different scenarios: example 1 is a community rooftop pv system connected with 12 rooftops pvs as shown in fig. 1, and example 2 is a 33-node distribution network connected with 15 pv plants as shown in fig. 7 . the proposed dspd algorithm is simulated on the matlab simulink platform. at the same time, the model is built on the gams23.95 platform, and the cplex solver is called to solve the centralized opf model for comparison. the computer uses an intel core i5-4570 processor and the memory is 16g."
"in this mode, the sst is connected to both the hvac and the lvac grids (i.e. both grid exist). since the grid is available at both ports, the energization is done from both ports simultaneously. the start-up in this case is less problematic and previously discussed in literature [cit] . fig. 9 shows the detailed step-by-step start-up procedure. the blocks are only in green because events in hv and lv sides takes place simultaneously."
"other authors convexized the non-convex power flow equation by cone relaxation and proposed a distributed algorithm of active power grid reactive power optimization by combining admm with penalty factor adjustment, which can accelerate convergence [cit] . however, it is more difficult for the shared data between different areas to reach an agreement of simultaneous convergence. based on consistency theory, some researchers proposed distributed optimal scheduling for an active distribution network [cit] . this method updates the local parameters through the exchange of a small amount of information between the autonomous node and the adjacent nodes to achieve the consistency of incremental cost. this method can adapt to changes in the network topology. however, because the fully distributed hierarchical scheme, which exclusively regulates the system frequency, may result in operation constraint violations (e.g., voltage, line capacity limits) further ongoing research is needed to elaborately design an all-round distributed hierarchical scheme (e.g., based on fully distributed optimal power flow model) to simultaneously regulate the system frequency and to ensure static security of the distribution networks."
"since the sst is at a commissioning stage, experimental validation of the proposed method on the full sst is not possible. however, the proved reliability of the simulation model for one sst module opens the way to the development of a precise model for the full sst. this model allows the verification of the proposed method over the full sst. this simulation model is developed taking into consideration the details of the full structure, such as: grid filters, balancing control for the chb cell capacitors, transients in line currents and voltages, etc. fig. 14 shows the simulation results for the case of the hv port operating in grid-feeding (procedure in fig. 7) . the obtained results proves the validity of the proposed method for starting up the sst with controlled transients."
"a) design variable: call matlab.putworkspacedata(\"orgm\", \"base\", orgm) 'original machine working conditions call matlab.putworkspacedata(\"wkm\", \"base\", wkm) 'work machine working conditions call matlab.putworkspacedata(\"pregrd\", \"base\", accgrd) 'precision grade call matlab.putworkspacedata(\"p\", \"base\", p) call matlab.putworkspacedata(\"n1\", \"base\", n1) call matlab.putworkspacedata(\"u\", \"base\", u) call matlab.putworkspacedata(\"z1\", \"base\", z1) call matlab.putworkspacedata(\"m\", \"base\", min) call matlab.putworkspacedata(\"fr\", \"base\", fr) call matlab.putworkspacedata(\"supp\", \"base\", supp)"
the start-up events are also schematically shown in fig. 10 . in this case each of ccell and clv pre-charges to their corresponding grid rectified voltage (i.e. 606 v for ccell and 650 v for clv). schematic showing the events taking place during the sst startup procedure in the case of both lv and hv ports operating in gridfeeding. fig. 11 . : chb-based sst module prototype (see fig. 2 ).
"note that if the h-th rooftop pv fault exists, as illustrated in fig. 6, the network can detect this change, and it will not transmit and collect information about the h-th rooftop pv; the faulty equipment is automatically removed during the iterative process. when the h-th rooftop pv is rectified and connects back to the grid, the network can also detect this change, and it will re-transmit and collect information to achieve decentralized optimization until convergence. similarly, even if there is a loss of information or a temporary interruption of communication during the message transfer, the problem can still converge. this is because, for an initial value that satisfies the condition, the trajectories of the dynamic equations (34)-(44) will gradually converge to the optimal solution; thus, even if there is information loss or communication interruption, each subject can still use the volume 7, 2019 figure 7. pv network topology using 33-node distribution. last reception. the incoming message continues to update the variables and can forward the coupled variables once the communication resumes until convergence. in this process, information loss or temporary interruption of communication will only affect the speed at which the trajectory of the dynamic equation converges without affecting the result of the trajectory convergence. under these circumstances, for the network element, only the coupling variables uploaded by the node have changed, and the other optimization tasks can continue without any other changes to the optimization model."
"this is the case when the sst is connected to the hvac grid and is required to form the lvac grid voltage. fig. 7 shows the detailed step-by-step proposed energization and start-up procedure for this operation mode. since the grid is available at the hv side, the energization is done from hv port. the blocks color coding is the same as the previous case."
"the upper and lower limits of the node voltages of the network, the light dissipation of the pv power station, and the reactive output/consumption are the constraints of the opf model expressed as"
"when f (x) is a convex function, but not a strictly convex function, the saddle point can also be constructed by augmenting the lagrangian function:"
"svd is similar to ed in theory and should give the exact same results when implemented with the same filtering strategy. more importantly, the work on svd is motivated by its simplicity, numerical accuracy, and most importantly enabling further development of rpca on omag."
"is the sum of the absolute values of all entries in s. static signal can be thought of as low-rank matrix, l, and blood flow signal can be thought of as sparse matrix, s."
those previously mentioned two facts imposes challenges on feeding the sst auxiliary electronics (i.e. energizing the sst) and its consequent start-up. this paper proposes an aps structure fitting the topology practical implementation constrains. a simple stepby-step procedure for energizing and starting up the sst is presented. the method is then detailed for all possible sst operation modes depending on the availability of the grid at each of its ports. validation of the proposed ideas is achieved using simulation and experimental results applied to the full-scale prototype of one sst module.
"optimization design of bear gear includes the continuous variables and discrete variables. the traditional method is to round the optimal design to the adjacent discrete points. thus, design point might run out of the feasible region, besides traditional optimization method mostly bases on gradient algorithm, which is likely trapped into local minimum search. many studies indicate that the genetic algorithm has strong ability of general optimization, which is very effective in treating optimization problem containing continuous and discrete variables [cit] ."
"in recent years, photovoltaic (pv) power plants and rooftop pvs have become more and more important in the installed capacity of power systems [cit] . the impact of distributed power sources such as pv on power system planning, simulation, scheduling, and control has also caused people to pay more attention [cit] . improving the control mode of pv inverters and utilizing the reactive power support capability of the inverters to the grid can further improve pv consumption capacity and grid voltage [cit] . by solving the optimal power flow (opf) model of the distribution network, the optimal set value can be provided for the reactive power injected/consumed by the inverter [cit] . at the same time, other authors identified that after the new energy is connected to the distribution network, whether for operation control or expansion planning, it is essentially the opf category of the distribution network, showing that the flow is in the distribution network [cit] . this development is of great significance."
"where, b is the face width, δ1, δ2 are respectively cone angle of pinion and gear, m is modulus, z1 is tooth number of pinion, r is cone pitch, z2 is tooth number of gear."
"step 2: the network transmits the (p h, q h ) t calculated in step 1 to the corresponding rooftop pv. accordingly, each rooftop pv transmits the p h,q h t calculated in step 1 to the network, as illustrated in fig. 5 ."
"'executing the matlab program of bevel gear optimization design matlab.execute (\"bevelgearopt\") 'results from matlab are transmitted to visual basic call matlab.getworkspacedata(\"m\", \"base\", mout) call matlab.getworkspacedata(\"z1"
"this paper proposes the optimization design software for straight tooth bevel gear based on genetic algorithm encoding with integer serial number, no invalid solution or duplicate solutions, the code length of the string reduced, the encoding and decoding efficiency improved."
"genetic algorithm, not requiring gradient information and continuous function, optimization results being global, applied to mechanical design optimization problems, can effectively avoid local optimal solutions, and get the global optimal solution [cit] ."
the circular cross-section of the tunnel is approximated by a square whose sides are as long as the circular tunnel diameter and has the same area.
"step 4: update of impulse response vector. the ssaicf updates the weight vector only when the reference input is one. since the square pulses in the reference input are sparse, the filtering and update process is executed over only small portion of the reference input. in fact, the filtering process in eq. (7) requires k multiplyand-accumulate (mac) operations, where k is the number of rsa-peaks within the time period covered by the nlength reference input vector. the lms update in eq. (8) also requires kmac's. therefore, the computational cost of the proposed ssaicf is much simpler than that of the conventional adaptive digital filter of order n. in the end, the proposed scheme has a huge advantage according to other adaptive algorithms because the computational simplicity is very important in the embedded system such as the aemgcfes system. nevertheless simplicity, performance of the proposed method is good and robust because of it is less sensitive to the reference input of adaptive filter"
"in all the derivations above, we have assumed for simplicity of illustration that the 277 noise in all voxels has the same temporal covariance structure. in reality, the 278 autocorrelation can vary over a large range across voxels ( fig. 1g ). so the structured 279 noise in each voxel would follow a different distribution. furthermore, the spatial 280 correlation in noise means the noise inβ is also correlated across voxels, which makes 281 the bias even more complicated. at minimum, noise correlation between voxels violates 282 the requirement of pearson correlation that pairs of observations should be independent. 283"
"communication and transformation rules can be interpreted as sell formulas mostly as we did in the previous section. figure 3 depicts the proposed encoding. assuming a set of n different components in the system, the current state of the system in the membrane i is defined as ! s i p(a 1, ..., a n ). rules manipulate the state of the system by consuming elements in the current time-unit and then, producing them in the future time-unit. hence, we shall use the formula ! s i f(a 1, ..., a n ) to model that, in the next time-unit, there will be a i additional species of the component a i in the membrane s i ."
"in these \"back stage-intensive\" or computational design contexts, it is more important to apply the design concepts and methods for \"document engineering\" [cit] or \"service-oriented architecture\" (soa) [cit] . these design perspectives view the service system abstractly as a set of cooperating services that interact by exchanging information through well-defined interfaces that specify the inputs and outputs of each service. the efficiency of this information exchange depends on how much the services agree on the meaning and encoding of the information they send and receive."
"across several runs, σ −1 i is a block diagonal matrix with each block diagonal elements 722 corresponding to one run, constructed in the same way."
"concern. to detect overfitting to noise, the difference between the cross-validated score 579 of the full model of brsa and a null model can serve as the basis for model selection. 580 we further extend the model to allow for estimating the shared representational 581 structure across a group of participants."
meaning that c i units of a i located in the space domain a i are consumed in k time-units to produce d j units of b j in the space domain b j .
"the estimated rsa signal using the ssaicf, the estimated vemg signal using the ssaicf and the estimated vemg signal using the ls algorithm are shown in fig. 4(c), fig. 4(d) and fig. 4(e) .the result shown in fig. 4(c), (d) and (e) illustrate that the signal estimation and elimination works well without reducing the information in the vemg signal. however, a detailed morphology of the estimated vemg can't be observed in fig. 4(d) and (e). for analysis of the detailed morphology of the estimated vemg using the ssaicf and the ls algorithm, as shown in fig. 4(f) and (g) we plotted the enlarged figures of dashed area in fig. 4(d) and (e). it makes certain that both the ssaicf and the ls algorithm estimate well vemg included in simulation input signal."
"many new and even some not-so-new technologies have inspired another domain of service design for location-based and context-aware services. \"location\" is the most obvious context attribute, but not the only one. [cit] defined context as \"any information that characterizes a situation related to the interactions between users, applications, and the surrounding environment.\" the environment consists of places, people, and things, and for each entity there are four categories of context information: location, identity, status (or activity), and time. this open-ended definition is bounded only by the variety and capabilities of the sensors by which context information can be acquired from the environment [cit] ."
"after information technology became readily available to businesses and service providers, service design concepts and methods were devised to handle \"technology infusion\" in service encounters [cit] . general purpose information technology like database systems, as well as specialized applications for catalog, order, and customer relationship management make service operations more efficient and reliable. in addition, information management technology has increasingly been used to further empower the frontline employee with the information needed to provide personalized and satisfying customer experiences. such technology ensures that that the information available to all frontline employees is more accurate, complete, consistent and accessible than the tacit personal memories of any of them taken individually."
in this section we show how spatial and temporal dependencies in biochemical reactions can be neatly modeled as formulas in sell . the encoding we propose not only gives a logical meaning to those systems but also exhibits a close correspondence between the behavior of the system and the shape of the proofs in
"from the perspective of service design, once again the key principle is that information replaces interaction. there is no need to ask a customer to supply location, time, or other contextual information that the provider has obtained or inferred from a back stage service or sensor. likewise, there is no value in providing information to the customer that isn't relevant to his location or context. for example, the results for a phone browser query for \"coffee\" in seattle should filter out any coffee shops in berkeley. likewise, a user searching for \"next bus\" would ideally receive just that part of the local bus timetable that specifies the schedule for his location in the next few minutes."
"the encoding we have here does not exhibit an adequacy at the level of derivations as in the previous section. the reason is simple. each time we fire a rule, we change from a negative phase to a positive one. then, applying the k rules of the system at the time-unit t requires flipping k + 1 times the polarity of the proof (the \"+1\" is due to the extra phase needed to fire the implication in the formula next). however, the focusing discipline and the subexponentials allow us to control correctly the proof. in particular, if a rule is fired, then needed reactants must be already available in the context. moreover, if the rule cannot be applied, it must be the case that the negative part of the rule is applied. the reason is that the encoding does not increase the number of components in the current state. then, if a rule cannot be applied now, it cannot be applied after executing some other rules (during the same time-unit). hence, what we observe is that the rules are applied non-deterministically and once all of them are fired, the system moves to the next time-unit."
"in a more complex example, a service system for residential energy efficiency could evolve incrementally by externalizing the information captured and created by embedded controllers and appliances (context 6), initially giving consumers more visibility into and control of energy use with \"smart\" thermostats and control panels (context 3), and then later allowing remote control access from other locations and devices (context 5). connecting the home system to the utility grid could enable appliances to control themselves in response to real-time energy pricing based on aggregate system demand (context 7)."
"techniques for designing, prototyping, and evaluating software user interfaces then developed rapidly and continue to evolve along with new technology platforms for self-service applications [cit] ."
"when a service provider becomes truly multi-channel by adding an internet channel to its existing person-to-person or self-service operations, much more is involved than just adding a self-service channel like atms or a telephone touchtone or ivr user interface. these self-service technologies often support only the small subset of services that can be completed in a short transaction or information request, so the self-service channel is not a full substitute for the person-to-person service."
the limiting factor on context-aware services might well be the willingness of people to allow service providers to use information about their current or previous contexts.
"where is the unit weight of water and is the thickness of stratum no., which is located above the centre point of tunnel face."
"a more fundamental change in service design than introducing technology to assist a human service provider is to use technology to transform person-to-person services into self-service ones. this eliminates the frontline employee and moves back the line of visibility between the front and back stage, giving the customer access to information that was previously visible only to the frontline employee."
"many mobile phones and pdas support limited web browsers, which gives people the expectation that they can use them to access services originally designed for browsers on personal computers. after all, they can check webmail, read blogs, weather and news, and conduct searches from their work and home offices; why not do that while commuting on a bus or train? many business applications for sales and customer management are inherently more valuable when they can be accessed by employees at customer sites and not just in their office locations."
"a more subtle way to understand the impact of introducing technology in a service encounter is that it changes the proportions of physical, interpersonal, and information actions. from this perspective, these proportions are design parameters that can be systematically adjusted by technologies that enable the different types of actions to substitute for each other. stored information and interpersonal interactions can often replace each other; there is no need to ask a customer to supply personal or preference information that the provider already knows from previous interactions or has obtained from data brokers."
"is the bias in the covariance structure. therefore, the bias n v (b 2 ii + b 2 jj − 2b 2 ij ) in 217 euclidean distance also depends on the task timing structure and the property of noise. 218 (see fig 1d) . 219 in our derivations above, point estimates ofβ introduce structured noise due to the 220 correlation structure in the design matrix. one might think that the bias can be 221 avoided if a design matrix is not used, i.e., if rsa is not performed after glm analysis, 222 but directly on the raw fmri patterns. such an approach still suffers from bias, for two 223 reasons that we detail below."
"in this paper, rather than trying to detect lesion voxels as outliers from a normal tissue model, we adopt an incorporation strategy. we propose to modify the tissue segmentation model so that lesion voxels become inliers for the modified model and can be identified as a genuine model component. compared to robust estimation approaches (eg. [cit] ) that consist of down-weighting the effect of outliers on the main model estimation, we aim to increase the weight of candidate lesion voxels to overcome the problem of under-representation of the lesion class."
"this would provide a simple design method for limit support pressure on the tunnel face. effect of a multilayered overburden also can be taken into account. in this paper, this mechanism will be used for the probabilistic analysis."
"however, an emphasis on the usability of the front stage's appearance and behavior can sometimes inadvertently de-emphasize the invisible actions in the back stage of the service system. this isn't a critical oversight for simple transactional online services in which the customer can request and quickly receive the desired service or information. but in more complex service systems that involve substantial processing of information or physical fulfillment, the back stage services have much more to do. for example, submitting an online application for employment or university admission, or ordering from an online store, initiates many actions and information flows that won't complete for days or even months. in such service systems, a narrow focus on usability of the self-service interface as a measure of service quality is seriously incomplete. an online shopping site must be usable, and many seemingly small design details can matter a lot, but the customer's ultimate satisfaction depends far more on whether what he ordered arrives when it was promised. a front-stage experience with acceptable usability is necessary, but it is insufficient and might even be counterproductive if it sets unrealistic expectations about the ultimate outcome of the service system operation. what matters far more is the effective and efficient operation of the back stage services, a service system design challenge that is discussed in an upcoming section."
"the expected bias structure when spatial noise correlation exists is difficult to derive. 454 we used (x t x) −1 as a proxy to evaluate the residual bias in the estimated similarity 455 using brsa. as expected, when the snr approached zero, the model over-fit to the 456 noise and the bias structure increasingly dominated the estimated structure despite 457 increasing the number of simulated participants ( fig 4b) top: average correlation (mean ± std) between the off-diagonal elements of the estimated and true similarity matrices, for each method, across snr levels (x-axis) and amounts of data (separate plots). bottom: the correlation between the average estimated similarity matrix of each method (for gbrsa, this is the single similarity matrix estimated) and the true similarity matrix. \"point-est\": methods based on point estimates of activity patterns; \"-crossrun\": similarity based on cross-correlation between runs; \"-whiten\": patterns were spatially whitened (similarity matrix not shown because the true structure could barely be seen) evaluation procedure to detect over-fitting in applications to real data, when the ground 459 truth of the similarity structure is unknown."
"to calculate the mean and variance of the posterior distribution γ(x 0test (t) ) of 880 x 0test, backward step is needed. we denote its mean asμ (t) x0, and covariance asγ"
"in this paper, the probability is associated with the different parameters which are governing the tunnel face stability and furthermore detailed in the following. the soil strength parameters, such as friction angle and cohesion, are assumed as random variables. the tunnel face below river is in the homogeneous soils. the three-dimensional model of tunnel face stability below river is based on the limit equilibrium theory and is adopted for the probabilistic analysis. the probabilities analysis and parameters uncertainty estimation are performed using the markov chain monte carlo (mcmc) simulation method which is good efficiency for highly nonlinear problem [cit], with a delayed rejection adaptive metropolis (dram) [cit] algorithm. the effects of uncertainty of strength parameters on the limit support pressure are discussed using the proposed bayesian framework."
"in the application that has been presented in this paper, the similarity in the performances of the ssaicf and the ls method is evident. however, computational complexity can be reduced with the ssaicf, because it uses an impulse train that is synchronized by the rsa signal. the ssaicf is a potentially powerful method for real-time rsa elimination and vemg estimation"
"we introduce weight parameters in the segmentation model and then solve the issue of prescribing values for these weights by developing a bayesian framework. this has the advantage to avoid the specification of ad-hoc weight values and to allow the incorporation of expert knowledge through a weight prior distribution. we provide an estimation procedure based on a variational expectation maximization (em) algorithm to produce the corresponding segmentation. furthermore, in the absence of explicit expert knowledge, we show how the weight prior can be specified to guide the model toward lesion identification. experiments on artificial and real lesions of various sizes are reported to demonstrate the good performance of our approach."
"in this paper, a bayesian framework for updating soil strength parameters in tunnel excavation below river is presented. the method employs a mcmc simulation based approach to derive the posterior distribution of the parameters. the posterior distribution can be used for probabilistic analysis of the limit support stress on the circular tunnel face. the 3d analysis model with the limit equilibrium method is used as deterministic model. the uncertain parameters considered in the analysis are the soil strength parameters as the friction angle and cohesion. comparing with the prior distribution, the means of prediction improve and the variation of prediction reduces by the proposed bayesian framework. therefore, the proposed method is effective in reducing the 7 uncertainty of soil strength parameters, demonstrating its potential as a practical geotechnical engineering tool. the bayesian framework with mcmc method might be more favorable in the uncertainty analysis and risk management."
"for example, the service blueprinting technique [cit] characterizes person-to-person services as \"dynamic, unfolding over time through a sequence or constellation of events or steps... that produce value for the customer.\" [cit] portray the sequence of touch points as a \"trajectory of interaction,\" [cit] describe it as the \"customer journey\" or \" [cit] call it the \"brand touchpoint wheel,\" [cit] call it the \"customer corridor.\""
"for example, fancy restaurants will employ a sommelier to make suggestions (person-to-person context), but the sommelier might sneak a peek at the \"wine snob\" [cit] application on his pda to refresh his memory about wines and food pairings before he heads out to the dining room (technology assisted context). and while the sommelier would never reveal to the customer that he has relied on wine snob to make a recommendation, in service domains like architecture or technology consulting it is easy to imagine the service provider and customer jointly using technological aids (technology-facilitated context)."
"this distinction between positive and negative phases is natural as all negative rules are invertible rules, that is, provability is not affected when applying such a rule. for example, the rule ∃ l belongs to the negative phase, as the choice of the name used for the eigenvariable is not important for provability, as long as it is fresh. a positive rule, on the other hand, is possibly non-invertible and therefore provability may be lost. for instance, the ∃ r rule belongs to the positive phase: one needs to provide a witness t for that rule. as another example, ⊗ l belongs to the negative phase because this rule is invertible. on the other side, ⊗ r belongs to the positive phase as this rules splits the linear context."
"an increasingly common design pattern for technology-enhanced person-toperson services and self-service is for the provider to support the creation and aggregation of preference information or other content from the users or customers of a service. contributing to this \"community content\" [cit], \"collective intelligence\" [cit], or \"crowdsourcing\" [cit] ) is partly self-serving because it enhances the quality of future service encounters for the contributors, as when customers rate restaurants, hotels, or other service establishments and subsequently choose only highly-rated ones. but it is often an act of generosity or altruism because many people contribute far more information or effort than pure self-interest would justify, even though they know that service will also be enhanced for those who don't contribute at all."
"nevertheless, design approaches for multi-platform or multi-device services that try to make the design problem scalable by applying systematic or automated transformations to a single \"mother of all designs\" can fail to take advantage of specialized functionality on supposedly lesser devices. for example, while phones have vastly less conventional processing power than desktop computers, they can have sophisticated audio processing capability, integrated cameras, text messaging, gps functions, and acceleration or orientation sensors."
"more thorough analysis of existing and potential services will identify design patterns that encourage service innovation at the service system level while preserving the best practices embodied in each of the service design contexts. in addition, it should be possible to extend the unifying ideas about service interfaces and information exchange to better understand service encounters and outcomes that arise in the intersection of service systems. as examples: a business traveler interacts with transportation, hotel, restaurant, and various professional service providers during a business trip; a patient interacts with his physician, hospitals and medical laboratories, insurance companies, and the benefits office at his workplace. it is surely impossible to anticipate all of these ad hoc or dynamic service system compositions, but it is surely necessary to recognize their inevitability. techniques for designing service interfaces that facilitate composition and substitution of contexts are under development and will become increasingly important."
"the remainder of the stimulation artifact can be removed with a blanking circuit, but the muscle response wave produced by stimulation (m-wave) still remains as a significant barrier to measuring vemg for aemgcfes. because the m-wave varies based on factors such as stimulation intensity, fatigue, and activation level of the muscle, an adaptive filter is required to eliminate the mwave [cit] . the power spectrum of m-wave signals does not differ greatly from that of vemg with some small but significant exceptions. the origin of interference due to stimulation is clear; the tissues and muscle cells of the subject are excited by stimulation potential by the emgcontrolled fes system. the voltages induced by the stimulation of muscles during electrically evoked contractions produce artifacts that are added to the emg signal. these residual stimulation artifacts (rsa) generated during electrically evoked contractions is the sum of contributions from synchronously firing motor units. the impulse response, ( ) x h t, of each motor units is due to instantaneous stimulation voltage source. it is quasideterministic and periodic, with a repetition rate equal to the stimulation rate. however the vemg signal is the sum of contributions from asynchronously firing motor units, and is stochastic with near-gaussian amplitude distribution."
"a customer walks into the independent local bookstore where he's bought books for years, but the longtime employee who knows him well isn't there, and the customer doesn't recognize the new clerk behind the counter. but after the customer introduces himself, the new clerk looks him up in the bookstore's computerized bookstore management application. in an instant the new clerk sees the customer's transactional history of prior purchases, along with notes about his reading tastes written by the longtime employee. the new clerk is now able to recommend some new books that have just arrived."
"to calculate the log likelihood, we notice that the predictive model of y res in testing 837 data by both models are dynamical system models in which x 0test is the latent state 838 and y res is the observed data. they are slightly different from the standard dynamical 839 system model [cit] in that not only the latent states, but also the noise, have temporal 840 dependency [cit] :"
"in this section, the probability distribution of the limit support stress is determined for a given condition. using updated soil parameters of friction angle and cohesion presented as posterior distributions, the limit support pressure on tunnel face can be estimated. the limit support pressures obtained by the posterior and prior distribution are shown in figure 6, respectively. the outcomes resulting from prior and posterior parameters are demonstrated. the cumulative distributions are shown in figure 7 . the distributions have considerable effects on limit support pressure on tunnel face. with all the soil strength parameters updated via bayesian framework, the three-dimensional model of tunnel face stability is able to compute probabilities more accurately and realistically."
"the unbounded subexponential ! tω allows us to use the set of reactions as many times as needed. the universal quantification l x : +1 says that at any time-unit the reactions are available. moreover, the use of the ll connective & allows us to choose (non-deterministically) one of the reactions and then discard the others."
"the consulting team's strategy was to build a \"smart multi-channel\" service system that better integrates the online and offline customer experiences, that uses rfid technology to enhance operational and customer services, and that incorporates the requirements of a broader range of stakeholders beyond the bookstore customer. their design explicitly provides services that target both frontline and back stage bookstore employees and the bookstore manager."
"the computational complexity of the ssaicf has been determined in this section. the ssaicf updates the weight vector only during the period corresponding to the width of the square pulses. conventional adaptive filter perform calculations to the filtering and update processes for each of the all-time points in the frame. on the contrary, ssaicf perform over only small portion in the frame because of the square pulses in the reference input are sparse. therefore, this scheme has advantage in simplicity of computational complexity. because of the zeros between the square pulses, the adaptive filter attempts to estimate only the components synchronized to the impulse."
"a major difficulty in estimating accurate limit support pressure arises from the uncertainties incorporated in the input parameters of the computer model. the input strength parameters, such as cohesion and friction angle, are usually determined by direct measurements in laboratory. these samples will be disturbed in the process of testing. meanwhile, the test conditions in laboratory cannot be exactly the same as in situ. this also leads to a significant uncertainty in predicting the limit support pressure for keeping tunnel face stability. this uncertainty poses challenges for obtaining 2 mathematical problems in engineering reliable design of tunnel excavation. the stochastic approach can improve the traditional deterministic methods for taking the uncertainties of the parameters into account."
"the apple iphone exemplifies the optimization of applications to specific devices; [cit], more than 50,000 applications exist for it, and most have been built exclusively for it [cit] ."
"our brsa method is closely related to the pcm [cit] . a major difference is that 632 pcm models the point estimatesβ after glm analysis while brsa models fmri data 633 y directly. the original pcm [cit] in fact considered the contribution of the noise in 634 pattern estimates to the similarity matrix, but assumed that the noise inβ is i.i.d 635 across task conditions. this means that the bias in the covariance matrix was assumed 636 to be restricted to a diagonal matrix. we showed here that when the order of task 637 conditions cannot be fully counter-balanced, such as in the example in fig 1, this 638 assumption is violated and the bias cannot be accounted for by methods such as pcm. 639 if one knew the covariance structure of the noise σ, then the diagonal component 640 of the noise covariance structure assumed in pcm [cit] could be replaced by the bias 641 term (x t x) −1 x t σ x(x t x) −1 to adapt pcm to estimate the covariance structure 642 u that best explainsβ [cit] if spatial noise correlation is not considered. however, as 643 shown in fig 1g, different voxels can have a wide range of different autocorrelation 644 coefficients. assuming a single σ for all voxels may be over-simplified. in addition,"
"bayesian approach can update the current state of knowledge about the model parameters based on the measurement data [cit] . many successful applications of bayesian approach have been reported, e.g., estimation of the parameters of hydrological model [cit], confidence interval of swcc [cit], and braced excavation [cit] . although previous studies have been done on the probability analysis of tunnel, a systematic bayesian framework and markov chain monte carlo (mcmc) simulation for the probabilistic analysis has not yet been developed. the effect of uncertainties of soil parameters on prediction uncertainty of tunnel face stability below river using a bayesian framework has not yet been investigated."
"in this paper, we demonstrated that bias can arise in the result of representational 550 similarity analysis, a popular method in many recent fmri studies. by analytically 551 deriving the source of the bias with simplifying assumptions, we showed that it is 552 determined by both the timing structure of the experiment design and the correlation 553 structure of the noise in the data. traditional rsa is based on point estimates of neural 554 activation patterns which unavoidably include high amounts of noise. the task design 555 and noise property induce covariance structure in the noise of the pattern estimates."
"for model selection purpose, design matrix x test for the testing data should be 822 generated in the same manner as they are for the training data by the researcher. for 823 full brsa model, x testβpost is the predicted task-related signal in y test ."
where v is an alphabet of symbols; μ 0 is the initial configuration; o is the label of the observable membrane; and r is a finite set of rewriting rules of the following forms:
"valid estimation of the tunnel face stability under excavation requires a reliable evaluation of limit support pressure which prevents soil collapse. this issue has been extensively studied with limit equilibrium method [cit], numerical methods [cit], and experimental methods [cit] . the limit equilibrium method is commonly used for predicting limit support pressure, being relatively simple compared with finite element analysis. investigations showed that soil parameters, such as cohesion and friction angle, are the most important soil properties for influencing the limit support pressure [cit] . therefore, accurate estimation of soil strength parameters is necessary for assessment of the tunnel face stability under tunnel excavation."
"in addition to inferring the representational similarity structure, our method also 566 infers activation patterns (as an alternative to the traditional glm), snr for different 567 voxels, and even the \"design matrix\" for data recorded without knowledge of the 568 underlying conditions. the inferred activation patterns are regularized not only by the 569 snr, but also by the learned similarity structure. the inference of an unknown \"design 570 matrix\" allows one to uncover uninstructed task conditions (e.g., in free thought) using 571 the full bayesian machinery and all available data."
"the rsa and vemg consists of similar frequency components. therefore, good performance cannot be expected with linear time-domain and frequency-domain fixed filters for suppression of rsa. the approach of the adaptive filtering techniques permits to remove rsa by time varying stimulation because the rsa during the previous stimulation period is strongly correlated with that of the current stimulation period because they are quasideterministic and periodic."
"if we knew φ and θ, we could directly compute the optimum weight vector. but in general, we do not have access to φ and θ . therefore, we use adaptive estimation method as one approach to find the optimum weight in eq. (5) .to address the time-varying feature of rsa signal in emg channel, the adaptive estimation of rsa signal is necessary. for the updating process, the filter weights the adaptive algorithms."
"second, averaging raw data 6 sec after events of interest over-estimates the similarity 251 between neural patterns of adjacent events, an effect independent of the fmri noise 252 property. this is because the true hrf in the brain has a protracted time course 253 regardless of how one analyzes the data. thus the estimated patterns (we denote byβ 6 ) 254 in this approach are themselves biased due to the mismatch between the implicit hrf 255 that this averaging assumes and the real hrf. the expectation ofβ 6 becomes patterns of adjacent events. if the order of task conditions is not fully counter-balanced, 263 this method would therefore still introduce into the estimated similarity matrix a bias 264 caused by the structure of the task."
"the model of a reaction (equation 1) is a formula that first checks if the needed reactants are available in the specific space domains. if this is the case, the reactants are consumed and the products are added k time-units later:"
"y i is the time series of voxel i. x is the design matrix shared by all voxels. β i is the 694 response amplitudes of the voxel i to the task conditions. i is the residual noise in 695 voxel i which cannot be explained by either x or x 0 . we assume that is spatially 696 independent across voxels, and all the correlation in noise between voxels are captured 697 by the shared intrinsic fluctuation x 0 ."
"to test the performance of brsa in a case where the ground-truth covariance structure 354 is known, we embedded structure into resting state fmri data. signals were simulated 355 by first sampling response amplitudes according to a hypothetical covariance structure 356 for the \"16-state\" task conditions ( fig 3a), and then weighting the design matrix of the 357 task in fig 1a by the simulated response amplitudes. the simulated signals were then 358 added to resting state fmri data. in this way, the \"noise\" in the test data reflected the 359"
"apte and his collaborators analyzed services in terms of the proportions of physical actions, interpersonal actions, and information actions \"that involve the manipulation of symbols.\" [cit] . information-intensive services are those in which the information actions are responsible for the greatest proportion of value created by the service system. the most information-intensive ones are those with few or no requirements for physical and personal interactions, or where personal interactions are narrowly focused on the information exchange needed to make decisions and apply other information. examples include accounting, data entry and transcription, translation, insurance underwriting and claims processing, legal and professional services, customer support, and computer programming. in these service domains documents, databases, software applications, or other explicit repositories or sources of information are ubiquitous and essential to meeting the goals of the service consumer or customer."
"many online retailers are virtual firms that don't own any product inventory. their catalogs contain the goods that they can reliably obtain from distributors. the \"storefront\" is a self-service front stage that collects the order information and then passes it on to back stage service providers that process credit card payments, operate the warehouses, deliver packages, and so on (see glushko and mcgrath, section 1.1, 2005) . taken together, this pattern of physical processes and information exchanges defines a type of service system known as \"drop shipment.\" some of these back stage services involved in drop shipment, such as those that check inventory, verify credit, and process payments, are \"pure\" information services and are typically carried out entirely by automated services without any human involvement or physical actions. of course not every back stage service can be completely automated, and shipment tracking, credit card fraud detection, customer support, and other processes in the drop shipment service system expose user interfaces to different sorts of people who need to handle exceptional or error situations. however, to the extent that a service system relies on complex back stage choreographies of information flows and the physical actions they direct, the front stage services and interfaces contribute proportionally less to the overall service outcome and user experience."
"contexts 1, 2, and 3 span a continuum of service designs that progressively incorporate technology to make services more transactional, with improved consistency, reliability, timeliness, and personalization. more abstract characterization of the service encounter facilitates the technological augmentation of the service provider or the substitution of a computational actor for him."
"for real-time application, the most recent advances in digital signal processing have attempted to reduce the m-wave using a comb filter [cit] that predicts the filtered emg output, y, for the nth sample of the raw emg, x, by subtracting ( ) tstim x n n −, the value of the raw emg from the same point in time from the previous stimulation cycle (eq. (1))."
"a premise that guides the design of person-to-person services is that the quality of the service is determined in the front stage encounter between the frontline service provider and the customer [cit] . it naturally follows that the typical design techniques for person-to-person services are ethnographic and participatory, immersing the designer in the customer's context to observe, participate with, and interview the customer to understand his goals and behavior [cit] . these methods yield a customer-focused service that emphasizes the touch points that he experiences in his interactions with the service provider."
"existing methods frequently avail of complementary information from multiple sequences. for example, lesion voxels may appear atypical in one modality and normal in another. this is well known and implicitly used by neuroradiologists when examining data. within a mathematical framework, multiple sequences enable the superior estimation of tissue classes in a higher dimensional space. however, given that the information content and discriminative power to detect lesions varies between different mr sequences, the question remains as to how to best combine the multiple channels. depending on the task at hand, it might be beneficial to weight the various sequences differently."
"nevertheless, just because some information technology has the potential to yield more consistent, reliable, and timely service, design choices must be made about whether and where to introduce it into the service system. technology can be used solely by the frontline employee to enhance his capabilities, or by both the frontline employee and the customer to more directly enhance their interaction. [cit] distinguish these two cases as \"technologyassisted\" and \"technology-facilitated\" encounters. but the most important choice is whether the technology should be used to replace the frontline employee entirely, leaving a self-service encounter."
"in addition to these differences, brsa explicitly models spatial noise correlation. it 649 also comes with the ability to select between a full model and null model based on 650 cross-validated log likelihood, and the method can be applied to fmri decoding. pcm 651 can additionally evaluate the likelihood of a few fixed candidate representational 652 structures given by different computational models. it can also estimate the additive 653 contributions of several candidate pattern covariance structures to the observed 654 covariance structure. these options are not yet available in the current implementation 655 of brsa. combining the strength of pcm and brsa is an interesting future direction. 656 many aspects of flexibility may be incorporated to brsa. for example, the success 657 of the analysis hinges on the assumption that the hrf used in the design matrix 658 correctly reflects the true hemodynamics in the roi, but it has been found that hrf in 659 fact vary across people and across brain regions [cit] . jointly fitting the shape of the 660 hrf and the representational structure may improve the estimation. in addition, it is 661 possible that even if the landscape of activity patterns for a task condition stays the 662 same, the global amplitude of the response pattern may vary across trials due to 663 repetition supression [cit] and attention [cit] . such modulation may not be 664 predictable by response time or stimulus duration. allowing global amplitude 665 modulation of patterns associated with a task condition to vary across trials might 666 capture such variability and increase the power of the method. 667 our simulations revealed that brsa is not entirely unbiased, that is, results cannot 668 be improved indefinitely by adding more subjects. we hypothesize that the residual 669 bias is due to the underestimation of the number of components necessary to capture 670 the spatial correlation introduced by intrinsic fluctuation. development of a proper but 671 less conservative algorithm for estimating the number of components suitable for brsa 672 may improve its performance."
"in the study, we proposed an adaptive algorithm for the suppression of m-wave signals. the proposed algorithm uses a stimulation-synchronous adaptive impulse correlated filter (ssaicf) whose reference signal is synchronous to the peaks of stimulation signals. the advantage of this proposed method is that it uses the impulse signal as the reference signal. so, its adaptation needs to be performed only once during the sampling period of the stimulation pulses, which greatly simplifies the algorithm for implementation."
"vemg data with the rsa fig. 4 shows a simulated data set and results of analysis using the simulated data set. the filter coefficients of the ssaicf were chosen for the order of the filter to be 600 and a learning rate (μ) was set to 0.05 on an empirical basis. the simulated data set as shown in fig. 4(a) and blanking circuit event synchronous impulse as shown in fig. 4(b) were applied to a primary input and a reference input of the ssaicf respectively so as to estimate the simulated rsa signal. to maximize performance of the ssaicf, the order of filter must be determined based on the duty cycle of the control signal of blanking circuit. we found the impulse response for a cycle by deciding the filter order as pointed above."
"why seven contexts rather than five or nine? like every classification system, this design framework is somewhat arbitrary, but the proposals in this paper don't depend on it being the best or the only way to analyze and organize design challenges and methods. the paper demonstrates that these seven contexts are conceptually coherent building blocks that enable the incremental design of many different kinds of service systems. furthermore, an informal analysis of service systems in numerous domains suggests that these seven contexts are sufficient to describe those that currently exist as well as many that are likely to be developed."
"the independent local bookstore exemplifies the person-to-person service setting with empowered frontline service employees, because such stores only survive if they provide highly personalized and empathetic service. bookstore employees are motivated to recognize customers; greet them by name; remember their favorite subjects, authors, prior purchases, and spending budget -and use all of this information to recommend new books. if the customer is a new one, the bookstore employee asks about preferences, suggests some books and uses the customer's feedback to refine the employee's model of the customer, and perhaps gives a personalized tour of the bookstore. [cit] classic statement that \"discretion (on the part of service employees) is the enemy of order, standardization and quality\" might be true of highly routinized person-to-person transactional services. we are all too familiar with the bureaucratic inflexibility of service providers like the department of motor vehicles where we fill out a form, submit it to a service employee, and have an experience that is never personalized. we all also know from our own experiences that people would prefer services \"their way.\" [cit] valiantly attempted to systematize techniques for \"sealing off the technical core of service operations\" to enable distinct levels of service flexibility on a continuum from \"full\" to \"restricted\" service. but they and other service design and operations researchers ultimately acknowledged the inherent tension between the goal of achieving standardization and efficiency for service providers and that of satisfying the often variable demands and preferences of service customers."
"first, we applied a series of open-loop submaximal sinusoidal electrical stimulations without any voluntary contraction to collect the rsa (fig. 4(a) first raw signal) . second, we asked the subject to contract his muscle without any electrical stimulation according to same sinusoidal reference and we collected the vemg (fig. 4(a) second raw signal. finally, we added two signals for making the rsa signal with vemg ( fig. 4(a) third raw signal). the m-wave can be more than a magnitude lager than the vemg. however when we collect the rsa from emg amplifier with a blanking circuit, the amplitudes of rsa and vemg are almost same [cit] ."
"several computational frameworks for modeling in different ways various aspects of biological systems have been defined in the last decade (see e.g., [cit] ). however, so far we have not seen one single formalism for modeling reaction systems with both time and space, and, at the same time, with the ability to express a logic for proving properties which can depend both on time and space locations. normally, there is one formalism and a language for the modeling and the specification of a biological system and at least another different formalism for expressing the properties of interest (e.g., a temporal logic) and for proving them (e.g., by using a model checker)."
"the proliferation of devices and network alternatives is a challenge for service system designers. if a service provider's intended customers use different or multiple devices, the service must be designed to work on all of them. this task might be considered an extension of the self-service design problem to multiple channels. because the devices and networks have different capabilities, this task is also analogous to service personalization, although the service is being adapted to the device and only indirectly to its user."
"thus, given a stimulation source ( ) n δ, ( ) h n can be estimated using the model in eq. (2). however, a priori information about ( ) n δ is generally available because ( ) n δ is directly detected and produced from the emg-controlled fes hardware system. therefore, we have to develop a method of estimating the impulse response ( ) h n . the contaminated emg signal contains both the vemg signal and rsa signal. the output of the emg channel can be expressed as"
"but even though the online encounter has completed with what is apparently a satisfactory result, most of the work to fulfill the customer's book purchase has not yet begun. fulfillment involves invisible physical actions by warehouse, shipping, and delivery personnel, and each action presents an opportunity for service failure. the wrong book can be picked from the warehouse, or it can be lost, damaged, or delayed in delivery because of a human error, traffic congestion, bad weather, or a host of other factors."
"information-intensive services, defined in the second section, are those in which information processing or information exchange, rather than physical or interpersonal actions, account for the greatest proportion of the co-created value [cit] . the third section describes seven different service design contexts and recasts many of their typical design concerns and methods in terms of the information required to perform the service (sometimes called the \"service interface\"), and how the responsibility to provide this information is divided between the service provider and service consumer. the fourth section shows how this abstract description of services makes the different service contexts into substitutable and combinable building blocks of service systems and suggests some unifying design concepts and methods that apply to all of them."
"for each reactant a in the system, we assume to have a constant symbol a in the logic. we also assume to have an uninterpreted binary predicate ct(·, ·). intuitively, the formula ! s b .2 ct(a, c) means that the concentration of a in the space domain s b is c during the second time-unit. as usual, c is defined as the n-th application of the successor function suc to the constant 0. we shall use suc n (x) to denote the n-th application of suc to x."
"the operation of the bookland service system can be described in terms of the flow of information between the different actors and contexts it contains. the key components of bookland, highlighting the information exchanges and touchpoints where the contexts interconnect, are as follows:"
"delivering some services on multiple devices or platforms (context 5) and with location-based and context-awareness capabilities (context 7, e.g., through sensors in mobile phones) are natural and even inevitable technology-enabled expansions of the scope of a service system. these contexts have become essential parts of service systems that run supply and distribution chains, deliver medical care, manage energy or facilities, or operate other information-intensive enterprise or inter-enterprise processes or systems of equipment (context 6)."
"because a competitor or alternate supplier is often \"just a click away,\" the usability and quality of service in a self-service application or web site is an important concern for designers. most usability specialists would agree with the claim that \"the success of online services is largely determined by the customer experience via the web site interface\" [cit] ."
"but the most ubiquitous and day-to-day use of gps technology in informationintensive services is in mobile phones. [cit] terrorist attacks, governments worldwide mandated location tracking of mobile phones. initially, telecom carriers used tower triangulation techniques to do this, so no location information was available in the phone itself. more recent phones have built-in gps, so the phone can now tell other applications where it is, not just the government! of course, phones both send and receive information, so once the phone reveals location or contextual information, applications can send location-based services to it [cit] . location is so intrinsic to mobile services that user interfaces that integrate or \"mash up\" information into maps have rapidly supplanted text-oriented techniques for presenting choices or search results [cit] ."
"the global positioning system was developed as a strategic military capability but it is far more important to most people for its commercial applications. gps navigation systems provide directions, dispatch emergency responders to vehicles [cit], and combined gps and rfid devices enable vastly more efficient inventory management in global supply chains [cit] ."
"generative model of bayesian rsa. the covariance structure u shared across all voxels is treated as a hyper-parameter of the unknown response amplitude β. for voxel i, the bold time series y i are the only observable data. we assume y i is generated by task-related activity amplitudes β i (the i-th column of β), intrinsic fluctuation amplitudes β 0i and spatially independent noise i :"
"more generally, we can find such a periodicity by using existential quantification on subexponentials, i.e., by looking at the final instantiation of the subexponential variable l in the proof of the sequent system(1),"
"assuming that the signal β is independent from the noise, it is then also independent 169 from the linear transformation of the noise, (x t x) −1 x t . thus the covariance ofβ is 170 the sum of the covariance of true β and the covariance of (x t x) −1 x t : 171 β ∼ n(0, u + (x t x) −1 x t σ x(x t x) −1 )"
"the above system, called sell, enjoys good proof theoretic properties: [cit] (resp. [cit] ) proved that sell (resp. sell ) admits cut-elimination. moreover, a sound and complete focused proof system [cit] for those systems can be given [cit] . focusing is a powerful discipline on proofs which can be seen as normal form proofs for proof search. in fact, we shall use focusing to prove the adequacy of specifications as shown in the forthcoming sections."
"when a customer logs in to identify himself on amazon.com or similar internet bookseller site, the generic catalog is replaced with a personalized one that reflects his shopping history and interests explicitly expressed in search queries, abandoned shopping carts and wish lists. but unlike the physical bookstore, where following the customer around would be obtrusive, the self-service context enables the easy capture of implicit preferences and interests based on the customer's browsing history. and while an experienced and insightful bookstore employee makes recommendations by reflecting on the purchases and preferences of customers he deems similar, amazon.com and other internet retailers employ very sophisticated recommendation services that aggregate and analyze millions of transactions and queries [cit], while also making dy-namic adjustments to catalog content and pricing based on the customer's real-time browsing behavior."
"emphasizing the information exchange aspect of service encounters makes it easier to design and understand service systems that combine multiple design con-texts because it treats them as complementary or substitutable components rather than antagonistic alternatives. it might not matter if the actor performing a translation or calculation service is a human or a computer, and the abstraction of information exchange hides the implementation. similarly, while there is an important social dimension in service systems that use community content or crowdsourcing to enhance service quality, the aggregation of preference information or content is the underlying mechanism."
"a way forward emerged with more nuanced analysis of service value creation in terms of \"value\" or \"profit\" chains in the \"service production system\" [cit] and the utility of recognizing an architectural boundary between \"front office\" or \"front stage\" services and those in the \"back office\" or \"back stage\" [cit] . service operations of the former variety involve interactions with the customer, while those of the latter variety contribute to the former while remaining inaccessible or invisible to the customer. when services are designed with a \"line of visibility\" separating the front and back stages in place, frontline service employees can be empowered with the discretion to adapt the service in the front stage when necessary to satisfy customers [cit] in any way that doesn't jeopardize the efficient operation of the back stage."
"onceû is estimated (after the iterative fitting procedure for l and x 0 ),û is 351 converted to a correlation matrix to yield brsa's estimation of the similarity structure. 352"
"to reduce this bias, we proposed a bayesian framework that interprets the 560 representational structure as reflecting the shared covariance structure of activity levels 561 across voxels. our brsa method estimates this covariance structure directly from data, 562 bypassing the structured noise in the point estimates of activity levels, and explicitly 563 modeling the spatial and temporal structure of the noise. this is different from many 564 other methods that attempt to correct the bias after it has been introduced."
where v is the effective earth pressure from the overlying prism and is the cross-sectional area of prism. vertical friction applied to lateral of the element is
"advances in information and communications technologies have enabled information-intensive activities that create information to be separated in space and time from other processes or services that use it. this is the principle that enables the \"outsourcing\" of services and 24x7 [cit] . more generally, technology-enabled service disaggregation has transformed vertically integrated and centralized firms into more virtual and network-like forms that function as compositions of collaborating services that can be located almost anywhere in the world, from boston to berlin to bangalore [cit] ."
"first, rsa on the raw activity patterns suffers from the second contributor to the 225 bias in rsa that comes from the temporal properties of fmri noise. to understand 226 this, consider that estimating activity pattern by averaging the raw patterns, for 227 instance 6 sec after each event of a task condition (that is, at the approximate peak of 228 the event-driven hrf) is equivalent to performing an alternative glm analysis with a 229 design matrix x 6 that has delta functions 6 sec after each event. although the columns 230 of this design matrix x 6 are orthogonal and (x t 6 x 6 ) −1 becomes diagonal, the bias 231 term is still not a diagonal matrix. because of the autocorrelation structure σ in the 232 noise, the bias term (x t 6 x 6 ) −1 x t 6 σ x 6 (x t 6 x 6 ) −1 essentially becomes a sampling of 233 the temporal covariance structure of noise at the distances of the inter-event intervals. 234 in this way, timing structure of the task and autocorrelation of noise together still cause 235 bias in the rsa result."
"the chains with total number of evaluations equal to 30,000 are used in mcmc simulation with a deam algorithm, and the posterior samples generated are shown in figure 3 . from the figure 3, it shows that the chains can quickly converge to a stable posterior distribution. the markov chain is effective in collecting representative samples. after verifying that the distribution of the values converged to the posterior distribution, the distribution of the parameters can be used to produce optimum simulations and generate an estimation of the model uncertainty. the initial 5,000 samples are discarded because the markov chain has not stabilized. the remaining 25,000 samples are used as the posterior distribution samples. within a small region and show normal shape. the posterior mean, standard deviation, and cov value are summarized in table 2 using the remaining 25,000 samples. the \"best\" parameter set is typically created by computing the mean for each input parameters [cit] . the mean of posterior distribution for the friction angle and cohesion is also smaller than that of prior distribution, the updated mean values of the friction angle, and cohesion decrease from 30 ∘ to 27.816 ∘ and from 5 kpa to 4.268 kpa, respectively. the means of values are getting increasingly closer to the measured parameters (friction angel 28 ∘ and cohesion 4 kpa) which express the ability of the mcmc method to converge to the measured parameters. according to table 2, the prior and posterior standard deviations are also given for a comparison. the distribution of friction angle and cohesion is significantly modified via bayesian updating. the standard deviation of friction angle is reduced from 0.15 to 0.114. the standard deviation of cohesion is reduced from 0.2 to 0.163. compared to the covs of the soil parameters of prior and posterior distribution, the covs of both friction angle and cohesion are reduced with the application of the bayesian framework. the posterior density will be more concentrated and more informative than the prior density. the covs for both friction angle and cohesion are reduced, implying reduction in the uncertainty of strength parameters. the updated parameters show that the uncertainty of friction angle and cohesion is reduced, which reflects the measurement on the variable."
"added bonus: inferring pseudo-snr map 398 although the voxel-specific parameters θ are marginalized during fitting of the model, 399 we can obtain their posterior distribution and estimate their posterior means. the 400 estimated pseudo-snrŝ is of particular interest, as it informs us of where the estimated 401 representational structure is more strongly supported in the roi chosen by the 402 researcher. as shown in fig 3d, the estimated pseudo-snr map highly resembles the 403 actual map of snr in our simulated data in fig 3c, up to a scaling factor."
"many subcomponents of the expressions in these equations do not depend on l and 793 thus can be pre-computed before optimizing for l. the fixed grids of (ρ, s) further 794 make several subcomponents shared across voxels when evaluating 16. these all reduce 795 the amount of computation needed."
"this section is devoted to show some examples of properties that can be verified with our framework. moreover, to give a more general picture of our developments, we show how to encode p-systems [cit] and some properties of such systems."
"a customer is browsing the shelves in his neighborhood bookstore when he receives a text message on his cell phone. the message directs him to the shelf where he can locate a book that he had recently viewed in the online website of the bookstore. he locates the book, takes a photo of its bar code with the cell phone camera, and launches a price check application. he learns that the book is available at a lower price in a competitor's bookstore a half mile away that will be open for another 45 minutes. the map application on his phone shows him the best route to the other store, and he buys the book there instead."
"if we decide to focus on the formulas on the left, we only have one possibility: to focus on eqs . in the following, we shall show that after the positive phase of the derivation, we end up with a formula of the shape [["
"rfid chips, essentially bar codes with built-in radio transponders, enable location tracking and context sensing to be automated. rfid receivers can be built into store shelves, loading docks, parking lots, toll booths, to detect when some rfid-tagged object is at some meaningful location. rfid tags can be made \"smarter\" by having them record and transmit information from sensors that detect temperature, humidity, acceleration, and even biological contamination [cit] ."
"where q m represent a location of the occurrence each stimulation frame. considering an n th order conventional adaptive filter (n : the adaptive filter length), the filter output and the error signal are computed as the filter weights can be updated using one of adaptive algorithms such as recursive least square (rls), affine projection (ap), and least mean square (lms). the lms algorithm is very simple and straightforward to implement in real time and adapt to a neighborhood of the wienerhopf least mean square solution. if the lms algorithm is used, the weight update is accomplished as"
"each of the 7 design contexts has characteristic design concerns and methods, highlighted in table 1 . at first glance, these different design concerns and methods may seem incompatible, making it easy to understand why there has been little research or practical work in service design that spans more than a few contexts. but as described in the previous sections, there are systematic relationships among the contexts that can be exploited as design parameters or patterns. furthermore, a more abstract look at the seven contexts suggests the unifying design concept that services and service encounters can be viewed as information exchanges. the abstraction enables the contexts to function as building blocks that follow design patterns for the incremental evolution of many different kinds of information-intensive service systems."
"considering an element has dimension within prism of height as shown in figure 1, the upper and lower vertical force of element are, respectively,"
"where is the actual limit support pressure; ( ) is predicting limit support pressure using the given tunnel face stability model; is vector of uncertain input parameters. the likelihood function gives a measure of the agreement between the available data and the corresponding model output. assuming that error is to be normally distributed, the likelihood function is the conditional probability density function of and can be written as"
"the test subject was able to proportionally control the stimulated muscle smoothly in real-time, over a continuous force range and able to voluntarily stop the stimulation. usage of the ls method is limited in real-time application. if vemg estimation performance of two methods is similar, we can judge that the ssaicf is more efficient than the ls algorithm in practical application due to advantages that the ssaicf requires only simple calculation."
"the proposed method was tested on the simulated signal and applied to actual signal thereafter. in order to assess the performance of the vemg estimation and rsa elimination using the proposed method, the ls algorithm is compared with the ssaicf."
"nevertheless, this point of view is arbitrary, and often many of the actors or services in a service system could be alternative or secondary points of view. what is a supply chain from one perspective is a demand chain from another. in an educational service system the conventional focus is on the teacher-student service encounter, but it is also essential to design the teacher-parent encounter. in a hospital, it is easy to default to the patient as the focal point of view, but in a teaching hospital, much of the service system is designed to educate medical students, and patients can in many ways be considered as service providers to them."
"the fifth section illustrates these new design concepts and methods using the design of a \"smart multi-channel bookstore\" service system that combines service components from many of the design contexts. value propositions and information flows will be described from the contrasting points of view of customers, front and back stage bookstore employees, and the bookstore manager that taken together yield a holistic perspective on the service system."
"s occurs in the right-hand side of the reaction the quantifier ∀x allows us to bind the current number of reactants in the system. the formula consume consumes the needed reactants and produce adds such reactants k time-units later. we note that once a rule is applied, the concentrations of the reactants that do not occur in the reaction are simply copied (without changes) to the time-unit t + k (due to the first cases of n i above)."
"the in-store service system from the customer's perspective the in-store service system for the employee's restocking tasks figure 6 presents a service blueprint for the bookstore employee, imposing a point of view that strongly contrasts to that of the customer in figure 5 . for example, some service system components that were invisible to the customer are now in the front stage, while others that were front stage to the customer are not visible to the employee. the bookstore blueprint shows the process of performing two kinds of restocking tasks. the first is to returning so-called \"zombie\" books that and have been left in the coffee shop, restroom, or any other location to their normal shelf locations. the second is restocking books that have been sold and taken out of the store."
"comparing the cross-validation score of the full model and a null model is one 674 approach to detect overfitting. one interesting finding is that when the design matrix 675 does not explain the real brain response (fig 5c where signal factors that cause bias in traditional rsa and impact the power of detecting similarity 684 structure. carefully designing tasks that fully balance the task conditions, randomizing 685 the sequence of a task across participants, and increasing the number of measurements, 686 are our recommended approaches in the first place. in the analysis phase of the project, 687 one can then use brsa."
"for comparison, we use a computationally efficient adaptive algorithm based on the least mean square (lms) adaptive filter proposed by j. r. laguna [cit] . the lms is an adaptive noise canceller for deterministic component of event-related signals that are time-locked to a stimulus, in which a pulse related to the stimulus was used as a reference input. the method estimates the deterministic component of the signal and removes the noise uncorrelated with the stimulus, even if this noise is colored, as in the case of evoked potentials and reflects transient changes in the deterministic signal better than an ensemble average method. in the method, the filter uses two inputs: the desired signal (primary input) and a pulse correlated with the deterministic component (reference input). since the pulse is composed of one sample, they refer to this approach as \"adaptive impulse correlated filter (aicf)\". the adaptive filter used in this study is a specific form of the previous aicf, in which the impulse train synchronized by the stimulation voltage is used as a reference input. the proposed algorithm is referred to as the stimulation synchronous adaptive impulse correlated filter (ssaicf). fig. 1 shows the block diagram of the ssaicf."
"by calculating 41, 42 and 43 recursively with t incremented by 1 until n t, the 877 predictive log likelihood 32 of both the full and null models can be calculated to serve as 878 the basis of model selection."
"in addition, two distinctive document engineering methods extend the basic soa philosophy in service system design. the first is that document engineering assumes that in information-intensive industries, documents and other sources of structured information better embody the functional and interface specifications for services than anything else. this assumption might seem tautological, but it merely restates the contrast noted earlier between information-intensive and experience-intensive services. in the former, documents and other information sources are ubiquitous and intrinsic to the goals and activities of the stakeholders and actors, so information is the most important thing to analyze. even if document implementation or management technology changes over time, the logical model of a document can endure far longer that the tenures of the specific people who produce and use the documents."
"another desirable feature for computational models is the ability to describe a biological system at different levels of abstraction. this may be useful to capture the variability of subnetworks in the topology of a biochemical reactions network, for instance, at the level of metabolic or signaling pathways."
this more abstract perspective on service design turns the different design contexts into building blocks that enable the incremental design of service systems. one typical trajectory for service system evolution starts with a person-to-person service and adds technology contexts to it. an alternative design trajectory adds customer-facing service contexts to exploit latent value from invisible back stage services.
"where f is 1 717 only at the superdiagonal and subdiagonal elements and 0 everywhere else, and d is 1 718 on all diagonal elements except for the first and last one, and 0 elsewhere. for 719 abbreviation, we can denote"
"an emg amplifier shutdown control can eliminate most, but not all, of the saturation effect [cit] . this problem is solved by using a low impedance node [cit], which is much less susceptible to high-amplitude transient spikes as are often seen with an analog switch."
"a b figure 4 . limited performance of brsa at very low snr and small amount of data. (a) the average correlation between the off-diagonal elements of the estimated and true similarity matrices (mean ± std) as the number of simulated subjects increases. each simulated subject had one run of data. legend shows average snr in task-responsive voxels. half of the voxels do not include any signal related to the design matrix. the correlation reaches asymptotic levels slightly below 1 with increasing numbers of participants except when the snr is extremely low (0.07), indicating that the bias is not fully eliminated.(b) the average correlation between the estimated similarity matrix and the expected bias structure assuming white noise. the estimated similarity structure is most dominated by the bias structure at the lowest snr simulated (0.07). the negative correlation at the highest snr reflects the weak negative correlation between the true similarity structure and expected bias structure (-0.055) fig 3e. this indicates that the cross-validation procedure is relatively 502 conservative. fig 5b shows only. the means and standard deviations of the t-statistics across simulated groups for 508 all simulation configurations are displayed in fig 5c. the differences in cross-validation 509 scores between full and null models are displayed in fig 5d. 510"
"in addition to improving operational efficiency, technology can be used to adapt a service to satisfy a specific customer or persona by personalizing it. the degree to which a person-to-person service can be personalized is limited by the extent to which the frontline employee is able to interact with the customer to obtain information about the customer's requirements and preferences [cit] . likewise, personalization depends on the customer's willingness or ability to provide the information. in some situations, this is limited by concerns that the service provider can't be trusted to maintain it in a private and secure manner. finally, even if the customer provides the information, personalization is constrained by how much of it is maintained by the service provider in an accessible and technology-supported format."
"analyzing person-to-person service encounters as information exchanges might seem to ignore the essence of highly experiential service. nevertheless, even for experiential services, it is almost always necessary for the two actors in a service encounter to engage in some amount of information exchange to identify requirements or expectations, to clarify their roles, or assess the status or quality of the service delivery."
"bayesian rsa significantly reduces bias in the estimated 284 similarity 285 as shown above, the covariance structure of the noise in the point estimates of neural 286 activity patternsβ leads to bias in the subsequent similarity measures. the bias can 287 distort off-diagonal elements of the resulting similarity matrix unequally if the order of 288 task conditions is not fully counterbalanced. in order to reduce this bias, we propose a 289 new strategy that aims to infer directly the covariance structure u that underlies the 290 similarity of neural patterns, using raw fmri data. our method avoids estimatingβ 291 altogether, and instead marginalizes over the unknown activity patterns β without 292 discarding uncertainty about them. the marginalization avoids the structured noise 293 introduced by the point estimates, which was the central cause of the bias. given that 294 the bias comes not only from the experimental design but also from the spatial and 295 temporal correlation in noise, we explicitly model these properties in the data. we 296 name this approach bayesian rsa (brsa) as it is an empirical bayesian method [cit] 297 for estimating u as a parameter of the prior distribution of β directly from data."
"for instance, in fig 3e, at the lowest snr and least amount of data (top left 428 subplot), the true similarity structure is almost undetectable using brsa. is this due to 429 large variance in the estimates, or is it because brsa is still biased, but to a lesser 430 degree than standard rsa? if the result is still biased, then averaging results across 431 subjects will not remove the bias, and the deviation of the average estimated similarity 432 structure from the true similarity structure should not approach 0. to test this, we 433 simulated many more subjects by preserving the spatial patterns of intrinsic fluctuation 434 and the auto-regressive properties of the voxel-specific noise in the data used in fig 3, 435 and generating intrinsic fluctuations that maintain the amplitudes of power spectrum in 436 the frequency domain. to expose the limit of the performance of brsa, we focused on 437 the lower range of snr and simulated only one run of data per \"subject\". fig 4a 438 shows the quality of the average estimated similarity matrix with increasing number of 439 simulated subjects. the average similarity matrices estimated by brsa do not 440 approach the true similarity matrix indefinitely as the number of subjects increase. 441 instead, their correlation saturates to a value smaller than 1. this indicates that the 442 result of brsa is still weakly biased, with the bias depending on the snr. it is possible 443 that as the snr approaches 0, the estimatedû is gradually dominated by the impact 444 of the part of x 0 not orthogonal to x. we reason that this is partly because the 445 algorithm [cit] we used to estimate the number of components in x 0 is a relatively 446 conservative method. in particular, in this simulation, the number of components of 447 simulated intrinsic fluctuations were 20±4, while the number of components estimated 448 from these simulated data by the algorithm were 13±3. however, empirically this 449 algorithm [cit] yields more stable and reasonable estimation than other methods we have 450 tested [cit] . it should be noted that brsa still performs much better than standard 451 rsa, for which the correlation between the estimated similarity matrix and the true 452 similarity matrix never passed 0.1 in these simulations (not shown)."
"the raw data was comprised of the rsa and vemg. as we can see at fig. 6, the comb filter was not able to satisfactorily filter out the rsa and resulted in false positive vemg signals. it is seen that the 6th-order gs pef and ssaicf are capable of reducing the rsa power to that of vemg at most frequency bandwidth."
we assume that the covariance of the multivariate gaussian distribution from which 703 the activity amplitudes β i are generated has a scaling factor that depends on its 704 pseudo-snr s i :
"with the derivative 21, the total log likelihood 18 can be maximized using 768 gradient-based method such as broyden-fletcher-goldfarb-shanno (bfgs) algorithm 769 to search for the optimal l [cit] ."
"what this all means is that the key strategy and design decisions for multichannel services concern the allocation of services to one or more channels and the manner in which the channels fit together. these decisions ultimately are implemented in terms of the content, direction, and reciprocity of information exchange between the channels. making these decisions and communicating the resulting design to customers requires design concepts and notations that depict a unified cross-channel view of the service system. a promising new approach here is an extension of the service blueprinting technique to use a \"service interface link\" symbol to interconnect the separate blueprints for different channels at the points where the process of service delivery moves from one channel to another [cit] ."
"the filtering process described in eq. (4) requires k additions, where k is the number of event peaks within the time period covered by the m-sample reference input. the lms update procedure described in eq. (5) also requires k multiply-and-accumulate (mac) operations. therefore, the computational cost of the proposed algorithm is much lower than that of conventional adaptive digital filtering methods with the same order. table 1 'm' is the number of samples in a \"suitably\" representative data sequence, 'n' is the adaptive filter length and 'c' is the number of sample points of impulse signal that is the reference input of the ssaicf"
"it is useful to consider multiple points of view when designing a service system, but it is essential to select one as primary. the choice shapes the priority of design requirements, constraints, and information sources; suggests relevant design patterns; identifies the front and back stages; and has profound implications for the creation and capture of value. for example, consider the restaurant service system, defined from the customer's point of view, with the front stage in the dining room and the back stage in the kitchen. almost the same facility could be used as a cooking school, but in that service system the customers are the student cooks, the front stage is the kitchen, and the people eating the food in the dining room are back stage feedback to the cooks."
"a simulation study was carried out to test the performance of the proposed method. in the case of vemg corrupted by the rsa, the emg signal comprises the rsa and background vemg signal. exact form of them has been unidentified so that it is hard to conduct a quantitative evaluation of the performance of the rsa estimation and elimination by the proposed method."
"\"service\" once only implied face-to-face interactions between two people, one offering the service and the other receiving it. today service domains and interactions are vastly more complex. \"service systems\" combine and integrate the value created in different design contexts like person-to-person encounters, technology enabled self-service, computational services, multi-channel, multi-device, and location-based and context-aware services [cit] . most service designers are familiar with some of these contexts, and each context has a research and practitioner literature that highlights their characteristic design concerns and methods. but few service designers are familiar with all of them, and because the design concerns and methods in one context can seem incompatible with those in others, there is relatively little work that analyzes design concerns and methods that span multiple contexts. this paper argues that for the substantial subset of service systems that can be described as \"information-intensive,\" it is desirable to take a more abstract view of service contexts that highlights what person-to-person, self-service, and automated or computational services have in common rather than emphasizing their differences. the view reveals the intrinsic design challenges that derive from the nature of the information required to perform a service, and emphasizes the design choices that allocate the responsibility to provide this information between the service provider and service consumer. taken together, the information requirements and the division of labor for satisfying them determine the nature and intensity of the interactions in the service system. this more abstract approach that applies to all contexts overcomes many of the limitations of design approaches that focus more narrowly on the distinctive concerns of each context."
"spatial and temporal structure of realistic fmri noise. to make the estimation task 360 even more challenging, we simulated a situation in which within the roi ( fig 3b; we 361 took the lateral occipital cortex as an roi in this simulation, as an example) only a 362 small set of voxels respond to the task conditions ( fig 3c) . this is to reflect the fact 363 that snr often varies across voxels and that an roi is often pre-selected based on 364 anatomical criteria or independent functional localizer, which do not guarantee that all 365 the selected voxels will have task-related activity."
"estimating shared representational similarity across participants 405 as mentioned above, brsa can be extended to jointly fit the data of a group of 406 participants, thus identifying the shared representational similarity structure that best 407 explains the data of all participants. this is achieved by searching for a single u that 408 maximizes the joint probability of observing all participants' data (group bayesian rsa 409 ;gbrsa). the rationale or gbrsa is that it searches for the representational structure 410 that best explains all the data. using all the data to constrain the estimation of u 411 reduces the variance of estimation for individual participants, an inspiration from 412 hyper-alignment [cit] and shared response model [cit] . fig 3h shows that the similarity 413 structure recovered by gbrsa has slightly higher correlation with the true similarity 414 structure than the average similarity structure estimated by other methods, across most 415 of the snr levels and amounts of data. cross-run rsa performs better only at the 416 highest simulated snr. however, low average snr is common in many brain areas and 417 this is where (g)brsa offers more power for detecting the true but weak similarity intrinsic fluctuation x 0 and the design matrix x are perfectly orthogonal, part of the 424 intrinsic fluctuation cannot be distinguished from task-related activity. therefore, the 425 structure of β 0, the modulation of intrinsic fluctuation, could also influence the 426 estimatedû when snr is low."
"a service system for a \"smart multi-channel bookstore\" called \"bookland\" was recently designed as a course project by a team of graduate students at the university of california, berkeley [cit] . the team -devin blong, jonathan breitbart, julian couhoult, and jessica santana -assumed the role of consultants to a large chain bookstore that also has a web retail site. their goals were to improve customer satisfaction, increase sales, improve the efficiency of store operations, and enable the company to gather more useful marketing information. the bookland service system is similar in many ways to the hypothetical service system implied in the presentation of the seven contexts in the third section of this paper, but the presentation here will emphasize more of the aspects of the service system that operate in the physical bookstore."
"these back stage fulfillment services are interconnected and coordinated by information exchanges among the online retailer and other businesses. the customer's expectations about the service outcome -in this case, the delivery of the book as promised -can be managed by providing him with information about the progress or state of these services. for example, the customer can be emailed the shipment tracking number from the delivery service. the customer's order of the book might have been prompted by a message from the \"just published\" alerting service that notifies customers when new books by their favorite authors are available. furthermore, this \"just published\" service was triggered by another back stage event when the package of new books was scanned on arrival at the warehouse."
"to derive the log likelihood of l for data of all voxels in the roi, we need to 724 marginalizing all other unknown parameters. below, we marginalize them step by step. 725 by marginalizing β 0i, we have"
"within a bayesian framework, inferences are made about the parameters of interest by a probability distribution given the data [cit] . bayesian framework is applied to estimate the values of unknown parameters of a model about which some prior information may be available. according to the bayes' theorem, the posterior distribution of is proportional to the product of the likelihood function and the prior distribution function [cit] . bayes' formula can be written as follows:"
"so an alternative to \"device family\" or \"model-based\" design is \"native\" design. this approach defines and implements the user experience and interface for each device to take maximal advantage of its capabilities. a telephone-based application that was originally designed to use a standard touch-tone keypad would also visually display the menu choices on mobile phones with a display screen. devices without keyboards, or with very small ones, would rely on voice input. mobile phones equipped with cameras and qr-code (2-dimensional barcode) detection systems can take photos of the codes on objects or in advertisements and retrieve related web pages [cit] . devices with sophisticated audio processing capabilities can even use music as inputs, as does the shazam service, which identifies a song from a recorded snippet [cit] )."
"as shown in fig. 4, there appears to be little difference between the qualities of the different filters, but there needs to be some quantitative evaluation of how the different methods perform. therefore, we used two parameters of correlation coefficients (between the simulated vemg and the estimated vemg) and mse (mean square error) for quantitative evaluation of two methods. as correlation coefficients of two methods was each 0.8179 and 0.8225, the result of ls method shows slightly higher correlation than ssaicf. mse of ssaicf and ls filter was each 0.0316 and 0.0291. consequently, ls filter showed slightly higher performance than ssaicf. however, unlike the ssaicf, the ls method requires more complex calculation than the ssaicf to estimate vemg."
"most people also use one or more other devices other than personal computers to obtain information services. in fact, many times more people in the world use mobile phones than personal computers, and many use pdas or other devices. these devices differ on multiple dimensions -computing power, memory capacity, portability, display size and resolution, voice recognition and synthesis capability, network bandwidth, gps capability, and so on. these capabilities are not always correlated and bundled into devices in the same combinations. some devices are optimized for different services, applications and information types. other devices strive with mixed success to be hybrid gadgets that combine a phone, camera, email, music player, game console, personal information manager, and computer applications platform."
"because of the complexity of these design problems, there is little consensus about the best approach for designing services to run on multiple devices or platforms. the earliest web browsers on mobile phones and pdas weren't very capable, so many web sites and services employed a design philosophy that could be called \"dumbing down\" or \"graceful degradation\" [cit] . sites and services designed for the most capable platform or device were adapted to other devices by applying transformations that systematically changed the user interface for more constrained devices. for example, web pages would be reformatted to fit small screen displays and eliminate navigation and selection controls that no longer worked well. for information in non-text formats, reduced display capabilities required reductions in content fidelity and resolution, and media compression and transcoding might be necessary, sometimes even dynamically [cit] ."
"linear logic (ll) with subexponentials (sell) [cit] shares with ll [cit] all its connectives except the exponentials ! and ?. figure 1 presents the introduction rules of the fragment of intuitionistic sell that will be used here. as one can observe from these rules, in particular in the ⊗ r rule, ll formulas are not always allowed to contract and weaken. these rules are controlled in ll by the exponentials !, ? and in sell by the subexponentials, written as ! a, ? a, where a is a label."
"many services are associated with information artifacts as tangible evidence, such as the service provider's business license hanging in the office, or invoices, receipts, warranties or diplomas given to the customer when the service is completed. for the most information-intensive services, the creation or processing of information is the sole intrinsic evidence of a service, so most of them are essentially invisible, and secondary information like transactional logs can be used to give them some persistence. this invisibility of information-intensive services no doubt contributes to the bias evident in most blueprints toward front stage services that more visibly produce customer value."
"to simulate the fmri noise in fig 4, we first estimated the number of principal 941 components to describe the spatial noise correlation in the 24 resting state fmri data 942 from hcp databse using the algoritm of gavish and donoho [cit] . the spatial patterns 943 of these principal components were kept fixed as the modulation magnitude β 0 by the 944 intrinsic fluctuation. ar(1) parameters for each voxel's spatially indepndent noise were 945 estimated from the residuals after subtrating these principal components. for each 946 simulated subject, time courses of intrinsic flucutations were newly simulated by 947 scrambling the phase of the fourier transformation of the x 0 estimated from the real 948 data, thus preserving the amplitudes of their frequency spectrum. ar(1) noise were 949 then added to each voxel with the same parameters as estimated from the real data. to 950 speed up the simulation, only 200 random voxels from the roi in fig 3b were kept for 951 each participant in these simulations. among them, 100 random voxels were added with 952 simulated task-related signals. thus, each simulated participant has different spatial 953 patterns of β 0 due to the random selection of voxels. 500 simulated datasets were 954 generated based on the real data of each participant, for each of the three snr levels. 955 in total 36000 subjects were simulated. the simulated pool of subjects were sub-divided 956 into bins with a fixed number of simulated subjects ranging from 24 to 1200. the mean 957 and standard deviation of the correlation between the true similarity matrix and the 958 average similarity matrix based on the subjects in each bin were calculated, and plotted 959 in fig 4a. 960 all snrs in fig 3 and fig 4 were calculated post hoc, using the standard deviation 961 of the added signals in the bounding box region devided by the standard deviation of 962 the noise in each voxel, and averaged across voxels and simulated subjects for each level 963 of simulation."
"that is, one can specify the subexponentials that behave linearly, namely those in i \\ u, and those that behave classically, namely those in u ."
"9. marketing and promotions system. marketing and promotion services dynamically develop customized recommendations, coupons, promotions, and bundles for each customer. these will be displayed online when the user logs in or accesses kiosks in physical stores. 10. inventory system. the real-time inventory system tracks the number and locations of all items in each store and warehouse. it generates fetch and restock alerts to employee dashboards and reorders items according to business rules. 11. customer profile. all customer browsing and purchasing information (both online and offline) is fed to a customer's profile."
"this structure in turn biases the covariance structure of these point estimates, and a 557 bias persists in the similarity matrix. such bias is especially severe when the snr is low 558 and when the order of the task conditions cannot be fully counterbalanced."
"so as to deal with spatial information, formalisms such as bio-pepa [cit], bionetgen [cit], bioambients [cit], and brane calculi [cit] have been equipped with a tree representation of the hierarchical structure of cellular compartments. whereas in biocham [cit], pl [cit], and beta-binders [cit], cellular compartments can be abstracted as symbolic locations by assigning labels to molecular compounds. in the π@-calculus [cit], restricted names are exploited to model compartments."
"our generative model of fmri data follows the general assumption of glm. in addition, 691 we model spatial noise correlation by a few time series x 0 shared across all voxels. the 692 contribution of x 0 to the i th voxel is β 0i . thus, for voxel i, we assume that"
because we made the assumption that i is independent across voxels. the log 752 likelihood for all data is the sum of the log likelihood for each voxel.
"people don't expect their service experiences to be identical on pcs, phones, and pdas, but they expect them all to be satisfactory and to exhibit some degree of consistency or predictability. unfortunately, \"consistency\" and \"predictability\" are often difficult to define [cit] . even when these goals can be specified in design terms, the differences among devices influ-ence the user interface for obtaining the service, the user interface through which it is delivered, the informational content of the service, and the latency of service delivery [cit] . furthermore, the same device or application might operate in both \"always connected\" and \"occasionally connected\" modes, which imposes the challenges of synchronizing information flows and switching transparently between local data storage and network service [cit] ."
"the customer's experience in the online bookstore seems superficially equivalent to the one in the local bookstore. he chooses a book (or accepts a recommendation), enters his credit card number and address into the shopping cart form, and with a couple of mouse clicks completes the check-out process."
"the delineation and quantification of brain lesions is critical to establishing patient prognosis, and for charting the development of pathology over time. typically, this is performed manually by a medical expert, however automatic methods have been proposed (see [cit] for review) to alleviate the tedious, time consuming and subjective nature of manual delineation. automated or semi-automated brain lesion detection methods can be classified according to their use of multiple sequences, a priori knowledge about the structure of normal brain, tissue segmentation models, and whether or not specific lesion types are targeted. a common feature is that most methods are based on the initial identification of candidate regions for lesions. in most approaches, normal brain tissue a priori maps are used to help identify regions where the damaged brain differs, and the lesion is identified as an outlier."
"a restaurant customer might launch the wine snob on his pda, and might ask the sommelier for a confirmation or second opinion. this last scenario, in which the customer provides his own ad hoc technology to enhance a service encounter, is increasingly common but not easy to systematize because by definition it was not expected by the service provider (if it had been expected, the encounter would be a \"technology-facilitated\" one). perhaps \"customer technology improvised\" is an appropriate category for this type of technology-enhanced service encounter. and of course, the customer might access the \"wine snob\" application from his home computer before heading out to dinner (self-service context)."
"the pulse amplitudes of the fes signal can be 1k-10k times greater than the vemg signals from the muscle of interest [cit] . these much greater signal amplitudes will saturate a conventional emg amplifier in attempting to measure vemg from sensor electrodes placed adjacent to stimulation electrodes. this poses a significant problem for an autogenic emg-controlled fes (aemgcfes) device due to positive feedback errors. the saturation of fes, which means that the electrical stimulation voltage would go up to the maximum, comes from the presence of positive feedback error."
"where ( ) c n and ( ) g n are the sampled versions of the pure emg signal and the rsa signal in the emg channel. to estimate the rsa signal in the emg channel, the minimum mean-square error (mmse) criterion can be applied to the models in eq. (2).using eq. (2), the m-wave in the emg channel can also be modeled as"
"one of the main difficulties of building computational models for biological systems arise from the characteristics of the available information. indeed, even for the beststudied systems, the known data cannot describe exhaustively the properties of each molecular species; even less known are the details of spatial information and the timing of events. thus, desirable features of a computational modeling framework should regard the capability of dealing with information often both incomplete and of non-uniform quality."
"in a realistic simulation using real fmri data as background noise, we showed that 573 brsa generally outperforms standard rsa and cross-run rsa, especially when snr is 574 low and when the amount of data is limited, making out method a good candidate in 575 scenarios of low snr and difficult-to-balance tasks. because temporal and spatial 576 correlation also exist in the noise of data from other neural recording modalities, the 577 method can also be applied to other types of data when the bias in standard rsa is of 578"
"spinal cord injury (sci) and stroke patients frequently are left with a partially paralyzed, or paretic arm. rehabilitation of these survivors can be enhanced with functional electrical stimulation (fes).the goal of fes is to restore functional movement and requires that the fes system be under the individual's control and reliable enough to improve the subject's function during normal daily activities [cit] . some studies have demonstrated value of adding emg feedback [cit] . these therapies should encourage patient use outside of the rehabilitation center and reduce the need for frequent intervention by medical personnel. non-invasive, reversible rehabilitative systems like surface electrical stimulation of muscles are especially well suited due to their easy application and utilization in therapeutic applications [cit] . therefore, a biofeedback signal like emg for fes control is a critical component. extensive research has been done using fes to improve the lives of persons with weak or paralyzed muscles following neurological insult. use of natural signals to control fes has been performed in at least three ways. first, emg-triggered fes detects a threshold emg signal to trigger onset of a predetermined stimulation sequence with no further emg measurements made during stimulation [cit] . secondly, emg-controlled fes continuously measures emg from one muscle to proportionally stimulate another muscle [cit] . finally, autogenic emg-controlled fes (aemgcfes) specifically refers to measuring the voluntary emg (vemg) from the stimulated muscle, allowing more physiologically appropriate closed loop control [cit] ."
"a male subject (36 yrs old, 83.9 kg) gave informed consent to participate in this study approved by the georgia institute of technology irb (protocol #h10018). the raw emg signal was recorded using pre-gelled disposable bipolar pellet surface electrodes (ag-agcl, 10mm diameter, noraxon, oh) which were placed with an inter electrode distance of 2 cm along the extensor carpi ulnaris muscle and the ground electrode was positioned over the bony protuberance on the elbow. we verified accurate placement of the recording electrodes with a qualitative check of the recorded emg signal during voluntary wrist extension. we placed self-adhesive stimulating surface electrodes (2.54cmx2.54cm, medical supplies shop, ny) with an inter electrode distance of 5cm. the raw emg signal and force transducer data were sampled at 16khz using data acquisition hardware (national instruments, pci-6289daqcard) and software (mr. kick, sensory motor institute, aalborg university, fig. 3 ).we set the stimulation frequency to be 25hz.the evaluation of the system performance, the subjects were asked to control their wrist extension torque using a target-force paradigm. following the recording of the maximum voluntary contraction (mvc), the subjects were asked to follow, as best as they could, a sinusoidal trace with maximal amplitude of 50% of their mvc. the tracking test is based on a visual feedback of the produced force compared to a reference target trajectory using a computer monitor. the outcome measure to determine the system efficacy would be the accuracy of the tracking."
"the information flow through all of these contexts is shown in figure 2, which schematically combines a floor plan for a physical bookstore with the online one and some of the important information sources and services. the tight integration and recurrent information flows between the two channels highlights the multichannel essence of the bookland service system. when items are moved from the shelf, their location can be tracked anywhere in the store. 2. self-service kiosks. kiosks are located near the bookstore entrance (#2 in figure 2 ) and throughout the store (#7), where customers can wave/swipe their membership cards or type in their customer id to log in. these kiosks display personalized welcome pages and tailored promotions (see figure 3) . they also provide search and browsing functions and display real-time inventory and location information on a store map. customers can scan any item in the store at a kiosk to obtain additional information, such as customer and editorial reviews and related items, at which time the book is added to the customer's profile. the kiosks allow customers to build shopping lists and can print the store maps. if an item is not in stock in the current store but is available online or at other store locations, the kiosk will suggest alternative ordering/purchasing options, including home delivery, shipping to the current store, or pickup at nearby retail locations where the item is in stock. all customer searching, browsing, and purchasing activity on the store kiosks is combined with similar activities online and added to the customer's profile (see #11 below). frontline employees use the bookstore management system's dashboard to display the customer's name, profile information including purchase and browsing history (both online and offline), and a list of tailored promotions for the customer. the dashboards also provide real-time inventory and location information for any item in the store."
"a web channel, however, can offer many of the same services as the physical channel along with additional personalization. this greater capability and opportunity raises fundamental business model concerns about channel conflict, sales cannibalization, customer segmentation, marketing, branding, and cross-selling [cit] . the service customer's experiences and expectations about functionality and quality are synthesized from every encounter across all channels, making the predictability of interactions important [cit] ). however, cross-channel predictability is constrained by differences in channel capability, and if those didn't exist, there would be no point in having multiple channels! consumers go online to do product research and to learn where to buy things or find service providers. consumers might shop for particular brands, but they don't always buy them from the same retailer. multi-channel retailers, on the other hand, want customers to treat their different channels as complements or substitutes for each other, because this will increase sales and strengthen loyalty [cit] . so many firms offer a \"ship to store\" or \"local pickup\" service that allows a customer to purchase or reserve a product in the online channel but obtain faster delivery from the physical channel. likewise, a \"return to store\" policy allows a purchase made and fulfilled from the online channel to be returned to the neighborhood store if it turns out to be unwanted or unsuitable. these services are simple to describe, but not easy to implement, because the ideal supply chains for online and physical channels are different [cit] ."
"not taking the infiltration in excavation face into account, the overburden strata are assumed to be permeable with high permeability, such as sand and gravel. so a complete hydraulic connection exists between the river water and groundwater. in this sense, the pore water pressure generated by river water can be expressed as"
"and s i .t represents the context for the formulas of the form ! s i .t f . note that the negative phase ends here since eqs is a negative formula (that must be introduced in the positive phase) and ct(·, ·) is an atom."
"if complex service systems are assembled from different service contexts that share the unifying principle of information exchange, then the creation of value in the service system can be described in terms of the content and choreography of information flows within and between the contexts and their component services. the actor or service at the end of an information flow, often the stereotypical \"end user\" or \"customer,\" is usually designated as the focal point of the service system, especially when the service system contains a person-to-person context. service design techniques like blueprints or storyboards emphasize this customer-centric point of view."
"the recognition that services vary according to both the absolute and the relative proportions of physical, interpersonal, and information actions is a critical insight. the most information-intensive services are entirely information-based, with no physical or interpersonal interactions required to carry them out, and can be readily automated as information systems, web services, or computational agents."
"this more abstract modeling philosophy of document engineering and soa contrasts sharply with traditional service blueprinting and other front stage or \"customer-centric\" approaches in some important ways. first, it de-emphasizes the differences among person-to-person, self-service, and automated or computational services, because this makes it easier to treat them as substitutable [cit] ). this assumption of potential equivalence is well-supported by modeling notations like sequence and activity diagrams [cit] that take a \"bird's eye\" or top-down perspective that doesn't automatically make the human customer the focus of the process model."
"when a customer in the local bookstore chooses a book (or accepts a recommendation for one), pays for the book with a credit card, and leaves the store with it, the customer's service encounter to purchase a book is complete."
"in contrast, in experience-intensive service contexts, the interpersonal interactions between the human participants are the most important things for designers to study. nevertheless, experiential service domains often have documents playing essential roles; for example, it would be difficult to understand a restaurant service system without analyzing menus and following customer orders from the dining room to the kitchen."
"many of the most complex service systems being built and imagined today combine person-to-person encounters, technology-enhanced encounters, selfservice, computational services, multi-channel, multi-device, and location-based and context-aware services. the research reported in this paper has examined the characteristic concerns and methods for these seven different design contexts to propose a unifying view that spans them, especially when the service-system is \"information-intensive.\" a focus on the information required to perform the service, how the responsibility to provide this information is divided between the service provider and service consumer, and the patterns that govern information exchange yields a more abstract description of service encounters and outcomes. this makes it easier to see the systematic relationships among the contexts that can be exploited as design parameters or patterns, such as the substitutability of stored or contextual information for person-to-person interactions."
"for data within one run, σ −1 i, the inverse matrix of the covariance of i, is a banded 716 symmetric matrix which can be written as"
"it is surprising that spatial whitening, that is often recommended [cit], in fact hurts 612 the result of standard rsa and cross-run rsa in our simulation. this may be because, 613 in our simulation, the spatial correlation of noise is not the same as the spatial 614 correlation in the simulated signal. while whitening reduces the correlation between 615 noise in the estimatedβ of different voxels, it may cause undesired remixing of true 616 signals between voxels. as discussed before, in practice, it is difficult to know whether 617 the intrinsic fluctuation and task-evoked signals share the same spatial correlation 618 structure, because we do not know the ground truth of signals in real data. the cost 619 and benefit of spatial whitening on standard and cross-validated rsa therefore awaits 620 more studies. instead of performing spatial whitening, brsa estimates a few time 621 series x 0 that best explain the correlation of noise between voxels and marginalizes 622 their modulations in each voxel. without remixing signals across voxels, it still captures 623 spatial noise correlation."
"we model the state of the system at time-unit t (i.e., the concentration of each specie in each space at time t) as the formula"
"in order to combine spatial and temporal modalities in sell we need first to define a subexponential signature as the one depicted in figure 2 . the only unbounded subexponentials are t ω (time) and s ω (space). the former will be used to mark the set of reactions that can be used as many times as needed. the second will be used in the encoding of p-system in section 4.2. the linear subexponentials 1, 2, 3, · · · represents temporal time-units. the subexponentials i+ represent the time-units starting from i. for instance, a subexponential variable l x : 4+ can be instantiated with any time-unit (in the future) starting from 4. those subexponentials will be used to specify system's properties as we explain in section 4. finally, the linear subexponentials s a .i will be used to mark the formulas (reactants) available in the space domain s a in the time-unit i."
blueprinting advocates suggest that every touch point should also be associated with tangible evidence that demonstrates or signals that the service is being delivered or co-created. person-to-person services that require substantial physical interactions have a great deal of intrinsic tangibility; clean and pressed clothes are clear evidence that a dry cleaning service was performed as expected. the physician's white coat and similar characteristic uniforms for other service providers tangibly reinforce quality expectations.
"the numerical order of the contexts defines a typical design trajectory for service systems, as was demonstrated by the progressive complexity of the service system implied by the seven bookstore scenarios that accompanied the presentation of contexts 1-7 in the third major section of this paper."
"pcm assumes all voxels within one roi have equal snr. however, typically only a 646 small fraction of voxels exhibits high snr [cit] . therefore, it is useful to model the noise 647 property and snr of each voxel individually."
"the bias demonstrated in this paper does not necessarily question the validity of all 583 previous results generated by rsa. however, it does call for more caution when 584 applying rsa to higher-level brain areas for which snr in fmri is typically low, and 585 when the order between events of different task conditions cannot be fully 586 counterbalanced. this is especially the case with decision making tasks that involve 587 learning or structured sequential decisions, in which events cannot be randomly shuffled. 588 even when the order of task conditions can be randomized, it may not be perfectly 589 counter-balanced. thus, a small deviation of the bias structure from a diagonal matrix 590 may still exists. if the same random sequence is used for all participants, the tiny bias 591 can persist in the results of all participant and become a confound. therefore, it is also 592 important to use different task sequences across participants."
"instead of starting with a person-to-person service (context 1) and adding technology contexts to it, an alternative evolutionary trajectory for service systems begins with a context 6 back stage or computational service. many enterprise applications, transactional systems, or sensors associated with objects or equipment generate information that is important to effective business operations but which might not initially be exposed in customer-facing interfaces. making this information available for self-service access (context 3) via the telephone, personal computer, or other device (context 5) can create substantial new value, often enough to justify a secondary customer support channel to handle exception or error cases (context 4). self-service package tracking is an extremely successful example of this pattern where a customer-facing service was created to exploit latent value from invisible back stage services."
"this method would be one that is able to sense the volitional intent to activate a paretic muscle and then translate that intent into an appropriate muscular activation and functional movement. this would provide the most natural way to reinforce the normal physiological pathways for activating and controlling functional movements. a fundamental challenge preventing the realization of such a natural closed loop fes system is the inability to measure a vemg continuously and accurately from the stimulated muscle. therefore, because the stimulation intensity is modulated in proportion to vemg amplitude at each stimulus and the characteristic of the m-wave varies, the key problem lies in being able to sense the vemg while ignoring the much larger amplitude stimulation artifacts that can create a problematic positive feedback loop [cit] ."
"more precisely, we shall show that it is possible to use two kind of subexponentials for representing the two main dimensions, namely time and space, for modeling biochemical systems where reactions depend on the location of reactants and on the duration of interactions. then, we show the expressiveness of our framework by encoding p-systems [cit], a general model of computation inspired on cells structures. we show that our logical characterization of p-systems has a strong level of adequacy, which means that derivations in the logical system follow exactly the rules (reactions) defined for the modeled system. we also show how to exploit the underlying logic for expressing, and proving, properties of interest that involve temporal and spatial modalities."
"normally the mean values of soil parameters are used in deterministic analysis. although the method can deliver accurate analytical results of support pressure, it requires the input parameters for every calculation point in situ and cannot control the spatial variability of the input parameters. in fact, geotechnical materials are natural materials, and their properties are affected by various spatially variable factors during their formation processes. the inherent spatial variability has been considered as one of the major sources of uncertainties [cit] and can be modeled using stochastic analysis [cit] ."
"each context is introduced with a scenario from a bookstore service setting, and each successive scenario builds on the previous ones to define a progressively more complex service system."
"the covariance matrix u can be parameterized by its cholesky factor l, a 347 lower-triangular matrix. to find theû that best explains the data y, we first calculate 348 thel that best explains the data by optimizing the marginal log likelihood:"
"other information-intensive services also involve essential personal or physical interactions, including traditional classroom education, emergency and surgical healthcare, logistics, sales, consulting, and personnel resources administration. furthermore, service types that are dominated by physical or interpersonal actions, such as physical therapy, massage, restaurant dining, and entertainment -and which are thus \"experience-intensive\" --usually require information exchanges to specify and co-produce the service."
"in the case when the rsa corrupts actual emg, the ls method and the proposed method were compared. fig. 5 . shows the result of analysis using a real data set. the patient's actual rsa contaminated vemg that is controlled by feedback signal that is shown in fig. 5(a) is shown in fig. 5(b) . the vemg signal estimated by eliminating rsa signal using the above 2 methods are shown in fig. 5(c) and fig. 5(d) . and the force signal estimated by the above 2 methods are shown in fig. 5(e) and fig. 5(f) . the result illustrates that above both methods well estimate the information of the vemg signal. as seen in fig. 5(c)~(f), the result by the ssaicf is similar to that of the ls method. for more quantitative evaluation of filter performance, we used correlation coefficients and mse between filter response and the force signal measured by a force sense. after that, values of two indexes are compared. correlation coefficients of the ssaicf and ls method are each 0.9545 and 0.9510. also mse of the ssaicf and ls method are each 0.0063 and 0.0065. in spite of lower calculation cost, the result of ssaicf shows slightly higher correlation than ls method. consequently ssaicf estimate the vemg based force signal a little better than ls method. this result is very significant result. 5 . the result of analysis using a real data set: (a) feedback signal to induce vemg; (b) a real data set of rsa corrupted vemg signal; (c) the estimated vemg signal using the ssaicf; (d) the estimated vemg signal using the ls algorithm; (e) the estimated force signal using the ssaicf; (f) the estimated force signal using the ls algorithm"
"an online bookstore can offer many services and a richer user experience to users on the home computers, but it wants to enable them to browse for books using their mobile phones. how should the catalog content and user interface be designed for multiple platforms?"
"if the stimulation by the emg system is applied to muscles, the m-wave induced in emg signal ( ( ) g n ) can be expressed as"
"as the web matured as a platform for online commerce and information services, upstart firms like amazon.com with no physical presence became competitive threats to incumbents like barnes and noble. for these \"brick and mortar\" firms, creating a web channel and finding the right mix of \"bricks and clicks\" was an urgent and critical strategic decision, and the concept of \"multi-channel services\" as a distinct service design context emerged [cit] . the web channel also inspired the vision of \"e-government\" services that would radically improve service delivery to citizens and let them avoid inefficient faceto-face encounters in government offices [cit] ."
"the second key method of document engineering is its alignment with the idea of industry reference models or best practices, which it uses as design patterns that define normative or idealized services, their choreography, and the information exchanges needed to request and perform them."
"a customer gets a recommendation for a new book in an online bookstore but wants it the same day. can he reserve it online for pickup the same day in the neighborhood bookstore store? when he arrives at the store, should the bookstore employees know what other books he looked online but didn't purchase so they can offer them at a discount? when the customer next visits the online store, are purchases he made in the neighborhood bookstore reflected in his purchase history and recommendations there?"
"but these systematic changes in the character of the service system can reduce its experiential and relational quality. as a result, many service systems integrate person-to-person encounters and self-service (context 4) to satisfy the types of customers who prefer the former and to provide services that create additional value through the exchange of information between channels. banking and catalog shopping are two other categories of service that have followed this pattern of service system evolution."
"furthermore, a subscriber who leaves the system due to a crash failure, network partitioning, or simply becomes un− interested in its old subscriptions but fails to appropriately unsubscribe may leave behind obsolete subscription entries at p/s brokers. the use of subscription leases enables bro− kers to automatically purge these entries from routing tables once they timeout and are not renewed by the clients."
"at a first glance, it might seem that the rss poll interval is the source of bt's long completion times. in order to in− vestigate this, we carried out another execution using bt in which all clients arrive immediately after the content is re− leased (marked by (5) in figure 12 ). despite this fast arrival rate which requires an impractically low rss polling inter− val, the downloads still took around 1700 seconds to finish (marked by (6) in figure 12 ). this implies that our approach still outperforms bt by about 30% lower download times even when there is a hypothetical mechanism that somehow trigger immediate downloads at bt clients. this significant gain is due to a combination of our initial segment seeding strategy and use of network coding for data dissemination."
"an important advantage of our peer−assisted data dis− semination approach is its scalability with the population of subscribers. this is due to the fact that the system band− width grows organically as more subscribers with overlap− ping interest in some published content are present. fig− ure 8 illustrates the network−wide aggregate throughput of the system in which a 100 mb file is published by one publisher and disseminated among all clients in configu− rations c−300 and c−1000. in the graph, configuration c− 1000 achieves three times network−wide throughput com− pared to c−300 (from about 40 mb/s to 130 mb/s). fur− thermore, despite more than three−fold increase in the num− ber of subscribers and aggregate volume of transferred data (from 30 gb to 100 gb), the total dissemination time only increases from 801 secs to 858 secs, or less than 8% in− crease. this demonstrates that a larger client population size has a marginal impact on delivery times to subscribers."
"we now study the impact of these factors experimentally using configuration c−120. we first ran the system using publiy + to disseminate 100 mb of published content from one source. next, we ran the system with the transmission bt [cit] and opentracker [cit], an open source bt client and tracker, respectively. the uplink bandwidth of the peers in all executions were capped at 100 kb/s. figure 12 illus− trates the results. the x−axis is the time since the release of the file and the y−axis is the percentage of peers who have started the download or finished receiving the entire 100 mb of the file. the graphs marked by (1) and (2) in− dicate the start and completion of client downloads in our approach. it can be seen that all clients start their download almost immediately after the content becomes available."
"to this end, we propose a hybrid two−layer architecture that combines the benefits of p2p content distribution with those of broker−based p/s systems. in our approach, we rely on p/s brokers to store clients' subscriptions and act as co− ordinators that guide clients with similar interests to directly engage in exchange of segments of content. direct peer ex− change enables us to tap into the bandwidth capacity avail− able at clients and relieve brokers from much of the burden of content forwarding. henceforth, we refer to the layer in which p2p content exchange takes place as the data layer, and the layer where brokers reside as the control layer. the two layers closely coordinate and act as one unified system. this way, we have knowingly avoided deploying separate services, one for notifying clients of release of some new content, and one for distribution of that content. as a mat− ter of fact, as we demonstrate in this paper, a close marriage between these two functionalities improves content deliv− ery times and eliminates the need to maintain two separate infrastructures."
"in this section, we report on our experimental evaluation results carried out using raccoon [cit] table 2 . evaluation setup. java−based prototype implementations of a multi−threaded coding engine and the p/s system that uses raccoon for bulk content dissemination, respectively."
"in addition to the windows and microsoft official accounts, most of the influencers were non-official users. hence, microsoft must quickly update these users with current information because many of their followers spread news regarding windows 8.1 provided by these influencers. fig. 2 shows that the top followers of the windows account were @microsoft_now, @windowsphone, @microsoftid, and @winobs. the top followers of the microsoft account were @microsoftasia, @dovellonsky, @microsoftth, @benthepcguy, and @lee_stott. in addition, the top followers that resent the tweets from the top followers of @windows and @microsoft were also illustrated in fig. 2 ."
"it is clear that a source must upload at least k data blocks for each content segment (here, k is 100). a direct con− sequence of our initial seeding dissemination strategy (see section 5.1) is that a source attempts to maintain this lower− bound by delegating dissemination of each segment to clus− ter of initial seeding subscribers. this allows the source to utilize its limited available bandwidth more effectively, i.e., to upload new segments as opposed to redundantly of− fer the same segments that were already injected in the sys− tem. this improves the dissemination process, especially in our usage which resembles an ultimate flash−crowd sce− nario. figure 9 illustrates the source's number of uploaded blocks per content segment (the network configuration is c−1000). the average uploaded blocks per segment is 136 which is just about 30% higher than the minimum. further− more, the standard deviation is also very small (about 21 blocks) which indicates a largely smooth and even effort on part of the source to disseminate each data segment."
"control layer: this layer consists of an overlay of p/s brokers that act as the core architectural components in our system. brokers maintain client subscriptions and guide subscribers with overlapping interest in similar content to engage in direct data exchange. in a simple scenario, a client connects to a broker that is deployed by its isp or belongs to the same administrative domain by performing a dns lookup or using a specialized directory service. we thus say that clients connected to a broker belongs to the same region as the broker. once connected, subscribing clients apply for a subscription lease (or many leases) from their regional broker. the subscription lease has an expiry date that is agreed upon by both client and broker and thus needs to be renewed periodically. the lease renewal pro− cess gives the broker the assurance of a client's presence in the system as well as its continued interest in the subscrip− tion over a long period of time. once a subscription lease request is granted by the broker, the client receives a lease acknowledgement message confirming that its subscription is now registered."
"our use of application layer brokers gives great flexibil− ity in coordination of clients and provides opportunities for shaping network's traffic patterns. to underscore the signif− icance of this capability, consider the case of p2p file shar− ing applications. a major portion of isps' operational costs associated with file sharing applications is due to the large volume of traffic that crosses isp domain boundaries. this is largely due to the nature of p2p file sharing applications which are oblivious to the clients' location in the system. 2 our broker−based approach helps mitigate such problems by having clients connect to brokers within their own isp domain and ensuring cross−regional seeding of content is brought to a minimum. this is achieved simply by reduc− ing the size of the push−lists sent between brokers in differ− ent regions. this way, regional seeding will represents the dominant bulk of data transfer within the network."
"in this study, we used twitter as the form of social media example to illustrate our approach. twitter is the most popular microblogging platform, and its prominence as a social media platform is growing [cit] . according to a report conducted by the pew research center [cit], 72% of u.s. adults online use social networking sites and 18% of them are twitter users. because of its great popularity, twitter has been exploited as a platform for the viral marketing of content, products, and political campaigns [cit] . in this case study, we used the windows 8.1 product launch event to demonstrate how the information from this event was disseminated on twitter. microsoft launched windows 8.1 on october 17, 2013, and a 1-month collection of tweets that included the key word \"windows 8.1\" was obtained by using the twitter search api. a total of 450,250 messages published by 206,792 users were collected. fig. 1 shows that nearly 60% of the related tweets were published during the first week after the windows 8.1 launch date. the daily tweets drastically dropped from more than 100,000 to less than 20,000 within 4 days. however, the number of tweets increased to more than 20,000 on to identify the social influencers of information dissemination regarding windows 8.1 event on twitter, we commenced with an investigation of the top users who posted the greatest number of related tweets. according to the results shown in table 1, the top 10 users (@windows, @hatiwin, @windowssupport, @atlazone, @arsenico, @notebookdeal, @softpediadriver, @winobs, @jorin5712, @scoop_india) posted an average of 723 related tweets during the first month following the launch of windows 8.1. however, the average number of tweets related to windows 8.1 posted by general users was only 2.18. hence, these influencers shared 100 times more related tweets than the average users did. among the top 10 users, only the users @windows and @windowssupport are from the official microsoft accounts. besides, the total tweets of the top 10 users account for only 1.60% of the investigated windows 8.1 tweets posted by 206,792 twitter users. this number indicates that numerous people were engaged in sharing information regarding the windows 8.1 launch event on twitter, and that this information dissemination was not limited to a small group of users. in addition, we identified the social influencers of information dissemination on twitter by investigating the number of related windows 8.1 messages in which other users mentioned the influencers. the mention mechanism on twitter is often used to establish conversations between users through the exchange of messages or simply by referring to a person in the message's text [cit] . table 2 shows a summary of the top 10 influencers of the windows 8.1 launch event based on the number of mentions. unsurprisingly, microsoft's official accounts @windows and @microsoft were among the most mentioned usernames in the related tweets. around 13.21% of the related tweets were disseminated from these microsoft's official accounts. although @ishibasystems, @youtube, @detiknews5, @sambung_cerita, @henextweb, @artem_klyushin, @mashable, and @windowsblog are not official microsoft company accounts, they were also mentioned by numerous twitter users. these non-official twitter users accounted for 16.65% of all mentions; therefore, microsoft may consider establishing close relationships with these influencers because they were crucial sources of information regarding the windows 8.1 launch event for twitter users."
"evaluation setup: for our evaluation, we used the scinet high performance cluster [cit] . each computing node on the cluster is equipped with 8 intel xeon 2.53 ghz cpu cores with 8 mb of l2 cache. in our deployments, each client and broker has exclusive access to one dedi− cated cpu core. cluster machines are connected using gi− gabit switches but our clients are configured with a capped upload bandwidth of 100 − 200 kb/s. this is enforced at the client's socket communication layer and creates a realis− tic scenario of limited bandwidth availability over the inter− net. our physical network also has a very low latency, but we believe that this is not a major issue in interpretation of our experimental results, especially since in our throughput− intensive usage scenario bandwidth is a far more significant determining factor."
"we use the term regional seeding and cross-regional seeding to refer to sending of data messages by a client to other peers in the same region, and in different regions, respectively. since the source is located only within one region, full system−wide content dissemination generally requires a combination of both regional as well as cross−regional seeding. further− more, since seeding clients are unaware of other peers who are interested in the content they possess, they send a re− quest message to their broker and include the metadata of the segments they are offering. the broker replies with a push-list message containing a list of interested clients to which the seed can push data messages."
"when the source receives a push−list, it contacts the peers referenced therein and starts to send, i.e., push, coded data messages. this is marked by ② in figure 4 . each data message includes one coded block of data, the random co− efficients used for coding as well as the unique content iden− tifier and the segment's byte offset that the coded data block corresponds to. receiving clients accumulate the coded blocks sent from all the seeds until k randomly independent coded blocks are available. at this point, the client decodes the blocks and reconstructs the original file segment. fur− thermore, the client sends a break message to the seeds in order to stop the data push and also notifies its own regional broker about successful reconstruction of the segment. the broker notes this information and refrains from referencing the clients in any future push−list for that particular segment."
"data layer: the data layer consists of all subscribing clients in all regions regardless of their specific subscrip− tion interests. in general, clients in this layer are completely oblivious to each other and purely rely on brokers (in the control layer) for instructions on how and when to commu− nicate with one another. more concretely, a source client who wishes to publish its content first contacts its own bro− ker for instructions on how to proceed. we refer to such a broker that the content source is connected to as the con− tent's home broker. as part of this interaction, the source re− ceives a list of other peers who are interested in its published content and to whom it must send the content. likewise, non−source clients who have received the content and want to contribute in the distribution process also contact their regional brokers in order to receive similar lists of peers in− terested in the content. in this regard, our system's control layer acts as a distributed registry service that coordinates dissemination of the content in the data layer by supply− ing the source and other seeding clients with lists of peers interested in the content. note that this is in contrast to con− ventional p/s designs in which brokers are directly involved in relaying publication messages. finally, our approach to engage clients in exchange of data packets allows us to tap into the otherwise unused clients' available bandwidth. this relieves the brokers from shouldering the heavy burden of traffic associated with forwarding of bulk content."
"before implementing this strategy, we observed signifi− cant number of packet dependencies in our test executions when early seeding was enabled. after implementing this strategy, the issue was entirely resolved even when peers start to seed after receiving a single data block of a segment."
"we distinguish between two types of nodes, namely clients and brokers. while the primary goal of clients is to publish or to receive some content, brokers are deployed as part of the infrastructure to help subscribing clients in their quest to receive their content of interest. for this pur− pose, brokers are placed at strategic locations, called regions, around the internet or within administrative domains to provide content dissemination service. as part of this task, brokers guide subscribers and facilitate exchange of data blocks between clients with mutual interest. based on the distinctive roles of clients and brokers, we develop a two−layer architecture consisting of data and control layers (depicted in figure 2 ). we next describe the layers in detail."
"a key objective in our design is to shape the traffic that flows within and between regions (see section 5.3). this is an important requirement for many practical scenarios. for example, file sharing traffic that crosses isp boundaries amounts to a significant portion of isp costs whereas the traffic that flows within an isp domain is virtually free."
"we have dealt with both these issues in our design: first, the reactive push−based nature of the p/s model provides a readily available mechanism to initiate downloads at sub− scribers and to shepherd them into direct exchange of data. this is reasonable for our usage scenarios, since compa− nies that offer the data dissemination service also provide the broker network infrastructure. second, the dissemina− tion strategy of section 5.1 boosts the bandwidth available to seed a given segment. this is especially effective in the early stages of dissemination of each segment. finally, the use of network coding allows subscribing peers to partici− pate in content dissemination as early as having received a single 10 kb data block. this is a much smaller size than the minimum transferable data units in bt (also called seg− ments) which is commonly 256 kb -1 mb in size."
"in addition, this hybrid design allows us to employ net− work coding which previous research has demonstrated to be a viable new direction [cit] . network cod− ing improves scheduling and exchange of packets and en− ables peers to participate in content dissemination as early as having received a single coded block of data. another desirable property of network coding is that it is resilient to packet loss and provides a virtually unlimited stream of coded packets each of which is equally useful (a.k.a. innovative) to reconstruct the original content. a client seek− ing to receive a missing piece of content can thus receive this data from any other peer who is also interested in the same content. in absence of network coding, however, the client's request could only be fulfilled by those peers who have previously received the exact same piece of the file. the search for such a peer can be time consuming. this adversely impacts the time to complete the download. fur− thermore, in comparison to the well−known bittorrent pro− tocol [cit], our solution includes strategies specifically tai− lored to lower dissemination delay by avoiding peer coordi− nation problems, and mitigate issues caused during flash− crowd scenarios [cit] . as a matter of fact, due to the reactive nature of the p/s model the flash-crowd phenomenon is expected to be a highly recurring scenario in our system."
"another key mechanism twitter provides is the retweet feature, which enables people to quickly share tweets with their followers. this feature enables individual tweets to be propagated throughout social networks and serves as a method for people to endorse their perspectives regarding specific topics [cit] . therefore, the influencers of information dissemination on twitter can be identified according to the number of retweets by other users. among the 450,250 related tweets during the first month since the windows 8.1 launch date, 19.92% were retweets. table 3 lists the top 10 users who had the greatest number of messages reposted by other users. because the retweets from these influencers accounted for 26.87% of total retweets, these influencers were vital to the windows 8.1 event information disseminated on twitter."
"in a real deployment, the p/s system is shared among many sources and subscribers have varying interests. an important aspect of operation in such an environment is how the traffic flows due to different content sources coexist to− gether. ideally, content flows with overlapping subscribers compete fairly but do not distort each other. to investigate this desirable behaviour, we experimented with a number of scenarios using c−1000. in the first case, we had 10 sources that each publish content that is of uniform interest to all 1000 subscribers. we observed that all content flows use an equal share of the system's bandwidth. in the second (and more interesting) scenario we grouped the publishers into 3 groups such that content from the first, second and third group was of interest to one third, two thirds and all of the clients, respectively. this involves dissemination of 1 tb of data with the above matching distribution among 1000 clients. figure 11 illustrates the proportion of traffic due to content with different popularity. the graph confirms that the traffic due to each content is proportional to its popular− ity. this implies that competing flows in a shared system are friendly to each other and do not inflict one another."
"in current implementations, a file update incurs costly network traffic from servers in the cloud to each and all of the synchronizing clients. our peer−assisted scheme largely cuts these costs by enabling clients to con− tribute in the dissemination of file updates."
"in addition, we measured the strength of the ties between a social influencer and their top followers according to the number of the user's tweets that were reposted by followers. table 4 shows the results regarding the strength of the tie between the nodes in fig. 2 . the results revealed that 4,137 retweets were from the official windows account, and that 1,647 retweets were from the official microsoft account. users who exhibit strong retweet influence are highly capable of generating content with pass-along value. the influences at the two initial stages were essential for the initial distribution of windows 8.1 event information. the social influencers at the later stages were critical for the dissemination of this information on twitter."
"next, we studied subscribers' contribution in the dissem− ination process. figure 10 illustrates the number of coded data blocks offered by peers in configuration c−1000. the graph is largely smooth and evenly distributed (note that all peers have equal uplink bandwidth). there were 10 sources that were distributed uniformly across 5 regions and each published 100 mb of data (100 segments each). full dis− semination of 1 gb of published content among 1000 sub− scribers results in 1 tb of traffic. this requires transmission of 100 million data blocks in total. our measurements indi− cate that each peer participates with it fair share of offering about 103, 000 blocks on average. the extra 3 blocks per segment are sent due to two factors: sending of data blocks after a break is requested but arrives late, or due to linear de− pendencies in received packets that necessitates a new send."
"tion, the completion of all downloads takes about 1750 sec− onds (graph (4) in figure 12 ). it is worth mentioning that a client who uses the default rss poll interval of 10 minutes for files that are released weekly, say, a weekly tv show, will unsuccessfully query the tracker about 140 times a day or about 1000 times during a week. all but one of these queries results in an actual hit. use of a shorter poll interval for better responsiveness increases this load at the tracker."
t . the resulting coefficients and coded blocks are then packaged and sent in a similar man− ner to when the original data matrix is fully available.
several systems adopt linear network codes in a variety of applications rang− ing from video transmission and playback over the net− work [cit] to file sharing applications [cit] .
"because of the viral nature of information dissemination on social media, identifying and analyzing the influential social media users is becoming increasingly important to business. this study not only provides different perspectives for investigating the influencers of event information dissemination, but also proposes a social network diagram approach to depict how event information is disseminated from the influencers. our proposed approaches can help companies identify the key users during an event information dissemination as well as model the relationships among top influencers. a case study regarding the tweets from the windows 8.1 launch event was also discussed to illustrate our approaches. our results suggest that these social influencers did play important roles on the information dissemination of the windows 8.1 launch event. we are also able to understand how event information was disseminated from the influencers based on the social network diagram. future studies may explore other events to evaluate our proposed approaches on identifying the influencers. besides, we can investigate how our approached can be applied on other social networking platforms such as facebook and youtube. we can also further examine how public sentiment towards an event can be assessed based on the related messages on social media."
"the middle graph in figure 7 depicts the impact of packet loss on segment completion times. an advantage of network coding is that data transfer is inherently resilient to packet loss: if a receiver misses a packet, it simply waits for a future packet from the same sender or another peer. we experimented by subjecting communication links to 5% and 10% of message loss. the graph shows that the impact on the completion times is proportional to the loss injected."
"to analyze the characteristics of an event information dissemination on social media, the related key words must first be identified, and the related posts are then retrieved by using the tools provided by social networking services. for example, on twitter, the streaming application programming interface (api) can be used to stream tweets in real time, or the search api can be used to perform ad hoc user queries from a limited corpus of recent tweets. based on the collected data set, the most intuitive method for identifying the influencers of event information dissemination on social media is to count the number of related messages posted by users. because different social networking services have distinct data structures or metadata tags, the related data fields must be understood. for example, on twitter, the user who posted a message can be identified by referring to the user_screen_name data field. hence, the users who have the highest frequency numbers in user_screen_name among the tweets related to the target event in a given observation time can be considered the influencers of information dissemination. in this study, we selected only the top 10 influencers to avoid generating excessively complex discussions. moreover, a user mention in social media is any message update that contains a person's username in the body of the message. hence, the users who receive the highest frequencies of mentions in the target social media feeds can also be considered influencers of information dissemination. on twitter, user mention data can be retrieved by a code like the follows."
"content publishing procedure: a file that is to be published by a source may come in different sizes rang− ing from a few megabytes to hundreds of megabytes. the source breaks up the file into a number of fixed sized units called segments which are handled by the system separately. each segment is associated with the content descriptor of the original file and matches the same set of subscriptions. a segment is uniquely identified by its metadata informa− tion which consists of the unique identifier of the original file, as well as the segment's byte offset within the original file. we distinguish between two types of closely related messages: a content notification message is a publication message that carries metadata information associated with a segment. on the other hand, a content data message (or simply data message for brevity) contains both metadata in− formation as well as a coded data block of a segment. in our two−layer architecture, notification messages flow be− tween brokers in the control layer while data messages are transmitted only between clients in the data layer."
"we verified the effectiveness of our scheme in limiting cross−regional traffic using configuration c−1000 with 5 dis− tinct regions. we placed one source in one region and had it publish 100 mb of content that is disseminated among all 1000 subscribers in all regions. note that in our approach, the cross−regional traffic is confined to the blocks of data sent to the initial seeds of each segment. this is set to 2 peers in each region (per segment). compared to the large population of subscribers in each region (about 200 peers) this represents a small percentage. furthermore, since these peers also exchange data as part of the cluster, this further reduces their reliance on the source. table 3 shows the results in terms of the percentage of network−wide aggregate traffic that flows between a send− ing and receiving region. it can be observed that the vast majority of data messages are sent within regions and this constitutes roughly about 20% for each region."
"we now use figure 4 to elaborate on details of the in− teractions between clients and brokers, as well as between brokers and brokers. a newly published content is initially seeded by its source. the source starts the dissemination process by sending the content's metadata information to the broker it is connected to, i.e., the content's home broker. the home broker uses the content descriptor and its locally stored subscription entries to identify a subset of its local clients with active subscription leases (i.e., not yet expired) that also match the content descriptor. this information is readily available at the home broker and allows it to imme− diately reply back to the source with a push−list containing interested peers for regional seeding. this interaction be− tween source and home broker is marked by ① in figure 4 ."
"an immediate benefit of this strategy is that during the time that it takes for the source to upload one segment of content into the cluster, all participants in the cluster con− currently retrieve and decode the entire segment (assuming that source and peers have equal uplink bandwidth). fur− thermore, dissemination of segments after this point takes place in a multi−source (rather than single−source) fashion and frees up the limited uplink capacity of source. the source can then quickly move on to offer other segments, improving availability of a diverse set of segments in early stages after release. finally, peers in the cluster participate in dissemination since receiving the initial packets, leading to less idle time and more effective bandwidth usage."
"to overcome this challenge, we argue in favour of a peerassisted dissemination scheme in which the system's avail− able bandwidth grows organically with the number of peers. in recent years, such hybrid schemes have gained popularity among online music and video streaming industry and have been shown to lower dissemination costs, avoid server over− load and maintain high quality of service (qos) [cit] . like many of these systems, we assume that peers are altru− istic (possibly by running a proprietary piece of software) and wish to cooperatively download their content of inter− est as early as possible. these are reasonable assumptions in all usage scenarios we outlined previously."
"to address these challenges, this paper makes the fol− lowing contributions: (i) in section 3, we devise a two−layer hybrid architectural framework that brings together the ben− efits of timely push−based p/s communication with the na− tive scalability of p2p bulk content dissemination applica− tions; (ii) in section 4, we elaborate on how network cod− ing can be incorporated to facilitate the exchange of data blocks in an efficient manner within our framework; (iii) in section 5, we devise a number of strategies to reduce dis− semination delay and improve fault−tolerance; and finally, (iv) in section 6, we report on our evaluation results based on a fully functional java−based implementation of our ap− proach, called publiy + [cit] ."
"on the other hand, the graph marked by (3) shows the start of downloads for clients using bt configured with the standard rss poll internal of 10 minutes. in this execu− tion times between our peer−assisted push−based dissemina− tion scheme that uses network coding (nc) and bittorrent (bt) [cit] (graph is best viewed in color)."
"we now report on the experimental comparison results with the bittorrent (bt) file sharing protocol [cit] . the standard bt protocol does not provide native support for clients to receive newly released content unless they explic− itly query a bt tracker where the content's \".torrent\" file is registered. despite this deficiency, many bt client pro− grams offer a workaround and use rss polling to accom− modate automatic download of newly released content. af− ter the content is released, clients' next rss polls result in hits and they proceed to download the file in a regular man− ner. furthermore, [cit] observe that during flash−crowds rare missing pieces of content may cause delay in the completion of download of peers in a bt system."
"we used three different network configurations with dif− ferent client population sizes. the p/s overlay network in each configuration is composed of 1 − 5 brokers while the client population ranges from 120 − 1000 peers. the small number of brokers is intentionally chosen to demon− strate that only a handful of p/s brokers can coordinate con− tent dissemination among a much larger population of sub− scribers. clients in each configuration are evenly distributed among brokers and unless otherwise stated all their sub− scriptions match published content. furthermore, for the purpose of all our experiments we used block size of 10 kb and segment size of 1 mb. this implies that each block is coded with 100 bytes of coding coefficients. this way, each data packet with the packet's header information, the cod− ing coefficients and the coded data blocks have a combined maximum size of 10, 140 bytes and transfers 10, 000 bytes of useful data. this brings the overhead of network coded packets to about 1.4%. we believe that this overhead is neg− ligible compared to the benefits of using network coding. in our implementation, content descriptors are not included in data messages and are sent to subscribers out−of−band and only once. this avoids redundancy and improves efficiency. figure 7 illustrates the impact of three key factors in the operation of the system, namely, content serving policy, packet loss, and source fanout (i.e., the maximum number of segments that the source is offering at any point in time). for all executions, we used c−300 with 200 kb/s uplink bandwidth allocation per peer. at time 0, a source publishes 100 mb of data that is delivered to all 300 clients."
"recent advances in file sharing [cit] and content dissemi− nation applications [cit] have enabled large−scale distri− bution of bulk content among thousands of internet users. as a common practice in these systems, a downloading client is required to actively seek its content of interest by querying a tracker and initiating download sessions to other peers who possess the file. if the requested content is not readily available, e.g., because it is not yet published, the client has to either abandon its quest for the file, or continu− ously poll trackers until the file becomes available. depend− ing on whether the polling interval is short or long, continu− ous polling by many clients either imposes massive load on trackers or increases clients' download times. as another problem in these systems, popular file sharing protocols are generally not optimized for minimizing clients' download times."
"the common denominators in the above application sce− narios are timeliness and selectivity of content distribution. these traits make the p/s model an ideal choice. in a conventional distributed p/s system, designated message routers (a.k.a. brokers) form an application layer overlay network and provide an access point through which subscribing and publishing clients communicate [cit] . brokers store clients' subscriptions and relay publica− tions towards subscribers with matching subscriptions. ex− isting p/s systems that adopt such a broker−based architec− ture also presume that publications have a relatively small size (tens of kilobytes). as a result, brokers are unlikely to face an overwhelming volume of traffic. in contrast, our fo− cus on distribution of bulk content (hundreds of megabytes) causes traffic−bearing brokers in these approaches to face serious limitations to scale to a large client population."
"to set up forwarding paths in the overlay, client subscrip− tions are injected into the system at their local brokers and sent throughout the network. brokers record the subscrip− tion's predicates as well as a reference pointer to a neigh− boring broker from whom the subscription was received. these reference pointers construct routing paths that are used for publication forwarding. for example, figure 1 il− lustrates a simple p/s network where a publication is routed towards interested subscribers along the overlay links des− ignated by arrows. in the p/s literature, many optimization techniques such as use of advertisements and subscription covering [cit] are proposed to avoid flooding of subscrip− tions and constructing concise routing tables at brokers. in our approach, we use publiy + [cit], our distributed p/s system as our underlying distribution network that also supports many of these techniques including subscription covering and efficient multipath publication forwarding."
"in recent years, the focus of the p/s research commu− nity has been on efficient distribution and routing of events or notification messages which are typically small (tens of kilobytes) in size. these techniques, however, are not read− ily applicable to dissemination of bulk content (hundreds of megabytes). hence, our objective in this paper is to bridge this gap and craft a scalable scheme for fast dissemination of bulk content. before delving into the details of our ap− proach, we first present a few examples of potential appli− cation scenarios that benefit from timely and selective dis− tribution of bulk content among large population of clients:"
"an important advantage of network coding is to allow clients to start seeding without having to wait until fully receiving a segment (i.e., early seed). enabling early seed− ing, however, substantially increases the risk of arrival of unuseful linearly dependant data blocks at subscribers. an early seed who chooses to participate in the dissemination process before fully decoding a segment must be cautious about whether its coded packets are indeed useful for its downloading peers. one solution is for the seed to check the independence of the coding coefficients it uses with the re− ceiver prior to transmitting the packet. to avoid this hurdle, we use a scheme that restricts the number of blocks offered by an early seed s to each of its downloading peers to be the number of coded blocks that s has received from peers who possess the decoded segments. this is done by including a flag in data packets that indicates whether a sender pos− sesses the full segment. an early seed counts the number of received packets with the flag set and caps the number of packets it sends to each downloading peer accordingly."
"because the value of the attribute screen_name in user_mentions indicates the username mentioned in a message, the users with the highest counts of screen_name in the target tweets can be considered the influencers of the related information disseminated on twitter. in addition, the users who frequently repost information related to an event were also considered influencers. on twitter, reposting of another person's tweet is called a retweet. this feature allows a user's retweets to be disseminated to all of the user's followers. the number of times that a username appears after the metadata tags \"rt\" or \"via\" in the target tweets can be counted to determine how many related tweets user has retweeted. after identifying the influencers, we further investigated how information is disseminated from these influencers by depicting a social network diagram. a node representing an influencer is allocated in the center of a network diagram and is associated with the top five users who have reposted the most messages from this influencer. this procedure can be repeated to obtain a social network diagram that illustrates how event information is disseminated from influencers as well as the influencers of information dissemination at different stages. the influencers at the first and the second stages are essential for the initial distribution of event information, and the influencers at the remaining stages are responsible for the subsequent information circulation."
"the top graph in figure 7 shows segment completion times of different content serving policies. the fastest com− pletion time is when peers start to offer coded blocks as early as having received 1 data packet. this proves that early seeding is an effective way to hasten the dissemina− tion process. on the other hand, the case in which clients wait to fully reconstruct a segment before seeding it is about 30% slower."
random lin− ear network coding has been an active research topic in information theory and has proved to be a practical ap− proach to improve the network's bandwidth utilization [cit] . this is particularly important for our data dissemination system in which scalability demands effective use of all peers' available bandwidth resources.
"at the broker side, a subscription is inserted into the bro− ker's local subscription routing table along with its lease ex− piry timestamp as well as the identity of the issuing client. we refer to these pieces of information collectively as a subscription entry. furthermore, the subscription entry is prop− agated to neighboring brokers as described in section 2 in order to set up forwarding paths throughout the overlay."
"to provide further insight into information sharing using social media, one of the objectives of this study is to discuss how the influencers of event information dissemination on social media can be identified. we propose three perspectives for investigating this topic: the number of related messages posted by the influencers, the number of related messages in which the influencers are mentioned by other users, and the number of influencers' messages that are reposted. because the perceptions of brands and companies are largely driven by the rapid expansion of social media channels through which influencers communicate with their social networks [cit], findings regarding influencers can help companies identify the key people or organizations with whom they must engage. in addition, we used a social network diagram to depict how event information is disseminated from the influencers. this diagram shows the top influencers at different stages of information dissemination. analyzing the process of information dissemination on social media is complex because a numerous people use social media and their complex topological relationships must be considered. effectively modeling relationships among top users and accordingly using them to filter or recommend information are fundamental for mining social networking services. the methodology used for the aforementioned objectives is detailed in the next section. to illustrate our approach, we used the tweets from the windows 8.1 launch event as a case study, which is discussed in section 3. finally, section 4 concludes our study."
"in this section, we elaborate on the details of our content dissemination protocol that uses network coding to facilitate exchange of blocks of data among clients."
"in this paper, we address the first drawback by adopting the publish/subscribe (p/s) model that provides two major benefits: (i) dissemination is inherently reactive and is ini− tiated as early as the content is released by a source; and (ii) published content is delivered to a select subset of clients determined based on the clients' pre−specified subscription interests. furthermore, to address the second deficiency, we develop content dissemination strategies that are specif− ically targeted to lower the clients' download times."
"based on these results, we continue with the rest of our evaluation by only using the best performing values for the parameters: fanout of 1 and content serving policy that al− lows clients with only one data block to seed the segment. also, the injected loss for all following experiments is 0%."
"our goal in this paper was to bring the benefits of timely and selective publication distribution available in p/s sys− tems to the field of bulk content dissemination and file shar− ing. to this end, we developed a two−layer architecture in which p/s brokers take the role of coordinators and guide subscribers with similar interests to engage in exchange of coded data blocks. this process is inherently reactive as it is initiated as early as the content is published by a source. we also demonstrated that a variety of usage scenarios can benefit from our peer−assisted approach to accomplish time− liness and mass distribution of bulk content while avoiding the costs of deploying dedicated large−scale content repli− cation servers. finally, we have implemented and experi− mentally evaluated our algorithms. our results confirm that by tapping into the clients' available resources, a handful of deployed p/s brokers can oversee and coordinate timely distribution of large volumes of published content at scale."
"in our study, we also discovered that dram error behavior may vary signicantly within an application run. for example, figure 4a shows how the number of ces induced by kmeans(8) changes in time for dram operating under 2.283 s t ref p at 60 c. we can indentify two dierent phases in this benchmark: at the rst phase the average number of ces per second is less than 20, while at the second phase this number achieves almost 300 per second. by proling and analysis of this benchmarks, we found that the rst phase corresponds to the i/o phase of kmeans where the input data for this benchmark is retrieved from a le. while at the second phase, the benchmark processes the input data reading and writing to dram intensively which explains the high error rate obtained at this phase. nonetheless, figure 4c shows that the average number of ces per second detected at this phase drops down to 50 for the single-threaded version of this benchmark, while the duration of this phase increases in 7.8⇥. note that we also observe variations of the dram error behavior within the nw benchmark. figure 4b and figure 4d depict the number of ces per second detected for the parallel and 1-thread versions of nw respectively. we see two distinct phases with a low and a high error rates. moreover, similarly to kmeans, the error rate is lower for the 1-thread version than for the parallel version of this benchmark."
"to compare any dierences between the number of error-prone locations discovered by the dpbenchs and the hpc workloads, we calculate the so-called coverage of unique erroneous locations detected when running a specic workload over the total number of error-prone locations, discovered by all benchmarks, as:"
"represents the variance of the residual speech distortion after enhancement. to account for this uncertainty, the clean posteriors (1) are replaced by their expectation [cit] as follows:"
"gmmd features are high dimensional. in practice, we reduce their dimension by the principal component analysis (pca) [cit] . the resulting features are concatenated with the spliced enhanced features y t and used during training and decoding. decoding can be achieved either in a conventional fashion or by uncertainty decoding. in the latter case, the uncertainty over the concatenated features [ y t− fig. 2 shows gmm and dnn posteriorgrams for one real chime-3 utterance. the dnn posteriorgrams are obtained by the conventional decoding. the gmm posteriorgram without uncertainty and the dnn posteriorgram without gmmd features both differ significantly from the ground truth. the gmm posteriorgram with uncertainty is closer to the ground truth, but has nonzero probability for several other states. the dnn posteriorgram with gmmd features matches best with the ground truth compared to any other posteriorgram, and it finds the transition from sil to aa1l."
"the impact of the noise environments on the wer in the real chime-3 development and test sets is shown in table iii . dnn uncertainty decoding based on fmllr+gmmd, dnnu and mc always improves the wer compared to the baseline. in 5 out of 8 test conditions, it is the best configuration. in the three remaining conditions, the difference with the best configuration is not statistically significant. for all features, dnnu systematically outperforms du and mc systematically outperforms ut. finally, without uncertainty decoding, fmllr+gmmd features improve the wer compared to the baseline in 7 out of 8 test conditions, while fmllr+du and fmllr+dnnu improve it in 4 and 3 conditions only, respectively."
"in this research paper, know your customer (kyc) information verification technique has been introduced as challenge question (cq) during login using user id and password in order to verify user more intensively. in that case kyc must be privatized with widespread dynamic user input. the kyc database enriches from account opening initial data, user interaction and dynamic update through the application; on the other hand user can add more confidential information or random question/questions with answer/answers to the kyc database to make the authentication process much stronger and secured. ranking on the kyc information will also be considered to be used as cq; cq will be asked to the user during login after success in user id and password verification. one or more cq will be assigned to ask the user based on the risk factors assessment result. top ranked cq will be asked to the user when the risk assessment result is comparatively higher; on the other hand low ranked cq will be asked for lower risk."
"based on these grounds, we conclude that the dram behavior may vary not only across workloads but also within a workload run. n w ( 8 ) n w ( 1 ) k m e a n s ( 8 ) k m e a n s ( figure 5 depicts the memory power and the reduction of power for each benchmark averaged across the dimms and the two temperatures when we relax t ref p . these saving can be attributed to the 35⇥ less refresh operations issued by mcus, caused by the same change of the t ref p . we observe the greatest reduction of power for nw(8) at 27.3 %, while many workloads have reduction close to 10 %. on average, we observe that we can save 11.5 % of the total memory power."
"in case of credential risk, this paper proposes a strong authentication method as all kinds of credential risk are vital and seldom used i.e personal information changing, failure to provide correct details. these criteria make a match between provided input and kyc database existing data. if user is failed to provide correct answer, the kyc information will be treated as suspicious and high ranked cq will be assigned for further verification."
"we assess the asr performance in terms of the word error rate (wer). for the chime-2 development and test sets, the 95% confidence interval is about ±0.4%. for chime-3, the confidence interval is about ±0.3% for the development set and ±0.5% for the test set. in the following tables, for each test condition, the best choice of features, acoustic model, and uncertainty estimation/propagation technique is shown in bold."
behavior-based model defense is based on the analysis of the exploit's interaction with the target. interactions outside the normal behavior groups would be suspicious and quarantined. this method then has the potential to detect and analyze potential zero-day exploits in real time [cit] .
"the remainder of this letter is as follows. section ii provides some background on uncertainty decoding and 1070-9908 © 2018 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"in this paper we have used hidden markov model (hmm) for transitional probability of user's behavioral risk predictions (different than users general transaction timing, exceeding regular transaction frequency, different from regular transaction purpose, different from regular url navigation etc)."
"a main memory sub-system based on drams is organized hierarchically into channels supporting a number of dram modules. each dual in-line memory module (dimm) usually has two ranks, each of which consists of dram chips. within each chip, dram cells are organized into banks, which are two-dimensional arrays, addressed based on rows and columns as shown in figure 1 . the main drawback of the dram technology is the limited retention time [cit] of the cell's charge. to avoid any error induced by the charge leakage over time, dram employs an auto-refresh mechanism that periodically recharges all cells in the array based on the worst retention time across them. conventionally, all ddr technologies adopt today a refresh period, t ref p, of 64 ms for refreshing periodically each cell of the dimm even if in reality many cells may have a much higher retention time than t ref p and the conditions in the eld after deployment may not be as bad as the ones assumed [cit] ."
cq level will be assigned based on the derived probability result. less probability indicates suspicious or fake user; on the other hand high probability means trusted and real user. suspicious user is checked by highly ranked cq and vice versa.
"the basis of our experimental framework is a state-of-the-art commodity 64-bit armv8 based server, the x-gene2 server-on-a-chip, which is the latest generation of the x-gene family of chips used in the popular hp moonshoot servers [cit] . as depicted on figure 1, the x-gene2 soc consists of four processor modules (pmds), each with two 64-bit armv8 cores running at 2.4ghz. the implemented memory hierarchy is representative of any modern high performance system consisting of a 32 kb l1 data cache and a 32 kb l1 instruction cache per core, a private 256 kb l2 cache shared between the two cores of each pmd and an 8 mb l3 cache shared across all four pmds through the cache-coherent central switch (csw)."
in this assessment all the navigation will be recorded and compared with previous traversal history. the proposed model will find out difference between the navigation attitudes. in case of new attitude the cq selection process will carry a new weight to select hard (high ranked) cq during login. this assessment is only applicable in the portal where number of url and pages is very high and user requested to login into an unusual url which is different from users historical behavior. sample of the traversal history log from live website [cit] has been depicted in the fig. 6 .
"as temperature has a signicant eect on the dram reliability [cit], it is also essential to develop a special thermal testbed and investigate the dram reliability running real workloads under high temperatures, which may be obtained in the worst case scenarios."
"the expectation can be computed as follows [cit] . mc consists of drawing random samples from the feature uncertainty distribution (2), which are input to the dnn. the average of the outputs approximates the posterior expectation. alternatively, ut consists of drawing samples from this distribution in a deterministic fashion and associating them with weights. the samples are passed through the dnn and the weighted average of the outputs approximates the posterior expectation."
"where h represents current hour from the above behavioral access time data, the probability of using the application in the current time can be determined. the probability result determines the selection of cq which will be assigned for the user on the login time. the probability of the current time from the historical timing of a particular user is calculated using hmm (2)."
"in this paper, it has been proposed dynamic kyc based mfa authentication method to secure access of the financial services through the internet. analysis and simulation results show that the proposed method provides control equal to existing mfa/2fa. the proposed method is costless and does not incur any hurdle to carry an additional hardware. as there is no chance of key theft, this method can be used on any private or public device. the method will ensure dynamic security by tackling most of the vulnerabilities in internet based financial transaction."
"the rest of the paper is organized as follows. section ii overviews the related work. description of our proposed model is introduced in section iii. section iv discusses the result and compares with other methods of mfa. finally, section v concludes the paper with fruitful remarks."
"table ii presents the wer on the chime-2 development and test sets as a function of the snr. in addition to the systems in table i, we also show the results obtained using fm-llr+du or fmllr+dnnu features. our main findings still hold: for all snrs, dnn uncertainty decoding based on fm-llr+gmmd, dnnu and mc outperforms other configurations. the improvement relative to the baseline increases with the snr, from 14% at −6 db to 28% at 9 db on the test set. also, with fmllr+gmmd features, dnnu outperforms du in all cases and mc often performs slightly better than ut. without uncertainty decoding, fmllr+gmmd features also systematically improve performance compared to fmllr features only, while fmllr+du and fmllr+dnnu provide a smaller, less consistent improvement."
"temperature. indeed many studies have shown that the retention time of dram cells decreases exponentially as the dram temperature t rises, described by ae 0055⇥t + c [cit] . in all existing studies the dram temperature is controlled by placing the whole fpga system in a thermal chamber [cit], or more lately by using heating elements on dimms [cit] ."
"64-bit arm based server with a linux os. in order to experiment under dierent dram temperatures, we also implemented a thermal testbed that allows to ne tune the temperature of each dimm on the server."
"in order to identify if the dierent dpbenches excite dierent error locations for dram operating at 2.283 s t ref p, we calculate co of error locations discovered by each dpbench, as seen in figure 2a and 2b. the highest co is observed for the random dpbench which is above 70 % for both temperatures, while the static dpbenches discover less than 25 % of all reported error-prone locations. these results are consistent with the observations made by liu [cit], where authors also reported that the highest co is observed in case of the random dpbench. in figure 2a and 2b, the circle's size and the number indicates the co (10minutes) for the last 10 minutes. we see that the co (10minutes) does not exceed 2 % for each dpbench at the end of the execution which implies that 120 minutes are sucient for identifying the majority of error-prone locations. note that for the calculation of co above, we only consider the errors incurred by the dpbenchs."
"we proposed gmmd features as additional inputs to dnn acoustic models for the noise-robust asr. these features encode the impact of the uncertainty, that is the variance of the residual speech distortion, on the acoustic scores. experiments on both simulated and real data showed that they systematically and significantly improve performance compared to fmllr features alone, or to using the estimated uncertainty itself as a feature. we also proposed a dnnu estimator that successfully operates in the fmllr domain and showed that applying uncertainty propagation to fmllr+gmmd features further improves the wer, up to 21% and 13% relative compared to the conventional decoding on fmllr features without uncertainty. in the future, we will explore joint optimization of the feature dimension reduction matrix and the dnn acoustic model."
"iii. proposed model in this model of authentication for financial application, some techniques of comprehensive risk factor judgment have been proposed to identify a suspicious login over many existing risk evaluation by analysis of user's historical activities data, and also proposed a new authentication method based on kyc information for authorizing the user during login as well as before performing transaction. in this method cqs are choosing from a collection of cqs where the level of the cq is defined by the risk factors calculation result. risk factors are calculated considering credential risk (new user, failed login attempt, user with no/very few identity information, changing nature of transaction), behavioral risk (changed general transaction timing, exceeded regular transaction frequency, changed regular transaction purpose, called to a new url, which did not call by this particular user previously), transaction risk (exceeded regular transaction limit, exceeded profile transaction limit), and location risk (changed geo-location, changed transaction device). for appropriate and secure transaction using application, proposed model performs two operations. first one is risk factor calculation and second one is assigning the cq based on the risk assessment result. the brief idea of the model is depicted in the fig. 1 . in the figure the initial step is login of the user with user id and password verification like other online applications. the forwarding stage is risk analysis for the login succeeded user. next stage assign one or more cq based on risk level formed by the result of prior stage. final stage of the verification is otp / email / otp & email confirmation if it is indicated by the result of risk analysis. in some other cases confirmation stage may not be applicable where cq is in final stage before performing and committing a transaction. the cq based authentication applies immediately after login and before performing and committing a transaction to verify the user rigorously. this cq replaces the 2fa or traditional question and answers mechanism from some other existing authentication models. user and calculate similarity between previous and present by introducing multiple techniques and algorithms."
"we computed the gmmd features as explained in section iii using dnnu in (6), irrespective of whether du or dnnu were used for possibly subsequent dnn uncertainty decoding. we reduced their dimension to 100 via pca before concatenation with the 440-dimensional spliced fmllr features. we denote the concatenated features as fmllr+gmmd. for comparison, we also considered the 40-dimensional uncertainty vectors estimated by du and dnnu themselves as features instead of gmmd. we denote the resulting concatenated features as fm-llr+du and fmllr+dnnu."
"the x-gene2 has two memory controller bridges (mcbs) which are connected to the csw providing access to dram. in turn, each mcb is connected to two ddr3 memory controller units (mcus). each mcu has one channel of ddr3 memory and support up to two dimms with two ranks each. in our campaign, we are experimenting with 4 micron ddr3 8gb dimms at 1866 mhz [cit], one dimm per mcu. in total, we are characterizing 72 chips of 4gb x8 ddr3 [cit], since each dimm includes 16 and 2 dram chips for data storage and ecc, respectively."
"in addition, to account for any potential errors of more than two-bits in a 64-bit word that cannot be detected by ecc, we compare the output of each execution with a golden reference output obtained when dram is operating under the nominal t ref p . in this way, essentially we were able to measure any silent data corruption (sdc) that could go undetected by secded ecc."
"this paper proposes an alternative method of authentication for financial services through the internet. it is a method to minimize financial fraud forgery on online financial network. the main challenge to avoid fraudulent activity in the financial network is to keep the system away from unauthorized person. the proposed method presented here includes sensitive personal information, which is called kyc information, to verify the actual owner of the account for online financial activity. it considers all the known and upcoming possible ways to theft information and unauthorized entry into the online financial system. the dimension to the risk of the hacking and information stealing is unlimited and tendency for these illegal operations is evolving from time to time; the method proposed is a way to extend the kyc database as required by risk assessment in a certain interval. as the fis already preserved customers' kyc information, so it is effective and fruitful to continue the reuse of kyc data for authentication purpose rather than to bear additional cost involving mechanism."
"in this section, we present the results of our characterization campaign of 72 dram chips using the described framework under the maximum allowed t ref p, i.e. 2.283 s, in the x-gene2 server at 50 c and 60 c."
"to address such eects some recent studies suggested the use of error correction codes [cit], strengthening the conventional secded ecc [cit] . alternatively, other works have tried to mask any error by exploiting the inherent error-resilient features of some applications or the implicit refresh incurred by every memory access but they did so mainly within simulators [cit], rather than on a real system, basing their fault injection schemes on the retention time discovered by the dpbenchs."
"dpbench. typically, a set of worst-case static (all 1's, all 0's, checkerboard) and dynamic (random, uniformly distributed data) dps are used during characterization since the retention time of each cell depends on the value of the stored data within each cell but also in the neighbouring cells. in fact, recent studies have shown that various circuit-level crosstalk eects are being excited by the stored data and may cause the retention time of each cell to vary [cit] . such ndings deemed ineective any existing approach that relied on oine characterization since the number and position of the identied weak cells could not be guaranteed after the dram deployment."
"we calculatew er for the dpbenches as can be seen on figure 3 . similarly to our experiments with co, we observe that the random dpbench does have the highest w er compared to checkerboard, all 0s and all 1s, specically 6.7 ⇤ 10 7 and 1.2 ⇤ 10 5 at 50 c and 60 c, respectively. comparing w er of dpbenches in case of 8 and 28 gb memory allocations, we get similar w ers for both temperatures. this shows that w er does not change with the size of the allocated memory, which is also an indication of the small variation of manifested errors and weak cells across the considered dimms."
"the rest of the paper is organized as follows. section 2 describes the background and the open challenges, while section 3 presents our experimental framework. section 4 analyses the results of our experimental campaign. finally, conclusions are drawn in section 5."
"several experimental studies have revealed the large spatial distribution of cells based on their retention time and tried to exploit it for relaxing t ref p for most of the cells [cit] . typically, such approaches rely on the characterization of the retention time of cells on custom fpga based setups using i) a set of worst-case dpbenches and ii) elevated temperatures, which are the main factors that excite worst-case circuit-level eects on drams [cit] ."
"location risk is determined by translating the ip address to location. user will be asked on different occasions to keep record for his/her upcoming movement into the system. the system will compare the given information and current location prior to giving access to the system or making a transaction. in case of hardware, address used by the user will be compared with the current hardware (desktop, laptop, mobile, tab, etc.). if the hardware used for the transaction is different from all previous hardware/devices, the cq will be harder based on the dissimilarity."
"however, authentication methods for internet based financial system are still pursuing to ensure the highest level of security. existing strong authentication methods sometimes make its nuisance to the genuine users who are operating their regular business using internet based financial system. current activity of any of the users has a linear relationship with user's many past usage patterns such as nature of uniform resource locator (url) navigation, access geo-location, regular transaction nature, regular transaction range, regular transaction timing, transaction performing hardware address, natural transaction frequency, regular transaction purpose, biometric behavior, and many other non bio-metric behaviors."
"table i presents the average results on the chime-2 and chime-3 test sets for both gmm and dnn acoustic models without uncertainty (called \"none\") and with various uncertainty estimation and propagation techniques. we observed a similar behavior on the chime-2 and chime-3 development sets (not shown here). we can make the following observations. 1) in all configurations, dnn acoustic models greatly improve the asr performance compared to gmm. in the following, we consider dnn acoustic models applied on fmllr features without uncertainty as our baseline. 2) compared to the baseline, uncertainty decoding based on dnnu and mc improves the wer by 5% and 1% relative for chime-2 and chime-3, respectively. 3) alternatively, using fmllr+gmmd features provides an improvement of 16% and 8% relative. this means gmmd features better handle uncertainty than classical dnn uncertainty propagation techniques. 4) combining fmllr+gmmd, dnnu and mc improves the wer by 21% and 13% relative compared to the baseline. this means gmmd features and dnn uncertainty decoding are complementary. to our knowledge, this is the largest improvement obtained for dnn uncertainty decoding on the top of an fmllr-domain baseline. 5) the proposed dnnu estimator outperforms the du estimator in all but one (gmm acoustic model on chime-2) situations. also, mc-based uncertainty propagation systematically outperforms ut-based propagation for all features, acoustic models, and uncertainty estimators."
"next, we execute single-and multi-threaded versions of the memory intensive hpc benchmarks under relaxed t ref p for 2 hours as in case of dpbenches. similarly to the dpbench experiments, we observe only ces, which were corrected by the available secded ecc, and no ues or sdcs 4.3.1 coverage. figure 2c shows co for dpbenchs and the hpc benchmarks for dram operating under relaxed at t ref p 50 c and 60 c. in this gure, we note the number of used threads after the name of each hpc benchmark. we see that random exhibits the highest coverage, 48 % and 51 % (at 50 c and 60 c), and essentially excites more error-prone locations than most of the hpc benchmarks. however, it is lower than co for random discovered in the previous experiment when we consider only error-prone locations detected with the dpbenches(see figure 2) . we also observe that kmeans(8) has the highest co among hpc benchmarks at 50 c, which is even slightly higher than co achieved by the random dpbench allocating 8gb of memory. these results imply that real workloads induce errors in more locations in comparison to the static dpbenches and, in some cases, even more than the random benchmark. finally, as no applications demonstrate high co, we conclude that each benchmark can trigger errors in memory locations which are not detected when running other benchmarks."
"in this assessment process least time difference between two consecutive transactions from a dataset [cit] of a live website has been measured. according to our model this assessment result can be applied after performing one or more transaction and before beginning the next transaction. user activities time records are obtained from the user activities log shown in fig. 4 and assessed over 10 least intervals from the collection, which is figured out in the fig. 5 by line chart. fraudsters always try to commit transaction within a short time or using some automated script. using this evaluation it has been considered short time intervals between the transactions which will prevent running script and quick transactions. on the other consideration we assessed users' regular short time intervals between the transactions which will match with users' regular behavior of transaction to indentify real user. any unnatural time gap between the transactions will be treated as high risk transaction. if two transactions are performed quickly within small time gap (30 seconds or less based on the application response time), it will temporarily block the transaction and notify the user as it may be generated by fraudster. in other case, if any transaction occurs in less than minimum time interval of users' previous activities will result to face a high rated cq during transaction."
"we choose to run the conventional dpbenchs based on: all 0s, all 1s, checkerboard and random dps as in most existing studies [cit] . we select also 4 hpc applications from the rodinia benchmark suite, which are typically used for benchmarking parallel systems [cit] . in particular, we use backprop, nw, srad and kmeans to cover a range of domains, i.e. machine learning, bioinformatics, image processing and data mining. we run hpc benchmarks with 1 and 8 threads to evaluate how parallel execution aects the dram error behavior. in our study, we are tyring to understand whether dram operating under t ref p could be characterized with dpbenchs. to investigate this, we need to identify all cells that have a small retention time, and thus are more prone to failures. this is achieved by executing dpbenchs ensuring that we cover all memory available for the user space, i.e. 28 gb on our experimental setup, while the remaining 4 gb is used by linux for the kernel, drivers and other system services. for hpc workloads, we limit the datasize of the benchmarks to 8 gb, while each execution has a random allocation in memory as it goes through the normal allocation policies of linux. we intentionally do not use all available memory at the user space to consider in our study a possible eect of these policies on the dram reliability and explore ability of dpbenchs to cover error-prone locations discovered when running real workloads."
"the obtained w ers for hpc workloads are depicted in figure 3 . we observe that the hpc benchmarks incur a higher w er than the static dpbenches, but less than the w er manifested by random and random(28gb) at 60 c. nonetheless, w er incurred by kmeans(8) 50 c is higher than w er obtained for random benchmarks allocating 8gb and 28 gb. as follows, real benchmarks may trigger errors in memory locations which are not covered by dpbenchs which implies that dram cannot be fully characterized with dpbenchs. we also see that w er vary across benchmarks: for example, w er incurred by nw is 2.5x higher than w er obtained for kmeans (8) . what is more, we found that w er incurred by kmeans at 50 c grows in 2.77x if we run it in parallel(8 threads). thus we may conclude that the dram error behavior is workload-dependent and it may signicantly change with the level of concurrency in programs."
"in this paper, we present a comprehensive characterization of 72 dram chips under relaxed refresh period and under various temperatures within a commodity server using conventional datapatterns along with a set of hpc workloads. to enable our study, we develop an experimental framework based on a state-of-the-art 64-bit arm based server integrated with a thermal testbed. we demonstrate that the number of excited error-prone locations vary from workload to workload and may dier from the ones discovered by the conventional data-patterns. moreover, we show that the dram error behavior may also change within a workload run. finally, we show that the dram refresh period can be relaxed by 35x on such a commodity system with all errors being corrected by the available ecc up to 60 c dram temperatures, resulting in 11.5% power savings."
"the x-gene2 provides access to a separate scalable light-weight intelligent management processor (slimpro), a special management core, which is used to boot the system and provide access to on-board sensors for measuring the temperature and power of the soc and dram. the slimpro also reports to the linux kernel all memory errors corrected or detected by secded ecc, providing information about the dimm, bank, rank, row and column that the error occurred. the available ecc can detect and correct single-bit errors in a 64-bit word, which we refer to as correctable errors (ces) and detect two-bit errors that cannot be corrected, which we refer to as uncorrectable errors (ues). finally, slimpro allows to congure the parameters of the mcus, such as timings and t ref p, specically from the nominal 64 ms to 2.283 s that is the maximum allowed t ref p in the x-gene2 server. the server runs a fully-edged os based on centos 7 with the default linux kernel 4.3.0 for armv8 and support for 64kb pages."
"several parameters of the deep memory hierarchies on servers, like the organization, the size of the caches and the supported memory bandwidth, can directly aect the number and frequency of accesses to dram and thus its reliability. the thorough study of the dram error behavior requires the execution of relevant single-and multi-threaded workloads on servers. furthermore, real workloads are expected to dynamically change the stored dps and load dierent components of a system and thus may cause various system level eects that may not excite the worst-case scenarios targeted by conventional dps."
propagation. section iii introduces the proposed gmmd uncertainty features. the experimental setup and the results are presented in sections iv and v. we conclude in section vi.
"challenge/response based internet banking considers some authentication techniques, where one based on short-time password, one certificate-based, and other relates them to the taxonomy. we further outline how these solutions can be easily extended for non-repudiation (i.e., transaction signing) and for more sophisticated content manipulation attacks."
"internet based financial services like balance transfer, ecommerce transaction, bill payment, investment to bank products (saving certificate, fixed deposit revenue) etc. are the appealing ways of doing business as well as performing all financial transaction location independently. as a result of these overwhelming facilities, it poses the highest point of risk as it uses public network over the world. the main endeavor of the banks and non bank financial institutions are to provide a consistent, secured and high available process of authentication to their customers with minimizing potential avenues of attack, especially attacking vectors beyond the control of either the customers or the financial institutions (fis)."
"we evaluated the proposed features on the chime-2 [cit] and chime-3 [cit] datasets. the chime-2 dataset was created by convolving clean wall street journal (wsj0) utterances with binaural room impulse responses and adding real domestic background noise at six signal-to-noise ratios (snrs) from −6 to +9 db. the training set contains 7 138 noisy utterances from 83 speakers. the development and test sets contain 2 460 and 1 980 noisy utterances from ten and eight speakers, respectively. the chime-3 dataset provides real and simulated noisy wsj0 utterances acquired by a tablet fitted with six microphones in four nonstationary noise environments: bus (bus), café (caf), pedestrian area (ped), and street (str). for training, 1600 real and 7 138 simulated utterances from 87 speakers were used. the development set contains 1 640 real and 1 640 simulated utterances from four speakers, and the test set 1 320 real and 1 320 simulated utterances from four speakers. the results are reported on the real development and test sets only hereafter."
"in order to overcome the approximate nature of mc and ut, we propose to use complementary gmmd uncertainty features during training and decoding. the procedure for computing these features is shown in fig. 1 ."
more personalized information will be high ranking and more public questions will be rated as low value. based on the risk factor taxonomy the challenge question will be transmitted to the user for further verification during login or prior to the transaction. the table i. describes some of the samples of ranking method of the challenge question (cq). frequently asked question (faq) for the user will be pop-up to the user during his/her work through the application and the answer will be updated accordingly. how often do you go abroad? 1 4.
"obust automatic speech recognition (asr) in noisy environments is still a challenging goal. front-end speech enhancement approaches aim to estimate clean features, which are then fed to the back end [cit] . these approaches alone yield limited performance improvement due to the fact that noise is not perfectly compensated and the enhanced features remain distorted. hence, they are generally used in combination with back-end approaches that retrain or adapt the acoustic model using enhanced data. these back-end approaches compensate the distortion on average over the whole training set. yet, the asr performance on a given utterance still depends on the distortion in that specific utterance."
"considering a gmm-based acoustic model with the same set of states as the dnn-based acoustic model, the log-likelihood of state s i can be computed from the enhanced features as"
"behavioral risk: is the user coming at an unusual time of day, performing a transaction involving an unusual payee, or unusual amount given those users' past behavior?"
"this criterion is related to the popularity of visual analysis among practitioners [cit], which makes necessary to develop and promote suitable complements to it. nap and slc are actually based on relevant visual criteria (i.e., data overlap, change in slope and in level) and thus potentially useful as complements 7 . specifically, visual inspection can be used to assess 7 [cit] found that no overlap technique had highest agreement with visual analysts for both data with and without a change. however, they did not include nap or tau-u [cit] in their study, and these two nonoverlap indices are considered to be superior, given their more solid statistical basis the adequacy of the baseline as a reference for comparison. the change identified visually can then be quantified in an objective manner. the numerical values also offer information that can be communicated among researchers and professionals and used for further analyses with different analytical techniques or as part of research synthesis (e.g., [cit], whereas the new developments on slc make possible its comparability across studies; [cit] ) ."
"once the intervention is introduced, there is apparently a decrease in the ecbi score on disruptive behavior. the downward trend is stable, as shown by the good fit of the ordinary least squares regression line to the data (upper panel of figure 4 ). for such data it is not meaningful to discuss level or variability around a mean or a median level; actually variability is only assessed looking at the (small) distance of the measurements from the fitted trend line."
"comparing the two phases in terms of overlap, the values in the beginning of the pdi-phase are similar to the ones in the cdiphase, but not so in the end. comparing levels is not meaningful. comparing trends is hindered by the lack of fit of the trend lines to the cdi data, but if we focus on the last four (out of five) cdi measurements, there is a deterioration that is reverted with the introduction of the pdi: thus a change in slope has taken place. the comparison between projected and actual data is done in two ways, projecting the baseline mean with limits based on the baseline standard deviation and projecting the split-middle trend line with limits based on 25% of the baseline median. in this case, both approaches lead to a very similar graphical representation, which is well-aligned with the conclusion that the last pdi data points are clearly lower that what would be expected (i.e., values within the limits) in case there was no difference between the two interventions. [cit] collected a posttreatment measurement equal to 38 -a value even lower than the last pdi-phase measurement and so the downward trend seems to continue, which could be interpreted as maintenance of the effect."
"the desired inertance ( ) is obtained according to acceleration of the sprung mass, relative acceleration and relative velocity between the sprung and unsprung mass, and damping-inertance ratio."
"non-overlap of all pairs is an improvement of the percent of non-overlapping data commonly used for quantifying the degree to which the measurements pertaining to each phase share the same values [cit] . it represents the number of non-overlapping data relative to all possible comparisons and it is actually identical to the non-parametric version of the probability of superiority [cit], which is related to the common language effect size [cit] . when a decrease in the behavior is expected, as in the example provided later, the formula for this indicator can be written"
"the results of time responses to a random excitation are shown in figures 9-11, and the rms values are listed in table 4 ."
"after the gci configuration has been completed, the adapted web editor can be loaded with an arbitrary web browser which triggers the gci to switch to operation's mode. in the operation mode, the gci is devoted to execute the record and replay dom manipulations' processes. as shown in figure 5, the two major components of the gci are: (1) the generic collaboration adapter (gca) and (2) the operational transformation engine (ote). a detailed view of the gca is shown in figure 7 outlining also the steps which are executed to record and to replay dom manipulations. for the former case, dom changes are recorded leveraging the dom events api [cit] . the addeventlistener method is used to register event handlers on the sync root nodes. for exam-1 currently, children of sync nodes cannot be excluded from the dom synchronization. in the future, we plan to establish an xpath-like addressing scheme [cit] to configure exceptions. ple, the svg node in figure 6 represents an appropriate sync root node since all graphics elements drawn onto the canvas are sub-nodes of the svg node. in contrast, the selected tool in the editor's palette, for instance, is not a sub-node of the svg node and therefore excluded from the synchronization. once event listeners are registered on the sync root nodes, all dom manipulations triggered by the editor are monitored by the dom event handler component (cf. figure 7) . currently, the following dom mutation events are supported [cit] :"
"according to figure 3, the following equation can be obtained, expressed as where v x and v y are the velocity components along the x and y direction, v l is the speed of the left wheel, v r is the speed of the right wheel, and b is the distance between left wheel and right wheel. generally, the moving speed of the sim is slow. in this study, it is assumed that the sprinkler irrigation machine only moves in the plane-i.e., two-dimensional motion in the x and y directions-and the influence of the undulating ground and lateral slip of wheels are ignored. when the sim moves in a plane, the speed of the sim v o and the steering angular velocity ωcan be expressed as"
"in order to verify the navigation performance of the self-developed sim and test the positioning accuracy of the sim by using the updated sage-husa adaptive kalman filter algorithm in navigation, the navigation tracking experiment is conducted. the experimental scenario is shown in figure 5 . coordinates of the gauss projection; and the speed sensor is used to obtained the longitudinal moving speed of the sim. the self-developed sim is a kind of continuous straight-line moving machine. therefore, the experimental verification is conducted by taking linear motion as an example. in this part, the updated sage-husa adaptive kalman filter algorithm is applied in the navigation."
"in this context, the present paper arises from our conviction that practitioners' professional practice, mainly aimed to help individual clients, can also contribute to informing fellow professionals about the results of applying certain interventions. in order to make this contribution possible and in order to be able to translate practice into research certain design and analysis considerations are necessary. the current paper mainly aims to answer two specific questions \"what can be done to improve the data analysis in my practice so that its results are more useful to the discipline, despite using a sub-optimal design?\" and \"how can i easily implement some appropriate analytical techniques?\" however, design and data analysis should be considered jointly [cit] and this is why we first review some aspects related to how the study is conducted."
"where x k and x k−1 are the state vector at time k and time k − 1, respectively, a k,k−1 is the one-step state transition matrix from time k − 1 to time k, and the system noise w k is the gaussian white noise with an average value of zero. the expectation values of system noise can be expressed as"
"where x k and y k are the coordinate position of the sim at time k, x k−1 and y k−1 are the coordinate position of the sim at time k − 1, v o is the longitudinal speed of the sim, t is the sampling period, and β k−1 is the heading angle of the sim at time k − 1."
"currently, the predominant conflict resolution scheme for shared editing in real-time is denoted as operational transformation (ot). [cit], ellis and gibbs proposed the first ot algorithm [cit] and after two decades of research, ot has been advanced to support more sophisticated features including undo [cit], operation compression [cit] and html document support [cit] . moreover, ot has been incorporated in various industrial-strength products such as google docs [cit] and sap gravity [cit] . one key characteristic of ot is that each ot implementation is tailored to a predefined set of operations. therefore, it is unsuitable for generic applicability since each editor employs its own set of operations. for example, while a word processor may require an operation to insert and delete a character, a graphics editor may demand a create shape as well as a remove shape operation."
"in order to integrate the gci into an existing web editor, two configuration steps are required: (1) the client-side gci implementation gci.js has to be included in the editor's html page and (2) a configuration file named gciconfig.js has to be adapted. figure 6 shows an html page properly including the gci.js. moreover, the html page in figure 6 contains a minimal svg editor consisting of a tool palette accommodated by the div element and an editor's canvas represented by the svg element. to support a selective synchronization behavior, the dedicated configuration file gciconfig.js allows to mark multiple dom nodes as sync root nodes meaning that solely manipulations affecting these root nodes and their respective child nodes will be propagated to all clients."
"participants of the evaluation started out with the shared drawing scenario that was directly succeeded by the shared editing scenario. both evaluation parts followed a fixed evaluation schedule comprising (1) a 5 min editor tutorial, (2) a 15 min shared editing session and (3) a 10 min questionnaire completion phase. first, subjects taking part in the usability testing had to learn the editor's capabilities in a 5 min training session. in the training session participants had to go through 10 small, non-collaborative exercises. detailed instructions illustrated the steps to master the exercises. for example, the svg-edit training session included exercises like create, move or fill shapes. after participants had become familiar with the editor, teams of two were formed. members of a team were located in the same room having its own office desk equipped with a standard pc. since desks were separated by a room divider, participants could speak to each other but they could neither see each other nor see each other's screens. to accomplish collaborative tasks, each team had a limited 15 min time slot. in the shared drawing session, the teams had to draw the floor plan of the evaluation room as detailed as possible. moreover, they had to sketch the existing inventory as well as new inventory items that could improve the work atmosphere (e.g. plants). in the second shared editing exercise, participants had to author a document using the collaborative ckeditor. the document should accommodate the inventory list of the evaluation office associated with typical product characteristics such as purchase price, maintenance costs, life span, etc. besides listing existing and proposed inventory items, team members had to write a letter to the office manager explaining why suggested inventory items were indispensable and should therefore be purchased. after finishing the shared editing assignments, each participant had to complete the questionnaire depicted in figure 8 . this questionnaire was created taking into account multiple software metrics tailored for our use case of evaluating collaborative editors."
"in order to achieve accurate navigation, it is necessary to establish a precise kinematic model. in this section, the kinematic model for a self-developed translational sprinkler irrigation machine is established."
"the procedures used here do not make explicit a priori assumptions about independence or homoscedasticity of the data, as serial dependence is likely to present in data obtained from the same individual [cit] . there are also no specific design requirements."
"to demonstrate the feasibility of our generic transformation approach, two existing web applications have been transformed into multi-user applications with shared editing capabilities. both editors were evaluated with respect to software and collaboration qualities. the results of an extensive user study showed that the transformed editors decently support collaborative work. in particular, characteristics like functionality, usability, efficiency and coordination were constantly assessed with high ratings meaning that users were satisfied with the offered support. however, especially the reliability and awareness aspects leave room for improvement."
"after one of the listed events is fired, the respective event handler starts constructing an ot operation leveraging data exposed by the event object (e.g. attributes of an inserted node). the ot operation is then passed to the interchangeable ot engine that has to support common tree operations such as insert or remove node. consequently, dom operations can be mapped to ot operations. ot engines capable of handling tree-structured documents are for example apache wave [cit] or sap gravity [cit] . once the ot engine receives an ot operation from a remote site, the operation must be transformed against local concurrent operations in order to resolve potential conflicts. eventually, the remote operation is integrated in the ot document. afterwards, sync messages containing novel local operations, timestamps, etc., are sent to the server. the server integrates the ot operation into its local persistent ot document and broadcasts the sync message to all other clients to assure that they are aware of the changes and update their local document copies accordingly."
", where x represent detrended values (i.e., after eliminating pre-intervention trend), instead of the original measurements. therefore, the intervention phase estimate of trend presents the average increase (or, if negative, decrease) from one intervention phase measurement occasion to the next one, after controlling for baseline linear trend. for instance, the slope change estimate reflects the average decrease in the number of tantrums in a child with each successive post-intervention measurement, that is, a progressive change."
"considering that the longitudinal speed of the sim is small and constant during operations, and the guiding path for the navigation of the sim is generally a straight line, the kinematic model of the self-developed sim depicted in figure 3 is employed for analysis. as shown in figure 3, the self-developed sim adopts four-wheel drive. the path tracking of the sim is realized by adjusting the wheel speed on both sides. in figure 3, the x-y coordinate system uses the world coordinate system: o is the center of sprinkler irrigation machine (sim), o is the instantaneous center of rotation, the distance between o and o is the turning circle of sim, the speed at point o -i.e., v o -can be regarded as the moving speed of the sim, β is the angle between the longitudinal axis of the sim body and the x-axis of the world coordinate system, and l is the distance between the front and rear wheels. let the coordinates of o -i.e., (x, y)-be the location of the sim. then, the state of the sprinkler irrigation machine at any time can be expressed by the state variables x(t), y(t) and β(t)."
"as stated in section 3, the main challenges devising a gci are the complexity of the conflict resolution scheme for the multitude of editor operations and the heterogeneity of editor apis. considering these challenges, we set out to create an application-agnostic collaboration infrastructure neglecting editor-specifics such as tailored operations or dedicated apis. we identified the application-agnostic elements of the browser platform and carved out the abstract editor architecture as depicted in figure 4 ."
"it is indicated in table 2 that the average deviation, maximum deviation and deviation variance become smaller after filtering through the updated sage-husa adaptive kalman filter, which means the positioning accuracy of the system is improved. after using the updated sage-husa adaptive kalman filter, the maximum deviation between the sim and the predetermined path is 0.18 m, and the average deviation is 0.08 m; these deviations are within a reasonable range. this proves that the updated sage-husa adaptive kalman filter is applicable for the navigation of sprinkler irrigation machines. it is indicated in figure 7 that the filtering precision of the updated sage-husa adaptive kalman filter is higher than that of conventional kalman filter. this is because when the filtering process is abnormal, r k is not adaptive to the current filtering. thus, r k is estimated by using equation (23) in order to adapt it to the current filtering process. the influence of abnormal observations on the positioning accuracy of the system can be restrained by the updated sage-husa adaptive kalman filter. accordingly, the accuracy and stability of the filter can be improved effectively."
"regarding the assessment of changes in slope, two situations should be considered: when pre-intervention data are stable and when baseline data show an upward or downward trend. in case of stability, it is possible to use the stability envelope [cit] or the two-standard deviations band used in statistical process control [cit] . the twostandard deviations band implies computing the average of the data for a specific condition and representing it with a solid line. the standard deviation of the same data is also computed and two dashed lines are represented: one located two standard deviations below the mean and the other two standard deviations above. the basis of this procedure is that, for a normally distributed variable, few points (less than 5%) are expected to be out of these limits in case there is no change in the behavior with the introduction of the intervention. however, we suggest using it only as visual aid and not as a formal statistical procedure, as the data cannot be reasonably assumed to be normal, continuous, or independent. [cit] code 3 that only requires inputting the data and specifying the number of pre-intervention observations. as an example see the lower left panel of figure 2, indicating that the reduction in behavior is beyond what is expected only by random variability as there are multiple observations with values smaller than the lower limit."
"the solar photovoltaic module of the sprinkler irrigation machine converts solar radiation energy into electric energy through the photoelectric effect and stores the electric energy in its battery. part of the stored energy is converted into mechanical energy through the stepping motor. the output torque of the motor is transferred to the power that drives the wheels to move, steered by the planetary gear reducer with a deceleration ratio i of 10 and worm gear reducer with a transmission ratio of 80:1. the other part of the electric energy is converted into the pressure energy of the pump through the horizontal centrifugal pump, which is used to draw water from the water source. the stepper motors used in the sprinkler irrigation machine are equipped with driving controllers to control the speed of the driving wheels. considering the structure of the sprinkler, the efficiency of the driving system and the convenience of control, the wheel drive is chosen as the driving mode of the sprinkler irrigation machine. the driving mode of the wheel drive is shown in figure 2 . the main technical parameters of the self-developed sim are shown in table 1 . the main technical parameters of the self-developed sim are shown in table 1 ."
"for the virtual skyhook inerter system, since it is impossible to arrange an inerter between the inertial reference frame and the sprung mass in a moving vehicle, the continuously adjustable inerter is used to approximate the virtual inerter. in other words, the continuously adjustable inerter is installed in place of the virtual skyhook inerter shown in figure 1, and in this way, the virtual skyhook inerter system can be realized by the semiactive inertance control suspension shown in figure 3 ."
"in the present study, the position variable x, y and the speed variable v are chosen as observation variables. the positional information x and y can be measured by the gps receiver, and the speed information can be measured by speed sensors. then, the observation vector z can be expressed as"
"the self-developed sim is a kind of continuous straight-line moving machine. therefore, the experimental verification is conducted by taking linear motion as an example. in this part, the updated sage-husa adaptive kalman filter algorithm is applied in the navigation."
"plug-in technologies: as noted in section 4, the foundation of our approach is the assumption that all editors comply with the abstract editor architecture depicted in figure 4 . plug-in technologies such as adobe flash [cit] or microsoft silverlight [cit] do not adhere to this architecture. instead of leveraging the dom as a representation of the application model, plug-in-based web applications use the dom only as a container to embed the plug-in frame. user interactions within the plug-in frame cannot be monitored by the gci because they bypass the dom apis. hence, editors built upon plug-in technologies are not supported by the gci. only a complete reimplementation based on dom-standards would allow to adopt the gci."
"different from passive suspension, semiactive suspension can achieve more desirable performance by employing variable stiffness and damping. variable stiffness can be realized through air spring or hydropneumatic spring. these elastic components of suspensions are subjected to the static load of vehicle bodies and hence difficultly controlled. the control for dampers is relatively easy and widely used in semiactive suspensions. a number of semiactive strategies, such as skyhook, ground-hook, and hybrid, are proposed, and they can be realized through semiactive dampers [cit] . these semiactive damping control strategies can effectively minimize the vertical motion of the sprung or unsprung mass, but it can hardly reduce the variation of sprung mass natural frequency and ride comfort between unload and full-load condition. this is due to the fact that both the passive and semiactive suspensions mentioned above are based on the suspension struts of spring-damper rather than inerter-spring-damper."
"as can be seen in figure 9, the amplitudes of ba values are less in the unload passive suspension and more in the fullload passive suspension, and semiactive inertance control shock and vibration results are in between. according to table 4, the rms value of ba of semiactive inertance control suspension has 28.8% reduction compared with the passive suspension in unload condition. although semiactive control suspension cannot obtain the ride comfort of the full-load passive suspension, it can be concluded that semiactive control can significantly obtain ride comfort improvement compared with the unload passive suspension, owing to that it can make fuller use of the suspension stroke. this implies the semiactive inertance control suspension can obtain desirable ride comfort in both unload and full-load condition. more importantly, there is no significant deterioration in suspension working space and dynamic tire load, as shown in table 4 and figures 10 and 11."
"slope and level change quantifies two aspects of behavior's evolution after a change in the conditions: change in slope and change in level. actually, this procedure first estimates pre-intervention linear trend ( β a ) as the average of the differenced first phase measurements, that is,"
"from figure 6a, it can be seen that when the observations have large deviations, the data obtained by the conventional kalman filter also have large deviations. it is seen in figure 6b that the data obtained by the updated sage-husa adaptive kalman filter also have deviations; however, the deviations are smaller than that of the conventional kalman filter. for a more intuitive comparison, the deviations between the predetermined path and the observation points, the path obtained by using the conventional kalman filter (ckf) and the path obtained by using the updated algorithm (ua) are depicted in figure 7 . it is indicated in figure 7 that the filtering precision of the updated sage-husa adaptive kalman filter is higher than that of conventional kalman filter. this is because when the filtering process is abnormal, is not adaptive to the current filtering. thus, is estimated by using equation (23) in order to adapt it to the current filtering process. the influence of abnormal observations on the positioning accuracy of the system can be restrained by the updated sage-husa adaptive kalman filter. accordingly, the accuracy and stability of the filter can be improved effectively."
"in practice, the established mathematical model has difficulty reflecting the real physical process. when the mathematical model does not match the observations, the application of the kalman filtering algorithm can lead to filter divergence. in the updated sage-husa adaptive kalman filter algorithm, the self-adaptive filtering process can be realized by correcting the covariance matrix of the observation noise of the system. therefore, the updated sage-husa adaptive kalman filter algorithm has the advantages of more flexibility and reliability, and it can be used to reduce estimation error."
"as is shown in the lower half of figure 2, the displacement-dependent inertance valve is an internal-helix fluid inertance valve and consists of a moving valve element having a helical channel surrounding its outer surface inside a valve cylinder filled with a fluid. the valve cylinder has an enlarged radius of the internal surface in the right half part, so that the length of the helical path can be variable during the motion of the valve element. in the upper half of figure 2, a hydraulic cylinder consists of a moving piston inside a cylinder filled with the fluid, and the moving piston has a piston rod which is connected to the unsprung mass of the vehicle with the semiactive inertance control suspension. the cylinder is connected to the sprung mass."
"the filtering results for the navigation of the sim are shown in figure 6 . it is seen from figure 6 that the tracking path of the updated sage-husa adaptive kalman filter and conventional kalman filter are close. this is because, during most of the path tracking time, the filtering process of the updated sage-husa adaptive kalman filter is normal-that is, is adaptive to the current filtering-thus, does not need to be estimated. therefore, there are no great differences between the filtering results of the updated sage-husa adaptive kalman filter and conventional kalman filter."
"in essence, single-user editors that are eligible for the proposed generic transformation approach have to expose the following characteristics. they are (1) entirely based upon dom standards without leveraging plug-in technolo-gies, (2) are designed for multi-user readiness (e.g. identifier management does not break in multi-user scenarios), and (3) feature an application model that is represented by the dom."
"firstly, when visually inspecting the data, it has to be kept in mind that both phases are treatment phases and thus in both some reduction in child's behavior is expected and desired. moreover, it has to be taken into account that the pre-treatment (i.e., actual baseline) value is 82, equal to the first cdi phase measurement. at the beginning of the first phase there is actually a reduction, but then a new increment starts. considering this alternating pattern the cdi does not seem especially effective. given the amount of variability in the first phase, neither the central tendency measure (mean represented on the lower panel of figure 4 ), nor the different types of trend fitted (upper and middle panel) seem to represent the data well-enough. this can hamper the comparison between this condition and the subsequent one."
"the output synchronization mechanism was devised by lowet and goergen [cit] and the authors propose to enrich existing single-user web applications with co-browsing capabilities. a reference browser records all dom manipulations and sends them to all client browsers which replay the dom manipulations. distinguishing between the reference browser and client browsers is required, since only the reference browser is allowed to distribute dom manipulations. thus, conflicts cannot occur but the solution is not suitable for shared editing scenarios."
"in order to establish the kalman filter model, the recursive relationship among the system states should first be obtained. in this study, the recursive relationship among the system states is established based on the method of dead reckoning."
"although the aforementioned solutions may speed up the time-consuming task of implementing a web-based groupware system, they exhibit a number of limitations. first, the presented transparent adaptation scheme [cit] entails the implementation of a collaboration adapter. if the transparent adaptation approach is adopted naively i.e. the collaboration adapter leverages application-specific apis, the reuse of the adapter is not permitted and a re-implementation for each application is required. second, the output synchronization mechanism [cit] lacks a conflict resolution scheme. consequently, shared editing scenarios where multiple users edit the same document simultaneously are not supported."
"in this study, the kalman filter uses the information from the speed sensor and electronic compass-i.e., the longitudinal speed v and the heading angle β of the sim-to estimate the process state of the sim at some time according to equations (28) and (29). the kalman filter uses the information from the gps receiver and speed sensor of the sim-i.e., the longitudinal speed v and the position (values of variable x and y)-to obtain feedback in the form of measurements. assume that the heading angle β of the sprinkler irrigation machine is unchanged during one sampling period t. if the initial position of the sim is (x 0, y 0 ), the position of the next moment of the sim-i.e., (x 1, y 1 )-can be calculated by using the sampling time, the speed and heading angle of the sim, where the speed and heading angle of the sim are obtained by the speed sensor and electronic compass, respectively. then, the known position (x 1, y 1 ) can be used to calculate the position of the next moment of the sim; i.e., (x 2, y 2 ). the real time position during the movement of sim can be deduced by analogy. the recurrence formula of navigation position can be written as"
"and generates o 1, o 2 . executing the remote operations o 1, o 2 renders two documents both containing the identical string aabbc. thus, the conflict is successfully resolved. in this simple example, the transformation function f only has to respect the case where two insert operations are applied concurrently. if the simple text editor would also provide a delete operation, the function f had to consider 3 additional combinations of operations (s1, s2 delete concurrently; s1 deletes, s2 inserts; and s1 inserts, s2 deletes). since the transformation function f has to consider all possible combinations of operations, the number of conflict resolution cases grows quadratically with the number of operations. for instance, a regular graphics editor offering 15 operations requires 225 conflict resolution cases. a reusable collaboration infrastructure has to encompass all operations of all supported editors which may lead to thousands of conflict resolution implementations. creating such a complex system is neither feasible nor cost-effective."
"statistical significance (i.e., p-values) can be estimated for d and the generalized least squares procedure on the basis of the comparison between the test statistic and a theoretical reference (the sampling distribution) and allows making inference about the population from which the individual was drawn. in contrast, randomization tests [cit] ) yield a p-value on the basis of a comparison between the test statistic and an empirical reference -the randomization distribution. in the current context of two-phase studies, this reference is the distribution of the test statistic values quantifying the difference between the two conditions for each possible intervention start point (i.e., for each possible way in which the data series can be split into two; [cit] ) . for this analytical option the inference is restricted to the case studied, referring to the likelihood of obtaining such a large difference in case the intervention was ineffective. randomization tests are versatile in terms of test statistic to use (e.g., it can be an effect size such as a non-overlap index) and offer flexible options for dealing with different situations (e.g., [cit] ) . however, the necessary randomization as part of the data collection process is both a strength ) and a limiting characteristic [cit] ) in a clinical setting (i.e., criterion 4 \"absence of assumptions and restrictions of use\" is not met). moreover, in certain conditions type i error rates are not controlled [cit] . randomization tests can be recommended when the aim is to obtain statistical significance and the point(s) of change in the conditions can be chosen at random. randomization tests are also accompanied by freely available software (bulté [cit] ."
"in this paper, we introduced an approach capable of transforming existing single-user web applications into collaborative multi-user applications. consequently, the reuse of existing application functionality and complex collaboration functionality is promoted and the development effort for multi-user applications is crucially reduced. furthermore, users can collaborate using applications they are familiar with."
"high load of dom events and operations: certain editor operations affect numerous dom nodes and produce vast sets of dom events. all events are processed by the gci and will in the end be reflected in dom replay operations. in rare cases, these high loads can crucially impair the performance and responsiveness of all application instances. examples for high load scenarios are: (1) fade animations or drag shape operations (produced up to 150 domattrmodified events per second), (2) copy and paste operations involving numerous objects (trigger multiple domnoderemoved and domnodeinserted events), and (3) format operations addressing various dom nodes (e.g. change the fill color of 100 table cells). this issue could be tackled by installing an operation composer capable of reducing the number of operations."
"where t is the sampling period, and β k−1 is the angular components in the planar coordinates of the gauss projection at time k − 1."
"multilevel models are an extension of piecewise regression and can be used to model several data aspects (e.g., trend, autocorrelation, heterogeneous data variability across phases) and they yield estimates of the change in the same measurement units as the target behavior and their statistical significance [cit] ) . the main drawbacks of multilevel models are the problematic estimation of variance [cit], their relative complexity for applied researchers with less statistical knowledge and the fact that they the replication of the intervention in several participants. actually, such a complex procedure is more suitable for more complex design structures that the two-phase ab [cit] b) . finally, most implementations of this analytical procedure have been done in commercial software (e.g., [cit] include sas code in their article)."
"according to the calculation method of the inertance of the fluid inerter [cit], the inertance of the continuously adjustable inerter can be expressed as"
"the former iso/iec standard defines the following software quality characteristics: (1) functionality, (2) reliability, (3) usability, (4) efficiency, (5) maintainability and (6) portability. according to bevan [cit], assessing software quality from the end user's point of view encompasses mainly characteristics (1) to (4) . since the conducted user study addressed the end user, we reflected the four iso/iec product quality characteristics in questions 1-16 (cf. figure 8 )."
"let 1 be the effective cross-sectional area of the valve element, namely, the working area of the valve element. let be the radius of the valve element, be the radius of the control rod, ℎ be the radius of the helical channel, ℎ be the pitch of the helix, and be the valve element width. let 2 be the channel cross-sectional area and be the fluid density. let be the relative displacement between the valve element and the valve cylinder, be the channel length, and ( ) be the length of the helical path. let 1 be the effective crosssectional area of the piston, c be the radius of the piston, and c be the radius of the piston rod."
"in case the pre-intervention data show a trend, it is necessary to compare the projection of this trend and the actually obtained measurements . for that purpose, there is another potentially useful r code 4 which allows applying the stability envelope to the trend line: (a) estimating split-middle trend [cit], (b) projecting it into the next phase, and (c) constructing an envelope around it. the envelope can be constructed on the basis of the baseline median 5, so that the lower limit is located 25% of the median below the estimated split-middle trend and the upper limit at the same distance above it [cit] . in case 80% of the data are within those limits, this would indicate trend stability, that is, it would suggest that no change in slope has been produced with the introduction of the intervention. for using this code only data input is required before copy-pasting it in r. the lower right panel of figure 2 shows an example with elizabeth's data. given that the projected trend and its stability envelope are lower than the actual observations, this is the only piece of graphical information that does not suggest improvement in the behavior, but practitioners should be cautious when trend is estimated from as few as four observations and when it is projected farther away in time into values that are out of the range of possible measurements [cit] b) ."
"in the study of navigation for the self-developed sim, the positional variables x, y and the speed variable v are chosen as the system state variables; then, the state vector x can be expressed as"
"these requirements reflect the aspects of a study or a professional practice that moderate the extent to which its findings are \"solid evidence\" and also affect the practitioner's confidence in the conclusions regarding intervention effectiveness. accordingly, using a sub-optimal two-phase design such as ab (referred to as \"pre-experimental, \" [cit], or \"quasi-experimental, \" [cit] ) is a drawback, but it does not necessarily preclude a study from being useful 1, as there are other characteristics that can increase the credibility in the obtained results. in the present work, we focus on one of these aspects -data analysis -showing how to meet the condition for an appropriate data analysis."
"considering that the longitudinal speed of the sim is small and constant during operations, and the guiding path for the navigation of the sim is generally a straight line, the kinematic model of the self-developed sim depicted in figure 3 is employed for analysis. the main technical parameters of the self-developed sim are shown in table 1 ."
"in this paper, to simulate the working condition of a light commercial vehicle, the passenger or cargo mass is chosen to be 1000 kg, which means the sprung mass in unload condition 2 is 500 kg while that in full-load condition 2 + is 1500 kg. to ensure that the natural frequencies are similar in the two different conditions, the semiactive inertance control is applied to simulate the full-load condition when the vehicle is actually in unload condition, in which the sprung mass 2 is 500 kg while the inertance of skyhook inerter sky is 1000 kg."
"dead reckoning is a commonly used positioning method for autonomous navigation. in the dead reckoning method, the recursive method is employed to accumulate the moving distance of the vehicle and the direction relative to the known points, and then the position and direction of the vehicle are calculated in real time according to a certain reference position. the schematic diagram of dead reckoning is shown in figure 4 ."
"another issue with the analytical method is that it might fail in certain situations such as the ones described in this paragraph (the list is not necessarily comprehensive). first, it is possible that the pre-intervention phase is too short or the measurements too variable for estimating trend with precision: the slc quantifications would be less useful, but if there is no clear evidence of trend, then the nap can be used as main quantification. second, if there is complete non-overlap between the observations of the two conditions, the nap will not be very informative, but the slc can be used as an unstandardized quantification of the amount of difference and the d-statistic as a standardized quantification if more than one participant is being studied. third, there might be a non-linear trend present in data, which is not an optimal situation for applying the slc. in such case running medians [cit] can be used as a visual aid via the scda plug-in for r, while data modeling via the generalized least squares approach and loess is also possible. fourth, there might be a delayed change in the behavior, not occurring simultaneously with the change in conditions [cit] . in such case, the descriptive statistics will reflect the delay with lower quantifications of the effect, but it would be crucial to explore the cause of the change among the external uncontrolled factors (i.e., the solution is not an analytical one), given that the immediacy of the effect is one of the cornerstones for demonstrating causality ."
"in this section, we discuss the limitations of the generic transformation approach and present crucial properties that a web editor must exhibit in order to allow a conversion into a collaborative editor with shared editing capabilities. from our experiences gathered over the last 12 months, the following limitations could be noted:"
"d k is the amnestic factor and b is the forgetting factor in the range of 0 and 1, p k is the covariance matrix of state estimation, p k,k−1 is the one-step estimation variance matrix, k k is the filter gain, v k is the remainder vector, and r k is the observation noise covariance. according to equations (17)-(23), it is demonstrated that the variance matrix of the observation noise r k should be calculated in the filtering process for every parameter k through the sage-husa adaptive filter algorithm, which leads to increasing filtering complexity and an extra amount of computation. therefore, it is difficult to guarantee the real-time performance of the sage-husa adaptive filter algorithm in practical application. in order to apply the sage-husa adaptive filter to a practical situation, its real-time performance should be updated."
"the resultant force applied to the sprung mass generated by the continuously adjustable inerter includes the inertial force and the parasitic damping force, and it can be represented as"
"the sage-husa adaptive filter is one of variants of the conventional kalman filter; that is, the sage-husa adaptive filter algorithm is proposed based on the conventional kalman filter algorithm. the calculation flow for the sage-husa adaptive filter algorithm can be described as follows [cit] :"
"dead reckoning is a commonly used positioning method for autonomous navigation. in the dead reckoning method, the recursive method is employed to accumulate the moving distance of the vehicle and the direction relative to the known points, and then the position and direction of the vehicle are calculated in real time according to a certain reference position. the schematic diagram of dead reckoning is shown in figure 4 . assume that the heading angle β of the sprinkler irrigation machine is unchanged during one sampling period t. if the initial position of the sim is (x0, y0), the position of the next moment of the sim-i.e., (x1, y1)-can be calculated by using the sampling time, the speed and heading angle of the sim, where the speed and heading angle of the sim are obtained by the speed sensor and electronic compass, respectively. then, the known position (x1, y1) can be used to calculate the position of the next moment of the sim; i.e., (x2, y2). the real time position during the movement of sim can be deduced by analogy. the recurrence formula of navigation position can be written as"
"the user study was carried out employing the usability testing technique [cit] with 30 participants that had to solve two collaborative tasks in teams of two. while the first collaborative assignment aimed to exploit the shared drawing capabilities of the converted graphics editor svg-edit, the second team assignment encompassed a shared editing scenario using the transformed ckeditor. in this user study, students studying computer science or a related subject took part."
"a second question concerned the availability of tools for implementing the procedures proposed as part of the analytical method. we have mentioned, referenced, and illustrated the output of several tools implemented in the freeware r. some of them are based on clickable menus, whereas others only require inputting the data before copying and pasting the code. the availability of software is crucial for eliminating the errors in obtaining the numerical and graphical results and in terms of time efficiency, both for short and relatively straightforward data series (e.g., [cit] ) and for longer series with and less visually clear data patterns (e.g., [cit] ) ."
"besides the complexity of the collaboration infrastructure injected by the multitude of editor operations, another challenge is heterogeneity of editor apis. editor apis are required in order to record local operations and to replay remote operations. for example, to support the scenario illustrated in figure 3, the simple text editor needs to provide an api capable of informing about insert notifications and allowing for character insertions. thus, all local insert operations could be recorded and eventually be replayed. however, the task of adapting the collaboration infrastructure to various editor apis is time-consuming since multiple editors may offer hundreds or even thousands of operations altogether. not only is the editor-specific adapter implementation costly, but also the familiarization process with numerous apis is tedious and demanding. this is particularly true for real-life examples such as the ckeditor with a code base of 110 000 lines or the svg-edit exposing 30 000 lines of code."
"as a limitation of the quasi-statistical component of the analytical method, it is debatable whether the numerical results can be presented confidently in absence of a conventionally accepted optimal procedure, i.e., when all analytical techniques can be criticized. considering the analytical method as a whole, further discussion is necessary on how to proceed when practitioners are faced with data that cannot be easily analyzed visually or quantitatively (e.g., short series, great data variability). one option would be to use the substantive criteria as basis for the conclusions and label the study as \"practice\" but not as \"research.\" in contrast, when all three pieces of information (visual, quantitative, and substantive) coincide, it still has to be kept in mind that not meeting current standards ) could render two-phase studies only a \"pilot\" status and, when included in meta-analysis, they are likely to be assigned lower weights and have less influence on the summary measures obtained."
"our proposal is to use the option of \"visual analysis aided by quasi-statistical techniques, \" where the latter are understood as descriptive measures that do not intend to yield statistical significance values due to various reasons. first, visual analysis is not only frequently used, but it is apparently the only kind of single-case data analysis that researchers seem to agree that is necessary (e.g., [cit] . second, the evidence on visual analysis suggests that its exclusive use is potentially problematic (i.e., visual analysis is not sufficient) and techniques increasing the reliability of visual analysis are necessary [cit] . third, we consider that certain quasi-statistical techniques with favorable evidence for their performance can be used as natural complements of the commonly used visual analysis, as they share the emphasis on the same main data features (overlap, level, and trend), whereas the visual aids also take data variability into account and allow comparing projected and actual data. fourth, applied researchers may not be willing to use the more complex statistical techniques whose results are more easily misinterpreted, in case of incomplete understanding of what exactly is being done with the data. fifth, the use of inferential statistical procedures may not be fully justified in the absence of random sampling [cit] . moreover, an inference to a population is not necessarily an aim of idiographic research [cit] ) that focuses on the needs and the improvement of the individual clients. sixth, easy to use software is available for the descriptive statistical procedures recommended here."
"the gci described in the last section was adopted to convert the presented graphics editor svg-edit and the word processor ckeditor. in this section, we report on the results of a user study analyzing the software and collaboration qualities of transformed editors."
"in order to establish the kalman filter model, the recursive relationship among the system states should first be obtained. in this study, the recursive relationship among the system states is established based on the method of dead reckoning."
"in the present section, we will illustrate the application of the analytical method and the information that can be obtained via visual and quantitative analyses, while also considering substantive criteria. this application focuses on the family context, where it is common to gather data before and after an intervention [cit] . one of the empirically supported interventions in this context is the parent child interaction therapy (pcit; [cit] ), which has been reported to increase positive parent behavior and reduce child behavior problems [cit] will be used. the participants are a 23-months-old premature-born child displaying difficult behaviors and his mother. the application of the pcit focuses on teaching parenting skills in order to improve the interaction with the child and to decrease his externalizing behavior. teaching takes place in two phases. first, child-directed intervention (cdi) takes place. it is similar to play therapy: the child is the leader and the parent has to learn how to act positively (e.g., praising the child, imitating the child's play). second, parent-directed intervention (pdi) phase occurs. it is similar to clinical behavior therapy: the parent is more directive and has to improve her way of disciplining so that a greater compliance is achieved. in order to assess intervention effectiveness, several sources of information are used: parent reports provided via inventories, observation of the parent-child interaction, and physiological measurements. in the running example, we focus on the parent weekly reports obtained via the intensity scale of the eyberg child behavior inventory (ecbi; [cit] ) on disruptive behavior, although a complete assessment entails exploring whether all available information converges to the same conclusion. [cit] ecbi data were chosen here given that there is a cut-off point at a t-score of 60 which indicates clinically significant results and eases the interpretation in substantive terms. the data gathered 9 on the ecbi scale are represented on figure 4 . the upper panel contains ordinary least squares trend lines provided by the scda plug-in for r, the middle panel contains split-middle trend for the first phase, and the lower panel represents the application of the two-standard deviations band fit to the first condition's data and projected into the second one."
"the present work focused on the question of what can be done to improve the data analysis in studies/practices using sub-optimal designs in such a way that results are more useful to the discipline. we recommended an analytical method consisting of structured visual analysis complemented with descriptive statistical procedures, while also keeping in mind substantive criteria (i.e., the opinion of the individuals involved in the process: family members, teachers, peers, coworkers, or supervisors). on the one hand, quantifications are useful for summarizing different aspects of the data and making the results available for subsequent meta-analysis. on the other hand, visual analysis is required for gaining an in-depth knowledge of the data and for assessing the adequacy of any specific quantitative procedures, due to the lack of consensus regarding the most appropriate technique [cit] ."
"visual analysis has been and still is popular among professionals in their everyday psychological practice [cit] and is still advocated for [cit] ) and used as a gold standard for assessing quantitative procedures . visual analysis has been considered both appropriate and sufficient for data gathered longitudinally [cit] . however, this sufficiency has been defended only for experimental studies [cit], which points at the need for complementing it with a quantitative procedure. [cit] advise for systematic visual analysis and it necessarily starts with assessing the baseline, specifically, whether the intervention can be introduced or it should be postponed until stability is reached [cit] ). alternatively, deterioration in the behavior of interest would suggest even more clearly the need for intervention. in that sense, deterioration is not expected to interfere with subsequent conclusions about intervention effectiveness [cit], given that it allows exploring whether an intervention reverts the situation. nonetheless, it is possible to assess intervention effectiveness even when the behavior is already improving before the intervention itself, as it will be shown later."
"in relation to the previous point, there is evidence that their performance is appropriate for a variety of single-case data patterns [cit] . nap is a suitable indicator when data is stable and even when data is variable. in contrast, in such situations visual analysis is more difficult to perform and means and medians are not informative and trends are not estimated with precision. on the other hand, nap is not suitable when the data show improving trend, but slc can be applied in such a situation -this complementarity relates to criterion 3 \"synergic application.\" slc is useful for separately quantifying the change in level and the change in slope in potentially meaningful terms. in relation to this criterion, it is important to discourage the use of methods for comparing conditions that have been shown not to perform appropriately, such as the binomial test applied after the split-middle method [cit] which does not control for type i error rates, itsacorr which presents modeling flaws [cit], or the c-statistic [cit], which is actually an estimator of autocorrelation [cit] ."
"one potential issue with the analytical method is that it is possible that, in some instances, the three components do not coincide. a cautious approach would be to gather follow-up data after a certain period of time in order to check whether the initial ambiguous result of the assessment still holds. in case the unclear change is maintained and perceived as a change by the participants, then there would be evidence in favor of its practical importance. if there is disagreement between the substantive criterion and the other two components, we think that if the clients' well-being, quality of life, functionality, performance, etc. is improved according to their own opinion, then the substantive criterion should prevail, regardless of its numerical expression. in any case, the general effectiveness of an intervention depends on replications [cit] and not on the numerical result in a single study. finally, if there is a divergence between the visual and quantitative information, it is important to know: (a) whether there is any data feature (e.g., preintervention trend, outliers) that might affect the performance of the quantitative analysis -in such case visual inspection should prevail; or (b) whether the data pattern prevents from getting a clear visual impression (e.g., due to highly variable data and/or a complex design structure) -in such case the quantitative summary is potentially more useful."
"in the case that the continuously adjustable inerter device is used to approximate the skyhook inerter, is equal to s (see (1) ); that is,"
"at present, agricultural machinery navigation methods mainly include machine vision and satellite positioning. since machine vision is sensitive to the external environment, it is difficult to meet the requirements of autonomous navigation in different environments [cit] . satellite positioning navigation has become a research hotspot in recent years, but its navigation accuracy needs to be further improved [cit] . generally speaking, a single navigation mode is limited by a lack of effective information, and it is difficult to continuously provide high-quality location information. [cit] pointed out that, due in large part to advances in digital computing, the kalman filter has been the subject of extensive research and application, particularly in the area of autonomous or assisted navigation. kalman filter technology, which can improve positioning accuracy, is widely used in navigation systems, and many attempts at the application of the kalman filter in navigation have been reported."
"to sum up, the self-developed sim has good navigation performance, and the updated sage-husa adaptive kalman filter can be applied in the navigation of the sim. the average deviation, maximum deviation and deviation variance of position for the sim, which are obtained before and after the filtering process through the updated sage-husa adaptive kalman filter, are shown in table 2 . table 2 . error statistics before and after the filtering process."
"naming collisions: converting single-user editors into multi-user editors exposed naming conflicts. some editors manage their content identifiers by means of an incremented integer. if two users simultaneously create a content object, both application instances assign the very same integer identifier to two independent objects and therefore, the uniqueness of identifiers is no longer ensured. however, this limitation can typically be addressed through a lightweight change of the editor implementation."
"to achieve this, we proposed a generic collaboration infrastructure consisting of an operational transformation engine for concurrency control and a generic collaboration adapter to couple the ot engine with existing web applications. a generic collaboration adapter renders the approach to be application-agnostic since neither changes to the application source code are required nor an application-specific adapter has to be implemented. nevertheless, tailoring the generic adapter to the dom api layer imposes limitations to its applicability. for example, applications based on browser plug-in technologies are not supported."
"considering specifically observational studies in which data is recorded continuously within a session, it is possible to follow an analytical approach different from the one used in single-case designs, namely, to apply sequential analysis to explore whether the occurrence of some behaviors make more or less probable that other behaviors take place [cit] . additionally, longer series of data gathered across time can be analyzed using markov chains or analyses of rhythm, according to the aims of the study [cit] ."
"in this paper, the self-developed translational sim is taken as the research platform, as shown in figure 1 . the self-developed sim is composed of a solar photovoltaic power supply part, mechanical structure part and control system part. it is illustrated in figure 1 that the self-developed sim mainly includes a solar photovoltaic module, battery, main water pipe, water pump, steering motor, highclearance chassis, control cabinet, walking motor reducer, nozzle and so on."
"whereas sma uses monte carlo methods or bootstrap for generating samples and estimating the likelihood of the value of test statistic in case there is not difference between conditions, bootstrap has also been suggested for single-case as a way of reducing bias and estimating standard errors [cit] and specifically for estimating confidence intervals of regression-based r-squared values [cit] . this option has not received much attention lately and it is unclear whether applied researchers would be willing to use it."
"in addition to the introduced frameworks primarily targeting the development of novel multi-user applications, there are also approaches supporting the transformation of existing single-user applications into collaborative multi-user applications."
"the iso/iec standard [cit] specifies only the characteristics without providing standardized questions and therefore, specific questions tailored to assess functionality, reliability, usability and efficiency for collaborative editors had to be authored. in addition to incorporate a validated and widely accepted standard, we also included software metrics denoted as mechanics of collaboration (moc). the moc respects communication aspects (e.g. workspace awareness) as well as coordination aspects (e.g. shared access to tools, objects, etc.). questions 13-22 (cf. figure 8 ) address these particular moc quality characteristics."
"the conducted user study has revealed the importance of awareness features in shared editing sessions. therefore, in future gci releases, the workspace awareness will be revised and extended. in addition, the gci will be extended by an operation composer to reduce the number of synchronization messages and eventually to improve the gci performance."
"where δ is the pressure loss of the fluid flowing through the helical path and is the viscosity of fluid. hence, the coefficient of parasitic damping of the hydraulic inerter device can be presented as"
"a continuously adjustable inerter device, a combination of a hydraulic cylinder and a displacement-dependent inertance valve, is proposed to apply to the semiactive inertance control suspension, which is illustrated in figure 2 ."
"the structure of this article is as follows. first, we comment on the characteristics of non-experimental studies in order to frame a context, where improvements are required [cit] . second, we present an analytical method meeting the criterion for appropriate data analysis; we refer to its strengths, limitations, and alternatives. third, we apply the analytical method to a real data set. fourth, we point out several analytically challenging situations and present our own advice to practitioners and applied researchers. with the justification and illustration of the analytical method and the software, we aim to offer practitioners and applied researchers a useful tool, and indications about its alternatives."
"to sum up, the self-developed sim has good navigation performance, and the updated sage-husa adaptive kalman filter can be applied in the navigation of the sim. the average deviation, maximum deviation and deviation variance of position for the sim, which are obtained before and after the filtering process through the updated sage-husa adaptive kalman filter, are shown in table 2 . table 2 . error statistics before and after the filtering process."
"generalized least squares regression analysis [cit] ) also enables computing an effect size index. its strengths include the fact that it can take into account changes in level and in slope (although they are quantified as part of the same overall indicator, unlike slc), the versatility in modeling (e.g., controlling for linear and non-linear trends), and that it deals explicitly with autocorrelation. however, autocorrelation estimation has been shown to be problematic [cit] b) and the analytical procedure requires several steps, some of them taking place iteratively (i.e., criterion 1 \"simple to compute\" is not met). this procedure is applicable to longer data series for which autocorrelation can be estimated with greater precision. moreover, we recommend that practitioners work together with a statistician, so that the analysis can be properly run. [cit] compared the agreement between visual analysis and several regression-based approaches and the best performer in this terms (related to criterion 2 \"complementary to visual analysis\") [cit] method, which is however affected by autocorrelation [cit] conducted their study and more evidence is necessary to assess its performance."
"mobwrite [cit] also facilitates real-time synchronization and collaboration services. it is based on the differential synchronization algorithm [cit] instead of the established ot algorithm. the mobwrite system is devoted to synchronize the content of html form elements. to integrate mobwrite into an existing single-user application, only one specific javascript library has to be added to the html file without having to change the application code. mobwrite easily extends existing web applications with form synchronization capabilities. nevertheless, the framework supports only a small fraction of potential collaborative applications."
"the autonomous navigation of agricultural machinery can not only solve the problem of insufficient labor force, but also reduce energy consumption, reduce costs and improve agricultural production efficiency. however, the accuracy and reliability of agricultural machinery navigation has been restricting the level of autonomous operation of agricultural machinery. therefore, agricultural machinery navigation technology has become one of the hot topics in current agricultural machinery research."
"in this study, the kalman filter uses the information from the speed sensor and electronic compass-i.e., the longitudinal speed v and the heading angle β of the sim-to estimate the process state of the sim at some time according to equations (28) and (29). the kalman filter uses the information from the gps receiver and speed sensor of the sim-i.e., the longitudinal speed v and the position (values of variable x and y)-to obtain feedback in the form of measurements."
"in order to verify the navigation performance of the self-developed sim and test the positioning accuracy of the sim by using the updated sage-husa adaptive kalman filter algorithm in navigation, the navigation tracking experiment is conducted. the experimental scenario is shown in figure 5 . in the experiment, the sim tracks along the predetermined path. a straight path is planned on the cement road for the verification experiment: the initial coordinates of the path are (3800970.522, 782548.0014), and the destination coordinates of the path are (3800967.693, 782516.8791). in order to facilitate the analysis and calculation, the relative coordinates, which are the difference between the collected coordinate data and initial coordinates, are used."
"in this section, we revisit the outlined challenges, describe the gci architecture consisting of the generic collaboration adapter (gca) as well as the operational transformation engine (ote) and demonstrate the conversion of single-user editors into multi-user editors. moreover, we describe the processes to record as well as to replay dom manipulations and conclude with the presentation of our gci implementation."
"assessing the relevance of an intervention cannot be constrained solely to visual and descriptive or inferential statistical analyses. it is important to assess aspects such as quality of life [cit], whether the behavior has moved from dysfunctional to functional ranges [cit], without forgetting subjective evaluation [cit] . regarding the latter, highlight the need to get to know the perceptions of the client and of significant others. according to the specific context being studied, these significant others would be the family members (parents, siblings, marital partner), the teacher, the coach, or the boss (as figure with a higher hierarchical role), and friends, classmates, or colleagues (at the level of \"peers\"). [cit] has referred to these groups of people as \"paraprofessionals, \" as they help detecting the behavior that requires intervention and they can also be the agents reinforcing the behavior of interest (e.g., a mother reinforcing a child's disruptive behavior by paying attention to it) or producing stimuli for discriminating conditions in which certain types of behavior are desirable (e.g., a boss may encourage jokes with one type of clients and more distant behavior with others)."
"the performance of the semiactive suspension employing a modified skyhook-inertance control strategy was studied. this modified control strategy was proposed to adapt to the parasitic damping inherent in the hydraulic device of continuously adjustable inertance, so that the device can be controlled to approximate the virtual skyhook inerter which is not passively implemented in moving vehicles. in the modified control strategy, the resultant force including the inertial force and the parasitic damping force generated by the hydraulic device is set to equal the force generated by the virtual skyhook inerter, to virtually increase mass to the sprung mass. the semiactive suspension exhibits good performance of frequency responses to sinusoidal excitations owing to its lower sprung mass natural frequency. it has shown good reduction of vibration at sprung mass natural frequency without significant deterioration at higher frequencies. also, the rms values of time responses to a random excitation demonstrate the semiactive suspension achieves over 28% improvement on ride comfort, compared with the passive suspension in unload condition. this implies the semiactive suspension can obtain desirable ride comfort in both unload and full-load condition. moreover, the simulation results of suspension working space and dynamic tire load show that the semiactive suspension is considered as a good compromise between ride comfort, suspension displacement, and road holding."
"starting our discussion from procedures similar to the ones included in the analytical method, tau-u [cit] b ) is closely related to nap and it is preferable when pre-intervention trend is present in the data. for both tau-u and nap p-values have been offered, although their basis has not clearly been explained in the presence of autocorrelation. however, tau-u is interpretatively and computationally less straightforward than nap (i.e., criterion 2 \"complementary to visual analysis\" is met to a lesser extent). for instance, even in case a baseline trend is generally deteriorating, if there is a single improving value in the baseline phase, as compared to a previous baseline data point, this would reduce the value of the non-overlap index. thus, in case trend is not reasonably clear, tau-u can be an excessively conservative procedure (i.e., it would overcorrect). furthermore, more evidence is required on its performance (thus the abovementioned criterion 5 \"appropriate performance\" [cit],b, offer only applications to real data, but no simulation study)."
"where z k is the observation at time k, h is the observation matrix, x k is the state vector at time k, and the observation noise e k is the gaussian white noise with zero mean."
"in this section, our work is put in context of existing collaboration frameworks and approaches that are commonly used to drive development efficiency implementing interactive groupware systems."
"vehicle suspension is designed to isolate the passengers from road disturbances, to limit rigid body motion of the vehicle, and to force contact between the wheels and the road, respectively, called ride comfort, suspension travel, and road holding [cit] . although multiobjective optimization is a trend in suspension design, the most important and basic issue to be studied is the ride comfort [cit] . it has been theoretically proved that vehicle suspensions can obtain the improvement of ride comfort with low sprung mass natural frequency [cit] . ride comfort of vehicles, especially for that of trucks, which have passive suspensions with stable design parameters such as spring stiffness and damping coefficient, is mostly influenced by loading conditions [cit] . compared with unloaded vehicle, full-load vehicle has lower sprung mass natural frequency and more desirable performance on ride comfort owing to the larger sprung mass."
"in this study, the self-developed translational sim is taken as the experimental platform. the navigation control system of the self-developed sim is composed of a navigation controller, gps receiver, electronic compass, speed sensor, speed controller and so on. the navigation controller is used to implement the navigation control algorithms, generate and output the control commands; the gps receiver is employed to obtain the position information (values of x-coordinate and y-coordinate); the electronic compass is used to measure the angular components in the planar coordinates of the gauss projection; and the speed sensor is used to obtained the longitudinal moving speed of the sim."
"movement of the piston causes the fluid to flow through the helical path which generates an inertial force due to the moving mass of the fluid in the helical path. during the motion of the valve element, the length of the helical path is changed, and the mass of the fluid in the helical path is successively varied. consequently, the inertance is varied. since the relative displacement between the control rod and the valve cylinder is continuously controllable, the inertance of the proposed inerter can be continuously adjustable."
"in this paper, a scheme for hydraulic device of continuously adjustable inertance is proposed. the novelty of this work is that not only the inertance but also the parasitic damping of the device is taken into account in the skyhookinertance control strategy. this modified strategy is proper for the hydraulic inerter device to approximate the virtual skyhook inerter. it is proposed for light commercial vehicle to control the hydraulic device, to reduce the variation of sprung mass natural frequency and ride comfort between unload and full-load condition."
"in extensive engineering applications, the actual value of the state variables of a system usually cannot be obtained directly; they can be extracted from observations which have random noise. the kalman filter is an effective method to obtain the actual value of the system state variables by analyzing the observations of the system. generally, the kalman filter is a linear minimum-variance filter, which can be used to estimate and correct the system state by using an iterative algorithm."
"in order to conduct a thorough user study allowing for resilient conclusions, we set out to identify relevant metrics for the evaluation of multi-user editors. on the one hand, converted editors represent regular software tools that have to be assessed against general software qualities such as functionality, usability or reliability. on the other hand, the user study is a means to evaluate the collaboration quality of converted editors. the collaboration characteristics are emphasized since they validate the usage of the gci to transform single-user editors into multi-user editors. based on these considerations, we selected two established software metrics: (1) the iso/iec tr 9126 standard for product quality [cit] exposing various evaluation criteria for arbitrary software products and the groupware-specific mechanics of collaboration catalog [cit] ."
"the specific data aspects, which are foci of attention, are the amount of overlap between data in the different conditions, within-and between-phase variability, slope and level change (slc; [cit] ) . a more objective assessment of the degree to which data share the same values (i.e., overlap), whether levels and trends are similar across conditions, and whether data become more stable or more variable after the intervention can be done using visual aids instead of relying on naked-eye impressions. finally, visual analysis focuses on the whole data pattern in order to assess whether it resembles the expected one, that is, a consistent improvement only during intervention. summarize the overall assessment as a comparison between projected and actually obtained measurements. specifically, in two-phase designs, it is relevant to project the baseline (in case it is stable or presents trend stability) into the intervention phase and compare this projection with the real treatment phase data."
"despite these guidelines on visual analysis, there are still no soundly based formal decision rules for all data aspects that are visually assessed [cit] and objective and replicable outcomes are also missing [cit] . these two drawbacks might be among the reasons for the frequently reported inadequate performance of visual analysts [cit], for a recent meta-analysis reporting insuficient interrater agreement, especially among single-case experts). moreover, the visual analysts' decisions are not directly useful for documentation or for meta-analysis [cit], which would allow establishing the evidence basis for interventions [cit], especially as generalization in single-case studies depends on replication 6 rather than on random sampling and statistical inference. as a result of these limitations, there is a consensus that visual and quantitative analyses should be used jointly [cit] ."
"to address these limitations, we propose a novel generic collaboration infrastructure (gci) for web applications comprising a generic collaboration adapter and an operational transformation engine. while the generic collaboration adapter is capable of tracking dom manipulations (e.g. create, delete, and modify dom elements), the operational transformation engine provides a synchronization service as well as a conflict resolution mechanism based on the prevalent concurrency control algorithm called operational transformation (ot) [cit] . besides implementing the proposed gci, we have validated the transformation of single-user applications into multi-user applications with two distinct webbased applications: the graphics editor svg-edit [cit] and the word processor ckeditor [cit] . after transforming svg-edit and ckeditor, the two collaborative editors were evaluated conducting an extensive user study."
". the filtering results for the navigation of the sim are shown in figure 6 . in the experiment, the sim tracks along the predetermined path. a straight path is planned on the cement road for the verification experiment: the initial coordinates of the path are (3800970.522, 782548.0014), and the destination coordinates of the path are (3800967.693, 782516.8791). in order to facilitate the analysis and calculation, the relative coordinates, which are the difference between the collected coordinate data and initial coordinates, are used."
"scattered application state: besides maintaining a complete external data model, there are also applications where parts of the applications' state reside in a separate javascript data structure. for example a select all operation capable of highlighting all shapes on the canvas of a graphics' editor might be realized using a javascript array where all object references to created shapes are preserved. assuming participant 1 inserts a new rectangle, the dom is immediately synchronized and participant 2 can instantly see the novel rectangle. however, the javascript array holding all shape references is only updated locally since the sync mechanism exclusively encompasses dom objects. the erroneous application behavior is disclosed once participant 2 carries out the select all function and notes that the newly inserted rectangle is not highlighted. fixing this limitation can be tedious since there might be a multitude of individual javascript data structures representing parts of the application state."
"an effect size index can also be computed from interrupted time series analysis via arima (autoregressive integrate moving average) models, which allow controlling for trend and autocorrelation [cit] . the main difficulties of this option are the need for long data series and the problematic initial model identification step. however, there have been suggestions for using some general models that make model identification unnecessary [cit] . a recent application of arima models has shown that these can be applied to two-phase data, but there might be convergence problems and, more importantly, the agreement with visual analysis is low [cit] . we consider that this latter drawback and the relative complexity of the technique make it less attractive to applied researchers with no statistical expertise."
"the sage-husa adaptive kalman filter and conventional kalman filter both have advantages and disadvantages. the sage-husa adaptive kalman filter has higher estimation accuracy; however, it has increased filter complexity, caused by large amount of computation. on the other hand, the conventional kalman filter has higher computational efficiency, but its estimation accuracy is low. on the basis of the covariance matching method, the sage-husa adaptive kalman filter algorithm can be updated in order to reduce the amount of computation and improve the real-time performance of the algorithm."
"the initial idea was due to jl and it was subsequently complemented and further developed by rm. the manuscript was written by jl (observational, non-experimental conceptual part in the introduction) and rm (analytical part in the analytical method explained, analytical method applied, and discussion). sc-m and ss-c made substantial contribution to the design of the work. all four authors (rm, jl, sc-m, and ss-c) participated in several revisions during the process of creating, discussing, and improving the manuscript, with rm leading all revisions and guiding the continuous improvement of the manuscript; gave their consent that this final version is submitted for publication; and agreed in their co-responsibility regarding all aspects of the work, such as the accuracy of the data and the integrity of the research."
"a virtual skyhook inerter system is aimed at adding virtual mass to the sprung mass and reducing natural frequency of the sprung mass. the virtual inerter is inserted between the sprung mass and the stationary sky (the imaginary reference frame) not only as a way of applying the inertial force to the sprung mass, but also as a tool of computing the desired inertial force. figure 1 presents the virtual skyhook inerter system arrangement. consider a quarter vehicle model made up of a sprung mass ( 2 ) and an unsprung mass ( 1 ). a virtual inerter with the inertance sky is placed between the inertial reference frame, rather than the moving surface, and the sprung mass. therefore, the inertial force is proportional to the absolute acceleration of the sprung mass. a spring with the stiffness coefficient and a damper with the damping coefficient are settled between the unsprung mass and sprung mass. the tire is modeled by a spring with the stiffness coefficient . in this model, 2 (resp., 1 ) is the vertical displacement of 2 (resp., 1 ) and 0 is the road profile."
"using descriptive measures like the ones provided by nap and slc makes it less likely for applied researchers to make inferences, which would be statistically incorrect in absence of random sampling of the participant or of the behavior of interest [cit] . we consider that inferential statistical techniques are more susceptible to being misunderstood and to prompt researchers to make dichotomous decisions [cit] about intervention effectiveness or behavioral change. [cit] ."
"apache wave [cit], formerly known as google wave, is a platform for shared editing applications. all application components have to be implemented in java and eventually, the client component is compiled to javascript using the google web toolkit compiler. the synchronization mechanism is also ot-based and offers an operation composer to reduce the number of exchanged operations. even so apache wave offers a rich platform; a major drawback is the dependency to the google web toolkit development methodology that is not compatible with differing approaches (e.g. java servlet or php projects)."
"the solar photovoltaic module of the sprinkler irrigation machine converts solar radiation energy into electric energy through the photoelectric effect and stores the electric energy in its battery. part of the stored energy is converted into mechanical energy through the stepping motor. the output torque of the motor is transferred to the power that drives the wheels to move, steered by the planetary gear reducer with a deceleration ratio i of 10 and worm gear reducer with a transmission ratio of 80:1. the other part of the electric energy is converted into the pressure energy of the pump through the horizontal centrifugal pump, which is used to draw water from the water source. the stepper motors used in the sprinkler irrigation machine are equipped with driving controllers to control the speed of the driving wheels. considering the structure of the sprinkler, the efficiency of the driving system and the convenience of control, the wheel drive is chosen as the driving mode of the sprinkler irrigation machine. the driving mode of the wheel drive is shown in figure 2 . the self-developed sim is composed of a solar photovoltaic power supply part, mechanical structure part and control system part. it is illustrated in figure 1 that the self-developed sim mainly includes a solar photovoltaic module, battery, main water pipe, water pump, steering motor, high-clearance chassis, control cabinet, walking motor reducer, nozzle and so on."
"the main conclusion of this application of the analytical method is that visual analysis is necessary for focusing at different aspects of the data, such as an unstable baseline which is not well-represent by mean or trend lines, a somewhat delayed slope change, and a considerable amount of overlap only in the beginning of the second condition but not at the end. the variability and relative shortness of the first phase (although it meets the current standards of five measurements; have to be kept in mind when comparing it to the measurements obtained in the subsequent condition. in the current case, the visual aids reflected this variability and suggested a similar conclusion as the one based on substantive criterion expressed as a cut-off point. all this information is critical for interpreting correctly the numerical yielded by descriptive statistical procedures. actually, we preferred to use a data set that is challenging for the quantitative analyses in order to alert applied researchers on the need to interpret numerical values with caution and to use all information available; we also wanted to avoid doubts about the data being picked up only to show the quantification in a positive way [cit] also contribute to building solid conclusions. the two-phase design may not be sufficient for establishing a causal effect in a scientifically sound way, but there is enough information pointing at the clinically important reduction of problematic behavior."
"for a virtual skyhook inerter system, its dynamic equations in unload condition are in consistent with those of the passive suspension system in full-load condition, in the case that the inertance of the skyhook inerter is equal to the mass of full cargo or passengers. that implies the virtual skyhook inerter system in unload condition and the passive suspension system in full-load condition have the same performance in theory. therefore, the skyhook inerter has the potentiality of reducing sprung mass natural frequency and improving ride comfort of passive suspensions in unload condition. since the skyhook inerter is a theoretical assumption that cannot be passively implemented in actual vehicles, the continuously adjustable inerter device is proposed to address this issue by approximating the virtual skyhook inerter. furthermore, in this paper, the governing differential equations of the semiactive inertance control suspension are derived and the differential equation model is built by matlab/simulink. the frequency and time responses prove the superiority of the semiactive control suspension in reducing sprung mass natural frequency and improving ride comfort in unload condition."
"the assessment of overlap can be done using visual aids, such as range lines, as provided by the scda plug-in (bulté [cit] 2 ) for r-commander. the upper left panel of figure 2 [cit] for a participant called elizabeth. this graph suggests a minor overlap between the observations. regarding the assessment of changes in level, the same software can be used to superimpose, for instance, the median of the behavioral observations in the pre-intervention and postintervention conditions.the upper right panel of figure 2 shows an example with the same data and suggests that there has been a reduction in the level of target behavior. however, the median is not very useful for the post-intervention observations in which there is a clear downward trend."
"the architecture illustrates the basic building blocks which are available in all web editors (excluding editors built upon plug-in technologies such as adobe flash [cit] or microsoft silverlight [cit] ). figure 4 groups the editor-specific parts (editor, editor api) as well as the editor-agnostic parts (dom api, dom). in contrast to the naive approach linking the collaboration adapter to the specific editor api, we tackle the conversion of single-user editors into multi-user editors by attaching the gca directly to the dom api layer. exploiting this dom api layer is the key to overcoming the specified challenges. since all editor-specific operations are eventually translated into a set of dom operations, the gci has to exclusively support dom operations instead of numerous sets of editor-specific operations. for example, a graphics editor may provide a specific draw rectangle operation with the method signature drawrectangle(width, height) which translates into the standardized dom operations (1) create rect element document.createelementns( 'http://www.w3.org/2000/svg', 'rect'), (2) set the width attribute rect.setattribute('width', 20) and (3) set the height attribute rect.setattribute('height', 10). hence, the focus on dom operations limits the complexity of the conflict resolution scheme since the number of dom operations is fixed and does not depend on the number of supported editors."
"external data model: analyzing various web-based applications, we observed that editors targeting large documents (e.g. multi-page office documents) often separated the application model from the view model. this design decision is motivated by performance considerations. having a multi-page document comprising thousands of lines represented in a dom can easily consume more than 100 mb of ram. in contrast, creating a specific application model using a tailored javascript implementation can efficiently compress large size documents. then, the dom is used as a view model only reflecting an excerpt of the application model. in this case, the gci is not able to access the model through the standardized dom api and hence, the gci cannot be properly bound to the application model. consequently, the gci does not operate correctly. this limitation can be fixed with a moderate implementation effort connecting the gci directly to the specific application model api."
"reuse is one of the fundamental software engineering principles [cit], since it increases software robustness, furnishes encapsulation and lowers initial development costs as well as maintenance costs. while devising a reusable collaboration infrastructure for the web, we identified the two main challenges that had to be tackled:"
". baseline trend is thus the average increase (or, if negative, decrease) from one baseline measurement occasion to the next one. this estimation can inform about the characteristics of the data before an intervention is introduced. moreover, baseline trend is removed from the whole data series so that it does not affect the quantification of the effects of the intervention. technically, each data point is corrected according to its position in the series of observational sessions. this initial step allows for applying an intervention even when the theoretically undesirable linear improvement is present already during the assessment period. thus, slc would show whether there is an effect of the intervention beyond the initial improvement. after the correction it is assumed that the pre-intervention phase shows zero trend (i.e., stable data) and thus the trend present in the post-intervention phase actually represents an effect (i.e., a change in slope). this effect is estimated in the same manner as in the initial step, that is, as the average of the differenced (and already detrended) post-intervention measurements:"
"web 2.0 technologies paved the way for complex interactive applications like word processors, graphics editors and integrated development environments. although collaborative multi-user tools like google docs are well established and adopted by millions of web users, the majority of web applications only facilitates single-user support. adding shared editing capabilities to existing single-user applications requires complex concurrency control systems which are costly to implement, test and maintain."
"the techniques are relatively simple to compute and offer straightforward interpretations for practitioners who are not experts in statistics [cit], suggests). the calculation does not entail statistical decisions about the likelihood of obtaining such a large difference under the null hypothesis. this criterion also relates to the need for easily trainable procedures [cit] ."
"the basic idea of the covariance matching method is as follows. the actual remainder vector v k is verified while filtering in order to determine whether it is compatible; that is, v k is verified in order to determine whether abnormal filtering occurs. when the actual remainder is incompatible under the null hypothesis q(k − 1) and r(k − 1)-i.e., q k−1 and r k−1 -then q(k) and r(k)-i.e., q k and r k -are estimated to replace the original assumptions q k−1 and r k−1 [cit] ."
"it is indicated in table 2 that the average deviation, maximum deviation and deviation variance become smaller after filtering through the updated sage-husa adaptive kalman filter, which means the positioning accuracy of the system is improved. after using the updated sage-husa adaptive kalman filter, the maximum deviation between the sim and the predetermined path is 0.18 m, and the average deviation is 0.08 m; these deviations are within a reasonable range. this proves that the updated sage-husa adaptive kalman filter is applicable for the navigation of sprinkler irrigation machines."
"the evidence-based practices movement aims to provide guidelines for carrying out methodologically sound research in fields such as psychology [cit] and special education [cit] . according to this movement, the studies providing solid evidence need to meet a series of criteria related to how an experimental effect is documented and how generality can be established [cit] . the first of these aspects refers, among other features of the study, to its design and analysis. in the current work, we focus on two-phase designs that do not meet the criteria established by the what works clearinghouse standards, unless they are part of a within-study replication, as in a multiple-baseline design. twophase designs may be weaker, from the perspective of internal validity, but they are still used (e.g., [cit] and can be useful as pilot studies and also due to the fact that establishing the evidence basis of interventions is related to the replication of results and their integration via systematic reviews and meta-analyses [cit] . such reviews can offer a comprehensive summary of findings while trying to avoid publication bias, which would take place when excluding studies on the basis of the design. in that sense, it is potentially useful to report the results of all studies and, afterward, consider whether some studies show no differences or negative results [cit] or whether there are differences according to the design used or the methodological quality of the study. [cit] suggest that experimental control can be used as a moderator variable in meta-analyses."
"criterion 3: [cit] criticized non-overlap methods for omitting relevant data aspects such as level, trend, and stability or variability: [cit] suggestion for quantifying separately level and slope change. moreover, slc yields unstandardized results, which help assessing the practical importance of the behavior change when using meaningful measures [cit] such as the number of tantrums or the number of self-injurious behaviors. in contrast, nap is bounded, which allows comparisons and quantitative integrations. thus, nap and slc can be used jointly as they provide different information. specifically, nap is an ordinal measure [cit] that does not distinguish between conditions once complete overlap is achieved. in contrast, slc can be used even in absence of overlap to quantify how different the measurements belonging to different phases are."
"where and are the coordinate position of the sim at time k, and are the coordinate position of the sim at time k − 1, is the longitudinal speed of the sim, t is the sampling period, and is the heading angle of the sim at time k − 1."
"1 for example, assuming that the svg element (cf. figure 6 ) is configured as a sync root node, all modifications changing the circle or rect element would be distributed to all clients. furthermore, child node manipulations such as adding or removing nodes would also be included in the synchronization process. however, selecting a new tool in the editor's palette encapsulated in the div element would not trigger synchronization since the div node is not a sub-node of the svg node."
"in this paper, the quarter vehicle model is used to evaluate the vertical vibration performance of the semiactive suspension system employing the modified skyhookinertance control strategy. although the parasitic damping inherent in the hydraulic device have been considered, the model is not suitable for evaluation of the pitch and roll vibration performances of vehicles. in the further work, fullvehicle and more practical vehicle models will be taken into consideration, and experimental research will be carried out to further evaluate the overall performance of the proposed control strategy."
"in summary, none of the existing approaches offers \"out-ofthe-box\" collaboration support to the extent of the presented gci approach. in particular, our approach is unique with respect to the gci configuration strategy incurring minimal conversion effort. moreover, in contrast to domain-specific solutions (e.g. html form synchronization), the gci approach has a broad application domain supporting numerous single-user web applications."
"from figure 6a, it can be seen that when the observations have large deviations, the data obtained by the conventional kalman filter also have large deviations. it is seen in figure 6b that the data obtained by the updated sage-husa adaptive kalman filter also have deviations; however, the deviations are smaller than that of the conventional kalman filter. for a more intuitive comparison, the deviations between the predetermined path and the observation points, the path obtained by it is seen from figure 6 that the tracking path of the updated sage-husa adaptive kalman filter and conventional kalman filter are close. this is because, during most of the path tracking time, the filtering process of the updated sage-husa adaptive kalman filter is normal-that is, r k is adaptive to the current filtering-thus, r k does not need to be estimated. therefore, there are no great differences between the filtering results of the updated sage-husa adaptive kalman filter and conventional kalman filter."
"betrfs leverages the linux page cache to implement efficient small reads, avoid disk reads in general, and facilitate memory-mapped files. by default, when an application writes to a page that is currently in cache, linux marks the page as dirty and later writes the page out (i.e., write-back caching). in this way, several applicationlevel writes to a page can be absorbed in the cache, requiring only a single write to disk. in betrfs, however, small writes are so cheap that this optimization does not always make sense."
"to search for an element, follow the path from the root to the target leaf. to insert or delete an element, first search for the target leaf, update that leaf, and then perform the necessary modifications to keep the tree balanced. the search cost dominates the rebalancing cost since it takes many insertions, along with their searches, before a leaf node overflows. to find all the elements in a range [a, b], first search for a and then scan through the leaves containing elements less than or equal to b."
"no single rule governs betrfs disk usage because stale data may remain in nonleaf nodes after delete, rename, and overwrite operations. background cleaner threads attempt to flush pending data from five internal nodes per second. this creates fluctuation in betrfs disk usage, but overheads swiftly decline at rest."
"rather than do an in-kernel implementation of a write-optimized data structure from scratch, we ported tokudb into the linux kernel as the most expedient way to obtain a write-optimized data structure implementation. such data structure implementations can be complex, especially in tuning the i/o and asymptotic merging behavior."
"the enormous variety of data in the municipalities offers plenty of options/potentials in many different perspectives; for example, information and insights for integrated urban development, urban environmental protection and policy-making can be gained and the local economy strengthened, for instance, with new business models and innovative data-based ideas. often however, systematically executed overviews and studies are lacking details about the data available in municipal organizations. the usage of these data is mostly restricted to limited areas in local governments. this reinforces the partially existing silo thinking within different domains and between individual departments. in addition, german and european municipalities often lack the technical infrastructure that enables a horizontal connection between the various municipal actors and supports the integrated use of data, as well as concrete municipal business models for sustainable data exploitation. furthermore, the municipal data economy is also hampered by the fact that a regulation regarding the utilization of created, transmitted and utilized data is missing [cit], and hence the question of communal data sovereignty is not sufficiently and practically answered as to enable the utilization of large amounts of municipal urban data. german and european communities/municipalities should now take action and secure their participation in the data economy. on the way to a modern, sustainable and networked city or community, communities must be accompanied and supported. hence, a set of tools is required that will navigate cities and communities in achieving systematical introduction and adoption of digital technologies, in order to become a data driven smart urban environment. one such tool could be the concept of an urban data space that is elaborated and defined as a goal of this work in the following."
"3. if a station successfully transmits or receives a view, it checks whether there is already an element in its view set s which corresponds to that view (i.e. if it has seen that view before). if so, is assumes multiple stations share this view, and so it changes its view to match and then replaces s with the empty set. otherwise it inserts that view into the set. thus, seeing two stations with the same view results in a synchronization of views."
"however, the upsert does not perform these operations. rather, the message (k, ( f, )) is inserted into the tree like any other piece of data."
"btrfs, wafl, and zfs are feature-rich no-overwrite file systems whose policies leverage copy-on-write semantics to implement advanced features like versioning, selfconsistency, and checkpoints [cit] . the write anywhere file layout (wafl) uses files to store its metadata, giving it incredible flexibility in its block allocation and layout policies [cit] . one of wafl's main goals is to provide efficient copy-on-write snapshots. btrfs uses copy-on-write btrees throughout, providing both versioning and checkpoints. btrfs also implements back-references to help with defragmentation. zfs rethinks the division of labor within the storage stack and seeks to ease both the use and management of file systems. zfs ensures that all data on disk are self-consistent, provides pooled storage, and supports numerous features including deduplication. all three of these file systems provide advanced features beyond what traditional file systems offer, but none addresses the microwrite problem."
"behavior data is given by digital data of/from citizens, which emerges based on their behavioral patterns. such digital information is based on automatically generated/obtained samples based on the behavior of citizens involved with some sort of sensor equipment (e.g., heartbeat sensors, gps, smartphone-based sensors, etc.). data obtained in a machine-generated or automated manner remains property of the corresponding citizens, no matter if it is anonymized or personalized, or if the data has been further processed and new information has been produced through interconnections and data center computations."
"the \"data-driven transformation\" influences the economy and society in an increasing manner. this development constitutes the so-called \"transformation phase\" towards a global \"data economy\". in parallel, a continuously growing amount of data is being generated, thereby building on new technological trends such as the internet of things, factories of the future, artificial neural networks, big data analytics, autonomous networked systems or smart city reference architectures. digital data and information provide the basis for these new technologies."
"this paper presented the results of a recent study that was conducted with a number of german municipalities/cities. thereby, the need was identified to setup and create so-called urban data spaces within cities and municipalities in order to reveal the vast potential offered by urban data. building on the recommendations emerging from the study, the authors classify the various types of urban data and elaborate on the characteristics of the identified data classes thereby relating to legal and monetary aspects."
"today's applications exhibit widely varying i/o patterns, making performance tuning of a general-purpose file system a frustrating balancing act. some software, such as virus scans and backups, demand large, sequential scans of data. other software require many small writes (microwrites). examples include email delivery, creating lock files for an editing application, making small updates to a large file, or updating a file's atime. the underlying problem is that many standard data structures in the file-system designer's toolbox optimize for one case at the expense of another."
"tokudb includes its own cache of on-disk nodes, which are retained according to an lru policy. in future work, we will better integrate the cache of on-disk nodes with the linux page cache."
"from the sources (e.g., meta-data catalogs such as ckan) and provide the data to other services and applications in the urban and municipal context. the data is stored according to its validity (for example, temporary sensor data from the internet of things) or versioned and archived according to pre-specified rules. additionally, in this layer, the data is analyzed and correlated. moreover, the data can also be provided on the basis of further processing, for example on the basis of statistical algorithms or on the basis of processing in the sense of machine learning. the layer (4. integration, choreography and orchestration capabilities) contains various types of services that offer innovative use cases within a community through the interplay and use of different data and information from the underlying layers."
"finally, we found a small mismatch between the behavior of futexes and kernel wait queues that required code changes. essentially, recent implementations of pthread condition variables will not wake a sleeping thread due to an irrelevant interrupt, making it safe (though perhaps inadvisable) in user space not to double-check invariants after returning from pthread_cond_wait. the linux-internal equivalents, such as wait_event, can spuriously wake a thread in a way that is difficult to distinguish without rechecking the invariant. thus, we had to place all pthread_cond_wait calls in a loop."
"key-value stores. wods are widely used in key-value stores, including bigtable [cit], cassandra [cit], hbase [cit] b ], leveldb [google, inc. 2015], tokudb [tokutek, inc. 2013a], and tokumx [tokutek, inc. 2013b] . bigtable, cassandra, and leveldb use lsm-tree variants. tokudb and tokumx use fractal tree indexes. locs [cit] ] optimizes lsm-trees for a key-value store on a multichannel ssd. instead of using wods, fawn [cit] writes to a log and maintains an in-memory index for queries. silt [cit] further reduces the design's memory footprint during the merging phase."
"we implemented betrfs as an in-kernel file system because the fuse architecture contains several design decisions that can ruin the potential performance benefits of a write-optimized file system. fuse has well-known overheads from the additional context switches and data marshaling it performs when communicating with userspace file systems. however, fuse is particularly damaging to write-optimized file systems for completely different reasons."
"the routing matrix and the price matrix in networks modeled by numtw model should be rediscovered. to achieve this, in numtw model, let the matrices k, h"
"the performed analysis and interviews show that the city of bonn is planning and accelerating the topic of digitalization, in order to remain attractive for citizens and the industry in the future, and to increase efficiency in the administration. this can be seen, for instance, in the project \"digital administration\" as well as within the newly created central coordination office/position for digitalization topics-the \"chief digital officer (cdo)\". having already pioneered the open data context, the city of bonn is still interested in taking part in new developments at an early stage and helping to shape them. this becomes apparent, among other things, in the smart city test areas in the city."
"because our log writes all data at least twice in our current design, our throughput for large writes is limited to, at most, half of the raw disk bandwidth."
"to the best of our knowledge, this paper is the first attempt to investigate the num theory in the two-way flow scenario. the modeling and solution lead to some straightforward discoveries of a critical problem with network bandwidth allocation and performance optimization in the num framework to account for the impact of ack packet flow."
"note, the scf scheme assumes that all the stations have the same view, so they regard the idle slots as having the same order. in the situation where not all views are synchronized, we will see that scf may still help to speed reconvergence to a collision free schedule."
"the proposed concept within \"data for london: a city data strategy\" [cit] treats the term \"city data\" as a central element of an embodiment of the greater london as intelligent, urban, ict-based ecosystem. great emphasis is placed on the successful implementation of relevant open-data strategies 1 . the so-called \"data for london strategy\" plans to extend these approaches by additional types of data and corresponding data providers. this complementary data is expected to come not only from the public administration, but also from the private sector. data providers can be urban utilities, as well as various infrastructure operators, distributors, start-ups and many more. all these data sources/providers would require a unified view and a sophisticated data exchange infrastructure, which is one of the set goals of the emerging urban data space concept."
"the use of reference architectures for municipal ict infrastructures is already happening on a broad scale. many initiatives for smart cities/communities develop solutions based on specific reference models. especially at the european level, various projects and collaboration initiatives (such as espresso, triangulum or streetlife) have developed municipal solutions based on reference architectures. in addition, the european innovation partnership on smart cities and communities (eip scc) at the european level and din spec oup 91357 in germany constitute two important initiatives that define a so-called \"open urban platform\". we recommend the construction of an urban data space based on ict concepts in the sense of a reference architecture as elaborated in eip scc and din spec oup 91357."
"betrfs's stacked file-system design cleanly separates the complex task of implementing write-optimized data structures from other file system implementation tasks such as block allocation and free-space management. our kernel port of tokudb stores data on an underlying ext4 file system, but any file system should suffice."
"the objectives of an urban data space are: (1) the increased availability and utilization of urban data, (2) improved access to and better transmission of data within the municipal administration, municipal enterprises and other stakeholders, (3) the transparency when handling non-personal data, (4) technically sound concepts for data security/protection and improved data quality, (5) interoperability and standardization of urban databases and communication protocols, (6) the development of municipal and regional data analysis; (7) the promotion of data-based business models in urban areas by the state and municipalities and in promoting development opportunities for innovative business ideas of small and medium-sized enterprises in the municipal area, (8) building a flexible technical it infrastructure that integrates all available meta-data and data, and (9) the liability and security related to the utilization of innovative technology."
"we measured the throughput of sequentially reading and writing a 1gib file, 10 blocks at a time. we created the file using random data to avoid unfairly advantaging compression in betrfs. in this experiment, betrfs benefits from compressing keys, but not data. we note that with compression and moderately compressible data, betrfs can easily exceed disk bandwidth. the results are illustrated in figure 7 . most general-purpose file systems can read and write at disk bandwidth. in the case of a large sequential reads, betrfs can read data at roughly 85 mib/s. this read rate is commensurate with overall disk utilization, which we believe is a result of less aggressive read-ahead than the other file systems. we believe this can be addressed by retuning the tokudb block cache prefetching behavior."
"the interviews with actors in the municipalities involved in the studies give a picture of a fragmented technology landscape. this applies to the heterogeneous datasets in the inventory and their availability in the context of an urban data space. in particular, there is a lack of conception and systematic structuring of the existing ict landscape. furthermore, the potential difficulties in establishing urban data spaces are not approached in a systematic way, taking into account existing components and ict systems in the investigated municipalities. possible steps to deal with the current situation and to enable further development towards an urban data space are listed below. these steps will later be presented separately as recommendations for action and aim at the sustainability of the proposed concepts."
"all experimental results were collected on a dell optiplex 790 with a 4-core 3.40ghz intel core i7 cpu, 4gb ram, and a 250gb, 7200 rpm ata disk. each file system used a 4,096-byte block size. the system ran ubuntu 13.10, 64-bit, with linux kernel version 3.11.10. each experiment compared with several general purpose file systems, fig. 5 . total time to create 500,000 200-byte files, using 1, 2, and 4 threads. we measure the number of files created per second. higher is better. including btrfs, ext4, xfs, and zfs. error bars and ± ranges denote 95% confidence intervals."
"a possible marketing and sales strategy for data, which are not to be assigned to the context of open data and where a monetary value is expected, has not been yet developed extensively in the city of dortmund (such development is expected mainly from the municipal companies). individual fee models for certain types of data and information are available, but a holistic cash-flow model does not exist. concerning the data, which a city like dortmund (mainly its municipal companies) could provide against monetary payments, a continuous balancing of the expenditures for the supply and marketing with respect to the expected income is to be considered. under certain circumstances, other factors are to be taken into account for some data-these factors might influence a decision against a commercial marketing of the datasets (e.g., social aspects and overall benefits of critical value for the population and community as a whole). despite the above considerations, the involved representatives of dortmund believe to some extent that no uniform fee or cash flow model will be established across different municipalities and domains."
"in packet switching networks, a flow, also called a traffic flow, a packet flow or a network flow, is a sequence of packets traversing from a source to a destination [cit] . some unidirectional or one-way protocols, e.g. user datagram protocol (udp), only have one flow traversing in the system; while others, e.g. transmission control protocol (tcp), have two flows transmitting packets in opposite directions, either through the identical or distinct path. flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from outrunning a slow receiver. if the receiver sends feedback to the sender, flow control is a closed loop. a feedback closed loop system has a feedback mechanism that directly relates the input and output signals. for example, tcp adopts acknowledge (ack) packet to feed back the congestion information (packet loss or delay) at the destination to the source."
"the \"data economy\" in that sense comprises an ecosystem of different stakeholders and market participants, such as companies, infrastructure managers, public administration, research and civil society, whose cooperation ensures that data can be made accessible and usable. in this context, the market participants/stakeholders can extract and derive value from this data by implementing and operating a variety of ict applications/services, opening a tremendous potential for improving our everyday lives, including vital aspects such as traffic management, traffic flow optimization or remote e-health services [cit] ."
"with respect to reference architectures for ict in smart cities, the smart city charter of the german federal institute for research on building, urban affairs and spatial development (bbsr) [cit] explicitly states that the utilization of open interfaces and standards in the urban context will lead to a structured and flexible way of digitalizing a municipality. a reference architecture will promote the integrative cooperation of several vendors and support the sustainable extension/enhancement of the ict infrastructure through new software and hardware modules."
"when an upsert message (k, ( f, )) is flushed to a leaf, the value v associated with k in the leaf is replaced by f (v, ), and the upsert message is discarded. if the application queries k before the upsert message reaches a leaf, then the upsert message is applied to v before the query returns. if multiple upserts are pending for the same key, they are applied to the key in the order they were inserted into the tree."
"the metadata index is designed to support efficient file creations, deletions, lookups, and directory scans. the index sorts paths first by the number of slashes, then lexicographically. thus, items within the same directory are stored consecutively, as illustrated in figure 4 . with this ordering, scanning a directory, either recursively or not, can be implemented as a range query, and the layout of metadata entries on disk is equivalent to a level-order traversal of the file-system tree."
"existing municipal ict systems can serve as a basis for creating an urban data space. existing implementations and artefacts should be captured as components and integrated into the overall technical infrastructure of the urban data space. missing components should be systematically added to maximize data and information exchange within the urban data platform and provide as many innovative services and offers as possible. in this case, the used ict components are to be regarded as part of a security-relevant infrastructure and to be evaluated for productive operation in accordance with the bsi cyber-security requirements as well as to be examined for potential vulnerabilities."
"another input of key importance for the abstract specification of an urban data space, is constituted by the investigations which were undertaken by the project team within three selected german communities (emden, bonn and dortmund). thereby, structured interviews were conducted that led to a number of insights and a long list of recommendations, which were subsequently utilized as requirements for the emerging urban data space concept. the most important recommendations/requirements are given by the need to adopt ict reference architectures for smart cities based on open standards and interfaces, by the requirement to work out an inventory of urban ict systems in a municipality based on abovementioned ict reference architectures, and to define an extension strategy for the municipal ict and data delivery infrastructure based on reference architectural principles leading to the avoidance of vendor lock-in."
"the most useful feature currently missing from the tokudb codebase is range upserts; upserts can only be applied to a single key or broadcast to all keys. currently, file deletion must be implemented by creating a remove upsert for each data block in a file; the ability to create a single upsert applied to a limited range would be useful, and we leave this for future work. the primary difficulty in supporting such an abstraction is tuning how aggressively the upsert should be flushed down to the leaves versus applied to point queries on demand; we leave this issue for future work as well."
we organize our evaluation around the following questions: -are microwrites on betrfs more efficient than on other general-purpose file systems? -are large reads and writes on betrfs at least competitive with other general-purpose file systems? -how do other file system operations perform on betrfs? -what are the space (memory and disk) overheads of betrfs? -do applications realize better overall performance on betrfs?
"unlink and truncate can both remove blocks from a file. assuming the file inode is already in the vfs metadata cache to check permissions on the operation, betrfs already knows the length of the file and thus can compute the data index keys for the operation. betrfs removes the inode and data blocks in the simplest possible way: by issuing a tokudb delete for each block in the data index. tokudb delete operations are small messages that are as efficient as upserts; nonetheless, issuing o(n) deletes can make this an expensive task. the evaluation shows that file deletion time is currently linear in the file's size (figure 8) ."
"when betrfs allocates, opens, or deletes a block store on the underlying ext4 file system, the module essentially chroots into an ext4 file system disconnected from the main tree. because this is kernel code, we also wish to avoid permission checks based on the current process's credentials. thus, path operations include a \"context switch\" operation, where the current task's file system root and credentials are saved and restored."
"this is an interesting generalization of a very famous part of networking theory. despite the fact that most derivation and results presented here were seen as rather straightforward by the reviewers, they all agreed that the full duplex scenario is an interesting case. clearly more work is required for porting into this case many of the known results for half duplex channels."
"the following layers: 5. generic city/community capabilities and 6. city/community capabilities stand for the various urban processes, everyday activities and general innovations that are made possible on the basis of din oup's ict processes. examples include the potential for improving administrative processes and optimizing public transport routes for improved mobility within a municipality."
"the only metadata updates that are not inserts in betrfs are unlink, truncate, and rename. we now explain the obstacles to implementing these operations as inserts, which we leave for future work."
"it is noticeable that there seem to be no changes in system goodput for dcf when slot drift probability increases. we can explain this using bianchi's model [cit] . we know the system goodput depends on the collision probability p and transmission probabilities τ in the network. both with and without slot drift, the network can be modeled using"
"when choosing the model municipalities, various aspects were considered of particular importance: interested municipalities should have an advanced status with regard to the systematic management of urban data and the existence of established it departments. it was also important that the municipalities actively shape the interaction between administration, urban society, science and digitalization activities, and express an interest in actively supporting the study. furthermore, recommendations from the german organization of municipal enterprises (vku-verband kommunaler unternehmen) on interested municipalities were taken into account. finally, we paid attention at selecting cities and regions of different sizes and types (e.g., port city) as well as different characteristics of integration in the surrounding region, in order to achieve a broader view on various aspects of relevance."
"these experiments demonstrate that several real-world, off-the-shelf applications can benefit from executing on a write-optimized file system without application modifications. with modest changes to favor blind writes, applications can perform even better. for most applications not suited to write optimization, performance is not harmed or could be tuned."
"in institutional and personal terms, one can imagine a data space as a network of actors. from a technical point of view, the data space is a data infrastructure with technical standards, where data can be securely exchanged and linked between actors in the data space. in legal terms, the data space can be constructed as a separate entity with rules in a clear legal framework that should be entitled to \"data security\" as well as \"data sovereignty\" [cit] of its participants. functionally speaking, the data space can be understood as a demand-oriented system that can be actively shaped by its actors [cit] ."
"to implement multiversion concurrency control, each key-value record contains a key and one or more values, with each value tagged by a transaction identifier. one possible value is a tombstone, which indicates that the record is deleted. when the value produced by a transaction is no longer visible, the system may remove that value from the key-value record."
"the current section provided a classification of urban data which is paramount for setting the context and defining the urban data space. in general, a large variety of classification schemes are possible depending on the considered data characteristics. the classification provided here encompassed the official institutional data (i.e., public sector data) and moved over to describing the properties and the data sharing conditions of enterprise data, which is of key interest in an urban environment. further, relevant data is classified in terms of its origin as research, personal and behavioral data, while at the same time it can be seen as freely available, commercially available and internally available in regard to its access characteristics. thereby, in the scope of the personal data classification, special emphasis was set on recently established regulations with respect to data privacy and data protection (i.e., gdpr)."
"betrfs yielded substantially higher performance on several applications, primarily applications with microwrites or large streaming reads or writes. in the case of the imap benchmark, marking or moving messages is a small-file rename in a large directory-a case betrfs handled particularly well, cutting execution time in half compared to most other file systems. note that the imap test is a sync-heavy workload, issuing more than 26k fsync() calls over 334 seconds, each forcing a full betrfs log flush. rsync on betrfs realized significantly higher throughput because writing a large number of modestly sized files in lexicographic order is, on betrfs, aggregated into large, streaming disk writes. similarly, tar benefited from both improved reads of many files in lexicographic order as well as efficient aggregation of writes. tar on betrfs was only marginally better than btrfs, but the execution time was at least halved compared to ext4 and xfs."
"the details of one possible implementation of scf is shown in alg.1. in each cycle, each station notes all the idle slot indices and the number of collisions. if its transmission collides, the station notes the colliding slot index and the relative position of the colliding slot in all the colliding slots in the cycle. at the end of a cycle 1, the station selects the next reserved slot by partitioning the idle slots among groups of colliding stations. note that since stations do the selection at the end of a cycle, when a station just enters a wlan, it should monitor the channel for one cycle. fig. 3 shows an example of scf. there are 10 slots in a cycle, and in this cycle there are 3 colliding slots and 5 idle slots. ideally, we aim to evenly split the idle slot sets into 3 subsets. however, as 5 can't be divided by 3, we allocate the first 3 idle slots (the slots are ordered by time) to 3 subsets and leave 2 slots which will be selected in a probabilistic way. now, consider a station which collides in the second slot (the block labeled \"colliding slot\" and numbered with 2). at the end of the cycle, the station needs to select the next reserved slot. since the colliding slot position is 2, it selects the second idle slot. meanwhile, with probability of 2/3, the station needs to select one slot from idle slots 4 and 5 and the selection is random. then the station puts the selected slots and the colliding slot into the candidate slot set. finally, the station randomly select one slot as the next reserved slot from the candidate slot set."
"the openness of the architecture enables institutions such as the federal office for information security (bsi) and related certification bodies to assess the security of an urban data space and to demand corresponding changes that increase cyber-security. in addition, it is possible to use standardized \"test suites\" that verify and check compliance and security-functional security as well as defense capabilities examined by penetration tests-of the it systems to be deployed."
"next compare the flow rates between the following scenarios s2a and s2b. in scenario s2a, both transmission directions of a duplex link are used by different data packet flows. the characteristic of s2a is that the routing matrixes a11, a21 are not zero and a21, a22 are zero matrices. in scenario s2b, ack packet flows are considered together with their data packet flows, and the routing path of an ack packet flow is the same as that of the corresponding data packet flow. then it has the following result."
"in the above described context, the following five roles/bodies can be defined (based on recent eu level research activities [cit] ), which are described here based on high level considerations: (1) data committee-the data committee is a decision body with the key role to define and coordinate directives and decisions. if problems arise within an urban data space, the data committee is the body which is expected to work out solutions and track their implementation. (2) governance officer-the governance officer is part of the data committee and disseminates, promotes and monitors the policies and decisions within the organization. he acts as the central coordinator for a specific urban data space within the organization in question. (3) data owner-the data owner is essentially in charge of one or more datasets from a business perspective. the responsibility relates to various topics such as the framework and regulations for further data set usage or to the quality of the data. in addition, the data owner takes care of the legal requirements and is responsible for the aspects of licensing and commercial cost/price of the data. (4) data steward-the data steward bears the responsibility for implementing the requirements of the data owner, e.g., proper (meta-)data management. in general, the data steward represents the link to the technical users of a dataset. (5) technology steward-the technology steward manages the technology platform in place for all the data of a stakeholder. the technology steward has to guarantee that the selected technological stack is sufficient to fulfil the data quality requirements. in addition, the required technical support includes aspects of data backup, data security and the (meta-)data archiving."
"in designing betrfs, we set the goal of working within the existing linux vfs framework. an underlying challenge is that, at points, the supporting code assumes that reads are as expensive as writes and necessary for update-in-place. the use of write optimization violates these assumptions because subblock writes can be faster than a read. this section explains several strategies we used to improve betrfs performance while retaining linux's supporting infrastructure."
"tokudb includes transactional updates to keys, implemented using multiversion concurrency control (explained further in section 5.3). betrfs internally uses transactions to ensure crash consistency across metadata operations. although exposing a transactional api may be possible, betrfs currently uses transactions only as an internal consistency mechanism. betrfs generally uses a single transaction per system call, except for writing data, which uses a transaction per data block. in our current implementation, transactions on metadata execute while holding appropriate vfs-level mutex locks, thus making transaction conflicts and aborts vanishingly rare."
"rather than reinvent the wheel, we stick with this design in our kernel port of tokudb. betrfs represents tree nodes as blocks within one or more large files on the underlying file system, which in our prototype is unmodified ext4 with ordered data mode. we rely on ext4 to correctly issue barriers to the disk write cache, although disabling the disk's write cache did not significantly impact performance of our workloads. in other words, all betrfs file system updates, data or metadata, generally appear as data writes, and an fsync to the underlying ext4 file system ensures durability of a betrfs log write. although there is some duplicated work between the layers, we expect that ordered journaling mode minimizes this because a typical betrfs instance spans just 11 files from ext4's perspective. that said, these redundancies could be streamlined in future work."
"in case of a personal reference within a dataset, the further usage is generally limited due to privacy protection issues. the data protection regulations legitimize extensive processing only with the existence of a legitimate legal basis and compliance with the data protection principles required by the gdpr. special care should be taken of so-called \"special personal data\" referring to particularly sensitive data such as information on ethnic and cultural origin, political, religious and philosophical beliefs, health, sexual orientation and further."
"the no-overwrite mechanism is powerful and flexible. however, it is impossible to maintain physical locality in the presence of versions and updates since file blocks can be physically adjacent in exactly one file version. extra bookkeeping for back-references can help to aid defragmentation in exchange for added complexity [cit] ]."
"in the current chapter, we set the frame for understanding and specifying the notion of an urban data space. a number of key aspects are discussed starting with a classification of urban data, proceeding with introducing the idea of data sovereignty as well as with a first understanding of an urban data space. furthermore, the chapter handles the identification of key stakeholders in the scope of an uds and introduces the extremely important concept of an urban ict reference architecture, which is at the heart of the proposed approach."
"commercially available data-as pictured in figure 1 -can be generated by either private or public agencies/institutions. typically, this type of data is provided by companies that are interested in selling the data. accordingly, the \"london data strategy\" [cit] defines \"commercial data\" as being distributed under a license that allows re-use and processing strictly only in exchange for a monetary payment. companies that sell commercial data are (for example) map or navigation system manufacturers, energy companies, mobile service providers or even post companies. for example, mobile communication providers can sell analysis data of traffic and mobility streams based on anonymous wireless network signaling data. this data enables so-called \"geo-marketing insights\", i.e., insights into urban matters that previously could not be analyzed, or could be understood only with great effort. these findings can be of significant interest to different actors (industry, service providers, political parties etc.). for instance, such data can provide information about the number of road users traveling between two districts or cities. based on such information, the volume of traffic on a route in the ecological and social sense can be influenced. in particular, similar evaluations can significantly support strategic decisions and operational improvements in transport and other sectors."
"the solution to model numtw can be induced to solve maximization (7) by bandwidth charging. for each ps, a unique maximizer, xs(ps), exists. because us(xs) is increasing, strictly concave and continuously differentiable function of xs in its argument, u s exists and is decreasing. by the kuhn-tuker theorem [cit], it can be obtained that"
"as expected, gvs performs better with smaller slot drift probability. for example, considering fig. 4(a), we see that for 10% of the time the majority group is smaller than 14 stations for 2% slot drift, but smaller than 10 stations for 5% and smaller than just 6 stations if the slot drift is 10%. however, overall gvs does a reasonable job at keeping views consistent. we also see that gvs works marginally better with scf than zc. we attribute this to quicker convergence times. though we have not shown results here, it is also possible to apply gvs to l-mac and l-beb."
"internal (non-leaf) nodes contain three kinds data: child pointers, pivot keys, and operation messages. for internal nodes, the child pointers and pivot keys provide the same indexing data found in a b-tree. the tree employs b-tree rebalancing, with internal nodes containing up to 16 children. the messages contain data that are traveling from the root of the tree to the leaves. for example, a typical message would contain an \"insert\" command that specifies a transaction identifier, a key, and a value. when the message arrives at a leaf, it updates the appropriate key-value record."
"the betrfs prototype is an ongoing effort. the effort has reached sufficient maturity to demonstrate the power of write optimization in a kernel file system. however, there are several points for improvement in future work."
"internal data are generally those data which are available within the authorities, companies or reside in private ownership, and which for various reasons cannot or should not be made available to the public as raw data-see figure 1 for the position of such data within the data layers of an urban environment. mostly, it is data that is intended for internal organizational purposes. hence, publication and external use is not considered."
the eu has already specified a number of legislative measures to open up public databases across the european union as an important source of information for the data economy. [cit] /98/ec on the re-use of public sector information has created an eu-wide framework that facilitates the cross-border provisioning and utilization of publicly funded data and constitutes a viable asset for the development of pan-european data-based products.
"in zc [cit], each station notes all the idle slots in a cycle. in the case of a transmission collision, the station uniformly selects one slot for transmission in its next cycle from a candidate slot set which is composed of all the idle slots and the colliding slot. in the case of successful transmission in a certain slot, stations continue to use that slot in the subsequent cycles for future transmissions."
"in this section, we look at another way to improving the performance of a collision-free mac. we aim to speed its reconvergence to a collision-free state after being perturbed. in particular, we propose the smart collision free algorithm, which is an evolution of zc. recall that in zc, the candidate slot set for a station in the reservation process is composed of all the idle slots and the colliding slot in a cycle. in the case that all the stations have the same view, all the colliding stations share the same idle slot sets in a cycle for selection. if we divide the idle slots in a cycle into multiple subsets and let colliding stations with different colliding slots select from different idle slot subsets, the collision probability can be reduced and thus convergence speed can be improved. this is the basic idea of scf: to partition the idle slot set."
"in the usual num theory, the rule is that the transpose of a routing matrix is the price matrix of the connections. the routing matrix of the num model described by (11) is the same with that of numtw model. it is expected that b"
"gvs is our preventative scheme to enhance dcfa. we aim to correct slot drift on stations before their next transmissions, so the potential collisions can be avoided and the collision-free state can thus be maintained. the basic idea of gvs is that it provides a slot labeling benchmark to facilitate the correction of slot drift on stations."
"upsert (write, (path, n), offset, v, ), which means the application wrote bytes of data, v, at the given offset into block n of the file specified by path. upsert messages completely encapsulate a block modification, obviating the need for read-modify-write. writes of an entire block are implemented with an insert (also called a put), which is a blind replacement of the block."
"in these schemes each station has a decentralized reservation process to reserve an exclusive slot for transmission in a cycle. this process is used when the station just joins the network or when it loses its reservation. the reservation process stops and the system converges to a collision free state when all the active stations in the system have reserved an exclusive slot. we name such schemes decentralized collision free access (dcfa) schemes. obviously, for convergence to a collision-free state, the cycle length should be no smaller than the number of stations in the wlan system. dcfas are appealing since collision-free access can outperform 802.11's dcf. however, the collision-free state is vulnerable to perturbations caused by channel noise, new entrants and, as we will show, slot drift. in dcfas, transmission failures are used as a signal that reservation has failed. thus, channel errors or new stations introducing collisions may move the system from a collisionfree state. however, previous studies have shown low rates of new entrants and channel noise do not significantly degrade dcfa's performance [cit] ."
"although wrapping pthread abstractions in kernel abstractions was fairly straightforward, static initialization and direct access to pthread structures created problems. the primary issue is that converting pthread abstractions to kernel abstractions replaced members in the pthread structure definitions. static initialization would not properly initialize the modified structure. once the size of pthread structures changed, we had to eliminate any code that imported system pthread headers lest embedded instances of these structures calculate the wrong size."
"in reusing ext4 as a block store, we faced some challenges in creating module-level file handles and paths. file handles were more straightforward: we were able to create a module-level handle table and use the pread (cursor-less) api to ext4 for reads and writes. we did have to modify linux to export several vfs helper functions that accepted a struct file directly, rather than walking the process-level file descriptor table. we also modified ext4 to accept input for reads with the o_direct flag that were not from a user-level address."
"it can be observed that this dual model is not identified to that of numtw. firstly, there are 2s prices in this dual model, while s prices in the dual model of numtw. furthermore, the rate of each flow in this model is"
"flow control approaches consist of two components: a source algorithm that dynamically adjusts rate (or window size) in the response to congestion in its path, and a link algorithm that updates, implicitly or explicitly, a congestion measure and sends it back to sources that uses that link. these algorithms, well-known as the primal-dual algorithm, are all resulted from the num model. however, these algorithms unfortunately failed to address the impacts of ack packet flow. with an extension of the usual num model being proposed in the current paper, future work would be worthwhile to develop the new primal-dual algorithms for the two-way scenario by following the numtw model to take into account the ack packet flow. guided by the numtw model and theory, new congestion control approaches and protocols would possibly be developed for a general network with data packet flow and ack packet flow."
"while the details of the reservation phase of each of these protocols is different, when the system converges to collision-free state, the behaviors are the same. these protocols can be implemented with a backoff counter, and do not require stations to agree on slot labeling (or indexing). however, we can still think of then as each station trying to reserve an exclusive slot in a cycle."
"the basis for the creation of the urban data space, which can be understood as a \"precursor\" for \"smart cities/communities\", is a clear overview of the existing urban data, as well as an easy retrieval and integrated access to the existing urban data. the urban data space should offer a comprehensive range of municipal data as well as an overarching utilization roadmap for the data in the overall communal context."
"inserts and deletes. insertions are encoded as \"insert messages,\" addressed to a particular key and added to the buffer of the root node of the tree. when enough messages have been added to a node to fill the node's buffer, a batch of messages are flushed to one of the node's children. generally, the child with the most pending messages is selected. over the course of flushing, each message is ultimately delivered to the appropriate leaf node, and the new key and value are added to the leaf. when"
"in the past few years there was increased research effort relating to the concept of an ict reference architecture for \"smart cities and communities\". various ict reference architectures have been developed and tested in a number of european and national research projects. reference architectures and models are increasingly used in telecommunications and the internet domain, thereby enabling global networking and communication of data, video and voice. two of the most prominent reference models are given by iso/osi [cit] and tcp/ip [cit], which have unified telecommunication and internet communication architectures and provide a common understanding for sustainable development of global communication technology. the development of these two protocol families has facilitated the internet and the digitalization of our societies in the first place. in particular, iso/osi and tcp/ip protocols provide inter-device interoperability-e.g., ranging from switches, routers, media gateways, to end user terminals such as smartphones, tablets, and desktops-from various manufacturers."
"from the discussions so far, there is automatically a (6) location (in terms of specific city/municipality) advantage through an improved ict infrastructure as well as improved (7) interoperability and the possibility of using open interfaces. in addition, an urban data space based on din oup 91357 promotes the use of (8) standardized components based on reference architectures."
"experiments show that, compared to conventional file systems, betrfs can give one to two orders-of-magnitude improvement to the performance of file creation, random writes to large files, recursive directory traversals (such as occur in find, recursive greps, backups, and virus scans), and meta-data updates (such as updating file atime each time a file is read)."
"one subtle tradeoff in organizing on-disk placement is between rename and search performance. betrfs keys files by their path, which currently results in rename copying the file from one disk location to another. this can clearly be mitigated by adding a layer of indirection (i.e., an inode number); however, this is at odds with the goal of preserving data locality within a directory hierarchy. we plan to investigate techniques for more efficient directory manipulation that preserves locality. similarly, our current prototype does not support hard links."
"in this paper, the num problem in networks with twoway flows is investigated. firstly, the numtw model in networks with two-way flows is proposed, and the flow rates are obtained by its dual problem. the model takes into account the routing information of the ack packet flows. secondly the flow rates in the two-way flow scenario are studied and compared to those in the one-way flow scenario. it is found that if two-way flows are present, the flow rates may be equal to or less than those of one-way flows."
"the work described in this paper was supported by grants from the national natural science foundation of china (no. 61070197, no. 61370107) and grants from the self-determined research funds (ccnu13f009, ccnu13a05013)."
"the term \"urban data\" refers to all types of data that are important in the urban context, regardless of the specific data origin, data management, the associated intellectual property rights and licensing requirements. urban data may include data that extends beyond the direct local context, for example, when needed for a municipal process based on data of supra-regional or global relevance, or simply if it has general effects on the urban space/environment-for example, climate data or financial data."
"in the following chapter, another extremely important aspect is emphasized, namely the data governance procedure for an urban data space that supports in achieving data sovereignty for key stakeholders within the emerging uds concept. the data governance and data sovereignty is designed on top of the proposed uds and is described in one of many possible concrete varieties that have the potential to guarantee a reasonable operation with respect to the data providers' and stakeholders' interests in an urban environment."
"porting a 45kloc database into the kernel is a nontrivial task. we ported tokudb into the kernel by writing a shim layer, which we call klibc, that translates the tokudb external dependencies into kernel functions for locking, memory allocation, and file i/o. section 6 describes klibc and summarizes our experiences and lessons learned from the project."
"keys in the data index are also sorted lexicographically, which guarantees that the contents of a file are logically adjacent and, therefore, as explained in section 2.4, almost always physically adjacent on disk. this enables file contents to be read sequentially at near disk bandwidth."
our write-through caching design keeps the page cache coherent with disk. betrfs leverages the page cache for faster warm-cache reads but avoids unnecessary full-page write-backs.
"in this model, the ack flows are included, modeled to be isolated flows, and not related to their corresponding data flows. therefore, the object function is the the sum of 2s utility functions. moreover, the rate of a data flow is not related to that of the corresponding ack flow."
"behavior data is given by digital data of/from citizens, which emerges based on their behavioral patterns. such digital information is based on automatically generated/obtained samples based on the behavior of citizens involved with some sort of sensor equipment (e.g., heartbeat sensors, gps, smartphone-based sensors, etc.). data obtained in a machine-generated or automated manner remains property of the corresponding citizens, no matter if it is anonymized or personalized, or if the data has been further processed and new information has been produced through interconnections and data center computations."
"along the steps for the establishment of an urban data space, corresponding goals for the technical implementation can be derived. in the first place, a technical implementation of an urban data space based on a standardized and expandable ict reference architecture for smart cities/communities is to be developed. this should be based on open interfaces and open formats. it is also recommended that local authorities expand their available amount of open data and promote the utilization of open source components. this will create an ict ecosystem for urban data spaces that will allow cities and communities to avoid widespread problems such as vendor lock-in, i.e., dependency on large platform manufacturers, and to ensure competition and data protection."
"thus, many b-tree implementations use small nodes, resulting in suboptimal range query performance. especially as free space on disk becomes fragmented, b-tree nodes may become scattered on disk (sometimes called aging), thus requiring a range query to perform a seek for each node in the scan and resulting in poor bandwidth utilization. for example, with 4kb nodes on a disk with a 5ms seek time and 100mb/s bandwidth, searches and updates incur almost no bandwidth costs. range queries, however, must perform a seek every 4kb, resulting in a net bandwidth of 800kb/s, less than 1% of the disk's potential bandwidth."
"for dcfa, it is important that each station has an accurate view of where it is in the transmission cycle, and ideally all the stations in the system should follow the slot evolution process synchronously and accurately. if a station miscounts the slots we refer to this as slot drift, which may lead to collisions. in practice, we have identified a number of possible reasons for slot drift: 1) station sensing errors, where carrier sense leads to the misidentification of slots; 2) clock errors, where the number of slots during an idle period is miscounted; 3) hardware/software miscounting, caused by implementation details."
"the rest of this paper is organized as follows: section 2 analyzes the frame (including data classification) for urban data spaces and defines the proposed concept. section 3 presents briefly the results from the study relating to uds and conducted in multiple german cities and municipalities. section 4 proposes an abstract design for urban data spaces and analyzes the benefits for cities/municipalities, thereby clearly outlining the required steps towards a successful large-scale implementation. the following section 5 shows how the important aspects of data governance and data sovereignty would be addressed, whilst the final section 6 concludes this paper and presents potential future research and development directions."
"in the worst case, a search requires one i/o per level, for a total cost of o(log b n) i/os. an update requires a search and possibly splits or merges of nodes. the split and merge costs are almost always constant and o(log b n) in the worst case, for a total cost of o(log b n) i/os. a range query that returns k items takes o(log b n) i/os for the search and o(k/b) i/os to read all the items, for a total cost of o(log b n + k/b) i/os. b-tree implementations face a tradeoff between update and range query performance depending on the node size (b). larger nodes improve range query performance because more data can be read per seek (the denominator of the k/b term is larger). however, larger nodes make updates more expensive because a node must be completely rewritten each time a new item is added to the index."
"as a result, we were able to largely treat the tokudb code we used as a binary blob, creating a kernel module (.ko file) from the code. we exported interfaces used by the betrfs vfs layer to use c linkage and similarly declared interfaces that tokudb imported from klibc to use c linkage."
"to summarize our approach: in order to achieve the definition of the urban data space, we proceed with the following steps. first, an extensive literature review and urban data classification is provided which leads to setting the conceptual framework for the upcoming design approach. secondly, we present a summary of the analysis of the situation in three german cities. the cities are selected based on recommendations from key stakeholder organizations in the german municipal it and data landscape, as well as on the different size of the cities and level of their digitalization. the analysis was conducted based on structured interviews with key persons in the belonging municipality leading to the validation of a couple of hypotheses, as well as to the extraction of key factual statements and concrete recommendations/requirements. based on these recommendations and the conceptual framework, we systematically derive the concept of an urban data space thereby highlighting its benefits for cities and communities and defining a possible implementation of data governance and data sovereignty procedures."
"we consider two ways to enhance dcfa schemes: preventative and reactive. in preventative schemes, we aim to prevent the system from diverging from the collision-free state. to this end, we propose global view synchronization (gvs) which aims to correct slot drift as it happens. in reactive schemes, we try to alleviate the impact deviation from the collision-free state. in this regard, we propose the smart collision free (scf) scheme, a modification of zc which speeds reconvergence."
"from the data packet flow's perspective, the matrix g can be called the extended routing matrix. this matrix includes not only the routing information of all flows, but also the relation between the throughput rates between a connection's data packet flow and its ack packet flow. therefore, the entry of g is not just 0 and 1, but may be θs or 1 + θs. thus the constraint (5) in numtw model becomes"
"upserts have many applications in file systems. for example, upserts can be used to update a few bytes in a file block, to update a file's access time, or to increment a file's reference count. upserts can encode any modification that is asynchronous and depends only on the key, the old value, and some auxiliary data that can be stored with the upsert message."
"figure 9 presents performance measurements for a variety of metadata-intensive applications. we measured the time to rsync the linux 3.11.10 source code to a new directory on the same file system using the --in-place option to avoid temporary file creation (figure 9a ). we performed a benchmark using version 2.2.13 of the dovecot mail server to simulate imap client behavior under a mix of read requests, requests to mark messages as read, and requests to move a message to one of 10 other folders. the balance of requests was 50% reads, 50% flags or moves (figure 9b) . we exercised the git version control system using a git-clone of the local betrfs source tree and a git-diff between two milestone commits (figure 9c ). finally, we measured the time to tar and un-tar the linux 3.11.10 source (figure 9d )."
"because we believe the stored slots were not labeled using the global view, and so the labels must be updated. index-keeping does not relabel the slots, because we believe the slots were correctly labeled while synchronized to the global view, and the stations view has only recently drifted."
"our current prototype also includes some double caching of disk data. nearly all of our experiments measure cold-cache behavior, so this does not affect the fidelity of our results. profligate memory usage is nonetheless problematic. in the long run, we intend to better integrate these layers, as well as eliminate emulated file handles and paths."
"perhaps surprisingly, the impact of slot drift may be significant. for example, fig. 1 shows a simulation of dcf, zc and l-mac in the presence of slot drift. in these simulations we adopt a simple model of slot drift: we simulate a probability p of slot drift by introducing a probability of p/2 that each station leads the idealized slot count by one slot and a p/2 that it lags the idealized slot count by one slot. these probabilities are applied once in each idealized slot. we observe that zc and l-mac are outperformed by dcf in terms of system goodput in the presence of 2% slot drift. consequently, we consider how to enhance dcfa schemes to be resilient to slot drift."
"in order to review the current situation of german municipalities, the conducted study provides an inventory analysis of municipal data and legal framework conditions in selected german municipalities. the following questions are of central importance: (1) what characterizes the urban data? (2) which data is already available in the municipal databases of the examined model regions? (3) which urban actors are interested in data exchange? (4) how are the legal framework conditions? (5) how can an it/ict architecture be designed sustainably for urban areas? based on these questions and taking into account the german \"smart city charter\" [cit] and the \" [cit] agenda\" [cit], the current paper formulates recommendations and guidelines that should orientate municipalities on how to efficiently design a future-proof urban data space."
"favoring blind writes. a latent assumption in much file system code is that data must be written at disk-sector granularity. as a result, a small write must first bring the surrounding disk block into the cache, modify the block, and then write it back. this pattern is reflected in the linux page cache helper function __block_write_begin(). whenever possible, betrfs avoids this read-modify-write pattern, instead issuing blind writes or writes without reads."
"this policy can speed up small writes by reducing write amplification due to the coarse granularity of block-based i/o. under the conventional policy, the file system must always write back a full disk block, even if the application only writes a single byte to the page. betrfs achieves finer-than-block granularity for its writes by encoding small writes as upserts, which are written to the betrfs log, and group commit lets betrfs aggregate and pack many of its subblock writes into a single block i/o."
"the current legal situation (related to the legal rights of data use) is still unsatisfactory [cit] . for different types of data, different usage rights apply depending on the context and varying between the domains. for years, a national and international political discourse has been ongoing about these imperfections of data laws and the potential creation of new legal frameworks for data ownership [cit] . as indicated, it is still a matter of debate whether data can even be conceived as individual private property, i.e., whether data can be treated more or less like material or intellectual 3 in german: \"gesetz zur regelung des zugangs zu informationen des bundes (informationsfreiheitsgesetz-ifg)\". property. furthermore, within the previous paragraphs we outlined the pending loopholes regarding the classification of behaviorally generated data and its ownership assignment. further issues are related to data exclusivity rights, or to whether clarifying rights to data utilization over contracts is sufficient. it is also unclear in most cases how data access rights can be defined and how this affects the business models and the evolution of a data economy. as part of building the data economy, the complex \"data ownership, data sovereignty, data exploitation and data protection\" aspects reveal numerous clarification needs."
"b-trees are a widely used on-disk data structure in file systems and databases. a btree is a search tree with fat nodes. the node size is chosen to be a multiple of the underlying storage device's block size. typical b-tree node sizes range from 512 bytes up to a fraction of a megabyte. the standard textbook b-tree analysis assumes that all keys have unit size, so that each b-tree node can hold up to b pivot keys and child pointers. thus, the fanout of a b-tree is (b) and the depth is (log b n), as shown in figure 1 ."
"the analysis presented in the previous chapter shows that the data and system landscape in the municipalities is very fragmented, as well as is the associated existing knowledge. it must be emphasized that the technologies and technical concepts are already existing, both for overcoming the fragmentation of the data aspects as well as for the hardware and software integration or orchestration for the purpose of creating a common urban data space. the present legal framework also offers development opportunities for municipal business models."
"1. each station announces its view in each transmitted packet. a station's view can be represented by its current slot index, so here the announced view is actually a slot index. the announcement only uses several bits in each transmitted packet. 2. each station maintains a \"view set\", s. the elements in the set correspond to views successfully sent by this station and successfully received from other stations. since slot indices are always evolving, the stored elements indicate the difference between the current slot index of the station and the successfully sent/received slot indices:"
"however, municipalities have so far lacked a holistic concept for the permanent and sustainable construction of urban data spaces. this chapter presents a practicable technical approach and advocates for the application of a \"standardized, open reference architecture\" as a blueprint for the construction of urban data spaces in german municipalities."
"as examples of these: 1) the 802.11 standard [cit] requires only a 90% cca detection for 9us slots; 2) for slots involving sleep and wakeup, clock drift/synchronisation problems are well-known in sensor networks; 3) the openfwwf firmware for broadcom wifi cards highlights the challenges in correctly implementing slot counting in software."
"recent german data eco-system studies [cit] handle the \"behavior-generated personal data\" from the perspective of individual property. behaviorally generated personal data must therefore be clearly distinguished from the data protection law term \"personal data\". in other words: the main subject of data protection legislation is personal data, whilst the subject of data ownership legislation should be correspondingly given (in this context) by the behavior-generated data. existing legal loopholes on the subject of \"data ownership\" call for the introduction of a primary intellectual property right/law for behavior-generated information of citizens. german consumer organizations are therefore calling for a clear classification of data, in order to determine the scope of ownership, and correspondingly the scope of the exploitation rights [cit] ."
"the analysis of urban data and its belonging classification result in a structured presentation and introduction to the large variety of data within an urban environment. according to its origin, data is classified as official institutional data, enterprise data, research data, personal data and behavioral data. additionally, based on its access regulations, data is structured as freely available, commercially available and internally available. besides the classification of the smart city data, further critical concepts are analyzed and introduced, including the notion of data sovereignty/governance, the relevant stakeholders in an urban data space (resulting in a couple of well-defined roles), and the concept of an urban ict reference architecture, which is at the heart of the emerging uds."
"since tokudb stores data in compressed nodes that can have variable size, tokudb relies on an underlying file system to act as a block and free space manager for the disk. conventional file systems do a good job of storing blocks of large files adjacently on disk, especially when writes to the file are performed in large chunks."
"the situation analysis in emden shows that the city is pursuing a structured methodological approach to digitalization. in particular, based on an established digitalization roadmap, an important tool is put in place, which represents a \"manual\" for the execution of the local smart city project."
"the betrfs prototype demonstrates that write-optimized data structures are a powerful tool for file-system developers. in some cases, betrfs outperforms traditional designs by orders of magnitude, advancing the state of the art over previous results."
a microwrite is a write that is too small to utilize the bandwidth to the underlying storage device because of overheads such as seek time or i/o setup time. microwrites can waste storage bandwidth if the system doesn't take steps to aggregate them.
the metadata index. the betrfs prototype maintains an index mapping full pathnames (relative to the mount point) to file metadata (roughly equivalent to the contents of struct stat):
"we generally minimized changes to the tokudb code and selected imported code at object-file granularity. in a few cases, we added compile-time macros to eliminate code paths or functions that would not be used but required cumbersome dependencies. finally, when a particularly cumbersome user-level api, such as fork, is used in only a few places, we rewrote the code to use a more suitable kernel api. we call the resulting set of dependencies imported by tokudb klibc . table iv summarizes the abis exported by klibc. in many cases, kernel abis were exported directly, such as memcpy, or were straightforward wrappers for features such as synchronization and memory allocation. in a few cases, the changes were more complex. the use of errno in the tokudb code presented a particular challenge. linux passes error codes as negative return values, whereas libc simply returns negative 1 and places the error code in a per-thread variable errno. checks for a negative value and reads of errno in tokudb were so ubiquitous that changing the error-handling behavior was impractical. we ultimately added an errno field to the linux task struct; a production implementation would instead rework the error-passing code."
"the boundaries of an urban data space are not necessarily within a specific municipal space. an urban data space can also be extended to the dimensions of an economic area that is important for the municipality, as well as to the associated administration, living, but also legal, experience, action, identification, communication and socialization space. the urban data space includes all data generated by persons, companies and/or machines (personal and non-personal) as well as behavioral data (i.e., data generated by human behavior), be it internal, commercial or freely available, provided that it is closely related to the corresponding urban space."
"in order to optimally structure and sustainably use all urban data in the sense of a smart city, the technical structure of a data platform is required-a data platform can link together all the available urban data. a data platform that extends horizontally across all domains of a municipality and that can access all the required data is seen as a fundamental part of a smart city/community ict infrastructure. an appropriate data platform as a database is included in all ict reference architecture models for smart cities."
"thus, for given µ l of all links and the matrix [a the numtw model considers the relation between a connection's data packet flow and its ack flow, which is not the direct application of the usual num model in networks where ack packet flows are included. if an ack packet flow is bound to an utility function, the num model is"
"as an \"urban data space\" we refer to such a data space containing all kinds of data that may be relevant to the urban community as well as to the urban economic and policy space. ideally, based on the \"smart city/community\" concept, it encompasses all data relevant to the municipalities and their stakeholders from all domains (energy, mobility, health etc.) in urban environments that arise in both analog and digital contexts."
"update-in-place file systems [cit] ] optimize reads by keeping related items, such as entries in a directory or consecutive blocks in a file, near each other. these file systems can support file and directory scans at near disk bandwidth. on the other hand, since items are updated in place, microdata update performance is often limited by the random-write latency of the underlying disk."
"data spaces contain data and serve as enablers for digital services, for example, linked data semantic and web technology-based platforms and services [cit] . the term data space is applicable only to digital data. digital data refers to basic data (raw data), value-added data (processed data), meta-data (data describing basic data and value-added data) and derived information (often derived from data by means of combining various datasets towards logically obtaining facts or interpretations); all called data for short. the data space applies to this data, but also to technical data stores and various types of data processing. the data space can also have a spatial scope. for example, the term \"european data space\" clearly refers to the territory of the european union."
"plfs leverages application-specific behavior to provide both good microwrite and sequential scan performance by essentially implementing both logging and update-inplace [cit] . plfs is designed for hpc checkpointing workloads, which are split into separate microwrite-intensive phases and sequential-read-intensive phases. plfs logs data during the write phases, then sorts and rewrites the data so that it can serve reads quickly during the read phases. however, a general-purpose file system cannot rely on this type of sorting, both because updates and reads are interspersed and because general-purpose file systems cannot tolerate long down times for sorting data. thus, the plfs design illustrates the challenge for general-purpose file systems: how to maintain performance for both microwrites and large sequential i/os for general file system workloads."
"according to a definition of the alliance of german science organizations, research data is data \"that arises in the course of scientific projects, for example based on digitalization, desktop research, experiments, measurements, surveys or questionnaires\" [cit] . research data can include measurement data, laboratory results, audiovisual information, data from studies, samples and probes that originate from, are developed or evaluated in the course of scientific work, as well as methodological test procedures, such as questionnaires, software or simulations [cit] . in the scope of providing research data into a research data space, various german science organizations and the \"council for information infrastructures\" (rfii) are currently working on setting up a german-wide \"national research data infrastructure\" (nfdi)."
"our performance analysis assumes that accessing a block of size b requires one i/o, and we measure performance of an algorithm or data structure in terms of the number of i/os that the algorithm makes during its execution [cit] ."
the classification provided in this section is of paramount importance for understanding key dimensions of the proposed urban data space and understanding important aspects and design decisions in this context.
"the challenges in data acquisition and provisioning in an urban environment emerge on multiple levels. firstly, the data sources should be secured and instrumented/convinced as to make their data available to a wider range of stakeholders. this phase might include the interaction with municipal departments, municipal companies, local service providers and enterprises, but also the installation of additional/new sensors across the urban area in question, and the deployment of services for data handling and analysis. secondly, the quality of the data should be ensured as to enable a data driven smart urban environment. data quality includes mainly the correctness, the timeliness and the machine readability of the urban data. thereby, the data quality might differ in its criteria depending on the type of data as classified further in this section, e.g., sensor data should be delivered within milliseconds or seconds, as to be useful for near real-time applications, whilst municipal public administration data might be adequate if delivered once in a quarter. furthermore, the quality of the data delivery infrastructure 2 is of serious concern with respect to conformance to standards, interoperability between components, performance, scalability, stability of the hardware/software implementations, network bandwidth, cyber-security (in terms of confidentiality, integrity, and availability), data protection and data privacy given its intrinsic nature as a critical infrastructure. moreover, the utilization of open standardized interfaces is of paramount importance as to enable future business models and avoid vendor lock-in with respect to data provisioning and utilization. in addition, an urban data space should provide marketplace like means for data exchange including data trading as to stimulate the provisioning and utilization of data for future use cases and business models."
"currently, the question remains as to how so-called behavior-generated data, which is often commercial data, can ideally be provided as part of an urban data space. so far, the problem in the area of corporate data is solved-e.g., in the context of traffic planning such data is handled based on jointly concluded contracts between data companies and the city/municipality. currently, the question of data ownership for behavior-generated data of citizens is discussed in a german debate on the introduction of a new \"data ownership\" legislation [cit] ."
"by implementing betrfs as an in-kernel file system, we avoid the performance overheads of fuse, which can be particularly deleterious for a write-optimized file system. we also expose opportunities for optimizing our file system's interaction with the kernel's page cache."
"with respect to future work, we aim at continuing our standardization work at various relevant standardization bodies and relating to different domains and use cases. in addition, a reference implementation of standard open source components for urban data spaces is envisioned, which would allow for quickly setting up an urban data space and evaluating different scenarios and business models. finally, the idea of quality assurance for ict components within urban data spaces is of paramount importance and will be pursued on research level."
"to measure microwrites to files, we wrote a custom benchmark that performs 1,000 random 4-byte writes within a 1gib file, followed by an fsync(). these results demonstrate that betrfs improves microwrite performance by 1 to 2 orders of magnitude compared to current general-purpose file systems."
"the disadvantage of this representation is that a key must be stored for every value. keys are highly repetitive, and this overhead is negated with compression."
"in general, companies do not distribute primary data (for example, customers' activities within the mobile network). normally, commercially available data is published as secondary data. secondary data is generated after further processing steps from primary data. possible processing methods of primary data can be: aggregation, generalization, interpretation and classification. for example, (primary) data is often tailored to specific customer needs, and anonymized and filtered based on data protection legislation, so that no conclusions are drawn to individual persons and thus the privacy of the citizens can remain protected. data driven enterprises or authorities spend considerable resources (for example, for data analysts) and infrastructure (such as high-performance computing capacities), which have to be financially reflected in the final price for data."
"in the following, we will go into more detail about the din spec oup. the din specification (spec 91357) \"reference architecture model open urban platform\" is the version of the eip scc reference architecture adapted for germany. the rough structure of the din spec oup and according to the eip scc ict reference architecture is shown in figure 3 . since both activities operate on an international level, din spec oup has the potential to be considered within the framework of iso for international standardization."
"note that betrfs's approach is not always better than absorbing writes in the cache and writing back the entire block. for example, if an application performs hundreds of small writes to the same block, then it would be more efficient to mark the page dirty and wait until the application is done to write the final contents back to disk. a production version of betrfs should include heuristics to detect this case. we found that performance in our prototype was good enough without this optimization, though, so we have not yet implemented it."
"nonetheless, there are some cases where additional work is needed, such as further data-structure optimizations for large streaming i/o and efficient renames of directories. our results suggest that further integration and optimization work is likely to yield even better performance results."
"consequently, gvs deals with two issues: 1) maintaining a global view; 2) correcting drift on stations. the station performing gvs must share the common cycle length, c, with the network and maintain some slot view information which it includes in its transmissions. stations can then observe if their view is not aligned with other stations. the details of gvs are:"
"since not yet existing in emden, the introduction of commercial and crowd-sourced data could provide corresponding potential within the design of urban data and belonging platforms, and ultimately in the identification of other business models. the previously selective exchange of emden with the municipality of monheim or other municipalities, which already use commercial data, could be extended into a strategic cooperation, so that in the long term the experiences can be re-used and new ideas can be generated together."
"(1) the openness of the architecture-as well as the use of open standards, interfaces, formats and data models-encourages the municipality to reduce dependency on individual manufacturers and operators and thus reduce the risk of vendor lock-in. vendor lock-in is generally understood as the full dependency of a customer on a particular ict provider, manufacturer or infrastructure manager. a vendor lock-in arises when, for example, service providers build on proprietary (that is, on non-open and not freely available) interfaces and data formats and thus sell a closed solution as a complete package, for example to municipalities. such a complete solution can extend over several layers of the presented reference architectures. in such a case, the maintenance-i.e., the bug fixes, but also the updates (software and hardware updates)-is completely in the hands of the corresponding provider and cause permanent costs due to a dependency of the community on the provider with no chance of improving the situation. such a situation violates the \"open\" platform thinking of an urban data space and may make it difficult or even deny regional smes access to a community's ict ecosystem. this situation is also detrimental to a municipality's claim to sustainability. closed commercial platform solutions can also jeopardize the sovereignty of communities over their data and restrict or prevent free access to urban data. in the event of a vendor lock-in, some data may become the property of the relevant platform operator and may only be accessible at a corresponding cost. this would give municipalities limited opportunities to participate in the use and refinement of their own data."
"as explained in section 2.4, upserts and inserts are messages inserted into the root node, which percolate down the tree. by using upserts for file writes, a write-optimized file system can aggregate many small random writes into a single large write to disk. thus, the data can be committed to disk by performing a single seek and one large write, yielding an order-of-magnitude boost in performance for small random writes."
"log-structured file systems and their derivatives [cit] are write-optimized in the sense that they log data and are thus very fast at ingesting file system changes. however, they still rely on read-modify-write for file updates and suffer from fragmentation."
"for example, data.gov.uk and data.london.gov.uk. 2 the aspect of the quality of the data delivery infrastructure is beyond the scope of this paper and will be addressed in our future work. official data refers to all data available from/for public-law institutions performing administrative tasks. examples of such data can be found in official statistics, i.e., statistics compiled by an official institution (in particular a statistical office), or for example, through official surveying conducted by the responsible institutions. further examples for official data in urban environments are also given by data from public offices, cadastres or municipal utility data, such as water supply data and energy data, if organized under public law."
"for data governance in general, it should be clearly defined which roles are relevant for the provisioning and processing of data, and how these roles are to be embedded in the decision-making process. the decision-making processes address aspects like data quality management, data access management, general data management and lifecycle management [cit] . in addition, there is the key task of managing the meta-data for the datasets in an urban environment, which is also important in the context of data sovereignty and quality. considering the above statements, different approaches can be derived for realizing data governance in urban data spaces. in general, very often the so-called raci notation is utilized to implement governance structures. raci is an abbreviation for responsible, accountable, consulted, informed [cit] . correspondingly, when defining the roles for urban data governance in a particular context, the definitions should be worked out in terms of the four raci characteristics for the decision-making area in question. thereby, it is important to emphasize that these roles differ from those defined in the uds stakeholder analysis in section 2.4. the uds stakeholder roles (section 2.4) focus on systematically classifying the players in an uds, whilst the roles defined here outline the different tasks to be executed towards guaranteeing data governance and data sovereignty for the involved uds stakeholders."
"with a few exceptions, we were able to use tokudb in the kernel without major modifications. this subsection outlines the issues that required refactoring the code."
"block pointers are simple and flexible, with a single pointer for each file block. reading or writing a file requires following each pointer. fragmentation does not impact file representation in pointer-based indexing schemes."
the remainder of this paper is organized as follows: section 2 outlines the related works; global view synchronization and smart collision free are separately elaborated in section 3 and section 4; section 5 presents the simulation results and performance analysis; we conclude the paper in section 6.
"another reference framework that has gained relevance in recent years is togaf (the open group architecture framework) [cit], which is increasingly being used in the development of enterprise architectures. togaf has inspired some of the key activities/collaborations on urban ict reference architectures in recent years-for example din spec oup 91357 [cit] and eip scc [cit] ."
"the interviews with actors in the municipalities involved in the studies give a picture of a fragmented technology landscape. this applies to the heterogeneous datasets in the inventory and their availability in the context of an urban data space. in particular, there is a lack of conception and systematic structuring of the existing ict landscape. furthermore, the potential difficulties in establishing urban data spaces are not approached in a systematic way, taking into account existing components and ict systems in the investigated municipalities. possible steps to deal with the 4 bsi stands for bundesamt für sicherheit in der informationstechnik/federal office for information security. as a result, the aim is to classify the local technical inventory into a general architecture. our recommendation is to use ict reference architectures designed especially for smart cities/communities-such as eip scc [cit], din oup 91357 [cit] or the \"triangulum\" [cit] and \"espresso\" [cit] research projects [cit] . the use of such reference architectures enables the systematic development of a municipal data and system inventory in the direction of an advanced urban data space, taking into account and integrating the specific needs and requirements of a city/municipality in the construction of an urban data platform. in addition, openness-in terms of open interfaces, open data, data models, and open standards-as the basic principle of an ict architecture, enables the involvement of all relevant actors in the design and construction of the urban data space. the open concept of a reference architecture composed of many interchangeable modules enables a vibrant and dynamic ecosystem of coexistence across multiple products and companies. the open urban platform is free of so-called \"vendor lock-in effects\"-which means that its modularity and the interoperable interfaces between the modules greatly reduce and at best prevent dependency on individual manufacturers and operators. the concept enables the participation of a large number and variety of involved actors: the local it, small and medium-sized enterprises as well as start-ups, large-scale industry, the open-source economy, various initiatives as well as citizens. the concept of an open urban platform enables many initiatives and companies (including smes) to set up pilot projects in cooperation with municipalities or economically-linked municipalities, thereby promoting the sustainable development of an urban data space and smart urban scenarios on top."
"on a rotating disk, a microwrite is a write for which the seek time dominates the datatransfer time. for example, if a disk has a 5ms seek time and 100mb/s of bandwidth, then a write of 0.5mb utilizes 50% of the disk bandwidth; a write much smaller than this wastes most of the bandwidth. in ssds, seek time is not an issue, and an ssd can perform many random i/os per second compared to a rotating disk. nonetheless, ssds can still face performance bottlenecks when writes are too small. this is because ssds perform writes in pages; a microwrite much smaller than the ssd's page size incurs an additional overhead of rewriting data that is already on the device. small writes can also trigger garbage collection, which consumes internal bandwidth, slowing down all i/o to the ssd."
"transactions are also beneficial to the performance of some file system operations. in betrfs, the file system sync operation is reduced to the cost of a tokudb log flush. these benefits are large for sync-heavy workloads, as shown in section 7.6."
"in general, we expect applications that perform small, random, or unaligned writes to see substantial performance gains on betrfs. applications that issue frequent sync operations should also benefit from its ability to persist new data quickly. applications that are bandwidth-bound will continue to be bandwidth-bound, but may run more slowly due to betrfs logging. the only applications that we expect to perform poorly are those that issue extraneous queries. this behavior has little consequence in a system where each write is preceded by a read, but the read-write asymmetry of writeoptimization renders such behavior insensible."
"unless otherwise noted, benchmarks are cold-cache tests. all file systems benefit equally from hits in the page and directory caches; we are interested in measuring the efficiency of cache misses."
"there have been various interpretations of the term \"data sovereignty\" in use by political parties in germany. basically, the debate about this term deals with the right to handle data relating to a person in situations, which are not already covered by existing regulations. if one explains the term in the light of the various arguments in the social and political discussion, two interpretations best cover the general understanding of \"data sovereignty\": a) sovereignty in terms of data protection and b) sovereignty in the sense of a property-like view."
"finally, it should be noted that the exchange of data and information between the components of the urban data space should be determined by the use of standardized communication protocols and data models. this requirement is explicitly emphasized at european and german level and is the basic prerequisite for the implementation of an open, inclusive, extensible and structured urban data space."
"within the context of dortmund, based on the need to accomplish dynamic and more complex tasks in the context of smart city/community, the availability of real-time data in various areas-above all transport, energy, or security/safety-is of paramount importance and should be considered an aspired goal of the smart city considerations in dortmund."
"as part of the smart city activities of recent years, \"open data\" has been given high priority. various \"smart city/community\" initiatives [cit] have been advocating for urban development in the direction of urban digitalization and ict-based ecosystems for urban environments; both in germany and in the broader european context. this is expected to be realized with the help of administrative data and its provision or \"opening\" via corresponding open data portals. examples are the open data portals of berlin (daten.berlin.de), cologne (open data-koeln.de), the transparency portal hamburg (transparenz.hamburg.de/open-data/) and govdata.de as the data portal for germany, which was initiated by the federal ministry of the interior. since such data are usually already available in the municipalities, many of the \"smart city/community\" pilot projects carried out throughout europe in recent years have called for the completely free opening of public databases. many ict research projects initially had to limit themselves to open data as key data input, or to generate data by crowd-sourcing for the concrete project needs, since other data was not available for technical, licensing or business reasons. for example, eu projects in the area of sustainable mobility showed interest in the mobility data coming from navigation system manufacturers [cit] . this data could not be used in the projects in general, because it was not freely available for research activities."
"compression is important to performance, especially for keys. both indexes use full paths as keys, which can be long and repetitive, but tokudb's compression mitigates these overheads. using quicklz [cit], the sorted path names in our experiments compress by a factor of 20, making the disk-space overhead manageable."
"finally, table vii shows timings for a nanobenchmark that measures various system call times. because this nanobenchmark is warm-cache, it primarily exercises the vfs layer. betrfs is close to being the fastest file system on open, read, and stat. on chmod, mkdir, and unlink, betrfs is in the middle of the pack. our current implementation of the write system call appears to be slow in this benchmark because, as mentioned in section 5.1, writes in betrfs issue an upsert to the database, even if the page being written is in cache. this can be advantageous when a page is not written often, but that is not the case in this benchmark."
"in this paper, we investigated the potential problem of slot drift in decentralized collision free access systems and proposed two schemes, gvs and scf, to deal with slot drift problems. we have shown how gvs can help the system maintain a common slot indexing and use it as a benchmark for correcting slot drift on stations. scf speeds up the convergence process to the collision-free state and thus alleviates the impacts due to system's deviation from the collision-free state."
"this scheme results in all stations converging to the same view in two cycles, providing there is no slot drift or collisions in these cycles. to see this, note that if two stations share a view, then the first two to announce the same view results in the network being synchronized. if no two stations share a view, then the first station to transmit in the second cycle will synchronize the network."
"the terms urban data governance and sovereignty are strongly related to identifying the actors involved in the management, provisioning [cit] and utilization of urban data, as well as the required communication and interactions among these actors/stakeholders. on one hand, it is required to develop guidelines determining the responsible stakeholders for the belonging processes of data provisioning and management, while on the other hand the variety of existing regulations and rules, which define the way urban data is to be handled, should be considered. hence, the required processes for data handling should be implemented by the identified stakeholders (within an organization) and monitored through appropriate structures in order to ensure compliance to belonging regulations. such structures would also enable the data owners to keep and evolve their sovereignty over the provided data."
"-code complexity. a production-quality wods can easily run to some 50,000 lines of complex code, which is difficult to shoehorn into an os kernel. previous wodsbased file systems have been implemented in user space. -fuse squanders microwrite performance. fuse [cit] ] issues a query to the underlying file system before almost every update, superimposing intensive searches on top of small-write-intensive workloads. although wods are no worse for point queries than any other sensible data structure, small writes in a wods are much faster than reads, and injecting needless point queries can nullify the advantages of write optimization. -mapping file system abstractions onto a wods. we cannot fully realize the potential performance benefits of write-optimization by simply replacing the b-tree or other on-disk data structure in a file system with a wods. the mapping from vfslevel hooks onto wods-level operations, as well as the use of kernel infrastructure, must be adapted to the performance characteristics of a wods."
"the use of data compression also means that there isn't a one-to-one correspondence between reading a file system-level block and reading a block from disk. a leaf node is typically 4 mb, and compression can pack more than 64 file system blocks into a leaf. in our experience with large data reads and writes, data compression can yield a boost to file system throughput, up to 20% over disabling compression. we note that our experiments are careful to use random or nontrivial data patterns for a fair measurement."
"as mentioned, stations in the dcfa system do not require consensus on the slot labeling. once a station has reserved a slot, it will periodically send packets (if it has packet to send) and the period is the cycle length. we refer to the slot labeling as a station's view. for example, in fig. 2 there are three stations with different views of the mac slots. the cycle length is 4 and the highlighted square shows their reserved slots. station 1 and station 2 both reserved slot 2, but these two \"slot 2\"s are different due to their staggered labelings. now, we aim to make all the stations have the same view and regard this global view as a benchmark. if a station encounters slot drift, their slot labeling will change, and so can be corrected to match the global view. since slot drift happen from time to time, hence it is not possible to make all the stations always have the same view. consequently, the global view will be the stations in the view held by some large subset of stations."
"recent german data eco-system studies [cit] handle the \"behavior-generated personal data\" from the perspective of individual property. behaviorally generated personal data must therefore be clearly distinguished from the data protection law term \"personal data\". in other words: the main subject of data protection legislation is personal data, whilst the subject of data ownership legislation should be correspondingly given (in this context) by the behavior-generated data. existing legal loopholes on the subject of \"data ownership\" call for the introduction of a primary intellectual property right/law for behavior-generated information of citizens. german consumer organizations are therefore calling for a clear classification of data, in order to determine the scope of ownership, and correspondingly the scope of the exploitation rights [cit] ."
"the only situation where a page in the cache is dirtied is when the file is memorymapped for writing. the memory management hardware does not support fine-grained tracking of writes to memory-mapped files-the os knows only that something within in the page of memory has been modified. therefore, betrfs's mmap implementation uses the default read and write page mechanisms, which operate at page granularity."
"betrfs exhibited substantially higher cumulative throughput than the other file systems. the closest competitor was zfs at one thread; as more threads were added, the gap widened considerably. compared to ext4, xfs, and btrfs, betrfs throughput was an order of magnitude higher."
"in this section, we measure the impact of the betrfs design on large directory operations. table vi reports the time taken to run find, grep -r, mv, and rm -r on the linux 3.11.10 source tree, starting from a cold cache. the grep test recursively searches the file contents for the string \"cpu_to_be64\", and the find test searches for files named \"wait.c\". the rename test renames the entire kernel source tree, and the delete test does a recursive removal of the source."
"the first issue we encountered was that tokudb makes liberal use of stack allocation throughout. one function allocated a 12kb buffer on the stack! in contrast, stack sizes in the linux kernel are fixed at compile time and default to 8kb. in most cases, we were able to use compile-time warnings to identify large stack allocation and convert them to heap allocations and add free functions. in the case where these structures were performance-critical, such as a database cursor, we modified the tokudb code to use faster allocation methods, such as a kernel cache or per-cpu variable. similarly, we rewrote several recursive functions to use a loop. nonetheless, we found that deep stacks of more modest-sized frames were still possible and increased the stack size to 16 kb. we plan to rein in the maximum stack size in future work."
"which is not equivalent to that in numtw model. lastly, the price matrix of this dual model differs that of numtw. that is, the routing matrix's transpose in this model is its price matrix, while that in numtw is not. therefore, the price matrix in networks with two-way flows can not be obtain from the transpose of the routing matrix b."
"keying by full path also makes recursive directory traversals efficient but makes it nontrivial to implement efficient renames. for example, our current implementation renames files and directories by reinserting all the key-value pairs under their new keys and then deleting the old keys, effectively performing a deep copy of the file or directory being renamed. one simple solution is to add an inode-style layer of indirection. this approach is well-understood and can sacrifice some read locality as the tree ages. we believe that data structure-level optimizations can improve the performance of rename and still preserve file locality as the system ages. we leave these optimizations for future work."
"writing new data to betrfs introduced very little overhead, as seen in table viii . for deletes, however, betrfs issues an upsert for every file block, which had little impact on the betrfs footprint because stale data are lazily reclaimed. after flushing, there was less than 3gib of disk overhead, regardless of the amount of live data."
"the only benchmark significantly worse on betrfs was git-clone, which does an lstat on every new file before creating it-despite cloning into an empty directory. here, a slower, small read obstructs a faster write. for comparison, the rsync --inplace test case illustrates that, if an application eschews querying for the existence of files before creating them, betrfs can deliver substantial performance benefits."
"for large files with few updates, extents are very space efficient. a single range can represent the location information for an entire file, regardless of size. however, extents introduce complexity when files become fragmented, which can occur for various reasons: insufficient contiguous free space to fit an entire file, copy-on-write updates to a file, or file versioning. in these cases, many extents must be managed per file, and logic for merging and splitting ranges must be introduced."
"in order to develop the urban data space sustainably and incrementally, one must begin with the systematic review and inventory of the data space and the mapping of the locally existing technical structure as exemplified in figure 2 . in particular, it is required to gain a complete picture of the ict systems in operation, historical databases, as well as the currently available, generated and consumed data through local community services and applications. in addition, the associated interfaces and data formats of all systems must be described and classified accordingly. 4 bsi stands for bundesamt für sicherheit in der informationstechnik/federal office for information security."
"fuse can transform write-intensive workloads into read-intensive workloads because fuse issues queries to the user-space file system before (and, in fact, after) most file system updates. for example, fuse issues getattr calls (analogous to calling stat()) for the entire path of a file lookup, every time the file is looked up by an application. for most in-kernel file systems, subsequent lookups could be handled by the kernel's directory cache, but fuse conservatively assumes that the underlying file system can change asynchronously-which can be true, such as in network file systems. this design choice is benign when the underlying data structure of the file system is a b-tree. since searches and inserts have roughly the same cost, and the update must be issued at block granularity, adding a query before every update will not cause a significant slowdown and may be required for subblock metadata updates. in contrast, these searches can choke a write-optimized data structure, where insertions are 2 orders of magnitude faster than searches. the tokufs authors explicitly cite these searches as the cause of the disappointing performance of their fuse implementation [cit] ."
"because the 4kb block size is only a schema-level parameter, for small and sparse files, betrfs can avoid writing zero-padded bytes into tokudb and lower layers. betrfs implements sparse files by simply omitting the sparse blocks from the data index. betrfs uses variable-sized values to avoid zero-padding the last block of each file. this optimization avoids the cpu overhead of zeroing-out unused regions of a buffer and then compressing the zeros away before writing a node to disk. this optimization significantly reduces the overheads on small-file benchmarks. for instance, this optimization improves throughput on tokubench (section 7) by 50%-70%."
"a station may have noted some important slot indices which may correspond to idle slots, reserved slot, etc. these noted slot indices need to be updated when a station updates its view. however, there are two reasons why a station may be updating its view: 1) it has just entered the network and needs to synchronize its view to the global view; 2) it encounters a slot drift. we handle these cases differently, as follows. a station enters a wlan system with state set to initial and changes to state stable after the first view synchronization operation. corresponding to these, we have two types of slot index updating operations: index-shifting in the initial state and index-keeping in the stable state. from the initial state, index-shifting updates stored indices by the difference in the labeling,"
"in general, the setup of an urban data space is seen by the city of dortmund as a helpful overall construct, which can support the above goals and aspirations in the context of smart city/community."
"in this section, we present the numtw model of two-way flows in networks with duplex links. furthermore, the optimization problem will be solved by its dual [cit] and the source rate solution will be given."
"the goal and contribution of this paper is to define and specify on an abstract level the concept of an urban data space, in order to tackle the imminent challenges to municipalities with respect to the emerging data driven economies across germany, europe and beyond. thereby, we aim at systematically describing the various data types within a smart city/community and at providing an abstract architecture, which can be instantiated in the form of various technical components, interfaces, data formats and processes towards the efficient implementation of a smart city/community. it should be a main feature of the urban data space concept that it is designed in a way as to guarantee data governance and data sovereignty for the involved partners and stakeholders, with special focus on municipalities within this paper. furthermore, the concept of an urban data space should be able to accelerate the adoption and introduction of smart city technology across europe and beyond by providing an abstract blue print and facilitating the development, replication and exchangeability of components, services, applications and process according to open standards."
"crash consistency. we use the tokudb transaction mechanism for crash consistency. tokudb transactions are equivalent to full data journaling, with all betrfs data and metadata updates logged to a file in the underlying ext4 file system. each operation, including all inserted values, is placed in a logical redo log. log entries are retired in order, and no updates are applied to the tree on disk ahead of the tokudb logging mechanism."
"the din oup ict reference architecture is divided into eight layers and two columns. each of these layers/columns has a number of capabilities that are to be realized as part of the layer/column. detailed lists of the performance characteristics of each layer can be found in the corresponding din specification [cit] and the european documents on the eip scc reference architecture [cit] . the lowest layer (0. field equipment/device capabilities) contains most of the data sources within a community. in particular, various sensors and measuring stations are located there, which generate data for the upper layers of the reference architecture. this is the next layer based on this basic infrastructure, the data sources are networked with the data platforms in the third layer (3. data management & analytics capabilities). this layer includes data management systems, databases, open data portals, and cloud platforms that store or properly describe the data from the sources (e.g., meta-data catalogs such as ckan) and provide the data to other services and applications in the urban and municipal context. the data is stored according to its validity (for example, temporary sensor data from the internet of things) or versioned and archived according to pre-specified rules. additionally, in this layer, the data is analyzed and correlated. moreover, the data can also be provided on the basis of further processing, for example on the basis of statistical algorithms or on the basis of processing in the sense of machine learning."
"an aspect of paramount importance for urban data governance is constituted by the need for a sustainable and adequate organization for controlling data originating from urban environments, thereby paying special attention to the needs of the community, the public administration and the municipal companies. this implies that organizational setups, guidelines and processes must be correspondingly derived and combined as to interplay successfully. public institutions collect vast amounts of data, which is a valuable resource for the development of innovative digital services/applications, urban optimization and improved policy-making processes."
"we initially decided the porting was feasible because tokudb has very few library requirements and is written in a c-like form of c++. in other words, the c++ features used by tokudb are primarily implemented at compile time (e.g., name mangling and better type checking) and did not require runtime support for features like exceptions. our approach should apply to other wods, such as an lsm tree, inasmuch as the implementation follows a similar coding style."
recent advances in write-optimized data structures (wods) [cit] are exciting because they have the potential to implement both efficient microwrites and large scans. the key strength of the best wods are that they can ingest data up to 2 orders of magnitude faster than b-trees while matching or improving on the b-tree's point-and range-query performance [cit] .
"there is a possibility that two stations encounter slot drift and then announce their views consecutively, or that collisions or errors may result in delays in synchronization. we evaluate gvs's practical capability to maintain a global view and improving performance in the presence of slot drift in section 5."
"in principle, institutional (public) administration data in germany should be openly available. this is the situation according to the belonging law [cit] governing access to information of the federal government (freedom of information act-ifg) 3 . it also applies to other federal bodies and institutions as long as they carry out public-law administrative tasks. the belonging authority can and should (upon request) provide information, grant access to a requested file or provide information in any other way."
"alternatives to update-in-place. no-overwrite (copy-on-write) file systems perform in-memory modifications to existing file blocks before writing them persistently at new physical locations. thus, file data can be logically, but not physically, overwritten. logstructured file systems are one class of no-overwrite file system with a very specific data placement policy, but other no-overwrite data placement policies also exist."
"the contact persons in the municipalities received the questionnaires with the request to complete them independently, as far as possible. based on this, (semi-)structured interviews were conducted with various employees from local government and municipal companies. the selection of the persons to be interviewed was carried out by the municipalities."
"according to the european commission [cit], public and private (service) providers can benefit enormously from the emerging new data market. municipalities are also part of this ecosystem and also have the potential to contribute and benefit. for municipalities, the first steps would be to examine, understand and define their own specific urban data, to work out and implement the necessary processes for data provisioning and data management, to build a powerful data infrastructure to support and automate these processes, and to ensure their own municipal data sovereignty."
"the term and the need for \"freely available data\" is closely related to the \"open data\" and \"open government\" movement (as illustrated in figure 1 ), which build on \"freely available data\" as follows: (a) in general, significant impulses for the improvement of political, social and economic data promoting social cooperation are expected (keywords: participation, transparency and cooperation) [cit] . the open data/government movement continues to assume that freely available it should be noted again that the term \"freely available data\" is not synonymous with the term \"open data\". \"open data\" is when public data is freely available to the public without restrictions. [cit] . in such cases, we have to deal with \"closed data\" or \"shared data\"."
"in this section, we explain how we ported a large portion of the tokudb code into the kernel, challenges we faced in the process, lessons learned from the experience, and future work for the implementation. table iii summarizes the lines of code in betrfs, including the code interfacing the vfs layer to tokudb, the klibc code, and minor changes to the linux kernel code, explained later. the betrfs prototype uses linux version 3.11.10."
"kvfs [cit] ] is based on a transactional variation of an lsm-tree called a vt-tree. impressively, the performance of this transactional, fuse-based file system was comparable to the performance of the in-kernel ext4 file system, which does not support transactions. one performance highlight was on random writes, where they outperformed ext4 by a factor of 2. they also used stitching to perform well on sequential i/o in the presence of lsm-tree compaction."
"the authors extend the network utility maximization (num) model of f. kelly to encompass the case of two-way flows, ie, flows in which data packets and acknowledgements share a full duplex channel."
"the technical prerequisite for a functional urban data space is the coherent, coordinated and networked data and system landscape of all actors, departments and organizations of the involved municipality."
"an open reference architecture-as described for example in din spec oup 91357-is characterized by its integrative and modular character. it fulfils principles such as interoperability, reusability, openness and scalability. these design principles for it architectures in public administration have been identified and promoted by saga [cit] as a key \"egovernment standard\" [cit] by the federal government's information and communication office in the german federal administration (bundesverwaltung). these principles are also used in open reference architectures for smart cities/communities, as in din spec oup 91357 [cit] . the application of saga also ensures that the selection of technologies is based on transparent criteria and consistent quality requirements. in addition, we suggest that the ict components of an urban data space used in specific implementations of din spec oup 91357 should be audited and certified as required by bsi's [cit] security requirements. the compliance to the bsi 4 security requirements and the design principles of saga-in the context of din spec oup 91357-ensure the security, resilience and trustworthiness of the urban data space."
"it should be stated that the realization of urban data spaces using standardized reference architectures with open interfaces and formats-in particular din oup 91357-builds up a (2) support of local self-government and the sovereignty of a municipality over its data-this point can be seen as a significant advantage of the presented technical approach. based on this consideration, there are further obvious advantages for the municipalities: (3) local smes can implement specific municipal requirements-the openness of the urban data space makes it possible for local smes to be assigned specific tasks and developments at any time. (4) easy integration of digital participation forms and initiatives-this point arises from the openness and the systematic expandability of the urban data space. in particular, the use of open source solutions as well as the consideration of specific needs of the society is possible. (5) the integrative approach supports a consistent cyber-security concept-the openness of the user interfaces makes it easy for different actors to perform certain types of tests that assess and improve the security of the utilized components."
"microwrites are pervasive in file systems. metadata updates, such as file creations, permission changes, and timestamps are all examples of microwrites in file systems, and they can pose performance challenges. for example, maintaining atime is a wellknown performance problem in many file systems [cit] ]. unix and linux system administrators commonly report 10%-20% performance improvements by disabling the atime option, but this is not always possible because some applications require accurate atime values. similarly, applications such as email servers can create many small files or make small updates within large files, depending on how a user's mailbox is stored."
"both the rename and delete tests show the worst-case behavior of betrfs. because betrfs does not include a layer of indirection from pathname to data, renaming requires copying all data and metadata to new points in the tree. although there are known solutions to this problem, such as by adding a layer of indirection, we plan to investigate techniques that can preserve the appealing lexicographic locality without sacrificing rename and delete performance."
"the emergence of the term \"data space\" is recent and related to the emergence of the concept of the \"european data economy\". [cit], the european commission presented the follow-up strategy paper on the european data economy-\"building a european data economy\" [cit] . it places the notion of a data space in the context of a data economy and emphasizes the fact that the digital transformation is not limited to a social scope (such as focusing on economic aspects, for example), but that it encompasses all areas of life. in its communication note, the eu defines the \"european data space\" as \"a seamless digital territory on a scale that enables the development of new data-based products and services\" [cit] ."
"one way to mitigate the microwrite problem is to aggregate or batch microwrites. logging, for example, batches many microwrites, turning them into large writes. the challenge with pure logging is preserving read locality: simple logging can convert a sequential read of a large file into a series of much more expensive microreads scattered throughout storage-trading one problem for another."
"regarding the flow control model, most expositions simplify the impact of ack packet flow by ignoring the ack packet size. the consequence is that all transmission protocols can be approximated by one-way protocols. this approximation is reasonable and may not affect the protocols' performance when the ack packet size is small enough not to affect the data transmission. for example, in client/server (c/s) services with symmetric links, ack packets are transmitted in the direction from the client to the server, and data packets are in the direction from the server to the client. in this case, ack packets and data packets are transmitted in opposite directions. the volume of ack packet flow, compared to the uplink capacity, is very small, thus it does not affect the data flow significantly."
"where, k is the retry limit and b k is the average number of backoff slots chosen at the k th attempt. to find p and τ we search for solutions of eq.1 and eq.2. we note that slot drift only has a direct impact on the b k . since we have modeled slot drift as a mean-zero random walk which is added to the backoff process, we expect that the b k will be unchanged, and so dcf's performance will be unchanged by this sort of slot drift. fairness is another concern in wlans. when slot drift is applied to all stations in a system, the system remains homogeneous and fairness is not an issue. however, slot drift probabilities on different stations may vary. in this section we consider similar networks to those in section 5.3 but here slot drift is applied to only one station. fig. 7(a) and (b) shows jain's index [cit], and reveals that system fairness can be improved with global view synchronization. our results indicate the station with drift can usually correct it before its transmission and thus avoid collisions."
"these benefits are important pillars for the digitalization and development of local communities, and for the exploitation of the data resources that can emerge in an urban environment and can contribute to improving the quality of life and work of citizens."
"according to our understanding, we define an ict reference architecture as follows: an urban ict reference architecture sets itself the goal (1) of describing an abstract structure of the ict infrastructure and related interactions, especially between the utilized ict components. based on this abstract structure, (2) by applying a reference model, the (4) classification and interaction of different ict components is supported via open, standardized interfaces/apis within the reference architecture, such that the (5) implementation of \"smart city/community\" scenarios based on integrative solutions is ultimately enabled. in addition, the continuous (6) extension of the municipal ict infrastructure is ensured by the addition of further components according to the rules of the applied reference architecture. in particular, by using a reference architecture, it is possible to (7) replicate ict-based smart city/community solutions between municipalities, and (8) further exploit the combination of components from existing smart city solutions and adopt in an integrative pattern into new innovative urban services and applications. it is particularly important to note that an ict reference architecture does not pursue a disruptive approach, but an evolutionary one that (9) takes into account the existing ict systems and maps/positions them within the framework of the reference model."
"range queries incur a logarithmic search cost for the first key, as well as a cost that is proportional to the size of the range and how many disk blocks the range is distributed across. the scan cost is roughly the number of keys read (k) divided by the block size (b). the total cost of a range query is o(k/b + log b n) i/os."
"we evaluated microwrite performance using both meta-data and data-intensive microbenchmarks. to exercise file creation, we used two configurations of the tokubench benchmark [cit] . in the first configuration, shown in figure 5, we created 500,000 200-byte files in a balanced directory tree with a fanout of 128. tokubench also measures the scalability of the file system as threads are added; in this experiment, we measured up to four threads since our machine has four cores."
"block indexing. as discussed in section 5.5, betrfs does not store location information in its inodes. instead, it relies on queries to the wods to transparently retrieve its data. alternative techniques for indexing file data include extents and direct pointers to blocks."
"the main contribution of the current paper is to generalize the usual num model from one-way flow scenario to two-way flow scenario to cover the ack packet flow. we propose a model called the num model for networks with two-way flows (numtw), which is different from the usual num model that only includes the ack packet flows. to model a network with two-way flows, the routing matrix covering the routing information of ack packet flow is introduced firstly and the relation between the sizes of data packet and ack packet is identified next. the numtw model is therefore defined and a solution of the source rate to the model is given by its dual problem. the difference of source rates between one-way flow networks and two-way flow networks are discussed."
"in the case of personal data, physical persons are entitled to \"informational self-determination\". for companies, authorities or third parties in general, the storage and processing of personal data is therefore only permitted with the consent of the data subject, i.e., the person the data refers to. in addition, physical persons have the right to inspect the data stored about them within companies/authorities and to initiate its deletion if necessary. companies and authorities that want to process personal data, or even use it commercially, must pay special attention to belonging data protection issues and regulations."
"in a wlan, multiple stations typically share a common physical channel. the medium access control (mac) plays an important role in arbitrating accesses to the shared medium and thus influences channel utilization and system throughput. csma/ca (carrier sense multiple access/collision avoidance) and tdma (time division multiple access) are two popular mac mechanisms. in csma/ca, a station that has a packet to send has to keep sensing the wireless channel. if the channel is sensed idle, the station is permitted to start the transmission. otherwise, it needs to defer the transmission, often using random backoff, and tries to transmit at some later time. in tdma, time is divided into fixed or variable slots and different stations transmit in different slots. these two mechanisms have inherent drawbacks. in the former, collisions are inevitable, and in the latter, there are signaling overheads and tight synchronization requirements."
"the open data concept continues to play an important role in the context of the establishing urban ecosystems and platforms as well as in all major international and national strategies, roadmaps, collaborations and standardization relating to this topic. further basic definitions of open data are given by the preliminary study on the govdata.de portal and by the sunlight foundation [cit] . the open data activities in germany are promoted by studies such as the current report [cit] of the \"technology foundation berlin\" on the status quo of open data in the berlin administration. among other things, this study notes that there is often a lack of clear responsibilities for the subject of data publication and provisioning. in this sense, it is recommended to set up a special body to coordinate the data publishing/provisioning activities of the municipality. this body would need to have the necessary authority to view the databases and prepare them for reuse within the individual offices and units; such processes are of high importance for systematic data publishing and provisioning."
"regarding open data, the psi [cit] /98/ec) directive clearly states that data originating from public institutions should be freely published and made available to the society as open data. thereby, open data platforms support this requirement by offering the means for publishing the datasets and handling the belonging meta-data. different roles are specified within each data portal/platform with data user, data provider and operator being some of the common roles within today's open data portals. beyond governmental institutions, municipal companies generate interesting types and amounts of data that can enable useful urban applications and services. however, municipal companies are mostly not solely publicly owned but also hold private shares and aim at achieving business goals which might also be based on selling data. hence, in these cases only particular datasets would be made publicly available in different forms, while others would be offered in exchange for appropriate assets (e.g., money or services). therefore, it is of paramount importance to emphasize the importance of standards when publishing datasets as open data. for instance, some initiatives [cit] require datasets to have the following properties, which are also listed as key principles within the open data community: complete, primary, timely, accessible, machine processable, non-discriminatory, non-proprietary, and license-free-such characteristics can be to some extent achieved by using proper technical standards such as linked data, rdf, csv, dcat and xml. furthermore, the data requires many additional interactions and discussions with those responsible for the datasets within the publishing institutions."
"after establishing a definition of an urban data space, the concept is analyzed in detail and a proposition for setting up an urban data space is worked out. the authors propose to setup urban data spaces based on emerging standards from the area of ict reference architectures for smart cities, such as din spec 91357 \"open urban platform\" and eip scc. thereby, the paper presents the transformation steps required from municipal perspective (especially in the german context) to successfully implement an urban data space. furthermore, the paper elaborates on vital aspects such as data governance and data sovereignty and shows how these would be realized within the proposed approach. this results in the proposition of a set of processes, roles and committees that aim at enabling the data providers in an urban data space to determine, steer and understand data publishing implications, regulations and standards."
"a major obstacle for the exchange of enterprise data is given by the risk that the provided data might contain corporate secrets. relevant data (in the domain of data-driven companies) could potentially include source code, algorithms or entire repositories, theoretical models, system architectures or other modelling artifacts, e.g., uml diagrams, use cases and other functional descriptions. as long as industrial property rights do not address data aspects and technical infrastructure, it is up to the company to determine to what extent certain internal enterprise data artifacts should remain protected [cit] ."
"the frame for the urban data space provided in this chapter constitutes a solid prerequisite for following the analysis of the situation on selected german cities as well as for defining the urban data space and its data governance and sovereignty aspects in the coming chapters. thereby, the needs identified in this and the coming chapters are specifically addressed during the design of the abstract blue print architecture of an urban data space with its recommendations and characteristics."
"these benefits of using din spec oup 91357 will be discussed in more detail below. the necessary steps for the establishment of an urban data space are discussed according to the method outlined above. next, the objectives of the technical approach are formulated and a motivation is provided for the use of established ict reference architectures for smart cities. based on this discussion, the benefits for municipalities are derived and specific technical artefacts required for the implementation of an urban data spaces are described."
"the second configuration of the tokubench benchmark is shown in figure 6 . this setup demonstrates the sustained rate of file creation. we again create 200-byte files in a balanced directory tree with fanout of 128, but for this experiment, we create 3 million files. each data point represents the cumulative throughput at the time the i th file was created. four threads were used for all file systems. in the case of zfs, the file system crashed before completing the benchmark, so we reran the experiment five times and used data from the longest running iteration. betrfs is initially among the fastest file systems and continues to perform well for the duration of the experiment. the steadystate performance of betrfs is an order of magnitude faster than the other file systems."
"as enterprise data, we refer to all data arising within a company. enterprise data can be obtained within a company itself or from external sources, such as market and customer data, consumer behavior or business relationships. for example, the data from the purchase of raw materials, consumables and supplies can be commercially available for production plants and with respect to final products. even companies can provide data as open data, as exemplified by the open data portal of the berlin energy provider (see: netzdaten-berlin.de)."
"the tablefs authors identified another source of fuse overhead: double caching of inode information in the kernel [cit] . this reduces the cache's effective hit rate. for slow file systems, the overhead of a few extra cache misses may not be significant. for a write-optimized data structure working on a write-intensive workload, the overhead of the cache misses can be substantial."
"the urban data space refers to the entire set of data that has relevance in the urban context (economic, urban, geographical, technical, climatic, health, etc.) and is needed, generated or collected within municipal processes. the \"smart city/community\" concept intends to open up this data such that the municipality or the municipal companies can facilitate and accelerate the corresponding provisioning processes. the ict-based services and applications of the municipality should also utilize this data. data can be provided directly as a good or used as a basis for innovative services."
"the data index. although keys and values may be variable-sized, the betrfs prototype breaks files into blocks of up to 4,096-bytes in size for better integration with the page cache. thus, the data index maps (file, offset) tuples to blocks:"
"kelly's network utility maximization (num) theory [cit] amazingly casts the issue of flow control and bandwidth allocation into a unified optimization framework, which proposes distributed computations running at sources and at * corresponding to liansheng tan (email: l.tan@mail.ccnu.edu.cn)."
the two pillars on the left side of the reference model are responsible for privacy and security (8. privacy and security capabilities) and overall system management (9. common services capabilities) for the emerging comprehensive integrative ict solutions within an urban data space. they cover several layers and ensure it security and the proper operation of the urban data space.
"the european initiative \"the marketplace of the european innovation partnership on smart cities and communities\" (eip scc) [cit] summarizes the concept of a reference architecture as a tool to support the smart cities and communities, an abstract it-technical perspective for the realization of an urban ict infrastructure. on the other hand, the abstract approach of the reference architecture as a blueprint allows for the consideration of specific needs of the community by strengthening the resulting real technical architecture of the community/municipality/city-as a result of the reference architecture through standards, open interfaces and interoperability aspects. moreover, it is a basic assumption that an ict reference architecture integrates or connects existing ict solutions in the urban data spaces. existing systems should remain in place, but at the same time fit into the new structure. this requires improving their interoperability or the interoperability of the entire existing technical architecture within the municipalities. the main goal of a generic reference architecture is to enhance these real technical urban it architectures and enable their sustainable extensibility and scalability, while at the same time reducing dependence on individual vendor/operators (vendor lock-in effects). finally, a reference architecture provides a common terminology that applies to all urban spaces and enables technical discussions between different actors."
"other benefits derived from the envisaged urban data space are already formulated indirectly-notably through the avoidance of vendor lock-in, the involvement of local smes, the use of open standards, and improved interoperability and security. all these aspects help a municipality in (9) promoting its general sustainability and (10) preserving its possibilities to act in an independent way when it comes to data related topics."
"in recent years, new schemes that overcome the drawbacks and retain the good aspects of both tdma and csma/ca have been proposed. examples include l-beb [cit], zc [cit] and l-mac [cit] . in these schemes, time is divided into mac slots and a fixed number of consecutive slots are grouped into a cycle. there are three types of mac slots: idle slots, busy slots with successful transmissions, and busy slots with collisions. note that these schemes allow stations to have slots of variable duration determined, say, by carrier sense."
"then numtw is, from the prospective of the formula form, the same with the usual num model in networks with oneway flows. according to the usual num theory, the transpose of g, i.e. g t, is the price matrix. notice that g t may not be the transpose of matrices b, k or h, it can be called the extended price matrix 3 . similarly, the entry of the extended price matrix may not be 0 or 1. the price of the data packet sources in numtw model, from (8), is"
"the term and the need for \"freely available data\" is closely related to the \"open data\" and \"open government\" movement (as illustrated in figure 1 ), which build on \"freely available data\" as follows: (a) in general, significant impulses for the improvement of political, social and economic data promoting social cooperation are expected (keywords: participation, transparency and cooperation) [cit] . the open data/government movement continues to assume that freely available data (b) contributes to better forms of governance (i.e., better governance in general) and (c) provides various added values for policy, administration and citizens at the procedural level, for example by promoting \"open innovation\". freely available databases have great innovative potential for business, administration and society, as well as for social innovation and economic development [cit] . therefore only permitted with the consent of the data subject, i.e., the person the data refers to. in addition, physical persons have the right to inspect the data stored about them within companies/authorities and to initiate its deletion if necessary. companies and authorities that want to process personal data, or even use it commercially, must pay special attention to belonging data protection issues and regulations."
"a leaf node becomes too full, it splits, just as in a b-tree. similar to a b-tree, when an interior node gets too many children, it splits and the messages in its buffer are distributed between the two new nodes."
"the seventh layer in figure 3 deals with the interactions-in the technical, social and economic sense-with the users of the application scenarios and the associated integrative solutions. this is the layer in which the benefits of an urban data space based on din oup become real. in the corresponding applications (such as smartphone apps, information portals, issue management systems, collaboration systems etc.) added value is created for the administration and also for the citizens of a municipality"
"personal data relates directly to physical persons, or allows concluding on different aspects relating to physical persons. in addition to general personal data such as address and age, further examples are given by bank details, hair color or dress size. personal data is subject to the general data protection regulation (gdpr) and may potentially be generated in companies, offices or even in research."
"the selected approach regarding the identification of suitable business models is an interesting way to kick-start the activities towards establishing an urban data platform and correspondingly an urban data space. whilst normally new technologies (e.g., data platforms) are directly introduced in german and european cities, it is first checked and analyzed in emden whether the introduction of an iot platform can be supported by sufficiently viable business models."
"the key benefits of this approach to building urban datasets are: (1) the systematic structuring of existing ict solutions and datasets along the blueprint image, (2) identifying the gaps in the city's ict architecture and the needed actions. at the same time, it becomes visible which systems or components already exist and how they can be linked and mapped to the blueprint reference architecture. (3) the openness of the interfaces and formats promotes interoperability as well as the reuse of components and solutions. existing legacy systems can be integrated by providing and interfacing with interoperable interfaces. (4) existing ict components from other communities can be interchanged and reused. (5) a standards-based approach with open interfaces and formats promises enduring, future-proof, high-security ict solutions. (6) on the basis of the general reference architecture, it is possible to develop a municipality specific urban data space that fulfils the locally defined requirements for a concrete municipality in the long term."
2) the entire process from the main interface to enter the site user predefined can be subdivided into three sub-tasks: car virtual display tasks; cars online modification tasks; automotive ordering process.
"the reconstructed values were calculated in the previous sub-section, from eq. 2. using eq. 4, m and l are computed to generate (120, 34, 23) as the compression code."
"because the user's age,cognitive level,different use habits,products shape,color,movement change of virtual display convey display contents will have a significant difference during receive information process. therefore, the virtual display strategic research is one direction of the interactive form researches [cit] ."
"interactive tasks processes only taking into account the logged-on user, the task flow of the user is not logged does not exist. users have bookmarked modified models direct booking process:"
"interaction design is the user's behavior to artifacts, environment and system, as well as convey that the shape elements of such acts design and definitions [cit] . interactive virtual display is a good carrier for interactive design, in recent years, there have many successful applications of interaction design examples, and the technology can provide a good virtual interactive and analog display environment."
"car modification view automobile brand culture as the basic features, the owner's favorite and preference as the trend, with the industrialized fixed models, combine a specific concept car design, take advantage of advanced techniques and technologies, then add a wide range of accessories, personality improve the car's ornamental and functional and make it comply with relevant national regulations and safety standards, ultimately meet the people of the modified car's personality aesthetic, versatile, diverse needs [cit] ."
"interaction details design needs to be designed each component and details one by one, then complete special effects and visual elements of the design, is the last step of page design."
"geant4 provides the most accurate simulation, which is essential for several but not all parts of physics analyses. the pie chart below shows the typical fraction of full simulation cpu time spent in each subdetector."
"a completely web design includes a range of information, processes and actions, the user mainly focus on web fluency, which including the pages began to enter the page, the user interaction flow in general, certain the function point of the process. therefore uncertainty of user process need optimize the other flow may occur to users, and in the procedure pages choose which use visual language to express the concept of design form, function and image, etc [cit] . it mainly focuses on process design and ignores the small details page with widgets on purpose."
"quadtree decomposition is a simple technique for image representation at different resolution levels, which partitions an image into variable block size regions based on a quadtree structure. studies have demonstrated that quadtreebased image segmentation can be effective and efficient mechanism for isolating blocks of distinct perceptual significance and thereby allowing different coding strategies that are perceptually suited to the individual segment categories [cit] . it provides an effective compromise between the accuracy with which the region boundaries are determined and the overhead required to specify the segmentation information."
"relying on computer and network technology, virtual display is a branch of display design in digitized direction. therefore, the virtual display design as main bodies which mainly based on the knowledge of the display design, computer and internet skills for technical support, and take use of information technology computing and practical equipment to achieve variety demonstration purposes."
"in recently stages, it already has a lot of user research methods to determine the target user, generally into two categories, the first methods user can directly involve in, such as questionnaires, focus groups, depth interviews; second methods are the user indirectly involved: web log analysis, professional assessment, task analysis, etc [cit] ."
"image mosaic is relatively simple method to achieve display, due to it based on virtual display is compared with threedimensional modeling technology. it can demonstrate the objects characteristics of each angel through multi-frame images or photographs continuous display mode, use flash or multimedia software to add sound, controls, and other ways to show or hide the control."
"the isf is fully embedded in the atlas framework (gaudi-athena). the general simulation flow is steered by the simkernel, which is a single algorithm that holds simulator services for the subdetectors, a particlebroker service and the truth service. the simkernel retrieves particles from the particlebroker and routes them to their associated simulation engines. all simulators fill a common set of hit collections which are then processed in the same digitization and reconstruction chain."
"1) according to the site's architecture can draw conclusion that task starting point is entry into the home, user predefined success as the finished point."
"users is the core of the entire design process throughout the design process, during the design process, it should base on the user's understanding of goals and motivations, accurate response user's psychological expectations. it is the cornerstone of the entire design on how to determine user expectations, identify the user's real goal and base on this basis to convey information accurately to the users."
"based on data analysis of target user groups and preference to summed up the user's functional requirements, under this basis, sorting and summed website design function points, then combined with the general features of the site, the overall information architecture of the site are as follows:"
"to model management page as an example: overall page layout to landscape can be divided into three: the middle block is called two rows; intermediate grid is 7 blocks on the left, the right side is 27; block space is10px grid gap. while the grids also standardize the text block's edge"
"in all, after compare and analyze interactive virtual display designs, it comes to the conclusion their three principles: accuracy, patency and consistency. accuracy principles used to guide the site's information construction, patency principles used to guide the task flow, the consistency principle to guide page builder, and are separately pointed out instructional methods and paths. it puts a specific analysis on the online modified car interactive design website based on virtual showcase interactive design model method. established website information architecture through analysis the target user groups and theirs preferences, then base on mission walkthroughs methods to analyze websites task flow, on the condition that user's mental model of and realistic model keep consistency to design the website interface layout and interactive form, verify interactive virtual showcase theoretical model of the design is reasonable and feasible to some extent."
"recently, the rapid development of the internet and ecommerce allows people can buy favorite products at home. automobile manufacturers through a virtual display approach provide a variety of options to solve the limitations problem of auto shops models and quantity."
"where x′ represents the smoothed gray value of the present pixel at which the center of the mask is located, j i x, denotes the gray level of (i, j)th pixel in the mask, j i w, denotes the weight of (i, j)th pixel defined as follows :"
"before mouse over figure ix. after mouse over 360 ° virtual showcase angle switching display using the mouse over and click trigger this asymptotic expansion in the form, the appearance of the page as the default display, then mouse over the button, the button is stretched to form a complete angle switching components. after mouse over login systems and parts modified navigation bar information are using a mouse click trigger form to ensure less interfere with the normal display processes during the process of the mouse over on the page."
"a different type elements which corresponds to the behavior of the user triggers an event or same type interaction elements of behavior caused by the same event, their interactions need to maintain the consistency. for example: all the dialogs that require confirm the operation by users must contain at least two, confirm button and give up button."
"website information architecture is an organizational structure to identify the product or service provides the information between the information. the main task of information architecture is build an open bridge between user awareness and information, it is the carrier of information and intuitive expression [cit] . web-based virtual display car modification online content structure should include three aspects: the first is user system; second is the virtual display and online conversion system; the third is the management system. 3) management system ： managers ensure the website normal operation, the information for managers including: 1. the product information to add, delete; 2.the user information management. it is undoubtedly inappropriate for multi-level managers, because this hierarchical approach make the managers workload doubled, managers need is a flat structure with a specific plan, so that all of the information with the adaptation of these structures [cit] ."
"monte carlo simulations of physics events, including detailed detector simulation, are indispensable for every analysis of high energy physics experiments. increasing the recorded luminosity at the lhc, and hence the amount of atlas data to be analyzed, leads to a steadily rising demand for simulated mc statistics. these mc requirements for more refined physics analyses can only be met through the implementation of fast simulation strategies which enable faster production of large mc samples."
"each block of nxn pixels is converted into a residual block by subtracting the sample mean from the original pixels. the residual samples are less correlated than the original samples within a block. here, two of the most important local characteristics of the image block are considered: central tendency, represented by the mean value as the neighboring pixels in the original block are highly correlated, the residual samples will tend to concentrate around zero. one can then quantize the residual samples prior to forming the histogram. the histogram of the quantized residuals may then be formed and analyzed by simply detecting its peaks. based on the distribution of the residual samples within the test images, we choose to apply a coarse quantization, in particular a 15-level non-uniform quantizer. we now define j q as the output of the quantizer with index j, as shown in \" fig is then analyzed by simply detecting its peaks. according to the number of detected distinct peaks on the histogram, image blocks can be placed into two major categories of uniform and edge blocks. a histogram with a unique peak at its centre (uni-modal histogram) identifies a uniform block. whereas, the existence of two distinct peaks implies that the processed block is an edge block and requires further segmentation. \" fig. 2 \" shows the histogram analysis of a 4x4 uniform block. in the decomposition approach, an image to be coded is first divided into blocks of 16x16 and then each block is repeatedly divided into four equal quadrants, if its residual histogram is not a uni-modal type. on the other hand, the decomposition process will stop if the residual histogram of the block has a dominant peak at its center. this block is regarded as a uniform block and all the pixels in the block will be represented by the block mean. if the smallest block size of 4x4 is reached and its residual histogram is still not a uni-modal type, it is regarded as an edge block. fig. 3 depicts the histogram analysis of a 4x4 edge block."
"consistency in the page layout model was convergence in real life and virtual operating model, the life of operating experience in the virtual operation can also be applied, to achieve the specific spacing page normative through the grid blocks, components, font size, etc."
"the simulation time can be reduced by more than an order of magnitude by using the atlfast-ii fast simulation, which uses fastcalosim in the calorimeter. the energy of single particle showers is deposited directly using parameterizations of the longitudinal and lateral energy profile. it is intrinsically less accurate, but can be tuned against data."
"since the pixels in each smooth block are represented by the block mean, the blocking effect between the boundaries of two blocks occurs unavoidably. to remove the blocking effect, a simple smoothing filter is provided. since only smooth blocks are filtered, the edge blocks will not blurred and edges will be preserved. the smoothing filter uses three"
"a peak on the qrh indicates a high score of residual values; therefore it is fair to conclude that there is a considerable number of pixels that have the same dispersion about the block mean. this, in turn will lead us to conclude that the gray level values of these pixels are very close to one another. hence, this group of pixels can be represented by a single gray value. in this analysis, a distinct peak on the qrh of the processed block represents a gray value j x, given as :"
"we have evaluated the performance of the proposed coding scheme through a computer simulation on a set of gray-level images including the image of \"lena\" shown in \" fig.7a \". the test images are 8 bits per pixel, and the proposed technique was tested on images of 256 x256 pixels and 512 x 512 pixels in size. the largest block size for 512x512 and 256x256 image sizes are 32x32 pixels and volume 5 16x16 pixels, respectively. the simulation platform is microsoft windows xp, pentium iii, and the proposed scheme is implemented using matlab. two performance matrices are used to measure the performance of the proposed compression schemes : compression ratio (bpp), and image quality. the peak signal-to-noise ratio (psnr) is used to evaluate image quality of a compressed mage generated by the proposed scheme."
"the total cpu time spent for the simulation of a typical t t event (see table above) differs by a factor ~3 between the full simulation with g4 and the fast g4 simulation, which uses pre-calculated \"frozen shower\" libraries for the electromagnetic particles in the forward calorimeter."
"steps are as follows: a) under the guidance of the accuracy principles to analysis the user groups and preferences. it analyzes the target users systematically mainly take some methods such as questionnaires, data collection, and other ways to be realized. b) through analyze the target user summarized list of the user's needs, and prioritize the list of requirements, assess whether it is appropriate and when the demand has to be realized. c) analyze the requirements lists; decompose the overall task process, make a number of relatively independent subtasks, observe user operation under the guidance of fluency principle. d) analyze the user's steps and actions, and previously divide into sub-tasks, supply and optimize the task flow. e) describe the user mental model expression form in the specific layout and details, through the principle of consistency. analysis and compare the user's mental model and realistic model to find the mapping between the two methods."
shower width wstot determined in a window corresponding to the cluster size in the high granularity strip layer 1. total recorded: 1.14 fb * wolfgang.lukas@cern.ch
"architecture based on the principles of accuracy during design process, firstly according to the user's gender, age, educational level, and other information, then the different user groups .it mainly focused on meeting the user group who are matches with products, finally make a target design for other user group."
"\" fig. 7 .b\" and \" fig. 7 .c\" show the quadtree segmented images of 'lena' for the image size of 256x256 and 512x512, respectively. \" fig. 7.d\" and \"fig. 7 .e\" show magnified portions of the coded images for image size of 256x256 and 512x512, respectively. the quadtree overhead was computed as 0.023 and 0.010 for 256x256 and 512x512 image sizes, respectively. a compression ratio of 0.32 bpp at 30.15 db, and a compression ratio of 0.28 bpp at 29.57 were achieved for the image size of 256x256, and the image size of 512x512, respectively. tables i and ii show the representation of the splitting mode for both image sizes, and table iii shows the decomposition results."
"in this section, the formal description of the proposed coding algorithm is given. the quad-tree decomposition algorithm is first presented. the coding of the image blocks through the proposed pattern matching technique is then introduced."
"a main point of quadtree segmentation is the evaluation criterion of image segmentation. in quadtree decomposition, a judgment is first made to see whether a block can be represented by a single gray value or whether it must be divided into four subblocks. in this paper, we present a method that operates based on the distribution of the block residuals and determines whether the processed block needs further divisions. this is accomplished by classifying a block either as a low-detail (uniform) or as a high-detail (edge) block. the classifier employs the residual values of a block and classifies the block according to the shape of the histogram of the residuals. the classification is carried out through a peak detection method on the block histogram. a brief description of the classifier is as follows."
"since the development of fatras was started, it has p debugging the track reconstruction algorithms and the the atlas detector, but also as a fast simulation engin show that the description of the physics processes and t a realistic way. the speed increase with respect to a d uses a much more complex description of the detector is is a perfect tool for investigating questions that are rela"
"grid system english name for \"grid systems\", which know and regulate web page layout and information distribution through regular grid array. this layout is developed from a flat grid system [cit] . under the guidance of grid system, the size of the components in the page there is a pattern, so that the page can greatly increase the normative. grid-based design that allows the layout of each page throughout the site keeps consistent and increase page's similarity, thus reducing learning costs and enhances the user experience [cit] . while page layout reducing the workload of the design greatly, it is standardized and reusable."
"accuracy principles performances in the web information architecture are they have integrity content and the user can easily understand the content, operate it with small error rate. in this section will induct and extract information based the target user's needs lists and preferences analysis in previous section, eventually get online car modification page information architecture."
"visual design of interactive elements often affects user interaction effect. the same parts of buttons adopt a consistent style of the visual design is a great help in maintaining user habits and improving interactivity. but there is no special balancing method about how to ensure consistent elements appearance and does not because the appearance of elements design becomes silent and dull, therefore after the competition, it needs to investigate the target user or obtain feedback for further improvements."
"it is uncertain that the order users replace colors, accessories and interior. after several modify the same type components as well as replacement parts, it starts to modified, the modification steps are as follows:"
"natural images can be segmented into regions having widely different perceptual importance. certain regions are critical to subjective evaluation quality, and relatively small quantization errors can perceptually have major degrading effect on the overall reproduction quality. such segmentation of the image is useful for efficient coding of image data [cit] . traditional block-based image coding algorithms, such as vector quantization, transform coding, and block truncation coding techniques require the partitioning of the original image into a number of, usually square blocks of pixels which are then encoded as separate entities. in all these schemes, the block size is a fundamental design parameter. variable-rate image coding that changes the coding resolution (in bits used per unit area) according to the local character and importance of the region to be coded, has become anew direction in image coding."
"virtual display display relying on the traditional display showcase strategy, but it has its own characteristics in the actual operation that the display forms of every aspect are included different interactive design patterns [cit] . during the design process, it mixes with different display forms, interactive design principles are as follows:"
"the result of online modification should present in front of consumers in a lively method, that achieve the presence methods have two main types: one is based on threedimensional modeling of virtual display, and the others is based on image stitching virtual display [cit] ."
"virtualization, namely information technology, reproduces the physical substance in specific digital information in a computer system. it becomes the contemporary physical substance virtualization development trend. display design virtualization will demonstrate the physical substance content into images, symbols, dynamic behavior, and other information media through virtual technology reproduce image and symbols referring to the physical material objects in the display system [cit] ."
"the fast simulation atlfast-ii with the calorimeter simulation fastcalosim has been developed in order to reduce the simulation time in the atlas calorimeter system. it can be tuned against data, has been validated against the geant4 [cit] for large-scale mc production."
"take use of professional modeling software, such as: 3dmax, maya, build three-dimensional and threedimensional objects in the scene model to simulate real-world scenes and objects. with the soaring levels of computer performance and software performance improvement, this modeling approach is an implementation with variety manifestation ability and strong demonstrate ability which can form through the scene and physical modeling, texturing and rendering; simulate real-world scenarios and physical form, material, etc. more accurately and output the animation."
"it should be also noted that, since for a uniform block, no pattern index is transmitted, therefore the compressed code for such a block is the pair )"
"in order to meet the increasing physics demands on mc samples, the integrated simulation framework (isf) is being developed. it allows the flexible combination of full and fast simulation strategies in a single event to provide an optimal balance between precision and execution time, depending on the required accuracy."
"the page task flow patency in the performance that users can successfully complete website functional tasks that does not lead the stagnation during user experience step. in practice, the introduction of specific design task walkthroughs method with user's tasks as a clue, record user interaction step, through observing and analyzing user interaction, and make process improvements to possible changes, and ultimately complete user mission objectives."
"two balancing feedback loops are of interest. when components with information (data) increase, the inspection time for components decreases, and the remanufacturing cycle time also decreases. when cycle time decreases, management is motivated to further increase the components with information, seeing it as a benefit to be reinforced, r1. when components with information increases, the number of components for remanufacturing also increases. this puts pressure on existing capacity, encouraging management to reduce the components with information so as not to overload the system, b1. both feedback loops are in conflict. the cld representing the two key feedback loops is shown in figure 4 . we proceed to draw the stock and flow diagram (sfd) based on the causal loop diagram. figure 5 expands the cld into an sfd. the stock and flow diagram is used to increase the understanding of the feedback and control process of a given system [cit] . the intended simulation model can be used to test various policies regarding whether the company should increase data about the components to remanufacture, on the assumption that the increased availability of information about the component means that it is more likely the component can be sent for remanufacturing and vice versa. the less that is known about the component, the less likely it will be sent in for remanufacturing."
"manufacturing companies implement various strategies in order to enable the transition to a more circular economy as well as enhance the performance and efficiency of the manufacturing systems. among various circular strategies employed, remanufacturing, which restores used products to a like-new state, offers great opportunities to recover products and their parts while adding great benefits to the economy of the localities and countries where remanufacturing activities are carried out. in addition, remanufacturing requires less effort and resources for recovery, as well a retaining part of the raw materials and added value. this has made remanufacturing particularly profitable and viable for automotive-inclined companies, as evidenced in the research. with the entry and growing influence of electric and hybrid cars which are fuel cell-and/or battery-operated, it is becoming increasingly important to understand remanufacturing of the components of the electric cars within the context of industry 4.0. this research was based on the hypothesis that data collected via sensors on the fuel cell can contribute towards their remanufacturing. the objectives of this research were hence set to identify and rank the remanufacturing parameters for the fuel cell, as well as to understand the relationships between data and remanufacturing using a simple cld and sfd. six respondents were selected from three remanufacturing/remanufacturing research companies identified as case studies."
"one limitation in this model is that the results are essentially time-driven. thus, important issues in the decision to remanufacture, such as cost, component value and remanufacturability of the component do not feature in the model. this could be an area for future research."
"this is one of the more obvious metrics (16)- (17) . to conserve energy, there should minimize the amount of energy consumed by all packets traversing from source node to destination node. i.e. we want to know the total amount of energy the packets consumed when it travels from each and every node on the route to the next hop. the energy consumed for one packet is calculated by the equation (1)"
"rich and detailed answers were sought during the qualitative interview, with the list of questions prepared serving as a guide. however, while all of the respondents had remanufacturing engagements outside the uk, they were all based in the uk. this may be argued to lack representation, as opposed to if the respondents had been based across different global geographical locations. it is recommended that a wider sample of respondents be utilised in further research."
"to map out the structure of a complex system, a causal loop diagram (cld) is used, as shown in figure 4 . in a complex system, [cit], the cause and effect connections often form loops which indicate information feedback between parameters. the structure and behaviour of this system is determined by the nature of these feedback loops. the cld is then expressed as a mathematical model after the different interactions and feedback among different variables of the elements are considered. this is then converted to computer simulations or the stock and flow diagram (sfd) [cit] . negative (−) and positive (+) polarities are assigned to the causal link on the cld. these polarities represent the relationships between respective connected parameters. [cit] state that these polarities also indicate how a dependent parameter changes when an independent parameter changes. the notation b and r signify a negative (or balancing) loop and a positive (or reinforcing) loop, respectively (these are feedback loops)."
"where u smo is sliding-mode control function;û d is estimation of the real disturbance voltage u d and ω c is the adaption gain forû d . unlike conventional sliding mode observer, the estimated disturbance is not directly reconstructed from switching function but extracted from sliding mode control function u smo as shown in (19) ."
"where s is laplace operator. from (40), it can be seen that estimation error of stator current is low-pass filtered value of estimation error of e u . the bandwidth of h(s) is"
"the researchers developed questions which requested respondents to give their views on the expanding parameters for data-driven remanufacturing. the data was gathered via face-to-face interviews; 2 were held at the companies' on-site location, and the third, for company a, was administered via webex video conferencing. for data collection, a two-part questionnaire was developed. part a consisted of questions relating to the company's experience in remanufacturing. part b consisted of questions on existing parameters for remanufacturing, new parameters for remanufacturing sensor-enabled components such as the fuel cell in electric vehicles, their rank of importance and efficiency calculation for fuel cell remanufacturing. the questions were developed based on an extensive review of literature and several discussions among the authors, and were first tested with the respondent from company a. this extensive evaluation of the questionnaire helped the researchers to produce a comprehensive list of questions and, based on the first feedback, produced a more objective list. after the first round of interviews, follow-up questions were sent to respondents via telephone calls and emails. this was done to ensure a thoroughness of the data collection process and to answer any criticism relating to issues of respondent numbers [cit] ."
"according to (43), the rotor position can be estimated based onû d aŝ whereû dα andû dβ are real and imaginary part ofû d respectively. as explained previously,û d is the result of u d passing a complex coefficient, which can attenuate noises and harmonics. hence, phase-locked loop (pll) is not compulsory for the proposed smdo. after obtaining rotor position, rotor speed can be directly calculated aŝ"
"to explore the data parameters for fuel-cell remanufacture, semi-structured interviews were undertaken. [cit], this type of interview method \"has predetermined questions, but the order can be modified based upon the interviewer's perception of what seems most appropriate. question wording can be changed and explanations given; particular questions which seem appropriate with a particular interviewee can be omitted, or additional ones included\". hence, this kind of interview offers greater room for flexibility to the interviewer and interviewee and can be administered via a face-to-face interview session or via a phone/video conferencing call session."
"all companies asked to remain anonymous for this research. the respondents for the three companies were selected primarily because of their knowledge and experience in remanufacturing, sustainability and the fuel cell electric vehicles."
"it is seen that ∆θ r is small and there is no large speed error during dynamic process. the actual rotor speed can return to its reference quickly after load change, indicating good robustness against load variation. fig. 9 shows tested responses during speed variation. initially, the motor is rotating at 150 rpm. after a moment, speed reference steps to 1500 rpm and then returns to 150 rpm. it can be seen that the actual speed can track the reference quickly and the controller can work stably during fast speed change (4500 rpm/s). as back-emf is small during low speed operation, the observer is more sensitive to non-ideal factors, such as deadtime, noises and dc bias in the measurement. as a result, the harmonic errors get larger as speed decreases. nevertheless, the average"
"the transportation industry is responsible for 17% of global greenhouse gas emissions every year [cit] . thus, in the development of clean energy technologies, electric vehicles (evs) have been touted as an excellent option for the reduction of emissions in the transportation sector [cit] . currently, evs are driven either by electrical energy stored in batteries or by fuel cell units. power units in evs are composed of two stacks, with each containing 40 fuel cells, making 80 fcs in all. there is a tank that supplies the hydrogen. battery-driven evs (bevs) have the challenge of weight and recharging is climate change [cit] . fuel cells provide sustainable solutions and energy security in response to these issues. the static nature of fuel cells also means that they can operate without the challenge of noise or vibration [cit] ."
"the majority of energy efficient routing protocols [cit] for manet try to reduce energy consumption by means of an energy efficient routing metric, used in routing table computation instead of the minimum-hop metric. this way, a routing protocol can easily introduce energy efficiency in its packet forwarding. these protocols try either to route data through the path with maximum energy bottleneck, or to minimize the end-to-end transmission energy for packets, or a weighted combination of both. a first approach for energyefficient routing is known as minimum transmission power routing (mtpr). that mechanism uses a simple energy metric, represented by the total energy consumed to forward the information along the route. this way, mtpr reduces the overall transmission power consumed per packet, but it does not directly affect the lifetime of each node. however, minimizing the transmission energy only differs from shortest hop routing if nodes can adjust transmission power levels, so that multiple short hops are more advantageous, from an energy point of view, than a single long hop."
"despite the increasing interest and social, economic and environmental benefits associated with remanufacturing, challenges associated with remanufacturing exist. in a survey of 188 [cit], several challenges faced by remanufacturing companies were underlined."
"so, mobile nodes in manets are battery driven. thus, they suffer from limited energy level problems. also the nodes in the network are moving if a node moves out of the radio range of the other node, the link between them is broken. thus, in such an environment there are two major reasons of a link breakage."
"2) for compact representation and to facilitate theoretical analysis, complex-vector based state variables rather than conventional scalar notations are used for stability and performance analysis of smdo."
"since it was identified as an \"environmentally aware energy supply\" [cit], fuel cell (fc) technology has been used to replace energy supply systems such as batteries. according to literature [cit], the benefits of fcs revolve around their high efficiencies and low emissions. hydrogen-powered fuel cells produce clean, pollution-free energy and have more than twice the efficiency of the conventional internal combustion (ic) engine [cit] . comparatively, the traditional combustion-based power plant would generate at efficiencies of 33 to 35%, while fuel cell systems would generate electricity at efficiencies of up to 60% [cit] and higher with cogeneration [cit] . sixty percent (60%) of the fuel's energy is used in the fuel cell system. this corresponds to a more than 50% reduction in fuel consumption in comparison to a traditional vehicle with a gasoline ic engine. composed of three active components, the fuel electrode (anode), an oxidant electrode (cathode), and an electrolyte in between them, a fuel cell is an electrochemical device that coverts the chemical energy of a fuel directly into electrical energy [cit] . the electrolyte is placed between the two electrodes, with bipolar plates on either side of the cell which help to distribute gases and serve as current collectors [cit] . the fuel cell provides an integrated cleaner alternative to the thermal processes involved in traditional combustion-based engines. current combustion-based engines and energy generating technologies cause harm to the environment and contribute to many global issues, the most predominant of which is climate change [cit] . fuel cells provide sustainable solutions and energy security in response to these issues. the static nature of fuel cells also means that they can operate without the challenge of noise or vibration [cit] ."
"communication has become very important for exchanging information between people from, to anywhere at any time. manet is group of mobile nodes that form a network independently of any centralized administration. since those mobile devices are battery operated and extending the battery lifetime has become an important aim. most of the researchers have recently started to consider power-aware development of efficient protocols for manets. as each mobile node in a manets performs the routing function for establishing communication among different mobile nodes the ''death'' of even a few of the nodes due to power exhaustion might cause disconnect of services in the entire manets."
"the fraction of dropped packets increases as the traffic intensity increases. therefore, performance at a node is often measured not only in terms of delay, but also in terms of the probability of dropped packets. dropped packet may be retransmitted on an end-to-end basis in order to ensure that all data are eventually transferred from source to destination. losses between 5% and 10% of the total packet stream will affect the network performance significantly."
"experimental tests were carried out on a 2.4 kw pmsm test bench. motor and control parameters are listed in table i. the developed algorithm is implemented on a 32-bit floating point dsp tms320f28335. all the data of waveforms are acquired by a yologawa's dl850e scopecorder. during experimental tests, a magnetic powder brake is used to apply external load to the tested motor. detailed experimental setup is shown in fig. 4 ."
"two balancing feedback loops are of interest. when components with information (data) increase, the inspection time for components decreases, and the remanufacturing cycle time also decreases. when cycle time decreases, management is motivated to further increase the components with information, seeing it as a benefit to be reinforced, r1. when components with information increases, the number of components for remanufacturing also increases. this puts pressure on existing capacity, encouraging management to reduce the components with information so as not to overload the system, b1. both feedback loops are in conflict. the cld representing the two key feedback loops is shown in figure 4 . we proceed to draw the stock and flow diagram (sfd) based on the causal loop diagram. figure 5 expands the cld into an sfd. the stock and flow diagram is used to increase the understanding of the feedback and control process of a given system [cit] . the intended simulation model can be used to test various policies regarding whether the company should increase data about the components to remanufacture, on the assumption that the increased availability of information about the component means that it is more likely the component can be sent for remanufacturing and vice versa. the less that is known about the component, the less likely it will be sent in for remanufacturing."
"the simulation results are compared with those of the current status in table 4 . from the table above, the current capacity utilisation is low because there are not enough components entering the system (because there are not enough components with information). the system can be slightly improved by allowing the components with information to vary according to current available capacity. for an ideal situation, the capacity should be increased."
"information about a component is needed before it can be remanufactured. components with information do not require inspection, as their status is already known from the data about the component. components without information need to be inspected physically before it can be determined whether to remanufacture them or not."
"information and data sharing by manufacturers and oems remains a challenge in research, as original data is increasingly viewed as a competitive advantage by companies. thus, accessing data such as data from sensors for eol research is a limitation of this research. while investigation of data sharing and collection paradigms is outside the scope of this research, oems and automotive companies can be encouraged to share data for research when there are viable benefits for them to do so."
"for power converters and motor drives, inner current loop plays an important role in performance of the whole control system. during past decades, many methodologies have been investigated for high performance current regulation, including proportional-integral (pi) control [cit], hysteresis control [cit], sliding-mode control [cit], and predictive control [cit] ."
"in digital implementation, there is typically one-step delay between the actual applied voltage and the calculated voltage [cit] . this means that the calculated u k s will be applied in the (k + 1)th instant in practical application rather than the expected kth instant. in this paper, model based prediction is employed to compensate for digital delay. by further shifting (4) one-step ahead, u k+1 s can be solved as"
"research related to remanufacturing is varied; however, it can be largely grouped into four categories; these include: (1) research into the processes in remanufacturing and design for remanufacture or dfrem; (2) the business models, frameworks and the wider supply chain associated with remanufacturing; (3) research into the benefits of remanufacturing; and (4) challenges experienced in remanufacturing."
"running the simulation model is intended to reveal how the system will behave when components with information is increased. the ideal situation is for 100% of components to have information. hence, there are two options worth considering:"
"where, n i to n k are nodes in the route while t denotes the energy consumed in transmitting and receiving a packet over one hop. then we find the minimum e c for all packets. the main objective of epar is to minimize the variance in the remaining energies of all the nodes and thereby prolong the network lifetime."
"to map out the structure of a complex system, a causal loop diagram (cld) is used, as shown in figure 4 . in a complex system, [cit], the cause and effect connections often form loops which indicate information feedback between parameters. the structure and behaviour of this system is determined by the nature of these feedback loops. the cld is then expressed as a mathematical model after the different interactions and feedback among different variables of the elements are"
"a reason for this slow uptake in fuel cell remanufacturing research is due to the many components in the fuel cell stack that have to be considered. this includes membranes, bipolar plates, catalysts and membrane-electrode assemblies, as well as the fact that an efficient fc remanufacturing must be low-cost and high-volume manufacturing process [cit] . remanufacturability of fuel cells, though, at end of life, is possible, as fuel cell stacks in fcevs have been shown to be recoverable when they reach their end of life [cit] . however, as the catalyst within the membrane degrades, the membrane would need to be replaced with the catalyst [cit] . this drives up the cost for remanufacturing of fcs, and is hence a limitation for fc remanufacturing, as the stacks cannot be reused multiple times without changing the membrane."
"thus, remanufacturing makes extensive reuse of a product possible, hence keeping the product within a chain of circularity. this benefit is in addition to the demonstrated economic, social and environmental benefits of remanufacturing [cit] observed across key remanufacturing sectors. [cit] reveals that remanufacturing was found to take place in 22 sectors of manufacturing activity, of which the aerospace, automotive and mechanically powered machinery sectors were found to have the highest uptake. [cit] states that 45% of gearboxes and 23% of engines in the aftermarket inventories of original equipment manufacturers (oems) are remanufactured worldwide."
"wireless network has become increasingly popular during the past decades. there are two variations of wireless networksinfrastructured and infrastructureless networks. in the former, communications among terminals are established and maintained through centric controllers. examples include the cellular networks and wireless local networks (ieee802.11). the latter variation is commonly referred to as wireless adhoc network. such a network is organized in an adhoc manner, where terminals are capable of establishing connections by themselves and communicate with each other in a multi-hop manner without the help of fixed infrastructures. this infrastructureless property makes an ad hoc networks be quickly deployed in a given area and provides robust operation. example applications include emergency services, disaster recovery, wireless sensor networks and home networking."
"what are the variables needed for remanufacturing the fuel cell? table 3 gives a compilation of the data gathered from the respondents as it relates to the questions asked. it can be seen in table 3 that there is no uniformity in terms of the variables needed to remanufacture fuel cells. this comes as no surprise, perhaps, as remanufacturing has been affirmed in literature to have no singular definition [cit] . ('manufacturers' include both oem and third-party remanufacturers). secondly, variables that affect fuel cell remanufacturing can be grouped into three categories: (i) variables from fc stack; (ii) variables required for traditional remanufacturing; and (iii) variables relating to location of oem, third-party remanufacturers and sustainability of the process. thirdly, while sensors produce useful information to enable remanufacturing of fuels, expertise from traditional remanufacturing is also needed. hence, effective remanufacturing of fcs is a collaborative effort. a pareto analysis and chart was used to rank these variables in order to understand the most important parameters required for remanufacturing fcs. the pareto principle [cit] states that in any population that contributes to a common effect, a few account for the bulk of the effect."
"where the hatˆdenotes the estimated variables. if t 2 sp related terms are not considered, the following equations can be obtained according to (50) and (51) deduced in the appendix."
"finally, it was observed that the values (built around the data) are more discretely occurring than dynamic. a part comes in for remanufacturing, the data is utilised and analysed, a decision is taken to remanufacture or not, and the process is completed. as a further work, it is recommended that other modelling and simulation techniques such as discrete event simulation are deployed in the investigation of the relationship between data from sensors and remanufacturing. this may uncover other values important to the remanufacturer. the modelling and simulation in this research was completed with hypothetical data which was approved by the team of experts constituting the respondents for this study."
"in the route discovery phase [cit], the bandwidth and energy constraints are built in into the dsr route discovery mechanism. in the event of an impending link failure, a repair mechanism is invoked to search for an energy stable alternate path locally."
"with table-driven routing protocols, each node attempts to maintain consistent [cit] up to date routing information to every other node in the network. this is done in response to changes in the network by having each node update its routing table and propagate the updates to its neighboring nodes. thus, it is proactive in the sense that when a packet needs to be forwarded the route is already known and can be immediately used. as is the case for wired networks, the routing table is constructed using either link-state or distance vector algorithms containing a list of all the destinations, the next hop, and the number of hops to each destination."
"epar algorithm is an on demand source routing protocol that uses battery lifetime prediction. in fig. 1, dsr selects the shortest path aefd or aecd and mtpr selects minimum power route path aefd. but proposed epar selects abcd only, because that selected path has the maximum lifetime of the network (1000s). it increases the network lifetime of the manet shown in equation (2) . the objective of this routing protocol is to extend the service lifetime of manet with dynamic topology. this protocol favors the path whose lifetime is maximum. we represent our objective function as follow:"
"extensive simulations were conducted using ns-2.33. the simulated network consisted of 120 [cit] x2000m area at the beginning of the simulation. the tool setdest was used to produce mobility scenarios, where nodes are moving at six different uniform speeds ranging between 0 to 10 m/s and a uniform pause time of 10s. table 2 shows the simulation parameter setting for the protocol evaluation. these were generated using the tool epar..tcl, with the following parameters. fig. 2 shows that the consumed power of networks using epar and mtpr decreases significantly when the number of nodes exceeds 60. on the contrary, the consumed power of a network using the dsr protocol increases rapidly whilst that of epar based network shows stability with increasing number of nodes. fig. 3 shows that the end to end delay with respect to pause time of network using mtpr and dsr increases significantly when the pause time exceeds 70secs. on the contrary, the end to end delay operating epar protocol increases slowly compared with mtpr based network shows a gentle increase with increasing number of pause time. observe that epar protocol maintenance the stable battery power while calculating the end to end delay. fig. 4 shows the throughput of dsr protocol becoming stable when the number of nodes exceeds 60 while the mtpr increases significally. on the other hand the throughput of epar increases rapidly when the nodes exceeds 60 with 80% efficiency than mtpr and dsr. fig. 5 shows that the dsr protocol becomes inefficient when the network consists of more than 700 traffic size for low density network while for high density network becomes inefficient when the network consist more than 1000 sources. epar shows the best performance with maximum network lifetime than mtpr and dsr. fig. 6 shows the network lifetime as a function of the number of nodes. the life-time decreases as the number of nodes grow; however for a number of nodes greater than 100, the life-time remains almost constant as the number of nodes increases. lifetime decreases because manet have to cover more nodes as the number of nodes in the network size increases. we observe that the improvement achieved through epar is equal to 85 %. energy is uniformly drained from all the nodes and hence the network life-time is significantly increased."
"company c is a uk-based hydrogen-powered fuel cell vehicle manufacturer. with a design studio in spain, their business model is one which puts stakeholders, sustainability and profitability at the core of the business. their prototype hydrogen fuel-cell car has a range of 300 miles on an 8.5 kw hydrogen fuel cell and with emissions of zero at the tailpipe (just water vapour). with an employee strength of 25, their partners include michelin, ks composites, sevcon, sdc seat design. circular economy is at the heart of what they do."
"the p t value must be the power that the packet is actually transmitted on the link. if for any reason a node chooses to change the transmit power for hop i, then it must set the p t value in minimum transmission power (mtp[i]) to the actual transmit power. if the new power differs by more than m thresh then the link flag is set. table 1 shows the data packet format for epar. the packet includes the dsr fields besides the special fields of epar."
"respondents agreed that it was important to rank the identified variables. in total, 19 different variables were computed, as shown in table 3 . the nine (9) parameters for remanufacturing, as presented in table 1, are grouped as \"trad reman\" or traditional remanufacturing variables. from table 3, it can be observed that there are no large deviations between the highest-and the lowest-ranked variable; the range of values is just 5. thus, it could be argued that while efficiency demands that variables are ranked, all variables are significant for sensor-enabled remanufacturing. based on figure 3, the most important variables within the top 80% were compiled and described."
"in practical application, the actual values of inductance and resistance are unknown and the output voltage u k+1 s of dbpc can only be calculated based on the estimated motor parameters, measured/estimated position and speed."
"these included high labour costs, quality of feedback, lack of sales channels, lack of product knowledge, volume or availability, lack of technology and customer recognition. there is also a lack of knowledge in the assessment of remanufacturing technical and organisational processes [cit] . these challenges can contribute to, among other things, long and variable remanufacturing process lead times [cit] . where large data sets are available, as in the case of fuel cells within the powertrain of hybrid and electric vehicles, it is becoming difficult to implement real-time and accurate remanufacturing on the shop-floor."
"the below data are all hypothetical estimates to enable the presentation of simulation results that mirror what may occur in real life. this hypothetical data was agreed upon with the respondents; however, using more realistic data (real estimates from one of the companies) would be ideal."
"3) a complex-coefficient filter is inherently incorporated in the designed smdo, which can preserve fundamental components while suppressing harmonics. neither low pass filter nor phase compensation is required. 4) both robustness of control system and sensorless operation are experimentally evaluated on a two-level inverter fed pmsm drive. the obtained results confirm that two functions can be achieved in one design."
"as can be seen from (5), the accuracy of calculated voltage depends on motor parameters. any deviation in these parameters will deteriorate control performance, which will be discussed in the next section."
"it is the time span from the deployment to the instant when the network is considered nonfunctional. when a network should be considered nonfunctional is, however, applicationspecific. it can be, for example, the instant when the first mobile node dies, a percentage of mobile nodes die, the network partitions, or the loss of coverage occurs. it effects on the whole network performance. if the battery power is high in all the mobile nodes in the manet, network lifetime is increased."
"the remainder of the paper is organised as follows. section 2 gives an overview of some terms and terminology utilised in the paper. in section 3, the materials and methods deployed in meeting the objectives of this paper are described. section 4 contains the analysis and discussion. in section 5, the results are presented. the conclusions, limitations and considerations for further work form sections 6 and 7."
"modelling results show that when components with information (data) increase, the inspection time for components decreases, and the remanufacturing cycle time also decreases. when the number of components with information increases, the number of components for remanufacturing also goes up. this puts pressure on existing capacity, encouraging management to reduce the components with information so as not to overload the system. furthermore, from the simulation, it is seen that the system will not be able to cope if the components with information are increased without a corresponding rise in manufacturing. this is important for manufacturers in order to ensure sustained efficiency in the system."
"in the choice of organisation and respondents to interview, an initial search for remanufacturing and automotive companies was undertaken using the \"european remanufacturing network\" database (www.remanufacturing.eu). this database holds case studies of 66 companies across range of 10 key industry sectors. however, the focus was on the automotive sector, so an initial pruning was performed. emails were sent and phone calls were made to the 19 companies who had automotive sector case studies recorded in the database. out of these 19 companies, 9 responded, and further discussions were held. three companies eventually agreed to participate. while this number is a fraction of the initial pool of 19 [cit] argue that smaller case studies improve the capturing of greater detail regarding the context within which the problem studied exists. these selected respondents also showed a willingness to participate and share the deeper characteristics of their companies. the selected companies had ongoing collaborations with research universities, and this was an enabling factor."
"company b is a uk and global leader in automotive remanufacturing and manufacturing. with over 45 years' experience, they have served the automotive industry in oem steering systems (manufacturing over 60,000 steering columns annually), military engineering, remanufactured steering, remanufactured hydraulics. the company is iso 9001: 2015 certified and is a member of various remanufacturing and manufacturing networks across the world. suppliers include jlr, ford, leyland trucks, arriva, volvo, textron, caterpillar, and a host of others."
"respondents were asked to give their expert judgment on the variables which they view as important in remanufacturing fuel cells in evs. as fuel cells are enclosed within a bms, respondents were encouraged to suggest possible remanufacturing variables which they felt should be considered for remanufacturing the fuel cell. this is shown in table 2 ."
"improving durability and reducing costs rank as the two most important challenges in the commercialisation of fcs. to serve as a viable alternative to fossil-powered systems, fc systems must be cost-competitive and perform better than or as well as conventional power technologies over the life cycle of the system. they should also have an eol mechanism that extends the life cycle of the fc stack. as other eol options such as recycling of fcs [cit] have their limitations, remanufacturing has been proposed as an appropriate eol option [cit] . despite the growing academic and manufacturing interest in fcs, end-of-life interest in fcs, especially remanufacturing, has received little research to date. a scopus search of \"remanufacture*\" and \"fuel cell*\" revealed only 8 results. of these 8 articles, only 2 come from journals and there have been only 4 [cit] . this suggests that the research area is still in its infancy. a scopus search of \"recycle*\" and \"fuel cell*\", on the other hand, yielded an initial 757 documents."
"where u d is the disturbance voltage as shown in (16) . in (15), u d is incorporated to compensate for the influence of inaccurate model parameters so that the responses of (15) is exactly the same as (1)."
"as explained in section v-b, the proposed smdo can be used for position-sensorless control with refined estimation of motor parameters. in the following tests, actual position and speed are acquired through an optical encoder. they are not used in the controller or observer, but only displayed in the scope for checking the accuracy of the estimation. fig. 7 shows steady state responses with rated load at 1500 rpm (100% rated speed) and 75 rpm (5% rated speed). it is seen that the estimated position almost coincides with the actual position. there is no chattering phenomenon or significant ripples though the position is directly calculated through arc-tangent function."
"in the auto industry, for example, nearly all global players are producing electric vehicles (evs) with parts which are sensor-embedded. this is expected to rise in the short term; [cit], over 11 million evs will have been sold globally [cit] . recycling has been proposed as the end-of-life (eol) option for parts such as ev batteries (called rechargeable energy storage systems) and fuel cells, but there are many issues that make recycling less viable. the degraded battery that is taken out of the ev, for instance, still possesses around 80% remaining capacity [cit] . recycling of such batteries reduces the active bulk of the batteries to material constituents, leading to the total loss of the remaining 80% of available capacity in the ev batteries [cit] . there have been concerns about the economic viability of recycling [cit] . for example, studies have questioned the ability of the market to absorb the enormous quantity of recycled materials, which may result in the long term in a situation where not all degraded batteries can be directly remanufactured into new batteries [cit] . there is also doubt regarding attaining the purity of recovered materials and sustainability objectives of evs [cit] ."
"organisational and respondent characteristics for this research are shown in table 2 . respondents are denoted by numbers (1)- (6) . the companies selected for the study are innovative and leading companies in their respective fields of digital manufacturing research, remanufacturing, and automotive manufacturing. below are the profiles for the participating companies. company a is a digital manufacturing and remanufacturing research company. it is an independent business, operating from three offices in the united kingdom. [cit] and is a strategic partner to the manufacturing sector, providing valuable services to government-funded programmes and private business as consultants and collaborators with academia. their capabilities include advanced manufacturing research, virtual engineering, circular value chains, data analytics for resource efficiency, and manufacturing new technologies. collaborators of company a include microcab industries ltd., mct reman ltd., env-aqua solutions ltd. and hydrogen london."
"to map out the structure of a complex system, a causal loop diagram (cld) is used, as shown in figure 4 . in a complex system, [cit], the cause and effect connections often form loops which indicate information feedback between parameters. the structure and behaviour of this system is determined by the nature of these feedback loops. the cld is then expressed as a mathematical model after the different interactions and feedback among different variables of the elements are considered. this is then converted to computer simulations or the stock and flow diagram (sfd) [cit] . negative (−) and positive (+) polarities are assigned to the causal link on the cld. these polarities represent the relationships between respective connected parameters. [cit] state that these polarities also indicate how a dependent parameter changes when an independent parameter changes. the notation b and r signify a negative (or balancing) loop and a positive (or reinforcing) loop, respectively (these are feedback loops). to develop the causal loop diagram and the stock and flow diagram, we shall consider these assumptions."
"the mobile node battery power consumption is mainly due to transmission and reception of data packets. whenever a node remains active, it consumes power. even when the node sleepy participating in network, but is in the idle mode waiting for the packets, the battery keeps discharging. the battery power consumption refers to the power spent in calculations that take place in the nodes for routing and other decisions. the number of nodes in the network versus average consumed battery power is considered as a metric."
"as filed-weakening operation is not discussed here, i dref is kept as zero and i qref is obtained from outer speed control loop. considering the bandwidth of outer speed control loop is generally much smaller than the sampling frequency of the inner control loop, it is reasonable to assume that i tsp assuming that ω r is constant within prediction horizons [cit] . hence, i k+2 ref can be simply calculated according to (6) as"
"epar schemes make routing decisions to optimize performance of power or energy related evaluation metrics. the route selections are made solely with regards to performance requirement policies, independent of the underlying ad-hoc routing protocols deployed. therefore the power aware routing schemes are transferable from one underlying ad hoc routing protocol to another, the observed relative merits and drawbacks remain valid. there are two routing objectives for minimum total transmission energy and total operational lifetime of the network can be mutually contradictory. for example, when several minimum energy routes share a common node, the battery power of this node will quickly run into depletion, shortening the network lifetime. when choosing a path, the dsr implementation chooses the path with the minimum number of hops [cit] . for epar, however, the path is chosen based on energy. first, we calculate the battery power for each path, that is, the lowest hop energy of the path. the path is then selected by choosing the path with the maximum lowest hop energy. for example, consider the following scenario. there are two paths to choose from. the first path contains three hops with energy values 22, 18, and 100, and the second path contains four hops with energy values 40, 25, 45, and 90. the battery power for the first path is 18, while the battery power for the second path is 25. because 25 is greater than 8, the second path would be chosen."
"however, to reduce overshoot during dynamic process and attenuate interference with sampling noise, the bandwidth of such controller is usually limited and hard to tune [cit] . additionally, classic pi control may suffer from instability during high speed operation [cit] . hysteresis control presents fast dynamic response and good robustness. the main issues are requirement of high sampling frequency, relatively larger current ripples and variable switching frequency. sliding-mode control (smc) features good robustness against disturbance and dynamic performance [cit] . in practical application, smc must be well designed to avoid high frequency chattering. generally, predictive control can be classified into finite-control-set control (fcsc) and continuous-control-set control (ccsc). in fcsc, there is no modulator and the discrete voltage vector is directly selected online by minimizing a cost function [cit] . fcsc has advantages of fast dynamic response and flexibility to handle system constrains, etc. however, the computational burden of fcsc is usually high especially for long prediction horizons [cit] and multiple vectors based schemes [cit] . additionally, variable switching frequency and relatively larger harmonic ripples are issues need to be addressed. in ccsc, the output voltage is usually synthesized by a modulator with fixed switching frequency and concentrated harmonic spectrum. among ccscs, deadbeat predictive control (dbpc) is one of the commonly used methods owing to its good dynamic performance and simple calculation. however, as dbpc is directly derived based on the system model to cancel tracking error at the next sampling instant, its performance is inevitably influenced by accuracy of model parameters."
"for modelling and simulation, this study uses system dynamics (sd). employed for computer simulation modelling, sd is a methodology used for understanding the dynamic behaviour of complex systems in order to analyse and solve complex problems [cit] . a system is described as a set of elements continuously interaction over a period of time [cit], while the term dynamics refers to the situation where these systems have variables that are constantly changing. [cit] states that in an sd environment, relationships and connections between the components are called the structures of the system. it is these structures that define the system's behaviour [cit] . the functional idea of sd is the perception of a system as a \"coherent whole\" [cit] in terms of its dynamics. thus, the sd approach allows the user to take into consideration information feedback, which exists in the modelled system, as well as describing the causal dependence of elements of the system which is being tested [cit] . discrete event simulation (des) is the most popular approach for simulation of models; however, due to its \"unnecessary complexity\" [cit] and time-consuming nature, it is not utilised in this research."
"this research paper mainly deals with the problem of maximizing the network lifetime of a manet, i.e. the time period during which the network is fully working. we presented an original solution called epar which is basically an improvement on dsr. this study has evaluated three power-aware adhoc routing protocols in different network environment taking into consideration network lifetime and packet delivery ratio. overall, the findings show that the energy consumption and throughput in small size networks did not reveal any significant differences. however, for medium and large ad-hoc networks the dsr performance proved to be inefficient in this study. in particular, the performance of epar, mtpr and dsr in small size networks was comparable. but in medium and large size networks, the epar and mtpr produced good results and the performance of epar in terms of throughput is good in all the scenarios that have been investigated. from the various graphs, we can successfully prove that our proposed algorithm quite outperforms the traditional energy efficient algorithms in an obvious way. the epar algorithm outperforms the original dsr algorithm by 65%."
"within the scope of remanufacturing implementation benefits, several authors [cit] have identified specific environmental, social and economic benefits. these include a decrease in the use of resources, water and energy consumption, creation of employment opportunities, as remanufacturing is highly labour-intensive, as well as a 40-65% reduction in used material costs, and lower capital investment in factories and equipment acquisition [cit] ."
"with on-demand driven routing, routes are discovered only when a source node desires them. route discovery and route maintenance are two main procedures: the route discovery process [cit] involves sending route-request packets from a source to its neighbor nodes, which then forward the request to their neighbors, and so on. once the route-request reaches the destination node, it responds by uni-casting a route-reply packet back to the source node via the neighbor from which it first received the route-request. when the route-request reaches an intermediate node that has a sufficiently up-to-date route, it stops forwarding and sends a route-reply message back to the source. once the route is established, some form of route maintenance process maintains it in each node's internal data structure called a route-cache until the destination becomes inaccessible along the route. note that each node learns the routing paths as time passes not only as a source or an intermediate node but also as an overhearing neighbor node. in contrast to table-driven routing protocols, not all up-to-date routes are maintained at every node. dynamic source routing (dsr) and ad-hoc on-demand distance vector (aodv) [cit] are examples of on-demand driven protocols."
"it was found that variables required in the remanufacturing of the fuel cells are inextricably linked to the kind of data producing them. for purposes of remanufacturing, it is important to sub-categorise this data in two forms, namely: data from sensors and data from other sources. within data from sensors, it was found that the vibration data which gives information about the physical state of the product was viewed as most important by the respondents within the context of remanufacturing. while the data from sensors are important, overall, for remanufacturing, the fuel cell is more dependent on data from other sources, named, \"traditional variables\". for manufacturers this finding is important, as it would mean that there is need for greater collaboration between remanufacturers of diesel engine vehicles and electric/hybrid vehicles. within the wider economic and environmental context, this provides evidence that data is important in enabling a circular economy."
v. deadbeat current control with smdo shown in equations (35) and (36) respectively. analysis in the previous section has proven that the designed smdo can track actual current and disturbance voltage accurately and thus the solution of (42) would be more robust against parameter mismatches than conventional solution (9).
"it can be seen that error of inductance would introduce cross coupling between i d and i q . with a small sampling period t sp, ∆rt sp is generally negligible and thus resistance variation usually has little impact on control performance [cit] . the error of rotor flux mainly influences tracking accuracy of q−axis current, especially at high speed. additionally, experimental results in some research [cit] show that a over estimated inductance would cause significant larger ripples in the current response. the reason can be explained as follows. when only inductance error is considered, the transfer function (14) can be obtained based on (10) and (11) as"
"the below data are all hypothetical estimates to enable the presentation of simulation results that mirror what may occur in real life. this hypothetical data was agreed upon with the respondents; however, using more realistic data (real estimates from one of the companies) would be ideal."
"for further research, it is recommended that results from sensor-embedded products, such as the fc or ress, be utilised as a validation procedure where the data is collected over a longer period of time. within the established cycle time, the bms produces data for each of the identified parameters. it is recommended that real-time figures be extracted for the fc and other sensor-enabled battery components such as the rechargeable energy storage system as a means for validation. this could further inform useful relationships between data and remanufacturing."
"air flows through channels to the cathode on the other side of the cell. when electricity has been produced (that is, work has been done), the electrons return and react with oxygen in the air and the protons (having already moved through the membrane) at the cathode. water is hence formed, and this exothermic reaction [cit] generates heat, which can be used outside the fc."
"in this paper, we propose a novel proactive key renewal scheme that resolves the security flaws of the reactive key renewal schemes. first, our scheme periodically changes ch role nodes in a cluster while preventing a compromised node from being elected as a ch. second, in our scheme, each node employs a distinct key from other nodes in the same cluster for communication with its ch. our scheme does not directly renew the communication keys in contrast with the reactive renewal schemes. instead, our scheme performs key renewals using secure ch elections, so it is called di-rect (dynamic key renewal using cluster head election) in this paper. after deployment, all sensors are grouped into some sectors to avoid the interference between ch elections in the network. then, sensors establish pairwise keys with other sensors in their sector. these keys are employed for the communication between a ch and its members in the sector. then, each sector elects a ch securely by defeating the malicious actions of compromised nodes. the secure ch election is periodically invoked until the network's extinction. as a result, the ch role nodes and the keys employed for communication between a ch and its members are renewed periodically. therefore, our scheme is robust against the compromise of chs and member nodes."
"in the commitment based scheme, each sensor creates a random value and sends its encrypted random value to other sensors in the unicast manner. then, each sensor sends the fulfillment value (that is, its random value) to other sensors. receiving sensors verify the fulfillment values using the shared key, and sum them to make an agreed random value if the verification succeeds."
"in this section, we describe our scheme, which is energyefficient and resilient against the increase of compromised sensors. to begin with, we make the following assumptions."
"in this paper, we have presented a key renewal scheme for wireless sensor networks, and it is based on periodic and secure ch election. for a secure ch election, our scheme employs two techniques: estimation of received signal strength and ordered transmission. using these two techniques, our scheme prevents the compromised sensors from being elected as chs. the simulation results show that our scheme remarkably enhances the integrity of the sensed data in the presence of compromised nodes as compared to other schemes. other simulation results show that our scheme nevertheless consumes less energy for key renewals and provides a longer network lifetime than other schemes."
"when there is a mixture of pure sensors and compromised sensors, we aim to minimize the illegal fabrication of data from the sensors by periodically renewing the ch role nodes. actually, in most of sensor network applications, illegal fabrication of sensed data has a worse influence on the security than that of illegal acquisition. this is because the illegal fabrication of sensed data makes a wrong decision of a user while the illegal acquisition cannot do so. in other words, the aim of our key renewal scheme is to prevent compromised sensors from fabricating a large amount of data, even though they share their illegally obtained keys with each other. we also want to reduce the energy consumed for key renewal process."
"to evaluate the efficiency of the key renewal schemes, we first take energy-efficiency into consideration. this is because sensors deployed in the duty field are all batterypowered devices and there is no way to recharge them in the routine operation of the network. this fact forces all protocols on sensor networks to be energy efficient. next, we investigate how the key renewal schemes influence the network lifetime. figure 8 shows the variation of energy consumption as the number of compromised nodes increases. in chan's scheme, sensors do not engage in a key renewal process after they established communication keys with their neighbors. therefore, sensors consume a constant amount of energy regardless of the increase in the number of compromised nodes."
"even though shell seems to provide the best confidentiality among three schemes, it is not the result of shell's function but the result of a matured ids function. furthermore, the integrity protection of shell deteriorates significantly as the number of compromised nodes increases. direct effectively protects the integrity of data via the secure ch election while it reduces the energy consumption and lengthens the network lifetime. to provide more thorough comparison among the key management schemes, we list various properties of three schemes and compare them qualitatively in table 2 ."
"before ch election, each sensor sets its timer interval to a predefined value. the timer interval is long enough to accommodate the ch election step and the data transmission step. then, sensors should elect a node which plays the role of ch in this round. our ch election scheme relies on an agreed random value like the commitment based scheme and the seed based scheme. so, a ch role node is likely to be changed at every election time. owing to the periodic changes of ch role nodes, the communication keys between a ch and its members are periodically changed. for simplicity, we assume that there are no collisions in the mac layers of sensors during the ch election. this assumption can be actualized using a broadcast order which is predetermined for broadcast of fulfillment values in each sector. initial broadcast order in a sector is settled by the order of ids of sensors. as shown in sect. 3.2, the commitment based scheme and the seed based scheme enable a malicious node to change the ch election result. our scheme prevents the arbitrary changes of ch elections result by forcing all sensors to follow the broadcast order of fulfillments. besides, a malicious node can make some sensors have a different sum of random values by selectively transmitting its fulfillment value. a malicious node can easily implement the selective transmission by lowering the power level of its fulfillment transmission. our scheme employs the received signal strength to defeat this selective transmission attack."
"to extend the lifetime of network, each member sensor sends its data in an allowed time slot and remains in a sleep state during the other time slots. to this end, each ch broadcasts a time division multiple access (tdma) schedule for its members after settling its role, and sensors send their data to their ch directly."
"the commitments broadcasted by sensors can contribute to the generation of the sum of random values only if corresponding fulfillment values are received from the sensors. so, each sensor broadcasts the random number which was used for commitment generation to other sensors. like the commitment broadcast, the transmission power level should be strong enough to reach the nodes which are at most four hops away. besides, each sensor should follow the predetermined order for the broadcast of fulfillment values. if a sensor violates the order, the sensor is identified as a suspicious node and recorded in the suspicious node list. besides, receivers discard the message from a suspicious node. if a fulfillment value is received in the accurate order, receiving sensors compare it with the corresponding commitment to check the equality. if they are equal, the receivers store the sender into normal node list. if a suspicious node violates the broadcast order again, the legitimate nodes exclude it from the member list and the suspicious node list. therefore, a compromised node can arbitrarily change the ch election result only once by suppressing its fulfillment transmission. if it tries the same misbehavior or delays its transmission, other legitimate sensors eliminate it from the member list as well as discard the fulfillment value. this technique occasionally makes an innocent victim because the suspension of fulfillment value transmission may be caused by a message loss. however, in the aspect of secure ch election, this technique is a necessary evil. to help understanding the scheme, we introduce an example in fig. 4 . in fig. 4, solid circles represent the legitimate nodes and dashed circles represent the compromised nodes (that is, c and f ). besides, the digits above the circles depict the fulfillment values of the nodes. in the first election, c delays its fulfillment transmission until all other sensors broadcast their fulfillment values. because this action violates the broadcast order, other legitimate nodes record c into suspicious node list and ignores its fulfillment value. nota that c is forced to broadcast its fulfillment value at the very first. in the second election, because c violates the order again, other legitimate nodes exclude c from its member list. as a result, c loses its right to join the ch elections in the sector. in the next two elections, f also loses its right to join the ch elections in the sector due to its consecutive violation against the broadcast order."
"therefore, a key management scheme which deals with the compromise of chs is required. to cope with the compromise of chs, a key management scheme should include a secure ch election mechanism which rotates the ch role nodes among non-compromised nodes."
"our scheme employs a random value based scheme to elect a ch in a sector. the commitment based scheme and the seed based scheme described in sect. 2 belong to the random value based scheme. the reason of using a random value based scheme is that it is more secure than the weight based schemes [cit] . the network in this paper comprises a sink, some chs, and many member sensors under the chs. all sensors are quasi-stationary nodes and can play a role of ch. that is, the ch role sensors are changed with the lapse of time. member sensors belong to only one ch and send their data to the ch nodes. the chs aggregate data from member sensors and send the aggregated data to the sink using a fixed spreading code and carrier sense multiple access (csma). that is, a ch first sense the channel to see whether there is a transmission from a different ch or not. if the channel is occupied by any other transmission, it should wait to transmit its data. otherwise, it sends its data to the sink using the sink spreading code. note that each cluster employs a different spreading code for intra-cluster communication to minimize the inter-cluster interference."
"that is, there are many nodes on the route from a sensor to the sink. even if only one intermediate node is compromised, data from the sensor to the sink is fabricated by the compromised node. the fabrication rate increases with the increase in the number of compromised nodes. shell greatly decreases the fabrication rate compared to chan's scheme. however, as the number of compromised nodes increases, the fabrication rate increases accordingly. this is because shell evicts the compromised sensors only when a compromised ch is detected. when a member sensor is compromised, shell just renews the keys. shell first renews the administrative keys known to the compromised nodes and then renews the group key using the renewed administrative keys. this eviction scheme is functionally useless, if all administrative keys employed in the cluster are exposed to attackers. because the number of administrative keys employed for key renewals is small (10 keys), all of them are likely to be exposed to attackers under the increase of compromised nodes. in this case, attackers can keep fabricating the data of the compromised nodes. in direct, the number of clusters (chs) is larger than shell and the compromised nodes are not evicted by an ids. because attackers randomly compromise the nodes, more chs tend to be compromised. so, the fabrication rate in direct highly depends on the number of compromised chs. direct reduces the probability that a compromised sensor is elected as a ch and rotates the ch role nodes. this makes the slope of the fabrication rate much gentler even if the number of compromised nodes increases."
"after electing a ch among the normal nodes, each legitimate sensor adjusts the broadcast order of fulfillment values. first, each legitimate node moves the suspicious nodes to the front of the broadcast order and moves the normal nodes to the tail of the broadcast order. in other words, the broadcast order of the next round is generated by concatenating the suspicious node list and the normal node list. the elected chs generate a tdma schedule and broadcast it. all members compute their transmission time and rest time in line with this schedule. they transmit their sensed data to the ch in their allowed time slots, and the ch transmits the aggregated data to the sink. this procedure is repeated until the timer which was set at the beginning of secure ch election step expires. if the timer expires, each sensor restarts the secure ch election."
"we assume that sensors are deployed by an aircraft without human intervention. to group all sensors into some clusters, we can employ a ch election scheme which is based on a weight value such as hccp or lidcp or wca. however, a weight based scheme frequently makes a ripple effect where a ch election result in a region affects that of other regions. that is, in a weight based scheme, all nodes invoke the ch election process at the same time and some nodes make a subordinate relationship between them. therefore, a weight based scheme necessarily requires the global synchronization among all nodes. this global synchronization requires a high communication and computation overhead. if the ch election is repeated periodically, this overhead increases heavily. to avoid this redundant overhead, we introduce the concept of sector, which plays a role of barrier between neighboring ch elections. initially, because there are no sectors in the network, a ch election scheme should be invoked to make them. we assume that a weight based scheme is employed once for the sector formation. this sector formation makes one leader in each sector and the leader is referred as sector manager hereafter. the sector manager helps a member establish a pairwise key with another member when the two members have no common keys. section 4.1 provides the detailed information about the sector formation. generally, sensors in a sector elect only one ch. in this case, a sector means a cluster. however, a sector sometimes has multiple chs due to malicious behavior of attackers. in this case, a sector includes multiple clusters."
"in sensor networks, sensors are deployed in an unprotected environment and their data is delivered via wireless communication, so attackers can acquire data by eavesdropping and even fabricate data delivered from sensors to the sink. to defeat these threats, data from the sensors should be protected by means of cryptographic keys. this protection requires a key management scheme that generates cryptographic keys between the sensors and distributes them to the sensors securely. in other words, key management in sensor networks is an essential prerequisite for wide deployment of sensor networks. however, the constrained resources of sensors such as low bandwidth and computing power, small memory space, and limited battery power make key management a complicated problem [cit] ."
"first, each sensor generates its random number and encrypts it with pairwise keys shared with other sensors in its sector to make commitments. the commitments are generated as many as the number of other sensors. each sensor makes a list of the commitments in the order of ids and broadcasts the list. after initial sector formation, distance between any two sensors in a sector is two hops and it is extended to at most four hops after the join of single sectors. therefore, each sensor broadcasts the list with the power with which a message can reach four hop distance nodes. sensors receiving the list first check whether the sender is a member in their sectors or not. if the sender is not the member, the receivers discard the message. otherwise, the receivers pick up its commitment from the list and decrypt it to store with the sender's id."
"above schemes have reactive renewal mechanisms in the key management. a ch is responsible for detecting a compromised node in its cluster and renews the administrative keys of the compromised node. then, the ch renews the group key of the cluster using the renewed administrative keys. so, the reactive renewal mechanisms highly depend on a matured ids (intrusion detection system). however, it is unrealistic that sensor networks have such a matured ids. besides, only one key that is referred as group key is employed for communication between a ch and its members so that one compromised node exposes the group key. even worse, the ch role nodes are not changed with the lapse of time. consequently, the chs become targets of compromise attack and the increase of compromised chs significantly impairs the security of network."
"the sink has a large amount of available resources and it is located in a sufficiently safe position to defeat various attacks. in contrast, chs and member sensors have very limited resources and are located in positions where they can be compromised at anytime. figure 1 shows the network model of the clustered sensor networks."
proposed lock (localized combinatorial keying) which is an extension of shell [cit] . lock applied the ebs to key renewal between chs and the sink as well as to key renewal between sensors and the ch.
"a lot of key management schemes have been proposed so far, and they are classified into two categories. the schemes in the first category have no key renewal mechanisms [cit] . in these schemes, sensors typically have some administrative keys pre-distributed from the sink and they establish communication keys with neighbor sensors using these keys. the administrative keys are never renewed, and they are employed until the network is extinguished. moreover, these administrative keys are taken by other sensors with a predefined probability, so these schemes severely degrade the security of the network as the number of compromised sensors increases. the schemes in the other category have reactive key renewal mechanisms. that is, they renew the keys revealed to compromised sensors whenever a compromised sensor (s) is detected [cit] . in most of these schemes, the physical network is divided into some logical groups, which are called clusters, to distribute the key management load to each logical group. each logical group elects its own leader which is called ch and the key management duty is delegated from the sink to the chs. if a ch is compromised, member sensors under the compromised ch are redistributed to non-compromised chs. the noncompromised chs distribute administrative keys used for key renewal to their new members and renew some of them if they are exposed to attackers. then the chs renew the group key used for intra-cluster communication using the renewed administrative keys. if only some members are compromised in a cluster, the ch renews the administrative keys of the compromised members and then renews the group key using the renewed administrative keys. because these schemes employ one group key per cluster for communication, only one compromised sensor in a cluster can expose the group key. a more serious problem is that the chs are likely to be targets of compromise attack because the ch role nodes are not changed."
"in clustered sensor networks, member nodes as well as chs are likely to be compromised since they are deployed in an unattended environment. if a member node is compromised by an attacker, its data is revealed to the attacker, and the attacker can send falsified data to the sink via the compromised node. the compromise of chs has a more serious impact on the network than that of member nodes because chs aggregate data from member nodes and deliver the aggregated data to the sink. an example showing the threat well is a military surveillance application. in this application, nodes monitor the movement of enemy troops and then notify headquarters of their invasion. however, compromised nodes can send forged information to the sink indicating that there is no suspicious activity. especially, if all chs are compromised, the control of the whole network is given to the enemies and their invasion is never detected."
"as shown in fig. 9, direct significantly lengthens the network lifetime compared to shell in all timer intervals. there are two reasons why direct extends the network lifetime significantly. first, direct evicts only a small number of compromised nodes every ch election time. second, direct consumes much less energy for key renewals than shell as shown in fig. 8 ."
"if a sensor receives fulfillment values (that is, random numbers) from all other sensors in the sector, it generates a sum of the random numbers and divide the sum by the number of normal nodes to get the remainder. note that all sensors keep the list of normal nodes which follow the broadcast order of fulfillments values. the remainder means the position of the ch node in the normal node list."
"at network boot-up time, each sensor exchanges its id with neighbors. then each sensor exchanges the neighbor list with its neighbors. through these exchanges, each sensor recognizes other sensors which are at most two hops away and consequently recognizes their assigned keys."
"above schemes do not have any renewal mechanism in the key management. therefore, administrative keys obtained from compromised nodes can be used for disclosing communication keys between the compromised nodes and their neighbors, and the administrative keys also exist inside many other nodes by a predetermined probability. as a result, the security of above schemes is deteriorated as the number of compromised nodes increases."
"because a ch is randomly elected in a sector and the ch and its members communicate directly, all sensors in a sector should share a pairwise key with each other. if any two sensors which are at most two hops away share at least one common administrative key, they can establish a pairwise key using the common key. however, some sensors do not share common administrative keys with each other. these sensors are considered as insecure sensors to each other. however, a sensor can indirectly establish a pairwise key with an insecure sensor using a helper node (that is, a node which shares common administrative keys with two sen- sors). however, if all sensors in a sector perform this indirect pairwise key establishment individually, it causes a lot of communication and computation overhead. to reduce this overhead, we use the sector manager. the reason why we use the sector manager is that it is located in the center position of the sector. the position enables any member in the sector to access the sector manager with minimum energy consumption. first, the sector manager establishes pairwise keys with its insecure members using its helper nodes. note that each sector manager has many potential helper nodes through exchanges of the id and the neighbor list. if all helper nodes do not share a common key with an insecure sensor, the sector manager establishes pairwise keys with the insecure sensor via the help of the sink. this is because the sink is the dealer of all administrative keys assigned to sensors in the network. then the sector manager broadcasts the list of members so that each member establishes pairwise keys with its insecure members via the help of its sector manager. that is, if a member requests the sector manager to distribute a key to it and its insecure sensor, the sector manager generates a pairwise key and distributes it to the two nodes securely."
"the rest of this paper is organized as follows. section 2 gives a brief overview of the existing key renewal schemes and ch election schemes. sections 3 describes the network and threat model that are assumed in this paper, and sect. 4 describes the direct in detail. we provide the simulation results and qualitative comparison in sect. 5 and deal with the synchronization issue in sect. 6. finally, we conclude this paper in sect. 7."
our future research focuses on the design of a pairwise key establishment scheme between different generation sensors for enhancing the scalability of our scheme. another interesting research item is to design a well-functioning ids for clustered sensor networks.
"in the seed based scheme, each node generates its seed value and broadcasts it. this seed value is the initial random value for generation of sum of random values. every ch election round, each node broadcasts its availability message. this availability message represents an intention for joining the ch election and it is similar to the fulfillment value of the commitment scheme. sensors receiving the availability message keep the list of the senders. then, all sensors make a sum of random values using the seed values of the senders and the number of ch election round. merkle's puzzle based scheme causes a lot of overhead due to the pairwise key establishment, generation of sum of encrypted random values, and the pairwise key distribution. the commitment based scheme and the seed based scheme are vulnerable to transmission suppression and selective transmission of attackers. the transmission suppression causes arbitrary changes of ch election result. besides, selective transmission causes the partition of clusters by separating one agreement of ch election into two or more agreements."
"the aim of attackers is twofold. first, they aim to illegally obtain data that is going from the sensors to the sink. more importantly, they aim to cause a user to make a wrong decision by fabricating a large amount of data that is sent to the sink. to achieve these aims, attackers need to compromise sensors because all data and keys of the compromised sensors are revealed to the attackers. in a clustered sensor network, because chs are the data collection points, smart attackers may target the chs rather than member sensors for compromise. this is because they can get the control of the whole network by compromising a small number of chs."
"here, e two ray amp is the energy consumed by the amplifier and b is the bandwidth of the channel. if d r is smaller than a predetermined threshold, the receivers discard the received fulfillment value. to facilitate understanding the technique, we introduce another illustrative example in fig. 5 . in fig. 5, the gray circle depicts the threshold of transmission range and a dashed big circle represents the transmission range of a compromised node. as shown in fig. 5, the compromised nodes a and c transmit their fulfillment values with a low transmission range. in this case, the commitment based scheme and the seed based scheme divides the sum of random values into three kinds as shown in the left bottom of fig. 5 . however, in our scheme, the legitimate nodes discard the fulfillment values of a and c because their transmission range is shorter than the threshold. as a result, all legitimate nodes have the equal sum of random values in the sector as shown in the right bottom of fig. 5 ."
"if a receiving node can know the transmission power of a sender, it can estimate the maximum reachable distance by the power(d r ) by eq. (2)."
"in a clustered sensor network, a ch election scheme is essential to transform a physical network into a cluster structure. representative ch election schemes are lidcp (lowest id clustering protocol) [cit] and hccp (highest connectivity clustering protocol) [cit] . lidcp prefers a lowest id node among neighbors for ch elections while hccp prefers a highest degree node among neighbors for ch elections. wca (weighted clustering algorithm) [cit] considers various parameters for ch election, such as degree, transmission power, mobility, and residual energy. these parameters are assigned different weights, in line with the relative importance of the parameter in the network application. a final weight is generated by multiplying each parameter by the corresponding weight and summing them. the prominent problem of above weight based schemes is that a malicious node can broadcast a forged final weight as if it has a highest priority among neighbors. in that case, it can become a ch."
"on the other hand, main work for key refreshment in direct is the secure ch election because it is periodically invoked. in the secure ch election procedure, all sensors consume their energy for two transmissions and two receptions. all other work such as sector formation and pairwise key establishments consume only a small amount of energy in the whole energy consumption perspective. this is because they are only invoked just one time at network bootup time. therefore, our scheme consumes less energy for key renewals as shown in fig. 8 . figure 9 shows how the key renewal schemes affect the network lifetime as the number of compromised nodes increases. chan's scheme renews no keys and evicts no nodes from the network, so there is little impact on the network lifetime as the number of compromised nodes increases. however, two other schemes evict the compromised nodes. shell evicts all compromised nodes when a compromised ch is detected and the number of evicted nodes is large. in direct, one compromised node is evicted only when it violates the broadcast order of fulfillment values more than one time. therefore, the rate of evicted nodes is much smaller than that of shell."
"on the other hand, shell evicts the compromised nodes and the eviction makes the variation in the energy consumption of sensors. in shell, sensor nodes consume more amounts of energy when the compromised nodes increase. this is because sensor nodes more frequently join the key renewal process. during the key renewal process, if a compromised ch is found, the compromised ch is expelled by the sink. then, the orphan sensors are redistributed to the pure chs, and the pure chs distribute their administrative keys to the orphan sensors before key renewal. the preparation phase greatly increases the energy consumption of sensors. however, further increase of compromised chs rather reduces the energy consumption of sensors. this is because the compromised nodes are expelled from the network whenever a compromised ch is found by an ids. the expelled nodes do not consume energy any more for the key renewals."
"in shell, whenever the sink detects the compromised chs, it evicts the compromised nodes as well as the compromised chs from the network through cluster reorganization. therefore, an increase in the number of compromised chs causes the number of evicted nodes to increase as well. nevertheless, the network lifetime lengthens as shown in fig. 9 . this is because the network lifetime is dominated not by the number of evicted nodes but by the number of remaining pure chs. that is, if the number of compromised chs is small (for example, one), then the sensors are redistributed to the remaining pure four chs. in this case, all sensors are properly distributed to four remaining chs so that the number of sensors served by a ch is small. this distribution makes the transmission schedule in a cluster short, and sensors frequently transmit their data to the ch. that is, because sensors consume large amounts of energy, the number of active nodes rapidly decreases. on the other hand, if only two pure chs (that is, three compromised chs) exists in a network, then the chs should take charge of many sensors. this makes the transmission schedule in the clusters very long and the transmission frequency of the sensors decreases greatly. this slows the energy consumption of the sensors and the number of active nodes decreases very slowly."
"except for administrative key distribution, direct consists of four steps; sector formation, pairwise key establishments within sectors, secure ch election, and transmission of sensed data. the former two steps are performed only once when the network boots. on the other hand, the other two steps are repeated periodically as long as the network is alive. figure 2 shows the steps of direct. next subsections describe all steps of direct respectively in detail."
"even though ch role nodes are changed periodically, the compromised nodes must try to become chs by participating in the periodic ch elections. they may change the ch election result arbitrarily. if they keep changing the ch election result and there are many compromised nodes in the sector, a compromised node is likely to be elected as a ch. also, the compromised nodes may produce several clusters in a sector. as the number of clusters in a sector increases, the number of members in a cluster decreases. therefore, the transmission schedule of a cluster is shortened and the members in the cluster should send their data more frequently. consequently, sensors deplete their energy in a short time. to materialize these attacks, attackers employ the following tricks. in a random value based scheme, all nodes cannot predict which node will become a ch except for one node which broadcasts its fulfillment value lastly. therefore, a compromised node may delay it fulfillment value broadcast to recognize which node will become a ch. then, it must suppress its fulfillment broadcast if a pure node is expected to become a ch. this action changes the ch election result. if a compromised node keeps changing the ch election results, the compromised nodes in the sector come to have many chances to become a ch. especially, as the number of compromised nodes increases, they have more chances to achieve the goal. if a compromised node broadcasts its fulfillment value with a low transmission power, some nodes in the sector cannot receive the value. so, they have a different set of random values from other nodes which receive it. as a result, they make a different sum of the random values and elect a dif-ferent node as their ch. in this case, one sector is separated into two clusters."
"to facilitate the understanding of the pairwise key establishment, we introduce an illustrative example like fig. 3 . as shown in fig. 3, the sector manager m has already established pairwise keys with members c, e, and g after exchanges of id and neighbor list. however, the sector manager cannot establish pairwise keys with the sensors b and f because it shares no common administrative keys with them. in those cases, it establishes pairwise keys with b and f using helper nodes e and c respectively. that is, the helper nodes generates a pairwise key and distribute the key which is encrypted with common administrative keys to two nodes. in case of the sensor a, the sector manager has no common administrative keys and no helper nodes. so, the sector manager establishes a pairwise key with the sensor a via the help of sink."
"to evaluate our approach and its flexibility, we apply our approach to three different scenarios. first, we show that our approach can be used to stay aware of relevant changes to the source code by applying it to feeds on source code changes similar to most existent approaches. second, we apply our approach to web feeds on changes to apis and evaluate the false positive and false negative rate. finally, we use our approach to find feeds that talk about bugs relevant to a developer's work. in the last scenario, we determine relevancy in terms of bugs that were later on changed again by the developer and discuss the false positive and false negative rate."
"in previous work [cit], we have shown how one can compose different kinds of information, such as source code, bugs, teams and change sets, to answer a developer's question. in this work, we extend the underlying model of our previous research with the concept of feeds to find relevant information a developer should stay aware of. figure 1 (a) shows a typical non-contextual list-based feed reader. a developer must scroll and read each feed item to understand what might be relevant. our approach allows a developer to compose the feeds with other information from a developer's workspace, providing a context for interpreting the feeds and supporting various means of ranking and filtering the feeds. figure 1 (b) displays our fragment explorer view in which a developer has composed the source code of his workspace with feeds, in this case web feeds and feeds on change set events. the kinds of information being composed, i.e., source code, web feeds and change set feeds, and the order of composition is represented by the icons in the top right corner of the view. the developer can also choose the time frame for what he considers relevant information by using the slider in the awareness picker on the bottom of the view. now, as soon as, for example, a new change set event comes in from a change set feed, it is automatically put in context of the source code of the developer's workspace and in case the change set affects the source code, it is presented in the view. an example of this is shown in figure 1(b) in which mike delivered a change set that affects class countselectionaction."
"some existing approaches ease this problem for certain kinds of feeds. palantir [cit], for example, makes developers permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. aware if any of the source code in a developer's workspace is currently changed by someone else. these approaches provide some fixed context for the new information; in the case of palantir, the context is the source code loaded in the developer's workspace. however, they provide little to no flexibility for the developer to choose the context."
"our approach is based on an information fragment model that supports the composition of different kinds of information and its presentation. previously, information fragments were defined as a static set of items of information, such as bugs or classes and methods, to answer a specific question. feeds are a dynamic concept and can not be captured by our original static information fragments. as a feed is a stream of information, it constantly changes with new incoming information items. furthermore, a feed has a certain lifetime aspect-information items that are too old are not relevant anymore and discarded. to incorporate feeds into our model, we expand it with the notion of a dynamic information fragment, which is continuously evaluated."
"in practice, a developer forms a dynamic information fragment by selecting feeds of interest, such as feeds on change sets or web feeds. he can then compose these information fragments with other information fragments, such as a fragment comprising the source code of certain packages. figure 1(b) shows the result of such a composition. previously, the composition was done once and then displayed in our view. with a dynamic information fragment, the composition is done continuously so that every time a feed changes, the view is updated accordingly. to reflect the lifetime aspect of feeds, the view also contains a slider, called the awareness picker, to pick the relative start time. in our example in figure 1(b), the start time is one day ago. every item older than that will be discarded. for now, the awareness picker applies to all dynamic fragments in the view, but it might be necessary to specify it separately. finally, our new approach also provides the option to stack information fragments as can be seen with the two feeds fragments in figure 1(b) . by stacking fragments, they are presented on the same hierarchical level in the tree viewer."
"a common solution to stay aware of relevant information is by subscribing to feeds. as a developer's feed subscriptions accumulate over time, he gets flooded with information, a lot of which is not relevant to his work. to find out if a feed is relevant to a developer's work, the developer has to read through the feed and find the relation to his work, often by following keywords or links in the feed."
"the result of our work is a fluid integration of dynamic and static development information in a developer's integrated development environment. this improves how different kinds of information can be integrated. in particular, it allows a developer to stay aware of relevant information from feeds not limited to a particular kind of information and have them presented in his work context."
"\"awareness is an understanding of the activities of others, which provides a context for your own activity.\" [cit] . as described in section 1, several approaches (e.g., palantir [cit] and an awareness environment built at ibm [cit] ) provide awareness for limited kinds of information in limited contexts."
"when building a software system, a developer has to stay aware of a lot of information, such as changes to the project's code, the work of his coworkers, new bugs or changes to the api of used libraries. missing relevant information may result in extra work and time that has to be spent. for example, damian and colleagues report on a case, in which a developer missed an important email due to the information overload he experienced, resulting in a broken build and productivity loss for the whole team [cit] ."
"other approaches have tried to recommend information that is relevant to the current piece of information a developer is working with. for instance, hipikat [cit] establishes links between artifacts based on a fixed schema and then uses these links to infer which artifacts might be relevant based on a developer's query. deep intellisense [cit] in other previous work [cit], we have introduced a degreeof-knowledge model to capture a developer's familiarity with source code and applied the model to find bugs that could be relevant to a developer. our previous work does not attempt to provide awareness to a developer."
"finally advantage is about its architecture references. the use of the methodology proposes the revision of several reference its architectures, not just one, which allows to use the best features of each one."
"where the b i (x )'s are some monomials in variables x . as a heuristic, the user can simply determine an upper bound on the degree of e[c], and then include all monomials whose degrees are no greater than the bound in the sketch. large values of the bound enable verification of more complex safety conditions, but impose greater demands on the constraint solver; small values capture coarser safety properties, but are easier to solve."
"a mixture of 1,3-dibromo-1,1,3,3-tetra-tert-butyl-2,2-diphenyltrisilane (5.00 g, 7.98 mmol), dichlorodiisopropylsilane (2.24 g, 12.1 mmol) and lithium (0.28 g, 40 mmol) in thf (50 ml) was stirred at room temperature for 1 day. the solvent was removed under reduced pressure. the residue was dissolved in hexane and passed through a short column of silica gel. after the silica gel was washed with hexane, the eluent was changed to diethyl ether. the diethyl ether eluate was evaporated. recrystallization of the residue from methanol-thf (ca 1:1) gave 1 (0.956 g, 21%) as colorless crystals."
"in the rvl task, in addition, regression analysis was performed using absolute rpe as a predictor of frn amplitude and oscillatory activity. we then determined whether the value of the slope was different overall from 0 for the group for rpe measure using a one-sample t test. a significant difference from 0 would suggest a relationship between the size of the prediction error and the size of the frn amplitude or tf activity. separate analyses for positive and negative prediction errors were also performed."
"if verification fails in line 11, our approach simply reduces the initial state space, hoping that a safe policy program is easier to synthesize if only a subset of initial states are considered. the algorithm in line 12 gradually shrinks the radius r * of the initial state space around the sampled initial state s 0 . the synthesis algorithm in the next iteration synthesizes and verifies a candidate using the reduced initial state space. the main idea is that if the initial state space is shrunk to a restricted area around s 0 but a safety policy program still cannot be found, it is quite possible that either s 0 points to an unsafe initial state of the neural oracle or the sketch is not sufficiently expressive."
"single crystals were obtained from methanol-thf (ca 1:1) by slow evaporation. 5, 24.7, 25.4, 32.8, 127.1, 127.4, 138.6, 142.1. 29 si nmr (119 mhz, cdcl 3 ): δ -6.2, 14.0, 20.4. ir (kbr): 3050, 2950, 2920, 2850, 1470, 1420, 1390, 1360, 810, 730,"
"the detailed architectures of the services were based on arc-it service architecture diagrams, adjusting them according to the general its architecture designed for popayán. an example of the detailed its architecture for one of the services is presented in figure 12 ."
"as second advantage, if the intermediate city for which it is wanted to design an its architecture, has characteristics that differ considerably from the city that of popayán, it is possible use the proposed methodology to develop another version of its architecture that will be more adjusted to the context."
"first, the most likely position of a potential break point is determined. then, the statistical significance of the magnitude of this most likely break point is computed to decide whether there really is a break point. if so, the amount of shift of the pattern is determined and the pattern is categorised as us or ds."
"the buffer zone was converted into a digital number. the image in august that would be used for classification also converted into a digital number. in one pixel there were three digital numbers. a digital number was obtained from band 7, band 4, and band 2 because of composite process. digital values derived from the buffer zone were matched with the digital values of the image in august. if those three digital values of the buffer zone equal to digital number of image in august, then the pixel has a buffer zone as the matched class. if in images acquired in august there are pixels that does not have a class or not equal with the pixels of the existing buffer zone, then the pixels are classified as a non peatland."
"to facilitate the design of its architecture for intermediate cities, we initially developed a methodology that included judicious review of relevant its architectures, at international and national level, methodologies of design of regional its architectures, and the particular context of an intermediate city in a developing country. we compiled best practices and relevant aspects adaptable to context, of each of the mentioned sources, and we established a logical structure in stages that could be logically followed. once methodology was developed, it was applied in design of its architecture of the city of popayán."
"we have applied our framework on a number of challenging control-and cyberphysical-system benchmarks. we consider the utility of our approach for verifying the safety of trained neural network controllers. we use the deep policy gradient algorithm [cit] for neural network training, the z3 smt solver [cit] to check convergence of the cegis loop, and the mosek constraint solver [cit] to generate inductive invariants of synthesized programs from a sketch. all of our benchmarks are verified using the program sketch defined in equation (4) and the invariant sketch defined in equation (7). we report simulation results on our benchmarks over 1000 runs (each run consists of 5000 simulation steps). each simulation time step is fixed 0.01 second. our experiments were conducted on a standard desktop machine consisting of intel(r) core(tm) i7-8700 cpu cores and 64gb memory."
"in figure 1, the first stage of methodology that we propose are presented. at this stage, a revision of the next documents must be done: updated versions of representative architectures at international level, standards (proposal of iso), national architecture, and regional (or city) architectures designed. service areas, that each reference architectures considers, is one of the comparison factors in the review. use of a matrix facilitates identification of services included for each architecture, the matrix would have as rows the service areas to be considered and as columns the its architectures. the selected service areas to be taken into account will be the sum of the ones that each architecture has, without repeating them. the number of its architectures in the matrix will depend on each case, although it is recommended to take into account at least the main international architectures, iso standard and national architecture."
"to determine the adequate its reference architecture some relevant aspects of city context were considered too. repeated disobedience of the traffic rules by pedestrians and drivers in the city and country in general, limited road infrastructure of the city (affected by maintenance works of public services), and high accident rates previously presented in this paper were considered in decision, selecting an its architecture that focuses on these aspects.. it was determined that most suitable its architecture for the city was arc-it. to this architecture was added service area of frame architecture called \"compliance with the law\", considered relevant by the aforementioned on the disobedience of traffic regulations. additionally, \"performance management\" services area proposed in the iso architecture, also included as a complement, because it provides a tool for continuous improvement of designed its."
"to motivate the problem and to provide an overview of our approach, consider the definition of a learning-enabled controller that operates an inverted pendulum. while the specification of this system is simple, it is nonetheless representative of a number of practical control systems, such as segway transporters and autonomous drones, that have thus far proven difficult to verify, but for which high assurance is very desirable."
"where t v (x) is a lower bound on the grade of membership of u derived from the evidence for x, and f v (x) is a lower bound on negation of x derived from the evidence against x. an lu and wilfred ng [cit] gave a detailed discussion of using the proper theory for imprecise or vague data."
"another important aspect related to its architecture for a colombian city is the difference related to mobility between large and intermediate cities in the country. these differences include: number of inhabitants, area occupied by the city, movements of the population, economic activities, means of public transport, road infrastructure and transportation budget. low annual budget in transport, limitations in road infrastructure, and characteristics of the means of public transport of intermediate cities do not allow a high investment in integral technological solutions to solve problems of road safety and traffic."
"in figure 2, the context analysis stage is presented. outputs from previous stage are used as inputs to this stage. context of the city, other entry in this stage, should be analyzed in a broader sense."
"the context analysis called pestle (19) was selected to identify and weight the particular political, economic, social, technological, legal and environmental conditions that are relevant in the design of the its architecture of the city. as a result of the application of this tool, the following information relevant to the context of the city was obtained:"
"the reward that a control policy receives on performing an action a in a state s is given by the reward function r (s, a). reinforcement learning. the goal of reinforcement learning is to maximize the reward that can be collected by a neural control policy in a given environment context c. such problems can be abstractly formulated as"
"a safety proof of a synthesized deterministic program of a neural network does not automatically lift to a safety argument of the neural network from which it was derived since the network may exhibit behaviors not captured by the simpler deterministic program. to bridge this divide, we recover soundness at runtime by monitoring system behaviors of a neural network in its environment context by using the synthesized policy program and its inductive invariant as a shield. the pseudo-code for using a shield is given in algorithm 3. in line 1 of algorithm 3, for a current state s, we use the state transition system of our environment context model to predict the next state s ′ . if s ′ is not within φ, we are unsure whether entering s ′ would inevitably make the system unsafe as we lack a proof that the neural oracle is safe. however, we do have a guarantee that if we follow the synthesized program p, the system would stay within the safety boundary defined by φ that algorithm 2 has formally proved. we do so in line 3 of algorithm 3, using p and φ as shields, intervening only if necessary so as to restrict the shield from unnecessarily intervening the neural policy. 4 algorithm 3:"
"after reviewing the aforementioned documentation, we determined to develop a methodology to achieve objective (architecture design), considering to obtain greater clarity, organization and ease of later use. additionally, we took into account that a methodology could serve as a model to formulate its architectures of cities with such context in the country or countries similar to colombia."
"as possible future work it is recommended to carry out a technical process for the selection of the reference its architecture in the first step of the methodology, in which a statistical analysis can be used to determine what is the difference (gap) between an architecture and another, measure correlations, and obtain a more precise result in this regard."
"with the identified components, we proceeded to design the views of its architecture of popayán. to present the general views of the architecture, we used diagram types presented in selected reference its architecture, in other words, we adapt diagrams of arc-it for its architecture for popayán, because these have an adequate level of understanding and a high level of use in the international context."
"study area in this research is peatland in kubu subdistrict and pasir limau kapas subdistrict, rokan hilir, riau province. this studied used remote sensing data, peatland map, and hotspots. satellite images used were landsat 7 in rokan hilir district, riau province which was taken from the usgs (united states geological survey). [cit] in riau was used to locate the peatland cover on the satellite image. the map of peatland that is represented in polygons was obtained from wetlands international. [cit] were obtained from modis fire firms / hotspot, nasa / university of maryland. hotspots were used to determine the classes before burned, burned, and after burned."
"each participant performed two experimental tasks; the presentation order was counterbalanced across participants. [cit], which consisted of 637 trials divided into 49 blocks (10-16 trials each). in each trial, two geometric figures were presented on either side of a central fixation point. the participants were instructed to select one of the figures. after a delay of 1000 msec, one of two possible types of feedback was displayed: a green tick (reward, +a0.04) or a red cross (punishment, −a0.04; figure 1 ). on each block, one figure was rewarded in 75% of the trials, whereas the other was rewarded in 25% of the trials. however, at the beginning of each block, the rule was reversed. during the first five trials following the contingency reversal, a figure 1 . rvl (left) and pl (right) tasks used in the study. both tasks consisted of 637 trials divided into 49 blocks varying from 10 to 16 trials each. on each trial, participants had to select between two geometric figures. using trial-and-error feedback, participants had to discover the most advantageous figure. after each block, the rule changed. in the rvl, rule changes were not informed, whereas in the pl, rule changes were indicated by the presentation of two new figures. selection of the previously correct stimulus would result in punishment."
"1. we present a verification toolchain for ensuring that the control policies learned by an rl-based neural network are safe. our notion of safety is defined in terms of a specification of an infinite state transition system that captures, for example, the system dynamics of a cyber-physical controller. 2. we develop a counterexample-guided inductive synthesis framework that treats the neural control policy as an oracle to guide the search for a simpler deterministic program that approximates the behavior of the network but which is more amenable for verification. the synthesis procedure differs from prior efforts [cit] because the search procedure is bounded by safety constraints defined by the specification (i.e., state transition system) as well as a characterization of specific environment conditions defining the application's deployment context. 3. we use a verification procedure that guarantees actions proposed by the synthesized program always lead to a state consistent with an inductive invariant of the original specification and deployed environment context. this invariant defines an inductive property that separates all reachable (safe) and unreachable (unsafe) states expressible in the transition system. 4. we develop a runtime monitoring framework that treats the synthesized program as a safety shield [cit], overriding proposed actions of the network whenever such actions can cause the system to enter into an unsafe region. we present a detailed experimental study over a wide range of cyber-physical control systems that justify the utility of our approach. these results indicate that the cost of ensuring verification is low, typically on the order of a few percent. the remainder of the paper is structured as follows. in the next section, we present a detailed overview of our approach. sec. 3 formalizes the problem and the context. details about the synthesis and verification procedure are given in sec. 4 . a detailed evaluation study is provided in sec. 5 . related work and conclusions are given in secs. 6 and 7, resp."
"the reference architectures and the methodologies of design of regional architectures are consulted online, because information by this means is permanently updated. we also obtained digital copies of iso 14813 (18) through the educational institutions (universities) to which we are attached. queries to the most used literature databases are made using the research resources provided likewise by the universities. finally, consultations on its services to the mobility secretariats are carried out personally (popayán and medellín) and through electronic consultations (bogotá, cali and bucaramanga)."
"several studies have revealed a crucial role of the medial pfc (mpfc) in both action monitoring and updating of action values [cit] . specifically, it has been proposed that the mpfc monitors behavior on the bases of reward prediction errors (rpes; discrepancies between expected and real outcomes), a process described by the principles of reinforcement learning (rl) theory [cit] . in addition, fmri studies have suggested that mpfc also encodes the rate at which new information replaces outdated evidence ( [cit] . these studies have shown that activity in the mpfc, specifically in the acc, increases in situations in which newly acquired information is highly relevant to optimize goal-directed behavior, such in uncertain environments. this information is indexed in rl models by the learning rate parameter (α). this parameter is greater in uncertain or volatile environments than in stable contexts. in addition, variations in acc activity during outcome monitoring predict the α values across participants, reflecting the relationship between mpfc and the updating of new information [cit] ."
"over 1000 simulation runs, we found 60 unsafe cases when running the neural controller alone. importantly, running the neural controller in tandem with the above verified and synthesized program can prevent all unsafe neural decisions. there were only with 65 interventions from the program on the neural network. our results demonstrate that cegis is important to ensure a synthesized program is safe to use. we report more details including synthesis times below."
"also in the national environment, a proposal was presented for the development of its services for colombian cities (14) . authors used the national its architecture of colombia and resorted to methodological support described in american architecture, in order to formulate a model for the development of new services that allow the interoperability, compatibility and expansion of technologies and services in its architecture. authors work was mainly focused on development of a service model, however, it did not present a design methodology for its architecture, as it is intended in our work."
"for example, if a pattern is classified as an ut by the ccpr system, the slope will be the most significant parameter when a nlm is fitted to the data."
"the present results show the first evidence that there is a fast evaluation of the learning rate in the mpfc, which is parallel to the processing of expectancy deviations. three independent results support this claim. first, variations in theta activity across participants were correlated with individual learning rates during the rvl task. second, theta activity was also sensitive to variations in learning rate within participants across the different blocks of the pl task. finally, differences in frontal theta activity between the two tasks were predicted by differences in their learning rate. these results provide evidence that frontal theta oscillatory activity is modulated not only on the basis of an unsigned rpe as previously reported [cit], see also below) but also by the learning rate across and within participants. learning rate is a key feature of the rl model and controls the impact of new information on the next action value estimate. for example, a learning rate value of 1 indicates that only new acquired information is being considered; in contrast, a learning rate value of 0 shows that new information is not being used, that is, there is no learning from new experience. therefore, the learning rate determines the weight of the value of rpe to update old estimates [cit] . previous studies have proposed a relationship between mpfc activity, specifically in acc, and the learning rate ( [cit] . [cit] showed that in high volatile (fast-changing) environments, the learning rate was higher than in stable environments, and those differences in learning rate resulted in differences in acc activity. additionally, and consistent with other studies [cit], individual differences in learning rate were correlated with acc bold signal. this increase of acc activity could figure 9 . scatter plot of differences in theta activity between both tasks for both negative (a) and positive (b) feedback and differences in their learning rate. the solid black line represents the slope of the linear fit."
"among the large number of shield interventions (as reflected in table 1 ), 74% of them are effective and indeed prevent an unsafe neural decision. ineffective interventions in oscillator are due to the fact that, when optimizing equation (5), a large penalty is given to unsafe states, causing the synthesized programmatic policy to weigh safety more than proximity when there exist a large number of unsafe neural decisions."
"however, because of the low temporal resolution of the fmri technique, it is still an open question whether these two processes, monitoring of behavior and updating of action values, are dissociated or not in the mpfc. the goal of this study is to determine whether computation of prediction error and determination of the learning rate are two independent neural processes or engage a common mechanism. to reach this goal, we will take advantage of the high temporal resolution of eeg. previous studies have described two electrophysiological responses during outcome processing, the feedbackrelated negativity (frn) erp [cit] and the mediofrontal theta oscillatory activity [cit] . previous studies using intracranial recording and source modeling have suggested that these two signals are generated in the mpfc [cit] . both signals peak around 250-300 msec after outcome delivery and are modulated by the degree of discrepancy between expected and real outcome [cit] . however, at present there are no studies addressing the modulation of these components by the learning rate."
"comparison algorithm is done to determine whether the sdt is better than traditional decision tree algorithm. according to the comparison of the four algorithms, the best algorithm for classifying peat fire was analyzed. the results were expected to be used to calculate extent of the area in the classes of before burned, burned, and after burned."
"the intermediate cities implement specific solutions to the urgent identified problems, through suppliers that adjust to available budgets, which does not represent a real solution to problem. on the other hand, although large cities also have mobility problems, these cities have better budget conditions that allow them to implement policies and partial solutions in mobility. therefore, colombian intermediate cities in particular urgently need to develop an its architecture that adjusts to special conditions and limitations."
"at the point of discussion of this document, it is specified the specific contribution of the work done with our proposed methodology and the application case."
where α is a predefined learning rate. such an update increment corresponds to an unbiased estimator of the gradient of θ [cit] for equation (5) . the algorithm iteratively updates θ until convergence.
"its are an important option for sustainable mobility (4) . these systems increase transport efficiency, promote safety and improve mobility (5) . conversely, traditional strategies to improve mobility are based on increasing the capacity of road infrastructure and number of vehicles, which are not sustainable from points of view: economic, spatial and environmental."
"the four svms trained in section 4.2 were put through 100 test runs. a test run consisted of applying 100 patterns of each type to the trained svm. the pattern recognition accuracies obtained in the one hundred runs were compared against those achieved with a svm trained using conventionally generated patterns. figure 4 shows the 95% ci for the mean accuracy of each kernel tested. table 6 shows the accuracies achieved by the four designs (no-pgs and the three levels of the proposed pgs) disaggregated by pattern type. anova of three factors with single, double and triple interactions was used to analyse the results. the three factors were: kernel type, four pattern generators and pattern type."
"the algorithm iteratively samples a counterexample initial state s 0 that is currently not covered by covers in line 4. since covers is empty at the beginning, this choice is uniformly random initially; we synthesize a presumably safe policy program in line 9 of algorithm 2 that resembles the neural policy π w considering all possible initial states s 0 of the given environment model c, using algorithm 1."
"the other elements that were identified for its architecture of the city of popayán: functional processes, subsystems, physical objects and logical objects, correspond to elements specifically used in the selected reference architecture (arc-it). the functional processes determined were: provide user services, provide public safety, public transport management, infrastructure management, provide support services, traffic management, archived data management, and performance management. aforementioned regarding subsystems required for its architecture, we established that the five subsystems proposed by arc-it reference architecture are required: support sub-system, sub-system of centers, sub-system of field equipment, sub-system traveler devices and sub-system vehicles. the inclusion of the five subsystems is due to the fact that all are required for the provision of the selected services."
"in recent years, machine learning (ml) algorithms have been implemented for the efficient identification of control chart patterns. the first phase in the operation of a supervised learning ml algorithm consists in training the algorithm by presenting patterns similar to those to be classified afterwards. ideally, observation samples (process data) should be collected from the real process environment and be used as inputs during the training. however, since a large amount of data is required for control chart pattern recognition (ccpr), synthetic samples need to be generated. this is commonly done using monte-carlo simulation [cit] ."
"where d and the parameters β 1 to β 5 are as defined in the appendix section. β 0 and t represent the intercept with the y-axis of the regression model and the random error at time t, respectively."
"its services developed for the colombian cities, in most cases, meet the requirements that were determined and contribute partially to solutions. however, each of services is developed independently, without taking into account whole panorama of mobility and related aspects of the city. so when trying to interoperate or integrate these services it is not easy to do it and sometimes there is no viability."
"as mentioned above, the proposed methodology for determining the type of mean change occurring in the control chart is based on the identification of potential change points and nested nlms. information criterion value. the following is the nlm assuming the existence of a change point:"
"in the proposed pgs, all the pattern parameters are randomly assigned to the patterns, i. e., even parameters such as break point position and cycle period were randomised. also in the literature review, it was found that these two parameters had been ignored by all the authors, despite that fact they are of interest in root cause analysis."
"the comparison of the four algorithms shows that the algorithm c5.0 had the best accuracy in multi-split criteria and algorithms sdt had the best accuracy in binary split criteria. although the c5.0 algorithm has 3.4% better accuracy than the algorithms sdt, but it is not efficient. the efficiency of an algorithm is performed in term of the speed, scalability, and interpretation [cit] . the speed of an algorithm was observed when the model is used to classify a new data. the rules are generated from the decision trees. the number of rules generated from c5.0 algorithm is greater than sdt algorithm with 595 rules and sdt algorithm only have 11 rules. the implementation of classification new data using c5.0 algorithm, may take longer time process. sdt algorithm will require shorter time because the number of rules generated is less than c5.0 algorithm. but, the accuracy resulted from sdt algorithm is smaller than those of c5.0 algorithm. both of these algorithms satisfy the criteria of scalability because they were able to build a model that had a fairly good accuracy with a large number of data. c5.0 algorithm was more difficult to interpret, because it has a complex rules and trees. it differs from the sdt algorithm that was easy to understand because of the simpler rules and trees. the time complexity of the c4.5 algorithm and c5.0 algorithm is о(mn 2 ), where m is the size of datasets and n is the number of attributes [cit] . the time complexity of the algorithm cart and sdt which applied the concept of a binary tree is о(n log n) [cit], where n is a number of attributes. cart and sdt algorithm had simpler complexity than c4.5 and c5.0 algorithm."
patterns are generated with only one possible change point in the time window examined; e.g. only one shift pattern can occur in the time window examined.
"in figure 4, the final stage of methodology is presented. outputs of stage 3 are presented in this stage as inputs. a design tool should be used to in addition to views of the general its architecture, its architectures detailed by each of services must be implemented. the detailed architecture of each services employs a subset of that components. an integral flow chart of the methodology (with four stages, inputs, and outputs), is presented in figure 5 ."
"possible misclassification due to the effect of noise during pattern generation was ignored in all the reviewed papers; thus, the potential increase in the probability of type 1 and type 2 errors was neglected."
"with respect to national reality, mayors' offices and mobility secretariats of main cities of country (bogotá (11), medellín (12), cali and bucaramanga) provides information of services provided, but they do not provide information on its architecture taken as reference.. this situation may indicate that, rulers of cities have not considered any its architecture, despite the fact that country has its initiative at a national level, called \"national its architecture\" (13) . [cit] of american architecture, which unfortunately no updates have been made since that date."
"using a suitable methodology for the formulation of an its architecture, regional or local, is a key aspect that guarantees that the result obtained considers good practices recommended by institutions and / or countries with proven experience and results in the area of intelligent mobility."
the emerging growth of data mining raises the large range of complex applications [cit] . it leads the broad study of data mining frequent patterns. mining frequent sets over data streams present attractive new challenges over traditional mining in static databases. data mining is generally used for retrieving the desire information to make it into knowledge from the large size databases.
"when the ccpr system is in use and a nlm is fitted to the cc data of the already identified pattern, it is expected that the most statistically significant parameter linked to one of the pattern classes will correlate with the pattern class determined by the ccpr system. this is because the ccpr system was trained with patterns whose membership to given classes was decided by the statistical significance level of their parameters."
"in summary, frn and theta oscillatory activity were modulated according to an unsigned rpe, and additionally, individual differences in frontal theta oscillatory activity predicted the learning rate across participants."
"subsequent to the development of methodology, we designed its architecture for an intermediate colombian city in particular (popayán) applying steps established in methodology. to graphically represent each of stages, we use a good practice used in description of project management methodologies of project management institute. the 4 proposed stages for methodology are the following."
"for background to and applications of phenyl-substituted oligosilanes, see: [cit] . for a related structure of a cyclotetrasilane without phenyl groups, see: [cit] . supplementary materials acta cryst. (2013) . e69, o149 [doi:10.1107/s160053681205074x] 1,1,3,3-tetra-tert-butyl-2,2-diisopropyl-4,4-diphenylcyclotetrasilane takayoshi kuribara and soichiro kyushin comment birefringent materials have a wide range of optical applications. single crystals of calcium carbonate and barium borate have been well known as inorganic birefringent materials. organic single crystals such as urea have also been known to show birefringence. since birefringence is related to crystal structures, studies on molecular structures and packing in a crystal are important. [cit], birefringence of tetrakis(4-phenylphenyl)silane was reported [cit] ."
"one use of satellite image is to make the process of classification. there are several classification algorithms such as decision trees, bayesian networks, naive bayes, maximum likelihood and minimum distance. some researches on satellite image classification have been carried out using decision tree algorithms. [cit] conducted a satellite image classification using the decision tree algorithm and compared with the isodata algorithm and maximum likelihood. the result shows that a decision tree has the best accuracy compared to other algorithms. the decision tree has proven to be an efficient algorithm for the classification of large datasets."
"since our goal is to learn a safe deterministic program from a neural network, we develop a counterexample guided inductive program synthesis approach. a cegis algorithm in our context is challenging because safety verification is necessarily incomplete, and may not be able to produce a counterexample that serves as an explanation for why a verification attempt is unsuccessful."
"as a final observation, we considered the use of an its architecture in a city, developed in an appropriate way (with the help of a methodology and an application example), will allow the prioritization of mobility services to be developed in the city in the short term. it will also facilitate interoperability and integration with existing services and with the services to be developed in the medium and long term. with the implementation of the services developed, it will be possible to find the solution to the most critical problems of an intermediate city, such as death rates due to traffic accidents and high levels of traffic"
"one may ask why we do not directly learn a deterministic program to control the device (without appealing to the neural policy at all), using reinforcement learning to synthesize its unknown parameters. even for an example as simple as 3 the inverted pendulum, our discussion above shows that it is difficult (if not impossible) to derive a single straight-line (linear) program that is safe to control the system. even for scenarios in which a straight-line program suffices, using existing rl methods to directly learn unknown parameters in our sketches may still fail to obtain a safe program. for example, we considered if a linear control policy can be learned to (1) prevent the pendulum from falling down (2) and require a pendulum control action to be strictly within the range [−1, 1] (e.g., operating the controller in an environment with low power constraints). we found that despite many experiments on tuning learning rates and rewards, directly training a linear control program to conform to this restriction with either reinforcement learning (e.g. policy gradient) or random search [cit] was unsuccessful because of undesirable overfitting. in contrast, neural networks work much better for these rl algorithms and can be used to guide the synthesis of a deterministic program policy. indeed, by treating the neural policy as an oracle, we were able to quickly discover a straight-line linear deterministic program that in fact satisfies this additional motion constraint."
"in latin american countries such as chile and brazil important advances in its architectures were identified at the national level, mainly related to american architecture, but no progress was detected in its architectures at a regional level or in a particular city, nor on methodologies that allow the design of these regional its architectures."
the goal of our synthesis algorithm is to find unknown values of θ that maximize the likelihood that p θ resembles the neural oracle π w while still being a safe program with respect to the environment context c: sample a set of trajectories c 1 using c[p θ +ν δ ];
"with the determined needs, we proceed to identify the specific services (of the areas used in the matrix) that will be taken into account for its architecture. the selected services will be a subset of services that the reference architecture has, adding relevant services of other architectures. these selected services must meet the specific needs of the stakeholders. finally, once the specific services to be provided in the its architecture have been determined, the components required in its architecture are identified. the specific components to be identified will depend on the its architecture selected as a reference."
"future work will be focused on two main paths: developing pattern generation schemes for processes where the inherent noise is modelled by time series; and developing a pattern recognition/prediction scheme for patterns that are slow or weak in a specific inspection window, thus enabling the appearance of a pattern to be anticipated and pre-emptive actions, such as predictive maintenance or repairs, to be taken."
the global safety property we wish to preserve is that the pendulum never falls down. we define a set of unsafe states s u of the transition system (colored in yellow in fig. 1 ):
"in detailed its architecture of the \"traffic metering\" service ( figure 12 ), two (center and filed) of five subsystems presented in figure 11 are used. the \"center\" subsystem uses the physical object \"transit and traffic management center\" and the subsystem \"field\" uses the physical object \"its roadway equipment\". each of the two physical objects uses several logical objects to perform its functionality. the service has two actors, its operator and driver."
". this equation aims to search for a program p θ at minimal distance from the neural oracle π w along sampled trajectories, while simultaneously maximizing the likelihood that p θ is safe. our synthesis procedure described in sec. 4.1 is a random search-based optimization algorithm [cit] . we sample a new position of θ iteratively from its hypersphere of a given small radius surrounding the current position of θ and move to the new position (w.r.t. a learning rate) as dictated by equation (2) . for the running example, our search synthesizes:"
"rules from sdt algorithm indicated that before burned class has band 4 value greater than the band 7 value, a burned class has band 7 value greater than the band 4 value, after burned class was in the middle value of the band, and the non-peat class has band 2 value greater than any other band. here were 11 rules resulted from the sdt algorithm:"
"this section reviews previous work on pattern generation for ccpr, application of pnn and svm in ccpr and the estimation of abnormal pattern parameters when ml algorithms are used for identifying patterns in control charts."
"the city of popayán, intermediate colombian city, located in the southwest of the country, is one of the oldest and best preserved cities of the american continent, which is reflected in its architecture and religious traditions. the city has an area of 512 km 2 and 400,000 inhabitants (aprox). according to the socioeconomic studies (carried out by the chamber of commerce), the main economic activities of the city are commerce, accommodation and food services and manufacturing industry."
"a. vague table generation the experiment was conducted on an fmcg database that contains the daily inventory of the products purchased by users. we classify items on the basis of products denoting each with a certain unique code. the vagueness is found in the database by using the vague table which consist all the variants of a particular product that is available in the database. this imparts vagueness in the dataset on which we apply out vague itemset miner (vie) algorithm. since the fmcg database is very large consisting of huge number of transactions, we only report results on a sample of transactions selected at random. the number of transactions t taken into account is 10 which consists a number of products with their codes, i.e. both vague items and non-vague items. first experiment was conducted on the sample dataset with the traditional apriori method and the results are noted in table 1 . the second experiment was conducted on the same sample dataset with the vague itemset miner (vie) algorithm and the result are noted in table 2 . the minimum threshold support was kept at 30% and minimum threshold confidence at 80%. it is clear from the table 1 & 2 that the rules that are generated from traditional apriori have only three rules (highlighted above for visual understanding) that are also found using vie algorithm but with difference in their support and confidence measures. some of the rules that are in table 1 are omitted in table 2 and vice versa. this demonstrates that the traditional apriori performs undermining and forms subversive rules whereas the vague algorithm vie performs over mining and forms puissant rules. thus vague sets when incorporated with association rules provide a contrasting meaning to the classical approach and gives better results."
"svm is a relatively recent algorithm in the field of ml. within less than two decades of being created, many of its advantages with respect to the best existing methods have become evident: generalisation capacity, ease of use and solution uniqueness [cit] . svms can deal with nonlinear formulations, provide a trade-off between dimensionality (space complexity) and accuracy and have shown good results in pattern recognition applications."
"this study used satellite images of rokan hilir that have peatland cover. overlay and crop satellite images with maps of peat are necessary to get the image that has peatland cover. overlaying was conducted to determine the areas of peatland cover, and cropping was carried out to take peatland area only. satellite images with peatland cover still have a lot of clouds; therefore it was necessary to select a subset of image with clean of the cloud. the results of subset images include in kubu subdistrict and pasir limau kapas subdistrict, rokan hilir, riau."
"the second task was a pl task, which also consisted of 637 trials divided into 49 blocks of 10-16 trials. as in the rvl, participants had to choose between two geometric figures that were rewarded differently (75% vs. 25% rewarded), resulting in two possible feedbacks: a green tick (reward, +a0.04) or a red cross (punishment, −a0.04). however, in this task, there were no uninformed reversal contingencies. after each block (10-16 trials), two new figures were presented. therefore, in each block, the participants had to discover the rule that would remain constant for the remainder of the block. during the first five trials of each block, a selection of the incorrect stimulus would lead to punishment. blocks were not followed by breaks or pauses; here we refer to block as periods in which cue-outcome associations remain stable. the duration of the stimulus presentation was the same as in rvl (figure 1 ). both tasks were preceded by a short training session."
vague association rule mining for profit pattern combine the statistic based pattern extraction with value-based decision making to achieve the commercial goals. the work we propose gives an advantage by incorporating vague sets in data mining. the vague sets allow us to consider the vague uncertainty existing in databases and to utilize it to mine such rules that ultimately give better correlation among items. the result calculated was better in comparison to the traditional approach and gives an alternative approach to data mining. the proposed approach not only improves the mining process but also provide the profitable rules in uncertain data. although a many researches has been carried out in association rule mining but still it requires more attention for defining the notion of profit which would help in improving business strategies and provide some recommender rules.
"the performance of the proposed pgs during the initial generation of patterns was measured. it was found that, as the significance level was increased, the percentage of discarded patterns also increased. the percentage of reclassified patterns remained approximately constant for the three values tested. thus, the significance level mainly affects the number of patterns to be discarded and it is necessary to generate more patterns initially as the significance level is increased."
"is safe in the environment c meaning that φ 1 ∨ φ 2 ∨ · · · is an inductive invariant of c[p] proving that c.s u is unreachable. example 4.3. we illustrate the proposed counterexample guided inductive synthesis method by means of a simple example, the duffing oscillator [cit], a nonlinear second-order environment. the transition relation of the environment system c is described with the differential equation:"
true confidence is the ratio of true support of the union of items a and b to the true supports of any of the item either a of b. it is denoted by .
"in both tasks, differences among conditions in both erp and tf data were determined by repeated-measures anova with two within-participant factors: valence (positive and negative) and absolute rpe (high, medium, and low)."
"pattern discovery from huge volume of data is one of the most desired attributes of data mining [cit] . however, in reality, a substantial portion of the available information is stored in text databases, which consists of large collections of documents from various sources, such as news articles, books, digital libraries and web pages. since web search engines have become pervasive and search has become integrated, retrieving of information from these search engines consist of three essentials: query, documents, and search results."
"some cities in the country (mainly large and intermediate cities) have developed technological services of urban mobility, with the objective of minimizing the occurrence of traffic accidents due to causes related to deficiencies in traffic management. these services have been developed within framework of the so-called intelligent transportation systems (its)."
"buffers with radius 1 km were created for hotspots data. this radius of 1 km was used because the area for one hotspot in average is 2.58657 km, therefore the radius of the circle is 0.90737 km. this value is considered as the radius of a buffer for a hotspot. outside the buffers, random points are generated as false alarm data [cit] . overlaying between buffer zone of hotspots and satellite image was useful to get information about the class before burned, burned, and after burned. cropping process was performed to get a buffer area in the satellite image."
"the classical (crisp) set theory define sets as the \"collection of objects (either similar or dissimilar) called elements of a set as a whole\". these are also referred as crisp in nature because they only tells whether an element is a member or not, i.e., either 0 or 1. it may be also given as an element belongs to or does not belong to particular set. the crisp set theory often times is unable to provide a better understanding of any object/element to be of a certain group. thus, leading to the fact that value might lie in between 0 and 1."
"in the present work, we used brain erps and time frequency (tf) decomposition of eeg data to study the neuropsychological markers of both the rpe and the learning rate. to reach this goal, the participants performed two probabilistic learning (pl) tasks: a reversal learning (rvl) task in which they had to adapt their behavior to unexpected changes in the environment and a pl task, which consisted of multiple blocks of novel cue-outcome associations without unexpected reversal rules. in both tasks, electrophysiological responses were analyzed based on the characteristics of a computational rl model. we hypothesized that frn and theta oscillatory activity would be modulated by rpe in both tasks. additionally, if these two signals are also the neural signatures of the learning rate, they should vary across participants and tasks (e.g., increasing in more uncertain environments such as the rvl task compared with the pl task)."
"the last step of the proposed pgs concerns the final classification of the training patterns. once it has been determined that the pattern under study has either a steady mean, continuous change in mean or step change in mean, the pattern is reclassified by fitting a nlm to the pattern data taking account of the statistical significance of the parameters."
"the performance of the pgs as represented by the recognition accuracy achieved by two very different ml algorithms, support vector machines (svms) and probabilistic neural networks (pnns) was measured. these algorithms are based on statistical learning theory. the main difference in their learning methods relates to their risk minimisation [cit] . in the case of svm, structural risk minimisation is used to minimise an upper bound based on the expected risk, whereas in pnn, traditional empirical risk minimisation is adopted to minimise the error during training [cit] . another advantage of svms and pnns is the number or free parameters to be set, being only one in the case of pnns, and at most four for svms. these two algorithms have achieved good recognition accuracy, with pnn requiring little training data."
"the classical approach to association rule mining uses the apriori or similar algorithms which are based purely on statistical significance of the items present in the database. the approach was easy to consider when the databases contained only textual data, or in other words, certain data. as the database technology evolved, the basic measures of support and confidence were proving to be scarce in finding relevant knowledge. it is evident that without support and confidence it is difficult to find rules but to improve the knowledge discovery some more measures or dimensions need to be incorporated. one way of doing it is by first understanding our database. there are many mathematical tools that have been used over time to fulfill ones requirement with databases. the approach we propose takes uncertainty in consideration. now, there are many types of uncertainty that could be handled each www.ijacsa.thesai.org with a different and specific tool. the uncertainty we consider is of vagueness type."
"another issue studied in this paper was the determination of the minima and maxima of pattern parameters during pattern generation. finding an objective method to set the range of parameter values was an aim of this research. this issue was addressed by nesting two nlms, with the p-value of the related parameter determining the pattern class. without the objective method proposed here, by using different parameter ranges during pattern generation, different decision boundaries are estimated. this makes the recognition accuracies achieved not comparable and is a common mistake found in the ccpr literature."
"as mere identification of patterns is sometimes not enough for efficient root cause analysis, further information related to the identified pattern needs to be extracted. since the ccpr system was trained using patterns that ensured the estimation of correct decision boundaries, generality of the model and statistical significance of the model parameters, details such as cycle amplitude, periodicity, slope, shift magnitude, change point position and systematic departure can be obtained by fitting a nlm to the control chart data as implemented in the proposed pgs."
"recently, birefringence of single crystals of phenyl-substituted linear oligosilanes and their application to polarizers have been reported [cit] . from these results, crystals of phenyl-substituted silicon compounds seem interesting as potential optical materials. we report herein the synthesis and x-ray crystal analysis of a phenyl-substituted cyclotetrasilane."
"the decision tree method can automatically select the appropriate supporting attributes that iteratively split the given dataset into smaller groups according to the different values of these attributes [cit] . the basic concept of the decision tree is to convert the data into a tree and decision rules. the decision tree consists of a root node at the top of the tree, the internal node which is a branch of the tree, and the leaf node which is the end of a tree branch."
"in the literature, models that deal with the recognition and classication of patterns in addition to estimating their corresponding parameters are very rare [cit] ."
"if verification at this stage succeeds with an inductive invariant φ, a new policy program p θ is synthesized that can be verified safe in the state space covered by φ. we add the inductive invariant and the policy program into covers and policies in line 14 and 15 respectively and then move to another iteration of counterexample-guided synthesis. this iterative synthesize-and-verify process continues until the entire initial state space is covered (line 3 to 18). the output of algorithm 2 is [(p θ 1, φ 1 ), (p θ 2, φ 2 ), · · · ] that essentially defines conditional statements in a synthesized program performing different actions depending on whether a specified invariant condition evaluates to true or false. algorithm 2), then the deterministic program p:"
"initially, we proceeded with the general functional view of its architecture, for which the relevant stakeholders and identified functional processes were taken into account. general functional view is presented in figure 10 . the functional view allows to visualize the processes selected, relationships between processes, and between processes and stakeholders."
"in our running example, suppose we wish to operate the inverted pendulum in an environment such as a segway transporter in which the model is prohibited from swinging significantly and whose angular velocity must be suitably restricted. we might specify the following new constraints on state parameters to enforce these conditions:"
"the main causes identified of these traffic accidents in colombian cities are: disobedience to traffic regulations and speeding, which are directly related to traffic management (3) ."
"the pgs presented in this paper comprises three steps: initial pattern generation, mean change classification and pattern categorisation. in the first step, the generality of the ccpr system is ensured due to the wide-range of training pattern parameters values used and the random assignment of these during pattern generation. in the second step, break points (if there are any) are detected. due to noise, this step is not straightforward."
"in this paper, we assume that a neural network is trained using a state-of-the-art reinforcement learning strategy [cit] . even though the resulting neural policy may appear to work well in practice, the complexity of its implementation makes it difficult to assert any strong and provable claims about its correctness since the neurons, layers, weights and 1 we derive the control dynamics equations assuming that an inverted pendulum satisfies general lagrangian mechanics and approximate nonpolynomial expressions with their taylor expansions. biases are far-removed from the intent of the actual controller. we found that state-of-the-art neural network verifiers [cit] are ineffective for verification of a neural controller over an infinite time horizon with complex system dynamics."
"the methodology proposed by the authors has already been revised, so that the four important results by applying the methodology for the city of popayán and a final discussion on it will be presented below."
"where α is the learning rate and δ represents the prediction errors, calculated as the difference between the outcome and the expectancy or weight of the selected figure. next, softmax action selection was used to compute the probability of choosing one of the two options:"
"in the systematic review of literature, looking for documents generated at most 12 years ago, no works of its architecture designs were found for colombian cities (or any other similar country) that used methodologies developed considering reference architectures and city context, as proposed in this paper."
"we used spearman correlation to study the relationship between participantʼs learning rate and both midfrontal theta activity and frn amplitude during rvl tasks. finally, we studied whether differences in learning rate between tasks and across participants may predict differences in frn amplitude and oscillatory activity. for that reason, we performed spearman correlations of overall frn amplitude and theta activity with the difference of the learning rates obtained in the rvl and the pl. to obtain a unique learning rate for each participant in the pl to compare it with the learning rate obtained in the rvl task, we average the learning rates obtained across blocks for each individual. we performed separate analyses for positive and negative feedback. participants with theta activity and frn amplitude greater than 2.5 sd in any of the conditions were not included in the correlation analysis of each specific condition."
"actors of system, subsystems and physical objects were taken into account for the general physical viewg. the physical view that we proposed for its architecture of popayán is presented in figure 11 . the general physical view allows to visualize proposed subsystems and physical objects within each subsystem. physical objects must provide the selected services in architecture. in addition to the two views of the general its architecture of the city, a detailed service-specific its architecture was designed for each of the most relevant services of the 35 selected for the its architecture of the city."
"this work applies the method of decision tree based spatial autocorrelation namely spatial decision tree (sdt) to classify peatland before burned, burned, and after burned in kubu subdistrict and pasir limau kapas subdistrict, rokan hilir, riau province. parameter and autocorrelation was added to cart algorithm for this work, because sdt have a similar concept with cart algorithm. this work tried the other parameter for the best result. the results from the decision tree based spatial autocorrelation algorithm are compared with the other decision tree algorithm like cart algorithm, c5.0 algorithm and c4.5 algorithm."
"the research scope is descriptive, at least in this first stage. it describes the methodology that we propose to carry out to design an its architecture for an intermediate city, additionally presents the application of this methodology in a specific city. to design its architecture for an intermediate city, we decided to review the following documentation: reference its architectures with worldwide application (16) (17) (18), papers related to development of its architectures at a regional level and study of the particular conditions of an intermediate city."
"prioritized needs of stakeholders, special considerations identified in the context of the city, service packages described in the reference architecture, were taken into account for determination of the services."
"we considered the methodology developed and details on design of its architecture of the city are useful tools for those interested in developing an its architecture for a city that has a similar context within the country or in a development country similar to colombia. in addition, its architecture proposed for popayán can be a reference for development of mobility services for the city, because this architecture defines the needs, priorities and services that need to be implemented, guaranteeing that developments carried out will use standardized elements and exchange of information between these is already defined. we believe that with the information delivered on the proposed methodology and the application case in this document, it is sufficient to apply it in another intermediate city and obtain a similar result, in case it is considered necessary, the interested parties can contact the authors to try to solve any concerns about it."
"to verify a neural network control policy π w with respect to an environment context c, we first synthesize a deterministic policy program p from the neural policy. we require that p both (a) broadly resembles its neural oracle and (b) additionally satisfies a desired safety property when it is deployed in c. we conjecture that a safety proof of c[p] is easier to construct than that of c[π w ]. more importantly, we leverage the safety proof of c[p] to ensure the safety of c[π w ]."
"this paper presents an inductive synthesis-based toolchain that can verify neural network control policies within an environment formalized as an infinite-state transition system. the key idea is a novel synthesis framework capable of synthesizing a deterministic policy program based on an user-given sketch that resembles a neural policy oracle and simultaneously satisfies a safety specification using a counterexample-guided synthesis loop. verification soundness is achieved at runtime by using the learned deterministic program along with its learned inductive invariant as a shield to protect the neural policy, correcting the neural policy's action only if the chosen action can cause violation of the inductive invariant. experimental results demonstrate that our approach can be used to realize fully trustworthy reinforcement learning systems with low overhead."
"where i is the index of a sample, varying from 1 to m (m is the number of samples). from equation (1), (2) and (3) spatial information gain is obtained as presented at following equation [cit] :"
"guided by π w 's actions on such collected states, p is further improved to resemble π w . the goal of the synthesis procedure is to search for a deterministic program p * satisfying both (1) a quantitative specification such that it bears reasonably close resemblance to its oracle so that allowing it to serve as a potential substitute is a sensible notion, and (2) a desired logical safety property such that when in operation the unsafe states defined in the environment c cannot be reached. formally,"
"the purpose of this paper is to present our methodology for design of its architecture for an intermediate city in a developing country, and also to present an example of application of this methodology in design of proposal for the its general architecture for city of popayán."
"for pattern identification and cause assignment, it is necessary to identify abnormalities in the current control chart and extract information such as frequency, magnitude, time when a certain abnormality happened, etc."
"in figure 3, determination of architecture components stage is presented. outputs obtained in stages 1 and 2 are used as inputs in this stage. additionally, specific components that must be identified to present its architecture of the city, should be considered as input to this stage. some components, which make up the views of its architecture of the city, vary depending on the architecture that was taken as a reference. stakeholders, their needs and services provided are common components in revised reference architectures, while other elements are particularly used by one or another architecture."
"from the its architecture proposed for the city of popayán, it is necessary to carry out a design and development of services, which seek an improvement in the identified aspects that require urgent attention, such as road safety and proper traffic management."
"this data is obtained to help the root cause analysis in the efficient identification of assignable causes of poor quality in the production system; such information can help to distinguish between patterns that are identified as the same but have different root causes, e.g., a cyclic pattern with period of 12 might be produced due to wear of a tool, while another pattern with period equal to 24 might be caused by variations in the input voltage."
"finally, also at a national level, a proposal for an architecture of a fundamental component in an its, an advanced public transport systems apts, is presented, specifically for a region of the country (15) . the proposed architecture is based on the american its architecture and presents an important advance in the development of this type of architecture for a specific region, however it does not carry out the design applying a proposed methodology or one that is already implemented."
"the proposed methodology is especially developed for commercial transactions where each item having an item code but for the different packing the code is differ for the same item. this will create the vagueness and cannot be deal by the conventional association rule mining for this purpose we use the vague association rule mining and generate those rules which generate the profit significance but ruled out due to statically violation caused by vagueness. we use an algorithm to mine vague set based association rules. as discussed in previous section, the vague sets have found application in association rule mining in many ways. we propose another important methodology to find support and confidence for mining association rules. the technique we propose consist three new formulas for finding support and confidence."
"the city of popayán and the majority of colombian cities are highly influenced culturally, socially, economically and technologically by the american culture, rather than by the european culture. the current public transport system has major problems in efficiency, safety and reliability."
"our search-based synthesis sets θ to 0 initially. it runs the deterministic program p θ instead of the oracle neural policy π w within the environment c defined in fig. 1 (in this case, the state transition system represents the differential equation specification of the controller) to collect a batch of trajectories. a run of the state transition system of c[p θ ] produces a finite trajectory s 0, s 1, · · ·, s t . we find θ from the following optimization task that realizes (1):"
"the first author acknowledges the support of the mexican national council for science and technology, conacyt, enabling him to pursue graduate studies at university of birmingham − united kingdom under grant no. 548421."
"at this stage the overlay process of hotspots with satellite image aims to obtain the required classes. burned class derived from hotspots was overlayed with the image in juli. before burned class derived from hotspots was overlayed with the image in mei. after burned class derived from hotspots was overlayed with the image in november. hotspots used in this study were taken from july 2 to july 5, 2002."
"the relationship of frn and theta oscillatory activity with both learning rate and unsigned rpe fits well with a new model that proposes that the mpfc detects actionoutcome discrepancies independently from their affective valence [cit] . the predicted response-outcome model (pro model) is able to correctly simulate some of the previously reported results on the activity of mpfc in error processing, conflict detection, and action monitoring. according to the model, mpfc neurons would fire when an action yields an unexpected outcome, that is, when the outcome is unexpected (positive surprise), but also when an expected outcome does not appear (negative surprise). therefore, according to the pro model, both the negative and positive prediction errors in the rl model are unexpected outcomes and therefore unexpected nonoccurrences of the expected response. the modulation of theta and frn activity with unsigned prediction error would then be related to the surprise signal of the mpfc. in addition, the pro model also predicts greater activity of the mpfc in environments showing greater variability (the rvl task compared with the pl task) as surprises are more constant in less predictable environments. [cit] showing that the mpfc tracks the volatility of the environment as well as present results showing that theta activity are greater in the rvl task than in the pl task would also be explained by the pro model. similar surprise signals have been reported in other brain regions connected to the mpfc such as the amygdala [cit] ) and its major target, the locus coeruleus [cit], which is the main noradrenergic nucleus of the brain. indeed, pharmacological studies have shown that noradrenergic drugs that lead to an increase of noradrenergic release increase frn amplitude (riba, rodríguez-fornells, morte, münte, & [cit] ) . thus, frn amplitude and theta activity could be related to attentional signals transmitted from the locus coeruleus by noradrenergic neurotransmission rather than reflect increase/decreases of dopamine in the ventral tegmental area. however, this requires further research combining different drugs to study the different roles of both dopamine and noradrenalin in outcome monitoring."
"our methodology for the design of an its architecture and its application for the city of popayán, have some advantages with respect to what currently exists. first, its architecture designed for popayán city can be used directly for an intermediate city with similar characteristics; while international reference architecture and standards have too large scope and a totally different context of development."
"given the environment state transition system c[p] deployed with a synthesized program p, the verification approach above can compute an inductive invariant over the state transition system or tell if there is no feasible solution in the given set of candidates. note however that the latter does not necessarily imply that the system is unsafe."
"as previously noted, the significance level set during pattern generation mainly affected the number of patterns discarded, and had a small effect on the pattern recognition accuracy of the two tested ml algorithms."
"the structure of following sections of article is as follows: section 2 presents related works; section 3 presents methods used for development; section 4 presents results obtained, mainly, with respect to its architecture formulated and a discussion about it; finally, section 5 presents conclusions and perspective. yokota and weilland formulated a proposal to implement its architecture in developing countries (7) . the authors presented four criteria that they considered relevant in construction of architecture: affordability, regional compatibility and integration, geopolitics and technical aspects. this proposal was presented more than thirteen years ago and versions of evaluated architectures have been updated several times. in addition, a clear and detailed methodology for design of its architecture is not established, although criteria are mentioned and a basic process of four activities is proposed."
"we identified the needs of the selected stakeholders, through the tools mentioned in the methodology. once identified, we evaluate them (using information presented in figure 8 ) to determine what need was required by certain stakeholders y what needs would be taken into account in the its architecture."
only some of physical objects and logical objects presented by the arc-it (included as components of the subsystems) were required to design the its proposal for the city.
"we solve the incompleteness challenge by leveraging the fact that we can simultaneously synthesize and verify a program. our cegis approach is given in algorithm 2. a counterexample is an initial state on which our synthesized program is not yet proved safe. driven by these counterexamples, our algorithm synthesizes a set of programs from a sketch along with the conditions under which we can switch from from one synthesized program to another."
"when determining the stakeholders to consider for the its architecture, their needs are determined through interviews, surveys and the help of the previously mentioned documents. then the needs with the largest number of involved stakeholders and high or medium level of influence will be considered."
"a drawback of monte-carlo simulation is that this method is not sufficiently robust to noise which can greatly affect the generalisation ability of the algorithm if special care is not paid to the patterns used for training. consequently, the statistical properties of the patterns generated for training the algorithm can be altered, e.g., a positive slope can appear shallower or even disappear altogether due to noise, and so the ml algorithm is trained with a-priori misclassified patterns, thus setting incorrect pattern classification boundaries. the aim of this research is to develop a pattern generation scheme (pgs) for ccpr that is robust to variations in the pattern parameters used for training. this pgs randomises all the parameters that categorise each abnormal pattern, such as cycle amplitude, periodicity, slope, shift magnitude, change point position and systematic departure [cit] . furthermore, the proposed pgs can be used by other authors as a standard scheme for producing data for training ccpr systems that can be compared."
"in our daily life, we face decisions and evaluate their consequences to obtain information about how to act in similar situations in the future. determining the value of a decision in different contexts is a complex issue, which is influenced by new evidences that are continuously collected and by the learning history. the relevance of both history and new information in guiding decisionmaking is influenced by the characteristics of the environment. in uncertain environments, new pieces of information have greater importance in the adaptation of behavior. in contrast, in stable environments, past experience is more relevant than recently acquired information. therefore, we constantly evaluate how accurate our predictions are and how relevant incoming information is according to the present context to update future estimates."
"these negative aspects in the development of its services can be solved by designing and using its architectures for cities. its architectures provide a reference on what mobility services should be implemented, what priorities exist, and what processes and functionalities should be implemented."
"given an ah-pair database, d, we can define different types of support and confidence for an itemset z or a var x  y,where x y  z [cit] ."
"for the determination of appropriate reference its architecture for the city of popayán, we compared the following its architectures: american its architecture (called arc-it) (16), architecture of the european union (called frame) (17), architecture proposed by iso in standard 14813 (18), and colombian architecture (13) . we use the service areas in which all the revised architectures are focused, the vast majority are common service areas for all architectures, however, the exclusive service areas of some architectures were also taken into account. comparison was made using a matrix (presented in figure 6 ) that identified which is the architecture which covered most of the service areas."
our synthesis approach is critical to ensuring safety when the neural policy is used to predict actions in an environment different from the one used during training. consider a neural network suitably protected by a shield that now operates safely. the effectiveness of this shield would be greatly diminished if the network had to be completely retrained from scratch whenever it was deployed in a new environment which imposes different safety constraints.
main results of research developed are: methodology for design of its architecture and its architecture designed for the city of popayán developed following one by one the stages described in methodology.
"vagueness is the property of an item that is difficult to comprehend and differentiate. the principles of vague set theory are used to deal with vagueness. unlike fuzzy logic which is a special case of vague logic, the vague sets allows to bound the existence of item(s) to an interval. any item belonging or not will be denoted by its true and false membership. we incorporate vague logic with the classical apriori algorithm to find more relevant yet vague rules."
"in arm, support and confidence are the basic measures that have been used since its inception, which define the statistical significance of any rule [cit] ."
"benchmark self-driving defines a single car navigation problem. the neural controller is responsible for preventing the car from veering into canals found on either side of the road. benchmark lane keeping models another safetyrelated car-driving problem. the neural controller aims to maintain a vehicle between lane markers and keep it centered in a possibly curved lane. the curvature of the road is considered as a disturbance input. environment disturbances of such kind can be conservatively specified in our model, accounting for noise and unmodeled dynamics. our verification approach supports these disturbances (verification condition (10)). benchmarks n-car platoon model multiple (n) vehicles forming a platoon, maintaining a safe relative distance among one another [cit] . each of these benchmarks exhibited some number of violations that were remediated by our verification methodology. benchmark oscillator consists of a two-dimensional switched oscillator plus a 16-order filter. the filter smoothens the input signals and has a single output signal. we verify that the output signal is below a safe threshold. because of the model complexity of this benchmark, it exhibited significantly more violations than the others. indeed, the neural-network controlled system often oscillated between the safe and unsafe boundary in many runs. consequently, the overhead in this benchmark is high because a large number of shield interventions was required to ensure safety. in other words, the synthesized shield trades performance for safety to guarantee that the threshold boundary is never violated."
"analysis includes four major steps, namely, (1) image pre-processing, (2) determining classes of image, (3) distribution of training data and testing data and classification process, (4) evaluation and comparative analysis of classification results."
"we do not require the user to explicitly define conditional statements in a program sketch. our synthesis algorithm uses verification counterexamples to lazily add branch predicates φ under which a program performs different computations depending on whether φ evaluates to true or false. the end user simply writes a sketch over basic expressions. for example, a sketch that defines a family of linear function over a collection of variables can be expressed as:"
"in both tasks, if the participants did not respond in the requested time (1000 msec), a question mark appeared on the screen after the stimuli. these trials were discarded from further analysis. self-paced rest periods were given after 35-40 trials. during these pauses, the participants were told how much money they had earned up to that point. the participants were encouraged to earn as much money as possible in both tasks. the participants were explicitly informed that one task involved uninformed reversals (rvl) and the other did not (pl)."
"the frn was studied by epoching eeg data from 100 msec time-locked before the outcome (baseline) to 600 msec after the outcome onset. following previous studies [cit], frn was analyzed by averaging the amplitude in a time window located 40 msec around the peak, which was located between 240 and 300 msec for each experimental condition at fcz. however, this mean amplitude is affected by the concomitant p300, which we hypothesized might respond differently to experimental conditions. to minimize this effect, erp epochs were first high-pass-filtered at 3 hz to remove slow-frequency noise such as p300 [cit] ."
an [cit] modeled hesitation information by the purchaser in online shopping using vague association rule and provide the notion of var for almost sold items in the online shopping by considering different user preference.
"we evaluate the proximity of these two policy programs to the neural oracle and, in line 6 of algorithm 1, to improve the policy program, we also optimize equation (5) by updating θ with a finite difference approximation along the direction:"
"objective of this stage of the methodology is to determine a reference its architecture by selecting some of the revised proposals, identifying (as a complement) relevant aspects of other reference architectures."
"finally, 35 services were taken into account for its architecture of the city (33 taken from arc-it and two additional ones from other reference architectures). the list of service is presented in figure 9 ."
"having an its architecture that recognizes the particular conditions of the city, allows the administration to propose and develop services in a medium and long-term perspective, gradually implementing said mobility services in priority order and in an organized manner."
"although algorithm 2 is sound, it may not terminate as r * in the algorithm can become arbitrarily small and there is also no restriction on the size of potential counterexamples. nonetheless, our experimental results indicate that the algorithm performs well in practice."
"before the classification stage, the data were divided into several experimental groups using k-fold cross validation with k of 10. in which 9/10 data were used as training data and 1/10 data were used as testing data. classification process was conducted using r software."
"the first preprocessing stage is georeferencing. georeferencing produces raster maps which had the projected coordinate system utm zone 47 n with wgs84, meaning that rokan hilir is located at 47 n zone in the utm (universal transverse merctator) projection system with geospatial reference system of wgs84."
"in pl, the amplitude of the frn and theta activity within trials may be modulated not only by rpe but also by the difference of learning rate among blocks. to test this hypothesis, we performed a multiple regression analysis with two independent measures: absolute rpe and the learning rate associated to each block. again, separate analysis for positive and negative feedback were performed."
"for all statistical effects involving two or more degrees of freedom in the numerator, the greenhouse-geisser epsilon was used as needed to correct for possible violations of the sphericity assumption. the p values following correction are reported."
the study shows [cit] that interestingness measures are distinct for different applications and substantiate that domain knowledge is necessary to the selection of an appropriate interestingness measure for a particular assignment and business objective and the goal of any business is to generate profit. so the profit can be taken as one of the measures with proper mining technique so it can help in decision making process of business.
"for all benchmarks, our tool successfully generated safe interpretable deterministic programs and inductive invariants as shields. when a neural controller takes an unsafe action, the synthesized shield correctly prevents this action from executing by providing an alternative provable safe action proposed by the verified deterministic program. in term of performance, table 1 shows that a shielded neural policy is a feasible approach to drive a controlled system into a steady state. for each of the benchmarks studied, the programmatic policy is less performant than the shielded neural policy, sometimes by a factor of two or more (e.g., cartpole, self-driving, and oscillator). our result demonstrates that executing a neural policy in tandem with a program distilled from it can retain performance, provided by the neural policy, while maintaining safety, provided by the verified program."
"in addition to the aforementioned review of services of the reference architecture, a context analysis tool should be used in this stage. authors recommend pestle (19) . pestle allows evaluation of political, economic, social, technological, legal and environmental aspects of the city, related to mobility, transport infrastructure, means of transport used and related normative documents. as a result of stage, the special considerations of the city context must be obtained. for the identification of stakeholders, it is recommended to review documents such as: history of projects related to mobility and transport in the city, its architecture documents in other similar international cities and information provided by local government entities. once the stakeholders have been identified, it is recommended to evaluate them by analyzing their level of influence and position on a possible its for the city, to determine whether or not it is taken into account for the needs analysis."
h 0 : there are no break points (the reduced model fits better) h 1 : a break point is detected (the full model fits better)
"fires in peatland/forest are very difficult to be handled than the fires that occurred in the area of non-peat. peat fires (ground fire) difficult to detect because it can spread to the deeper or spread to more distant locations without being seen from the surface [cit] . processing of satellite images produced from remote sensing is able to provide convenience for stakeholders in monitoring the fire that has happened, is happening, and estimates the incidence of fires in the future. additionally it can estimate the area burned and predicted environmental changes caused by the fire for a certain period [cit] ."
"framework. we construct a policy interpretation mechanism to enable verification, inspired by prior work on imitation learning [cit] and interpretable machine learning [cit] . fig. 2 depicts the high-level framework of our approach. our idea is to synthesize a deterministic policy program from a neural policy π w, approximating π w (which we call an oracle) with a simpler structural program p. like π w, p takes as input a system state and generates a control action a. to this end, p is simulated in the environment used to train the neural policy π w, to collect feasible states."
"our results are summarized in table 3 . when the underlying environment sightly changes, learning a new safety shield takes substantially shorter time than training a new network. for cartpole, we simulated the trained controller in a new environment by increasing the length of the pole by 0.15 meters. the neural controller failed 3 times in our 1000 episode simulation; the shield interfered with the network operation only 8 times to prevent these unsafe behaviors. the new shield was synthesized in 239s significantly faster than retraining a new neural network for the new environment. for (inverted) pendulum, we deployed the trained neural network in an environment in which the pendulum's mass is increased by 0.3kg. the neural controller exhibits noticeably higher failure rates than in the previous experiment; we were able to synthesize a safety shield adapted to this new environment in 581 seconds that prevented these violations. the shield intervened with the operation of the network only 8.7 number of times per episode. similar results were observed when we increased the pendulum's length by 0.15m. for self-driving, we additionally required the car to avoid an obstacle. the synthesized shield provided safe actions to ensure collision-free motion."
"eeg was recorded from the scalp (0.01 hz high-pass filter with a notch filter at 50 hz; 250 hz sampling rate) using a brainamp amplifier with tin electrodes mounted in an electrocap (electro-cap international) located at 29 standard positions (fp1/2, fz, fcz, f7/8, f3/4, fc1/2 fc5/6, cz, c3/4, t3/4, cp1/2, cp5/6, pz, p3/4, t5/6, po1/2, oz) and the left and right mastoids. an electrode placed at the lateral outer canthus of the right eye served as an online reference. eeg was rereferenced offline to the linked mastoids. vertical eye movements were monitored with an electrode at the infraorbital ridge of the right eye. electrode impedances were kept below 5 kω. trials with absolute mean amplitudes higher than 100 μv were automatically rejected offline. six participants were excluded from the study because they had trial rejection rates higher than 20%."
"to overcome these difficulties, we define a new verification toolchain that reasons about correctness extensionally, using a syntax-guided synthesis framework [cit] that generates a simpler and more malleable deterministic program guaranteed to represent a safe control policy of a reinforcement learning (rl)-based neural network, an important class of machine learning systems, commonly used to govern cyberphysical systems such as autonomous vehicles, where high assurance is particularly important. our synthesis procedure is designed with verification in mind, and is thus structured to incorporate formal safety constraints drawn from a logical specification of the control system the network purports to implement, along with additional salient environment properties relevant to the deployment context. our synthesis procedure treats the neural network as an oracle, extracting a deterministic program p intended to approximate the policy actions implemented by the network. moreover, our procedure ensures that a synthesized program p is formally verified safe. to this end, we realize our synthesis procedure via a counterexample guided inductive synthesis (cegis) loop [cit] that eliminates any counterexamples to safety of p. more importantly, rather than repairing the network directly to satisfy the constraints governing p, we instead treat p as a safety shield that operates in tandem with the network, overriding network-proposed actions whenever such actions can be shown to lead to a potentially unsafe state. our shielding mechanism thus retains performance, provided by the neural policy, while maintaining safety, provided by the program. our approach naturally generalizes to infinite state systems with a continuous underlying action space. taken together, these properties enable safety enforcement of rl-based neural networks without having to suffer a loss in performance to achieve high assurance. we show that over a range of cyber-physical applications defining various kinds of control systems, the overhead of runtime assurance is nominal, less than a few percent, compared to running an unproven, and thus potentially unsafe, network with no shield support. this paper makes the following contributions:"
"the rest of the paper is organized as follows, in section 2 we discuss the fundamental basis of association rules, and vague set theory that deal with uncertainty and vagueness, section 3 introduces the related work that has been done so far with this theory in accordance with association rules called vague association rule (var) [cit] . in section 4 describe our methodology and discuss the how var is helpful to business problem and its decision making process. in section 5 discussions of result and comparison with classical and vague rules. finally section 6 concludes the paper."
"the remainder of this paper is organised as follows. section 2 reviews the issues studied here. the proposed pgs for supervised ml algorithms is introduced in section 3. section 4 describes the results obtained. finally, conclusions of this work and suggestions for further research are provided in section 5."
"where d (π w, p, c) measures proximity of p with its neural oracle in an environment c; h defines a search space for p with prior knowledge on the shape of target deterministic programs; and, safe(c, h ) restricts the solution space to a set of safe programs. a program p is safe if the safety of the transition system c[p], the deployment of p in the environment c, can be formally verified."
a city that makes an incremental deployment of its portfolio of mobility services requires an its architecture that allows the integration and interoperability of new services with the existing technology and services platform.
"it was observed from this anova that using the proposed pgs significantly increased the mean accuracy by 6.90%. also, the kernel that achieved the best accuracy was the laplace kernel. continued on next page continued on next page for the pnn, one hundred test runs were carried out, each involving 50 patterns of each type. figure 5 shows the 95% ci for the mean accuracy obtained using the proposed pgs as well as the results for a pnn trained conventionally. table 7 shows the mean accuracies achieved during the testing of the pnns disaggregated by each of the four designs, the seven simple patterns and the five sample sizes. anova of three factors with up to triple interactions was also employed to analyse the accuracies achieved by the pnns. the three factors considered were: sample size, pattern generation design and pattern type. it was found that the mean accuracy was significantly increased when the pgs scheme was employed. continued on next page as mentioned in the introduction, it was also of interest in this study to assess the proposed scheme with two very different ml algorithms, namely, svm and pnn. the performance of the proposed pgs was measured for the three aforementioned α levels. figure 6 shows the mean and the 95% ci of the accuracies achieved. it can be seen that with both ml algorithms the accuracy was increased when the proposed pgs was used. these accuracies marginally increased when the α level was changed to 0.01 for both algorithms."
"the third column of table 3 gives the parameter values used for the initial pattern generation. the fourth column shows which model fits each pattern better and the p-values obtained from the f-test. to obtain the fourth column, it was necessary to fit two models to each pattern. then, using the sses from these, the f-value and its respective p-value were obtained to determine which model fits better. the fifth column lists the p-values of the significant term of the model that was determined to fit better in the previous step."
"where ∥·∥ is a suitable norm. as illustrated in sec. 2.2, we aim to minimize the distance between a synthesized program p θ from a sketch space and its neural oracle along sampled trajectories encountered by p θ in the environment context but put a large penalty on states that are unsafe. random search. we implement the idea encoded in equation (5) in algorithm 1 that depicts the pseudocode of our policy interpretation approach. it take as inputs a neural policy, a policy program sketch parameterized by θ, and an environment state transition system and outputs a synthesized policy program p θ . an efficient way to solve equation (5) is to directly perturb θ in the search space by adding random noise and then update θ based on the effect on this perturbation [cit] . we choose a direction uniformly at random on the sphere in parameter space, and then optimize the goal function along this direction. to this end, in line 3 of algorithm 1, we sample gaussian noise δ to be added to policy parameters θ in both directions where ν is a small positive real number. in line 4 and line 5, we sample trajectories c 1 and c 2 from the state transition systems obtained by running the perturbed policy program p θ +ν δ and p θ −ν δ in the environment context c."
"algorithm 2 takes as input a neural policy π w, a program sketch p[θ ] and an environment context c. it maintains synthesized policy programs in policies in line 1, each of which is inductively verified safe in a partition of the universe state space that is maintained in covers in line 2. for soundness, the state space covered by such partitions must be able to include all initial states, checked in line 3 of algorithm 2 by an smt solver. in the algorithm, we use c.s 0 to access a field of c such as its initial state space."
"to design the proposed pattern generation scheme, it was necessary to develop a robust procedure for identifying and categorising break points in the mean value in control charts. such a method not only detects the potential existence of sudden changes in the mean but also statistically estimates the magnitude of these changes. the proposed scheme is also able to handle noise as the estimation of the p-values employed during pattern categorisation is based on the ratio of the estimated parameters and their standard errors."
"evaluation was conducted on accuracy, size of trees, and number of rules of the four algorithms. accuracy was calculated using confusion matrix. next, comparative analysis of classification outcomes used spatial decision tree based spatial autocorrelation algorithm, the cart algorithm, the c4.5 algorithm and the c5.0 algorithm."
"suitable sketches. providing a suitable sketch may need domain knowledge. to help the user more easily tune the shape of a sketch, our approach provides algorithmic support by not requiring conditional statements in a program sketch and syntactic sugar, i.e., the user can simply provide an upper bound on the degree of an invariant sketch. our experimental results are collected using the invariant sketch defined in equation (7) and we chose an upper bound of 4 on the degree of all monomials included in the sketch. recall that invariants may also be used as conditional predicates as part of a synthesized program. we adjust the invariant degree upper bound to evaluate its effect in our synthesis procedure. the results are given in table 2 . generally, high-degree invariants lead to fewer interventions because they tend to be more permissive than lowdegree ones. however, high-degree invariants take more time to synthesize and verify. this is particularly true for high-dimension models such as 8-car platoon. moreover, although high-degree invariants tend to have fewer interventions, they have larger overhead. for example, using a shield of degree 8 in the self-driving benchmark caused an overhead of 26.85%. this is because high-degree polynomial computations are time-consuming. on the other hand, an insufficient degree upper bound may not be permissive enough to obtain a valid invariant. it is, therefore, essential to consider the tradeoff between overhead and permissiveness when choosing the degree of an invariant."
"to study which components of erp and tf during feedback evaluation were associated with the prediction errors extracted from the model, negative and positive trials were independently sorted into three bins according to the size of the absolute rpe: those with high (hpe), medium (mpe), and low (lpe) prediction error (with each group defined by the 33rd, 66th, and 100th percentile of the range)."
"the molecular structure of 1 is shown in fig. 2 . compound 1 has the crystallographic c2 symmetry, and therefore the cyclotetrasilane ring has a completely planar structure. the silicon-silicon bonds [2.4404 (8) packing diagram of 1 is shown in fig. 3 . four molecules are present in a unit cell. all cyclotetrasilane rings are oriented toward the same direction with the line through the si1 and si3 atoms parallel to the b axis. there is no intermolecular ππ interaction among phenyl groups."
"regarding the service areas presented in the arc-it (selected reference architecture), we consider it pertinent to exclude services related to the areas of \"commercial vehicle operations\", \"vehicle safety\", and \"weather\". none of services in these areas was considered for its architecture of the city, because these are services that do not have a considerable priority for stakeholders, besides, the context of the city makes them little relevant. the services \"compliance with the law\" and \"performance management\" identified as a complement to the reference architectures were considered for the its architecture of the city."
"it was also noted that the pattern with the lowest discarding percentage was the normal pattern. as for the shift patterns, it was found that the reclassification percentage decreased as the significance level increased. continued on next page"
"although our synthesis algorithm does not guarantee convergence to the global minimum when applied to nonconvex rl problems, the results given in table 1 indicate that our algorithm can often produce high-quality control programs, with respect to a provided sketch, that converge reasonably fast. in some of our benchmarks, however, the number of interventions are significantly higher than the number of neural controller failures, e.g., 8-car platoon and oscillator. however, the high number of interventions is not primarily because of non-optimality of the synthesized programmatic controller. instead, inherent safety issues in the neural network models are the main culprit that triggers shield interventions. in 8-car platoon, after corrections made by our deterministic program, the neural model again takes an unsafe action in the next execution step so that the deterministic program has to make another correction. it is only after applying a number of such shield interventions that the system navigates into a part of the state space that can be safely operated on by the neural model. for this benchmark, all system states where there occur shield interventions are indeed unsafe. we also examined the unsafe simulation runs made by executing the neural controller alone in oscillator."
"to deal with uncertainty in mining, some soft computing techniques must be incorporated which helps to reason with the databases. neural networks, fuzzy logic, genetic algorithm, rough sets, vague sets are some of the soft computing techniques that does deal uncertainty to some extent]."
"as it is unknown if there is a break point and if its magnitude is statistically significant, the selection of which model best fits the patterns is a model selection problem with nested models, raising the following two hypotheses:"
"neural networks have proven to be a promising software architecture for expressing a variety of machine learning applications. however, non-linearity and stochasticity inherent in their design greatly complicate reasoning about their behavior. many existing approaches to verifying [cit] and testing [cit] these systems typically attempt to tackle implementations head-on, reasoning directly over the structure of activation functions, hidden layers, weights, biases, and other kinds of low-level artifacts that are far-removed from the specifications they are intended to satisfy. moreover, the notion of safety verification that is typically considered in these efforts ignore effects induced by the actual environment in which the network is deployed, significantly weakening the utility of any safety claims that are actually proven. consequently, effective verification methodologies in this important domain still remains an open problem."
"the decision tree algorithm based on spatial autocorrelation was successfully implemented by involving nsar (neigborhood autocorrelation split ratio) to the information gain of the cart algorithm. that algorithm is able to improve the accuracy of cart algorithm. although c5.0 and c4.5 algorithm had high accuracy, but the number of rules generated from the tree and the size of tree was very large and the classifier was quite complex, so it could reduce the efficiency in the used of the classifier to classify new data. in addition, the results of classification using sdt algorithm shows that there is similarity of pixels between after burned class with burned class, and after burned class with before burned class. this is because the land after burned has begun to change back became peat or has not changed. the most noise resulted around non peat class, burned class, and after burned class. the c5.0 algorithm had less noise, because this algorithm had the best accuracy."
"a central controller interface (hub), contains the task auctioneer, teleoperation controls, as well as a graphical user interface that allows it to control the simulation and all its various components. the user interface is developed using 2) simulation results: we have tested our sliding autonomy approach with an autonomous mode, a system-initiative mode, a mixed-initiative mode and a teleoperation mode. a robot will initiate a request for help when it enters an erroneous state. for example, when a robot has taken too long to complete a task, it will trigger a timeout and a request for help. an error message is sent to the hub (e.g., stuck, expected completion time exceeded, see table i ). when the hub got the message, it would change the robot's status on the user interface to show the error and notify the operator. the operator would typically intervene in situations such as when two robots are on a collision course; the robot didn't quite line up well with the boxes (causing a box to get pushed diagonally); or if the collision avoidance behaves erratically. figure 3 shows the time line of a typical run with the various events."
the goal of this research is to build a low-cost ground station that enables interoperability between multiple robots and a single human operator without putting a heavy load on the operator. the major contribution of this work lies in the following areas: 1) the introduction of peer-to-peer interaction mode to sliding autonomy; 2) the development of a simulation experimentation testbed for sliding autonomy; and 3) the physical experiment to demonstrate peer-topeer interaction under sliding autonomy. we validated our proposed sliding autonomy approach on the site clearing task using the usarsim simulator [cit] and on physical robots. we demonstrated that our system can easily switch between different levels of controls. the overall team performance can be improved over pure autonomy.
"for the bc phase, the relay broadcasts the decoded t r back to the nodes by a nested lattice code or a random gaussian codebook. since each node knows their own transmitted signals, the decodability of the desired message is guaranteed once each node can correctly decode t r . therefore, the achievable rates for node a and node b are given by"
"to enable the switch between different operation modes, the human operator will oversee the execution of the entire team via a graphical interface on a base station. each robot will also constantly monitor its own task execution. if a robot detects a certain error condition that it cannot recover from while under the autonomy mode, it can request help from a human operator to guide it to a condition the system can recover from. the human operator can also dismiss the request if he/she is currently busy with other higher priority jobs. meanwhile, if the human operator observes a robot behaving incorrectly or inefficiently, the human operator can intervene the control of the robot to improve its behavior."
we have described our sliding autonomy approach to solve the site clearing task. the human operator can monitor the status of the execution through the a graphical control interface and intervene the operation at anytime to improve performance. the robots can also initiate a request for operator's participation. we validated our approach in the usarsim simulation and also demonstrated the peer-to-peer interaction mode on physical robots.
"we also designed physical experiments to demonstrate the feasibility of our approach with a focus on the peer-to-peer interaction mode. the robot we used is an irobot create robot equipped with a hokuyo laser range finder, and an acer netbook equipped with a camera. we use the laser range finder to detect the box and the camera to locate the goal location (an orange blob). the goal of the task is for the robot(s) to push the box to the goal location with no obstacles on the course (see figure 4) ."
"we present some numerical results here. in each figure, we plot the exchange rate achieved by both the frequency domain scheme and time domain scheme with optimal input spectrum as mentioned in remark 4. moreover, we provide the cut-set bound [cit] for the case that joint encoding across sub-channels is allowed and the case that it is not allowed. for the cut-set bound that joint encoding is allowed, since we wish to have an outer bound for the time domain scheme, we restrict the power constraint to the individual power constraint p as we did in the time domain scheme. for comparison, we also provide the achievable exchange rate for the scheme performing the twophase df [cit] at each sub-channel with the optimal power allocation strategy and each phase uses a half of channel uses."
"our approach to the above problem is to apply sliding autonomy to control the multi-robot team. our sliding autonomy approach features levels of control from fully autonomous operations, to human-intervened operations and pure teleoperation. additionally, there is a great potential for human and robots to work together side by side and for each team member to contribute to the task objective based on their capabilities. thus, we introduce a peer-to-peer interaction mode for the human operator to work closely with the robot team as peers rather than just as supervisors."
"multi-robot systems are widely used in today's robotic applications because of their advantages over single robot systems [cit], such as improvements in robustness, reliability and efficiency. increased autonomy will reduce human intervention, thus reduce the operating cost and increase human safety. however, challenges also arise with this type of complex system, such as mechanisms for coordinating team members, fault tolerance of the team and the operator control of the team. while researchers strive to build a fully autonomous system to perform various tasks, the robot team inevitably faces many unforeseen circumstances in an open and dynamic world. the team can either adapt to the dynamics through some life-long learning process, or by seeking help from the more competent human operator. our goal is to develop a human-robot team with collaborative robots assisting the human operator to accomplish tasks while adapting to new or unexpected situations with the help from the human operator."
"we chose an open source controller application iridium that's compatible with usarsim. iridium, however, only provides some basic features for controlling a single robot. we have heavily extended iridium to act as each robot's simulated control system with capabilities for teleoperation, navigation, obstacle avoidance, box detection and additional udp communications. the iridium controllers are also given the ability to act as a bidder for task allocation. to support sliding autonomy, the controller also includes functionalities such as detecting erroneous situations in which to request human aid; returning to autonomous mode after a problem is resolved, and human-intervened control at any point."
"a. simulation 1) simulation setup: our simulation used usarsim [cit], a high-fidelity, extensible simulation developed to facilitate human-robot interaction in the search and rescue domain. our task involves a team of three robots working cooperatively to push obstacles against two opposite walls in the test room (see figure 2) . the robots are equipped with an ins sensor to help the robot determine its position and direction using wheel encoders. we set it such that the ins sensor is subject to drifting, causing accumulated errors over time. it is also equipped with a laser range finder for obstacle or box detection."
"the remainder of the paper is organized as follows. we discuss the related work in section ii and describe the details of our approach in section iii. in section iv, experiments are performed to validate our approach. we finally conclude our work in section v."
we now try to maximize the total achievable exchange rate of this scheme through an optimal power allocation strategy among nodes. the corresponding optimization problem is
"in our implementation, the robot uses its laser range finder to detect the box and navigate to it once it has been allocated the task. as shown in figure 6 a and b, the detection process works by moving to the nearest object based on its laser scan, then detecting and moving to the ends of the box. here we assume that the box is large enough and it will require two agents to push on each end. both agents also need to coordinate their behaviors such that the box orientation remains about the same. to enable the peer-topeer interaction between the operator and the robot, both agents use passive communication when they push the box together. the robot constantly updates the relative angle of the box, when the angle of change passes a certain threshold, the robot considers that the box has been pushed on the other side."
"as a future work, we will extend our current sliding autonomy approach with more sophisticated mechanisms for the robot to decide when and how to initiate help. we would like to extend the peer-to-peer interaction mode to enable dialogue and speech recognition between the operators and robots. we would like to enhance the current graphical control interface to provide more situation awareness. additionally, we plan to develop the control interface on portable devices such as cell phones or tablets such that human operators can carry them while interacting with the robots."
"in this work, we studied the achievable exchange rate for bidirectional relaying over isi channels from both frequency and time domain viewpoints. interestingly, different from its point to point communication counterpart, for the bi-directional relaying over isi channels, there exist examples for which the time domain scheme performs better than the frequency domain scheme and even beats the cut-set bound of separate encoding schemes despite the fact that our choice of linear filters is by no means optimal. one potential future work is to further close the gap between the achievable exchange rate and the cut-set bound through optimizing the choice of linear filters."
"with sliding autonomy, we expect that the robots can behave autonomously most of the time with occasional requests for cooperation at critical time, thus very few human operators and little operation time are expected. this type of system should solve the problems of full autonomy and teleoperation without the associated drawbacks of the two. with the addition of the peer-to-peer interaction mode, we augment the traditional sliding autonomy to handle more complex situations when the other modes do not meet the requirement."
"due to the lack of a second robot, we only demonstrated the autonomous and teleoperation modes with a single robot. in the peer-to-peer mode, two robots are supposed to push the box, we simulate a robot break down before they start pushing, which will trigger an error message been sent to the human operator to request for help. the operator will then choose to accept or deny the request. accepting the request will make the robot wait for the operator for a period of time before finding another agent (robot or human) who is willing to help. in our test, we assume the operator will accept the request immediately and start working with the robot to push fig. 6 . the physical demonstration of the peer-to-peer mode: a) a robot moves towards the box. b) the robot reaches one end of the box and asks for help, and waits for the operator to push on the other side. c) the robot and the operator push the box together to reach the goal (unfortunately the operator was not photographed in the process). the box together towards the goal."
"passive communication is always a challenging task for the robots. we rely on the environment to provide the media of communication. when pushing a long box in our site clearing scenario, two robots are required to push the box on its two ends in turns in order to maintain the box's orientation and trajectory. with explicit communication, we can have the robots send each other a message once it has pushed the box on one end and thus signals the push on the other end. with implicit communication, we need to add a new capability on the robot such that it can recognize that the a long box has been pushed on one side by a human operator or another robot. the detection works by finding the degree of change in the box's relative position to the robot. when the degree of change passes a certain threshold, it means the other end has been pushed. furthermore, we also enable the robot to have a voice feedback to communicate through sounds when the human operator is pushing too fast or too slow."
"to validate our approach, we applied our approach to both simulated robots and physical robots with the site clearing task. simulation allows us to demonstrate the smooth transitions between different modes of operation (except for peerto-peer interaction) under sliding autonomy. while the physical experiment allows us to demonstrate the effectiveness of the peer-to-peer interaction mode."
"additionally, robots inform the operator of their possible failures so that the operator can assist them in assessing or recovering from the failures. based on the flow of information that are required to accomplish a task, humans and robots can communicate information with each other when necessary. for example, an operator can help a robot find its way home by teleoperating it with direct commands or giving the relative goal position to the robot. the information exchange between operators and robots depends on the various interfaces with which they communicate. we developed a basic graphical interface to facilitate the communication including components such as command and control, dialogue and vision."
"our sliding autonomy control interface allows human operators to monitor the task execution status, intervene to improve efficiency and react to unforeseen issues. it also enables the system to seamlessly switch between different levels of control. it helps establish the interaction between humans and robots by allowing them to influence each other's action selection and decision making."
"c )}. moreover, it can be seen that e is independent fromx a andx b . we then minimize the variance of the random variable n eq (whose particular realizations are n eq ) by choosing α as"
"to handle multiple tasks of pushing obstacles, we implement a basic task allocation approach based on the contract net protocol [cit] . here is the process: 1) task announcement: initially, the human operator introduces the site clearing task t to an \"auctioneer\" agent built in the operator control interface. assuming there's a planner to decompose the task into subtasks and save them in a task queue. the auctioneer then announces the subtask one at a time to the robots. each subtask t i holds task specific information, such as the weight, the position of the obstacle to be removed and the destination. 2) bid submission: each idle robot submits a bid to the auctioneer including its current distance from the obstacle and its pushing forces. 3) winner determination: once bids are collected, the auctioneer then uses a greedy approach to determine the winning robot(s) for the current task. the winning robot(s) should have a summed force (f sum ) greater than the weight of the box. there are further two preferences. first, we favor robots that are closer to the obstacle and thus ensures fast completion time. second, we try to minimize f sum so that we allocate as little resources as possible to execute the current tasks. unsuccessful allocation will result in the subtask being reinserted back to the task queue. when multiple robots are allocated to the same task, they collaborate to push the box to the final destination. if a robot fails during the collaboration, its team member will inform the failure to the ground station. the operator can either reinsert the task back to the task queue and set the rest of the team members to be idle, or manually allocate another agent (robot or human) to complete the task."
we use the open source robot operating system (ros) [cit] as our robot controller. we also designed a control interface using qt (see figure 5 ).
"to apply the sliding autonomy mechanism to a multi-robot application with requirement for tightly-coupled cooperation, we also need to include other components to facilitate the human robot collaboration and communication between multiple robots. we describe these components in the following sections."
"when multiple robots work collaboratively as peers, we often rely on explicit communication when communication is reliable and bandwidth is abundant. however, in the case of human robot collaboration, explicit communication is difficult since it will require the operator to carry a wearable or portable device with him/her, which will make it harder for the operator to interact with the robot. it is also more natural for humans to communicate through dialogue or observation rather than reading some encoded messages. thus, we apply implicit communication when the operator interacts with the robot(s) as peers. the operator can observe the status of task and the robots since they directly work together at the task site. robots can also observe the status of the task or the actions of the operators through sensory feedback."
"to measure the performance of our sliding autonomy control, one expert user and one novice user each performed ten trials under every control mode. we collected data on completion time (in seconds), solution quality and operator workload. when measuring solution quality, one point is awarded for a box being pushed close enough to the wall, a half point awarded for incomplete pushes, and 2 points for completion time of under 5 minutes. the operator workload is determined by having the operator fill out the nasa tlx (task load index) survey [cit], with scores ranging between 1 and 100. the data in this area is very subjective, but still shows expected trends. table ii compared the averaged results of using sliding autonomy control by an experienced operator and a novice operator respectively. from the experimental results, we could see the improvement of using sliding autonomy over the fully autonomous systems. the different levels of control allows the system to deal with degraded performance as well as faulty conditions. there is an increase in solution quality from autonomy to system-initiative, mixed-initiative and teleoperation modes, but also with an increasing workload on the human operator and more time for completion. comparing the systeminitiative mode with the mixed-initiative mode, we notice that when the operator has the ability to intervene early, the overall solution quality can be improved, and so is the completion time, with an increasing workload. we expect that teleoperation has the best solution quality but the worst completion time and workload. the autonomous mode has the best completion time and workload but the worst solution quality. the other two modes sit in between autonomy and teleoperation."
"we ran each mode 5 times and averaged the results. table iii showed the completion time, success rate and operator workload for each operation mode. the workload is measured by the amount of time that requires an active participation of the human operator. there are many factors that can influence the success rate of the execution. for example, the box gets pushed off the course during the execution and the robots fail to detect it. additionally, in the autonomous mode with a single robot, it was difficult to push the box along a straight line towards the goal and we had to constantly adjust the course. this is due to the fact that the robot is of a round shape and it is relatively difficult to find the right point on the box to push such that its orientation would be maintained."
where s g (θ) is the power spectral density of g. note that e(t) is in general not gaussian since the pre-cursor isi is in general not gaussian. it should also be noted that we do not literally perform decision-feedback equalization at the relay. instead we adopt the lattice precoding at both nodes to get rid of the post-cursor isi. the reason of this substitution will be discussed later.
"the communication is said to be reliable if this p e can be made arbitrarily small as the block length n goes to infinity. moreover, we say an exchange rate is achievable if there exists a coding scheme with this rate and a corresponding decoding function such that the reliable communication is possible. therefore, the exchange rate can be defined as the maximum possible rate for both nodes to exchange their information reliably with the same rate. i.e.,"
"where s a (t) and s b (t) denote the post-cursor isi induced respectively by node a and node b and e(t) is the sum of precursor isi and the filtered awgn noise. note that e(t) can be regarded as the error obtained by the unbiased mmse-dfe equalizer. thus, the variance of this error can be computed as"
"therefore, the achievable exchange rate is given by (24). we then maximize the exchange rate through the power allocation strategy that maximizes r ex subject to (32), (33), and the total power constraint."
"from the physical experiments, we showed that a single operator can flexibly switch between different modes to help the robots when necessary. by incorporating the peer-topeer mode, we also showed that the team can improve its performance and even deal with situations that robots cannot handle by themselves."
"the operator gets to select the appropriate level of control depending on his/her work load and the status of task execution. our goal is not to overload the operator's job; especially when a single operator needs to attend to the entire multi-robot team rather than a few robots. in a semiautonomous mode, a human operator is typically expected to share critical information with the robots (e.g., a new waypoint) or provide minimum guidance or assessment on the current situation (e.g. the camera view is blocked). thus it only requires bounded levels of human interaction but not full attention. the human operator can still multi-task while helping the individual robot. the teleoperation mode provides more precise control by constantly sending direct commands and controls, it however requires operator's full attention. there is also the challenge of providing enough situation awareness."
"f (m) is the fine lattice and λ (n) (m) is the coarse lattice (shaping lattice) where m represents the sub-channel index and n denotes the lattice dimension. the second moment per unit dimension of the coarse lattice is denoted as p λ (m)/2. nodes a and b first respectively map their messages onto lattice codewords t a (m) and t b (m) and then transmit the dithered version of these codewords on the m th sub-channel. the frequency domain transmitted signalsx a (m) andx b (m) are given bỹ suppose a sufficient cyclic prefix is inserted at both nodes and is perfectly removed at the relay, after a discrete fourier transform the equivalent frequency domain received signal is given bỹ"
"it is easy to verify that this problem is convex since its objective function is concave and all constraints are convex. therefore, one can efficiently solve this optimization problem and obtain the optimal power allocation strategy."
"so that two lattices are perfectly aligned at the relay. upon receiving, the relay first performs unbiased mmse equalization so that at each time index t we have"
"as a motivating example, consider a site clearing task that requires a specific area to be cleared of obstacles, which we simplify to be box-shape obstacles with different weights or sizes. the objective of the task is to clear the site in as little time as possible. the task can be decomposed into a series of subtasks with each removing one obstacle. we assume there is a general planner that determines the ordering constraints of subtasks. note that obstacles come with different weights, and thus may require multiple robots to push them depending on the capabilities of the robots (e.g., push forces) and the weight of the box. when multiple robots are needed, they need to work closely with each other to manipulate the box to the goal location. we expect that robots can accomplish the tasks autonomously most of the time. however, human operator may be needed to help the robots in special situations. for example, when the sensor on department of computer science, california state polytechnic university pomona, 3801 w. temple ave., pomona, ca ftang@cpp.edu a robot is obstructed by a piece of cloth, wheels get stuck, or extra forces are needed to push a heavy box."
"as a result, teleoperation has the highest success rate, however, the robot is hard to teleoperate and thus takes the longest time to complete the task. teleoperation also requires full operation attention for execution. the autonomous mode has the lowest success rate with an average completion time due to the fact that it is relatively difficult for one robot to push a large box. while peer-to-peer mode has a better success rate and completion time because the human operator can help the robot to push the large box together in a more effective way."
"our approach to sliding autonomy implements different levels of interactions to enable the human operator and the robot team to accomplish a task in a collaborative manner. under sliding autonomy, the system can dynamically switch between autonomy, semi-autonomy, teleoperation, and peerto-peer interaction modes depending on the situation. the different levels or modes represent an increasing level of human involvement in task execution. by default, all robots start in an autonomous mode. full autonomy is most desirable when the robots have the capabilities to handle the task with efficiency. however, we also recognize that robots work in a dynamic world with unforeseen uncertainties, which cannot be easily handled with an autonomous solution. thus, human operators can assist the operation of robots at critical times when robots face irresolvable issues. they can change the mode of operations at anytime when an autonomous mode has a degraded performance or fails, see figure 1 ."
"in the mac phase, since we wish to again make use of the addition property of lattice codes, we need to somehow enforce the lattices chosen at node a and node b to be perfectly aligned at the relay. to this end, we introduce linear filters f a and f b respectively at two nodes so that the transmitted signals become"
"for the m-th sub-channel, we choose an appropriate (in the sense that it achieves the awgn capacity in the limit of dimension) nested lattice code (λ"
"for cases that robots cannot handle solely by themselves even with the shared information or direct control, the human operator will need to physically work with the robots as peers to accomplish the task. we include this peer-to-peer mode in our model, which is the mode that will require the operator's full attention and mechanisms for the peers to communicate and interact with each other. no matter what mode the robot is in, the system will resume in an autonomous mode after the problem is resolved. the core concept behind the sliding autonomy approach is to combine the control methods of both autonomy and teleoperation in a way that best highlights each control method's advantages."
"for the sliding control, we designed it such that the robot can switch between autonomous, teleoperation and peer-topeer mode. all robots start with the autonomous mode. the operator will constantly observe the execution status and decide to select the appropriate operation mode. the operator can intervene at anytime to switch modes, while the robot(s) can also initiate a help request. robots can also resume autonomous mode at anytime if the operator decides to switch back from other modes."
"the superiority of the time domain scheme over the frequency domain scheme in fig. 3 can be explained by the corresponding channel transfer functions which are more mismatched than that in fig. 2 . thus, the frequency domain scheme may end up in wasting more power to the sub-channels that are mediocre for both directions and discarding some channels which are very good for one direction but very poor for the other. on the other hand, instead of channel gains at individual sub-channels, what really matters in the time domain scheme are the effective snr obtained by the unbiased mmse-dfe (25) and (27). therefore, this severely mismatched channel condition provides an opportunity for the time domain scheme to outperform the frequency domain scheme."
"in our proposed approach, human and robots communicate both explicitly and implicitly. human operators and robots exchange information depending on different levels of interaction between them. data communication is bidirectional. at a high level, operators assign tasks to robots with task specification such as defining the goal position for the robots to achieve. robots inform operators of their current task-execution status, for example, informing the operator the completion of its current task. at a low level, the operator may teleoperate robots through direct commands, or exchange sensing or computational information with robot as needed."
"irrigation communities are corporations attached to basin organizations, which are responsible for the management of the shared use of public common water. to this end, they relied on automatons and specific irrigation systems for years, which were tightly coupled, allowing for full interaction between the devices and the software, but avoiding any other possibility of interacting with third-party devices and software applications."
"motivated by the digital transformation, the agricultural sector is providing its farms with new devices and services (sensors, actuators, weather information, drones, and satellite images) that allow for the optimization of the resources, to improve productivity and simultaneously reduce the impact on the environment. for the digital transformation of the agricultural sector, a common integration framework is needed that unifies the entire dataset, so that new services are generated according to the needs of the sector. the data generated by the farms are crucial and have to be part of a unique semantic model that organizes and evaluates all the data collected."
"multiple problems exist in the domain of agriculture, such as irrigation, the application of pesticides and fertilizers, and the monitoring of crops, land, and livestock. different researchers are working to provide the best possible solutions:"
"the basis of many decisions is the uniformity of architectures, and etsi isg cim provides the correct interfaces (006 v1.1.1 (2019-07)) and context information management (cim) and allows for the unified representation of the information through next generation service interfaces with linked data (ngsi-ld) [cit] . this reduces the time and resources required for the management of daily tasks and for the improvement of the productivity and benefits."
"society is characterized by uncertainty and constant technological changes. the productive sectors that are more digitized tend to improve their productivity faster than those that are less digitized. irrigation farming communities have benefited for many years from the use of information and communications technology (ict), which allows them to improve the current agricultural development, facilitate the fulfillment of daily tasks, and convert farms into efficient and sustainable production systems. the use of these new technologies not only enables the digital transformation process demanded by the sector, but also supports irrigation communities and other farming groups with tools for analysis and prediction, improving optimization."
"header figure 4 shows the general scheme that was followed to address the deployment of the platform. the first step was to channel water from the headwater reservoir, with its corresponding water analyses, to the agricultural plots. the plot is one of the key elements of the system. all the devices needed for its definition are integrated using dataloggers and iot connectors. through this process, scada data and the sensors that interact with the plot (soil moisture sensors, agricultural weather station, and hydrants) are integrated into the systems. the information is transferred to the data model for later visualization (using mobile or web applications) and analysis by the users of the irrigation community. the deployment involved the installation of controllers for monitoring, and remote control in outdoor environments were provided by odin solutions [cit] . the cost of implementing the platform was reduced due to the open source feature of the platform. this feature decreased the budget needed for dataloggers managing different devices or sensors in the platform. in this instance, the cost of the chosen datalogger, ipex12, has an average price of around 600 euros (including configuration and installation) and varies according to the number of sensors integrated into the system. the main characteristics of ipex12 are: 32-bit cpu and 4 mb of memory expandable with microsd, ethernet, usb, can, 3xrs232, 1xrs485, and 12 i/o ports, which can be configured by software as digital or analogue input/outputs. it supports third-generation networks (3g), ipv6 over low power wireless personal area networks (6lowpan), sigfox, narrow band iot (nb iot), and lora. each ipex12 is able to manage eight additional slaves i/o boards using can. the controllers are configured effortlessly through their web server, which permits us to configure tasks independently from the mqtt commands received from the cloud computing layer. ipex12 is specially designed for outdoor settings such as the agricultural scenarios, since it is provided with water-and dust-proof enclosure. it is provided with a battery and a solar panel."
"as a result of the use of the platform and the analysis of the data obtained, and with the help of an agricultural technician, the relevant irrigation limits are established or adjusted. in addition, other useful parameters can be established, such as the root network, the type of soil, or the disposition of the probes. once the irrigation programming was changed, figure 9 shows that the irrigation was efficiently applied. the water contributions and vertical column are in line with the humidity curves, showing stable and uniform sharing. by controlling the hydrants and applying filters based on the limits set, irrigation was controlled to maintain the humidity within the desired parameters, or to inform of possible alerts to activate programmed irrigation to avoid a reduction in production. the versatility of the platform is stressed by the ability to export selected data for further processing or study, as shown in figures 8 and 9 using the export to csv option."
"once the different modules that form the platform were configured, the entities were defined in the broker so that their subscriptions could be made later. the broker manages subscriptions to the creation or changes of entities. all these changes are stored in mongodb, a nonrelational json-ld object-based database, to provide fast and flexible access to information. the speed of processing is fundamental because, otherwise, a bottleneck is created that would slow down the rest of the implementations."
"is article selects the single-point crossing method, which is located in lines 13-20 of the pseudocode in algorithm 2. in this way, individual selectivity is enhanced. generate a random number between [cit] . if the number is less than or equal to the crossover operator cr, then select a certain dimension of the individual randomly and execute the crossover operations at the selected point. a large number of experiments have confirmed that when the value of cr is 0.4, the effect is better."
"e definition of spatial density is obtained by averaging the distances of similar individuals in the population. e minimum spatial density is defined as h 1 ρ o . e maximum spatial density is defined as h 2 ρ o . under normal circumstances, when the value of h1 is 0.2 and h2 is 1.3, relatively good results can be obtained. note ρ o is the density of the overall objective space and ρ i is density of the subspace. e adjustment process is divided into two situations described below:"
"(2) when the subspace density is greater than the objective space density, determine whether the subspace density is too large. if the density of the subspace is"
"the cloud server is a poweredge r7515 server, with an amd epyc 7402 2.80 ghz, 32 gb of ram, and two solid state drives (ssd) of 480 gb each. it also runs the ubuntu 18.04 lts server and libvirt/kvm. a single virtual machine was used here for the global broker, data saving, big data processes, and to host management with web services. the virtual machine uses 28 gb of memory and 400 gb of hard disk capacity. a good performance is possible at the moment, but given the flexibility of our architecture, more resources could be added by modifying or moving virtual images."
"is paper proposes a many-objective optimization algorithm based on weight vector adjustment, which increases the individual's ability to evolve through new differential evolution strategies, and at the same time, dynamically adjust the weight vector by means of the k-means to make the weight vector as evenly distributed as possible on the objective surface. e nsga-iii-wa algorithm has good convergence ability and good distribution. to prove its effectiveness, the nsga-iii-wa is experimentally compared with the other five most advanced algorithms on the dtlz test set and wfg test instances. e experimental results show that the proposed nsga-iii-wa performs well on the dtlz test set and wfg test instances we studied, and the obtained solution set has good convergence and distribution. however, the proposed algorithm has high complexity and it only plays the role of alleviating sensitive frontiers. further research will be conducted on the above problems."
"the core of this platform is the ngsi-ld broker. it is an information broker that exposes an http rest api based on ngsi-ld for both registration and consultation, as well as a subscription/ notification approach. this is the second point where interoperability is a significant aspect. the use of ngsi-ld provides various possible interactions with third-party platforms in both directions. one of these solutions that has been integrated into the platform through the development of a connector is mega [cit] . the mega standard (iso 21622) provides guidelines for the implementation of a standardized model applied to irrigation that improves the specification of irrigation schedules and the control of the system requirements. within this management layer, the stored information of each entity is overwritten, with only the current state being kept; the information is stored in a nonrelational database, mongodb. this database contains data referring to infrastructure and information flow, as well as information about users and organizations. all this information is modeled using data models that allow for the unification of data structures using standards and also ensure the generality necessary for the subsequent extensibility."
"data interoperability is of critical importance. here, the proposed solution (data model and platform) provides the necessary data translation mechanisms by combining the use of a data model together with standardized solutions, such as those provided through ngsi-ld. this proposal enables the integration of multiple systems/devices or data sources, resulting in an open and interoperable data integration model according to the changing requirements of farms. the use of data models contributes to obtaining performance indicators, decision making, and sharing data with different farms. figure 8 shows the evolution of irrigation before the platform was installed. the analysis of the data revealed the irregularity of the irrigation system. to have a continuous and homogeneous irrigation system, we deemed it necessary to define irrigation thresholds provided by the agricultural technicians. as the field capacity and the wilt point determine the maximum and minimum limits of the soil humidity that can be used by the crops, we concluded that the amount of water between these two values is the functional water or humidity available to the plant. once the community technicians defined the limits for the crops, tests were conducted on an apricot tree plot to evaluate the irrigation efficiency: the first as soon as the platform was deployed (between march and april), and another after improving the irrigation programming based on the results of the first test (between june and july). once the first test was analyzed, figure 8, we found that during irrigation, two critical zones were generated, a and b, with extreme peaks that were outside the recommended limits, indicating that water use was not efficient, thus a scarce resource was being wasted. zone a indicates that root asphyxiation was produced (a limitation of the capacity of the plants to breathe through the roots) in the crop by excess water; zone b indicates that the plant does not have ability to supply itself with water, consequently reducing the quality of the production of the crop. once the maximum and minimum thresholds are set, rules can be defined for efficient irrigation. when the thresholds are exceeded or not reached, the platform generates notifications to users through the mobile application. these rules can also be associated with the activation of digital inputs for the start-up of any device that improves irrigation efficiency."
"(1) when the subspace density is less than the objective space density, determine whether the subspace computational intelligence and neurosciencedensity is too small. if the density of the subspace is less than the minimum space density h 1 ρ o, then the subspace density is considered too small. in this case, the weight vector should be evacuated. at this point, using the two nearest neighbor weight vectors and adding their sum vectors to the set of weight vectors, the parent vectors are deleted. otherwise, the weight vector should be fine-tuned to achieve uniformity across the objective plane. at this time, according to the density difference, the nearest two weight vectors in the subspace are adjusted according to (7) and (8) . among them, the vectors w_unit(k) and w_unit(l) are the closest weight vectors. let mt be the minimum distance. let ρ i be the density value. e vectors w_unit(gwk) and w_unit(gwl) are neighbor weights of the respective weights."
"e experimental results are shown in tables 4-6 . ey are the average values and standard deviations of 30 independent running results. e best results are shown in black and bold, and the values in parentheses indicate the standard deviation; the number in square brackets is the algorithm performance ranking, which is based on the whitney-wilcoxon rank-sum test [cit] . to investigate whether nsga-iii-wa is statistically superior to other algorithms, wilcoxon's rank-sum test is performed at a 0.05 significance level between nsga-iii-wa and each competing algorithm on each test case. e test results are given at the end of each cell, represented by the symbols \"+,\" \"�,\" or \"−,\" which indicate that the nsga-iii performance is better than the algorithm in the corresponding column, equal to, and worse. at the same time, the last row of tables 4-6 summarizes the number of test instances that nsga-iii-wa is significantly better than, equal to, and below its competitors. tables 7-9 show the results of the comparison of nsga-iii-wa algorithm with the other five algorithms under different objective numbers."
"the behaviour of the platform was evaluated based on the information distribution characteristic that the broker had acquired. to this end, the response time obtained after performing various operations on the different broker operating modes is analyzed; table 2 shows the most significant values obtained in the tests. the graphs shown in figure 6 show the times for the different modes: entity management, subscription management, and context provider control."
"at the european level, an open initiative, fiware [cit], offers a framework using a set of standard and open application programming interfaces (apis) based on next generation service interface (ngsi), promoted by the open mobile alliance (oma), or next generation service interfaces with linked data (ngsi-ld) [cit], promoted by etsi industry specification group for context information management (etsi isg cim), to define a universal set of standards based on the contextual management of the data. therefore, adhering the standards and abandoning the code and proprietary technologies can contribute to improving the competitiveness of the agricultural sector. in a world where everything is connected, isolated solutions have no place. by contrast, interoperability must be the cornerstone feature that all solutions adopt, thus contributing to a more productive environment where information is exchanged for a greater good. the proposed solution is a platform that allows for the integration of different suppliers' devices, and implements standards and open interfaces and data models based on ngsi-ld. additionally, the platform exploits the stored information to analyze, in real time, the factors associated with the production process, the evolution of the crops, and the optimal use of water for irrigation. the core features the platform are as follows:"
"from table 5, the nsga-iii-wa can get the best results especially the objectives 5, 10, and 15 on the dtlz1, 8, 10, and 15 on the dtlz2 and dtlz3, 3, 5, and 8 on the dtlz4, and 5, 8, and 15 on the dtlz5. moreover, the objectives 3 and 8 on the dtlz1, the objectives 3 and 5 on the dtlz2, the objective 3 on the dtlz3, and the objective 10 on the dtlz5 achieve the second best results. nevertheless, on the dtlz5 and dtlz6, the results of nsga-iii-wa are not significant because the dtlz5 and dtlz6 are used to test the ability to converge to a curve. owing to the reason that nsga-iii-wa needs to build a hypersurface, m extreme points cannot be found in the later stage of the algorithm to construct the hypersurface, and it cannot converge to a curve well. in addition, nsga-iii-wa has noticeable effects on other test functions and is a kind of stable and relatively comprehensive algorithm. table 8 shows summary of statistical test results from table 5 . it can be seen from the table that nsga-iii-wa performs best on the other five algorithms."
"as a result of the lack of the compatibility necessary to manage the large amount of data generated by the agricultural sector, agricultural managers must be provided with standardized information models that permit farmers to make the best possible decisions, allowing them to take advantage of the available data and knowledge. therefore, a data model is proposed here that fits the functional requirements of the users."
"computational intelligence and neuroscience e population r t is subjected to nondominated sorting to obtain layers of individuals with nondominated levels (f1, f2, and so on). individuals with nondominated levels are sequentially added to the set s t of the next generation of children until the size of the set s t is greater than n. e nondominated level at this time is defined as the l layer. pick k individuals from the l level so that the sum of k and all previous levels is equal to n. prior to this, the objective value is normalized by the ideal point and the extreme point. after normalization, the ideal point of s t is a zero vector, and the provided reference point is located exactly on the normalized hyperplane. e vertical distance between each individual in s t and each weight vector (connect the origin to the reference point) is calculated. each individual in s t is then associated with a reference point having a minimum vertical distance."
"a diverse range of ontological models are available for the iot, developed with different objectives [cit] that describe various areas, such as fiesta-iot ontology, which reuses the results of projects and current eu strategies in semantic web technologies, such as openiot, dul, vital, spitfire, iot-o, iot-a, iot-lite, and sensei. one recent ontology, which is promoted by the etsi smartm2m technical committee, is the smart appliances reference (saref) ontology, a shared model of consensus that facilitates the matching of existing assets in the smart appliances domain and allows for the separation and recombination of different parts of the ontology according to specific needs. for the agricultural field, the saref4agri version [cit] was created to provide services for animal husbandry, intelligent irrigation, and the integration of multiple data sources to provide support services for decision making."
"from the results in table 12, it can be seen that nsga-iii-wa has obtained the best performance for most of the highdimensional objective problems. nsga-iii works well on the wfg1 and wfg2 test instances, and vaea also gets good results on the objectives 10 and 15 on wfg3 test instances. rvea obtains the best hv value on the objectives 3, 5, and 10 on wfg4. moea/d and moea/d-m2m are not quite effective on these five instances. table 13 shows summary of statistical test results from table 12 . e three-dimensional performance of the nsga-iii-wa algorithm is not very prominent."
"en, in order to ensure the uniform distribution of weight vectors in the solution surface, the objective space is divided into several subspaces by clustering the objective vectors. according to the adjustment of the weight of each subspace, the spatial distribution of the objective is improved. we will carry out simulation experiments on the dtlz standard test set [cit] and wfg standard test set [cit] . we compare the proposed algorithm with the five algorithms that are currently performing better on the optimization problem of 3 to 15 objectives. e gd, igd, and hv are compared as performance indicators. e experimental results show that nsgawa has good effect in convergence and distribution. e rest of the paper is organized as follows. section 2 introduces the original algorithm. section 3 describes the proposed many-objective evolutionary algorithm. section 4 compares the similarities and differences between this algorithm and similar algorithms. section 5 gives the experimental parameters of each algorithm and comprehensive experiments and analysis. finally, section 6 summarizes the full text and points out the issues to be studied next."
"e overall performance indicators achieve the best results on the dtlz1 and dtlz4 and get the second best results on the dtlz2 and dtlz3. although the minimum value is obtained on the dtlz5 and dtlz6, there exist abnormal values, indicating that the algorithm is relatively unstable. is is because dtlz5 and dtlz6 test the ability of the algorithm to converge to a straight line, while nsga-iii-wa needs to build a hypersurface, so it cannot converge to a curve well. however, the overall robustness of nsga-iii-wa is relatively better with all test function results."
"to improve the data of the irrigation plots, the data model proposed in section 2.1 was used according to the needs proposed by the users of the irrigation community. a model instantiation was constructed through ngsi-ld of an apricot tree plot where a soil moisture sensor was installed. the soil moisture sensor is listed in listing 1 and the crop is defined in listing 2. listing 1. next generation service interfaces with linked data (ngsi-ld) example representing an agridevice."
"e evolutionary strategy is essential to the convergence speed and accuracy of the solutions because it will determine the quality of new solutions to subquestions directly during evolutionary process. in order to improve the convergence speed, this paper proposes a new differential evolution strategy to replace the original strategy."
"e overall performance obtains the best results on the dtlz1-dtlz3, dtlz5, and dtlz6. e nsga-iii-wa under the 15 objectives can get the minimum on dtlz5 and dtlz6 but is relatively unstable. e breadth achieves the best results on the dtlz1, dtlz3, and dtlz4 and gets the second best results on the dtlz2, dtlz5, and dtlz6, and there exist abnormal values. on the 15 objectives, it is evident that the outliers of each algorithm increase. at is explained by the fact that the stability of algorithms in the high-dimensional space will decline due to the increase of the spatial breadth. depending on the results of all test functions, nsga-iii-wa has better stability."
"first, each weight vector is associated with the population member (line 1 in algorithm 4). secondly, the solution space is decomposed into many subspaces using the k-means clustering method (lines 2-5 in algorithm 4), as shown in figure 1 . to prevent errors caused by excessive differences in the solution set in space, the subspace should not be too large or small, and it can be divided into c spaces according to the size of the population. a large number of experiments confirmed that when c � 13, better results can usually be obtained. e solution set is decomposed into [n/c] cluster spaces, and the weight vectors are adjusted by comparing the density of the entire objective space and subspaces (lines 6-17 in algorithm 4). as shown in figure 2, w2 should be away from w1 and approach w3. finally, the number of weight vectors is adjusted to ensure that it can match the original number. if the number is greater than n, then the weight vector is deleted at the densest position in the entire objective space. if the number is less than n, then a weight vector is added in sparse position (lines 18-20 in algorithm 4)."
"from figures 9-14, it can be seen that the nsga-iii-wa has the ability to handle most problems under the 15 objectives. e convergence on the dtlz1-dtlz5 is significantly better than the other five algorithms."
"in order to further improve the convergence and distribution of many-objective algorithms, based on nsga-iii, a many-objective optimization algorithm (nsga-iii-wa) based on weight vector adjustment is proposed. first, in order to enhance the exploration ability of the solution to the weight vector, an evolutionary model in which a novel differential evolution strategy and a genetic evolution strategy are integrated is used to generate new individuals."
"finally, the niche operation is used to select members from f1. a reference point may be associated with one or more objective vectors or there are also possibilities that none of the objective vectors is associated with a reference point."
"this section validates our data model and the architecture of the platform. it was implemented in an irrigation community where different systems, such as supervisory control and data acquisition (scada), gis systems, and sensors from different manufacturers, were integrated by the platform."
"it can be seen from the experimental results in table 4 computational intelligence and neurosciencesuperior to the other five algorithms, only 8th, 10th, and 15th dimensions are superior on the dtlz5, and nsga-iii on the dtlz6 gets the best results. it shows that in solving many-objective problems, the convergence of nsga-iii-wa is more effective than that of nsga-iii algorithm and is better than other algorithms. table 7 shows summary 10 7.146e + 00 (3.182e − 02) 7.071e + 00 2 (5.709e − 02) 7.238e + 00 + (4.083e − 02) 7.182e + 00 + (4.281e − 02) 9.541e + 00 + (3.219e − 01) 7.816e + 00 + (9.023e − 02) 15 8.942e + 00 (1.284e − 01) 9.079e + 00 + (1.673e − 01) 9.057e + 00 + (3.073e − 01) 9.149e + 00 + (3.726e − 01) 1.183e + 01 + (2.961e − 01) 1.235e + 00 + (3.618e − 01) wfg2 3 2.149e 2 01 (6.137e − 02) 2.839e − 01 + (1.040e − 01) 3.218e − 01 + (8.931e − 02) 3.157e − 01 + (4.366e − 02) 1.317e + 00 + (7.013e − 02) 3.714e − 01 + (4.827e − 02) 5 5.237e 2 01 (9.814e − 02) 6.125e − 01 + (1.375e − 01) 9.052e − 01 + (2.781e − 01) 7.026e − 01 + (1.437e − 01) 3.971e + 00 + (2.739e − 01) 1.411e + 00 + (3.894e − 01) 8 2.316e + 00 (1.835e − 01) 3.146e + 00 + (1.379e − 01) 2.007e + 00 2 (2.371e − 01) 2.572e + 00 + (1.638e − 01) 8.837e + 00 + (5.462e − 01) 2.885e + 00 + (4.732e − 01) 10 2.037e + 00 (2.171e − 01) 2.923e + 00 + (4.518e − 01) 3.592e + 00 + (4.178e − 01) 2.964e + 00 + (3.926e − 01) 1.027e + 01 + (1.001e + 00) 2.1416e + 00 � 8 1.308e + 00 (2.748e − 02) 1.709e + 00 + (1.061e − 01) 1.427e + 00 + (2.135e − 02) 1.604e + 00 + (3.375e − 02) 1.829e + 00 + (4.873e − 02) 2.487e + 00 + (3.823e − 02) 10 1.864e + 00 (2.073e − 02) 2.176e + 00 + (2.874e − 01) 1.725e + 00 2 (3.271e − 02) 1.845e + 00 � (4.273e − 02) 2.966e + 00 + (7.627e − 02) 3.369e + 00 + (6.379e − 02) 15 2.815e + 00 (2.733e − 01) 4.206e + 00 + (1.537e − 01) 2.963e + 00 + (2.736e − 01) 3.028e + 00 + (1.893e − 01) 5.265e + 00 + (8.733e − 02) 6.738e + 00 + (1.284e − 01) wfg4 3 2.043e 2 01 (2.274e − 03) 2.147e − 01 + (3.859e − 04) 2.317e − 01 + (7.352e − 03) 2.272e − 01 + (3.72e − 03) 2.475e − 01 + (3.758e − 03) 3.581e − 01 + (3.146e − 03) 5 9.635e − 01 (3.762e − 03) 9.865e − 01 + (4.873e − 03) 9.535e − 01 + (5.378e − 03) 9.526e 2 01 2 (3.288e − 03) 1.284e + 00 + (3.725e − 02) 1.676e + 00 + (1.003e − 02) 8 3.021e + 00 (4.887e − 03) 3.262e + 00 + (6.256e − 03) 3.023e + 00 � (1.567e − 02) 3.114e + 00 + (6.331e − 03) 6.642e + 00 + (1.526e − 02) 4.6209e + 00 + (2.462e − 02) 10 4.063e + 00 (1.879e − 02) 4.621e + 00 + (2.834e − 02) 3.982e + 00 � table 4 . nsga-iii-wa is compared to five other more advanced multiobjective algorithms and counts the number of wins (+), equal to (�), and number of loses (−). as can be seen from the table, nsga-iii-wa is clearly superior to the five most advanced designs selected."
"from table 6, it can be seen that the nsga-iii-wa can effectively handle most test problems. it can get the best results especially the objectives 3, 8, 10, and 15 on the dtlz1, 5 and 10 on the dtlz2, 3, 5, 10, and 15 on the dtlz3, 10 and 15 on the dtlz4, 5, 8, and 10 on the dltz5, and 3, 5, and 8 on the dtlz6. moreover, the objective 5 on the dtlz1, objective 15 on the dtlz2, objectives 3 and 5 on the dtlz4, objectives 3 and 15 on the dtlz5, and objectives 10 and 15 on the dtlz6 achieve the second best results. however, the performances of objective 3 on the dtlz2 and 8 on the dtlz3 are poor. although nsga-iii, vaea, rvea, and moea/d can obtain optimal values for a particular dimension in the function, nsga-iii-wa has the best overall performance considering all dimensional objective results. table 9 shows summary of statistical test results from table 6 . as can be seen from the table, nsga-iii-wa hypervolume performance is better than the other five algorithms."
"this section provides an evaluation of the response times of the platform based on the three layers that form its architecture. figure 7 shows the times obtained for the system after the execution of 100 requests. figure 7a shows the notification times at its two levels: at mqtt level (mqtt broker notifies the iot agent) and at ngsi-ld broker level. the times of the information management layer are longer, with an average of 21.69 s, since all the semantic enrichment provided by the use of the data model is performed in the database (mongodb). however, in exchange for this increase in time, it is possible to have all the relevant information of the irrigation community homogenized and integrated into a single point. figure 7b shows the system's capabilities in terms of performance management. the graph shows that if the device is connected, the response time is less than if it is not; the average response time of 31.18 ms is considered low."
"currently, some resources speed up the data modeling of a specific location or sector. in the field of agriculture, [cit] presented different types of tools to help us develop this task:"
"many-objective optimization problems (maops) [cit] refer to optimization problems whose number of objectives is over three and need to be processed simultaneously. as the number of objectives increases, the number of nondominated solutions will grow explosively in the form of e exponent [cit], most of the solutions are nondominated, the pros and cons between the solutions become more difficult to be evaluated, and namely, many-objective evolutionary algorithms (moeas) have poor performance in convergence. in addition, the sensitivity of the pareto front surface [cit] will increase along with the increase of the spatial dimension, which makes it difficult to maintain the distribution among individuals."
this section describes the validation of the platform in a community of irrigators from three different perspectives. the tests were conducted under a controlled environment with the equipment described in section 3.2.
"from figure 15, it can be seen that nsga-iii-wa and rvea find the final solution set in this problem to be similar in convergence and distribution. in contrast, moea/d-m2m and nsga-iii are slightly less distributed than the above three algorithms. vaea finds that the distribution of the solution is poor. moea/d appears the concentrated solution. lose extreme solutions at 12 objectives and the distribution of moea/d is seriously missing."
"the digital transformation and continuous technological advances that are currently occurring, e.g., the internet of things (iot) [cit], big data, cloud computing [cit], artificial intelligence, and aerial images [cit] to name a few, are providing the agricultural sector with new tools that help determine the real needs of farms and improve their efficiency. specifically, the use of the iot has changed the traditional paradigm regarding access to and the management of sensors and actuators, making all objects accessible through the internet, and transferring the management and integration of information and real knowledge into the digital world [cit] . this transformation has produced improvements compared to the techniques traditionally used [cit], allowing new mechanisms to help in the management of farms. these advantages are mainly focused on the following:"
"in the future, the speed of computing and latency should be compared by replicating the platform in edge computing with current cloud computing to detect which one provides better performance and response times. one of these involves the processing of satellite images. at the plot level, this feature will monitor the main characteristics of the crop with applications based on remote sensors. this new attribute will focus on the diagnosis, management, and control of irrigation. currently, this feature uses images of the sentinel 2 satellite, which is more focused on agricultural issues, but soon the paz satellite (the first spanish radar earth observation satellite, which is included within the national earth observation programme) will be used. another future line of research is the development of a data analysis module that will allow us to cross check and analyze the history of the data acquired. this will generate better results in agricultural management, increase productivity, and reduce expenses, such as by reducing the consumption of water, nutrients, and phytosanitary products. funding: [cit] research and innovation program through the demeter project (contract: 857202), by the autonomous community of the region of murcia and the european regional development fund (feder/erdf) through reusagua project (ref. 2i16sae00165) under the ris3mur programme, and by the european regional development fund (erdf) through project feder 14-20-25 \"impulso a la economía circular en la agricultura y la gestión del agua mediante el uso avanzado de nuevas tecnologías-iagua\"."
"it is worth emphasizing that the edge vector is immovable; otherwise, the search range of the algorithm will be affected. half of the maximum number of iterations was selected as an enabling condition for the weight vector computational intelligence and neuroscience adjustment strategy and adjust every four generations in this paper. in this time, the objective vectors have approached the pfs, so the guidance of the weight vectors is relatively accurate, and the population update is relatively stable (i.e., it is close to the pf)."
"e performance under the eight-dimensional algorithm is the same as that of the rvea algorithm, but the nsga-iii-wa algorithm can achieve better performance in high-dimensional objective problems. in general, the nsga-iii-wa algorithm outperforms the other five algorithms in this performance."
"e purpose of the niche operation is to select the k closest reference points from the f1 layer into the next generation. firstly, calculate the number of individuals associated with each reference point in the s t /f l population and use ρ j to represent the number of individuals associated with the jth reference point. e specific operation is as follows:"
"e performance indicators of the wfg test function are mainly from the results in table 10, it can be seen that nsga-iii-wa can handle most of the considered examples well. in particular, it achieved the best overall performance on the objectives 3, 5, and 10 on wfg2 instances and the objectives 5, 8, and 15 on wfg3 instances. in addition, it achieves the best performance on the objective 15 on wfg8 and the objectives 3 and 8 on wfg9. e vaea performed well on the objective 8 on wfg1 and wfg2 test instances and also achieved good results on the objectives 3 and 10 on wfg3 and the objective 4 on wfg3. rvea obtains the best igd value on the objective 3 on wfg1 and the objectives 5 and 10 on wfg4. it is worth noting that rvea performs poorly for wfg2 and wfg3 instances. but it performs relatively well compared to the nsga3 and moea/d algorithms. nsga-iii and moea/d-m2m typically have moderate performance on most wfg problems, and good results can only be achieved on specific wfg test instances. moea/d does not produce satisfactory results in all wfg test instances. as the number of objectives increases, the results gradually deteriorate. table 11 shows summary of statistical test results from table 10 . it can be seen from the table that the performance of nsga-iii-wa is significantly better than that of the other five algorithms."
"the remainder of this paper is structured as follows: in section 2, the proposed solution is introduced, detailing the most unique elements. section 3 outlines the deployment in a community of irrigators. section 4 shows the evaluation of the system at different levels of implementation and the results obtained. finally, in section 5, the conclusion and new proposals based in these results are described."
"erefore, in order to improve the distribution of many-objective algorithms, a weight vector adjustment strategy whose framework is shown in algorithm 4 is proposed. e distribution of weight vectors is appropriately adjusted according to the shape of the nondominated frontier. in order to prevent the weight vector adjustment in the high-dimensional space from being concentrated to a certain objective, the k-means clustering method is used to divide the weight vector into different subspaces. e specific operations are described below."
"in order to verify the performance saliency of the proposed nsga-iii-wa algorithm on many objective optimization problems, general performance evaluation indicators gd, igd and hv were used. it is compared with five good algorithms, moea/d, nsga-iii, vaea, raea, and moea/d-m2m, and representative algorithms with the objectives 3, 5, 8, 10, and 15 on the dtlz1-6 test function and wfg1-4 test instances. is section shows the results and analysis of the gd, igd, and hv performance test data of the dtlz1-6 test function."
"the main element of the platform is a ngsi-ld broker, which is responsible for storing all the information as well as processing the different queries, answers, subscriptions, and the management of the information providers. for this reason, an analysis was performed by selecting the following metrics: cpu usage, memory consumed, and response times, according to the operations performed."
e initial population is randomly generated whose size is the same as the number of weight vectors in its space. is article uses das and dennis's systematic method [cit]
"given the above factors, interoperability mechanisms must be generated that allow for the integration of different devices, applications, or platforms to filter the vast amounts of information that many systems are producing [cit] . the shared data models are useful as they are essential resources that improve the communication, knowledge recovery, and interoperability of information systems to develop better applications for the agri-food sector [cit] ."
"1. scalability and flexibility:the platform, instead of being locked to a single provider, is up to date with protocols, technologies, and features that vary rapidly. it is network-independent and can be integrated to work with all vital technological systems; 2. interoperable: the platform offers a wide range of iot agents that facilitate the connection with devices that use standardized iot protocols, such as lightweight machine to machine (lwm2m) over constrained application protocol (coap), javascript object notation (json), or ultralight (ul) over http/mqtt, with the possibility of using parameterizing agents to integrate any other type of future protocols. at the application level, the platform is open to integration with other third-party platforms using both api and data model based on ngsi-ld."
"first, a population a of size n is set up, and population p t is operated by genetic operators (selection, reorganization, and variation) to obtain a population q t of the same size, and then population p t and population q t are mixed to obtain a population r t of 2n."
"this work demonstrated the importance of interoperability in the field of agriculture, water management, and irrigation systems. herein, an interoperable and open platform is presented which is capable of integrating heterogeneous data sources at the iot level to aid and improve the decision making in a community of irrigators. the platform enables the integration of controllers and sensors from different manufacturers, and thus represents a unique access point for all this information. as such, improving the techniques used in the agricultural sector is possible, thereby sustainably obtaining higher economic, environmental, and social yields. as a result of its interoperable nature, the platform can be combined with other systems to expand its range of services. the interoperability aspect was tested in a real-world context where successful integration with other systems already deployed in the facilities of the irrigation community was possible thanks to this characteristic. the platform integrates different protocols at the iot level, such as mqtt, coap, lora, sigfox, and http. they transport the information following different data formats. nevertheless, a set of them promote interoperability, such as lightweight machine to machine (lwm2m), json, and ultralight. these representation formats were adopted by the presented platform as to facilitate its integration into iot devices, making use of this technology. the agents of the iot backend of our platform perform the adaptation of this information to the ngsi-ld interface and data model. after analyzing the obtained results in the community of irrigators, the following conclusions were obtained: 1. a homogeneous data model was proposed that meets the specific needs of agriculture, such as efficient water management. this model was validated on the platform using an ngsi-ld broker. the application of this model in an irrigation community provides its managers with the capacity to manage the agronomic information and its relationship with the devices that provide information to improve irrigation water; 2. the platform was validated using metrics to check its behavior. firstly, the scalability of the main component, the ngsi-ld broker, was analyzed based on the average use of the cpu (83.21%) and memory (56.39%). at the latency level, measures were recorded on the most relevant operations of the broker, highlighting among them the average time, 510.28 ms, of the creation of a context provider. the platform was validated as a whole, showing the time delay from the moment the device receives the information and when it is received by the platform (2.97 s); and from the moment the action is performed (31.17 ms); 3. at the user level, the platform was validated in several apricot tree plots, improving the management of the water used. this improvement was achieved because of the definition of optimal irrigation thresholds for each crop and by generating filter notifications that, under certain conditions, allow the hydrants to be adjusted, thus enabling efficient water use."
"another feature to be considered is the aerial images management, represented by the agriscene entity, including images or scenes from any satellite or drone; these scenes are composed of bands, agriband, of different wavelengths. in turn, these bands can generate derivative products, agriproduct, after processing, such as mosaics (adding two or more adjacent scenes) or indices of vegetation or water, useful for detecting problems in the fields. the main feature of the iot-based interoperability mechanisms is the possibility of exchanging information homogeneously in different systems or applications. one of the elements that enables this continuous exchange of information is the use of data models. to acquire more significant value, the specific vocabulary of the sector must be unified, which is defined by the set of attributes that compose the entities and their relationships [cit] . as ngsi-ld allows us to link data from other vocabularies or ontologies, we added a level of interoperability to the model. for these reasons, the use of information models is considered beneficial to allow for the integration of any device or characteristic element of the study sector."
"firstly, different execution tests were conducted to check the platform ability to perform the changes without losing the quality of service. the tests were performed with the same type of requests, checking their response time in terms of both cpu and ram. all tests were based on a one-minute execution varying the number of simultaneous communications or connections with which it was performed. figure 5 shows the results of the tests of the behavior of the processor and the memory. figure 5a shows that the cpu requirements increased by around 20% in the first intervals (2, 4, and 8 threads) . from this moment on and up to 1024 threads, the cpu remained stable with average values close to 90%. figure 5b shows that the behavior in the use of the ram remains practically stable from the beginning to the end without altering the consumed resources. the increase in the use of the ram during the tests performed varied between 5% and 9%, values that are considered optimal."
"one of the main problems to be solved is the integration of the different technological elements (scada, drawing software, independent sensors, and third-party services) that provide information to the community in a single control point. the data from scada that are integrated into the platform are shown in table 1 . the results derived from the water analysis conducted at least twice a month were also integrated into the platform. table 1 . type of facilities from supervisory control and data acquisition (scada) that are integrated into the platform and the associated variables."
"some models are promoted by public administrations, such as the standardized water management model applied to irrigation model (mega) [cit], an initiative of the ministry of agriculture of spain, included in the iso 21622 standard and managed by tragsa group. the purpose of mega is to establish a standardized model that, when applied to irrigation, allows for interoperability between different systems that coexist in the same facility and thus to efficiently manage irrigation water use. the introduction of the standardized model improves how the irrigation schedules and the requirements of the control systems are specified, establishing a clear separation between decision making and execution."
"adjustment weight vectors by formulas (8) and (9) greater than the maximum space density h 1 ρ o, then the subspace density is too large. in this case, the weight vector should be aggregated. at this point, take the two furthest neighboring weight vectors and add their sum vectors to the set of weight vectors. otherwise, at this time, according to the density difference, adjust the weight vectors according to (9) and (10) . among them, the vectors w_unit(k) and w_unit(l) are the furthest weight vectors. note mx is the maximum distance and ρ i is the density value."
the use of the data model to homogenize the management of the different elements that are part of a farm; 4. efficiency and competitiveness: the model allows precise and timely decisions to be made in terms of management and agricultural processes. the ability to automatically document the health status of the crop or natural resources provides an efficient and effective diagnosis technique for managers.
"e previous section described the nsga-iii-wa algorithm in detail. in this section, we compare the similarities and differences between nsga-iii-wa, nsga-iii, and vaea. e nsga-iii-wa algorithm divides the objective space into several subspaces and adjusts the weight vectors according to the individual density of the objective space. is method can better ensure the uniformity of the weight vectors on the objective surface, thus ensuring the uniformity of the solution set. e difference is that vaea normalizes the population according to the ideal and lowest point of the population, while nsga-iii-wa obtains the intercept of each objective axis by calculating the asf and then normalizes the population. e latter is more universal and more reasonable."
"in order to associate each individual in s t with the reference points after normalization, a reference line is defined for each reference point on the hypersurface. in the normalized objective space, the reference point with the shortest distance is considered to be related to population members."
"the proposed data model is based on interviews with farmers, responsible technicians, and farm managers about the useful and necessary information that should be managed on a farm. the existing models are too specialized; although they cover the information that needs to be managed, their implementation is time-consuming. therefore, we decided to develop a new model that would meet the needs of our users. conceptually, the model presented is based on the proposed by fiware harmonized data models [cit], specifically, the agrifood model that includes requirements for irrigation and crop control. the model was constructed as an agile tool to respond to the requirements farmers allowing them to characterize the plots, reduce the hydric needs of the crops, and control the soil and water quality and the associated atmospheric conditions. this first version of the model focuses on the efficient management of the crop plots and all the included elements, as can be seen in the class diagram in figure 1 ."
"the miraflores irrigation community is located in the municipality of jumilla (murcia) [cit] in the segura river basin and has almost 1000 community members. the area comprises about 1330 ha of agricultural land, mostly devoted to woody crops and irrigated by localized irrigation. the main crops produced are fruit trees, especially pear, peach, and apricot. the community uses surface water, 3.8 hm 3 annually, and water from the jumilla sewage plant, 1.5 hm 3 annually. this treatment plant delivers the reclaimed water at its exit, from where the community drives the available flows to six interconnected regulation rafts, with a total capacity of 1 hm 3 . the distribution network conducts water from the rafts to the various irrigated farms, equipped with automatic flow control counters and setpoints, ensuring an average allocation of 4025 m 3 /ha/year/farm."
"controlled vocabulary (a set of preselected terms or words for a specific domain): an example is agrovoc vocabulary, promoted by the food and agriculture organization of the united nations (fao) and available in multiple languages."
"the proposed data model focuses on the management of crop plots represented by the agriplot entity, which is part of an agricultural holding, represented by the agriexploitation entity. the plot is characterized by four pillars that allow it to be uniquely defined:"
"traversing every individual in the population, calculate the distances between itself and all reference points and record the number of individuals associated with each reference point using ρ j, which represents the number of individuals associated with the jth reference point."
"e number of decision variables for all test functions is v � m + k − 1, and m is the number of objective functions, k � 5 for dtlz1, and k � 10 for dtlz2-6. e number of decision variables for all wfg test functions is v � k + 1, where the position variable is k � m − 1 and the objective dimension is m; the distance variable is l � 10."
"in order to visually reflect the distribution of the solution set in the high-dimensional target space, parallel coordinates are used to visualize the high-dimensional data as shown in figure 15 ."
"the last layer, service, serves as an interface between users and the central layer to offer different solutions to the problems that are generated in an agricultural operation, such as water management, irrigation planning, data analytics, and monitoring of environmental parameters. the incorporation of other modules allows us to provide our solution with more capabilities due to the possibility of integrating new components through connectors with ngsi-ld. the ngsi-ld interface is used to send data updates and receive notifications about data changes. a change in the configuration parameters triggers control actions that are managed by subsystems at the user level. as a result of the use of specific connectors, the information is consulted or notified to higher modules that are in charge of propagating it for other interests, such as historical information, large-scale data management, or integration with geographic information systems (gis). for this purpose, a representational state transfer interface (rest) is used, which applies ngsi-ld for communication between the final applications and the analysis modules."
"the architecture of the platform proposed in this paper is depicted in figure 2 . it has a layered modular form ranging from the deployment of sensors and the monitoring of techniques for data extraction to the intelligent processing of data. each of these layers is based on open and standard initiatives, such as the one provided by the fiware community [cit] or the etsi isg cim group. the first layer, device and data acquisition, represents the different sources of information that form the system; these include sensors, actuators, open information available on the internet, and databases with updated information of interest. these devices employ different communication technologies to transmit information to an iot backend module formed by iot agents. this module acts as an intermediary with the second layer and transforms information unidirectionally or bidirectionally (in the case of actuators), allowing the iot devices to interact with the platform."
"a specific module is required because of the restrictions imposed by the modules on the processing capacity, memory, and even the available power to these devices. these restrictions usually prevent them from using heavy protocols at the application level, so they use lightweight protocols such as coap or mqtt or integrate other solutions such as lora and sigfox to perform communications with the different devices through ethernet or mobile networks (e.g., gprs/3g/4g/nb-iot/5g). as such, the iot backend module component acts as an intermediary by translating the information sent through these protocols to the interface made available by the broker through ngsi-ld."
"since the new iot devices and the new communications technologies provide the user with faster and more efficient communication, we propose an affordable platform, independent of the provider and interoperable, as the cornerstone for all connections between applications/services and the irrigation elements. in this way, we offer a common framework so that applications can interact with any aspect that is part of a farm in a similar way, while being able to act on any device or controller using the same set of instructions, thus allowing for greater interoperability between different manufacturers or services."
"since the magnitude of the respective objective values is different, it is necessary to normalize objective values for the sake of fairness. first, calculate the minimum value z i of each dimension for every objective function. e sets of z i constitute the ideal points. all individuals are then normalized according to (5), where a i is the intercept of each dimension that can be calculated according to the achievement scalarizing function (asf) shown in (6) ."
"if the number of populations associated with this reference point is zero (but there is an individual associated with the reference point vector in f l ), then find the point with the smallest distance and extract it from f l to join the selected next generation population. in the setting, the number of associated populations is increased by one. if each individual is not referenced to the reference point in the f l, the reference point vector is deleted. if the number of associated populations is not zero, then the nearest reference point is selected until the population size is n."
"today, different data models are available; many of them were developed by the organizations in charge of developing their standards. the open geospatial consortium (ogc) data models are used primarily in geosciences and environmental domains, including the sensorthings api, based on the observations and measurements (o&m) data model (ogc/ [cit] 6:2011), which transforms the numerous unconnected iot systems into a fully connected platform where complex tasks can be performed and synchronized. the open connectivity foundation specifies data models based on vertical industries such as the automotive, healthcare, industrial, and smart home sectors. the world wide web consortium thing description provides some vocabularies to describe real things but does not focus much on the data. other vocabularies, such as those offered by the inspire directive [cit], define the rules on the interoperability of spatial data sets and their associated services; these are mandatory."
"this article proposes an open and interoperable platform for irrigation community management. the platform, based on standard and open interfaces and protocols, allows for the integration of heterogeneous information sources as well as interoperability with other third-party solutions for exchanging and exploiting this information. additionally, this platform exploits the stored information to analyze, in real time, the factors associated with the production process, the evolution of crops, and the optimal use of water for irrigation."
"e uniformity of the solution surface cannot be achieved when the algorithm reaches a certain stable state, although the weight vectors distribute uniformly in the space. is is because of the complexity caused by the irregular shape of the pfs of the objective functions. e distribution of weight vectors is particularly important when all individuals are indistinguishable from each other and locate on the first level of the dominance level."
"the management of external context suppliers is a process that speeds up access to the information, making the process more transparent for the final client. the broker's mission is to act as a proxy between the client and the context provider. for this reason, search and consultation times are shorter compared with the rest of the operations."
"the second layer, information management, contains software components in charge of data storage, processing, and distribution. the distributed infrastructure is composed of servers in the cloud that work together to manage massive amounts of data and make them available to the upper layer. the ngsi-ld interface is used to send data updates and receive notifications about data changes. figure 3 presents an example of how the information provided by an iot gateway, which implements json over mqtt, is integrated into the platform. the element chosen was the device that analyses the water at the exit of the wastewater treatment plant and determines the levels of turbidity and ph of the water. once the entity is created in the broker using the ngsi-ld data model, consumers can retrieve this information following two different approaches: query or subscription."
"the dataloggers have the ability to monitor sensor readings of soil conductivity, soil moisture, soil temperature, water meter readings, and meteorological parameters, among others, and to operate the solenoid valves to control the irrigation system."
"in order to further improve the convergence speed and distribution of nsga-iii algorithm, a multiobjective optimization algorithm based on weight vector adjustment (nsga-iii-wa) is proposed. algorithm 1 is the framework of the nsga-iii-wa algorithm. e algorithm is mainly improved in two aspects: evolution strategy and weight vector. is paper also adds the discriminating condition for enabling weight vector adjustment, which speeds up the running of the algorithm without affecting the performance of the algorithm. first, we initialize population p t with population size n and weight vector w_unit. secondly, we enter the algorithm iteration process, generate the population q t by the operating population p t using the differential operator, and then obtain population r t sized 2n using the combination of p t and q t . r t should be updated through the environmental selection strategy. e next generation of population p t+1 is obtained. lastly, adjust the weight vector and determine if the termination condition is satisfied. if so, output the current result and terminate the algorithm; otherwise, continue iterating."
"in order to express the effect of the algorithm more intuitively, the performance of the algorithm is presented in the form of a box diagram. due to space limitations, only the analysis of the box diagrams of the four algorithms under five goals and fifteen goals is given here. figures 3-8 show the performance box diagram under the four goals, and from figures 3-8, it can be seen that nsga-iii-wa can achieve better results when dealing with most test problems. its convergence and breadth are significantly better than the other five algorithms."
"rather than using all possible cluster merges as the candidate set of actions the agent selects from, we use the scores produced by mention pair models to reduce the search space. first, we order all mention pairs in the document in descending order according to their pairwise scores. this causes clustering to occur in an easy-first fashion, where harder decisions are delayed until more information is available. secondly, we discard all mention pairs that score below a threshold t under the assumption that the clusters containing these pairs are unlikely to be coreferent. in our experiments we were able able set t so that over 95% of pairs were removed with no decrease in accuracy. lastly, we iterate through this list of pairs in order. for each pair, we make a binary decision on whether or not the clusters containing these pairs should be merged. this formulates the agent's task so it only has two actions to chose from instead of a number of actions proportional to the number of clusters squared. algorithm 1 shows the full testtime procedure."
"our entity-centric \"agent\" builds up coreference chains with agglomerative clustering. it begins in a start state where each mention is in a separate single-element cluster. at each step, it observes the current state s, which consists of all partially formed coreference clusters produced so far, and selects some action a which merges two existing clusters. the action will result in a new state with new candidate actions and the process is repeated. the model is entity-centric in that it builds up clusters of mentions representing entities and merges clusters if it predicts they are representing the same one."
"however, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions [cit] . in this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. we first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. we then describe how the probabilities produced by these models can be used to generate expressive features between clusters of mentions. using these features, we train an entity-centric incremental coreference system."
"using entity-level information is valuable because it allows early coreference decisions to inform later ones. for example, finding that clinton and she corefer makes it more likely that clinton corefers with hillary clinton than bill clinton due to gender agreement constraints. such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions [cit] ."
"imitation learning is challenging because it is a non-i.i.d. learning problem; the distribution of states seen by the agent depends on the agent's parameters. model stacking offers a way of decomposing the learning problem by training pairwise models with many parameters in a straightforward supervised learning setting and using their outputs for training a much simpler model in the more difficult imitation learning setting. furthermore, mention pair scores can produce powerful features for training the agent because the scores indicate which mention pairs between the clusters in question are relevant; high scoring and low scoring pairs can indicate when a merge should be forced or disallowed while other mention pairs may provide little useful information."
"by sampling trajectories under the current policy, dagger exposes the system to states at train time similar to the ones it will face at test time. in contrast, training the agent on the gold labels alone would unrealistically teach it to make decisions under the assumption that all previous decisions were correct, potentially causing it to over-rely on information from past actions. this is especially problematic in coreference, where the error rate is quite high. even when using dagger, this problem could exist to a lesser degree if the model heavily overfits to the training data. however, the agent has a small number of parameters thanks to our model stacking approach, reducing the risk of this happening."
"in addition to presenting these new metrics, we showcase their application in analyzing various coding approaches over the gaussian wiretap channel. although explicit code constructions exist that can both correct errors for legitimate parties and keep secrets from eavesdroppers over discrete memoryless wiretap channels [cit], some works have sought solutions to tandem secrecy and reliability coding for real-world channels using a layered, or concatenated, coding approach [cit] . the contributions of this paper include a systematic and structured analysis of error-control coding, interleaving, scrambling, and coset-based secrecy coding, as layers in a physical-layer security coding system. we chose these layers so as to represent fundamentally different approaches to coding for secrecy over the gaussian wiretap channel, and leave additional layers of coding, such as other constructions of wiretap codes [cit], for future work. note also that scrambling and interleaving have been considered as keyed layers of coding as well [cit], although herein we only consider keyless versions of these. the analysis of both reliability and security is carried out for each of the layers in isolation, as well as for various combinations of layers of coding. benefits and drawbacks are provided for the inclusion of each layer in a concatenated coding system for physical-layer security. among them, we show that keyless layers of scrambling and interleaving provide no secrecy benefit over the uncoded case, error-control coding layers can be used to increase security or reliability depending on the signal-to-noise ratios at the legitimate and eavesdropping receivers, and the secrecy benefit of coset-based secrecy codes can be shaped using error-control coding."
"lastly, we take one feature conjunction with a boolean representing whether both clusters are size 1. in total, there are only 56 features after the feature conjunction. however, these features provide strong signal because they are directly related to the probabilities of mentions being coreferent. in contrast, the pairwise models use thousands of features (after feature conjunctions), including lexical features that are extremely sparse. the pairwise models can easily exploit this much bigger feature set because they operate in a classic supervised learning setting. the entity-centric model, on the other hand, learns in a much more challenging non-i.i.d. setting. model stacking avoids the difficulty of directly training the entity-centric model with a large set of weak features by decomposing the task into first learning to produce good pairwise scores and then using those scores to generate a manageable set of strong features."
"we face a sequential prediction problem where future observations (visited states) depend on previous actions. this is challenging because it violates the common i.i.d. assumptions made in statistical learning. imitation learning, where expert demonstrations of good behavior are used to teach the agent, has proven very useful in practice for this sort of problem [cit] ). we use imitation learning to set the parameters θ e of our agent by training it to classify whether a particular action is the one an expert policy would take in the current state. in particular, we use θ e as parameters for a binary logistic classifier that predicts which action (merge or do not merge) matches the expert policy."
"since explicit secrecy code constructions that achieve information theoretic security over the gaussian wiretap channel are still missing [cit], non-information theoretic security metrics have arisen to address practical concerns, e.g., the security gap [cit], degrees of freedom in decoders [cit], etc. although some of these security metrics allow for analysis over any wiretap channel model, they have met resistance due to their weaker security guarantees. these approaches are typically based on probability of error analysis at the eavesdropper assuming a specific decoder, rather than information theoretic analysis that would hold regardless of the chosen decoder. more recently, however, an additional information theoretic security approach has begun to take shape, where the eavesdropper's decoder outputs are used to estimate the security of a system [cit] . this allows one to precisely and efficiently conduct the security analysis of systems over any wiretap channel variation through monte carlo simulation [cit] . since this new technique must also choose a specific decoder for the eavesdropper, it therefore requires one to assume that the eavesdropper will use the best (and hopefully provably best) decoder available. in this paper, we formalize the new information theoretic security definition and say that a code achieves practical secrecy if the entropy of the message given the best known decoder output can be shown to be approximately equal to the entropy of the message at the eavesdropper. since the metric is a practical one, its application to finite blocklength codes with known explicit constructions is highlighted herein. along with this new security metric, we present the idea of a code's secrecy benefit, which, over the gaussian wiretap channel, shows the increase in confusion at the eavesdropper for the coded scenario over the uncoded scenario as a function of signal-to-noise ratio."
"our experiments were run using system-produced predicted mentions. we used the rule-based table 1 : metric scores on the development set for the classification and ranking pairwise models when using best-first clustering (b.f.) or the entity-centric model (e.c.). [cit], which first extracts pronouns and maximal np projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities or pleonastic it pronouns."
"the entity-centric system builds up coreference chains with agglomerative clustering: each mention starts in its own cluster and then pairs of clusters are merged each step. we train an agent to determine whether it is desirable to merge a particular pair of clusters using an imitation learning algorithm based on dagger [cit] . previous incremental coreference systems heuristically define which actions are beneficial for the agent to perform, but we instead propose a way of assigning exact costs to actions based on coreference evaluation metrics, adding a concept of the severity of a mistake. furthermore, rather than considering all pairs of clusters as candidate merges, we use the scores of the pairwise models to reduce the search space, first by providing an ordering over which merges are considered and secondly by discarding merges that are not likely to be good. this greatly reduces the time it takes to run the agent, making learning computationally feasible."
"our two models are designed to capture different aspects of coreference. the first one is built to predict coreference for all of the candidate antecedents of a mention. this makes it useful for providing scores when the current mention has clear coreference links to many previous mentions. for example president clinton might be linked to the president, bill clinton, and mr. president."
"we found the dagger [cit] imitation learning method (see algorithm 2) to be effective for this task. dagger is an iterative algorithm that aggregates a dataset d consisting of states and the actions performed by the expert policy in those states. at each iteration, it first samples a trajectory of states visited by the current policy by running the policy to completion from the start state. it then labels those states with the best action according to the expert policy, adds those labeled examples to the dataset, and then trains a new classifier over the dataset to get a new policy. when producing a trajectory to train on, the expert policy is stochastically mixed with the current policy; with probability β i the expert's action is chosen instead of the current policy's. we set β so it decays exponentially as the iteration number increases."
"this same code is used as the base linear code for the coset coding. finally, soft-decision decoding is used whenever possible, although some layers require hard decisions to do the decoding operations, e.g., scrambling and coset decoding. soft demodulation and soft-information belief propagation at the ecc decoder are both used."
note that the averaged features have a natural probabilistic interpretation; the average probability corresponds to the expected number of coreference links between the involved mention pairs while the average log probability corresponds to the probability that all mention pairs will have a coreference link. all of these features are computed twice: once with the classification model and once with the ranking model.
"the \"rolling out\" procedure means we naively have to visit o(t 2 ) states each iteration instead of t, where t is the length of a trajectory. however, the highly constrained action space described in section 3.1 combined with the use of memoization allows the algorithm to still run efficiently."
"where ⊕ indicates addition in f 2 . bit c is then generated by one of the simplest parity-check equations possible for error control over bits a and b. let the corresponding outputs from the channel for a, b, and c, be given as"
"adding the additional features had no substantial impact on scores, suggesting that features derived from pairwise scores are sufficient for capturing this kind of entity-level information. a disagreement between clusters necessarily means there will be disagreements between some of the involved mentions, so features like the average and minimum probability between mention pairs will have lower values when a disagreement is present."
"several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster [cit] . our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. [cit] train a classifier to do this with a structured perceptron algorithm. entity-level information has also been successfully incorporated in coreference systems using joint inference [cit], but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does."
"because the entity-centric agent relies on the output of pairwise classifiers, they should not be trained on the same data. therefore we split the training set into two sections and use one for training the pairwise models and the other for training the agent. when evaluating on the development set, we use 80% of the documents in the training set to train the mention pair models and the rest to train the entity-centric model. when evaluating on the test set we use the whole training set for the mention pair models and the development set for the entity-centric model. we also tried using cross-validation instead of a single split, but found this did not improve performance, which we believe to be because this trains the agent with different pairwise models than the ones used at test time."
"mention pair scores alone are not enough to produce a final set of coreference clusters because they do not enforce transitivity: if the pair of mentions (a, b) and the pair of mentions (b, c) are deemed coreferent by the model, there is no guarantee that the model will also classify (a, c) as coreferent. thus a second step is needed to coordinate the scores to produce a final coreference partition. a widely used approach for this is bestfirst clustering [cit] . for each mention, the best-first algorithm assigns the most probable preceding mention classified as coreferent with it as the antecedent. the primary weakness of this approach is that it only relies on local information to make decisions, so it cannot consolidate information at the entity level. as a result, coreference chains produced by such algorithms can exhibit low coherency. for example, a cluster may consist of [hillary clinton, clinton, he] because the coreference decision between hillary clinton and clinton is made independently of the one between clinton and he."
"for our initial policyπ 1, we set the parameters of the agent so it operates with simple bestfirst clustering (initializing all feature weights to 0 except for the maximum-score, anaphor-seen, and bias features). for m, the performance metric determining the action costs, we use a linear combination of the b 3 [cit] and muc [cit] metrics, which are both commonly used for evaluating coreference systems. the other metric used in our evaluation, entity-based ceafe (ceaf φ 4 ) [cit], was not used because it is expensive to compute. we found weighting b 3 three times as much as muc to be effective on the development set."
"in this section, we contrast the simple rate-one encoding schemes from the previous section with coset codes for the wiretap channel [cit] . while it was shown that rate-one codes are of no effect over symmetric discrete memoryless channels (and hence, hard decision gaussian channels), wiretap codes, on the other hand, are of immense effect in increasing the practical secrecy."
"in this section, we consider several layers of coding that are meant to secure the message against eavesdropping, and analyze the utility of keyless scrambling (and interleaving as a special case), in addition to coset-based secrecy coding."
"for the classification model, we consider each pair of mentions independently with the goal of predicting coreference correctly for as many of them as possible. the model is trained by minimizing negative conditional log likelihood augmented with l1 regularization:"
"although the entity-centric model has so far only table 2 : metric scores on the development set for the entity-centric model with and without the addition of entity-level agreement features. used features derived from the scores produced by mention pair models, other entity-level features could easily be added. we experiment with this by adding four cluster-level agreement features based on gender, number, animacy, and named entity type. each of these features can take on three values: \"same\" (e.g., both clusters have gender value feminine), \"compatible\" (e.g., one cluster has gender value feminine while the other has value unknown), or \"incompatible\" (one cluster has gender value feminine while the other has value masculine). the cluster-level value for a particular feature is the most common value among mentions in that cluster (e.g., if a cluster has 2 masculine mentions, 1 feminine mention, and 1 unknown mention) the value is considered masculine. table 2 shows the results."
"mention pair models predict whether or not a given pair of mentions belong in the same coreference cluster. we incorporate two different mention pair models into our system. however, other pairwise models could easily be added; one advantage of our model stacking approach is that it can combine different simple classifiers in a modular way."
"however, mentions often only have one clear antecedent. this is especially common in pronominal anaphora resolution, such as in the sentence bill arrived, but nobody saw him. the pronoun him is directly referring back to a previous part of the discourse, not some entity that other mentions may also refer to. however, there still might be coreference links between him and previous mentions in the text because of transitivity: any other mention about bill would be coreferent with him. for such mentions, there may be very little evidence in the discourse to suggest a coreference link, so attempting to train a model to predict these will bear little fruit. with this as motivation, we also train a model to predict only one correct antecedent of the current mention."
"the entity-centric model outperforms bestfirst clustering for both mention pair models, demonstrating the utility of a learned, incremental clustering algorithm. the improvement is much greater for the classification pairwise model, causing it to outperform the ranking model with the entity-centric clustering algorithm even though it performs significantly worse than the ranking model with best-first clustering. this suggests that although the ranking model is better at finding a single correct antecedent for a mention, the classification model is more useful for producing cluster-level features. incorporating probabilities from both pairwise models further improved scores over using either model alone, indicating that the mention pair classifiers were successful in learning scoring functions useful in different circumstances."
"our model stacking approach further distinguishes this work by providing a new way of defining cluster-level features. the majority of useful features for coreference systems operate on pairs of mentions (in one of our experiments we show the addition of classic entity-level features does not improve our system), but incremental coreference systems must make decisions involving many mention pairs. other incremental coreference systems either incorporate features from a single pair [cit] or average features across all pairs in the involved clusters [cit] . our system instead combines information from the involved mention pairs in a variety of ways with with higher order features produced from the scores of mention pair models."
"we compare the effectiveness of the entity-centric model with the commonly used best-first clustering approach, which assigns mentions the highest scoring previous mention as the antecedent. unlike the entity-centric model, the best-first approach only relies on local information to make decisions. we also compare the effectiveness of the ranking and classification pairwise models. table 1 shows the results of these models on the development set."
"coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions [cit] . however, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs."
we will find it useful to compare several various coded cases with the uncoded case in terms of reliability and secrecy. a new mechanism for showcasing the usefulness of a coding technique for physical-layer security can help.
"to tackle this problem, we build an entitycentric model that operates between pairs of clusters instead of pairs of mentions, guided by scores produced by the pairwise models. it builds up clusters of mentions believed to refer to the same entity as it goes, relying on the partially formed clusters produced so far to make decisions. for example, the system could reject linking [hillary clinton] with [clinton, he] because of the low score between the pair (hillary clinton, he)."
"in this section, we establish the system model and discuss metrics for quantifying both reliability and security. for notation, we assign capital letters to random variables and matrices, lowercase letters to realizations of random variables, calligraphic letters to ranges of random variables, superscripts to indicate the size of vectors and matrices, and subscripts to index the elements of vectors and matrices."
"note that these results hold only for encoding functions s that form a bijection over f k 2, which is not true if multiple coded blocks are interleaved or scrambled together. thus, there may still be some benefit to inter-block rate-one encoding functions, although intra-block rate-one encoding functions appear to have no effect of increasing the information theoretic security."
"the rest of the paper is organized as follows. section 2 provides the setup for the paper, including the system model and the metrics that define our new approach to information theoretic security. in section 3, we consider layers of coding for reliable data transfer between legitimate nodes in a network, and show how error-control coding must leak information in the strictest sense of security, but, with the new approach, it may be considered a help to security efforts for a range of signal-to-noise ratios at the eavesdropper's receiver. layers of secure coding are considered in section 4, where we start with simpler (and more controversial) layers of coding such as interleaving and scrambling, and move on to wiretap coding. several combinations of layers of coding are then analyzed and discussed in section 5, and conclusions are made in section 6."
"the first two values depend on the current model, so the saved values must be cleared between iterations of training. [cit] corpus, these tables had 76%, 94%, and 93% hit rates respectively after 50 passes over the dataset."
"in conclusion, this paper provides a fresh approach to information theoretic security over the gaussian wiretap channel for finite blocklength codes. it is shown how to quantify the practical secrecy and the secrecy benefit of a code, and this approach is used to systematically analyze the effects of various combinations of layers of coding in a concatenated coding scheme. scrambling and interleaving are shown to have no effect on the practical secrecy, and thereby achieve zero secrecy benefit. on the other hand, error-control coding and wiretap coding are shown to have a significant secrecy benefit over uncoded transmissions. layers of error-control coding may be used to shape the secrecy benefit of wiretap coding when concatenated coding schemes are employed."
"during training, the agent will see many of the same states and actions multiple times. we can exploit this with memoization, significantly improving the algorithm's runtime. in particular, we store the following values:"
we found a classification model to be well suited for the first task and and a ranking model to be well suited for the second one. these two models differ only in the training criteria used. both models use a logistic classifier to assign a probability to a mention m and candidate antecedent a representing the likelihood that the two mentions are coreferent. the candidate antecedent a may take on the value na indicating that m has no antecedent. the probability of coreference takes the standard logistic form:
"for all possible x in . notice that s and s −1 define a bijection between f k 2 and itself, and constitute a layer of rate-one coding."
"our agent uses features that are derived from the scores produced by the two mention pair models. although these scores only operate on mention pairs, they are combined to capture clusterlevel interactions by being aggregated in different ways over pairs of mentions from the clusters. mention pair scores can produce powerful features for training the agent because they show which mention pairs between the clusters in question are relevant, and often a small subset of the mention pairs provide far more information than the rest. for example, a strong negative pairwise figure 1 : examples of features generated for a candidate cluster merge. weights on edges are the probabilities of coreference produced by a mention pair model. link like hillary clinton and he should disallow a merge, while other mention pairs, such as two instances of the pronoun she far apart in the text, might provide very little information. using the mention pair models for probabilities, we compute the following features over all pairs of mentions between the clusters (i.e., each mention is in a different cluster)."
"we introduced a new approach to coreference resolution that trains an entity-centric system using the scores produced by mention pair models as features. the brunt of task-specific learning occurs within the mention pair models, which are trained in a straightforward supervised manner. guided by the pairwise scores, our entity-centric agent then learns an effective procedure for building up coreference clusters incrementally, using previous decisions to inform later ones. the agent benefits from using multiple mention pair models designed to capture different aspects of coreference. experiments show that the agent, which learns how to coordinate mention pair scores, outperforms the commonly used best-first method. [cit] shared task and report a significant improvement over the current state of the art."
"we experimented with bag of experts (boe) architectures for crf and lstm based slot tagging models. our experimental results over a set of 10 domains show that boe architectures are able to use the information from reusable expert models to perform significantly better than their nonexpert counterparts. in particular, the lstm-boe model shows a statistically significant improvement of 1.92% [cit] instances. when training with 500 instances, the improvement of lstm-boe model over lstm is even higher at 4.63%. for multiple domains, an lstm-boe model trained on only 500 instances is able to outperform a baseline crf model trained over 4 times the data. thus, the boe approach produces high performing models for slot tagging at much lower annotation costs."
"where n is the number of pixels in the map and θ i andθ i represent a gold standard map (t1,t2 or pd) and its corresponding reconstructed map, respectively, and j is a spatial index."
"we built a dataset of 10 target domains for experimentation. table 1 shows the list of domains as well as some statistics and example utterances. we treated these as new domains -that is, we do not have real interaction data with users for these domains. the annotated data is therefore prepared in two steps."
"in this paper, we present a model-driven adaptation approach for slot tagging called bag of experts (boe). in section 2, we first describe how this approach can be applied to two popular machine learning methods used for slot tagging: long short term memory (lstm) and conditional random fields (crf) models. we then describe a dataset of 10 target domains and 2 reusable domains that we've collected for use in a commercial digital assistant, in section 3. using this data, we conduct experiments comparing the boe models with their non-expert counterparts, and show that boe models can lead to significant f1-score improvements. the experimental setup is described in section 4.1 and the results are discussed in section 4.3. this is followed by a survey of related work in section 5 and the conclusion in section 6."
"we presented the flor method for high quality reconstruction of quantitative mri data using mrf, by exploiting the low-rank property of the mrf scheme. thanks to the fact that we exploit low-rank on top of the well known sparsity of mrf in the dictionary matching domain, we are able to obtain high quality reconstruction from highly under-sampled data and provide results that are comparable to conventional mrf from 100% of the data, using only 15% of the data. in addition, comparison against cs-based methods for mrf shows the added value of low-rank based reconstruction. future work will focus on exploiting spiral acquisition trajectories, which were proven to be successful in a few mrf applications, in conjunction with low-rank mrf."
"the length of the label vector l j i is the number of labels in the expert domain, with the value corresponding to the label predicted by c j for word w i set to 1, and values for all other labels set to 0. for each word, the label vectors for all the expert crf models are concatenated and provided as features for the target domain crf training, along with the n-gram features."
"for our lstm model, we follow a standard bidirectional lstm architecture [cit] . let w 1 ...w n denote the input word sequence. for every input word w i, let f c i and b c i be the outputs of the forward and backward character level lstms respectively, and let m i be the word embedding (initialized either randomly or with pretrained embeddings). the input to the word level lstms, g i, is the concatenation of these three vectors:"
"and m i has the same dimensions as the pre-trained embeddings. the forward and backward word level lstms take g i as input and produce f w i and b w i, which are then concatenated to produce h i :"
"in mri, data is acquired in the spatial image domain (k-space), where the acquisition time of a high resolution, 3d mri lasts a few minutes. since mrf is based on rapid acquisition of multiple contrasts, severe under-sampling is needed to obtain the temporal resolution required for mrf. in the original mrf paper, reconstruction from under-sampled k-space data is preformed by zero-filling of the missing kspace values [cit] . later works added sparse representation and compressed sensing (cs) methods [cit] to improve reconstruction. [cit] developed an approach called blip (bloch response recovery via iterative projection) that is based on gradient descent and projection onto the dictionary sub-space. zhe [cit] suggested a method that exploits the sparsity in the wavelet domain of each imaging contrast, together with changing the acquisition trajectories between time stamps. although those techniques have shown improved performance over the original mrf approach, they do not exploit the temporal similarity across frames, which is a fundamental nature of the mrf acquisition scheme. such similarity is exploited, via modeling the data as low-rank data, in many mri applications with high temporal resolution, such as cardiac imaging [cit] and functional mri [cit] ."
"figures 1,2 and 3 show the resulting maps for the recovery of t1, t2 and pd maps, respectively. it can be seen that low-rank mrf reconstruction from 15% of data outperforms blip, and provides error in the level obtained by mrf using 100% of data. note that the conventional mrf maps were generated using 100% of the data without noise, and are used for comparison against fully sampled data. the errors in those conventional mrf maps are quantization errors that arise from the discretization in the dictionary only."
"a drawback of the data-driven adaptation approach is that as the repository of data for reusable slots grows, the training time for new domains increases. the training data for a new domain might be in the hundreds of samples, while the training data for the reusable slots might contain hundreds of thousands of samples. this increase in training time makes iterative refinement difficult in the initial design of new domains, which is when the ability to deploy new models quickly is crucial."
"we want to verify if boe models can improve slot tagging performance by using the information from reusable domains. [cit], 1000 and 500 training examples from every domain. we use stratified sampling to maintain the input distribution of the intents across the three training datasets."
"as an example, in the purchase domain, the lstm-boe model achieves an f1-score of 70.66% with only 500 [cit] training instances the crf model achieves an f1-score of only 66.24%. thus the lstm-boe model achieves better f1-score with only one-fourth the training data. similarly, for flight status, travel, and transportation domains, the lstm-boe model gets better performance with 500 [cit] training instances. the lstmboe architecture, therefore, allows us to reuse the domain experts to produce better performing models with much lower data annotation costs. as the target domain training data increases, the contribution due to domain experts goes down, but more experimentation is needed to establish the threshold at which it is no longer useful to add experts."
"conditional random fields (crf) are a popular family of models that have been proven to work well in a variety of sequence tagging nlp applications [cit] ). for our experiments, we use a standard linear-chain crf architecture with n-gram and context features."
"the results in table 3 (a) also show that both the crf-boe and lstm-boe outperform the basic crf and lstm models. lstm-boe has a statistically significant mean improvement of 1.92 points over lstm. crf-boe also shows an average improvement of 2.19 points over the crf model, but the results are not statistically significant. looking at results for individual domains, the highest improvement for boe models are seen for transportation and travel. this can be explained by these domains having a high frequency of timex and location slots, as shown in table 4 ."
"when training on a target domain, for each word w i, we first compute the character level lstms f c i, b c i similarly to section 2.1. we then compute a boe representation for this word as:"
"using the dev data set for the 10 domains, we experimented with using different pretrained embeddings, dropout probabilities and a crf output layer in our lstm architecture. the results are summarized in table 2 . for each of the 10 domains, we trained using each variant with 10 different seeds, and computed the mean f1-score for each domain. for comparing two variants, we computed the mean difference in the f1-scores over the 10 domains and its p-value."
"an alternative strategy is to use model-driven adaptation approaches [cit] b) as shown in figure 2 (b). here, instead of retraining on the data for the reusable slots, we train \"expert\" models for these slots, and use the output of these models directly when training new domains. using model-driven adaptation ensures that model training time is proportional to the data size of new target domains, as opposed to the large data size for reusable slots, allowing for faster training."
"we tried word level glove embeddings of 100, 200 and 300 dimensions as well as 500-dimensional word embeddings trained over the utterances from our commercial pda logs. both 100 and 200 dimensional glove embeddings led to statistically significant improvements, but the word embeddings trained over our logs led to the biggest improvement. we also tried using a crf output layer [cit] and different values of dropout keep probability, but none of them gave statistically significant improvements over the default model. based on this, we used pda trained 500-dimensional word embeddings for our final experiments on test data."
"in particular, for each token, we use unigram, bigram and trigram features, along with previous and next unigrams, bigrams, and trigrams for context length of up to 3 words. we also use a skip bigram feature created by concatenating the current unigram and skip-one unigram."
we train our crf using stochastic gradient descent with l1 regularization to prevent overfitting. the l1 coefficient was set to 0.1 and we use a learning rate of 0.1 with exponential decay for learning rate scheduling [cit] ).
"the goal in mrf is to recover, from the measurements y, the imaging contrasts x and the underlying quantitative parameters of each pixel defined in (2), under the assumption that every pixel in the image contains a single type of tissue and that θ 1 is known."
". h i is then input to a dense feed forward layer with a softmax activation to predict the label probabilities for each word. we train using stochastic gradient descent with adam [cit] . to avoid overfitting, we also use dropout on top of m i and h i layers, with a default dropout keep probability of 0.8. we experiment with some variations of this default lstm architecture, the results are described in section 4.2."
"an initial work describing the implementation of low-rank model for mrf has been developed recently [cit] . here, we apply different a reconstruction algorithm and compare our low-rank based mrf reconstruction to other cs approaches. we exploit the low-rank property of the temporal domain of mrf, via an iterative scheme that consists of a gradient step followed by a low rank projection using the singular value decomposition. the low rank estimate is then fit to one of the dictionary elements in an iterative fashion, in order to identify the tissue. experimental results are presented using mri data of a human patient, and demonstrate superior performance using only 15% of k-space data."
"first, utterances are obtained using crowdsourcing, where workers are provided with prompts for different intents of a domain and asked to generate natural language utterances corresponding to those intents. next, the generated utterances are annotated by a different set of crowd workers, using the slot schema for each domain. inter-annotator agreement as well as manual inspection are used to ensure data quality in both stages. the amount of data collected varies for each domain based on its complexity and business priority. dataset size statistics for the data used in our experiments are presented in section 4.1. test and dev data are sampled at 10% of the total annotated data, with stratified sampling used in order to preserve the distribution of the intents."
grouping the reusable slots into domains in this way provides additional opportunities for a commercial system: the trained reusable domain models can be used in other related products which need to identify time and location related entities. models trained on the timex and location data have f1-scores of 96% and 89% respectively on test data from their respective domains.
"in mrf, a random pulse pattern is executed during the scan, and each tissue responds to this sequence in a different manner. by varying the acquisition parameters (e.g. repetition time (tr), echo time (te), radio frequency (rf) flip angle and the readout trajectory), unique signals are generated from different tissue types. after acquisition, a pattern recognition algorithm is used to match the acquired signal to an entry from a dictionary of possible tissue candidates. the dictionary entries are created by simulating the acquisition sequence on a range of parameters, demonstrating different biological tissues. the simulations are based on the bloch equations and use the t1 and t2 time constants of the tissues, together with the selected random pulse pattern. the resulting dictionary demonstrates the reaction of the different materials to the pulse sequence. the quantitative parameters, such as the tissue's t1 and t2 relaxation times, can be simultaneously retrieved from the data by matching the signature acquired to the most correlated entry in the dictionary."
"we first describe our lstm and crf models for slot tagging, followed by their boe variants: lstm-boe and crf-boe. tensorflow [cit] was used for implementing the lstm models, while a custom c++ implementation was table 1 : list of target domains used for our experiments, along with some statistics and example utterances. the test and development data sets are sampled at 10% of the total annotated data. \"flight stat.\" stands for \"flight status\", \"soc. net.\" stands for \"social network\", and \"transport.\" stands for \"transportation\"."
"natural language understanding (nlu) is a key component of dialog systems for commercial personal digital assistants (pdas) such as amazon alexa, google home, microsoft cortana and apple siri. the task of the nlu component is to map input user utterances into a semantic frame consisting of domain, intent and slots [cit] . the semantic frame is used by the dialog manager for state tracking and action selection."
"for each training dataset, we train the four models as described in section 2 and compute the precision, recall and f1-score on the test data. fixed seeds are used when training all models to make the results reproducible. table 3 summarizes these results, with only f1-scores reported to save space. we describe these results in section 4.3."
"in this data-driven adaptation approach, we build a repository of annotated data containing date, time, location and other reusable slots. we then combine relevant data from the reusable repository with the domain specific data during model training. figure 2 (a) shows an example of this architecture where reusable date/time data is used for training travel domain."
"the shopping model shows a regression for boe models, and a reason could be the low frequency of expert slots (table 4) . however, low frequency of expert slots does not always mean that boe methods can't help, as shown by the improvement in the purchase domain. finally, for sports, social network and deals domains, the lstm-boe improves over lstm, while crfboe does not improve over crf. our hypothesis is that given the query patterns for these domains, the dense vector output used by lstm-boe is able to transfer some information, while the categorical label output used by crf-boe is not. training data instances. note that the improvements are even higher for the experiments with smaller training data. in particular, lstm-boe shows an improvement of 4.63 in absolute f1-score over lstm when training with 500 instances. thus, as we reduce the amount of training data in the target domain, the performance improvement from boe models is even higher."
"table 3(a) shows the f1 [cit] training instances for each of the 10 domains. lstm based models in general perform better than the crf based models. the lstm models have a statistically significant average improvement of 3.14 absolute f1-score over the crf models. the better performance of lstm over crf can be explained by the lstm being able to use information over longer contexts to make predictions, while the crf model is limited to at most the previous and next 3 words."
"we experiment with two domains containing reusable slots: timex and location. the timex domain consists of utterances containing the slots date, time and duration. the location domain consists of utterances containing location, location type and place name slots. both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse. 1 data for these domains was sampled from the input utterances from our commercial digital assistant. each reusable domain contains about a million utterances. there is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains. the data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains."
"the input to the word level lstm for word w i in the target domain is now a concatenation of the character level lstm outputs (f c i, b c i ), the word embedding m i, and h e :"
"one detail skipped in the algorithm is that we also need to assign intra-cell distances to the first few buckets of the sdh. in particular, given a cell a with diagonal length of q, the distances between any two particles in a fall into the range ½0; q, and can be modeled as the following random variable:"
"monte carlo simulations can help us obtain a discrete form of the pdf of the distance distribution, given the pdfs of the particle spatial distributions [cit] . one important note here is that the method works no matter what forms the spatial distributions follow. however, to generate the particle spatial distributions, it is infeasible to test the ms data set for all possible data distributions. instead, we focus on testing if the data follows the most popular distribution in ms-spatial uniform distribution. we should note here that the particle spatial distribution is different from the distribution of distances between particles."
"now, let us take a look at a different random variable q. assuming q is a normal random variable with parameters ðc à a; ðbàaþ 2 6 þ, the probability density of q can be written as follows:"
"an important note here is that our analysis has so far concentrated on the errors introduced in an individual distribution operation (i.e., between one pair of cells). however, our work [cit] has revealed the fact that errors generated by different pairs of cells can cancel out, and reduce the error in the whole sdh to a great extent. we call such a phenomenon error compensation. in particular, our qualitative study shows that the error (at the entire sdh level) caused by prop can be loosely bounded by 10 percent. since this is not a tight bound, we expect to see much smaller errors in practice, as shown in our experimental results for the adm-sdh algorithm (section 8.2). for the same reason, the effects of type i error can also be reduced by error compensation, making the type i error a negligible quantity."
"objectives: our goal with this work is to perform sdh computation on a high level of efficiency and accuracy. specifically, our approach fundamentally improves over existing solutions by achieving on-the-fly query processing. this is accomplished via a number of techniques that take advantage of spatiotemporal locality within the data and multi-core parallel processing architecture of modern graphical processing units (gpus). we provide theoretical proof for guaranteed error bound that is validated with experimental results."
"the distribution of distances between a pair of cells, say a and b, can be determined based on their spatial distribution of particles, by running monte carlo simulations. monte carlo simulation is a way to model a phenomenon that has inherent uncertainty [cit] . if the spatial distributions of particles in a and b are known to be uniform, the simulations can be done by sampling (say n s ) points independently at random from uniform distributions within the spatial ranges of a and b. then, the distance distribution is computed from the points sampled in both cells. a temporary distance histogram can be built for this purpose. all n 2 s distances are computed (bruteforce method), and put into buckets of the temporary histogram (e.g., those overlapping with ½u; v in fig. 2) accordingly. this temporary histogram is used to obtain the pdf of pointto-point distances between cells a and b (the gðtþ of eqs. (1)- (3)), which is then used to update the sdh buckets."
"1) particles often interact with each other in groups and move randomly in a very small subregion of the system; 2) with particles moving in and out of a cell, the number of particles in that cell does not change much over time."
"two groups of datasets were adopted to evaluate the proposed method. one is the high similarity datasets, including z277 and z498, which consist of 277 and 498 protein domains respectively. the other group consists of all low similarity datasets, i.e., 1189 [cit], d640 [cit], 25pdb [cit], d8244 [cit] and d1185 [cit] . pairwise sequence similarities in these datasets are all lower than 40%. the detailed information about the seven datasets is listed in table 1 ."
"1) generation of a closed-form via analyzing the pdfs ofṽ a andṽ b as well as eq. (4); or 2) monte carlo simulations using the pdfs ofṽ a andṽ b as data generation functions. in practice, it is difficult to get a closed-form pdf for d even when the particle spatial distributions follow a simple form. in section 4.2, we present the results of our efforts in obtaining such a closed-form pdf for d under uniformly distributedṽ a andṽ b ."
"whereṽ 0 a is an independent and identically distributed variable toṽ a . let us further assume the range ½0; q overlaps with buckets 0 to j. then we can follow the same idea shown in eqs. (1)- (3) to assign the distance counts of cell a into the relevant buckets (appendix g, available in the online supplemental material)."
"with a limited number of training examples, a small amount of features often result in a better generalization of machine learning algorithms (occam's razor) [cit] . meanwhile, the increased dimensions of the feature vectors would increase the amount of calculation of some machine learning methods, such as support vector machine and neural network. for this reason, an r script from svm-rfe algorithm package [cit] was introduced to select top features. firstly, pssm, profeat and go features of each protein were integrated into a feature vector. all the feature vectors of proteins for each dataset would be used to construct a feature matrix, where each column represents a feature and each row represents a sample. then, training an svm with a linear kernel, we ran the svm-rfe algorithm to get a rank list of all features by removing only one feature with the smallest ranking criterion each time. the first item in the rank list is the most relevant to perform protein structural class prediction, and the last item has the least relevant feature. finally, we were able to select different top kfeatures according to the ranking list."
"we utilize the shared memory to optimize the montecarlo simulations on gpu. given two cells, a set of random numbers are generated between range 0.0 to 1.0, for each cell, in the shared memory. these random numbers are mapped to the boundaries of the cells. the numbers are organized in the shared memory such that all the accesses belong to different banks. then we perform the simulations and compute the distance distribution. the distributions are stored in a hash table that is created on global memory (as shared memory contains simulated points). the hash table is then used by the algorithm, eliminating the factors that would affect gpu performance in performing all need-based simulations."
"the separation of simulation and other computations made the algorithm running time almost constant for consecutive frames, for fixed bucket width. fig. 12 shows the processing time of all 10 frames using the a3 algorithm implemented in both cpu and gpu. employing the gpus reduces the computation time of first frame significantly. also, all the simulations can be done within 100 ms, significantly reducing their contribution to the algorithm's running time. hence, the sdh can be computed efficiently in real time."
"as it is known, the right hand side of the above equation is a noncentral chi-square distribution. this means that the distances between the two cells' points can be described as a noncentral chi-square distribution with the parameters ð2; þ, where can be defined as follows:"
"the running time of the algorithm utilizing only the spatial uniformity property is contributed by the following factors: 1) quad-tree construction time oðn log nþ where n is the number of particles in simulation; 2) identification of uniform regions. this can also be bounded by oðn log nþ, as the count in each leaf node is used for at most log n chi-square tests; 3) distribution of distances into buckets; for this, all pairs of cells on a dm need to be computed-in a dm with m cells, the time is oðm 2 þ. 4) monte-carlo simulations that require oðmt s þ time according to theorem 1. here t s is the time of each individual simulation. theoretically, the first two costs will dominate as their complexity is related to system size n. in practice, the oðm 2 þ time for factor (3) can dwarf others if we choose a density map on the lower levels of the quad-tree-m approaches n when the level gets lower (this happens to the adm-sdh algorithm when the bucket width w gets smaller). however, evaluation of our experimental results shows that m is orders of magnitude smaller than n."
"to highlight the effectiveness of the recursive-based feature selection, we compared it with another commonly used feature selection tool, f-score [cit] (fig. 3) . as is shown, the prediction accuracies by svm-rfe are remarkably higher than those by fscore. taken 1189 dataset as an example, the total accuracy by svm-rfe strategy is 96.40%, which is 24.93% higher than that by f-score. it shows that the recursive-based feature ranking, which could grasp the combination effects among different features, is superior to individual-based feature selections."
"where f is the sàfield upon which the probability space is defined. note that since p is absolutely continuous with respect to q in our case, then we have"
"sufficient number of points are needed to get reasonably high accuracy of the sdh generated [cit] . the cost of running such simulations can be high if we were to perform one simulation for each pair of uniform regions. this, fortunately, is not the case. first, let us emphasize that the simulations are not related to the number of particles (e.g., n a and n b ) in the cells of interest-the purpose is to approximate the pdf of distance distribution. second, and most importantly, the same simulation can be used for multiple pairs of cells in the same density map, as long as the two cells in such pairs have the same relative position in space. a simple example is shown in fig. 1 : cell pairs ða; bþ and ða; b 1 þ will map to the same range ½u; v and can definitely use the same pdf. a systematic analysis of such sharing is presented in following theorem. proof. see appendix d, available in the online supplemental material. t u theorem 1 says that, for the possible oðm 2 þ pairs of uniform regions on a density map, there are only a linear number of simulations needed to be run. furthermore, as we will see in section 5, the same cells exist in all frames of the data set, thus a simulation run for one frame can be shared among all frames. given the above facts, we can create a lookup table (e.g., hash-based) to store the simulation results to be shared among different operations when a pdf is required."
"where n . this actually gives us the number of distances changed between cells a and b of density map dm k, going from frame f 0 to frame f 1 . there are also intra-cell distances to be processed here, details of which can be found in appendix a, available in the online supplemental material."
"here, n denotes the total number of proteins, m denotes the class number, m i ð þ and m j ð þ are the numbers of the proteins in classes i and j; p n (i) and p n j ð þ represent the numbers of the correctly predicted proteins of class i and class j by binary classifier n. tp i, fp i, tn i and fn i denote true positives, false positives, true negatives, and false negatives in class i, respectively. shows the pipeline that goes from the query sequence to the final output as well as all intermediate steps."
"to capture the variations of system states over time, there is a need to compute sdh for a large number of consecutive frames. we denote the count in bucket i at frame j as h j ½i."
"the second type of uniformity is about the significant similarity of the spatial distributions among consecutive frames. we have observed that such similarity is reflected in the final results of the sdh obtained for neighboring frames. so, given two frames f 0 and f 1, if we have already computed the sdh of f 0, we can obtain the sdh of f 1 by dealing only with the regions that do not exhibit similarity between the two frames while ignoring regions that are similar. to take advantage of such similarities among frames, we design an incremental algorithm that can quickly compute sdh of a frame from the sdh of a base frame obtained using traditional single-frame algorithms."
"a remark here is: as compared to adm-sdh that is based on prop heuristics, our algorithm shows an advantage in accuracy: error will be lower by ðe a à e u þp u ."
"as a typical pattern recognition problem, computational methods for protein structural class prediction consist of three main steps: i) protein feature representation; ii) algorithm selection for classification; iii) optimal feature selection. among the three steps, feature extraction is the most critical factor for the success of protein structural class prediction. for this step, models in common use include amino acid composition (aac), polypeptide composition, functional domain composition, physicochemical features [cit], psi-blast profiles [cit] and function annotation information [cit] . despite some success in prediction tasks, a carefully engineered integrated feature model generally offers higher accuracy and stability than those with a single feature. from this basic point, information from psi-blast profiles, profeat and gene ontology is integrated into the principal features of our model."
"t he advancement of computer simulation systems and experimental devices has yielded large volume of scientific data. this imposes great strain on the data management software, in spite of effort made to deal with such large amount of data using database management systems (dbms) [cit] . but the traditional dbmss are built with business applications in mind and are not suitable for managing scientific data. therefore, there is a need to have another look at the design of the data management systems. data in scientific databases is generally accessed through high-level analytical queries, which are much more complex to compute in comparison to simple aggregates. many of these queries are composed of few frequently used analytical routines which usually take super-linear time to compute using brute-force methods. hence, the scientific database systems need to be able to efficiently handle the computation of such analytical queries. this paper presents our work related to such type of a query that is very important for the analysis of molecular simulation (ms) data. molecular (or particle) simulations are simulations of complex physical, chemical or biological structures done on computers. they are extensively used as a basic research tool for analyzing the behavior of natural systems under experimental framework [cit] . the number of particles involved in mss is large, oftentimes counting millions. in addition, simulation data sets may consist of multiple snapshots (frames) of the system's state at different time points."
"another inherent property of the ms is that the particles often exhibit temporal locality which can be utilized to compute the sdh of consecutive frames even faster. the existence of temporal locality is mainly due to the physical properties of the particles in most of the simulation systems [cit] . more specifically, such properties can be observed at the following two levels:"
"summary: computation of sdh based on spatial uniformity delivers the significant performance boost over existing algorithm while generating more accurate results. the idea of utilizing the temporal locality can work on top of the spatial uniformity idea to achieve higher performance and also better performance/accuracy tradeoffs. this idea by itself did not show clear advantage, as demonstrated by the bad performance of a2 under small bucket width. monte carlo simulation should be the choice in making distance distribution decisions, although the approach based on the cdf of noncentral x 2 is only marginally worse. the simulation-based approach generates very little error even when the simulation size is small, making it a winner over the cdf-based approach. the advantages of the new algorithm over adm-sdh become small under large bucket width, but this does not generate a concern since the target of the new algorithm is the smaller bucket width, which is preferred in scientific data analysis."
"mathematical analysis towards a closed-form, or monte carlo simulations; exploiting this property makes algorithm running time independent of the sdh bucket width w -such dependency (as discussed in section 2) is the main drawback of existing algorithms. on the other hand, working with the pdfs of distance distribution guarantees very little error will be made, as shown by our rigorous analysis of the algorithm (section 6)."
"summary: the gpu versions of our algorithm demonstrate the great potential of gpus in large-scale data analytics. for the sdh problem we tested, speedup over the single-cpu implementation reaches 25x-that is a significant improvement of performance. the speedup decreases under larger bucket width, but it is always the cases of smaller bucket width that make the sdh problem difficult. such diminish of speedup, as well as the different optimization strategies, however, indicate that gpu programming is a non-trivial task. finally, the combination of multi-core gpu's and efficient algorithm to utilize the spatio-temporal uniformity, delivers very high performance. as a result, we are able to analyze scientific simulation data in a real time manner. 4. active power: power measured for the entire database server less the system idle power. it can be viewed as the power used for processing the workload. we used wattsup power meter in our experiments."
"note that, since our discussions started with the only assumption that points in a and b are uniformly distributed, the parameters of above pdf have no relationship with the actual number of points in cell a or cell b."
"using noncentral x 2 distribution: the noncentral x 2 distribution approximation of the distances between two cells is applied to compare with the monte-carlo simulations. specifically, for each pair of cells, we distribute the distance counts into the relevant buckets based on the values obtained from the cumulative distribution function (cdf) of the noncentral x 2 distribution. such values are computed by calling a matlab library [cit] and cached into a hash table to avoid repeated computations (exactly the same as what we did for the monte carlo simulation results). fig. 9 shows the comparison of errors in the sdh obtained and the running time. the errors generated by using the cdf of noncentral x 2 are slightly higher than those by the monte carlo simulation. this is expected as we know there is a systematic error in using the cdf (lemma 1) while the monte carlo simulations are shown to be very accurate (section 6.1.2). the simulationbased method also beats the cdf-based method in efficiency. this is because the cdf of noncentral x 2 distribution has a very complex form [cit] therefore the time used for numerical computations in matlab is non-trivial."
"the remainder of this paper is organized as follows: in section 2 we give an overview of the work done in the field related to the sdh problem. then, in section 3 we introduce the main concepts and techniques utilized in our work. sections 4 and 5 discuss the utilization of the spatio-temporal properties of the data to enhance the algorithm. then, in section 6 the performance (running time and errors) of the proposed technique in utilizing the spatio-temporal property of the data is analyzed. in section 7 we briefly look at the basic architecture of the gpus and their programming paradigms and we modify our algorithm to map onto the gpu. section 8 presents the results obtained through extensive experiments. finally, we conclude this paper with section 9 in which we also discuss our future work. due to space limitation, certain details are presented in respective appendices which are part of the supplementary materials, which can be found on the computer society digital library at http://doi.ieeecomputersociety.org/10.1109/ tkde.2014.2298015."
"the gpu versions of the proposed algorithm were implemented under cuda, v.4.0 [cit] . the performance of the algorithms was evaluated on nvidia geforce gtx 570. we report results for processing the 8-million-atom data set."
"the idea behind the approximate algorithm (adm-sdh) is to recursively call resolvetwocells only for a predetermined number (m) of levels in the tree. if after visiting the m levels, there are unresolved pairs of cells, heuristics is being used to greedily distribute distances into relevant sdh buckets. we will study the heuristics for distance distribution in section 4. the main benefit of this algorithm is: given a user specified error bound, our analytical model can tell what value of m to choose [cit] . although adm-sdh is fast in regard to the data size n, its running time is very sensitive to the bucket width w. the main reason for this is: when w decreases by half, we have to start the algorithm from the next level of the tree. as a result, the number of pairs of cells i increases by a factor of 2 2d (d is number of dimension). since the sdh is a discrete approximation of a continuous distribution of the distances in the ms system, more information is lost with the increase of w. scientists prefer smaller values of w so that there are a few hundred buckets in the sdh. here, we present an efficient and accurate multi-frame sdh computing algorithm whose performance is insensitive to both n and w. this new algorithm uses the same region quad-tree for data organization as in the dm-sdh and adm-sdh algorithms."
"it is our belief, based on the work we have done on this matter, that to get an explicit and more accurate closed form for the distribution of the distances between points of the cells is a really challenging, if not impossible to solve, problem."
"a dm-based algorithm depends heavily on resolving cells to achieve the desired accuracy. it applies heuristics to distribute the distances into relevant buckets after visiting m levels of the tree or after reaching the leaf nodes. that is the main reason for the long running time. our idea to remedy that problem is to greedily distribute distances between very large regions of the simulation space, even when no pairs of such regions are resolvable. in other words, we use heuristics for distance distribution as early as possible. however, the distribution of distances between two large regions may yield arbitrarily large errors. therefore, the key challenge is to design a heuristic with high accuracy even under large regions. our first idea to address the aforementioned challenge is to take advantage of the spatial distribution of data points in the cells. as illustrated in fig. 2 : two cells have a distance range ½u; v which overlaps with three sdh buckets (i.e., from bucket i to i þ 2). a critical observation here is: if we knew the probability distribution function of the point-topoint distances between cells a and b, we can effectively distribute the actual number of distances n a n b into the three overlapping sdh buckets. specifically, the total number of n a n b distances will be assigned to the buckets based on the probability of a distance falling into each bucket according to the pdf. for the case in fig. 2, the relevant sdh buckets and the number of distances assigned to them are as follows:"
"in statistical prediction, jackknife test, independent dataset test and sub-sampling test are the most commonly used methods for evaluating the effectiveness of predictors. due to its objectivity and rigidity, the jackknife test is more prevalent for examining the power of predictors than other cross-validation procedures [cit], so it was adopted to validate our predictor. the accuracy, overall accuracy and matthew's correlation coefficient (mcc) are formulated as follows:"
"in this part, we show the results of our efforts in obtaining an approximate closed-form for the general case: finding distance distribution between points in any two cells. the main claim is: if the data points in cells a and b are uniformly distributed, then the square of the distance between the two cells' points can be approximated by a noncentral chisquare distribution and the distribution is not related to the number of points in cell a or b."
"this paper presents a highly efficient and practical algorithm for processing sdh of large-scale ms data with improved efficiency and accuracy over existing solutions. to achieve this, the algorithm takes advantage of the two types of uniformity widely present in ms data. to further improve the running time of the algorithm, we utilize graphics processing units (gpus)."
"in this section, we look at the basic architecture of the gpus and their programming paradigms. then we modify our algorithm of utilizing spatiotemporal uniformity to map onto the gpu programming environment. our discussions, however, will focus on how to optimize our algorithm in a typical gpu architecture rather than a straightforward implementation. this is because the gpu architecture is very different from that of cpus thus, code optimization requires special (and sometimes unintuitive) techniques. for example, the gpu hardware provides a hierarchy of programmable memories with heterogeneous capacity and performance. for that, the data can be organized, on these memories, in such a way that the access latency is minimized."
"actually, there are still many proteins without known go annotations and structural classes. motivated by the observation that similar proteins are prone to share the same go annotation [cit], we here propose a possible solution to this problem, and wish to incorporate it into our future prediction model. given a new protein without known of go terms, we first collect all proteins homologous to it in terms of sequence similarity by blast, and then use all available go terms of its homologies to measure the go features of this query protein. for example, we could simply use the geometrical center of all its homologous go features to represent this protein."
"so, our conclusion is that the square of the distance between any two points from two cells follows (can be approximated to) a noncentral chi-square distribution. since the pdf of a noncentral chi-square distribution has a closed form [cit], the pdf of d (i.e., the square root of the noncentral chi-square) can be obtained through jacobian transformation. however, we stop here after obtaining the (approximated) pdf of d 2 since it can already be used to guide distance distributions in our algorithm with minor tweaks. recall the scenario in fig. 2 : the share of distance counts that should go into bucket i is now"
"the huge speedup under small w values is due to two factors: (1) all cells of the density map are processed in parallel; and (2) reduced divergence in the threads of each gpu block. even though the computations diverge in processing some pairs of cells, the speedup is achieved by processing on different multiprocessors. each multiprocessor has its own dedicated shared memory and does not interfere with other multiprocessors' execution."
"however, initial feature vector inevitably contains noisy information and some redundancies, which could severely affect the prediction results. in order to highlight the actual informative features, a feature selection step is needed. commonly adopted feature selection algorithms for classification problems include fscore, t-statistic, mit correlation, x 2 -statistics and so on [cit] . however, majority of these feature selection algorithms are based on the evaluation and ranking of individual features. hence some weak features, which may have a strong combination effect but weak signal evaluated individually, could be neglected by these algorithms. another group of feature selection tools, such as correlation-based feature selection (cfs) [cit] and genetic algorithm [cit], could rank the values of features as subsets rather than individually. but they may fail to select locally predictive features, especially when these are overshadowed by strong and globally predictive ones. to overcome the shortcomings, svm-rfe was proposed by ranking features based on the mutual information in the whole feature space [cit] . svm-rfe discretely removes only one feature from the whole feature vectors, and thus could take advantage of locally predictive features with relatively less computational cost."
"the brute-force method for sdh computation calculates the distances between all pairs of particles and updates the relevant buckets of the histogram. this method requires quadratic time. some of the popular software for analyzing the ms data, like gromacs [cit], still utilizes the brute-force method. but, the current state-of-the-art models for sdh computation involve methods that treat a cluster of particles as a single processing unit [cit] . space-partitioning trees (like kd-trees [cit] ) are often used to represent the system, each node of the tree representing one cluster. the main idea in such approach is to process all the particles in each node of the tree as a whole. this is an obvious improvement in terms of time over the brute-force method which builds the histogram by computing particle-to-particle distances separately. a density-map based sdh algorithm (dm-sdh) using a quad-tree data structure is presented in our previous work [cit] . it has been proven that the running time for dm-sdh is qðn 3 þ for 3d data. we will go over the main idea of dm-sdh in more detail later in this paper. although the dm-sdh algorithm is an improvement over the brute-force method for sdh computation, it is still not a practical and efficient solution for the following reasons:"
"the problem of sdh computation over multiple consecutive frames is related to persistent data structures [cit], which allow for various versions of the computation results to be maintained and updated over time for quick query processing. building persistent index schemes on complex spatio-temporal data allows for a time efficient retrieval [cit] . there has been a detailed survey of applications, made by kaplan [cit], in which persistent data structure has been used to improve efficiency. such structures are designed to resolve the i/o bottleneck problem. but, the multi-frame sdh problem involves heavy computation at each instance, overshadowing the i/o time. thus, the techniques developed for persistent data can hardly be used for efficient multi-frame sdh computation."
"the first type of data uniformity used by the algorithm refers to the spatial distribution of data points (e.g., atoms) in ms data sets. it is well known that parts of natural systems tend to spread out evenly in space due to the existence of inter-particle forces and/or chemical bonds [cit] . because of this, there are many localized regions (we call uniform regions) in the simulation space in which the particles are uniformly distributed. 1 we treat such regions as single entities when computing sdh. once we identify these uniform regions (using the x 2 test), we derive the probability distribution functions (pdfs) of the distances between all pairs of these regions by either:"
"factor (4) is also worth a special note. although the simulation time t s can be regarded as a constant (as it is unrelated to n and w), a larger number of points in the simulation is preferred for better accuracy. thus, it is crucial to study how many data points we have to simulate to reach desired accuracy. such analysis is shown in section 6.1.2."
"following the above procedure, each protein of the five datasets could be represented as a feature vector, with the dimensions 3245, 2555, 4740, 941 and 931 for 1189, d640, 25pdb, z277 and z498, respectively. here 1189 dataset was selected for optimization of the parameters in libsvm, and chosen to predict the structural class of a new protein. 1189 dataset is selected as the benchmark dataset due to its low pairwise sequence similarity, large population to ensure a high statistical power and wide adoptions in many published works."
"bank conflicts: the shared memory is organized as banks in the hardware such that the threads read different banks in parallel. if threads read different addresses in the same bank, it gives rise to an access conflict called bank conflict. fig. 7 shows an example of bank conflicts. the contiguous array of properties technique used for coalesced access helps us in eliminating the bank conflicts. memory banks can be accessed in parallel when every thread requests 4 bytes of data from different bank [cit] . the cell properties, like coordinate or atom count, are actually of 4 bytes. contiguous placement of these properties in the shared memory places them in different banks. when threads within a cuda warp access these banks in parallel, there are no bank conflicts."
"as the case study, we predicted the structural classes of ten proteins, most of them are colorectal cancer-related proteins (table 10) . for example, the centrosome (cep55_human) is the major microtubule-organizing centre of animal cells and through its influence on the cytoskeleton is involved in cell shape, polarity and motility. it belongs to the all-a folding class and is upregulated in colon cancer according to our previous research. as is shown in table 10, this protein was consistently predicted as ahelical protein by our predictor on all five training datasets. another example is the tyrosine-protein kinase receptor ufo (ufo_human), which is highly expressed in metastatic colon tumors and primary colon tumors. our predictor training by all five datasets also correctly predicted it as an all-b protein."
"in this study, we introduced a recursive feature selection scheme based on linear kernel svm in order to select the optimal features from three kinds of important features, i.e., protein go function annotation, amino acid physical-chemical properties and psi-blast profile. validation tests on seven benchmark datasets show that the selected features are more effective in identifying protein structural classes than those of other feature selection methods. for two high similarity datasets, z277 and z498, our prediction accuracies reach 99.64% and 99.79%, which respectively are 8.64% and 3.99% higher than state-of-the-art methods. moreover, the selected top features are very consistent, in which profeat constitutes the greater part (fig. 5) . as for the low similarity datasets, i.e., 1189, d640 and 25pdb, the total accuracies are 96.40%, 95.70% and 94.34%, which are higher than other approaches based on the same datasets. as for our test on datasets d1185 and d8244, high total accuracies of 84.61% and 88.42% were achieved, which are 4.5% and 5.3% higher than those of the predicted secondary structure-based methods. however, our method suffers from marginally higher computational complexity than the f-score bases for feature ranking methods. our method may be unable to predict the secondary structural class for a few proteins due to a lack of their go numbers. despite these observations, our approach could effectively catch more core features than other feature ranking methods and thus helpful to improve the prediction of protein structural classes. this effectiveness in recognizing classification patterns provides encouragement and support to future studies. we could apply our method to other classification problems. some examples include protein-binding sites prediction, highly effective antiviral peptides prediction and sirna efficacy prediction."
"lemma 1. if x and y are independent random variables uniformly distributed on ða; bþ and ðc; c þ b à aþ, and c ! a, then y à x is a triangular random variable and can be regarded as a normal random variable with total variation distance 0:1."
"d and use the integration of the pdf to guide distance distribution in step (2) of our algorithm, the number of distinct integrations is also oðmþ."
"main results: a comparison of results of different implementations of the proposed algorithms are shown in figs. 10 and 11, in which we show the performance of processing the first frame only using. the running time on gpu shows the trend similar to that on cpu, but much faster. the speedup of varies with the use of different types of memory. when only the global memory is used, the speedup achieved by a3 ranges from 7x to 10:4x. the use of shared memory pushes the speedup further by a factor of 2 (i.e., actual speedup ranges from 14:9x to 25:8x). the speedup is limited by the random memory access patterns emerging due to divergence in the thread computation. the thread divergence also serializes the execution of some of the threads. the size of the dm cells that are stored in the memory also affect the access patterns due to bank conflicts in shared memory."
"given the above analysis, we show our algorithm is tunable in that the user can choose a level of dm-tree to get a desired error guarantee. suppose p u is the fraction of pairs of cells that are uniform on a given level, the total error produced by our algorithm based on spatial uniformity is e u p u þ e a ð1 à p u þ:"
"from eq. (21), we can solve p u to obtain a guideline on the level of the dm tree from which we run the algorithm:"
"using the aforementioned findings, we now regard the differences ðx pa à x pb þ and ðy pa à y pb þ as random variables with normal distribution. in the following section, we continue with the proof of our main claim. here we show that the square of the distance can be viewed as random variable with non-central chi-squared distribution."
"given that, if we know the pdfs of bothṽ a andṽ b, the pdf of d can be derived by one of the following strategies:"
"since the points are chosen randomly, their coordinates can be regarded as random variables. furthermore, jx pa à x pb j and jy pa à y pb j can be viewed as random variables that follow a triangular distribution. but using triangular distribution would make the result and the analysis really hard (if not impossible) to achieve. in order to ease the analysis process we will approximate the triangular distribution with a normal distribution. so, naturally, we continue by first figuring out how much error will be introduced by such approximation. the following section shows that the introduced error is only 10 percent."
"in order to analyze the ms data, scientists compute complex quantities through which statistical properties of the data is shown. often times, queries used in such analysis count more than one particle as basic unit: such a function involving all m-tuple subsets of the data is called an m-body correlation function. one such analytical query discussed in this paper, is the so called spatial distance histogram (sdh) [cit] . an sdh is the histogram of distances between all pairs of particles in the system and it represents a discrete approximation of the continuous probability distribution of distances named radial distribution function (rdf). being one of the basic building blocks for a series of critical quantities (e.g., total pressure and energy) required to describe the physical systems, this type of query is very important in ms databases [cit] ."
"as we know, if x pa and x pb are independent random variables uniformly distributed on ða; bþ and ðc; c þ b à aþ respectively, ðc ! aþ then x pb à x pa follows a triangular distribution that we saw can be approximated with normal, introducing an error of not more than 10 percent."
"gpu implementation of algorithm a2 showed speedup from 4x to 23x (again, due to temporal locality). the speedup numbers give an impression that a1, a2 are much faster than a3 and a4. but, actual running times are much higher than algorithm a3 (compare fig. 10a and fig. 11a) . use of shared memory for other algorithms would not improve the performance due to following reasons: (1) multiple tree levels can't be loaded into (limited size) shared memory for a1; (2) advantages of temporal locality in a2 and a4 are shadowed by time required to load into, and access from, shared memory. also, the temporal locality property in a2 and a4 increases histogram errors [cit] ."
"we have implemented a composite algorithm combining the above ideas and tested it on real ms data sets. the experimental results clearly show the superiority of the proposed algorithm over previous solutions in both efficiency and accuracy. for example, with the proposed algorithm, we are able to compute 11 frames of a 8-million-atom data set in less than a second! in addition to a highly efficient and practical algorithm for sdh processing, we also believe that our success will open up new directions in the molecular simulation paradigm. our work builds a solid foundation for solving the more general and difficult problem of multi-body (m-body) correlation function computation [cit] . with a o à n m á complexity in nature, such problems can be addressed using the methodologies proposed in this paper."
"the basic gpu architecture, for both nvidia [cit] and amd [cit] products, is illustrated in fig. 5 . the gpu consists of many multiprocessors that execute instructions on a number of gpu cores in single instruction multiple data (simd) manner at any given clock cycle. the gpu devices have a considerable amount of global memory with high bandwidth. for example, the nvidia gtx 570 we used has 15 multiprocessors, each of which encapsulates 32 gpu cores. it also has about 1.2 gb of global memory with a bandwidth of 152 gb/s. 3 apart from the global memory, the gpus have programmable, very fast cache memory (called shared memory). this type of memory is on-chip and shared by all gpu cores in a single multiprocessor. since it is on-chip the access latency is very low. in contrast to that, the global memory has high access latency (400 to 800 clock cycles [cit] ). therefore, the access pattern should be optimized to reduce the overall latency caused by global memory."
"energy efficiency: energy consumption has become a major concern in database system design [cit] . the product of computation time and active power 4 consumed for sdh processing define the energy efficiency of the algorithms. fig. 13 plots the energy consumed by both cpu and gpu versions of the a3 algorithm. although the active power consumption of a gpu is a couple of times higher than that of the cpu (46 watts versus 17 watts as we recorded), the efficiency of the gpu algorithms makes it an energy efficient device for sdh computation-active energy consumption is 5:39 to 9:13 times lower for the gpu code using shared memory. even for the one that uses only global memory, energy efficiency is 2:51 to 3:81 times higher. to calculate the total energy consumption for the whole machine, we have to add an idle power of 114:5 watts to the active power readings and that will translate into even larger energy savings for the gpu implementations."
"finally, our algorithm takes advantage of the multi-core parallel processing feature of gpus. they provide a lowcost and low-power platform to improve efficiency as compared to computer clusters. however, the gpu architecture imposes challenges in developing software that takes full advantage of their computing capability. to address such challenges we develop several techniques that are very different from those used in optimizing cpu-based systems. the techniques generate significant boosts in performance (and energy efficiency) as compared to straightforward gpu implementations."
"in this work, we propose a novel computation method that combines svm with psi-blast profile, physical-chemical property and functional annotations to further improve the prediction of protein structural class. here, a simple and powerful sequence representation model (pssp-rfe) is employed to transform the original profile. the feature vector is then input to an svm classifier to perform the prediction. jackknife crossvalidation tests on seven widely used benchmark datasets show that our method presents satisfying prediction accuracies in comparison with other existing methods."
"memory access latency: the operations of our algorithm are computation intensive rather than memory access. once, the information about cells is accessed into shared memory, a large number of operations are performed. moreover, the coalesced memory access pattern reduces number of read requests issued to global memory. the nvi-dia gpus used in our experiments can access up to 128 bytes of memory in single request [cit] . thus the combination of computation intensive property of the algorithm and special features of gpu shadows the latency involved in global memory accesses."
"data sets: two data sets from different simulation systems were used for experiments. the first data set consists of 10;000 frames captured from a collagen fiber simulation system made of 890;000 atoms. the second data set is collected from a cross membrane protein system with about 8;000;000 atoms and 10;000 frames. we randomly selected a chunk of 100 consecutive frames from the first data set and 11 frames from the second data set for our experiments. the main bottleneck in testing the algorithms is computing the correct histogram of the frames, needed to compute the error. obtaining correct histogram is basically running the naive or dm-sdh algorithm, which is computationally expensive. therefore, we could only get the correct histograms of 11 frames from the 8 million data set (by bruteforce in 27 days!)."
"abstract-this paper focuses on an important query in scientific simulation data analysis: the spatial distance histogram (sdh). the computation time of an sdh query using brute force method is quadratic. often, such queries are executed continuously over certain time periods, increasing the computation time. we propose highly efficient approximate algorithm to compute sdh over consecutive time periods with provable error bounds. the key idea of our algorithm is to derive statistical distribution of distances from the spatial and temporal characteristics of particles. upon organizing the data into a quad-tree based structure, the spatiotemporal characteristics of particles in each node of the tree are acquired to determine the particles' spatial distribution as well as their temporal locality in consecutive time periods. we report our efforts in implementing and optimizing the above algorithm in graphics processing units (gpus) as means to further improve the efficiency. the accuracy and efficiency of the proposed algorithm is backed by mathematical analysis and results of extensive experiments using data generated from real simulation studies."
"a large number of threads can be executed in simd fashion on the gpus. the major difference between cpu and gpu threads is that the gpu threads have low creation and context-switch time. we follow the terminology of nvidia's compute unified device architecture (cuda) [cit] to describe the operation of gpu multiprocessors. a group of threads executing on a multiprocessor is called block. the blocks are scheduled dynamically on different multiprocessors. threads within a block share all the resources, such as registers, l1 cache, etc., available on the multiprocessor. thirty two consecutive threads make a warp. threads within a warp execute in lock-step. any divergence in instructions causes them to execute in sequence (determined by the scheduler). the multiprocessor views a block of threads as group of warps, and is responsible for scheduling them. an interesting feature of the memory in gpus is that different threads in a block can read different memory locations simultaneously. this is achieved only when threads read consecutive memory locations. the underlying hardware groups the consecutive memory access requests into one access. this process is called coalesced access."
"221 by the fisher's exact test. actually, we also tried feature vectors optimized by the other three datasets, and the corresponding predictions for all datasets are quite similar, which showed the robustness of our algorithm to the selection of feature vectors."
"in other words, a user will choose to work on a dm where the fraction of uniform cells is at least ffiffiffiffiffi p u p, in order to get an error lower than . more details about the percentage of the cells marked as uniform can be found at the end of appendix h, available in the online supplemental material."
"the running time is determined by the number of cell pairs that do not satisfy the temporal locality condition, i.e., ratio products are not in the range of 1:0 ae . due to the sorted list of ratios in the rdm, all cell pairs satisfying the above condition are skipped by the algorithm. suppose p r is the fraction of such cell pairs, only ð1 à p r þ pairs of cells need to be processed by the algorithm. the sorting and searching of the cells can be performed in oðmlogmþ time. hence, the running time of the algorithm is bound by ð1 à p r þt þ oðm log mþ where t is the time for processing the base frame. in other words, by utilizing the temporal locality, we achieve a ð1 à p r þ-factor improvement in running time."
"we stop building the tree when the number of particles in a node drops below 5 because, otherwise the cost of resolving nodes could be higher than directly retrieving the particles and calculating point-to-point distances [cit] . we refer to such a tree as the density-map tree (dm-tree)."
"in order to compare the other algorithms on gpus, we implemented their global memory versions. algorithm a4 achieves a small improvement in the running time (and speedup) from the temporal locality of atoms. similarly, the performance of algorithms a1 and a2 are compared with their cpu implementations, as shown in fig. 11 . we observed speedup in the range 3x-18:5x for approxmate algorithm a1. in obtaining this result, we restricted the tree traversal up to two levels. further traversal causes thread divergence and un-coalesced memory accesses, killing the performance gain, making it worst than cpu implementation."
"the fundamental part of the dm-sdh algorithm is a procedure we call resolvetwocells. this procedure takes two cells (e.g., a and b in fig. 1 ) from a density map as an input and computes the minimum and maximum distance (denoted as u and v) between them in constant time. the main task of this procedure is to determine whether the two cells are resolvable or not. we call a pair of cells resolvable if both u and v fall into the same sdh bucket i. in such case we increment the distance count of that bucket by n a n b, where n a and n b are the number of particles in cell a and b, respectively. in the case of non-resolvable cells, we either: 1) go to the next density map with higher resolution and resolve all children of a with those of b, or 2) compute every distance between particles of a and b and update the histogram accordingly, if it is the leaf-level density map. to get the complete sdh of the ms system, the algorithm executes the resolvetwocells procedure for all pairs of cells on a given density map dm k (the dm where the diagonal of a cell is smaller than or equal to the bucket width w). so, basically, the algorithm calls resolvetwocells recursively (i.e., action (1) above) till it reaches the leaf level of the tree (i.e., action (2) above)."
"now let us study how well the normal distribution approximates the triangular distribution. let p be the triangular distribution with pdf pðxþ given by eq. (15) and q is the normal distribution nð0; . although these indicate the similarity between the two distribution, we still want to quantify how close the two probability measures are."
"in this section, we introduce the main concepts of our existing work [cit] that will serve as a foundation for the proposed algorithm. to represent the simulation data space we use a conceptual data structure that we call density map (dm). a dm divides the simulation space into a grid of equal sized cells (or regions). a cell is a square in 2d and a cube in 3d. 2 to generate a density map of higher resolution, we divide each cell of the grid into four equally sized cells. we use a region quad-tree [cit] to organize different density maps of the same data. each cell of the dm is represented by a tree node, so a density map is essentially the collection of all nodes on one level of the tree. each node of the tree contains the cell location (i.e., coordinates of corner points) as well as the number of particles in it. one thing to note here is that 2 . in this paper, we focus on 2d data to elaborate and illustrate the proposed ideas."
"profeat is a web server for computing the frequently used structural and physicochemical features of proteins and peptides from amino acid sequence [cit] . these features include dipeptide composition, quasi-sequence-order descriptors, sequence-ordercoupling number, and various structural and physicochemical properties. profeat provided a satisfactory way to predict the structural, functional and interaction profiles of proteins and peptides irrespective of sequence similarity. in this study, by inputting a query protein sequence and selecting all the profeat features, we finally acquired a 1080-dimension vector of profeat feature for each query protein."
"pseudocode of the algorithm is shown in algorithm 1. the algorithm identifies uniform regions (appendix b, available in the online supplemental material) and chooses a density map dm k (section 6) before computing sdh. physical study of molecular systems have shown that it is normal to see a small number of large uniform regions covering most of the particles, leaving only a small fraction of particles in non-uniform regions [cit] . this is also verified by our experiments using real ms data sets (section 8). this translates into high efficiency of the proposed algorithm. furthermore, the time complexity is unrelated to the bucket size w."
"in general, the pdf of interest can be obtained by the spatial distribution of particles in the two relevant cells. the coordinates of any two particles-one from a and the other from b-can be modeled as two random vectorsṽ a andṽ b, respectively. the distance between these two particles can also be modeled as a random variable d, and we have"
"the proposed techniques are based on temporal and spatial uniformity of data set. such cell wise uniformity is not only observed in ms, but also in many traditional spatiotemporal database applications [cit] . hence, it can be applied to very different data sets such as crowd of people and stars in astronomical studies."
we tackle this by studying the extra errors our algorithm generates for a frame f 1 on top of those in the base frame f 0 . the error introduced when utilizing the temporal locality can be categorized based on two cases:
"an important direction of research would be to study computation of general m-body correlation functions in scientific databases. such functions, despite the high scientific value they carry, have not been used for ms system analysis due to their computational complexity. we strongly believe the idea based on spatial uniformity as well as gpu programming can be extended to m-body correlation function computation. another direction of our future work might be the extension of spatiotemporal idea in 3d space and the integration of our algorithm into simulation software so that effective tuning of the simulation process becomes feasible."
"in the first step, all go terms corresponding to the five datasets were searched for the protein entries. note that current available go terms did not cover all proteins, so the proteins without known go terms were discarded from the datasets in the following analysis. since the numbers (28, 34, 60, 3 and 14 for 1189, d640, 25pdb, z277 and z498) are quite small compared to the total number of sequences, this filtering step would not affect the final accuracy seriously. after this step, 3245, 2555, 4740, 941 and 931 different go numbers are obtained for 1189, d640, 25pdb, z277 and z498, respectively. to further simplify the representation of proteins in all datasets, we created a vector to represent the go terms as follows. suppose p 1 is a protein in 1189 dataset and it corresponds to n p1 go numbers. we first list all 3245 go terms related to the entire dataset and formed a vector:"
"an efficient approximate solution to the spatial distance histogram query is provided in this paper. the algorithm presented in this work achieves higher efficiency and accuracy by taking advantage of the data locality and statistical data distribution properties. it makes it practically feasible to perform sdh analysis on data with large number of frames continuously. the efficiency and accuracy claims are supported by mathematical analysis and extensive experimental results. we have also shown that, through experiments, utilizing power of modern gpus gives very significant improvement in the performance. the scientific data analysis can be performed in real time by using such modern hardware systems."
"we next compare our model with some previous methods based on the same datasets ( tables 3-9) . as is shown, our method attained higher accuracies for low similarity datasets compared to previous methods. for instance, the overall accuracy of our method on 1189 dataset is 96.40%, higher than that by all other methods (from 12.9% to 42.5%). to illustrate the prediction performance of our method across different parameter settings, a receiver operating characteristic (roc) curve was implemented. as we know, roc curve is applicable to evaluate the prediction performance of a binary classifier, but structural class prediction is a four-class prediction problem. to deal with this problem, we first transformed structural class prediction to four binary classifiers using one-versus-rest strategy, and then averaged the four binary roc curves as the final output of a method. fig. 4 shows the averaged roc curves for 1189 dataset by our method and the other three approaches. we could observe that the area under curve (auc) of our method is 0.9738, which is significantly higher than those by pssm, profeat and go features individually (aucs are 0.9085, 0.9099 and 0.9172, respectively). similar results were obtained for the other six datasets (figs. s1-s6). in addition, our method also obtained better prediction results on the other two low similarity datasets. for d640 dataset, our method achieved an accuracy of 95.70%, which was significantly higher than those achieved using methods listed in table 4 (from 12.26% to 33.40%). for 25pdb dataset, our method achieved an accuracy of 94.34% and also outperformed all other methods listed in table 5 . in addition, good performances were also obtained on two non-redundant datasets d1185 and d8244 (tables 6 & 7) . for two high similarity datasets z277 and z498, our method reached the overall accuracies of 99.64% and 99.79% (tables 8 & 9 ), which are still better than the other classifiers including rough sets [cit], logitboost [cit], information-theoretical approach [cit], aac-pssm-ac [cit], and svm-based methods [cit] . we noticed that the other approach based on pssm features, aac-pssm-ac, also achieved a very high prediction accuracy. this illustrates that psi-blast profile is indeed a very useful predictor for protein structural class prediction."
"as the basic compositions of life, proteins play a central role in most cellular functions such as gene regulation, metabolism and cell proliferation. in order to interpret the function of a new protein sequence, it is fundamental to understand its 3d structure. since the knowledge of protein structural class provides useful information towards the determination of its 3d structure, prediction of protein structural class from sequence data becomes a hot topic in computational biology, especially with the development of high-throughput technologies [cit] . generally, proteins have irregular surfaces and complex 3d structures, but they are formed regularly in regional fold patterns at secondary structure level. based on the contents of their secondary structures, known protein structures are classified into four categories, all-a, all-b, a/b and a+b. all-a and all-b proteins consist of only ahelices and b-strands, respectively. the a/b and a+b proteins are mixed with a-helices and b-strands, where the former consist of parallel b-proteins and the latter anti-parallel b-proteins. experimental approaches to determining the structure information of a protein, including x-ray diffraction and nuclear magnetic resonance, are costly and time-consuming, and thus not capable of completely meeting researchers' demands. therefore, highthroughput computational approaches are brought to the forefront of this issue."
"a1: the adm-sdh algorithm [cit] to process individual frames using prop heuristic; a2: the algorithm utilizing only temporal locality to compute sdh continuously over multiple frames; a3: the algorithm utilizing only spatial uniformity to compute sdh frame by frame; a4: the algorithm utilizing both temporal locality and spatial uniformity to compute sdh continuously. implementation details of the last technique and thorough comparison of all of these techniques are discussed in appendix e and f, respectively, available in the online supplemental material. errors in the algorithms are computed by comparing the approximate sdh results with the correct sdh of each frame. the error (in percentage) of each frame is calculated as"
"case 2: this case will not cause any additional errors. when the temporal locality condition is not satisfied for a pair of cells in f 1, we update the histograms as if we are running the algorithm for the base frame. therefore the error will be on the same level as in the base frame. on the other hand, we do not save any processing time in such cases."
"the game-board features the whitechapel district and is divided into several hexagonal areas called hexes. some of the gaslights are lit and all the hexes next to one of these are considered as illuminated, and if a character stops by a lit area he/she is considered as a seen character. this rule also applies if two characters are on two adjoining hexes. the other characters (i.e. that are not on an illuminated area or are not close to another character) are considered to be in the darkness and so to be unseen."
"the patients retrospectively included for this study were treated for advanced nsclc and had pet/ [cit] . the patients were instructed to fast 10 to 14 h before the fdg injection was administered, after which the patients were encouraged to rest (no music, reading, or talking) in a cool (23°c to 24°c), dimly lit room. the total injected activity of fdg was expressed in mbq. the injected activity was 5 mbq∕kg of fdg for all patients and was dependent on the weight of the patient and limited at 370 mbq with the average adult dose range between 7 and 8 mci. free-breathing pet and ct images were acquired at 60 (50 to 70) min. the scans were done at the christian medical college, vellore, india. the institutional review board and the ethical committee of the hospital approved this study."
"each player will embody several characters. the game is divided into several turns where each player embodies one of the 8 characters. at each turn, fours characters are randomly drawn and used (two by the investigators and two by jack). each character can make a move from 1 to 3 hexes (see fig. 1 for a graphical view of the board). each character has the ability to move across the street hexes (grey hexes), the other hexes (building) cannot be crossed. when a character enters a manhole hex that is open, he can use 1 movement point to go to any other hex that has an open manhole."
"finally, with an \"inertial\" stimulus, with a future trajectory that depended on both the previous state, xt, and velocity, xt − xt−1 (fig. 2b, bottom), the optimal solution was a high-pass filter so as to encode information about velocity. the strength of this highpass filter also increased monotonically with ∆ ( fig. 2 c and d and si appendix, fig. s2a, bottom) ."
"taken together, \"phase diagrams\" for optimal, temporally extended codes show how regimes of decorrelation/whitening (high-pass filtering) and of smoothing (low-pass filtering) are preferred depending on the coding capacity, c, and decoding lag, ∆. we verified that a qualitatively similar transition from low-to high-pass filtering is also observed with higher dimen-sional stimuli and/or more neurons. importantly, we show that these phase diagrams depend in an essential way on the stimulus statistics already in the linear gaussian case. we next examined what happens for non-gaussian, high-dimensional stimuli."
"the igc contours were provided as a starting contour that could be edited by the clinician for defining the final gtv, which included composite information from ct and pet. the time taken for manual gtv contouring versus the time to edit the igc contours to create the gtv was assessed as a measure of clinical efficiency in this approach. the difference in contouring time and edit time was compared using wilcoxon test and was considered significantly different when the p-value was lower than 0.05."
"in recent years, 18 fluoro-deoxyglucose ( 18 f-fdg) positron emission tomography (pet) has emerged as an essential imaging modality in staging, tumor volume delineation, prognosis assessment, and evaluation of treatment response of nonsmall cell lung cancer (nsclc). the superior sensitivity and specificity of 18 f-fdg pet particularly help to differentiate atelectasis in lungs and identify the presence of malignant lymph nodes, which are often difficult to visualize with computed tomography (ct). consequently, the amount of normal tissue included in the target volume decreases and geometric mass is reduced. 1 however, uncertainty in target definition still contributes to the majority of the errors in radiation oncology. the inclusion of biological imaging may have the highest impact in the reduction of interobserver variability in target definitions. 2 [cit] s, there has been a surge of pet-based automatic and semiautomatic contouring methods that have been proposed to help delineate the metabolic tumor activity. the methods range from simple to complex methods. 1, 3, 4 the segmentation methods include but are not limited to constant and adaptive threshold methods, [cit] region growing methods, [cit] gradient-based methods, [cit] fuzzy models, [cit] and gaussian mixture modeling. 10, 19 all these methods offer different compromises in terms of versatility and performance and compare well against manual segmentations by clinicians or the pathological measurements with varying rates of success. 3 yet there is not enough consensus among the clinical community on algorithms that work well to be incorporated into commercially available imaging software and treatment planning systems. 3, 4 most of these state-of-the-art algorithms are proprietary and are currently unavailable for clinical use. there is a lack of adequate documentation for these methods and they often do not have user/developer support, which makes it difficult to replicate the results in a similar clinical setting. other limitations include the methods being optimized for specific scanner properties, the results of which are often difficult to reproduce. 20 the segmentation task is further complicated when tumors display cellular heterogeneity."
"the impact of the pet-based segmentation method to delineate treatment volumes for the purpose of pet-guided radiation therapy treatment planning on patient management and clinical outcome is still not clear. it may be answered only with further clinical trials involving a larger sample of patients. nevertheless, in comparison with the threshold segmentations that are in current clinical use and with no additional requirements of phantom experiments for implementation, the igc algorithm could be used as a reliable starting point to delineate gtvs, which helps to reduce the inter-and intraobserver variability. when the igc contour was provided, the clinician often only needed to incorporate the anatomical regions from the ct that are not part of the metabolic volume which resulted in less contouring time. it also helped in identifying regions that are often difficult to gauge on ct alone (atelectasis)."
"an important point should be to deal with the addition of an evaluation function.we have used expert knowledge for the decision-making process, but it could be also good to use this heuristic to evaluate a situation. in the simulation step, instead of doing a random simulation, one can only play 5 or 6 random moves, and then use the evaluation function [cit] . using determinization to simulate possible future cards in the game could also be interesting."
"in this experiment, we have to tune the critical parameter k uct . we have to take into account that this parameter could be different according to the player (jack or the inspector). each experiment is done with 10000 simulations and 200 games are performed in order to have a small standard deviation. table 3 corresponds to the results from the point of view of the inspector, table 4 corresponds to the results from the point of view of mr jack k uct around 0.6 seems to be a good value for both mr jack and the inspector points of view."
"in fig. 6, the scatter represents the variation for each lesion for the different autosegmentation methods considered. the box represents the range between the lower 25% quartile and the upper 75% quartile with the median represented by the line across the box. the most extreme values in the data within 1.5 times the interquartile range correspond to the whiskers extending from each end of the boxes. the filled gray dots in the scatter plots represent the ideal value for each metric."
"pet has been increasingly used as an adjuvant imaging modality with anatomical imaging for tumor delineation to provide complementary information. the limiting factor often is the difficulty associated with reliable pet segmentation methods. although many automated methods have been proposed, manual delineation is still the clinical norm because it is quite simple to use and most vendors provide the required tools to export pet tumor contours as rt structures to the treatment planning system. 4 hybrid imaging of pet/ct with standardized protocols has helped to reduce the observer variability for nsclc tumor segmentation, 6 but the process of manual contouring itself poses a large source of variation. on the other hand, the clinical standard manual contouring done slice-by-slice is both labor intensive and time consuming, 29 an average of 4.46 min per lesion in this study."
"a node is created for every possible position of every character and possibly and the possible results of each power. let's take an example to make the things clear. if the sir william gull character is to be played, (mr gull may exchange his location with the location of any character), and for instance, 6 moves are possible (see fig 4) . v1 will exhaustively create nodes for all 6 moves and for each result of sir william gull's special ability."
"the main challenge for applying mcts to the mr jack board game lies in several aspects. first, the number of states may become really huge. second, as explained in section 2, one decision consists, in fact, of three steps:"
"s ensory neural circuits perform a myriad of computations, which allow us to make sense of and interact with our environment. for example, neurons in the primary visual cortex encode information about local edges in an image, while neurons in higher-level areas encode more complex features, such as textures or faces. a central aim of sensory neuroscience is to develop a mathematical theory to explain the purpose and nature of such computations and ultimately, predict neural responses to stimuli from first principles."
"the treating clinician manually segmented the gross disease once for each patient based on combined ct and pet information. the display protocol with a fixed window/level setting for the ct scan (lung window) and upper window set to suv max in the lesion of interest, with the lower window set at 0 for pet images for delineation. delineations were performed on the eclipse™ treatment planning system (varian medical systems inc.). the clinician-defined volumes were used as the reference volume and henceforth are referred to as manual contours."
"if the contours had perfect agreement, the union and intersection volumes would be equal and there would be no regions of uncertainty. lower uncertainty region volume meant there was less variability in defining the contours. the wilcoxon signed rank test was used to compare the uncertainty differences between the 3d slicer growcut and the proposed igc segmentation method."
"to specialize our theory to the biologically relevant case, we later investigate efficient coding of natural stimuli. a hallmark of natural stimuli is their sparse latent structure (18, 22, 25, 26) : stimulus fragments can be constructed from a set of primitive features (e.g., image contours), each of which occurs rarely (fig. 1c ). previous work showed that, in consequence, redundancy between neural responses is minimized by maximizing their sparsity (si appendix, efficient coding models) (22) . here, we investigated what happens when the objective is not to minimize redundancy but rather, to efficiently predict future stimuli given finite coding capacity."
"when we investigated efficient coding of naturalistic stimuli, we found solutions that are qualitatively different from known sparse coding results, in which individual neurons are tuned to directional motion of local edge features (27) . in contrast, we found that neurons optimized to encode the future are selective for motion speed but not direction ( fig. 3 and si appendix, fig. s6) . surprisingly, however, the neural population as a whole encodes motion even more accurately in this case (fig. 4e ). we show that these changes are due to an implicit tradeoff between maintaining a sparse code and responding quickly to stimuli within each cell's rf ( fig. 4 g and h) ."
"in the clinical setting, a trial-and-error method of initialization adds to the existing variability in target delineation. hence, improvements were made to the initialization of the growcut algorithm in addition to some image preprocessing. the schematic diagram of the workflow is shown in fig. 1 and the following sections describe the steps in detail. although initially the workflow may seem complex, the steps can be easily learned."
"mcts algorithms are known for having particular scaling properties [cit] . when the number of simulations increases, their impact becomes less important. the most common explanation is that there are some issues that the algorithm is not able to deal with whatever the number of simulations is. then the algorithm reaches a plateau and increasing the number of simulations is useless. tables 5, 6 and 7 show that this behaviour holds true for this game. table 5 .scaling of the v3 algorithm.point of view of the inspector. we can see that playing the inspector is easier. when the level of the algorithm is higher the game becomes more and more balanced. table 6 .scaling of the v3 algorithm.point of view of mr jack.2n simulations against n. as expected, the advantage of having more simulations decreases with the number of simulations."
"another limitation of this study is that the igc algorithm fails to perform well in lesions surrounded by high-metabolic uptake regions, e.g., brain lesions. predominantly, this is due to the limitation of the fixed threshold of 25% to provide the initial estimate. this may be addressed by attempting a different initialization approach in future studies. since this method is currently validated only in lung cancer patients, further testing is required if it is intended for other anatomical tumor sites."
"for this experiment, we have chosen to compare versions with fixed times (1 second and 10 seconds) for each version (instead of a fixed number of simulations) because the heuristic is more time costly than the random version."
"as discussed before, the main limitation in the use of alternative hosts for synthetic biology is the lack of suitable genetic tools for genetic manipulation of these organisms. in this context, there has been tremendous progress in the development of novel genetic tools for the engineering of nonmodel and biotechnologically relevant organisms such as the cyanobacterium synechococcus sp. strain pcc 7002 (18), the grampositive bacterium streptomyces venezuelae (19), the yeast pichia pastoris (20), as well as many phylogenetically unrelated gram-negative bacteria (21) . the list of organisms for which new-generation genetic tools are available is growing at an impressive speed every year, and use of these new tools would allow the construction of even more complex circuits in several organisms of interest. finally, as the number of wellcharacterized biological parts is very limited, a potential solution could be searching for new parts in microbial genomes or metagenomes, thus allowing the expansion of the portfolio of applications that could be performed in the field (22) . attempts to overcome this bottleneck include the use of synthetic circuits to identify new functional biological parts, such as small-metabolite transporters (23) or regulatory sequences capable of inducing gene expression in response to compounds of interest (24) (25) (26) . these types of approaches are quite interesting, since synthetic circuits can be used to mine functional elements that in turn can be used to construct novel engineered strains."
"the game goes on for eight turns. when the detective puts a character on the same hex as another investigator/character and rightfully accuses him, the detective player wins. when a character is accused falsely, or jack escapes whitechapel or he manages to stay out of the hands of the police during the 8 turns, the jack player wins."
"as we expected, the third version (v3, with knowledge for choosing the power) is the best one. the second version (v2) is roughly at the same level as the first one (v1). this shows that both power and move are an important step in making a decision."
in mr jack the domain knowledge is mostly driven by the powers that any character may/must use during his turn. this is quite sticky as the use of powers can change the whole set up of the game and interfere with many if not all characters. so getting a decent knowledge domain in mrjack implies an efficient use of each power.
(c) we further explore how optimal codes change when there is a sparse latent structure in the stimulus (natural image patch; right) vs. when there is none (filtered noise; left).
"efficient coding has long been considered a central principle for understanding early sensory representations (1, 3), with wellunderstood implications and generalizations (23, 37) . it has been successful in predicting many aspects of neural responses in early sensory areas directly from the low-order statistics of natural stimuli (7, 22, 32, 43, 44) and has even been extended to higherorder statistics and central processing (45, 46) . however, a criticism of the standard theory is that it treats all sensory information as equal, despite empirical evidence that neural systems prioritize behaviorally relevant (and not just statistically likely) stimuli (47) . to overcome this limitation, bialek and coworkers (14, 15) proposed a modification to the standard efficient coding theory, positing that neural systems are set up to efficiently encode information about the future given fixed information about the past. this is motivated by the fact that stimuli are only useful for performing actions when they are predictive about the future."
"as we have seen that the third version of the mcts algorithm is the best one, we use it for all the remaining experiments. table 2 .percentage of wins of v3 against v1 and v2. each experience is the average of 100 games (splited in both jack and the inspector). as we expected the third version (v3) is by far the best version."
"the monte-carlo tree search algorithm has been recently proposed for decision-making problems [cit] . mcts has been applied to numerous applications, and in particular in games [cit] ."
"in this study, we have tried to delineate heterogeneous lung tumors using the growcut algorithm made available in the freely accessible 3d slicer software. however, the algorithm is dependent on the accuracy of user-defined initial labels. to overcome this limitation, we provide a threshold-based initial estimate of the tumor and a three-dimensional (3-d) background shell as the initial seeds. the objective of this study was to improve the existing growcut algorithm. we tested the repeatability of the improved growcut (igc) algorithm from the same observer at multiple time points in comparison with the 3d slicer growcut algorithm. we postulate that the igc algorithm would be able to provide reliable functional nsclc tumor volumes compared with the clinically used threshold-based methods. the algorithm was tested in 11 nsclc heterogeneous lesions and validated against the clinician-defined manual contour and compared against the 25% of maximum standardized uptake value [suv-(max)], 40% suv max, and adaptive threshold methods. we also tested whether the proposed igc method could offer itself as an effective initial guess for the gross disease segmentation as an alternative to the manual segmentation currently employed to delineate the nsclc. the time taken for manual gross tumor volume (gtv) contouring versus the time to edit the igc contours was assessed as a measure of efficiency in this approach."
"the growcut algorithm developed by vezhnevets and konouchine 21 follows an iterative labeling procedure, where the user-defined \"seed\" pixels try to occupy the neighbors. the automaton defines a set of local transition rules, which determines the value of each cell given the value of the cells note: patients 2* and 9** had two lesions and three lesions, respectively, with high pet uptake."
one of the 2 players is jack the ripper while his opponent plays the investigator. jack's goal is to escape the investigators taking profit of the darkness while his opponent has to catch him. jack is the only one to know who he is among the 8 starting characters.
"unlike many board games, mr jack is quite an easy game to learn. both players use the same tokens throughout the game but have different goals. and besides a simple aspect, mr jack is a deep and complex game that remains a challenge for ia applications. one of the challenges is to consider how a move of one character will affect the possible moves of many other pieces over the next turns and how to prevent the opponent to guess his future moves."
"as we have previously seen it, in mrjack a decision consists of several parts. precisely we have to take into account -the choice of one character among the possible cards."
"we can validate in the case of the game of mr jack the efficiency of some well-known techniques coming from abstract games. this is another demonstration of the genericity of the monte-carlo tree search approach and we can note that some properties of the algorithm, such as the scaling perform the same way. it could really be interesting to test other known improvements such as progressive widening and bersntein rule. the use of the famous rapid action value estimate (rave) improvement is not possible in this game, as there is no possibility of having a permutation of moves."
"the idea is to build an imbalanced partial subtree by performing many randomn simulations from the current state, and to bias theses simulations toward those giving good results. the construction is then done incrementally and consists of four different parts : selection, expansion, simulation and backpropagation as illustrated in fig. 3 . sometimes in the literature, mcts algorithm is presented in three different parts: the difference is only that the backpropagation part is presented with the expansion part."
"with xj the average reward for the node j, it is the ratio of the number of wins over the number of simulations, nj the number of simulations for the node j, ns is the number of simulations in s, and kuct is called the exploration parameter and is used to tune the trade-off between exploitation and exploration. at the end of the descent part, a node, which is outside the subtree is reached. first, this new node is added to the subtree; this is the expansion part. because this is a new node, there is no information yet, then, in order to evaluate it (as a reward is needed), a playout is performed. a playout is a random game: for both players, each decision is taken randomly until the end of the game is reached. this corresponds to the simulation part. eventually, the reward is updated in all the nodes in the subtree which have been crossed during the selection part. this last step is called backpropagation."
"most of the progress experienced in the field has been possible due to both technological and methodological improvements achieved in recent years. of note are a number of high-throughput technologies (ranging from deep dna sequencing techniques, microfluidics, automatization of molecular biology platforms, new dna synthesis procedures, etc.) which have allowed not only large-scale analyses of genes and genomes but also the fast and precise, simultaneous construction and testing of several variants of synthetic circuits in living cells (6, 8) . on the other hand, the recent rise of novel dna editing techniques, with the indubitable central role of the clustered regularly interspaced short palindromic repeat (crispr)/cas9-based approach, have allowed the enhanced modification of native organisms, either to add minor changes in its regulatory networks or to introduce entirely new synthetic circuits (9) . in addition to dna editing, the crispr/cas9 technologies have permitted editing-independent activation or repression of genes based on the usage of cas9 mutant variants not able to perform dna cleavage. in these methods, the guide rna (grna) is designed to target the promoter of the gene of interest, allowing either the recruitment of the rna polymerase (rnap) for activation of the target promoter or the blockage of binding of the rnap, leading to gene repression (10) . this type of approach is revolutionary, since it allows easy targeting of the gene of interest without the necessity of laborious engineering of the binding specificities of transcriptional factors (tfs) by amino acid mutagenesis."
"it is notable that, in our simulations, strikingly different conclusions are reached by analyzing single-neuron responses vs. the population responses. specifically, looking only at single-neuron responses would lead one to conclude that, when optimized for predictions, neurons did not encode motion direction; looking at the neural population responses reveals that the opposite is true. this illustrates the importance of population-level analyses of neural data and how, in many cases, single-neuron responses can give a false impression of which information is represented by the population."
"therefore, if the strength of the attacker is greater than that of the defender cell, the attacker overtakes the defender, leading to a change in its label and strength which are replaced by the function"
"there were no observed variations between the contours generated with the igc with multiple runs. hence, there was no uncertainty volume for the igc method. large volume variations (median 17.34 cc, range 0 to 31.84 cc) were seen in the contours produced by the 3d slicer growcut algorithm."
"how to deal with this kind of decisions is not an easy task. of course, the most accurate way should be to treat all decisions separately: one decision is one character to one location with one power (the pure mcts approach). the problem with this representation is that the branching factor becomes by far too large (see section 4.1)."
"one area of potential confusion is between different ideas of why and how neural networks may need to make predictions. for example, given low noise, efficient coding predicts that neurons should remove statistical dependencies in their inputs so as to achieve nonredundant, statistically independent responses (3, 4, (7) (8) (9) . this can be implemented within a recurrent network where neurons encode a prediction error equal to the difference between their received inputs and an inter-nally generated expectation, hence performing \"predictive coding\" (10) (11) (12) (13) . however, bialek and coworkers (14, 15) recently proposed an alternative theory, in which neurons are hypothesized to preferentially encode sensory information that can be used to predict the future, while discarding other nonpredictive information (14) (15) (16) (17) . while both theories assume that neural networks make predictions, they are not equivalent: one describes how neurons should compress incoming signals, and the other describes how neurons should selectively encode only predictive signals. signal compression requires encoding surprising stimuli not predicted by past inputs; these are not generally the same as predictive stimuli, which are informative about the future (16) ."
"another type of code that has been studied extensively is \"sparse coding\": a population code in which a relatively small number of neurons are active at any one time (18) . while there are various reasons why a sparse code may be advantageous (19) (20) (21), previous work has shown that sparse coding emerges naturally as a consequence of efficient coding of natural sensory signals with a sparse latent structure (i.e., generated by combining many sensory features, few of which are present at any one time) (22) . sparse coding has been successful in predicting many aspects of sensory neural responses (23, 24), notably the orientation and motion selectivity of neurons in the primary visual cortex (25) (26) (27) (28) (29) . nonetheless, it is unclear how sparse coding is affected by other coding objectives, such as efficiently predicting the future from past inputs."
"the descent part is done by using a bandit formula, i.e. by choosing the node j among all possible nodes cs which gives the best reward according to the formula :"
we chose to experiment three versions of mcts according to specifications introduced in the previous section. -v1 would be roughly considered as a pure mcts scheme. the tree is fully developed and explored according to the standard mcts rules.
"as mentioned earlier, predictive coding has been used to describe several different approaches. clarifying the relationship between inequivalent definitions of predictive coding and linking them mathematically to coding efficiency provided one of the ini-tial motivations for our work. in past work, alternative coding theories are often expressed using very different mathematical frameworks, impeding comparison between them and sometimes leading to confusion. in contrast, by using a single mathematical framework to compare different theories-efficient, sparse, and predictive coding-we were able see exactly how they relate to each other, the circumstances under which they make opposing or similar predictions, and what happens when they are combined."
"as introduced in section 2, a move in the game consists in each character making a move from one to three hexes. it is a bit difficult to evaluate how many hexes can be reached from a given position as a character may interfere with another character (by blocking a hex), a wall or a sewer can be opened or closed (thus opening or closing new short-cuts). we ran many simulations to evaluate the number of hexes that can be reached by one move. on average, a character can reach about 12 new hexes."
"-v3 is the most advanced mrjack version. instead of randomly applying the character's ability, a set of rules is used to, hopefully, select one of the best ability of the character. these rules are a priori rules that we built and extracted. these rules have been coded using the lua language. lua is a lightweight programming language designed as a scripting language. lua is quite flexible and many interpreters for the lua language exist."
"at the end of every other turn, there is a \"call for witness\". it means that the jack player must announce whether or not the jack character (i.e. remember that he is the only one to know who jack is) is visible or not. once the call for witness has been made, the investigators' side can possibly clear some of the characters."
"to initiate the segmentation, the user has to specify the seeds corresponding to \"foreground\" and \"background\" using different colored brush strokes, which set the initial labels and strengths of seed pixels. the strength of the seed pixels is set to one and the neighboring pixel strength to zero. the seed pixels grow over the image until some contact is made with the edges of two different labels. from that point forward, the pixels try to occupy their neighbors following the transition rule."
"the repeatability of the igc algorithm from the same observer at multiple time points was compared with the growcut algorithm made available in the 3d slicer. the user provided the initial seeds for the 3d slicer growcut algorithm as random brush strokes in two different colors. for the igc method, the preinitialized initial tumor estimate and the 3-d background shell were provided as the initial seeds. the segmentations with both the methods were performed thrice with an average interval of 3 days between each run. an uncertainty region, as described in ref. 26, was calculated as the difference between the union and intersection volumes of the three runs (represented as a, b, and c) for the growcut and igc segmentation methods:"
"regarding the first bottleneck, one potential solution would be the generation of novel technologies allowing the compression of as many regulatory interactions in the shortest dna fragment as possible. in this \"logic compression\" approach, many different tfs would recognize the same promoter, and the result of their interaction would define the expression level and expression dynamics of the gene of interest. it is worth mentioning that this approach is completely different from the synthetic circuits based on cascade of tfs. in fact, some recent progress has been made in this direction. by using genetic algorithms based on the recognition of patterns of well-known tfs, it has been possible to engineer synthetic promoters simultaneously recognized by two or three tfs, demonstrating the potential of computer models to generate functional, new (new to nature), regulatory elements in bacteria (14, 15) . while these initial works have focused on overlapping binding sites for different tfs, the construction of promoters with arrays of binding sites would potentially generate very complex regulatory patterns. in order to understand the regulatory rules necessary for the construction of these complex promoters, we recently characterized several synthetic complex promoters in e. coli. by using this approach, we unexpectedly found that complex promoters containing arrays of binding sites for different tfs are prone to emergent properties, in which the combination of simple tf-binding sites generates an expres-sion behavior that cannot be predicted on the basis of the individual behaviors (16) . this finding has strong implications not only for the engineering of complexity in bacteria but also for our understanding in how gene regulation operates in response to multiple signals in natural systems (17) ."
"the strength of mcts is that this family of algorithms performs a lot of random simulations in order to evaluate a situation. then, their use is especially relevant for applications for which it is hard to design an evaluation function. an evaluation function is a function which gives a score from a situation [cit] . alpha-beta techniques (with the different improvements) are state-of-the-art algorithms when such a function exists. in other games, mcts is now considered as a standard. the most known implementation of mcts is upper confidence tree (uct). we now present this particular algorithm."
"in fig 5 you can see that there a first node is created for move 1 and for switching gull with character number 2, the second node is for the same move and for switching gull with character number 3, and so on. in this case when dealing with the sir william gull character for these 6 possible moves, 6 x 8 nodes need to be created. (8 characters are available as it is possible for gull not to use his ability, that means that he switches his position with himself)."
"where the first term (to be maximized) is the mutual information between the responses between t − τ and τ and the stimulus at time t + ∆, while the second term (to be constrained) is the mutual information between the response at time t and past inputs (which we call the coding capacity, c ). a constant, γ, controls the tradeoff between coding fidelity and compression. this objective function can be expanded as"
"to assess the merit of the proposed igc method, the contours were compared with manual contours using the following metrics: dice similarity coefficient (dsc), the relative volume error (rve), and the 95% hausdorff distance. dsc measures the spatial overlap of the pixels in the segmented contours (s) with the pixels in the manual contour (t). a perfect agreement between contours results in value 1:"
"the combinatorial complexity of the mr jack game can be roughly estimated with the game tree size. the game tree size can subsequently be estimated by bl, where b is the average branching factor of the game, and l is the game length."
"even though manual contour in pet has its limitations of inter-and intraobserver variability owing to various window level settings used by different operators, the clinician-defined tumor volume takes into account all contextual information about the tumor from multiple imaging modalities and pathological findings. it has been argued that the clinicians usually outperform computer algorithms in the high-level task of tumor recognition. 35 this was our motivation to use manual segmentation as the gold standard to validate computerized image segmentation techniques in the absence of the surgical specimen of the tumor that serves as the gold standard. 36 due to practical difficulties in finding more experienced clinicians in contouring lung lesions based on pet imaging, we had to use a single clinician-defined volume for validation. comparison against multiple clinician-defined tumor volumes may be part of our future studies."
"in this section, we briefly sum up the rules of the mr jack board game. mr jack is a 2-player board game which aims at recreating the jack the ripper story. 8 characters are used throughout the game and one is jack."
the proposed segmentation method improves the interactive semiautomatic growcut algorithm using a threshold-based initialization. this removes any variability in the contours due to the user inputs. this study demonstrates that the igc segmentation algorithm could be easily implementable in the freely available 3d slicer platform to achieve reliable and highly repeatable functional volumes with no user inputs required for the initialization of the growcut algorithm. the igc algorithm also shows promise to serve as an effective initial guess that can well minimize the time spent on labor-intensive manual gtv delineation.
"presently, the segmentation pipeline is not fully automated, which makes the whole process seem too long. the total computational time for the preprocessing and segmentation recorded is less than 10 s. the majority of the time is spent in loading the images and choosing the appropriate parameters involved in the workflow. this limitation would be addressed when we automate the steps into a click and use module in the near future."
"with a \"two-timescale\" stimulus constructed from two markov processes that vary over different timescales (fig. 2b, middle), the optimal solution was a low-pass filter to selectively encode the predictive, slowly varying part of the stimulus. the strength of the low-pass filter increased monotonically with ∆ ( fig. 2 c and d and si appendix, fig. s2a )."
"the algorithm is simple to implement and allows the user to interactively guide the segmentation while the algorithm is still iterating. however, experiments have shown that the method is very sensitive to small changes in the user-defined seeds and often required many user seeds distributed over the entire image for an adequate segmentation."
"in this paper, we first present the game of mr jack in section 2, before presenting the mcts algorithm in section 3. then, in section 4, we present the difficulties one can have for designing an artificial intelligence for the game of mr jack. in section 5, we present the experiments and the results we get for the creation of an mcts player. conclusion and future works are then discussed."
"at the next turn, the four remaining characters are played. note that unlike many 2-player games, each player does not play alternatively. the first player (a) chooses a character and plays it, then player (b) chooses two characters and plays them. finally, a plays the last remaining character. for the next turn, the play order is b a a b. figure 1 gives an overview of the board as well as an example of seen and unseen characters. four characters can be seen (standing next to a gas-lit) while the four others remain unseen."
"an attempt to categorize the diverse types of efficient coding is presented in si appendix, efficient coding models. to consistently organize and compare these different ideas, we present a unifying framework based on the information bottleneck (ib) (30) . in our work, a small set of optimization parameters significance sensory neural circuits are thought to efficiently encode incoming signals. several mathematical theories of neural coding formalize this notion, but it is unclear how these theories relate to each other and whether they are even fully consistent. here we develop a unified framework that encompasses and extends previous proposals. we highlight key tradeoffs faced by sensory neurons; we show that trading off future prediction against efficiently encoding past inputs generates qualitatively different predictions for neural responses to natural visual stimulation. our approach is a promising first step toward theoretically explaining the observed diversity of neural responses."
"another possibility is to represent a decision as a character and one location on the board and to choose the power randomly; hoping that the impact of the position of the character on the board is more important than the impact of the power. this is a rather \"crude\" but \"classical\" way of dealing with the branching factor."
"-v2 would be considered as a slightly improved version of v1. when a node has to be created inside the mcts tree and that at this node a special ability can/must be played, only one random node is generated for the ability. in a word only the movement is taken into account. it means that, for instance if the sir william gull character is to be played, one of the eight possible characters (including gull) is randomly selected to switch his position with gull."
"in summary, we propose that major efforts in the field should be directed to the development of novel approaches focused toward engineering of complex regulatory logic in shorter dna fragments, either by assembling arrays of crispr/cas9 regulatory modules or by constructing tf-based complex promoters. consequently, this approach should reduce the limitations of handling large dna constructions and shortcomings due to the propagation of expression noise in cascades of tfs. additionally, increasing the diversity of microbial chassis using novel genetic tools will be essential for the implementation of the final circuits into hosts more suitable for the final application of interest. finally, the further development of novel theoretical concepts imported from electronic engineering, such as control theory and retroactivity (27, 28), should enhance the design of synthetic circuits with reliable performance and robust dynamics. these characteristics are crucial to allow the construction of novel microbial systems having significant impact in biotechnology."
"a third way of coping with the specific domain knowledge is to get some artificial intelligence for using the character ability. a simple heuristic may be used each time a character has to use his power, and a \"best\" move is returned. in order to keep the overall computation time to a reasonable time the heuristic for each character must be rather fast. for instance, a list of guilty and innocent characters is built as well as a list of visible and non-visible characters. each time a character is cleared it moves from the guilty list into the innocent list. the use of the heuristic is based on both these lists. it only aims at splitting the characters of the guilty/innocent lists towards a visible or non-visible."
"efficient coding of naturalistic stimuli. natural stimuli exhibit a strongly non-gaussian statistical structure, which is essential for human perception (22, 41) . a large body of work has investigated how neurons could efficiently represent such stimuli by encoding their nonredundant or independent components (4) . under fairly general conditions (e.g., that stimuli have a sparse latent structure), this is equivalent to finding a sparse code: a form of neural population code, in which only small fractions of neurons are active at any one time (22) . for natural images, this leads to neurons that are selective for spatially localized image contours, with receptive fields (rfs) that are qualitatively similar to the rfs of v1 simple cells (25, 26) . for natural movies, this leads to neurons selective for a particular motion direction, again similar to observations in area v1 (27) ."
determines the goals and constraints faced by sensory neurons. previous theories correspond to specific values of these parameters. we investigate the conditions under which different coding objectives have conflicting or synergistic effects on neural responses and explore qualitatively unique coding regimes.
"a major challenge in sensory neuroscience is to derive the observed cell-type diversity in sensory areas from a normative theory. for example, in visual area v1, one observes a range of different cell types, some of which have spatiotemporally separable rfs and others do not (48, 49) . the question arises, therefore, whether the difference between cell types emerges because different subnetworks fulfill qualitatively different functional goals. one hypothesis, suggested by our work, is that cells with separable rfs have evolved to efficiently encode the future, while cells with nonseparable rfs evolved to efficiently encode the past. more generally, the same hypothesis could explain the existence of multiple cell types in the mammalian retina, with each cell type implementing an optimal code for a particular choice of optimization parameters (e.g., coding capacity or prediction lag)."
"let us have a look at two different characters and see how their power can affect the game tree size. if we look closely at the miss stealthy character she can make a move between 1 to 4 hexes and she can cross during her move any hex including building hex. actually, from our simulations she usually can reach almost any hex of the board. the board is actually made of 54 hexes (let us say 50 hexes to make the computations easier). it means that each time this character is used the branching factor is around 50 instead of 12."
we chose the lua language as the rules for our expert knowledge program can be tuned off-line. they are easy to understand and can possibly be refined without being compiled. furthermore it is not necessary to know the insight of the mrjack code to modify the rules.
"testing such hypotheses rigorously against quantitative data would require us to generalize our work to nonlinear encoding and decoding models (si appendix, table s1 ). here, we focused on a linear decoder to lay a solid theoretical foundation and permit direct comparison with previous sparse and robust coding models, which also assumed a linear decoder (25-27, 35, 36) . in addition, a linear decoder forces our algorithm to find a neural code for which information can be easily extracted by downstream neurons performing biologically plausible operations. while the linearity assumptions simplify our analysis, the framework can easily accommodate nonlinear encoding and decoding. for example, we previously used a \"kernel\" encoding model, where neural responses are described by a nonparametric and nonlinear function of the input (31) . others have similarly used a deep convolutional neural network as an encoder (50) ."
"despite the remarkable improvement undergone in the field of synthetic biology, a number of bottlenecks are emerging. first, most of the engineered circuits are limited to one or few inputs, and usually the construction of more-complex circuits requires several tfs and cognate promoters, leading to final circuits composed of several kilobases of dna fragments. for example, a recently engineered synthetic circuit allowing escherichia coli to sense and respond to three wavelengths of light (red, green, and blue) required the engineering of around 48 kb of rewired genetic material (11) . this feature strongly limits the complexity of the circuit that can be engineered and implemented in the host cell. second, most of the synthetic biology work has focused on model organisms such as e. coli and saccharomyces cerevisiae for which there are well-established genetic tools. yet, many applications relevant to biotechnology or with biomedical significance require a different type of host (or chassis), more adapted to the final environment of interest. for instance, bacteria engineered to invade cancer cells focus on the usage of salmonella enterica serotype typhimurium (12), while a more metabolically robust bacterium such as pseudomonas putida would be required for industrial applications with process-specific parameters (13) . finally, the number of biological parts well characterized for synthetic biology applications is very limited, and more variants have to be mined from different sources."
the algorithm is iterated till the automaton converges to a stable configuration and there are no more local label updates. the algorithm always converges since the cell strength is increasing and bound.
"eventually, in order to get a decent artificial intelligence, game studies of mcts have shown that the inclusion of domain knowledge can significantly improve the performance of the algorithm [cit] ."
"for example, the heuristic for sergeant goodley brings the characters from one of these two lists closer to a gaslight to try to make them visible or non-visible."
"in our implementation, we investigated several ways of including domain knowledge: an implicit approach where each power increases the branching factor of the nodes when it is used and a specific domain knowledge. we call this approach pure monte-carlo as the specific domain knowledge is fully embedded in the mc tree."
"icroorganisms play a central role in human life, not only because they cause many diseases but also because they are used in several industrial processes. in addition to their critical importance for humankind, their fast growth in laboratory conditions and relatively easy genetic manipulation have allowed us to gather a tremendous amount of information regarding the molecular details of cell physiology and about the way microorganisms process information. in particular, the collection of this information in well-curated data banks and the advent of genomic and postgenomic approaches permitted the field of synthetic biology to emerge, initially based on microbial engineering (1) . in the main workflow of synthetic biology, living cells are treated analogous to electronic devices, and cell engineering is executed through design-buildtest cycles (1) . in this sense, synthetic biology approaches are directed to modify the regulatory network of the cell, enabling organisms to perform novel molecular functions, with applications ranging from biosensing toxic or dangerous compounds, to searching for and killing pathogens and cancer cells, or producing compounds of interest, among many others (2) . in this context, it is critical that the gene regulatory system of the target organism is modified to perform precise and reliable computations based on a set of inputs of interest, and for this, the analogy to logic gates is very useful (3, 4) . over the years, a considerable amount of effort has been dedicated toward the construction of reliable logic circuits, particularly in bacteria. for this, features such as noise in gene expression (5), composability of the biological parts used (6), signal detection range, and specificity of the circuits have been of special interest (2, 7) . thus, significant advancement has been experienced in the field, and along with it, our basic understanding of the molecular mechanisms related to the control of gene expression through the many cellular regulatory networks of interest has increased dramatically."
"there is a lua script for each character. in most scripts, the program iterates through the different characters to evaluate how far they are from the calling character. the scripts then make a decision based on these information. based on our experiments, these scripts do not add a lot of extra computation time. for instance, for sergeant goodley, the script tries to bring some characters closer or away to lit hexes so that they can possibly be cleared or not at the end of the call. the number of nodes for the gull character remains as low as the number of nodes needed in the v2 version."
"the implications of such a coding objective have remained relatively unexplored. existing work only considered the highly restrictive scenario where neurons maximize information encoded in their instantaneous responses (15, 38, 40) . in this case (and subject to some additional assumptions, such as gaussian stimulus statistics and instantaneous encoding filters), predictive coding is formally equivalent to slow feature analysis (39) . this is the exact opposite of standard efficient coding models, which (at low noise/high capacity) predict that neurons should temporally decorrelate their inputs (3, 33) ."
"in fig. 7(a), the absolute volumes of the autosegmented contours were compared with the clinician-defined manual contour volumes. except for lesion 11, all the other volumes correlated well. the relative error in the tumor volumes for igc, t adaptive, t 25, and t 40 segmentations against manual-defined contours is shown in the box whisker plots in fig. 7(b) ."
"so, the game can roughly be summed up as a deduction game. the investigator will try to narrow down his search by splitting the innocent or guilty characters into smaller and smaller sets while jack will try to get the investigator confused by preventing him from building these sets."
"in fig 6 you can see that only 6 nodes for gull are created, one node for each move. the special ability is randomly chosen. it is quite obvious that such a random choice for the ability may seriously impede the ai overall level but it allows to greatly reduce the search space. in this case when dealing with the sir william gull character for these 6 possible moves, only 6 nodes need to be created. a random character is randomly selected to switch his position with gull's."
"then, by utilizing the nonlinear eigenspace of cluster k, the proposed method can estimate the missing highfrequency components. in this scheme, we, respectively, use approximations 2 and 3 once through equations 31-41."
"more importantly, the specific term networks drastically outperform general-purpose sources such as wordnet. the standard dictionary wordnet achieves for all us classes only low performance in recall. this may be attributed to the fact that patent searchers, (1) expand class-specific query terms using general terms, (2) expand query terms w.r.t. part of speech, (3) relate terms, which have the same meaning in a specific class, (4) use popular trademarks and (5) patent applicants are allowed to create their own terms for query expansion. these kinds of synonyms, equivalents and relations between the vocabulary are not included in standard dictionaries, such as wordnet, but are needed for automatic query expansion in the patent domain."
"in sect. 5.2, we addressed the low precision performance of the term networks learned from large classes achieving highest recall and coverage scores. in this section we evaluate the performance of the class-specific term networks when used for patents from other classes. we assume that this will help to detect classes where cross-domain applications might be useful, in particular to improve recall and coverage of small classes providing accurate precision scores. we test the term networks ts2 of the small and medium classes and the term networks ts3 of the large classes across test sets from other classes without excluding out-of-vocabulary words. we have not considered out-of-vocabulary words, because our goal is to detect related class having the most common synonyms. across different patent classes we cannot assume that expansion terms were used later in time. table 9 shows the achieved recall measures."
"the experiments show for the class-specific term networks, that recall measures increase with the availability of a larger set of query logs. best recall measures are provided, on average, by the term networks learned from training sets with a size larger than 6,000 query logs. the class-specific networks provide up to 7 out of 10 synonyms, which are used by the patent examiners for query expansion. we assume that the recall measures will further increase with the rise of the training set size. because the uspto publish new query logs regularly for each us class, the size of the collections rises automatically. this will be advantageous for our expansion approach."
"through the analysis of the failed synonym relations provided by wordnet we learn that (1) patent examiners expand class-specific query terms using general terms. for example, in the class 379 called ''telephonic communications'' they expand the specific query term ''cellphone'' using the general expansion term ''device''. a further example is the expansion of the class-specific term ''camper'' with the general term ''vehicle''. further (2) the examiners expand query terms w.r.t. part of speech, such as ''burn'' for ''burning'' or ''coat'' for ''coating''. furthermore (3) they relate terms, which have the same meaning in a specific classes, such as ''portable'' for ''handheld'', in particularly for the class 379 called ''telephonic communications''. additionally, through analysis of the vocabulary, which is not covered by wordnet, we find out, that patent examiners (4) use popular trademarks, such as ''iphone'', ''ipad'' or ''blackberry'' for query expansion. further, (5) the patent bold values indicate the best performing training sets for each class applicants are allowed to create their own terms, such as ''pocketpc'' for ''notebook'', ''watergas'' for ''steam'' or ''passcode'' for ''password''. because of these highly specific expansions of query terms in the patent domain, standard dictionaries, such as wordnet, achieve only low performance. in these standard dictionaries, such patent domain specific vocabulary and relations are not included. but even these kinds of synonyms, equivalents and relations between the vocabulary are needed for automatic query expansion in the patent domain. using our approach to learn term networks from the patent domain and directly from the query logs of patent examiners fulfills the requirements of this highly domain specific query expansion. finally, the experiments show, that the term networks learned directly from the patent domain, in particular lognet and the best performing class-specific term networks, drastically outperform the general term network wordnet. the standard dictionary wordnet achieves for all us classes only low performance in recall. as expected, all term networks, the patent domain specific and the general term network wordnet, achieve low precision measures. the reason for that is, that we have not considered the meaning of the query terms. in sects. 5.1 and 5.2, we show how to counter the low precision values of term networks when used for automatic query term suggestion by (1) suggesting class-specific expansion terms first, followed by additional expansion terms provided by the classindependent term network; or (2) by suggesting expansion terms provided by the classindependent term networks in the order of their support in the training set."
"furthermore, to avoid the problems of previously reported methods, we introduce two novel approaches into the estimation of missing high-frequency components for the corresponding patches containing low-frequency components obtained from a target lr image: (i) an inverse map, which estimates the missing high-frequency components, is derived from a degradation model of the lr image and the two nonlinear eigenspaces of each cluster and (ii) classification of the target patches is performed by monitoring errors caused in the estimation process of the missing high-frequency components. the first approach is introduced to solve the problem of the assumptions utilized in the previously reported methods. then, since the proposed method directly derives the inverse map of the missing process of the high-frequency components, we do not rely on their assumptions. the second approach is introduced to solve the outlier problem. obviously, it is difficult to perfectly perform classification that can avoid this problem as long as the high-frequency components of the target patches are completely unknown. thus, the proposed method modifies the conventional classification schemes utilizing distances between lr patches directly. specifically, the error caused in the estimation process of the missing high-frequency components by each cluster is monitored and utilized as a new criterion for performing the classification. this error corresponds to the minimum distance of the estimation result and the known parts of the target patch, and thus we adopt it as the new criterion. consequently, by the inverse map determined from the nonlinear eigenspaces of the optimal cluster, the missing high-frequency components of the target patches are adaptively estimated. therefore, successful performance of the sr can be expected. this paper is organized as follows: first, in section 2, we briefly explain kpca used in the proposed method. in section 3, we discuss the formulation model of lr images. in section 4, the adaptive kpca-based sr algorithm is presented. in section 5, the effectiveness of our method is verified by some results of experiments. concluding remarks are presented in section 6."
"then, from equations 23 and 25, the criterion c in equation 12 can be calculated. it should be noted that for calculating the criterion c, we, respectively, use approximations 1 and 2 once through equations 21-25."
"in the above equation, the vector of the original hr patch is approximated in the nonlinear eigenspace of cluster k, where we call this approximation [approximation 3]. the nonlinear eigenspace of cluster k can perform the least-square approximation of its belonging elements. therefore, if the target local patch belongs to cluster k, accurate approximation can be realized. then the proposed method introduces the classification procedures for determining which cluster includes the target local patch in the following explanation. next, by substituting equations 26 and 30 into equation 28, the following equation is obtained:"
"hence, the experiments show that the query logs are valuable resources to learn term networks from the patent domain for query expansion achieving high recall and precision scores."
"note that (i, j)th element of k is obtained by j(x i )'j (x j ). in kernel methods, it can be obtained by using kernel trick [cit] . specifically, it can be obtained by some kernel functions (x i, x j ) using only x i and x j in the input space."
"again, we measure coverage of the respective networks by determining the number of out-of-vocabulary words. table 11 shows the vocabulary covered by the term networks. wordnet being the most comprehensive thesaurus provides best coverage followed by lognet. best coverage is provided by wordnet for the class 128 at 98.55 %. lognet has the highest coverage for class 384 at 96.04 %."
"the experiments show, that we can use the confidence values to iteratively suggest an increasing number of expansion terms as the search evolves. this allows the system to strike a reasonable balance between increasingly higher recall/coverage by suggesting additional expansion terms that have a lower support in the training set at the cost of lower precision after having initially suggested the most likely, highest-precision expansion terms."
"bold values indicate the best performing training sets for each class vocabulary, in some cases, particularly for classes 180 and 417, with larger training sets the recall scores go down. the reason for that is, with the larger training sets more synonyms and equivalent terms appear in the term networks (the terms are not out of vocabulary any more) but not necessarily as synonyms. best recall measures are provided, on average, by the term networks learned from the training sets of the large us classes with a size larger than 6,000 query logs (with one exception, class 128). in particular, best recall is provided by the term network ts5 learned for the class 623. the term network ts5 provides with a recall of 73.33 %, on average, 7 of 10 synonyms, which are used by the patent examiners for query expansion."
"in patent searching the boolean operator ''or'' is used to expand a query term with an expansion term, which has the same meaning, such as ''drill'' for ''burr'' or ''tool'' for ''instrument'' in the medical domain concerning dentistry equipment. we use that for automatically detecting synonyms in the query logs based on the boolean operator ''or'', which indicates that two query terms are synonyms, or can at least be considered as equivalents. the process works as follows: we extract all text queries including the search operators between the query terms from the query log collection. we then filter all 3-g generated from the text queries in the form ''x b y'', where b is the boolean operator ''or'' and x and y are query terms. in addition, to exclude mismatches and misspellings and for ranking of the extracted synonyms according to their support in the specific classes, in particular for suggesting initially the synonyms having the highest support followed by additional terms that have a lower support later-on, we utilize a confidence value cv. we measure the frequency of each synonym in the specific class. table 3 shows for each class the number of extracted synonyms based on the confidence values cv 1 to cv 5, i.e. that have a support greater than or equal to 1-5."
"in this paper, we have presented an adaptive sr method based on kpca with a novel texture classification approach. in order to obtain accurate hr images, the proposed method first performs clustering of the training hr patches and derives an inverse map for estimating the missing high-frequency components from the two nonlinear eigenspaces of training hr patches and their corresponding low-frequency components in each cluster. furthermore, the adaptive selection approach of the optimal cluster based on the errors caused in the estimation process of the missing high-frequency components enables each hr patch to"
"for larger training sets the recall measures of the learned term network lognet increases. for the large classes lognet is learned from training sets having more than 6,000 query logs files and provides, on average, for all classes, best recall scores. compared to lognet and the best performing class-specific networks, wordnet achieves only low recall for all classes. a comparative performance is only achieved for class 454. over all classes, wordnet provides, on average, only a recall of 22.06 %. comparing the precisions measures, wordnet achieves as expacted, like lognet, only weak precision across all classes, peaking at 5.49 % for class 398."
"where f and f are, respectively, vectors whose elements are the raster-scanned intensities in the lr image f and its corresponding hr image f. therefore, the dimension of these vectors are, respectively, the number of pixels in f and f. d and b are the decimation and blur matrices, respectively. the vector n is the noise vector, whose dimension is the same as that of f. in this paper, we assume that n is the zero vector in order to make the problem easier. note that if decimation is performed without any blur, the observed lr image is severely aliased."
"as shown in figure 1, by upsampling the target lr image f, we can obtain the blurred hr imagef . however, it is difficult to reconstruct the original hr image f fromf since the high-frequency components of f are missed by the blurring. furthermore, the reconstruction of the hr image becomes more difficult with increase in the amount of blurring [cit] ."
"case 1 : lr images are captured based on the lowpass filter followed by the decimation procedure, and any aliasing effects do not occur, where this case corresponds to our assumption. therefore, we should estimate the missing high-frequency components removed by the low-pass filter."
"we focus only on case 1 to realize the sr, but some comparisons between our method and the methods focusing on case 2 are added in the experiments."
"in the field of image processing, high-resolution images are needed for various fundamental applications such as surveillance, high-definition tv and medical image processing [cit] . however, it is often difficult to capture images with sufficient high resolution (hr) from current image sensors. thus, methodologies for increasing resolution levels are used to bridge the gap between demands of applications and the limitations of hardware; and such methodologies include image scaling, interpolation, zooming and enlargement."
"an adaptive sr method based on the kpca with a novel texture classification approach is presented in this section. figure 2 shows an outline of our method. first, the proposed method clips local patches from training hr images and performs their clustering based on the kpca. then two nonlinear eigenspaces of the hr patches and their corresponding low-frequency components are, respectively, obtained for each cluster. furthermore, the proposed method clips a local patchĝ from the blurred hr imagef and estimates its missing high-frequency components using the following novel approaches based on the obtained nonlinear eigenspaces: (i) derivation of an inverse map for estimating the missing high-frequency components of g by the two nonlinear eigenspaces of each cluster, where g is an original hr patch ofĝ and (ii) adaptive selection of the optimal cluster for the target local patchĝ based on errors caused in the high-frequency component estimation using the inverse map in (i). as shown in equation 9, estimation of the hr image is ill posed, and we cannot obtain the inverse map that directly estimates the missing high-frequency components. therefore, the proposed method models the degradation process in the lower-dimensional nonlinear eigenspaces and enables the derivation of its inverse map. furthermore, the second approach is necessary to select the optimal nonlinear eigenspaces for the target patchĝ without suffering from the outlier problem. then, by introducing these two approaches into the estimation of the missing high-frequency components, adaptive reconstruction of hr patches becomes feasible, and successful sr should be achieved."
"in the previously reported methods, the obtained hr images tend to be blurred in edge and texture regions. in detail, the proposed method keeps the sharpness in edge regions of test image lena as shown in figure 7 . furthermore, in the texture regions which are shown in figure 8, the difference between the proposed method and the other methods becomes significant. furthermore, in figures 9 and 10, the center regions contain more high-frequency components compared with the lr image of (a). hr image reconstructed by (c) sinc interpolation, (d) reference [cit], (e) reference [cit], (f) reference [cit], (g) reference [cit], (h) reference [cit], and (i) the proposed method."
"also whole documents or whole sections of the query documents, like the title, abstract, description or the claim section are used for query generation and query expansion )."
"from the above explanation, we can seec k in equation 45 is a suitable criterion for classifying the target local patch into the optimal cluster k opt . then, the proposed method regards h k opt estimated by the selected cluster k opt as the output, and l + h k opt becomes the estimated vector of the target hr patch g."
"generally, actual lr images captured from commercially available cameras tend to be taken without suffering from aliasing. thus, we assume that such captured lr images do not contain any aliasing effects. however, it should be noted that for realizing the sr, we can consider several assumptions, and thus, we focus on the following three cases:"
blurred hr imagef whose vector is bf is obtained by applying the low-pass filter to the hr image f. its size is the same as that of the hr image.
"the proposed method enables the calculation of the inverse map which can directly reconstruct the high-frequency components. in the previously reported methods [cit], they simply project the known frequency components to the eigenspaces of the hr patches, and their schemes do not correspond to the estimation of the missing high-frequency components. thus, these methods do not always provide the optimal solutions. on the other hand, the proposed method can provide the optimal estimation results if the target local patches can be represented in the obtained eigenspaces, correctly. this is the biggest difference between our method and the conventional methods."
"that may invalidate the patent. patent searching is usually performed by examiners in a patent office and patent searchers in private companies [cit] . for searching patent databases the patent searchers follow a strict scheme. they compartmentalize the invention into searchable features and expand the features using synonyms, equivalents and co-occurring terms, using the concept of an invention diagram. to narrow the search topic, query terms are specialized into keyword phrases [cit] . table 1 shows an example of a diagram including the features of the invention completed with expansion terms as they are used for query generation by the patent searchers."
"in particular, we test the best performing class-specific term networks, the class-independent network lognet and the dictionary wordnet based on the test sets generated for each specific class and used in the sects. 5.1 and 5.2."
"finally, we show some experimental results obtained by applying the previously reported methods and the proposed method to actual lr images captured from a commercially available camera \"canon ixy digital 50\". we, respectively, show two test images in figures 19a and 20a and their training images in figures 19b, c and 20b, c. the upper-left and lower-left areas in figures 19a and 20a, respectively, correspond to the target images, and they were enlarged by the previously reported methods and the proposed method as shown in figures 21 and 22, where the magnification factor was set to eight. it should be noted that the experiments were performed under the same conditions as those shown in figures 4, 5, and 6 . from the obtained results, we can see that the proposed method also realizes more successful reconstruction of the hr images than those of the previously reported methods. as shown in figures 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, the difference between the proposed method and the previously reported methods becomes more significant as the amount of the high-frequency components in the target images becomes larger. in detail, regions at sculptures and characters, respectively, shown in figures 21 and 22 have successfully been reconstructed by the proposed method."
"there are several kinds of queries in the search query element as shown in fig. 1 . text queries, such as queries s1 and s2, are used for querying whole documents (fulltext search) or only sections of patent documents, such as the title section (title search) using query terms. non-text queries are used for searching patent document numbers, classifications, or application and publications dates. for example, the non-text query ''@ [cit] 0604'' is used for searching patent documents applied before 4th [cit], as shown in query s4. a further kind of query is the reference query, such as query s3, which is a combination of earlier queries, in particular of query s1 and s2, i.e. re-using the terms of a previous query and expanding it with further elements, thus avoiding to have to re-type an earlier query. text queries include search operators between the query terms. the types of search operators are (1) boolean operators, such as ''and or or'' and (2) proximity operators, like ''same, adj(acent), near, or with''. furthermore, truncation limiters, such as ''$'', are used for query formulation. if the search operators are added manually, they are shown between the query terms in the text query element, else they are indicated by the default operator element. for our purposes we are specifically interested in the queries including the boolean operator ''or'' to learn synonyms."
"further, the experiments show that for almost all classes the recall measures of the class-specific term networks can be further improved using the class-independent term network lognet. lognet suggests, on average, up to 8 out of 10 synonyms, which are used by the examiners for query expansion. expectedly the class-independent term network achieves lower precision scores."
"with the increasing class size the coverage of the networks obviously increases. best coverage scores of the class-specific term networks are provided by the term networks learned from the large classes. in particular, the term network learned for class 379 provides 81.48 % of the query terms from the test set. for all classes, on average, the classindependent term network lognet provides a coverage of 87.90 %."
"in this paper we radically extended these initial studies, proving a method to create valuable domain-specific term networks to assist in query expansion in this highly professional search setting. we collected and preprocessed a significantly larger corpus of patent query logs, facilitating more in-depth studies and providing reasonably comprehensive term networks. each query log of the uspto is a pdf file consisting of a series of queries. figure 1 shows an example of such a query log. each query has several elements. we focus on the search query element showing the query formulated by the patent examiner. further elements are: reference, hits, database(s), default operator, plurals, and time stamp."
"as mentioned in sect. 2, most approaches use standard dictionaries for automatic query expansion, particularly for finding synonyms. in this section we evaluate the performance of our approach compared to the dictionary wordnet [cit] ."
"first, the vector j* is projected onto the d k -dimensional nonlinear eigenspace of cluster k by using the eigenvector matrix u k as follows:"
"case 2 : lr images are captured by only the decimation procedure without using any low-pass filters. in this case, some aliasing effects occur, and interpolationbased methods work better than our method."
"in this section we use the term networks for automatic query expansion in patent searching. for the evaluation we expand query terms of queries from real query sessions of patent examiners (gold standard). based on the suggested terms from the term networks and the expansion terms used by the examiners, we calculate recall and precision scores to evaluate the performance of the learned term networks."
"query logs are being intensively studied in many information retrieval settings, specifically for web searchers [cit] . the main focus is on the analysis of the queries to enhance searches [cit] . in previous research for learning lexical term networks using query logs, terms are extracted directly from the query log collection. relations, specifically synonym relations, are generally learned by using external sources, such as lexica, glossaries or databases like wordnet [cit] . further relations are retrieved by analyzing the retrieved, particularly the clicked, documents. if two queries are related with the same document, these two queries are associated with each other [cit] . all approaches to find synonyms depend on external sources such as lexica or glossaries. these approaches do not utilize relations between the query terms in the query logs. yet, we need these and can use these for learning synonyms in the patent domain due to the radically different setting in which patent searches are performed. contrary to conventional query logs, where searchers are usually one-step single query events, patent search sessions extend over many queries that are gradually refined, and that rely more heavily on the use of synonyms to ensure coverage, as patent applicants are permitted to be their own lexicographers, i.e. they can define their own terminology."
"in this paper we presented an approach to support query expansion in the domain of patent search. we used real query expansion sessions done by patent professionals to learn term networks for automatic query expansion. several us class-specific term networks and a class-independent term network, referred to as lognet, were learned from the query logs provided by the uspto to capture synonym relations. we evaluated these term networks based on query expansion done by patent professionals in real sessions."
", where d k is the dimension of the eigenspace of cluster k, and it is set to the value whose cumulative proportion is larger than th. the value th is a threshold to determine the dimension of the nonlinear eigenspaces from its cumulative proportion. furthermore,"
"as shown in table 9 the learned class specific term networks achieve respectable recall measures across some class boundaries. in particular, the term network ts3 learned for the us class 128 called ''surgery'' achieves a recall measure of 29.73 % for class 623 called ''prothesis''. this hints at the fact that term networks learned from one class may, in fact, be applied across related classes. yet, there is no guarantee that this will always work for each pair of related classes. for example the term network learned from query logs of class 148 called ''metal treatment'' achieves at 12.68 % a better recall measure for class 384 called ''bearings'' than the corresponding term network learned from class 384 when applied to class 148 with a recall score of only 4.59 %. as known, ''bearings'' are generally created from metal material. ''metal treatment'' is a common process for manufacturing bearings and popular in the class 384. yet, ''bearings'' may be irrelevant for treating metal and thus for the class 148 leading to a non-bijective relationship. ts1 ts2 ts3 ts4 ts5 ts1 ts2 ts3 ts4 bold values indicate the best performing training sets for each class table 9 recall measures achieved when using class-specific term network for other than the class they were based upon hence, the experiments show that for the classes marked in bold cross-domain applications seems promising. this can be used for expanding smaller sets of query logs of specific classes to learn improved term networks of related classes providing improved recall and coverage scores, i.e. for classes where few query logs are available."
"furthermore, we evaluate the class-independent term network lognet. in almost all classes the recall measures of the class-specific networks ts1 to ts5 are further improved. in particular, best recall is provided for class 417 with a recall of 78 %. as explained above, in two cases the recall decreases because of the synonyms that appear in lognet, but not as synonyms. mirroring the trend observed with the class-specific term networks, lognet achieves only weak precision measures across all classes, peaking at 7.14 % for class 128."
"unfortunately, it is well known that the selected hr patches are not necessarily optimal for the target lr patches, and this problem is known as the outlier problem. this problem has also been reported by datsenko and elad [cit] ."
"as expected, we notice for all classes a considerable decrease of the number of synonyms having a support between 2 and 5. because patent searching is a recall orientated total number of extracted synonyms based on cv 2 (64,750) set in bold task, we consider those synonyms that were encountered at least two times as synonyms (cv 2 ) in the specific class to learn the term networks. this reduces spurious mismatches, but provides as many synonyms for automatic query term suggestion as possible. table 4, shows the learned term networks which resemble thesauri of english concepts. the thesauri provide english synonyms for each specific patent class. in each term network terms that have the same meaning are linked to each other. in total, the learned term networks provide 64,750 unique synonym relations based on 36,601 unique query terms. finally, the learned term networks can be used, particularly in each specific patent us class, for (semi-) automated query suggestion, particularly query expansion."
"as shown in table 2 the number of query logs for the classes differs between 1,820 and 16,864 files. for our experiments, particularly for learning the class-specific term networks and for evaluating them, we conceptually grouped the classes according to their size: (small) having less than 4,000 query logs (medium) having up to 8,000 files and (large) having more than 8,000 logs. the grouping allows us to assess, on how far the performance of class specific term networks depend on the class size (number of query logs) and whether a minimum number of query logs is needed to achieve accurate performance in automatic query expansion. furthermore, we selected some classes that are topically related (e.g. classes 384 and 148; or classes 128, 433 and 623 from the medical domain) as well as completely disjunct classes to evaluate in how far term networks can be learned on a more generic level, using the hierarchical relationship between classes."
"in the experiments, the parameters of our method were set to simple values from some experiments. these parameters should be adaptively determined from the observed images. thus, we need to complement this determination algorithm."
"the experiments show that for almost all classes the recall and coverage measures of the class-specific term networks rise with the class size and can be further improved using the class-independent term network lognet. on the other hand, the class-specific term networks achieve much better precision scores than lognet. in this case, query terms are expanded in a certain context. to provide term networks for automatic query term suggestion achieving high recall/coverage and precision scores, either (1) the recall measures of the class-specific term networks or (2) the precision scores of lognet have to be improved. we address this issue in the following experiments. table 6 query expansion based on training set size and class ts1 ts2 ts3 ts4 ts5 lognet ts1 ts2 ts3 ts4 to optimize precision values we re-run the experiments for the large classes which achieve highest recall and coverage scores but provide the lowest precision measures of the classspecific term networks utilizing the confidence values for the synonym relations. in this setting, we only considered those expansion terms that were encountered at least five times as synonyms in the training sets, i.e. that have a higher support for the respective mapping in the training set. the resulting scores are provided in table 8 . while the recall achieved with this more limited term network obviously decreases considerably, we also observe a drastic increase in precision compared to the values provided in table 6 . for example, the term network ts5 of class 422 provides with a recall of 59.83 %, on average, 6 of 10 synonyms, which are used by the patent examiners for query expansion. the corresponding limited term network provides with a value of 22.00 % a considerable lower recall score. but a drastic increase in precision can be observed. the limited term network achieving a precision score of 23.05 % drastically outperform the term network ts5 providing a value of 7.22 %."
"in future work we aim to improve the performance of the learned class-specific and classindependent term networks. to this end, we want to learn class-related term networks to improve recall and coverage of the class-specific term networks that achieved best precision measures. further, we want to use a dynamic thresholding on the support scores for synonym relationships, in particular for the class-independent term network lognet providing best recall and coverage scores but lowest precision measures. thus, we will be able to sort the expansion terms in such a way as to optimize precision while keeping recall high. in addition, we want to consider the default operator element and the set operator (which can be set at ''and'' or ''or'') to learn synonyms from co-occurring query terms, which are not linked to each other by the boolean operator ''or'' to expand our term networks."
"the remainder of the paper is organized as follows. we first review related work on automatic query expansion in patent searching and enhancing query generation using query logs. in sect. 3 we present the structure and characteristics of query logs of uspto patent examiners. we then present our approach to learn synonyms from the query logs and the term networks each learned for a specific us patent class in sect. 4. experiments based on query expansion done by patent examiners are provided in sect. 5, followed by conclusions and an outlook on future work in sect. 6."
"as shown in table 6, for almost all classes the recall measures increase with the increase in training set size. specifically, we can assume that the recall scores will further increase with even larger training sets. because we excluded synonyms that are out of the 128 500 6,000 7,000 8,000 --379 500 6,000 7,000 8,000 9,000 -422 500 6,000 7,000 8,000 9,000 10,000 439 500 6,000 7,000 8,000 9,000 10,000 623 500 6,000 7,000 8,000 9,000 10,000"
"the precision values show, that with increasing training set sizes the achieved precision scores decrease as the number of suggested synonyms increases. the term network ts3 learned for class 128 provides with a score of 44.17 % best precision. on average, 4 out of 10 terms that are suggested by the term network as synonyms were actually used by the examiners for query expansion. considering the term network providing the best recall performance (ts5 for class 623) on average only 2 out of 10 terms suggested are used by the patent examiners for query expansion. note that the lower precision may not be a serious impediment for deployment of the query term expansion: as patent search is recall-oriented rather than precision-oriented, i.e. preferring a higher number of potentially irrelevant documents in a result set over a more limited result set missing relevant documents, especially when suggested by a system for manual deployment rather than performed in a fully autonomous manner, may be assistive rather than harmful. furthermore, precision is likely to be under-estimated, as the fact that certain suggested expansion terms are incorrect rather than potentially useful but having been not thought of by the searcher would need to be confirmed by expert searchers. this would help to determine whether the terms suggested but not used in the original patent verification search are actually wrong, or whether the examiner performing the validation search simply did not think them."
"overall, we downloaded and preprocessed 103,896 query logs available for the fifteen us classes, making it the largest collection of query logs used for experiments in the patent ir domain. table 2 shows the number and title of the selected classes and the number of downloaded query logs for each class."
"note that the hr image, the blurred hr image, and the high-frequency components have the same size. in order to define the blurred hr image, the lr image, and the high-frequency components, we have to provide which kind of the low-pass filter is utilized for defining the matrix b. generally, it is difficult to know the details of the low-pass filter and provide the knowledge of the blur matrix b. therefore, we simply assume that the low-pass filter is fixed to the sinc filter with the hamming window in this paper. in the proposed method, high-frequency components of target images must be estimated from only their low-frequency components and other hr training images. this means when the high-frequency components are perfectly removed, the problem becomes the most difficult and useful for the performance verification. since it is well known that the sinc filter is suitable one to effectively remove the highfrequency components, we adopted this filter. furthermore, the sinc filter has infinite length coefficients, and thus we also adopted the hamming window to truncate the filter coefficients. the details of the low-pass filter is shown in section 5. since the matrix b is fixed, we discuss the sensitivity of our method to the errors in the matrix b in section 5."
"as described above, the approximation of the matrix u k is performed. this is a common scheme in kpcabased methods, where we call this approximation [approximation 2], hereafter. since the columns of the matrix u k are infinite-dimensional, we cannot directly use this matrix for the projection onto the nonlinear eigenspace. therefore, to solve this problem, the matrix u k is approximated by equation 18 for realizing the kernel trick. note that if d k becomes the same as the rank of ξ k, the approximation in equation 18 becomes equivalent relationship."
"first we use the specific term networks ts1 to ts5 of each class and the class-independent term network lognet to evaluate the performance of the term networks based on the size of the training sets and for each class. for calculating the recall and precision measures we excluded synonyms in the test sets which are out of the vocabulary of the term networks, i.e. terms that did not appear in any earlier query log. table 5 shows the achieved recall and precision measures."
"in addition, to see if the differences between the results of wordnet and the best performing query log based expansion method, in particular lognet, were statistically relevant, we run a t test. the test allows us to conclude that there were a statistically significance (p \\ 0.05). with respect to the measures for each specific class achieved by wordnet and lognet, the t test confirms that for all classes except class 280 (0.27), the differences are significant (p \\ 0.05)."
"in order to realize the adaptive sr algorithm, the training hr patches must first be assigned to several clusters before generating each cluster's nonlinear eigenspaces. therefore, the clustering method is described in detail in 4.1, and the method for estimating the missing high-frequency components of the target local patches is presented in 4.2."
"this section presents the formulation model of lr images in our method. in the common degradation model, an original hr image f is blurred and decimated, and the target lr image including the additive noise is obtained. then, this degradation model is represented as follows:"
"in this paper, we present an adaptive example-based sr method using kpca with a novel texture classification approach. the proposed method first performs the clustering of training hr patches and generates two nonlinear eigenspaces of hr patches and their corresponding low-frequency components belonging to each cluster by the kpca."
"in equation 26, since the matrix σ is singular, we cannot directly calculate its inverse matrix to estimate the missing high-frequency components h and obtain the original hr image. thus, the proposed method, respectively, maps j* andφ onto the nonlinear eigenspace of hr patches and that of their low-frequency components in cluster k. furthermore, the projection corresponding to the inverse matrix of σ is derived in these subspaces. we show its specific algorithm in the rest of this subsection and its overview is shown in figure 3 ."
"next, we discuss the sensitivity of the proposed method and the previously reported methods to the errors in the matrix b. specifically, we calculated the lr images using the haar and daubechies filters and reconstructed their hr images using the proposed and conventional methods as shown in figures 16, 17, 18 . from the obtained results, it is observed that not only the previously reported methods but also the proposed method is not so sensitive to the errors in the matrix b. in the proposed method, the inverse projection for estimating the missing high-frequency components is obtained without directly using the matrix b. the previously reported methods do not also utilize the matrix b, directly. then they tend not to suffer from the degradation due to the errors in the matrix b."
"furthermore, we learn a class-independent term network, which we call lognet. for this we use the largest training sets of each specific class. table 4 shows the selected training sets in bold. this network is still domain-specific in the sense that it is based on patent query logs, yet it stretches across class boundaries and is thus less specific."
"as shown in table 3, in particular for cv 1, for each class the number of unique synonyms extracted from the query logs increases with the size of the query log collection. the highest number of synonym relations (36,366 relations) could be extracted from the large class 422 for ''chemical apparatus and process disinfecting, preserving, or sterilizing''. furthermore, using the claws part of speech tagger for english terms [cit] we identified the synonyms based on the confidence value cv 1 w.r.t. part of speech and find out that more than half of the terms are nouns (69.61 %) followed by adjectives (15.53 %) and verbs (14.87 %)."
"in equation 32, if the rank of σ is larger than d k, the matrixû k u k becomes a non-singular matrix, and its"
"case 3 : lr images are captured based on the lowpass filter followed by the decimation procedure, but some aliasing effects occur. in this case, the problem becomes much more difficult than those of cases 1 and 2. furthermore, in our method, it becomes difficult to model this degradation process."
"in the input space are, respectively, the results projected onto the nonlinear eigenspace of cluster k. then, in order to calculate them, we must first obtain the projection result φ k j onto the nonlinear eigenspace of cluster k for each"
"for conceptual search (searching by meanings rather than literal strings) in the patent domain, the international patent classification (ipc), in particular their categories and short descriptions, or standard dictionaries, such as wordnet, or lexica, like wikipedia, are used for query refinement [cit] . especially, to provide synonyms for conceptual search, the related experiments for automatic query expansion commonly rely on the usage of the standard dictionaries and lexica [cit] . we assume, that these standard dictionaries will achieve only fair performance in the patent domain. in sect. 5 we will reassess this assumption."
"tors span the nonlinear eigenspace of cluster k. from the above reason, equation 13, therefore, cannot be calculated directly. thus, we introduce the computational scheme, kernel trick, into the calculation of equation 13 . the eigenvector matrix u k satisfies the following singular value decomposition:"
"in addition, we measure coverage of the respective networks by determining the number of out-of-vocabulary words, i.e. expansion terms that were used later in time that the network could not learn. this provides an indication on the comprehensiveness of the expansion term suggested. table 7 shows the coverage of the class-specific term networks and lognet."
"where t k is calculated as follows: figure 3 overview of the algorithm for estimating missing high-frequency components. the direct estimation of hr patches from their lr patches is infeasible since the matrix σ is singular and its inverse matrix cannot be obtained. thus, the proposed method projects those two patches onto the low-dimensional subspaces and enables the derivation of the projection corresponding to the inverse matrix of σ."
"to evaluate the lexical term networks based on recall and precision of the suggested expansion terms, we query the synonyms from the test sets. particularly, to calculate the recall scores, we compare the suggested terms from the term networks with the synonym or equivalent terms from the test sets, which were used by the patent examiners for searching. to compute precision we compare the synonyms used by the examiners in the test sets with all expansion terms suggested by the term networks."
"furthermore, we want to use the query log collections to learn further semantic relations that are needed for automatic query expansion in patent searching. in particular, we aim to learn term networks of keyword phrases, which we use for automatic query limitation in patent searching. to learn the keyword phrases we will use the proximity operators appearing in the text queries of the query logs."
"in this derivation, approximation 1 is used once. the criterionc k represents the squared error calculated between the low-frequency components l k reconstructed with the high-frequency components h k by cluster k's nonlinear eigenspace and the known original low-frequency components l."
"thus, from equations 14 and 40, the vector h k, which is the estimation result of h by cluster k, is calculated as follows:"
"the uspto published about 2.7 [cit] . the applications are classified into 473 classes each including hundreds or thousands of subclasses. hence, on average, about 6,000 application documents are available for each class. because patent searchers use the classification system to narrow the search, we selected fifteen classes for our experiments. at first we collected all application numbers of the published patent fig. 1 example of a uspto query log applications for the fifteen classes and generated a list of download links for each class based on the download url ''http://storage.googleapis.com/uspto-pair/applications/app_ num.zip'', where we replaced ''app_num'' in the url with the application numbers. secondly, we harvested the zip files via wget 4 a free software package for retrieving files from web servers, unzipped and filtered the files using the file name ending ''srnt.pdf'' of the query log files. following, we carried out ocr conversion using abcocr 5 a product to extract text from images on a windows 7 platform and converted the pdf files to txt files. subsequently, all terms were fed into the extraction process."
"for each class we split the query log collection in a training set and a test set for evaluation. the training set is further divided into several sub-sets to learn multiple term networks for each class to evaluate size and time dependency characteristics. specifically, having the query logs ordered by time of application of the patent, we use the first set of query logs of each class for training, with the test set being created from the chronologically last set of query logs in each class. for each class we generate up to five training sets for learning the class specific term networks (ts1 to ts5). the size of these sub-sets depends on the class size. for the five classes having less than 4,000 query logs (grouped as small) we learn 17 term networks based on training sets having between 500 and 2,500 query logs in increments of 500. for the medium grouped classes we learn 20 term networks having between 3,000 and 5,000 query logs. finally, for the large classes we generate 21 term networks based on training sets having between 6,000 and 10,000 log files. in total, for all classes we learn 59 term networks based on specific class and training set size. table 5 shows the generated training sets used for learning the class specific term networks."
"in conventional sr methods using the pca or kpca, but not including our previous work [cit], there have been two issues. first, it is assumed in these methods that the lr patches and their corresponding hr patches that are, respectively, projected onto linear or nonlinear eigenspaces are the same, these eigenspaces being obtained from training hr patches [cit] . however, these two are generally different, and there is a tendency for this assumption not to be satisfied. second, to select optimal training hr patches for target lr patches, distances between their corresponding lr patches are only utilized."
"we introduce the new criterion into the classification of the target local patch as shown in equations 42 and 45. equations 42 and 45 utilized in the proposed method represent the errors of the low-frequency components reconstructed with the high-frequency components by equation 40. in the proposed method, if both of the target low-frequency and high-frequency components are perfectly represented by the nonlinear eigenspaces of cluster k, the approximation relationship in equation 32 becomes the equal relationship. therefore, if we can ignore the approximation in equation 38, the original hr patches can be reconstructed perfectly. in such a case, the errors caused in the low-frequency and high-frequency components become zero. however, if we apply the proposed method to general images, the target low-frequency and high-frequency components cannot perfectly be represented by the nonlinear eigenspaces of one cluster, and the errors are caused in those two components. specifically, the caused errors are obtained as"
"the goal of this paper is to learn synonyms for the patent domain to enhance patent searchers in query generation, particularly in query expansion. particularly, patent searchers shall be allowed to formulate queries and to evaluate patent applications in the same way as patent examiners do it. to this end, we learn domain-specific term networks from the query logs created by patent examiners as part of the application validation procedure."
"in this subsection, clustering of training hr patches into k clusters is described. in the proposed method, we calculate a nonlinear eigenspace for each cluster and enable the modeling of the elements belonging to each cluster by its nonlinear eigenspace. then, based on these nonlinear eigenspaces, the proposed method can perform the clustering of training hr patches in this subsection and the high-frequency component estimation, which simultaneously realizes the classification of target patches for realizing the adaptive reconstruction, in the following subsection. this subsection focuses on the clustering of training hr patches based on the nonlinear eigenspaces."
"] is defined and its projection result onto the nonlinear eigenspace of cluster k is defined asφ k j in the feature space, the following equation is satisfied:"
"again, we calculate the recall/coverage and precision scores based on the suggested terms from the various term networks and the expansion terms used by the examiners. for the expansion of the query terms we use all lexical relations included in the patent domain specific term networks and in wordnet. we will not consider the meaning of the query terms, as our main focus is on the recall score in automatic query term suggestion. thus, wordnet should benefit from higher recall due to the large number of synonyms added without a potentially harmfuly limitation to specific word senses. we are aware, that the precision measures can be improved when considering the word senses. in spite of this rather defensive assumption, lognet achieves better recall measures than the standard dictionary wordnet across all classes, as shown in table 10 ."
"we show, that our approach to learn term networks from the patent domain, specifically directly from the query logs helps in meeting the requirements of this highly domain specific setting."
"as described above, the mse cannot reflect perceptual distortions, and its value becomes higher for images altered with some distortions such as mean luminance shift, contrast stretch, spatial shift, spatial scaling, and rotation, etc., yet negligible loss of subjective image quality. furthermore, blurring severely deteriorates the image quality, but its mse becomes lower than those of"
"in the proposed method, we assume that lr images are captured based on the low-pass filter followed by the decimation, and aliasing effects do not occur. furthermore, the decimation matrix is only an operator which subsamples pixel values. therefore, when the magnification factor is determined for target lr images, the matrices b and d can be also obtained in our method. specifically, the decimation matrix d can be easily defined when the magnification factor is determined. in addition, the blurring matrix b is also defined by the sinc function with the hamming window in such a way that target lr images do not suffer from aliasing effects. in this way, the matrices b and d can be defined, but in our method, these matrices are not directly utilized for the reconstruction. the details are shown in the following section."
"in addition, we show how to strike reasonable balance between increasingly higher recall/coverage and lower precision. we learned that the class-specific term networks provide better precision scores than the class-independent term network lognet. query terms are expanded in a certain context (patent class). on the other hand the class-independent term network lognet achieves best recall and coverage scores. hence, (1) expansion terms can be incrementally suggested initially from the class-specific term networks providing high precision scores, followed by more generic terms from the classindependent term network lognet achieving higher recall measures later-on, or (2) expansion terms can be suggest in the order of their support in the training set. after having initially suggested the most likely and highest-precision terms (i.e. using expansion terms that were encountered most frequentlys as synonyms in the training sets), additional terms that have a lower support and lower precision (i.e. encountered at least one time) can be suggested. furthermore, (3) related us classes allow the use of class-specific term networks across boundaries, providing valuable expansion opportunities, specifically for smaller classes, in example for classes where few query logs are available."
"the first column includes the searchable features of the invention selected from the source document, particularly from a patent document or an invention report. the second column provides the corresponding expansion terms. the terms are synonyms or equivalents, such as ''screen'' for ''display'', co-occur in the source document, for example ''signal'' with ''transmitter'', or limit a feature of the invention to a keyword phrase, such as ''control module'' for ''module''. particularly for finding synonyms to the searchable features of the invention, there is an increasing need to assist patent searchers as this process is very time-intensive and the probability to miss relevant expansion terms is high [cit] . yet, no sources providing synonyms, such as patent domain specific lexica or thesauri, are available."
"the permutation flowshop scheduling problem is a famous combinatorial optimization problem. this problem has become an attractive research area in manufacturing. it is not only a theoretical field of research but also taking the very essential effect in industrial process [cit] . to the best of our knowledge, in most cases, the majority of real world flowshop scheduling problems naturally involve the optimization of multiple conflicting objectives, such as, makespan, total flow time, machine idletime and so on. these objectives typically are conflicting, i.e. achieving the optimal value for one objective requires some compromise on one or more of other objectives. in this sense, it would be much better to design the permutation flow shop scheduling problem as a multi-objective problem rather than a single-objective problem, which is usually less general and computationally more expensive than the former."
"recently, a well-known algorithm, moea/d [cit] was proposed for multi-objective optimization problem. the main idea of this algorithm is to decompose the multi-objective optimization problem into a number of scalar optimization subproblems. the objective of each subproblem is a weighted aggregation of the original objective functions. each subproblem is optimized by using information, mainly from its neighborhood subproblems. the subproblems in one neighborhood are assumed to have similar fitness landscapes and their respective optimal solutions are most probable be close to each others."
"the goal of the initial phase is to identify good-quality solutions with respect to each individual's objective function of the multiobjective permutation flowshop scheduling problem. this set of solutions initializes the search process in order to ensure that the algorithm provides a good covering of the pareto front. in the beginning of the algorithm, we use problem-specific neh heuristic to initialize the current population as follows:"
"in global search, a heavy perturbation operation is executed on the non-dominated solutions in order to find individuals of high quality which cannot be found in the stage of further volume 4, 2016 local search. does this heavy strategy used in global search really have a positive effect on mmsa? in this subsection, we present a performance comparison between the heavy perturbation operation and other two perturbation operations including insert operation and swap operation. it is necessary to note that the same framework is used in this comparison. we will replace the heavy perturbation operation using the insert operation and swap operation. the comparative results are presented in table 7 and 8 in terms of two performance indicators, i.e. i h and c-metric. for the c-metric, as shown in table 7, the heavy perturbation operation has similar performance with the swap operation and exhibits superior performance compared with the insert operation. for the ta011-ta020, the insert operation can provide better solutions than heavy perturbation operation. compared with the swap operation, the swap operation can obtain the better solution for ta031-ta040 and ta061-ta070. for the i h, the insert perturbation operation can obtain better solutions than other operations for ta011-ta020. for the ta061-ta070 and ta071-ta080, the swap perturbation operation can perform better. for the rest instances, the heavy perturbation operation exhibits the highest efficiency than other operators."
"let a and b be two approximations to the pf of an mop, c (a, b) is defined as the percentage of the solutions in b that are dominated by at least one solution in a as follows:"
our global search method aims to perturb the nondominated solutions to improve the individual in the whole population. this part plays an important role of global optimization within our multiobjective memetic search algorithm. algorithm 2 described the perturbation based global search method.
"if the current solution is improved successfully in local search, the new solution will be stored in the current population. otherwise, one means that this solution traps into local optimum and needs to be perturbed. heavy perturbation is used in this design obtain some characteristics of the previous solution. simultaneously, the last main subalgorithm updates the current population using the new generated individual as shown in algorithm 5."
"the vectors including in the pareto optimal set are named the nondominated individuals. definition 4 (pareto front): for a given multi-objective optimization problem, pareto optimal solution set corresponding to the objective function value vector is known as the multi-objective optimization problems' pareto front."
"inspired from the ideas of moea/d [cit], we propose a multi-objective memetic search algorithm (mmsa) for multiobjective permutation flow shop scheduling problems in this paper. during the search, a problem-specific neh heuristic is first used to initialize the population and decomposed the multi-objective optimization problem into a number of scalar optimization subproblems. then, a global search embedded with a perturbation operation is applied to enhance the quality of the entire population. next, insert based local search is used to improve each subproblem in the population. if the subproblem cannot be improved, a further insert based local search is used to exploit the new individual. the experimental results show that the proposed mmsa provides better solutions than four state of the art algorithm."
"the molsd is a multiobjective algorithm based on pareto local search and decomposition. the momad is a simple yet efficient memetic algorithm for combinatorial multiobjective optimization problems, which also combines the decomposition and pareto local search. nnma is a memetic algorithm, called nnma, by integrating a general multiobjective evolutionary algorithm (nsga-ii) with a problem-specific heuristic (neh). ripg is a new algorithm based on iterated greedy technique for solving the multi-objective permutation flowshop scheduling problem. all the test instances are run for 10 independent times by all algorithms and the two performance metrics of them are evaluated to verify their performance. due to the limitation of space, only the average metric value of each problem scale is presented."
"1) in our algorithm, we use the global search method instead of pareto local search. as we know, in molsd, pareto local search is used to update the population randomly. in our algorithm, we use the one to one method to update the population to ensure each individual in the population that can be updated. by using this method, the whole population can be improved better compare with pareto local search. meanwhile, we don't make the new offspring to compare with each individual of the population in the global search. it also reduces more time for the entire process. 2) in our algorithm, we don't use the non-dominated sorting method on the current population to find out a certain number of elitist individuals. non-dominated sorting method is very time consuming. in our algorithm, if an individuals quality is not improved after a local search, then the algorithm will decide whether this individual should be undergo further exploration. 3) in our algorithm, we use a speed up method to reduce the computation time of objective evaluation. because our algorithm uses the time as the stopping criteria, we can generate more new individuals for the algorithm."
", where the f i,max and f i,min are the maximal and minimal value of objective i obtained over the 10 runs of all compared algorithms, respectively. the i h is shown in figure 1 . since the elements of the normalized objective vectors always lie in the interval [cit], the point (1.2, 1.2) was used as the reference point in our experiment."
"the performance of a moea for solving combination optimization problems is usually evaluated for two aspects. first, the obtained nondominated set should be as close to the true pareto front as possible. second, the solutions in the nondominated set should be distributed as diversely and uniformly as possible. in this paper, the following two metrics are used:"
"the rest of this paper is organized as follows: section ii describes multi-objective optimization problem. the detail of proposed method is given in the section iii. in section iv, we conduct the computational experiment. finally, we draw the conclusions in section v. volume 4, 2016"
"mmsa uses a decomposition based framework which is the similar with the moea/d. it decomposes the multiobjective permutation flow shop scheduling problem into a number of scalar subproblems by using the weight sum approach. each individual in the population represents a subproblem. the size of the population of individuals is equal to the number of the subproblems. when solving each subproblem, the global search and further local search are applied to enhance the quality of the entire population. this section will introduce the decomposition technique, global search, further local search, and other efficient strategies used in mmsa."
"to conclude, the results obtained in this paper are highly competitive. in addition, our approach is also quite efficient. the above discussion substantially establishes mmsa as a competitive alternative for multiobjective permutation flow shop scheduling problem."
"in this paper, we address the permutation flow shop scheduling problem with minimization of makespan and total flow time as the objective. a promising memetic search algorithm that combines the neh heuristic, global search and further local search strategy is proposed to solve the multiobjective permutation flow shop scheduling problem. firstly, a problem-specific neh heuristic is first used to initialize the current population. secondly, a global search embedded with a heavy perturbation operation is used to enhance the quality of the entire population. then, further local searches based on the insert operation are applied to enhance the quality of each solution. in order to show the effective of proposed algorithm, 90 public problem instances with different problem scales are used in this paper. the results show that our algorithm has better performance than the other state of the art algorithms, such as, nnma, molsd, ripg, and momad, for multiobjective permutation flow shop scheduling problems. the experimental results confirm the success of the proposed integration of global search and further local search in mmsa."
"the comparative results are presented in table 9 and 10 in terms of two performance indicators including i h and c-metric. in addition, we also plotted the net sets of non-dominated solutions on the two objective space for all 90 problem instances. the distributions of final sets obtained by all algorithms on some large scale problem instances of pfssp are plotted in figure 3 from which the quality difference between mmsa and four compared algorithms can be shown."
the further work is to study the theoretical aspects as well as the performance of the technique. another problem is to extend mmsa to solve other combinatorial problems such as multi-objective job shop scheduling.
"end while 23: end if 24: end if 25: end for neighbors to update the current population p l and the nondominated solution p e, which can extend the search space and enhance the converage rate of the whole population. if x ns is better than x ns, the x ns will be replaced by x ns . otherwise, the global search method will be decided whether it should be experienced multiple global search, called further exploration. what's more, this allows the perturbed solution to maintain some characteristics of the non-dominated solutions. the heavy perturbation operation can be described as follows:"
"like most memetic algorithms, the population size is an indispensable parameter. the task of selecting an appropriate population size for solving particular classes of problems has been known to be a challenge and often puzzling question in and of itself. by using a small population size, the algorithm can not explore the search space and be easy to lead to premature convergence. however, if the population size is too big, a large number of fitness evaluations will be required during each generation."
"the main purpose of using this heavy perturbation is to explore large search space using the appropriate perturbation strength. the perturbation strength has to be sufficient to lead the trajectory to a different attraction basin leading to a different local optimum. compared with the swap perturbation, this perturbation with larger interference strength can provide enough perturbation for the non-dominated solutions. moreover, it can enhance the diversity of the population and improve the convergence rate of the whole population."
"the number of weight vectors is determined by both parameter n and the number of objectives m. as i increases, the importance in the aggregated objective increases on f 1 and decreases on f 2 ."
"in this section, we will compare the mmsa with neh based problem-special heuristic and without neh based problemspecial heuristic. the average performance in terms of i h and c-metric are summarized in table 5 and 6. mmsa represents the mmsa with neh based problem-special heuristic. mmsa noneh represents the mmsa without neh based problem-special heuristic. the table 5 lists the c-metric of different methods. it can be seen from table 5 for the same computational time, the overall mean c-metric values yielded by the mmsa algorithm with neh based problemspecial heuristic are 44.2717%, which are much better than those (18.8326%) generated by the mmsa algorithm without neh based problem-special heuristic. the mmsa algorithm with neh based problem-special heuristic can provide better solutions than the mmsa algorithm without neh based problem-special heuristic except ta031-ta040 and ta051-ta060. for the ta031-ta040 and ta051-ta060, mmsa algorithm without neh based problem-special heuristic can obtain the better results. the table 6 lists the i h of different methods. the results reported in table 6 indicate that the mmsa algorithm with neh based problem-special heuristic generates significantly better values i h than the mmsa algorithm without neh based problem-special heuristic using the same computational time. the mmsa algorithm with neh based problem-special heuristic generates the better solutions than mmsa algorithm without neh based problem-special heuristic except ta011-ta020 and ta021-ta030. as the problem size increases, the superiority of the mmsa algorithm with neh based problem-special heuristic is better than the mmsa algorithm without neh based problem-special heuristic. from these results, it is concluded that the mmsa algorithm with neh based problem-special heuristic is more robust, effective and efficient than the mmsa algorithm without neh based problem-special heuristic for multiobjective permutation flow shop scheduling problems."
"to gain empirical insight into the influence of the population size on the search performance of our proposed algorithm, the population size has been varied from 100, 200, 300 and 1000 for multiobjective permutation flow shop scheduling problem. the results in terms of i h and c-metric are summarized in table 3 it can be observed that our proposed algorithm with population size 100 can provide better solutions than other population size settings. therefore, it can be observed that the population size 100 is the best choice."
"in the first experiment, we will discuss different aggregation functions for multiobjective permutation flow shop scheduling problem to investigate the effects of different aggregation functions. in this paper, we will compare the proposed method with the original weighted sum aggregation and tchebycheff aggregation, which are denoted by mmsa w and mmsa t, respectively. their average performance in terms of i h and c-metric are summarized in table 1 and table 2 . the table 1 summarized the final results obtained mmsa w and mmsa t in terms of the c-metric values. from the results, we can find that the proposed method can perform better than the original weighted sum aggregation function and tchebycheff aggregation function, except the ta011 to ta020. for example, the average c-metric value in mmsa and mmsa w are about 40.7398% and 15.4243% on the instance, respectively. for the ta011 to ta020, the original weighted sum aggregation function can provide better solutions than the proposed method. the average c-metric in mmsa and in mmsa w are about 0.25% and 0.5% on the instance, respectively. the average c-metric value in mmsa and mmsa t are about 39.2586% and 16.9304% on the instance, respectively. table 2 provides the hyper-volume values for the compared three methods. as can be seen in table 2, we can find that the proposed method can provide the best solutions 1.2559 compared with other two aggregation functions with the value 1.2065 and 1.2094, respectively. from these three methods, we can find that the original weighted sum aggregation function provides the worst average solutions. therefore, it implies that the proposed aggregation function is good in solving the multiobjective permutation flow shop scheduling problem. in this paper, we use the proposed aggregation function as the final aggregation function."
"the general scheme of multiobjective memetic approach for multiobjective flowshop scheduling problem is summarized in algorithm 1. basically, our multiobjective memtic search algorithm begins with an initial population of solutions which are firstly improved by the neh algorithm and then repeats the global search and local search for a number of times. at each generation, a perturbation operation integrated the non-dominate solution is used to generate the new offspring solution, which is further improved by the local search algorithm. in the following subsections, we will give more details on the components of our multiobjective memetic search algorithm. population improved: using further local search() to improve each individual in the population and obtain the p * l, then update the p l using updatep l () method."
"memetic algorithms are known to be an effective approach in solving a number of hard combination optimization problems. in fact, a memetic search algorithm uses a recombination operation to generate solutions located in promising regions in the search space and a local search optimization to search around the newly generated solutions. by offering the possibility for the search process to effectively explore the space of local optima, memetic algorithms have proved to be quite effective in solving a number of difficult combinatorial optimization problems [cit] . based on the feature of memetic search algorithm, we propose a multiobjective memetic search algorithm to solve multiobjective flowshop scheduling problem. inspired by the moea/d, the proposed algorithm uses the aggregation approaches with weight vectors to decompose multi-objective optimization problem to a number of single objective optimization. in our algorithm, the weighted sum approach is used to decompose the multiobjective flowshop scheduling problem into a number of single objective flowshop scheduling subproblems. for simplicity, an associated weight vector"
"the number of coherence blocks, k, that the jammer listens to is also a parameter that affects the ability to accurately estimate the frame timing. the longer the jammer listens, the better the estimate, but to be somewhat realistic this number cannot be too high, since the terminals are able to communicate freely, without being jammed, as long as the jammer is listening. in addition, the clock drift might also become a problem, if the jammer listens for too long."
"once t 0 is estimated, the jammer usest 0 to calculate the timing of all the frames according to equation (1), but with t 0 replaced with its estimate,t 0 . the estimate of frame f is denotedf, see figure 2 ."
"to assess how good the proposed jammer is, a performance metric is needed. the metric chosen is the upper bound on ergodic capacity, which assumes coding across multiple coherence intervals:"
"a few observations can be made from looking at the figure 4 . first, when the jammer is close to the primary users, the jammer performance is close to perfect. second, for large relative distances, the proposed jammer performs no better than the omnidirectional jammer. the distance between the jammer and terminal a obviously degrades the performance of the jammer due to the increased attenuation of the jammed signal, but this alone does not explain the rapid deterioration of the proposed jammer. the effect of increased attenuation of the jamming signal on its own can be seen on the performance of the genie jammer. the distance between terminal a and the jammer affects the received signal power at the jammer. the further away the jammer is, the lower the snr at the jammer, and at some point, the received signal will mostly consist of noise. this means that the frame timing will be difficult to estimate, and as a consequence, the channel estimates will be poor. if the jammer is very far away, the dominating eigenvector of the sample covariance matrix will be a random vector, associated with the noise w."
"in practice, the monitoring procedure is commonly realized by sampling and analyzing in real-time the voltage waveform. in order to capture some voltage disturbance of interest, meters are required to sample at a significantly high rate (e.g., 3 .6khz could be used for 60hz ac supply), and moreover, the meter readings are subject to the influence of noise [cit] . specially, the noise level could be comparable to that of the disturbance when the low-voltage distribution system is considered. consequently, there are two main steps involved in voltage quality monitoring: 1) detection and 2) classification [cit] . in the first step, the occurrence of a voltage waveform change needs to be detected based on the noisy measurements. in the second step, once a waveform change is declared, the system control can take further measures in real time, such as, activating meters to record informative measurements for off-line assessment and analyzing the distorted waveform. the second step is able to determine whether the detected deviation corresponds to normal operations, where no online treatment is required, or it is related to potential hazardous event. for the latter case, the disturbance type will be identified so that automatic protection or man-force involvement will be decided. specifically, this paper investigates the first step of the voltage quality monitoring, i.e., to detect the voltage disturbance from the observed noisy waveform as soon as possible after its occurrence."
"is the fraction of time the jammer beamforms to terminal a, when terminal b is transmitting, i.e., when the jamming does damage, as discussed in section iii-a. the rest of the time is wasted by beamforming to a while a is transmitting. this means that for a fraction 1 −"
"a is the part ofâ during which the jammer listens to the correct terminal; the smallerā is, the worse the channel estimate will be. similarly,b is the part ofb during which the jammer beamforms to terminal a when terminal b is transmitting. the smallerb is, more time will be spent on trying to jam terminal a, when it is actually transmitting."
"in this section, we formulate the online detection of voltage disturbance as a sequential change detection problem with unknown post-change parameters. then a generalized local likelihood ratio-based detector is derived."
"moreover, the ar model also requires a small number of parameters and its performance is robust to the model order p [cit] . in particular, the post-preprocessing signal after the disturbance occurs can be modelled as"
"where x t is disturbance signal modelled by an ar process with meanμ, w t is the driving noise of the ar process and y t is the meter observations. we further write (4) as"
"we still need to decide the change direction r. note that (15) is a quadratic function of the change direction r, which implies that after we replace s k k−n k +1 in (14) with its second-order"
"accurately estimating the frame timing is key to be able do disrupt the primary link as much as possible. a poorly estimated t 0 will have two major effects, both of which degrades the jammer's performance. the first is a \"degree of freedom\" loss, as the jammer loses jamming opportunity; the second is a degradation in the accuracy of the channel estimate. this will be explained in more detail in the end of section iii-a."
"we next consider the decentralized implementation of (34)-(36). in the decentralized setup, it is important to devise an efficient communication scheme between the distributed meters and the central meter, by which the local statistics are sent to the central meter less frequently and using small number of bits at each transmission. in the following subsections, we propose an efficient decentralized implementation based on the level-triggered communication scheme at the distributed meters and its associated decision rule at the central meter."
"we next incorporate more meters in the network and examine the performance of cooperative detection. focusing on the power sag event, the disturbance signals observed at meters 1-3 are illustrated in fig. 10 . we see that the disturbance signals induced by the same event occur at the same time to multiple buses but vary from each other in terms of the waveform. in fig. 11, the decision statistics of the single-meter detector (i.e., s-gllr), the centralized cooperative detector (i.e., c-gllr), the decentralized detec- tor based on level-triggered sampling (lts-gllr) and the enhanced lts-gllr (elts-gllr) are plotted. first, the cooperative detector exhibits steeper increase of the decision statistic compared to the single-meter detector, implying a more prompt reaction to disturbance signals. in the mean time, the global decision statistics of lts-based decentralized detectors are updated with a much lower frequency than the centralized one. in particular, the original lts-gllr detector clearly diverges from the centralized one due to overshoot accumulation over time, while the enhanced decentralized detector matches closely with the centralized detector."
"which is essentially the decentralized counterpart of (32). every time the global statistic is updated at the central meter, it is used to perform the gllr test given by (34)-(36). there are two decisions to make, i.e., triggering the alarm that a disturbance is detected or continuing to receive information bits from the meters. the procedure at the central meter is summarized as algorithm 2."
"for the proposed jammer to be able to disrupt communication more efficiently than an omnidirectional jammer, estimation of the frame timing is crucial. a poorly estimated frame timing deteriorates the jamming performance substantially. there is a threshold, most notably in the relative distance after which the frame timing estimate proposed is not accurate. if certain conditions are met, the jammer can successfully estimate the channel blindly, without any assumption of the structure of the channel. future work will include extending the jammer to a multi-user setup and also how to handle wideband systems and frequency hopping."
"this is a desirable method when the statistical property of post-change parameters is not known a priori. still, the maximization in (14) is difficult to solve since s k k−n k +1 given by (10) is not a convex function of the parameters. moreover, solving (14) at every time k leads to high computational complexity. in this paper, we apply a generalized local likelihood ratio (gllr) test to solve our problem, which is elaborated in the next subsection."
"on level-triggered sampling in this section, we consider the cooperative detection of voltage disturbances within a substation network at the distribution level. cooperative detection takes advantage of the fact that the substation buses are affected at the same time by a voltage disturbance, and allows us to achieve faster decision. particularly, it is implemented by deploying multiple meters at local buses across the substation network that communicate wirelessly with a central meter which is responsible for making the global decision (cf. fig. 1 ). consider l meters that are linked wirelessly with a central meter and perform the cooperative disturbance detection. the straightforward scheme is to make the distributed measurements fully available to the central meter by transmitting very finely (infinite-bit) quantized measurements at every sampling instant, i.e., the centralized setup. however, in practice, the wireless links between the distributed meters and the central meter are characterized by limited bandwidth. therefore, in designing a practical system, two constraints need to be considered, namely the rate constraint (i.e., the distributed meters should communicate with the central meter at a lower rate than the local sampling rate) and the quantization constraint (i.e., each meter should transmit a small number of bits every time it communicates with the central meter). in particular, considering the high sampling rate at distributed meters (e.g., for 60hz ac supply in north america, the sampling rate could be 3.6khz with 64 samples per cycle) and the large number of quantization bits in order to achieve an acceptable accuracy at the central meter, it is inefficient to inform the central meter of the local observations at every sampling instant. thus the decentralized detection, where the distributed meters communicate with the central meter in some low-rate fashion, becomes necessary. to that end, we propose a level-triggered sampling scheme which efficiently lowers the communication overhead in terms of both the communication frequency and the number of information bits at each transmission, while preserving the time resolution of the disturbance detection."
"whereĝ f is the jammer's estimate of the channel in frame f . how the channel estimates the channel is described in section iii-b. since q f is unknown, the estimateq f is used instead. for a given, hypothetical t 0, the estimate of q f is the sample covariance matrix"
"comparing (34)-(36) with (26)-(28), we find that the cooperative gllr detector differs from the single-meter gllr detector by summing distributeds k,( ) k−n k +1 instead of only using that at one meter. as such, the centralized gllr detector requires the distributed meters to quantize and transmit the local statistics k,( ) k−n k +1 to the central meter at every local sampling instant k."
"the general structure of the jamming scheme was mentioned in section ii-a and is explained in more detail here. to be able to jam the primary link, the jammer must:"
"we quantity the extent of the harm that can be caused as functions of some key system parameters: number of antennas at the jammer; coherence time (determined by mobility); slot timing (determined by design of the primary system, must be less than the coherence time); and the distance between the primary system and the jammer."
"we demonstrate that if massive mimo technology falls in the hands of adversaries, it can be used as a jamming device and cause significant harm to conventional wireless communication links that operate in tdd mode. the adversary must know the slot duration, but needs no other knowledge of the system to be jammed (such as channel state information, timing synchronization, pilot sequences or codebooks). also, no \"rank assumptions\" on the propagation channels are needed. a tdd system is fundamentally sensitive to jamming attacks, because during transmission mode, it reveals its \"location\" so that an adversary could estimate the transmit strategy that would cause maximal harm. some pmr systems-for example tetra, used for police and firefighter communications-operate in tdd mode and hence are exposed to the vulnerability that we demonstrate here."
"note that the statistical models before and after the disturbance, i.e., (3)-(5), correspond to a standard change detection formulation. the change detection (also termed as the quickest detection [cit] ) aims at detecting the change point as quickly as possible. it achieves high time-resolution by sequentially observing the measurements in the time domain, and the detection delay is minimized subject to a false alarm constraint. the optimal algorithm is obtained by finding the stopping time t 1 such that"
"1) estimate the frame timing, t 0 2) estimate the channel to terminal a, g a 3) beamform noise to terminal a to estimate the channel to terminal a, the jammer relies on reciprocity; the channel from a to the jammer and the channel from the jammer to a, will be the same within a coherence block. this means that once b starts transmitting, the jammer can beamform noise in the direction of the channel it learned in the previous frame. the jammer has to estimate the channel every coherence block, while the frame timing is only estimated once."
"the key idea of the local approach is to approximate the decision statistic by a linear expansion around the nominal (pre-change) parameter. thus we begin by expanding the conditional log-likelihood ratio in (10) up to the second order, yielding"
"to choose a proper b, first we introduce the detectability condition for the general sequential change detection [cit], which states that the two distributions before and after the occurrence of the disturbance are detectable with finite stopping time if and only if"
"remark: the accuracy of estimating t 0 will depend on the distance between terminal a and the jammer, as well as the output power of the primary link, ρ p, since these factors determine the snr of the received signal at the jammer. a higher output power will increase the achievable rate of the primary link, but also make the channel easier for the jammer to estimate."
"we first describe the level-triggered sampling strategy, which is essentially a single-bit quantization of the local statistic, where the transmission of the local statistic is only triggered once it hits a certain value, thus is observationadaptive. moreover, all meters communicate with the central meter asynchronously, which avoids the use of a global clock for synchronization."
"conventional jammers spread the jamming signal omnidirectionally in order to jam a particular target. this is highly inefficient, because most of the output power will not reach the intended target and is therefore wasted. since only a small portion of the output power is destructive to the target, very large output powers are generally needed. this huge output power and omnidirectional output makes the this type of jammer relatively easy to locate."
"where e t 0 means the expectation given the change point at t 0, and e ∞ the expectation without any change point. therefore, the objective function in (6) corresponds to the average detection delay and e ∞ (t ) corresponds to the false alarm period, i.e., the time a false alarm appears. intuitively, (6) aims to minimize the mean detection delay while controlling the period before a false alarm to be longer than γ."
"where t denotes the detection delay and γ is the false alarm period. therefore, given the false alarm period, the mean detection delay is minimized when"
"another noteworthy approach to combating the noise is to employ cooperative meters. in reality, voltage disturbances tend to occur to a group of connected electrical buses within a certain network at the same time (in fact, they propagate through the network in the speed of light), which brings about the opportunity of detecting the disturbance occurrence in a collaborative fashion. that is, by employing multiple meters at these connected electrical buses, one can draw on the diversity across meters to achieve better detection performance than using a single meter [cit] ."
"how the number of antennas at the jammer affects the performance can be seen in figure 5 . the beamforming ability improves for each antenna that is added to the jammer. ideally, the beamformed noise will scale linearly with the number of antennas, m, as the transmitted signal from the jammer experiences an array gain equal to m . as stated before, an incorrect estimate of t 0 can greatly degrade the jamming performance. there are two main factors that contributes to how well the frame timing can be estimated, the first being the snr observed at the jammer, and the second how many coherence blocks the jammer listens. figure 6 shows how the relative distance affects the accuracy of the t 0 estimate. here, the jammer has 100 antennas and the jammer listens to 50 coherence blocks. it can be seen that as long as the snr at the jammer is good enough, the jammer estimates the frame timing quite well for this scenario. however, just as one can suspect from looking at figure 4 there is a threshold when the snr is just too low to estimate the frame timing accurately. the distribution of the errors can be seen going from a very peaky normal distribution, to a uniform distribution from −τ c /4 to τ c /4. similar results can be seen when comparing the accuracy of the frame timing when listening to different number of coherence blocks. the distribution is close to normal when few blocks are used, but gets more peaky as the number of blocks increases. to be able to get this performance, the jammer needs to be close enough to the terminal to not have low snr."
"where x(t) ∼ cn (0, ρ p ) is the transmitted primary symbol at time t, g ∼ cn(0, i) is the channel, and w ∼ cn(0, i) is white (thermal) noise. to make notations easier, it is assumed that both of the primary terminals transmit with the same output power, ρ p . to estimate t 0, the jammer listens to k coherence blocks. define the set of all frames"
"we have developed a cooperative sequential change detection framework for online power quality monitoring. specifically, local meters observe the voltage signal independently and communicate wirelessly with a central meter to detect the disturbance. the goal is to achieve the quickest detection under a certain false alarm constraint. first, based on the ar modelling of the disturbance and the sequential change detection framework, we have proposed a sequential gllr test that does not require the knowledge of the model parameters. unlike the conventional rms or stft method, the proposed technique exploits the statistical distributions of the observed waveform before and after the occurrence of disturbance, thus provides superior performance, especially in the noisy environment. we have also developed the decentralized version of the gllr detector, which is specifically tailored toward the low-bandwidth requirement imposed by the wireless transmissions between the distributed meters and the central meter. this is achieved by a novel level-triggered sampling scheme that features single-bit information transmission. finally we have provided extensive simulation results to demonstrate the superior performance of the proposed centralized and decentralized cooperative detectors over the existing methods."
"given a target false alarm period γ, the threshold is given by h ≈ ln(γ ) [cit] . note that at each time k, the test statistic g k is computed and compared with the threshold h. t is the first time that g k exceeds h and when the disturbance is declared to occur. if both θ 0 and θ 1 are exactly known, (9) is equivalent to the cusum test and the decision statistic g k can be recursively computed as"
"the existing disturbance detection methods can be roughly categorized into non-model-based and model-based approaches [2, ch. 7], [cit] . in particular, the non-model-based approach directly examines the instantaneous changes from the observed waveform. for example, one method is to use a high-pass filter to capture the high-frequency component induced by the abrupt transition when the disturbance occurs. intuitively, this method becomes ineffective if the waveform changes smoothly. moreover, the high-frequency component at the transition point can be buried by the noise. the most widely used method thus far is by monitoring the root mean squared (rms) sequence, which is computed over a sliding window of length w (usually one cycle of the nominal waveform) as follows:"
"through the level-triggered sampling scheme, we efficiently recover the decision statistic at the central meter by collecting local statistics from meters. specifically, compared to the centralized setup where observations are transmitted at every sampling instant with multiple quantization bits, the leveltriggered sampling features lower communication frequency (which can be controlled by the parameters and ) and one-bit representation of each sample."
"is the transmitted statistic up to time k by meter given by (42). a main problem with the leveltriggered sampling scheme in (38) is that the overshoot errors accumulate over time. in general, using (37)-(38), we can write the actual statistic at the th meter as"
"traditionally, jamming of wireless links has been considered a \"military\" problem, even though the awareness of the threat to civilian systems seems to be increasing. there may be a parallel with cyber-security on the internet. consider d-dos attacks. [cit] 's-at least, the internet and its protocols and servers were clearly not designed with this threat in mind-but this treat is now widespread and taken very seriously at all levels. it is not implausible that wireless jamming can undergo the same evolution, because: the harm that can be caused is substantial; society is relying more and more on wireless communications solutions; inexpensive, military-grade technology is proliferating; and the ability of an adversary to hide or to operate covertly is increased, for example due to the availability of cheap unmanned aerial vehicles (uavs)."
"the reminder of this paper is organized as follows. in section ii, we formulate the disturbance detection as a sequential change detection problem based on the ar model and develop a generalized local likelihood ratio (gllr) test. in section iii, we further study the multi-meter cooperative detection and develop the decentralized gllr test based on the level-triggered sampling. simulation results are provided in section iv and finally, section v concludes the paper."
"the relative distance, d r, is the ratio between the distance between the terminals in the primary link and the distance from the jammer to terminal a. this parameter, similarly to ρ p, affects the snr at the jammer. the further away the jammer is, the worse performance of the jammer."
"directional jammers, such as the one presented in this paper, will need much less output power to be destructive to the primary link, due to the large number of antennas, m . the m antennas will make the jammer able to beamform \"noise\" to the target which provides an array gain over the omnidirectional jammer. owing to the decrease in output power and the fact that most of the signal power is directed towards the target, a directional jammer is much more difficult to locate."
"applying the same local approximation as in the last section to sup θ ( ) 1 s k j ( ), then we further evaluate (31) as follows:"
"there are several kinds of musical features can be extracted from audio files, such as timbre, tonality, rhythm and chroma [cit] . in this paper, the audio identification algorithm consists of beat tracking and chroma features [cit] . beat tracking is important as a step in understanding how people process temporal information in the editing of audio data. chroma feature is a powerful and interesting representation for audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones of the musical octave. a beat tracker is used to generate a beat-synchronous representation with one feature vector per beat and the representation of each beat is a normalized chroma vector which sums up spectral energy into 12 bins. to identify the downloaded piece, two beat tracks obtained from original audio file and downloaded piece are matched by using cross-correlation."
"the artefacts were not removed from the eeg signal. we analyzed only artefact free segments of the eeg trace, as we did not want to distort the signal by \"fusing\" selected parts (after artefact removal). from artifact-free traces we extracted three epochs for further analysis; every epoch was 5s (5000 samples) long. altogether there were three epochs for every person recorded, resulting in 2337 epochs for further analysis."
"in the experiments, word vectors that are pretrained with raw text are used to initialize the entity vectors. this pre-training leads to a big improvement compared with random initialization, as will be shown in section 4."
"for this reason, only entity vectors are learned (the global relation vector can be absorbed into the entity vectors). this is similar to the unstructured model in bordes's early work, except that distance between vectors is measured by inner product in our model, while bordes's work used euclidean distance. we make this choice for two reasons: firstly, to make the graph knowledge learning consistent with the joint text learning so that their results are comparable, and more importantly they can be combined into a joint text and graph learning as will be presented in the next section; secondly, word vectors trained with raw text can be used to pre-train (initialize) the entity vectors, which has been demonstrated to be highly effective, as will be seen in section 4."
"in the last test, the responses of the pll algorithms have been tested under variable frequency grid condition. as can be seen in figure 12, the frequency of the grid voltages has been increased from 50 hz to 55 hz at the time of 0.1 s, the grid frequency has been decreased from 55 hz to 45 hz at the time of 0.2 s and the grid frequency has been increased from 45 hz to 50 hz at the time of 0.3 s. it is assumed that the grid voltages are balanced and there are no harmonics or other disturbing effects. figure 13 and figure 14 show the responses of the srf-pll and ddsrf-pll algorithms, respectively. as can be understood from the figures, the performances of srf-pll and ddsrf-pll against frequency changes are almost the same. in the case of the variable frequency grid, the maximum overshoot value of the srf-pll is 8 hz while it is 13 hz in the ddsrf-pll. on the other hand, the srf-pll and ddsrf-pll algorithms have an average settling time of 65 ms. there is no steady-state error in each pll algorithms. thus, the phase angle of the grid is determined without error in both pll algorithms."
"as a result of the findings, the srf-pll stands out with its simplicity, easy applicability and frequency response. however, under non-ideal grid conditions such as unbalanced and/or harmonic grid, the stability of the system becomes worse. on the other hand, since the ddsrf-pll algorithm can accurately obtain positive and negative sequence components and the filtering capacity is high, the response and accuracy in such adverse conditions are at a desired level. the results of comparative simulations clearly demonstrate the importance of the pll algorithm which should be preferred according to the grid distortions."
"with the development of deep learning and the establishment of large knowledge bases, knowledge embedding has gained much interest in natural language processing. in general, knowledge can be represented by some entities that represent semantic concepts, plus the relations among them. knowledge embedding involves representing entities of knowledge bases in a low-dimensional continuous space so that the relations among them can be well represented. the embedding can be conducted with different objectives with different tasks in concern. this paper focuses on the semantic learning task which intends to optimize the semantic relevance among entities by knowledge embedding, e.g., inferring appropriate knowledge (entity) vectors."
logistic regression [cit] estimates the class conditional probability using a linear combination of features and logistic regression function. the model coefficients are estimated using the logitboost algorithm [cit] .
"using only the first principal component, it is possible to achieve a classification accuracy of up to 95.12%, using the naïve bayes method ( table 2 ). the classification accuracy generally increases with the number of principal components used as classifiers' inputs (the average accuracy of all classifiers is 88.15% with 1 and 93.73% with 10 principal components used)."
"for the labeled text learning, the entity vectors are derived as the mean vectors of the words involved in the text descriptions. this approach in fact involves both raw text learning (for word vectors) and labeled text learning (vector average), however the main knowledge source is the entity labels of the descriptions. the results with labeled text learning are reported in the row denoted by 'lt-mean' in table 2 ."
"the synchronous reference frame-phase locked loop (srf-pll) algorithm is used extensively in three-phase systems. in figure 2, the block structure of srf-pll is shown."
every recording lasted 3 minutes. subjects were instructed to reduce any movement. we had to discard records of two patients from further analysis due to low voltage eeg in one male participant's case and epileptic seizure very close in time from the recording of one female participant. we used records from 21 patients diagnosed with depression and 20 age-matched healthy controls for this study. artifacts were carefully inspected by two independent experts.
"in this study, we demonstrated that higuchi's fractal dimension and sample entropy are capable of distinguishing between participants diagnosed with depression and healthy controls' eeg. if a feature extraction method results in good classification accuracy regardless of applied machine learning technique, this provides the evidence that the feature extraction method is useful. these results encourage further investigation with larger sample sizes towards potential diagnostic application in clinical medicine and psychiatry."
"different from the mean-vector approach which first trains word vectors and then derives entity vectors, the joint text learning learns word vectors and entity vectors together. two configurations are tested: in jt-rand, the entity vectors are randomly initialized, while in jt-prt, the entity vectors are initialized (pre-trained) by corresponding word vectors. note that all the multi-word entities can not be pre-trained as the phrase vectors are not trained, however this does not much impact the resulting performance since the test sets do not involve multi-word entities."
"from the results presented in figure 1, very different patterns on the two test sets are observed: for wnet-n, the best β is close to 0.0, which means that involving graph knowledge in the learning simply reduce the performance. however for yago-a, the best β is around 1.0, which means that to achieve the best performance, the contribution from text and graph resources should be balanced. this discrepancy on the optimal β can be explained in the same way as in the previous experiment, that yago-a is a specific domain test so that the entities can not be well trained by either text data or graph knowledge, so the two resources need to be utilized together, which is where the joint training contributes. in practical applications, people should increase the β when domain becomes narrow."
"as mentioned in section 1, knowledge bases such as wordnet and yago contain plenty of entities and their relations, leading to complex knowledge graphs. since these entities and relations are annotated by people, graph knowledge is highly reliable and can be used to embed entities. note that different knowledge bases contain different types of relations. for wordnet, nearly half of the relations are the hypernym-hyponym (is-a) relation, and for freebase, the relation types are much more complicated. although it is possible to learn different relations, this study does not consider it since our focus is semantic relatedness instead of relation prediction. more discussions about relation type learning will be given in section 5."
"another observation is that the dimension of the entity vectors indeed impacts the performance. a larger dimension tends to perform better, at the cost of higher complexity in model training. in the following experiments, the dimension will be set to 100."
"in present clinical practice, depression is diagnosed using clinical interviews and structured and semi-structured symptom severity scales (beck's depression scale, american psychiatric association -dsm 5, icd-10), all of which require accurate self-report from the patient. to the best of our knowledge, clinical professionals are not using any kind of quantification of eeg in the diagnostic process. multiple factors [cit] make detection and diagnosis of depression often difficult. the use of biomarkers could facilitate diagnosis and potentially prevent future episodes, all of which garnered significant attention in the past decade."
"mpeg audio compression is one of many methods to compress audio in digital form trying to consume as little space as possible but keep audio quality as good as possible. it is a lossy compression, which means we loose some audio information. but this lost can hardly be perceived because the compression method tries to control it. to decode these compressed data, we must need several parameters to represent compression strength, compression method, compressed data size and location of compressed data. these parameters are allocated in header and side information."
the simple approach to learning with labeled text is to average the vectors of words involved in the description of the current entity e. this is formulated by:
"where γ grh is the boundary margin which is empirically set to 1.0 in this study, and (e l i, e r i ) is the corrupted version of (e l i, e r i ) (only one entity corrupted). again, the sgd algorithm is used to optimize l grh with respect to the entity vectors."
"it is also clear that the two direct inference approaches outperform the graph learning approach when no pre-training applied, however with pretraining, the graph learning approach is much more superior, particularly for the wnet-n task. this suggests that learning with broad domain knowledge base is pretty hard, and extra information from raw text is essentially important. table 4 : experimental results with various graphbased entity relateness inference methods."
"in the srf-pll method, vd and vq voltages appear as dc components. in ideal grid conditions where the grid voltages are balanced and there are no harmonics or distortions, the estimated phase angle (θ * ) equals to the phase angle (θ) of the grid voltage. the estimated phase angle is the same as the phase angle of the voltage va of the grid. as can be seen from equation (3) and equation (4), vq equals zero while vd equals the peak value of the grid voltage. as can be understood from the equations, vq contains information about the phase angle error of the grid. on the other hand, vd gives the amplitude information of the grid voltage in steady-state. besides, the srf-pll offers the estimated frequency (f) information [cit] ."
"this study is an extension to the existing joint learning methods. particularly, we also learn knowledge embedding from descriptions(labeled text). this is contrary to most of the existing researches which learn the knowledge only from context and knowledge graph. additionally, a new joint learning framework is presented in this work, which integrates text and graph learning as a unified learning processing. moreover, the contributions of different resources in different situations will be investigated."
"a variety of classification methods have been used in domains similar to ours: support vector machines (svm), linear regression (lr), linear discriminant analysis (lda), k-nearest neighbors (knn), enhanced probabilistic neural networks [cit] . the choice of a specific classification method is frequently a matter of bias of researchers [cit] . moreover, in absence of standardized data repositories, that exist in other domains [cit] and a strict statistical test for comparison of, frequently non-linear and non-parametric, classifiers [cit] ."
"the experiments are conducted with three information resources: wikipedia as the raw and labeled text, wordnet and yago as the graph knowledge. the entity relatedness task is selected to evaluate the performance of the learning methods. two scenarios have been conducted, one is based on wordnet and the other is based on yago. the test on wordnet is a general domain task while the test on yago is a specific domain task. the experimental results show that the joint text learning offers consistent improvement compared to learning with raw text only. when involving graph knowledge, the performance on the general domain task does not show apparent improvement, however on the specific domain task, a significant performance improvement has been observed. these results confirm the importance of learning with heterogeneous information resources."
"the rest of the paper is organized as follows: section 2 briefly describes the related works, section 3 presents the joint learning approach. the experiments are presented in section 4, and some discussions are in section 5. section 6 concludes the paper."
"where e is the set of entities to learn, and γ txt is the boundary margin which has been empirically set to 0.5 in this study. (w i, w i ) is a pair of words for which w i is sampled from the description of entity e, and w i is sampled from a proposal distribution. w i is constrained to be different each time of sampling for a particular entity. k is the number of samples for each entity. in our study, k is set to 30 unless the description is shorter than 30 words. the proposal distribution for sampling w i is set to be the unigram distribution of all the words involved in the descriptions of all the entities. we call this model the joint text learning model. the stochastic gradient descendent (sgd) algorithm is employed to optimize l txt with respect to the entity and word vectors. [cit] . in pv-dbow, paragraphs are represented by paragraph vectors (pv) and are trained together with word vectors. the pvs correspond to the entity vectors in our model. a main difference between our model and the pv-dbow model is that the cost function of our model is based on the hinge loss, while pv-dbow uses the softmax function. this new cost function provides almost the same performance but offers a lower computation complexity because we don't need to count the sum of distances of the whole dictionary (which is what softmax does)."
"as seen from the equations, coupled 2 nd harmonic components (2ω) are added to positive and negative sequence components of vd and vq voltages. the positive and negative sequence components of the grid voltage are obtained in a decoupled manner by eliminating these coupled components."
"there is a kind of a consensus between researchers that relying on one nonlinear measure might be misleading [cit], and there are also extremes in published studies with ten or more nonlinear measures applied [cit], showing that every one of them is providing another kind of information about the signal under study. in our previous work, while testing the effectiveness of different combinations of nonlinear measures we found that higuchi's fractal dimension (hfd) and sample entropy (sampen) are particularly well matched in a methodological sense (in press) showing different sensitivity for frequency content of the signal."
"for raw text learning, various unsupervised learning algorithms have been proposed to learn word representations from large-scale raw text [cit] . these methods hypothesize that statistics of word co-occurrences in contexts involve rich semantic and syntactic information and can be utilized to embed words and/or phrases."
"for the test on yago-a, we propose a new test set animal-143, which contains 143 pairs of common animal names and 92 different animals including mammals, birds, insects and marine animals. all the names are entities of yago-a. the relatedness score of each pair has been evaluated by 9 persons, and the average is used as the human judgement. the range of score is from 0 to 3. for instance, the score between antelope and swan should probably be 0, and the score between cattle and bison can be 3. table 1 summarizes the data sets used in the experiments."
"among others, a recent study offered a new insight into the possible mechanisms behind major depressive disorder (mdd) [cit] . de kwaasterniet and colleagues confirmed abnormal functional connectivity in the fronto-lymbic system by utilizing digital tractography imaging (dti) and fmri. a compromised second part of uncinate fasciculus in mdd seems to be correlated with increased functional connectivity but also with the severity of the disease. [cit] showed by applying graph theory to eeg that 'functional topological architecture of the resting-state brain network is disrupted in bipolar disorder'. that study also revealed impaired neural synchronization at resting state as well as a disruption of functional connectivity. both studies confirmed that the disturbance of white matter tracts is associated with abnormal functioning of fronto-lymbic system -a key indicator of depression."
"nowadays, the pll algorithm is also used as a new area to ensure synchronization in grid interactive inverters. in recent years, there has been an increase in use of the pll in this field. the pll must provide fast and precise synchronization between the inverter and the grid. furthermore, it must have a good response to harmonics, imbalances, phase jump, frequency changes, and various disturbing effects in grid voltages. therefore, pll algorithm plays a major role for grid interactive inverters [cit] ."
"as mentioned, this study chooses the semantic relatedness task to evaluate the performance of learned entity vectors. this task computes the relevance (distance) of two entities and then compares the resulting relevance score with the human-specified score. the spearman coefficient is a widely-adopted metric to evaluate correlation between two variables and is used in this study to measure the consistence of the derived relevance with the human specification. to test the performance on wnet-n, a subset of wordsimilarity-353 4 is used. the wordsimilarity-353 collection is a well-known test set for semantic relatedness tasks, which contains 353 word pairs and the relatedness scores of all the pairs are manually annotated. after filtering out the words that are absent from the entities of wnet-n, the resulted 301 pairs are used as the test set, named as sim-301 in the following sections."
"the srf-pll operates as a feedback servo system to instantaneously detect the phase angle (θ) of the grid voltage. in this system, the three-phase grid voltages are firstly measured. then, the measured three-phase grid voltages are transformed to the stationary frame variables (vα, vβ) by clarke rotation matrix given in equation (1). after, the vα and vβ voltages are converted to the rotating (synchronous) frame variables (vd, vq) by park rotation matrix in equation (2). the estimated phase angle (θ * ) of the grid voltage is fed back to operate the abc to dq block so that the park rotation can be performed. that block also works like the pd block [cit] ."
"note that both wnet-n and yago-a maintain a connected graph structure which means that for any two entities in the graph, there is at least one path that connects them. this enables the simple connection-based relevance inference which derives relateness of two entities as the connection strength between them in the knowledge graph. section 5 will compare this simple approach with our proposal."
"the aim of their study was to improve the accuracy of classifiers in the combination with different nonlinear measures. in addition, it has been found that support vector machines (svm) provided the best classification results compared to other methods such as decision tree (dt), k-nearest neighbor (knn) and naïve bayes (nb) [cit] . recently published detailed mathematical description of interconnection of hfd and fourier analysis (classical spectral analysis applied usually in eeg) components strongly suggest that the use of higuchi's' fractal dimension and fast fourier transform (fft) is redundant because fractal dimensions are weighting functions of fourier's amplitudes [cit] . complex signals are information-rich. they are also fractal in its nature for showing multiscalling and self-similarity."
"learning entity vectors with raw text can be simply implemented by treating entities as words (or phrases) and learning them together with other words. there are a number of approaches to learning word vectors [cit] . in this study, the skip-gram model implemented in the word2vec tool 1 is adopted."
"the rest of this paper is organized as follows. in section ii, the working principle of bittorrent technology is presented. in section iii, we give an overview of the structure of mp3 file and point out important information used to decode the piece of audio. in section iv, we present the audio identification algorithm. finally, we discuss the evaluation of simulation results in section v."
"decision trees [cit] recursively partition feature partition space in regions corresponding to classes by choosing a feature that provides the highest information gain. the partition stops when the minimal number of 2 samples per node of a decision tree is reached. in the pruning phase, based on the estimation of the classification error (using a confidence level here set to 0.25) the complexity of the model may be reduced and its generalization capacity thus improved [cit] ."
our results indicate that the use of sampen features may result in classification results comparable to or better than hfd. five out of seven examined classifiers provided better accuracy while six provided a higher area under roc curve when applied on sampen features.
various approaches have been proposed to utilize heterogeneous resources. [cit] demonstrated that text data can help discovering new relation in graph completing task. used text data for the same purpose while they used word vectors that may leverage text resources more effectively.
"the determination of the minimal number of pcs depends on the desired classification accuracy and is generally problem dependent [cit] . in this specific case, if a minimal auc is set to 0.85 (corresponding to diagnostic tests considered good) (the source that was used to generate http://gim.unmc.edu/dxtests/default.htm), three pcs are sufficient, according to table 2 ."
"in the field of relevance learning with graph knowledge, early studies focused on measuring word similarity based on the graph theory, for instance, [cit] . [cit] ."
"note that we examined classification methods with a range of underlying paradigms and complexity; the methods belong to statistics and supervised machine learning. even the simplest methods, such as logistic regression (widely accepted in the medical community although not as a classification method in the strict sense) provided excellent classification accuracy. in fact, high classification accuracy of methods such as svm with linear kernel indicate that, after a nonlinear transformation, the data may become close to linearly separable; we are however aware that this may be specific for a particular dataset and should be tested on further data. consistent with known properties of the naïve bayes classifier [cit], it had good accuracy (e.g., 92.68%, auc of 0.983 when applied on sampen and hfd features combined) albeit the underlying assumption about feature independence is not satisfied."
"difficult to find out where to start decoding. in this case, we have to analyze another piece. thus, header and side information play an important role in process of file analysis. after the compressed data is decoded, then we extract the feature from the audio data."
"many researches have been conducted to learn semantic relevance from raw text data, labeled text data or graph knowledge bases. most of the studies learn from single information resource."
"the major finding of this study is that the extraction of non-linear features linked to the complexity of eeg signals can lead to high and potentially useful separation between signals taken from control subjects and patients diagnosed with depression. specifically, we demonstrated that higuchi's fractal dimension (hfd) and sample entropy (sampen) could be used as suitable features for various machine learning classification techniques. in other words,"
"an mp3 file is built up from a number of small parts called frames. these frames are independent items and each frame has its own header and side information. there is no file header, thus, we can cut any part of mp3 file and play it correctly. but this is not always correct especially for mpeg 1 layer iii files, because frames are sometimes dependent on each other. the parameters of header and side information are not always the same. variable bitrate mpeg files may use bitrate switching which means the bitrate changes according to the content of each frame to make better compression while keeping high quality of audio. this way lower bitrates can be used in frames and it will not reduce audio quality. the header is constituted by the very first 4 bytes in a frame. the first 12 bits of a header are always set and they are called frame sync. therefore, we can search through the file for the first occurrence of frame sync. after the frame sync is found, we verify next parameters and check if the values are correct. then we read the whole header if there is no invalid value. fig. 2 shows the structure of the header."
"using only the first principal component, it is possible to achieve a classification accuracy of up to 95.12% (table 2 ). the best performance was achieved using the naïve bayes method (in this case, when only one feature is utilized, the assumption of feature independence is automatically satisfied). the classification accuracy generally increases with the number of principal components used as classifiers' inputs (the average accuracy of all classifiers is 88.15% with 1 and 93.73% with 10 principal components used)."
"bittorrent is a p2p file distribution protocol which is designed to allow efficient distribution of large files, such as video and audio files. to share a file, bittorrent splits the file into several fixed size pieces. the size of a piece is usually with byte sizes of a power of 2 and typically between 32 kb and 16 mb. bittorrent users called peers connect to each other directly to send and receive these pieces. however, there is a central server called tracker coordinates the action of the peers [cit] . it acts as an information exchange center and sends a randomly chosen subset of peers who have pieces of the file. the subset of peers is called swarm list. firstly, there should be at least one file owner with the whole pieces of a file generates a torrent file which contains metadata about the file to be shared and the address of the tracker, and this user is called initial seeder. the torrent file also contains sha1 hash values of each piece to verify integrity of the pieces. after the torrent file is created, it is registered into a tracker and the file owner places it on websites to make the torrent file available to other bittorrent users. after downloading the torrent file from the bittorrent website, the downloader opens it with a bittorrent client program which partial identification analysis for mp3 music xun jin and jong weon kim connects to the tracker and manages the transfer of the pieces. the working protocol of a bittorrent network is shown in fig. 1 ."
"pcs. this can be in part explained by cover's theorem [cit] that data in higher dimensional spaces tend to be more linearly separable. the random forest method benefited from a larger number of utilized principal components since the method is based on randomly choosing one from a set of available features to split a decision tree node (when the number of pcs used is small, the set of available features is small)."
"according to the information resource that is used to learn with, knowledge embedding can be classified into three categories: raw text learning, labeled text learning and graph knowledge learning. in the raw text learning, the entities are treated as words or phrases, and the local word context information in the raw text is used to drive the embedding. various approaches of word/phrase embedding belong to this category [cit] . in the labeled text learning, the embedding is based on the description text associated to each entity. a simple approach belonging to this category derives the vector of an entity by averaging the word vectors of the description associated to the entity. essentially, the knowledge used in this learning is the cooccurrence statistics of the words in the descriptions. finally, in the graph knowledge learning, the relations among entities labeled by people are used to direct the embedding. representative approaches of this category include transe and ntn [cit] ."
"in figure 1, block structure of the pll is shown. this structure automatically synchronizes the phase of the output signal to the phase of the input signal as it is a feedback system [cit] . the pll structure consists of the phase detection (pd), the loop filter (lf) and the voltage controlled oscillator (vco) blocks. the pd block determines the phase difference between input signal (vi) and output signal. in addition, it produces a proper error signal [cit] . this error signal is transferred to the lf block. the lf demonstrates the low-pass-filter (lpf) characteristic to provide stability of the system. moreover, it typically comprises of the first-order lpf or a proportional and integral (pi) controller. in other words, the lf block specifies the dynamics of the system [cit] . the signal at the output of the lf block generates the output signal in the same phase as the input signal by driving the vco. thereby, the output signal follows the input signal [cit] ."
"under unbalanced grid condition, positive sequence and negative sequence components of grid voltage occur. since these components cannot be controlled independently in the srf-pll method, errors occur in synchronization between the inverter and the grid. the basis of the ddsrf-pll algorithm is based on the conversion and independent control of the positive sequence and negative sequence components of the grid voltage. this algorithm highly removes the errors in determining the phase angle of the grid in the conventional srf-pll [cit] . furthermore, the ddsrf-pll can be used in wind energy systems due to its very good response to grid frequency changes [cit] ."
"our results suggest that classifiers that could be implemented on a simple and inexpensive hardware and embedded to existing eeg devices. this itself may ultimately lead to potential everyday clinical usage of our methodology for providing computer-aided diagnostics of depression. the technology could be of interest, e.g., when burnout or extreme amount of stress, using the current diagnostic methods, are mistaken as symptoms of depression. other researchers are testing whether similar methodology can help identify a patient as a good responder to particular treatment be it medication or transcranial direct electrical stimulation [cit] ."
"in line with previous findings [cit] about measures used for characterization of eeg, in our study hfd detected increased complexity of eeg recorded from patients diagnosed with depression in comparison to healthy controls. [cit] who demonstrated that non-linear features, such as hfd, correlation dimension, and lyapunov coefficient are more discriminative than linear features. the main difference with our work is that we analyzed broad band signal, and other divided the signal on standard spectral bands. [cit] reported classification accuracy of 91.3% when using two differently calculated fractal dimension algorithms (higuchi and katz) as features and enhanced probabilistic neural networks for classification. our results are in qualitative agreement with this finding. in addition to confirming previous results, we not only showed that sampen can also effectively discriminates these two categories of eeg signals, but also it could have performance superior to hfd, see table 1 ."
"phase lock loop (pll) [cit] s, the pll could not find a wide range of applications due to the difficulty of its implementation. the pll has begun to be largely used in modern communication systems by means of the rapid development of integrated-circuit (ic) [cit] s. later, it was used in different industrial fields such as speed control of electric motors and static power converters [cit] ."
"to the best of our knowledge, there are no previous attempts to utilize a combination of hfd and sampen features. in our experiment, the use of an augmented feature set consisting of hfd features and sampen features did not lead to further improvement of accuracy for the majority of attempted classifiers ( table 1 ). note that hfd features are less correlated to sampen features, then hfd features or sampen features are correlated among themselves. namely, the maximal correlation between one hfd and a sampen feature is 0.79. in contrast, the maximal correlation between two sampen features or between two hfd features is larger than 0.98. the combination of two relatively uncorrelated features, such as hfd features and sampen features, provides an opportunity for training of more expressive classification models that could result in better classification accuracy. this would fully exploit orthogonality of features when using more complex classification models that would generalize better if trained on larger datasets [cit] ). however, the prerequisite for achieving such increased accuracy is large enough size of the data set, which may be produced in follow-up studies. if the available data set is not sufficiently large, some machine learning algorithms suffer from potential of overfitting-when a learning algorithm, in attempts to minimize error on a training set, results in a model that has poor generalization abilities [cit] . our results suggest this may be a case with random forests and multilayer perceptron, where the accuracy achieved with the combined (hfd+sampen) feature set is smaller than using hfd or sampen separately (table 2 ). in contrast, machine learning models that have small or controlled complexity (e.g., expressed through vc dimension [cit] ), such as support vector machines, naïve bayes or logistic regression, did not express this behavior; in this model, the use of the augmented data set led to the same or increased accuracy."
"the ddsrf-pll consists of the dq +1 frame rotating in positive direction (with angle θ) and the dq -1 frame rotating in negative direction (with angle -θ). the components of the dq +1 and dq -1 frame are given by equation (5) and equation (6), respectively [cit] ."
"beat tracking is an important initial step in computer emulation of human music understanding, since beats are fundamental to the perception of western music. even though people cannot completely identify every audio component, we can track musical beats and keep time to music by foot-tapping. therefore, we can build a computational model of beats and track the beats [cit] ."
"pi controller is usually used in the control algorithm of the srf-pll. the pi controller also works as a loop filter in the system, which controls vq and detects the dynamics of the system. in order to determine the phase angle of the grid voltage fast and precisely, the pi parameters must be adjusted appropriately. in variable grid conditions, if the pi parameters are not adjusted properly, errors occur at the determined phase angle and the system works unstably [cit] ."
"equation (5) and equation (6) are rearranged to determine the decoupled components as in equation (7) and equation (8) . in figure 3 and figure 4, block diagrams of these decoupled components are given."
"from the results, it can be observed that the three learning approaches behave differently on the two test sets. the text-based learning exhibits clear advantage compared to graph knowledge learning on the wnet-n test, however on the yago-a test, the graph knowledge learning is superior. this can be explained by the fact that wnet-n is in the general and involves popular entities that can be well trained with raw and labeled text, however for yago-a, most of the entities are domain-specific and so it is not easy to learn the entities (and their relations) from unstructured text data. in this case, the human-specified knowledge, i.e., the relations offered by the graph knowledge, tends to provide the most valuable information. on the other hand, the graph knowledge of the general domain tends to be sparse and noisy, which will be discussed in sec 5, while the graph knowledge of specific domains are generally less sparse and also quite clean. this also leads to more reliable inference with graph knowledge in specific domains."
"since the data are close to linearly separable, svm with linear kernel resulted in relatively high accuracy (85.37%) when only one pc is used. the accuracy further increased to 90.24% with 10"
"where β is a hyper-parameter that is set to balance the contributions of the text data and the graph knowledge. the sgd algorithm is employed to optimize l joint with respect to the entity vectors and the vectors of words that are involved in the entity descriptions. in practice, an iterative strategy is adopted in this work, which performs the joint text learning and the graph knowledge training alternatively and iteratively, with their respective negative sampling schemes applied."
"in this study, two different phase locked loop algorithms for grid synchronization are presented comparatively. the srf-pll and ddsrf-pll algorithms are simulated in matlab/simulink software. based on the results obtained from these simulations, the performances of plls are compared under different conditions."
"the data used for this research were recorded at the institute for mental health in belgrade, serbia. the subjects were 23 patients diagnosed with depression (13 women and 10 men), 24 to 68 years old (mean 31.53, sd 10.21). all of them were examined by senior clinical psychiatrist (the diagnosis was made according to the icd-10 classification) and all were medicated. as a control we used the eeg records of 20 age-matched (mean 30.14, sd 8.94) healthy controls (10 males, 10 females) with no previous history of any neurological or psychiatric disorders, recorded at the institute for experimental phonetic and speech pathology in belgrade, serbia."
"some approaches have been presented to learn with raw text and labeled text together. [cit] introduced the explicit semantic analysis which represents a word by its distribution over the labeled wikipedia pages instead of the latent concepts as in lsa [cit] and lda [cit] . its great performance owes to learning from the combination of raw text and labeled text (wiki pages labeled by entries) resources. recently, paragraph vector (pv) was also applied to semantic relevance tasks [cit], which infers word vectors and paragraph vectors together, as the joint text learning presented in this study."
"here, model complexity was measured using the vapnik-chevronenkis (vc) dimension. [cit] indicates that the models with a small number of parameters may have larger vc dimension and complexity. according to statistical learning theory [cit], the classification accuracy on test data (measured by ten-fold cross-validation method in this study) decreases with a factor that is directly proportional to the vc dimension of the model and inversely proportional with the size of the training data set. among the classification method considered in this paper, multilayer perceptron and decision tree have vc dimension that increases with the number of utilized features for classification."
"in this paper, we proposed a method of audio identification on a few pieces of mp3 files downloaded by bittorrent client program. the pieces split by torrent file are randomly ordered, and passed to the piece verification module to verify if the piece is valid by checking the important parameters of frame header. then we decode the piece and extract the feature to identify the piece. the experimental results show the probabilities of successfully identifying the pieces with different piece sizes and different number of pieces. when the piece size is greater than or equal to 512 kb, the probability of successful identification will be greater than 90%. because of the unpredictable bitrate of mp3 file, the playback time of audio decoded from pieces of same size is variable. it increases difficulty of audio identification and needs to be further discussed in the future research."
"this article is organized as follows: in the first section, the pll algorithm is introduced and the purpose of the study is explained. in the second section, two different pll algorithms and the block structures of each pll algorithm are mentioned. in the third section, the plls with block structures are simulated in matlab/simulink under different grid conditions such as balanced, unbalanced, harmonics and variable frequency, and their performances are compared. in the last section, the advantages and drawbacks of the plls are emphasized."
"in grid interactive power converter applications, phase locked loop (pll) algorithms are very important to realize grid synchronization. the performance of pll should not be affected by adverse conditions such as voltage unbalance, harmonics, frequency and phase changes. otherwise, synchronization errors occur between the grid interactive inverter and the grid. in this paper, two different pll algorithms are simulated by modeling under matlab/simulink. the performances of the plls are comparatively presented under four different grid conditions such as balanced, unbalanced, harmonics and variable frequency. in this study, synchronous reference frame-pll (srf-pll) and decoupled double synchronous reference frame-pll (ddsrf-pll) which are mostly used, state-of-arts and effective pll algorithms are analyzed by modeling in grid synchronization applications. it has also been demonstrated the positive and negative aspects of the plls based on the obtained results."
"finally, the pll algorithm which should be preferred according to different grid conditions is presented in table 1 . if the grid interactive inverter is used only for a balanced and/or variable frequency grid condition, the srf-pll is more suitable to prefer. if the grid interactive inverter is used only for an unbalanced and/or harmonics grid condition, the ddsrf-pll is more suitable."
"useful musical information can be obtained from the distribution of chroma even without the absolute frequency. chroma features consist of a 12 element vector with each dimension representing the intensity which is associated with a particular semitone. record a single feature vector per beat and 12 element chroma features are used to capture both dominant note and the broad harmonic accompaniment. to identify strong tonal components in spectrum and get a higher resolution estimate of underlying frequency, use phase derivative within each fft bin. each recording is represented by a matrix of 12 chroma dimensions. fig. 3 shows chromagrams of an audio recording and one piece of it. finally, the identification is implemented by cross-correlating the two feature matrices of piece and original."
"when compared to the present literature our research has substantial originality, in the present literature, only a few studies applied an approach similar to ours [cit] . fractal dimension [cit] and both linear and nonlinear measures of eeg [cit] were applied to the classification of patients diagnosed with depression and healthy controls. it was found that nonlinear features gave better results, when compared to spectral ones, in the classification of patients diagnosed with depression [cit] . note that the use of reductionistic approaches, such as fourier's analysis, was found inferior [cit] . the rationale here is a part of complexity theory which led to consensus among researchers dealing with nonlinear analysis; key properties of linear systems are proportionality and superposition [cit] . nonlinear systems defy comprehensive understanding by a classic reductionist approach (like fourier's analysis), since they do not obey proportionality and superposition. since human brain is one of the most complex systems we know of, analyzing the signal originating from it (eeg) by utilization of reductionistic method could be misleading."
"paper in order to demonstrate that accurate classification is possible using a small number of principal components. note this technique is typically utilized to decorrelate features, such as hfd and sampen in our case (see figure 2 )."
"different information resources possess their respective advantages and disadvantages. raw text is totally unstructured and unsupervised (no data annotated). the training data is easy to be collected and in most cases, it offers good entity coverage. the shortcoming, however, is that the useful information is often buried in noise and therefore it is not trivial to extract the desired information. finally, the learning purely relies on word occurrence statistics, which often under-estimates entities that are infrequent in the training data."
"proper choice of a non-linear feature extraction method (hfd/sampen) simplifies an important classification problem and makes it tractable. to the best of our knowledge, we were the first to apply this specific feature extraction method on this particular classification task."
"finally, it can be seen that the pre-training with word vectors (trained with raw text) contributes to both the text learning and the graph knowledge learning: the pre-trained systems (jt-prt and grprt) significantly outperform the random initialized systems (jt-rand and gr-rand) . this from another perspective confirms the importance of involving multiple and heterogeneous information resources in knowledge embedding. in addition, the poor performance of jt-rand is mainly due to the incompleteness and bias of descriptions. and the bad results on gr-rand are probably caused by the loss of relation types in databases and also the loss of the variation of length on edges since every edge is trained equally. thus, pre-training method helps a lot since additional knowledge can be added in and the incompleteness of both description and graph can be hugely solved. sults of the previous section, it has been found that learning with multiple resources is helpful, even if with the simple pre-training. the joint text and graph learning takes into account both word contexts and relations when learning the entity vectors, and so may utilize text data and graph knowledge in a more effective way. figure 1 presents the experimental results with the joint text and graph learning. the two curves present the spearman coefficients on wnet-n and yago-a respectively. the learning rate of the sgd algorithm is set to 0.01, and the iteration number is set to 200. the dimension of the entity vectors and word vectors is set to 100. according to the cost function (1), the learning is impacted by the hyperparameter β, so the results with various values of β are reported in figure 1 ."
"the patients' eegs were recorded after a visit to a recommended psychiatrist. eeg was recorded in the resting state with standard 10-20 system using nicoletone digital eeg amplifier (viaysys healthcare inc. neurocare group), sitting upright in the comfortable chair, with closed eyes and without any stimulus. both rooms were in faraday's cage by design of building, noise was kept on both places below 42db (measured using phonometer), temperature was kept at 22 degrees celsius, the light is a dimly daily light, the person was sitting surrounded by white curtains with some daily light through it. all the participants (from both groups) were recording between 10am and 12h (noon). eegs were obtained from 19 electrodes in a monopolar montage with reference set to earlobes (electro-cap international inc. eaton, oh usa) using sampling rate of 1 khz and electrode resistance of less than 5 kω. bandpass was 0.5-70 hz. for the control group, the same setup was used (10/20 system for electrode placement, the same electroconductive gel, resistance, calibration, etc.), but on the nihon kohden apparatus, eeg 1200k neurofax with electrocap (model number 16755) international, inc. previous studies [cit] compared results for the same subject on different equipment and concluded that intra-subject variability is small. we kept all the settings identical in all measurements."
"for the graph knowledge learning, two configurations have been tested as well: with and without pre-training. for those entities that can't be pretrained, random initialization is employed. the results are reported in the rows denoted by 'grrand' and 'gr-prt' in table 2, for the configurations with and without pre-training, respectively."
"presumably, due to the high correlation of features (see fig. 2 ), there was no benefit of using random forests in comparison to standard decision tree classifiers."
"most recently, joint learning approaches have been proposed to learn from heterogeneous resources. [cit] learned word vectors by considering not only the word context in text data but also relations in knowledge bases. their training algorithm draws close the words that are proximate in both text and the knowledge graph. [cit] considered relation types in the joint training process. [cit] learned lexicon knowledge by forcing each word in the lexicon to be close to the corresponding pretrained word vector. these studies demonstrated that learning word vectors with both text data and graph knowledge is beneficial to semantic relevance learning."
"in machine learning, model complexity (of the classifier not of the signal) is defined as ability of a classifier to distinguish among classes that are separated with multivarious surfaces."
"this section reports the experimental settings and results. the semantic relatedness task was chosen in the study, which measures semantic relatedness among entities and compare the measurements with human-specified scores. we start by presenting the databases, and then report the results on a general domain task and a specific domain task. the data sets and codes are available online. 2"
"the non-linear features (both hfd and sampen) were computed using a custom program written in java. the classification was performed using weka software (weka v. 3.8, university of waikato) [cit] ). principal component analysis was computed using matlab (matlab v. [cit] b, mathworks) . statistical analysis was performed using spss software (ibm"
"since the overall goal of the study was to demonstrate the usefulness of non-linear features for classification of patients diagnosed with depression based on eeg signals, we calculated higuchi's fractal dimension (hfd) and sample entropy (sampen) of eeg time series (series of data points indexed in time order from raw signal). subsequently, we applied supervised machine learning algorithms to assess classification accuracy. to determine the linear dependence of eeg features, we utilized correlation analysis. we applied pca to determine the influence of linear feature extraction on classification accuracy. also, pca is known from the literature for its possibility to reduce dimensionality of feature set making machine learning models more sensitive. we examined various classification algorithms, ranging from simple and linear to highly non-linear: logistic regression (lr), support vector machines (svm) both with the linear and polynomial kernel, multilayer perceptron (mp), decision tree (dt), random forests (rf) and naïve bayes (nb)."
"it can be also observed that the mean vector (ltmean) approach does not work well, probably due to the information loss with the simple average. the joint text learning with pre-training (jt-prt) outperforms both lt-mean and word2vec. as jtprt makes use of both raw text and labeled text, this superiority confirms that learning with heterogeneous information resources is beneficial, and the joint learning (jt-prt) is an appropriate way to utilize heterogeneous information effectively."
"in our experiments, all the relations in the graph knowledge bases are treated indifferently. one may argue that different types of relations should be distinguished and learned distinctively. this is true for some tasks such as relation prediction; however, for the semantic learning task, it is still challenging to use relation information. table 5 : performance with transe in graph knowledge learning. this can be explained as follows. intuitively, when people evaluate the relatedness of two entities, both the relation types and the number of relations (directly and indirectly) between them are considered. although different relation types may impact the judgement differently, learning relation types may force entity vectors to learn to distinguish different types of relations. this is an extra constraint that is irrelevant to our semantic relatedness task. if the constraint is too strong (transe for example), it may lead to biased learning. still, relations should be used, but maybe a weak constraint is more appropriate. this is one of the future work."
"the first experiment studies the performance of learning with raw text, labeled text and graph knowledge individually. as mentioned already, the test are conducted on two data sets: wnet-n and yago-a, which represent a general domain task and a specific domain task, respectively. the impact of the dimension of the entity vectors is also investigated. the results in terms of spearman coefficients are reported in table 2 ."
the first stage of beat tracking converts the audio into a one-dimensional function of time at a lower sampling rate which reflects the strength of onsets by taking the first-order difference along time in a log-magnitude 40-channel mel-frequency spectrogram. discard negative values and sum across frequency. then slowly varying dc offsets are removed by a high-pass filter. the onset strength for the entire signal is auto-correlated to estimate an approximate global tempo. tempo is a pace reference that typically ranges from 40 to 260 bpm (beats per minute) with a mode roughly around 120 bpm. the best bpm is passed to the beat tracking module which is implemented with dynamic programming. it attempts to find a sequence of beat times that optimize both the onset strength at each beat and the spacing between beats.
"the joint text learning and the graph knowledge learning can be combined. in fact, the two learning approaches are based on the same measure space (the inner product space) and the objective functions are both hinge loss; additionally, both the learning methods train the model using sgd and negative sampling. this means that they are highly consistent and can be easily combined without much change, except that the objective function is modified to integrate the loss derived from both text and graph knowledge. this is formulated by:"
"in the first test, the pll algorithms have been tested under unbalanced grid condition. as shown in figure 6, the three-phase grid voltages are set to 311 v peak value and 50 hz grid frequency under balanced grid condition until 0.1 s. without frequency changes, harmonics and other disturbing effects are ignored. in simulation, the unbalanced grid phase voltages were injected to the system after 0.1 s. 373 v peak value for a-phase and 285 v peak value for b-phase and cphase are used as unbalanced grid voltages. figure 7 and figure 8 show the response of the srf-pll and ddsrf-pll at unbalanced phase voltages, respectively. as seen in the figures, the srf-pll responds faster than the ddsrf-pll under balanced grid condition (up to 0.1 s) due to its simple construction and low process requirements. however, the srf-pll causes a fault in determining the grid phase angle under unbalanced grid phase voltages. in this case, maximum phase error of srf-pll is 0.081 rad."
"the accuracy of a linear model (svm with the linear kernel) increased by almost 10% when applied to sampen features. similarly, the accuracy of svm with the polynomial kernel increased by almost 15%. the relatively low accuracy of polynomial svms is in agreement with previously reported results [cit] ."
the mathematical expression of the park transform used for the positive sequence components (αβ/dq +1 ) in figure 5 is as in equation (2). the mathematical expression of the park transformation block used for the negative sequence components (αβ/dq -1 ) is given by equation (10).
"in figure 5, the block structure of ddsrf-pll is given. this block structure is the extended form of the classical srf-pll block structure. figure 5 is given by equation (9)."
"finally, we would like to emphasize that an extension of the method on larger data sets is needed prior to making a final conclusion about class separability and the potential applicability of the classification techniques for diagnostic purposes."
"finally, graph knowledge is the most structured and supervised information resource. it is annotated by people and therefore is much more clean and reliable, and the relations among entities can be far beyond the ones that are represented by word local contexts as in raw text. additionally, the learning does not rely on word statistics and so is mostly suitable for new and infrequent entities, for instance those in a specific domain. an obvious disadvantage of graph knowledge is the high cost in data annotation and the low coverage of the entities and relations. the emergence of large-scale public knowledge bases such as freebase and yago partly solved the problem, however for many infrequent entities, the annotations are far from satisfactory and most of the relations are missing."
our results showed that hfd for patients diagnosed with depression ranged from 1.0812 to results of hfd analysis (fig. 1a) there is no statistically significant difference between electrodes inside the p and c group.
"the piece length specifies the nominal piece size, and is usually a power of 2. the piece size is typically chosen based on the total amount of file data. if the piece size is too small, resulting in a large torrent metadata file, and piece sizes too large cause inefficiency. the most common sizes are 256 kb, 512 kb and 1 mb. bittorrent clients download pieces in a random order to increase the opportunity to exchange data. when a peer finishes downloading a piece, the client checks that the hash matches, then passes the piece to the piece verification module. it is almost impossible to split mp3 file from the beginning of the header, which means the beginning of the piece may be frame data, side information or header. thus, when a piece is downloaded, we begin to search the header immediately. if there is an available header, we decode the piece and begin the process of audio identification, otherwise we have to analyze next piece. if the next piece is just next to the pieces already downloaded, the probability of successful identification will be increased."
this section first presents the joint text learning approach which learns entity vectors based on the descriptions that are extracted from wikipedia. then the graph knowledge learning is described. finally the joint text and graph learning is presented which learns with both text data and graph knowledge.
"the joint text and graph learning with the individual learning methods are compared in table 3, where the optimal values of β (0.0 for wnet-n and 1.0 for yago-a) have been applied. it can be seen that the joint learning contributes significantly to the specific domain task on yago-a, while for the general domain task on wnet-n, no improvement is found. nevertheless, since the individual learning is a special case of the joint learning, the latter should be not worse than the former, given that the optimal β is applied. in section 4, it has been found that graph knowledge training does not work well on the general domain task wnet-n (refer to table 2 ). this is possibly caused by the incompleteness of relations when domain becomes wider and the loss of the variation of length on edges in the knowledge base since every edge is trained equally. notice that, the number of relation pairs in wnet-n is close to yago-a while entities in wnet-n is more than entities in yago-a. to further investigate the problem, two simple 'direct inference' algorithms are employed to conduct the tests on wnet-n and yago-a respectively. the first algorithm is based on the shortest path between two entities in query, and the second one is wu& [cit] which considers not only the shortest path but also the depth of their common parents. note that both these two models do not learn any entity vectors but infer relatedness from the relations in the knowledge base, so the results can reflect the quality of the knowledge base."
"this paper presented a joint text and graph learning method which can learn entity vectors with text data and graph knowledge bases together. we evaluated the proposed method on the semantic relatedness task, and found that involving both text data and graph knowledge does improve performance. particularly, the experimental results demonstrated that for general domain tasks, the graph knowledge tends to be incomplete thus learning with raw or labeled text is the most effective, however for specific domain tasks, the graph knowledge tends to be more complete, that it can contribute a lot to learning."
"random forests classifier utilizes an ensemble of unpruned trees [cit] ). the classification is performed by combining classes predicted by ensemble members. unlike c4.5 algorithm, in each node of a tree, a random subset of the features is considered for partitioning."
"for the raw text learning, the entities are treated as words or word sequences (phrases). the vectors of these words and word sequences are then learned by the word2vector tool, together with other words. the raw text data of wnet-n and yago-a are merged, and combined with additional 200mb plain text to form a training data set to conduct the word vector training. using the text of both the two data sets is to demonstrate the advantage that word vectors can be learned with out-of-domain data. note that multi-word entities can be learned as phrase vectors [cit] table 2 ."
"sampen shows better performance in the lower frequency band, and hfd in higher frequencies of eeg. both nonlinear measures are used to examine the complexity of signal: hfd is a complexity measure operating directly in time-domain, while sampen is regularity statistics showing how predictable/irregular signal is. we then turned to establishing a methodology with measures which are computationally fast and robust to artifacts in the signal, and which could be clinically applicable. that methodology could be applied at different points in the diagnostic process to give the clinician additional support for a diagnostic decision. we did not attempt here to examine the correlation between previous medical histories of patients, but to show how proper quantification of their eeg can be used as an independent biomarker. therefore, in this study, we tested the use of hfd and sampen to discriminate the complexity of the brain's neuronal activity in patients diagnosed with depression."
"the most common types of copyright infringement of audio on bittorrent are pop songs being used illegally. thus, we evaluate the proposed method with 30 pop song samples and sample file size is between 2 mb and 5 mb. suppose the piece size is 256 kb, the playback time of the piece will be 16 seconds with the bitrate of 128 kbps. but in fact, the bitrate of audio file on bittorrent is unpredictable, which means it depends on how the mp3 file owner compress the audio, thus, the bitrates of samples were not fixed. generally, the larger piece size is the better result we get. fig. 4 shows the probabilities of successfully identifying pieces of 30 samples against different piece sizes. in this test, each sample was identified with only one piece of it. the results show if the piece size is less than 256 kb, it is difficult to identify the piece correctly. it also means if the playback time of an audio is less than or equal to 8 seconds, the audio will be unrecognizable. there are several samples were encoded with bitrate of 256 kbps, therefore, the playback time of pieces with size of 256 kb from those samples is equal to 8 seconds. this kind of phenomenon will have a negative impact to the probability and needs further research. to analyze the relationship between number of pieces and probability, we also evaluate probabilities of successful identification with different numbers of pieces randomly ordered. fig. 5 shows the probabilities of successful identification against different number of pieces with fixed piece sizes. in the case of piece size being equal to 64 kb and 128 kb, all the samples are evaluated 15 times, whereas 8 times for 256 kb, and each time the number of pieces is increased by 1. obviously, the results show the more pieces we have, the better identification results we can obtain. as shown in fig. 5 (a), it is difficult to correctly identify the pieces, especially when the pieces are less than 12, the probability drops below 60%, and most of probabilities drop below 50%. fig. 5 (b) shows better results than (a), but there are still half of probabilities drop below 60%. fig. 5 (c) shows the evaluation results of piece size of 256 kb, and all of them are greater than 77% as well as half of them are greater than 85%. when the piece size is equal to 512 kb, the probability will be greater than 90% if we have more than 1 piece. as mentioned above, the most common piece sizes used on bittorrent are 256 kb, 512 kb and 1 mb, and our method shows good performances on these sizes."
"labeled text offers a text description for each entity, so it is more supervised than raw text in the sense that some human-specified annotations are involved. however, the supervision is rather weak, since the relations among entities are not explicitly annotated but implicitly encoded within word co-occurrences of entity descriptions. a particular advantage of the labeled text learning is that the entities that are difficult to learn with raw text because of their limited occurrences can be learned by referring to the words in the descriptions, for instance by averaging the vectors of the words."
"the results are presented in table 4 . it can be seen clearly that both the shortest-path approach and wu&palmer's model show much better performance on yago-a than on wnet-n. these results provide strong evidence that the general domain is much more complicated, so that the lack of graph knowledge and the problem caused by identical length of edges will easily hurt graph-based inference."
"in the loop filter design of the srf-pll, it is very important for the dynamic performance of the system that the estimated phase angle is fast locked to the phase of the grid and shows good filtering characteristics. however, in the srf-pll, these two conditions cannot be met at the same time. in ideal grid conditions, the high bandwidth of the filter ensures that the grid voltage and phase angle can be determined quickly and accurately [cit] . if the grid voltage is disturbed by high-order harmonics, the band-width is reduced to ensure stable operation of the srf-pll, but in this case, synchronization time is increased. moreover, vd voltage cannot exactly determine. when imbalances in the grid voltage occur, reducing the band-width cannot stabilize the system. this problem can be solved by adding a simple low pass filter to the system. while the addition of low pass filter improves the stability of the system, it greatly reduces the dynamic response of the system [cit] ."
"healthy controls underwent general medical examination and testing with clinical psychologist to confirm their inclusion in the study. the participants from both groups were all right-handed, according to the edinburgh handedness inventory [cit] . all the participants were informed about the experimental protocol and signed informed consent forms. helsinki declaration and its later amendments or comparable ethical standards."
"the aim of this paper is to compare the performances of two of the most preferred, state-of-arts and the most effective pll algorithms under four different grid conditions such as balanced, unbalanced, harmonics and variable frequency. in this study, srf-pll and ddsrf-pll algorithms have been modeled. the performances of the pll methods have been compared based on the obtained results. the results have been presented in a comparative table and the advantages and disadvantages of these pll algorithms have been pointed out. in this respect, according to the grid disturbances, pll algorithm which should be preferred has been put forward."
"while the srf-pll has a good response under ideal grid conditions and variable frequency grid condition, it causes errors in determining the phase angle of the grid voltage under non-ideal grid conditions such as distorted and/or unbalanced [cit] . in non-ideal conditions, different filtering methods should be used [cit] ."
"another nonlinear measure, sample entropy (sampen) [cit] . sampen estimates signal complexity by computing the conditional probability that two sequences of a given length, m, similar for m points, remain similar within tolerance r at the next data point (when self-matches are not included). mathematically, sampen is the negative natural logarithm of the conditional probability that two sequences similar for m points remain similar at the next point. thus, sampen measures the irregularity of the data (the higher values, the less regular signal) that is related to signal complexity [cit] ."
"moreover, one of the most common reasons for internal traffic congestion is data communication between the map and reduce phases [cit] . usually, this shuffles a large chunk of the map results in the reduce phase based on the key value of the mr job, and thereby on the internal traffic of the mr job. experimental results show that our solution is well adapted to big data spaces. a shortened form of this paper has been presented previously in conference papers [cit] . here, we expand on the theoretical and evaluation aspects of the optimization model and propose a multiobjective serviceselection algorithm for big data spaces. we are among the first to propose bidirectional (internal and external) traffic optimization based on qos awareness for multiobjective service-selection algorithms in a distributed environment. our main contributions are:"
"definition 2 (combinatorial service selection): select the composition plan that satisfies the qos requirements, namely the upper limit of negatively affected qos criteria and maximum normalized qos criteria."
"this section describes the proposed algorithm for service selection in big data space. finding the optimal service composition sequence is a cumbersome task, mainly because of the complex composition requirements sought within a large search space. it requires high-performance infrastructure to complete such a resource-intensive heavy-duty job. we propose an mr algorithm that aims to meet heterogeneousselection requirements, achieve global optima for linear and combinatorial selection requirements, and near-optimal multivariate-selection requirements."
"in addition to that, service selection is a natively np-hard problem for finding the optimal utility (optimized for linear, combinatorial or multivariate) values of qos in composition plan among services available. to address these issues, we employed the heuristic methods to find the optimal selection composition plan. then it is obvious, the given selection process always tries to achieve the global/local optimal utility qos plan among the given services. then service selection also generates the 'hot' services and composition plans during the selection process. therefore, we assume, it has more tendency to affect the intrinsic traffic congestions which are caused by zipf and pareto phenomena's than these two traffic congestions appear in the native traffic in the hadoop environment."
"definition 3 (multivariate optimal service selection): select the composition plan that satisfies the combinatorial qos-constrained set of each service, which is the resultant service in the composition plan that satisfies the respective minimum upper limit for negatively affected qos criteria with respect to the maximum normalized qos criteria."
"in this section, we present our proposed solutions to the issues described in section ii. in sections iii-a, iii-b, and iii-c, we present the proposed solutions for multiobjective selection requirements. in section iii-d, we develop the proposed solution for bidirectional traffic concerns."
"in other cases, users need to satisfy a combinatorial optimization of their qos requirements with respective to a maximum upper bound for the overall negatively affected values, while maximizing the overall positively affected qos of the services. here, we propose a 0-1 multi-constraint knapsack problem (0-1 mckp) [cit] to satisfy these requirements."
"effectiveness: to evaluate the effectiveness, we conducted experiments that observed, in terms of computational complexity, the internal, external, and jointly optimized traffic effectiveness of multiobjective selection methods while increasing the number of plans and data nodes. in particular, we observed the precision of the proposed methods."
scenario 1: users need to find the global optimal service composition sequence that maximizes the overall profit of qos criteria as shown in eq. 1.
the respective outputs of the reducers represent an optimal composition planner for the given selection criteria. this executes the respective key-related results from the reducers and outputs the respective optimal planner associated with each key (servicing the first tasks in the composition requirement).
"the ws selection requirement defined by definition 3 in section iii is simulated using the abc algorithm [cit] . the utility functions for the positively affected qos (profit) and negatively affected attributes are represented by eq. 16 and eq. 17, respectively. table 3 gives the respective value distributions across the candidate services for the given task. the utility qos of the jth candidate ws of the ith task is positively affected by u i,j:p, whereas u i,j,β:,n is the βth negatively affected utility qos of the jth candidate ws of the ith task. the user sets β constraints for the β qos attributes for candidate ws of the jth task. initially, the abc algorithm initializes the generated ''food source'' randomly. here, it uses eq. 16 and eq. 17 to set the respective utility values for the food sources."
"it is evident that the native service selection process is a well-known np-hard problem [cit] . moreover, conventional standalone processing platforms are reaching their limits in dealing with np-hardness [cit] . thus, we move toward big data-related research topics. hadoop and spark are currently the most widely used big data processing platforms. however, the spark platform is generally regarded as involving expensive in-memory processing and lacking its own filemanagement system. this hinders the study of native traffic congestion occurring during the selection process. hadoop is the most popular native big data processing platform. it includes its own file-management system and facilitates a diverse range of techniques for file handling. therefore, it allows studying the occurring-traffic during the selection in a more effective manner. it also provides a gateway to addressing a diverse range of problems associated with research and industry domains in a cost-effective manner. mapreduce (mr) is a well-known fundamental programming technique in the hadoop space. therefore, we devised our selection method based on mr in the hadoop space."
"imr agent: the mapper phase generates large chunks of intermediate data that are passed on to the reducer phase for further processing. at the beginning of the reducer phase, large chunks of intermediate data are shuffled to facilitate reducer processing. this leads to massive network congestion. to address this, we propose to introduce an imr agent that reduces network congestion and relieves the workload of the reducer phase. the imr agent mainly handles the task of shuffling the intermediate chunks, which is separated from the reducer phase and plays a crucial role in reducing network congestion. it is important to note that the primary job of the imr agent is to process the output data from the mapper before being passed to the reducer phase to reduce the workload of the reducer phase and minimized the shuffling data chunk. therefore, imr results in two major benefits, firstly, it accomplishes the part of the workload of the reducer phase, and secondly, it reduces the shuffling data traffic. we have employed a combiner to implement the imr agent class."
"according to the fig 4, the highest computation cost is shown by the abc and lowest by the dijkstra. however, abc shows exponential growth and the considerable gap in the processing time compared to the other two methods."
"moreover, based on eq. 22, traffic caused by all services of the i th job can be expressed as shown in eq. 23. here, n is the number of services and γ is the number of jobs in the mr process."
"the 0-1 mckp method demonstrated the highest joint optimization among the methods, with abc being next best. the 0-1 mckp method could achieve 49% efficiency by joint optimization when the number of data nodes was increased to four. for abc, 40% could be achieved and dijkstra could achieve a maximum of 28%. the selection process usually reduces the efficiency of joint optimization when the process introduces a second mapper. however, the process increases efficiency as the number of plans in the process increases. this implies that the proposed method for jointly optimized traffic-aware selection can work efficiently."
"the zipf phenomenon in a hadoop distributed file system (hdfs) refers to an access pattern distribution of replicas in a given file according to zipf's law. this results in a ''hot'' replica (higher access rate) among replicas available in a given file. in the service-selection process, the aim is to deal with qos service preferences during this process, but the hotness phenomena tend to favor services with lower qos preferences. this leads to a reduction in the accuracy of the selection process. in addition, because of the limited number of replica preferences, it generates heavy traffic congestion and causes a reduction in overall performance. to address these concerns, we propose a qos-aware service distribution method."
"according to the network traffic and the data flow of the selection process in the mr job, we can state the problem as follows, in terms of an effective-selection job. fig. 2 shows the anatomy of an mr job. it has to pass rigorous steps in the data flow while facing severe constraints, namely, internal and external traffic. c mr i is the cost of the i th job of the mr process for the selection. c joint i is the traffic cost that affects the i th job of the mr process. in addition, c sel i is the cost of the selection process during the i th job of the mr process. we can then describe the cost of the i th job of the mr process as shown in eq. 6."
"there has been a dramatic surge in the availability of candidate services, which, in turn, has led to ''resource starvation'' in selection processes. the variety and uncertainty (veracity) of quality of services (qos) among the high volume of candidate services increases the np-hardness in the search for solutions [cit] ."
"initial setup: first, we prepared the availability of the service as follows. the hdfs needs to contain the respective service information according to definitions 7 and 8 in volume 6, 2018"
"the abc method demonstrated a higher traffic efficiency than the other two methods. according to fig. 4, abc also has the highest processing cost. this implies that abc should have the highest external traffic. therefore, the proposed method for external traffic efficiency works best for increased external traffic in the selection process."
"evaluation metrics: our aim was to evaluate the efficiency and effectiveness of the internal, external, and joint traffic optimizations of multiobjective selection methods in a hadoop environment. to achieve this, we defined four modes (mode 1, mode 2, mode 3, and mode 4) and their respective computational complexities (p, q, r, and s) as shown in eq. 31, eq. 32, eq. 33, and eq. 34."
"according to eq. 25, traffic to s j and s j+1 caused by zipf for the entire mr process is: (26) services that have higher normalized qos have more traffic during the mr selection process. then, from eq. 24 and eq. 26, traffic increments of s j and s j+1 are proportional to their normalized qos. this is represented by eq. 27."
"the pareto raised most of its concerns in the split and map stages of the mr process. these areas are highly correlated with specific replica accesses for specific needs when pareto phenomena and hot replicas occur. to address this concern, we propose the trd rule."
"we now consider the external traffic efficiency for the three selection methods. we calculated the external traffic efficiencies using eq. 39 above. tables 7, 8, and 9 give results for the external traffic efficiencies for the dijkstra, 0-1 mckp, and abc methods, respectively. again, m represents the number of plans, in millions."
"the algorithm then calculates the probability of the fitness value. here, it evaluates the nectar information from all employed bees and chooses a food source (identified solution) with a probability related to the amount of nectar (overall profit). if it cannot find a better solution, then it enjoins a ''scout bee'' to find a new food source in the same way as does the employee bee. it continues this process until the termination condition is reached."
"in section a, we introduce some preliminaries to selectionrequirement issues. in section b, we elaborate on the selection data flow in the mr process. section c formulates the selection traffic problem for big data space."
"the 0-1 mckp method maintained relatively high traffic costs for the various traffic types. this is because 0-1 mckp has to solve combinatorial selection requirements, unlike the dijkstra method. this implies that the linear optimal selection requirement solved by the dijkstra method causes it to generate the lowest possible traffic cost among the three methods."
"the dijkstra and 0-1 mckp methods always result in a globally optimal solution. therefore, we needed to consider only the abc method to measure the precision of the composition plan for multivariate optimization. we conducted experiments using the qws dataset under various types of candidate services to find the average precision of the given result results for the abc algorithm. we found the respective error deviations, as shown in eq. 41 and calculated the average precision, as shown in eq. 42. here, d sum is the deviation from the ascending ordered result and n is the number of attempts. 8 shows the results with and without the batchwise operator for the qws data set. it shows that the proposed method consistently maintained a relatively high precision, whereas the alternative would have a drastic reduction in precision. this indicates that the batchwise operator is the main reason for maintaining the effectiveness of the results in a consistent manner."
"we considered the internal traffic efficiency of the three selection methods, namely dijkstra, 0-1 mckp, and abc. we calculated the internal traffic efficiencies using eq. 38 above. tables 4, 5, and 6 give results for the internal traffic efficiencies for each method. here, m represents the number of plans, in millions. fig. 4 shows the processing costs for mode 1. this can be used as a reference for the processing costs for the various methods in the hadoop environment. for all three methods, the efficiency is suddenly reduced as the number of plans changes from 2m to 4m. the number of plans is increased by changing the number of mappers. for 2m, a single mapper is used. for 4m, two mappers are used. if a single mapper is used, the internal efficiency will be higher than when two mappers are used because using two mappers will generate more internal traffic. that is, increasing the number of plans will increase the number of mappers, thereby increasing the internal traffic. in our experiments, we gradually increased the number of mappers and the number of data nodes in the selection process. the internal traffic should increase when increasing the number of mappers because of the increased cross shuffling in the selection process. however, by using an imr agent, the internal traffic efficiency is increased, from two mappers upwards. that is, as the internal traffic of the selection process increases, the imr agent causes an increase in the internal traffic efficiency of the selection process. we found that, across the two-node, three-node, and four-node modes, the 0-1 mckp method improved from an average 16% internal traffic efficiency to 23%. for dijkstra, it was from 10.8% to 11.2%. for abc, it was from 10.8% to 15.6%."
"two commonly identified types of external traffic congestions are the zipf and pareto phenomena's [cit] . they affect negatively to the overall performance of an mr job. both phenomena's occur naturally in any environment, including local machines, local area networks, and clouds. however, they show particularly adverse behavior in the hadoop space."
scenario 2: users need to find the global optimal service composition sequence that maximizes the overall profit and sets the maximum upper bound for negatively affected qos criteria as shown in eq. 2 and eq. 3.
"in this subsection, we present the proposed solutions for the bidirectional traffic concerns. we first describe a solution for external traffic and then for internal traffic. finally, in section iii-d-3, we present a joint optimization of bidirectional traffic concerns."
"we conducted experiments to evaluate the proposed method. we considered two key research metrics, namely the efficiency of the proposed method and the effectiveness of the heterogeneous-selection approaches in the big data space. we, therefore, performed experiments in these two main areas."
"according to our observations, the shuffling stage of the selection process faces heavy traffic congestion because of a large number of intermediate results. to address this concern, we propose a middle agent for the mr process that uses a combiner. this job-wise combiner method allows us to sort and short-list the shuffling results of each job for both the mapping and reducing sides. the result is a considerably reduced overall job in the reducer phase. we call this the intermediate mr (imr) agent."
"according to the definition 1, 2 and 3, the proposed solutions for the multiobjective selection requirements are described in previous section iii-a, b, and c. the selection solutions are running behind the distributed environment. in addition to that, traffic solutions are applied. section a describes the initial setup and overall flow, which is called as driver procedure; section b describes the algorithm of the mapper; section c describes the algorithm of the imr agent; section d describes the algorithm of the reducer and retrieve the final output."
"by aggregating these three techniques for overall external and internal traffic, we minimize the overall traffic caused by the selection process. that is, we obtain:"
"the main constraint behind a successful selection process is optimal resource utilization. however, traffic congestion is one of the most constraining factors behind optimal resource utilization in a big data environment. this accounts for considerable inefficiencies in the overall process. according to our literature review, we identified two key types of traffic congestion that affect the mr process internally and externally. we, therefore, define key traffic congestion in these two categories as follows."
"the experiments were conducted in centos 7, with hadoop 2.2 and java 1.8 installed on a four-node hadoop volume 6, 2018 cluster. the master node contained an intel core i7 3.4-ghz 8-core processor and 8 gb ram. each data node contained an intel core i5 3.0-ghz 4-core processor and 8 gb ram."
"the expansion in services has led to increased opportunities. studies show that the growth in revenue has more than doubled in bi-annually, with an increase more than 220% [cit], 2016 [cit], 2014 [cit] . 1 in addition, studies show a doubling of the volume of services in the programmableweb.com store each year. moreover, programmableweb is becoming a popular platform for wellknown providers such as ibm, google, and microsoft [cit] . this confirms that growth in the consumer market and the availability of services is extensive."
"these three graphs plotted the caused and solved the average traffic for the various selection methods, implying that the proposed traffic solutions work effectively with respect to the various traffic concerns occurring with the various methods."
"finally, we consider the efficiency of joint optimization of the internal and external traffic for the three selection methods. we calculated the jointly optimized traffic efficiencies using eq. 40 above. tables 10, 11, and 12 give results for the jointly optimized efficiencies for the three methods while increasing the number of plans and the data nodes in the hadoop environment. again, m represents the number of plans, in millions."
"in section iv, we proposed a threshold plan for each batch, which is an approach that addresses the precision of the proposed method by reducing the search space and segmenting it in terms of batchwise content. to evaluate the proposed method from the perspective of the efficiency of the results, we conducted two different tests with two data sets."
"joint traffic efficiencies of each method are 28% by 0-1 mckp, 25% by abc and 17% by dijkstra. the highest joint traffic efficiency is earned by the 0-1 mckp and minimal by the dijkstra method. dijkstra has minimal computation complexity, therefore linear optimal algorithm earned minimal joint traffic benefits. however, joint traffic efficiency doesn't show the proportional relationship to the computation complexity of the objective function."
"-the multiobjective service-selection algorithm in a distributed environment. -qos-aware rule-based traffic solutions to optimize traffic caused by the zipf and the pareto. -combiner based traffic solution on optimizing the traffic in shuffling stage. the remainder of the paper is structured as follows. in section ii, we introduce some preliminaries and formulate the problem statement. in section iii, we present our proposed solutions for multiobjective selection requirements and bidirectional traffic concerns. section iv discusses the proposed traffic-efficient multiobjective selection algorithm. in section v, we discuss evaluation, and section vi considers related work. section vii concludes the paper."
"proof: according to lemma 2, the hotness caused by a particular replica will be directly proportional to the traffic. this can be applied to pareto, where the hotness caused by the pareto is directly proportional to the traffic."
let f be the functional representation of the proportional distribution of services and ϕ the functional representation of the hotness that occurs from zipf during service selection. the rules are formulated in eq. 18 and eq. 19.
"service-selection processes in big data spaces can involve excessive traffic congestion because of both internal and external factors in the mr process [cit] . considering the mr algorithm as the base, shuffling traffic called the internal traffic occurs during the mr process, and zipf, pareto traffic occurs due to the hotness of the data and outside of the mr algorithm. then they are called as the external traffic perspective to the mr algorithm. we refer to these two issues as ''internal traffic'' and ''external traffic,'' respectively, throughout this paper."
"next, it decides the types of qos-awareness (linear optimal, combinatorial and multivariate) need for the composition. if he needs the linear optimal then he should use the dijkstra method and doesn't have to do anything, just need to start the selection process. if he needs combinatorial, then he should use the 0-1 mckp and define the minimum lower bound for the collective value of negatively affected criteria's as shown in eq. 3. if he needs multivariate, then he should use the abc and define the minimum lower bound for each of negatively affected criteria's as shown in eq. 5. finally, the user sets the above parameters in the selection procedure 1 line 3. procedure 1 shows the flow of selection procedure and arrangement of the driver of the mr process. lines 2 to 9 contains the driver with three sections: n number of the mappers, an imr agent, and the reducer. at the beginning of the selection (line 3), value for the threshold_plans and the type of selection are assigned. next, n number of mappers are specified (line 5). line 6 includes the imr agent in the procedure, which is responsible for reducing the traffic in the shuffling stage and thereby reducing the burden on the reducer stage. line 7 includes the reducer class, which processes intermediate results given by the imr agent and outputs the optimal (or near-optimal) composition service sequences."
"in section iii-d-2, we proposed an imr agent approach to address the key internal traffic congestion occurring during selection. we conducted experiments in modes 2 and 3 with an imr agent to obtain the respective metrics."
"proof: we can apply theorem 1 to the pareto traffic in services, which is proportional to the normalized qos criteria of the services. however, replicas in the hdfs represent particular services. therefore, we can extend theorem 1 to the replica level of the s j service, assuming s j is replicated by r 1, r 2, .., r n, with n being the default replication factor in the hdfs. according to the pareto rule, it makes hot replicas from available replicas among the given services. in addition, according to theorem 1, we can address this traffic by increasing the number of instances of a particular file. this implies that an increment of hot replicas according to their popularity (hotness) is inversely proportional to the traffic."
"plans are made by the given candidate services. we use eq. 12 to calculate the distance of vertices (services) between the i th and the (i + 1) th tasks containing any two services, which is called the l(s i, s i+1 ). according to the fig. 3 shown dag graph, it does not allow to create edges in between services in the same task. fig. 3 dag allows edges between adjacent tasks only, where c is a constant. for service s i+1, its utility value u s i+1 is given by eq. 13."
"as for internal traffic efficiency, a sudden reduction in efficiency occurs when the second mapper is introduced, as the number of plans increases from 2m to 4m. however, efficiency is increased as the number of mappers is further increased. in our experiments, we gradually increased the number of mappers and the number of data nodes in the selection process. this means that the popularity of the service data of the previous user case and the test case will directly affect the following test case. according to our hypothesis, popularity should be proportional to the hotness of the service data, which implies that increasing the popularity of service data should reduce external traffic. we observed that the proposed rules for external traffic successfully reduced the overall traffic for the selection process. we found that, across the two-node, three-node, and four-node modes, the abc method improved from an average 21.2% external traffic efficiency to 30.4%. for 0-1 mckp, it was from 12.2% to 22.6%. for dijkstra, it was from 10.4% to 21.6%."
"summaries of the respective methods are discussed in the following. to find the summarized average values of the respective methods, first we calculate the average values of three rows separately and then calculate the average of the respective three average values of table 4 to 12."
"external traffic efficiencies of each method are 26% by abc, 18% by 0-1 mckp and 17% by dijkstra. the highest external traffic efficiency is earned by the abc and lowest by the dijkstra. that means, during the execution of the multivariate algorithm (abc), process earned the highest external traffic efficiency compared to the other two methods. relatively abc has an exponential increase of processing cost and this is caused by the computation complexity of multivariate composition requirement. however, abc results in more hot data due to the highest computation complexity than the other two methods. in return, abc results in the highest efficiency as well. this implies, proposed external traffic solution works better while the presence of more hot data in big data space. meantime, dijkstra has minimal computation complexity compared to the other two methods and it results in relatively lowest efficiency. this means the computation complexity of the objective function shows a roughly proportional relationship to the external traffic."
"dataset: we conducted our experiments with a real-world dataset provided by al-masri and mahmoud [cit] called the qws dataset, which contained 2500 items of real service information for 10 types of qos data. for our testing purposes, we use two negatively affecting qos criteria (response time, and latency) and three positively affecting qos criteria (availability, throughput, and reliability). we repeated each test case 30 times to obtain the average values for that test case. we prepared 2,000,000 to 10,000,000 planners in a big data environment from the qws data."
"in this subsection, we describe preliminary studies in the problem domain. first, we define the types of selection requirements and traffic congestion. next, we define a problem statement for selection in the big data domain and model the selection problem under the external and internal traffic concerns."
"next, it sends the employed ''bees'' to the food sources (identified plans) and determines the amount of ''nectar'' (overall profit). it then calculates the fitness values for each food source. greedy selection is applied to the current solution and its mutant. if the mutant solution is an improvement, it replaces the previous solution and the trial counter of the solution is reset. if a better solution cannot be found, the trial counter is incremented."
"given this aim, we propose three types of selection approaches by considering the three main types of composition requirements, the first being based on linear programming and the other two involving dynamic programming."
"therefore, to have an effective and efficient selection process, we have to address the concerns described in eq. 10. this means that the respective c map i, c s i, c red i, c z i and c p i values need to be reduced to reduce the respective c joint i . this directly affects c mr i, as shown in eq. 6. we can then deduce the eq. 11 to show the effects of traffic congestion in reducing the overall traffic. this implies that we can reduce the overall execution cost of the mr job of the selection process when we minimize the collective internal and external traffic congestions, as shown below. here γ is the number of jobs in the mr process."
"introducing new research areas such as data science (ds) also increases the complexity of the composition system. lengthy processes such as data analysis (da) for business intelligence in a ds field typically comprise four different phases: preparation, analysis, reflection, and dissemination [cit] . in the preparation stage, data must be collected, formatted, and cleaned. in the analysis stage, the analysis, debugging, and inspection of substages must be cleared. in the reflection stage, there are comparisons and calls to reexecute the substages of the analysis stage. in the dissemination stage, steps such as displaying, deploying, and retrieving the statistics of the detailed process must be at least minimally performed. each of these substages can contain multiple tasks and modern composition systems have lengthier processes than do their conventional counterparts. we recognize this as a challenge of modern service selection. in response, new da research is emerging in ds, involving approaches such as big data analytics (bda) [cit], which is lengthier than general da methods."
"the 0-1 mckp method demonstrated a higher traffic efficiency than the dijkstra and abc methods. according to our investigation of the internal data used in the shuffling stage, 0-1 mckp generated more internal data than the other two methods. this implies that the imr agent works more effectively when increasing the internal data of the process. more generally, the proposed imr agent works efficiently when increasing the number of data nodes, the number of mappers, and the internal data in the selection process."
"internal traffic efficiencies of each method are 19% by 0-1 mckp, 13% by abc and 11% by dijkstra. the highest internal traffic efficiency is earned by the 0-1 mckp and lowest by the abc. that means, for combinatorial requirement (0-1 mckp) result in the highest number of intermediate results while multivariate requirement (abc) has minimal intermediate results during the process. all three methods show relatively low increment value while increasing number of nodes for a particular number of plans and increasing the number of plans for a particular number of nodes."
"in addition, the mapper part is working hard to achieve the optimal composition plan among the given list of plans, as described in sections iii-a, iii-b, and iii-c. here, exponential traffic occurs to the highest qos because all three volume 6, 2018 native techniques are designed to follow the optimal qos. therefore, the mapper stage traffic for the i th job of the mr, t"
"efficiency: to evaluate the efficiency, we conducted experiments that observed the internal, external, and jointly optimized traffic efficiencies of multiobjective selection methods while increasing the number of plans and data nodes in the hadoop cluster."
"in section iii-d-1, we proposed two approaches to address the key external traffic congestions occurring during selection. we integrated these two methods to conduct experiments that evaluated the efficiency of the proposed method under multiobjective selection approaches. first, we sorted the wss based on their utility qos values and partitioned the dataset into n sets. we then took the average qos of each batch and found their proportional values. we set the default mode as one and the remaining batches according to their proportional values. this is simulated in the qsd rule defined in section iii-d. next, we multiplied the respective proportional values by m to simulate the trd rule, also defined section iii-d. we then observed the traffic-aware replica distribution across the network (here, m and n are positive integers). according to these results, we distributed services and their replicas across the hdfs. we conducted experiments in mode 3 and 4 with external traffic solutions to obtain the respective metrics."
we define service-selection requirements that are identified as three of the most common types. we use these three requirements to show the adaptability of the proposed method. one of these three requirements will be used in the relevant selection scenario during the mr process in the big data space.
"abc shows the highest traffic effectiveness compared to the other two methods. it maintains a considerable gap between the other two methods. this caused by the computation complexity of the abc method. respectively 0-1 mckp and dijkstra show relatively low and increment in all three methods (internal, external and joint) while increasing the number of plans in the environment as shown in fig. 5, 6 and 7."
"for this metric, we mainly considered the computational complexities and the precision of the three methods in the hadoop space. the results are shown in figs. 5, 6, and 7, with respect to their internal, external, and jointly optimized traffic costs. all three graphs maintain the same pattern with only slight deviations. the average line graph for abc is concave in shape, whereas 0-1 mckp and dijkstra maintained a nearly linear relationship with execution time. this means that the derivative of the abc line graph increases with an increasing number of plans, but the other two methods do not show such an exponential increment. that is, internal, external, and jointly optimized abc traffic effectiveness for the abc method increases exponentially, whereas the increase is linear for the other two methods. this is because the abc method has a multiobjective selection requirement, in contrast to combinatorial and linear selection requirements. numbers of service plans are directly affected to increase traffic in all three methods."
"here, c int i is the internal traffic cost of the i th job of the process and c ext i is the external traffic cost of the i th job of the process. these external and internal costs can then be further divided and expressed by eq. 8 and eq. 9 as follows."
"to calculate these measures, we executed multiobjective selection methods for the various modes by increasing the number of plans while increasing the number of nodes of the hadoop environment. we maintained a default mode (mode 1) with minimal replica and block distribution and no imr agent. modes 2, 3, and 4 were devised to satisfy particular traffic solutions as follows."
"we calculated the efficiency of the proposed internal, external. and jointly optimized methods as e(t internal ), e(t external ), and e(t joint ), respectively, based on the computational complexities of four modes (eq. 31, eq. 32, eq. 33, and eq. 34) and three basic traffic metrics (eq. 35, eq. 36, and eq. 37)."
"we propose an efficient, rule-based traffic technique to address the concerns that occur during the selection process in an mr job. from our observations, zipf raised the most concerns, especially in the split and map stages of the mr process shown in fig. 2 . these areas are highly correlated with specific files and their replica access for specific needs in the selection process, incurring zipf phenomena and hot files. to address this concern, we propose the qsd rule."
"here, it is most actively and exhaustively used for the highest normalized qos replicas. this means that the highest traffic occurs for the highest qos replicas, and the lowest traffic for the lowest qos replicas received."
"in some cases, users need to find the global optimal qos requirements from a given set of candidate services. here, to satisfy the linear programming requirements, we propose a dijkstra algorithm-based method as a novel technique for selection domain."
"next, based on the computational complexities of eq. 31, eq. 32, eq. 33, and eq. 34, we defined three traffic metrics c int, c ext, and c joint in eq. 35, eq. 36, and eq. 37."
"finally, peer users may seek multivariate optimization of their qos parameters for their composition system. to satisfy this requirement, we propose using selection criteria based on the artificial bee colony (abc) algorithm. these three methods are integrated into the multiobjective selection method referred to in this paper."
"the pareto phenomenon in general environments refers to 80% of the data usage being represented by 20% of the data. this is known as the ''80/20 rule'' [cit] . however, the hadoop process demonstrates the further adverse effect in the hadoop environment. this phenomenon causes to increase the hot files. to address this traffic congestion phenomenon, we propose a traffic-aware service-replica distribution method."
"section ii-c contains the problem statement for overall traffic congestion. according to eq. 8, we have to minimize the traffic concerns occurring during the selection stage of the mr process. these are expressed as c mr i, where i is the i th job of the mr process. in terms of the proposed techniques explained earlier in section iii-d, we minimize the overall traffic concerns as follows."
"-qos values for web services (ws) that are affected positively by the performance of the service are denoted by qos p, and there will be based on these notations, we define the heterogeneousselection problem using definitions (and scenarios) 1, 2, and 3 below."
"we found that, across the two-node, three-node, and fournode modes, the 0-1 mckp method improved from an average 18.2% jointly optimized traffic efficiency to 36.2%. for abc, it was from 20.4% to 31.6%. for dijkstra, it was from 15.8% to 19.6%."
"we have proposed a multiobjective serviceselection solution that considers external and internal traffic congestions in big data space. proposed method addresses the concerns arising from the flooding of services and then the selection processes can be handled more efficiently in big data space. data traffic caused by the zipf and pareto are called as the external traffic congestions. data traffic caused by the shuffling stage of the mr process is identified as the internal traffic. we have proposed a complete model for traffic control while dealing with the selection process in mr jobs. novel qos-aware traffic-efficient methods have been proposed for external traffic congestion. in addition, we have introduced a middle agent to address the internal traffic congestion, which eases the reducer workload in an efficient manner. we adopted three selection criteria's for the multiobjective selection methods, which are linear optimal, combinatorial and multivariate qos optimizations. these methods are based on both linear programming and dynamic programming. the proposed distributed algorithm can adapt easily to dynamic selection requirements. our experimental results demonstrate that the proposed method handles traffic congestions efficiently and effectively in producing composition plans for multiobjective selection requirements. internal traffic efficiency shows relatively low compared to the external traffic efficiency. only external traffic efficiency shows a nearly proportional relationship to the computation complexity of the objective functions. the proposed method is a well-behaved qosaware rule-based traffic-efficient service-selection method for big data space. in future work, we aim to investigate qualitative qos criteria and uncertainty in the qos values for the selection method."
"service composition is a key component of the highly diverse ds [cit], including the fields of bda and deep learning. while facilitating these highly diverse composition requirements, a peer user should have the freedom to select and switch between composition requirements for the given problem using the given algorithm without affecting the fundamental data structure, thereby avoiding complex steps during decision-making or changing preferences. however, there are no existing methods that facilitate such dynamically changing composition requirements and most have rigidly specific composition patterns, such as optimizations that are focused only on linear [cit] combinatorial [cit], or multivariate patterns [cit] . therefore, it is very important to facilitate multiobjective selection methods, which are flexible with respect to different selection requirements without affecting the overall flow of the algorithm. accordingly, our aim is to propose a multiobjective selection algorithm that facilitates user-driven flexibility about selection methods."
"moreover, a given web application usually consists of different kinds of web pages: in a web forum, there are pages that display lists of forums, pages that display the list of posts under specific forums, and pages that point to individual posts with their comments. thus, the knowledge base will describe the different kinds of web pages under a specific web application and then, based on this, we can define different crawling actions that should be executed against this specific page level."
"the paper is structured as follows. section ii discusses the research that provides the background to this analysis. section iii describes the space ground legacy system, and section iv describes the formal approach used. sections v and vi describe the model of the system and its analysis. section vii discusses the results. section viii concludes the paper."
"3) adding more panels: the other panels defined for the system have similar characteristics. for example, the sin panel features safety relays in addition to variables. each relay indicates whether it can be released or not. relays are defined as boolean attributes of the sin interactor. the sin panel also has the additional feature that it can be set to automatic mode."
"crawling and extraction. after detecting the application to whom the current web page belongs, the next stage is to determine the corresponding crawling actions. crawling action scopes go beyond just a list of urls to add to the queue. it can be any action that involves using apis to extract relevant data from the social network sites, such as twitter, or performing complicated interactions with ajaxbased applications, or identifying web objects in particular web application. more specifically, crawling actions are of two kinds: navigation actions: to navigate to another web page or web resources."
"formal verification techniques have an important role in reducing risk in safety-critical interactive systems. the modelchecking-based approaches used in this paper have the rigor, and the tool support needed to perform exhaustive and automatic verification of key properties of system designs. the approach also has potential to prove systematically that capabilities are supported."
"the first two implications specify that when the bd1a or bd1b variables set their values the setting of the variable is done in the telemetry panel too (i.e., setvalue2 happens when any of the variables are set). the third and fourth implications state that setvalue2 for bd1a and bd1b are only permitted when bd1a and bd1b, respectively, are set. in other words, setvalue2 cannot happen unless a variable is set."
"properties can be checked using the model checker and refined iteratively to verify the circumstances in which a property is true. if it fails to be true, then the trace offers a counterexample. these counterexamples provide three types of information. property failure can be the result of a deliberate strategy by the analyst, in which the originally property is negated in order to obtain a sequence of actions representing a potentially significant scenario where the required property is satisfied. for example, alternative sequences that involve a confirming action can be used to assess situations in which the confirming action could be used. the failure of a property can also provide valuable feedback to indicate why the modeled system fails the property. this can be useful in redesign, or in deciding that the circumstances in which the property fails are not significant in terms of the correctness and safety of the system. the failure of a property can, finally, indicate that the property is not adequate as its stands to represent the requirement. this kind of failure would result in refinement of the property."
"we similarly want a declarative language for describing all crawling actions (again, the hope is to have an easily maintainable knowledge base, including machine maintainability). we therefore need a navigation and extraction language able to access data from the deep web as well as regular urls. we will use oxpath [cit] . oxpath is an extension of xpath, with added facilities for interacting with web applications and extracting relevant data. it allows the simulation of user actions to interact with scripted multipage interfaces of the web application (the evaluator relies either on a mozillabased or webkit-based browser). it inherits from xpath as well as allows to use css-based selectors. it makes possible to navigate through different pages by using clicks and even allows to extracting information from previous pages. an open-source implementation is available, that will be integrated into our system."
"this analysis has shown how more complex display structures can be modeled and analyzed using the ivy tool. in this particular case, the model was developed from the user operation manual, while interaction with the iae team was kept to a minimum. the model, therefore, represents the information that is provided to operators as a guide rather than necessarily reflecting the actual implementation. proving properties can, therefore, be understood as investigating the quality of the provided information. the analysis made it possible to identify instances where not enough information is provided by the manual. we argue that this lack of appropriate information is relevant on two accounts. on the one hand, it might lead to automation surprises during the operation of the system. the model that the operator is able to build from the manual is incomplete. this is particularly relevant in systems such as the one being analyzed where operating procedures are followed, and little or no exploration of the systems outside these procedures is carried out. on the other hand, if we consider the operator manual as the de facto specification of the user interface, then the fact that it is incomplete will have a negative impact on the design and testing of updates or future versions of the system."
"in either case, setting the values in the panels must be constrained to happen only when setvalue happens in the main interactor; otherwise, coordination would not be guaranteed:"
models are developed in the mal interactor language. the interactor is a structuring object [cit] that describes how part of its state is rendered to some presentation medium. more explanation of the notation and its use can be found in section v. mal is used because it describes state transitions in a similar form to those specified by graphical notations such as simulink statecharts. notations such as these are increasingly acceptable to industry (see [cit] for example).
"this property also fails. the property can be generalized to all panels by introducing a variable ranging over the set of all panels, generating 23 properties:"
"the first step in the analysis was to formulate properties expressed in ctl. once a property has been formulated, it is checked using the model checker, and the result interpreted. checking the property is an automated step. however, formulating properties and interpreting results, although tool supported, requires human judgment."
"the patterns provided by the ivy tool were used to generate properties for verification. an often-cited user-related requirement of such a safety critical system (feedback) will be used to illustrate the process. providing appropriate feedback is important in maintaining situation awareness and avoiding mode confusion problems. feedback is best considered in the context of a system's goals. an example is whether feedback is always provided to the operator when a variable generates an alarm or alert. for example, in the case of the telemetry panel and the displayed attribute bd1a (monittmt.bd1a), the instantiation of the feedback pattern is presented in fig. 6 . the instantiation specifies that the alarm/alert state is represented by the error attribute. feedback is provided by changing the color of the telemetry panel's access button (modeled by attribute monittmt.colour)."
"the model described in the paper together with a model of another subsystem and a number of alternative and complementary variants were developed in the context of a masters dissertation in software engineering over a period of six months. a considerable part of this time involved study of the user manual and of the alternative modeling approaches. a final validation of the model by the first author was done in the space of a week. the student (manuel sousa) had a background in formal methods, part of the m.sc. curriculum, but no extensive experience of formal methods usage in a system of the scale of this one. he had no background in human-computer interaction. while formal methods use is still not widespread and requires appropriate expertise, tools such as the ivy workbench aim to ease the learning curve for specific application domains. results show that, with reasonable knowledge of formal methods (such as obtained at m.sc. level) and adequate guidance, an approach such as the one described can be used to identify potential problems in the user interface of a complex safety critical system. additionally, they show that the analysis can be carried out independently, based on a description of the system under analysis and using interaction with domain experts to validate their understanding of the provided documentation (effectively hiding the technical details of the approach) and still provide relevant results."
both formulations of the property fail indicating that help is not possible from the access panel (the initial state). further possibilities can be explored by removing the access panel from consideration:
"2) panels: besides defining the values being displayed and the actions supported, each interactor representing a panel also models the color of the panel's access button and its blinking characteristic."
constructing a model from the user operation manual that was sufficiently rich to provide meaningful feedback to the aeronautics and space institute (iae) team offered a significant challenge.
"though application-aware crawling in general has not yet, to the best of our knowledge, been addressed, there are some efforts on content extraction from web forums [cit] . the first approach, dubbed board forum crawling (bfc) [cit], leverages the organized structure of web forums and simulates user behavior in the extraction process. bfc deals with the problem effectively, but is still confronted with limitations as it is based on a simple rule and can only deal with forums with some specific organized structure. the second approach [cit], however, does not depend on web forum structure. their irobot system assists the extraction process by providing the sitemap of the web application being crawled. the sitemap is constructed by randomly crawling a few pages from the web application. this process helps in identifying the rich repetitive regions and then further clusters them according to their layouts [cit] . after sitemap generation, irobot obtains the structure of the web forum in the form of a directed graph consisting of vertices (web pages) and directed arcs (links between different web pages). furthermore a path analysis is performed to provide an optimal traversal path which leads the extraction process in order to avoid duplicate and invalid pages. our goal would be to develop similar techniques but for arbitrary web applications, not only web forums."
"2) expressing and verifying properties: patterns help express properties. they have been collected from a number of sources [cit] . the tool supports the selection and instantiation of patterns, with the actions and attributes of the model, thus facilitating the identification and development of ctl properties that describe requirements. the verification step itself is performed by invoking the nusmv model checker [cit] from the ivy tool. mal interactor models are translated automatically into equivalent smv models."
each of the panels of the ev subsystem presents information about a specific aspect of the system. modeling a panel involves defining the variables represented on the panel along with the additional actions and control logic that relate to them. the manual describes these in detail.
"the motivation for this study is twofold. on the one hand, the existing system is due for replacement, and it is understood by the organization that building a thorough understanding of it (including its user interface) will help shape the requirements for the new system, its testing, and acceptance. the challenge for the present paper is that requirements should be expressed at an appropriate level and be capable of formulation as properties so that evolutions of the design can be shown to satisfy the requirements. a further aim is that the evolution can be tested in relation to the requirements."
"the property proved to be false, and a trace was generated (see fig. 3 ). this trace highlighted a situation where the bd1a variable is red under an acknowledged alert condition. this situation conflicted with an understanding of how the system worked based on the manual. analysis of the model indicated a lack of specification of what happens to a noncritical alert. the manual on which the model was based did not treat these conditions accurately. in practice, this only represents a problem if it is possible for a noncritical alert to occur, and as a result, the operator could be surprised by this unexpected and undocumented behavior. this analysis enabled us to uncover the potential for an automation surprise. this finding led to discussion with the operator of the system, and the model was updated to consider noncritical alerts."
"whether a variable is critical or not is represented by the boolean attribute critical, and the upper and lower alert and alarm limits by infalertlim/supalertlim and infalarmlim/supalarmlim, respectively. the condition expressed in the first sentence of the quote above can, therefore, be expressed as"
"approaches in the second group (focusing on use) work either with task models of how the users are supposed to use the system (their observed behavior) [cit], or with cognitive models of the mental process that drive that behavior [cit] ."
"the user interface of the ev subsystem contains a top row of navigation buttons and a display area where the panels are shown. which panel is being displayed is captured in the model by an attribute (display). the attribute has type screens, representing all panels that compose the subsystem: this action is only permitted when the appropriate navigation button appears on the user interface. as described in the manual, the row of navigation buttons is not available on a number of panels, including the login (sysaccess) and exit (endscr ) panels, and when the system is blocked (screenlock ). for action sinbtn, this is specified by the permission axiom:"
"the example illustrates that while this formulation makes the coordination rationale somewhat more explicit, it is more complex to formulate. in addition, it does not completely eliminate the implicit execution of actions. as a result of this exercise, an approach based on invariants was favored."
"since this phd work has only been started a few months ago, we have focused on developing an architecture and the methodology described in the previous section. a basic prototype of the application-aware helper has been implemented, with a proof of concept that it is indeed feasible, with recognition for a couple of web applications. for instance, the prototype has been evaluated against the vbulletin application, on a variety of web sites using this content management system. for now, the system is able to detect the type of the web application, the level in the web application, and executes the corresponding crawling actions. recently, we have integrated the yfilter system [cit] (a nfa based filtering system) for efficient indexing of detection patterns, in order to quickly find the relevant web applications. now the system is able to extract more interesting pages with a minimum effort as compared to traditional crawling approach. we still need to execute crawling actions by using an oxpath evaluator (or directly convert them into urls or xpath expressions when possible) and interface our helper with the crawler, but the initial results are satisfactory and promising for our future research."
"the guis of two of the subsystems have been modeled so far. the ev (flight events' sequencing) subsystem has been chosen as illustration. this supervisory control and data acquisition system enables the operator to monitor the state of different aspects of the system and relevant flight events during sequence testing and launching. it enables the operator to activate different tests and allows the operator to activate the rocket's security mechanical devices during the automatic sequencing for launching. the ev subsystem's gui consists of panels, each presenting a different view of the system (examples are telemetry readings and a synoptic view of the system). a row of buttons at the top of the panel allows access to the different panels available in the subsystem. each panel contains tens of graphical elements representing system variables (examples are a voltage reading and the state of a relay). when a parameter falls outside specified bounds, an alert or alarm is raised and the displayed parameter changes color (yellow for alerts, red for alarms) or blinks depending on severity. the button that accesses the panel also changes color or blinks. two panels are designated to collect and display the alarms and alerts reported in the other panels. more details about the system are provided in section v."
"knowledge base of web applications. the crawler will be assisted by a knowledge base of web applications that describes how to crawl a web site in an intelligent manner. this knowledge base will specify how to detect specific web applications and which crawling actions should be executed. the knowledge base will be arranged in a hierarchical manner, from general categorizations to specific instances (web sites) of this web application. for example the social media web sites can be categorized into blogs, web forums, microblogs, video networks, etc. then we can further categorize these specific types of web applications on the basis of the content management system they are based on. for instance, wordpress, movable type, etc., are examples of blog content management system, whereas phpbb and vbulletin, etc, are examples of web forum content management systems."
"1) variables: displayed variables can have behavior; for example, they change color under certain conditions. these variables are captured as interactors, which are then aggregated into the panel's own interactor. simpler variables, where color and display characteristics are not relevant (for example, the state of a relay represented as a boolean value), are included directly as attributes. in most cases, the manual describes variables as having a value that can change over time, a color used to display the value, and a display characteristic, which can be either fixed or blinking."
"the full axiom representing these two sentences is presented in fig. 5 . the type of error and the units associated with the variable are also described in this instantiation. the keep operator is used to express that the value of the listed attributes is kept unchanged. the value of an attribute will change nondeterministically in mal, unless this is ruled out by the keep operator, or the value is explicitly set by the modal axiom."
the functional requirements for the tpgs were established separately for each subsystem and are described in a 250-page document. the rocket's preparation process for flight includes thoroughly checking specific parts of the rocket electrical network. the operator of each subsystem must follow a comprehensive preparation checklist that performs critical procedures and interactions with the rocket's hardware and software in real time. the preparation and testing process is triggered by operator interaction with the system via the user interfaces. the gui of each subsystem must support these procedures and interactions so that the checklist is successfully completed.
"to detect a particular web application, our knowledge base allows describing several rules, based on url patterns, http metadata, textual content, xpath patterns, references to a classifier, and, possibly, web-graph-based features. the identification of the page level inside a web application can also be done by categorizing the page according to structural properties."
"two possibilities avoid this. the first is to check simply whether the help panel is always reachable (property 7), but this says nothing about how it is reached. the other is to check explicitly that the action is available in all states. with this approach, property 6 becomes property 8"
"blinking yellow: for a critical variable, when the current value of the variable is in non acknowledged alert (value within the alert range), there is no acknowledged alarm in the variable, and the previous criterion [non acknowledged alarm criterion] is not satisfied. if over the same critical variable an acknowledged alarm exists, then fixed red prevails. for a non critical variable, when the current value of the variable is a non acknowledged alarm (value within the alarm range)"
"the setvalue2 action is used to change the value of a variable on the panel. the first argument identifies the variable, the second its new value. it is coordinated with bd1a.setvalue and bd1b.setvalue (depending on the first argument) by adding constraints to ensure the requirement that when a variable sets its value the panel performs the same action (the effect operator asserts the occurrence of an action):"
"this section introduces the application-aware helper module. this module assists the archiving crawler for acquiring content from the social web in an intelligent and adaptive manner. this module enriches the functionalities of the crawler, and makes the crawling process more efficient."
"for publishing information, discussing political issues, sharing videos, posting comments, managing blogs, and also stating their personal opinion in ongoing discussions. currently the social web is not only used by ordinary users but is also getting much attention from political leaders. nowadays it is getting quite common in the usa and the uk to answer parliamentary questions using twitter. recently, on 6 [cit], president barack obama became the first us president to use twitter as a tool for public communication [cit] . thus the social web is also becoming a part of political campaigns as well as driving the future political agenda. this strengthens the necessity to preserve social web data."
"for instance, web forums hold dynamic characteristics which mean that, for extracting semantic content or for crawling optimization, the nature of these web forums needs to be understood. usually the content of a web forum is stored in a database. when a user makes a request, the response page is automatically generated using a predefined template. when two requests require the same piece of content, the server will return two dynamic pages with same or similar content but with two different urls. these dynamic pages create redundancy that may be harmful, both because they will require more resources to be crawled, and because the final archive will be of worse quality. blog systems also contain redundant information: there may be for instance both monthly and yearly archives, that contain duplicate content organized slightly differently. when crawling web forums and blogs with the traditional crawler approach, we will encounter many of these redundant cases. in extreme cases, the crawler can fall in a spider trap because it has infinitely many links to crawl. there are also several noisy links such as to a print-friendly page or advertisement, etc., which would be better to avoid during the constitution of the archive. traditional approaches are also unable to crawl data from highly scripted web applications or from the deep web (data accessible behind forms). finally, classical archiving crawlers do not attempt any form of data extraction whereas archivists and archive users would like to have access to more semantics about the content of the archive, such as timestamps of blog messages, identifiers of contributors, etc., even using complex page navigation for structuring the related information across several pages. for instance, a web application that organizes its content in a way that one needs to navigate through the calendar to extract the targeted information or, in the case of a web forum, where one thread can have several posts consisting of several pages and one needs to extract this related information using effective page navigation, the traditional approach will face limitation to do so efficiently, i.e., spending more time in crawling for few interesting pages with no semantics about content."
"the same variable can be represented in more than one panel thereby creating interdependencies between the panels. if the value changes in one panel, it must also change in the other. examples are the alarms and alerts panels, which feature lists of alarms and alerts fed with information from the other panels. the approach used in the tmt panel is used here. the alerts and alarms variables are defined and aggregated to create the panels' models, as captured in the aler and alar interactors. a distinctive feature of these panels is that their content is solely defined by the alarms and alerts occurring in the other panels in the system, and that the acknowledgment of alarms/alerts must also be reflected in the original variable. to support this an acknowledge action was added to the other interactors (e.g., tmt and sin)."
"the value of a variable is modified according to underlying system properties (e.g., temperature of some specific device). the model assumes nothing about the processes that the subsystem manages. therefore, it provides no restriction on the behaviors of variables, which permits an analysis of error behavior."
"at this point, it must be stressed that the goal of the analysis is not to prove the system correct. what the analysis provides is an exhaustive investigation of the model against a collection of properties found relevant of the system under consideration. two sources of properties can be identified. the first is knowledge of the domain. clearly, domain knowledge plays a relevant role in identifying relevant properties that a particular user interface should or should not exhibit. the other regards intrinsic properties of the user interface that are commonly considered as good practice (for example, nielsen's usability heuristics [cit] ). for both cases, ivy provides a collection of property specification patterns to help identify and express relevant properties (see section iv-c)."
"obviously, an additional challenge throughout this work is to come up with metrics that allow formal evaluation of the performance of our system (both in terms of effectiveness and efficiency) with respect to classical web crawling approaches."
3) analyzing results: visual representations of counterexamples where properties fail are offered to provide support for analysis. a tabular representation (see inset in fig. 2 ) uses columns to represent states and lines for actions and attributes. the yellow/lighter background color is used to highlight actions or attributes where values have changed. an alternative statebased representation (see fig. 3 ) is also offered that represents each interactor by a column that shows the states that the interactor goes through. actions are indicated by labels on the arcs between two consecutive states.
"to adapt the behavior of traditional crawlers according to our requirements, we have chosen to extend the traditional architecture of a web crawler in the way depicted in figure 2 . here the page fetching module (see figure 1) is replaced by some more elaborate resource fetching component that is able to retrieve resources that are not just accessible by a simple http get request (but by a succession of such requests, or by a post request, or by the use of an api), or that are individual web objects inside a web page (e.g., a blog post, a comment, a poster's name). an application-aware helper module is then introduced in place of the usual link extraction function, in order to identify the web application that is currently being crawled, and decide and categorize crawling actions that can be performed on this particular web application."
"the software components of tpgs are often updated as appropriate to accommodate new mission requirements, different rocket configurations, and new operating systems releases. the hardware is also occasionally upgraded as a result of equipment obsolescence. an important goal is that despite these updates, operating procedures are changed as little as possible. as main contractors, the space institute have responsibilities for these activities. a goal of the space institute is to provide a set of formally specified and validated critical requirements to serve as a baseline (an oracle) for system acceptance. this goal includes providing a set of user interface requirements for correct mission-safety system operation that do not change as the system evolves through updates and upgrades. it is this role that the reference model is designed to play."
patterns are used in this paper to explore a model of a safety critical system's user interface systematically. the aim is to verify that the model exhibits relevant generic properties related to the modeled system's safety as well as its usability. these properties specify requirements that are recognized by regulators as mitigating relevant risks. this model is capable of being used as a benchmark against which the system and its evolution can be assessed. this process should enable identification of relevant aspects of the system that might impact its safety and therefore require further scrutiny.
"the middle sentence of the manual extract is not directly included in the axiom in fig. 5 . because that sentence is about the red color, it is addressed in a different axiom, dealing with the red color."
"(3) we also must ensure throughout our work the possible fine integration with the crawler(s) by developing mechanism for interacting with the other components. among the challenges here is the fact that the crawler should still be responsible for all web interactions, in order to maintain politeness constraints, whereas, for instance, some crawling actions may require going through an external program (an api crawler, or an oxpath evaluatior)."
"the ev subsystem operation manual describes the ev subsystem as consisting of 23 interdependent panels each featuring output elements exhibiting slightly different behaviors. interdependency is most evident in the alarms and alerts panels, which aggregate information variables that are in an alarm/alert condition from the other panels. previous analyses using the ivy tool have involved multiple panels (see, for example, [cit] ), but they have used a flat model structure, employing a single interactor. this single interactor represents the union of all panel information. while this made it simpler to describe coordination between different elements of the panel, it was only capable of dealing with relatively simple display combinations. more structure was sought to achieve a more general approach. the specified model uses the object-oriented features (in particular the aggregation relation) of the mal interactor language to create a tree-like structure of panels. fig. 4 shows the architecture of the model describing four of the panels. an interactor for each panel is aggregated in the main interactor. each panel interactor aggregates interactors that represent the state, the display behavior, and controls associated with the displayed variables. many instances of variables are needed to represent the information that a panel displays. this structured approach enables better management of the scalability of the model. however, a remaining challenge is expressing coordination between panels."
"but archiving web data from the social web in an intelligent manner is still an ongoing challenge. our aim is to deal with this challenge by introducing a new adaptive approach which relies basically on a knowledge base about the different kind of (social) web applications. a web application is any http-based application that utilizes the web and web browser technologies to publish information. we focus in particular on social aspects of the web, which are heavily based on user-generated content, social interaction, and networking, as can be found for instance in web forums, blogs, or on twitter."
all actions that are used to navigate between panels are specified similarly. to complete the navigation model the initial panel of the system is described. this is done using the initialization axiom:
"we briefly discuss next the state of the art. then we present in section 3 the proposed approach: to introduce an application-aware helper in the crawling process, that assists the crawler throughout the crawling process and will ensure that data is crawled efficiently. in section 4 we describe our methodology in more detail by describing our web application detection patterns, a possible structure for a web application knowledge base, and which kind of crawling actions we want the crawler to perform. we evoke in section 5 preliminary results and end this paper with a discussion of our future research."
"before verification of the model can proceed, confidence that the model serves as the required reference model must be established. confidence can be established by appealing to the process that was used to construct the model. the user operation manual contains a detailed description of the interface and its operation. this information was systematically translated into the model as has already been illustrated. the modelers' understanding of the manual was checked through discussion with a subject matter specialist from the iae. the model was based on our understanding of what the user interface should be. at the end of the process, the model was checked against the manual by a member of the modeling team."
"an additional issue relevant to this exercise was the scalability of the approach. while the paper describes the simpler versions of the models (considering very few variables), the complete study involved adding more detail to the model. building the larger model involved just adding more variables and corresponding axioms and involved no particular complexity. the model with four panels (telemetry, synoptic, alarms, and alerts), with four system variables (two represented through aggregation and two through attributes), plus all the navigation logic, amounts to 490 lines of mal (including comments). more relevantly, the model consists of nine interactors, totaling 168 axioms (of which four are initialization axioms, 52 are permissions, 64 are modal, and the remaining 48 are invariants) producing a state space of approximately 8e 6 reachable states. although exact numbers will depend on the concrete type of variable being added, the size of the model grows roughly linearly with every new variable added (around an extra 50 lines of mal code, with seven to eight new permission and modal axioms, and the number of interactors remaining constant)."
"the knowledge base is to be specified in a declarative language, so as to be easily shared and updated, hopefully maintained by non-programmers, and also possibly automatically learned from examples. the w3c has normalized a web application description language (wadl) [cit] that allows describing resources of http-based application in a machine processable format. wadl is used for describing the set of resources, their relationship with each other, the method that can be applied on each resource, resource representation formats, etc. wadl may be a candidate component and export format, of our knowledge base, but does not satisfy all our needs: description of web application recognition patterns, and web application interactions that go beyond simple get and post requests. consequently, our knowledge-based will be described in custom xml format, well-adapted to the tree structure of the hierarchy of web applications and page levels."
"(2) one significant challenge is to investigate the possible automatic, unsupervised, learning of new web applications (by the inference of common patterns), and the adaptation to slight changes in the templates that render the wrappers unusable."
"properties for verification are specified in computational tree logic (ctl) [cit] . these express assumptions about the expected behavior of the device. the example focuses on a general problem in user interfaces that confusions can occur when modes or other aspects of the system state are not clearly indicated, and as a result, the behavior of actions is unclear [cit] ."
"more generally, a few works [cit] aim at identifying a general category (e.g., blog, academic, personal, etc.) for a given web site, using classifiers based on structural features of web pages that attempt to detect the functionalities of these web sites. this is not directly applicable to our setting, first because the classification is very coarse, and second because these techniques function at the web site level (e.g., based on the home page) and not at the level of individual web pages."
"on the other hand, the user interface of the system has two features that make it challenging from the perspective of formal modeling. it features a number of workstations with relatively complex displays, each involving a number of overlapping but 2168-2291 © 2015 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"models of the system's user interfaces derived from the operator manual contribute to this goal. this is achieved by establishing a set of human interaction sequences with the system along with the critical properties to be verified within each sequence. the purpose behind this strategy is to run tests on a new or updated system that will enable the clients (e.g., subject matter experts) to use the verified set of user interfaces as a basis for accepting the new operation environment."
"these modifications will be implemented in two web crawlers: the proprietary crawler of the internet memory foundation, with whom we are closely collaborating, and into a customized version of heritrix [cit], developed by the athena research lab in the framework of the ar-"
"for the sake of simplicity, the axioms above describe a panel with only two variables. adding more variables amounts to adding more axioms, or adding conditions to the existing ones using the same principle. it should be noted that in the final version of the model, simplification was achieved by combining some of these axioms."
the described work was funded by the european union seventh framework programme (fp7/2007 [cit] ) under grant agreement 270239 (arcomem). we are also grateful to julien masanès (internet memory) for discussions on the topic of this phd.
"the properties hold when the color is green or red (properties 3 and 4), but not when it is yellow (property 2). a counterexample indicates that the button may be yellow already because another variable is being alerted. the button's color, on its own, is not enough to guarantee that feedback is provided when new alerts are raised. in fact even the beep feature does not solve the problem because that feature can be turned off. this is not necessarily a problem, but raises discussion about the level of feedback provided by the system. is immediate feedback about individual variables needed, or is the goal to call attention to alarms and alerts in general? discussion with operators clarified that the goal is to generate a general call to attention. it is clear that properties being proved and results of proofs must always be interpreted in the context of the broader system and its requirements and with a sense of the future versions of the system. for example (also in the light of the results for property 2), consider whether property 3 being true is a positive result. if an alarm is issued, surely the button should remain red. looking more closely, this two-variable model shows that bd1a is the only critical variable in the tmt interactor. this means that the color can only be red when bd1a is in an error state, as demonstrated by the fact that property 5 below fails. checking for models with a larger number of variables indicates the same situation relating to yellow"
"variables can be critical or noncritical, depending on which system property they represent. the telemetry monitoring panel (modeled by interactor tmt ) is a good example because it includes more complex variables that feature additional characteristics including \"working limits,\" description, and measurement units. the working limits specify when alerts or alarms can occur. alerts and alarms must be acknowledged by the operator by selecting the corresponding entry on the alerts or alarms panel. the tmtvariable interactor specifies attributes including value, color, and other display characteristics. changing the variable and its attributes is achieved generically through the setvalue(int) action. this action specifies how the panel reacts to changes to the underlying system properties. axioms defining this action are of a general form, where priming is used to reference the value of an attribute in the state after the action has happened, as follows:"
"where ival1 ranges over the possible colors for the button (yellow, red, and green). this will generate three properties, one for each value of ival1. note that these properties do not have to be written; they are automatically generated by successive instantiation of values of ival1."
"to understand how current archiving tools are not fully up to the task of social web archiving, consider the simplified architecture of a traditional web crawler (such as heritrix [cit] ) depicted in figure 1 . a web crawler (also known as web spider or robot) is a computer program that inspects the web in a methodical manner and retrieves targeted documents. traditional web crawlers crawl the web in a conceptually very simple way. they start from a seed list of the urls to be stored in a queue (e.g., the starting url may be the homepage of a web site). web pages are then fetched from this queue one after the other and links are extracted from these web pages. if they are in the scope of the archiving task, the newly extracted urls are added to the queue. this process ends after a specified time or when no new interesting urls can be found."
"a focused, or goal-directed, crawler, crawls the web according to a predefined set of topics [cit] . this approach is a different way of influencing the crawler behavior, not based on the structure of web applications as is our aim, but on the content of web pages. our approach does not have the same purpose as focused crawling: it aims instead at a better approach for known web applications. both strategies for improving a traditional crawler are thus complementary."
"introduced by new technologies. failures often arise because of mode confusions [cit] that are a consequence of poor feedback. while it is possible to explore these mode complexities and confusions using a range of techniques, successful analysis should be systematic and exhaustive. formal analysis techniques have been developed that provide such analyses. the advantage of these techniques for identifying potential problems is that they are precise, concise, and amenable to automated computeraided analysis. their disadvantage is that, although significant progress has made these tools more effective and usable, the scope of analysis is narrow in comparison with traditional usability analysis techniques. the challenge is to develop tools that can be used effectively by developers."
"the ivy tool (see fig. 2 ) supports the development of models of the interactive device, the formulation of required properties of the device's behavior, and their verification through model checking. when verification fails, the counterexamples 2 the effected operator is part of mal, not ctl."
"a mal interactor is defined by 1) a set of typed attributes that define the interactor's state; 2) a mapping of the state to some presentation medium; 3) a set of actions that define operations on the state; 4) a set of axioms written in mal that define the semantics of the actions in terms of their effect on the state. those parts of the state that are rendered are indicated by decorating the attributes with modality annotations. mal axioms define how an interactor's state changes in response to interactor actions. axioms are of three types: 1) propositional axioms-which define axioms that must always be true of the state of the interactor; 2) modal axioms-these axioms define the effect of an action on the state of the interactor; 3) deontic axioms-these axioms define the conditions under which an action is permitted or obligatory. models are constructed by composing interactors hierarchically. a model can, therefore, be seen as a state machine, where the states are defined by the attribute values and the transitions are labeled by the actions that cause changes to the attributes."
"our main claim is that different crawling techniques should be applied to different types of web applications. this means having different crawling strategies for different forms of social web sites (blogs, wikis, social networks, social bookmarks, microblogs, music networks, web forums, photo networks, video networks, etc.), for specific content management systems (e.g., wordpress, phpbb), and for specific sites (e.g., twitter, facebook). our proposed approach will detect the type of web application (general type, content management system, or site) currently processed by the crawler, and the kind of web pages inside this web application (e.g., a user profile on a social network) and decide on further crawling actions (following a link, using an api, submitting a form, extracting structured content) accordingly."
"web application detection module. one main challenge in intelligent crawling and content extraction is to identify the web application and then perform the best crawling strategy accordingly. there is not much work done on the web application identification, but there are a few efforts for classifying web pages under different categorized web applications [cit] ."
"the tpgs is a ground-based system that has been used to support space mission preparation for more than 15 years as part of the brazilian space launcher program. it is a legacy safety-critical system containing customized hardware and software components. these components are responsible for ground control, testing, and prelaunch preparation of a satellite launch rocket. it consists of six subsystems each providing specific functionality relevant to the rocket's testing and preparation. each subsystem is operated through a dedicated workstation whose user interfaces comprise both a gui-based interface and a physical panel of buttons and actuators. fig. 1 shows the macroarchitecture of the system."
"the paper has described how the ivy workbench was applied to an existing aerospace system. unlike previous work, the model was based solely on a description of the system, making the exercise closer to that of applying the approach during development. the size and complexity of the system also meant that a more structured approach to modeling the different panels in the user interface, and (especially) their coordination had to be developed."
"this approach does not confront the challenges of web application crawling: the nature of the web application crawled is not taken into account to decide the crawling strategy or the content to be stored; web applications with dynamic content (e.g., web forums, blogs, etc.) may be crawled inefficiently; some content may be missed when it is only accessible through complex crawling actions (ajax requests, form submissions)."
"the tmt interactor is, therefore, described as follows (note that since its purpose is to provide the means to monitor variables there are no user actions relating to the variables in this case): the manual states that an access button is red and blinking when at least one critical variable of a given panel is in a nonacknowledged alarm state. these constraints are expressed as an invariant over the interactor as is illustrated by the following invariant that expresses the blinking red condition:"
"as long as no dependences between the panels exist, adding more panels to the model is only a matter of aggregating their corresponding interactors in main. when interdependencies exist, the states and/or actions of the relevant interactors must be coordinated as will be discussed in the next section."
"although alternative approaches have been explored (cf., [cit] ), the composition of interactors is currently strictly hierarchical. the main interactor coordinates dependences between all the other interactors. it ensures that changes to the state of a panel variable are reflected throughout the model, including, for example, the alarm and alert panels. these panels provide a summary view of the variables that are in an alarm or alert condition. furthermore, each interactor representing a panel is responsible for its own internal dependences. this approach favors an incremental style of modeling, in which detail can be gradually added to each interactor as modeling progresses and/or more information becomes available. this happens both in terms of adding additional panels to the model, and in adding detail to each of the panels. it remains to be seen, however, how this centralized approach will scale as more variables and panels are added to the model, not only in terms of expressing the coordination, but also in terms of computational resources during verification."
"care must be taken in interpreting what has been proved. property 6 expresses that in all states, if the helpbtn action is executed, then the help panel will be displayed. this, however, is not the same as saying that the help panel is always reachable using the helpbtn action. in fact, the helpbtn action is not always available."
"still on the topic of web application detection, there are some approaches for detecting deep web sources [cit] . the former effort, named form-focused crawler [cit], was introduced for detecting online databases. this technique uses a focused crawler for a given topic with the assistance of a link classifier which detects searchable forms and prioritizes detected links. this technique, however, has a few limitations: manual tunning, link classifier dependency, and heterogeneous results. the latter approach [cit] introduced a more adaptive approach (an adaptive crawler for hidden-web entities) that addresses these limitations. it efficiently explores the entry points to the hidden web with an unknown pattern, then automatically adding this experience to the learning process."
"our experience has shown that the proposed approach provides a practical and feasible way to systematically specify, and automatically verify, the user interfaces of complex systems. in most of the cases, such verification activities are currently restricted to inspections of documents and test results analysis, where the interpretation of the results can still be quite subjective, and the test scenarios may not cover all the possible combinations of actions that can take place. the support of a computer-aided tool would greatly expedite the accomplishment of this activity. the overall approach described in this paper represents a significant step forward. he is an emeritus professor of informatics and a senior research investigator with newcastle university, newcastle upon tyne, u.k., and a research fellow with queen mary university of london, london, u.k. (funded to work on the analysis of medical devices) and with haslab/inesc tec, braga, portugal (funded to work on the verification of safety critical interactive systems). his research focuses on the systematic analysis of the functional behavior of interactive systems using a combination of model checking and automated theorem proving techniques. these techniques have also been used to model human aspects of ubiquitous systems with the aim of enabling a larger scale analysis of the properties of implicit actions that can take place within a physical environment augmented by technology."
"an alternative approach expresses coordination directly using modal axioms in the definition of actions. this can be done at the level of actions or at the level of attributes. at the level of actions, the coordinating main setvalue action invokes the actions in the coordinated panels: monitsin .setvalue(bd1aid, v ) )."
confidence in the model was further achieved by investigating its behavior. this involved stating that some goal of the system cannot be achieved and checking template properties. this process aims to get counterexamples that can then be analyzed with subject matter specialists to check whether the illustrated behavior is consistent with understanding of the system. examples of how this was done are presented in the next section.
"when two axioms define conflicting behaviors for an action, the model becomes empty and anything can be proved true of its behavior. although the model checker (nusmv) can be set to check for transition relation totality and the absence of deadlocks, the negation of some of the initially proved properties was also checked as a means to prevent any problems at this level."
"each interactor must then have (at least) one action that enables setting the value so that when it changes in one interactor it also changes in the other. the action being used, however, is not defined explicitly. while this mechanism for coordination scales well, it can generate unwanted side effects. two different axioms might, for example, express requirements over attributes that, although not contradictory, mean that no actions exist that achieve both effects. this will create a deadlock in the model (i.e., a situation where no action is possible) whose cause can be hard to diagnose. auxiliary actions must be added to solve the problem which can increase complexity."
"further analysis considered the consistency of the user interface, for example, in the acknowledgment of panels and the reversibility of user actions. patterns were instantiated as described above."
"specification patterns can be used to address the problem of property formulation [cit] . these patterns are helpful, not only to support the process of writing correct properties, but also as prompts for potentially relevant properties that should be verified. in the specific case of interactive systems, patterns capturing relevant usability properties (e.g., feedback) have been proposed [cit] . alternative approaches include the automatic generation of properties from task models [cit] ."
"1) building models: models are developed in the mal interactor language. ivy offers a textual editor that supports syntax highlighting, code completion, undo/redo, and other usual editing facilities. the editor also presents as a side panel a tree view of the model that allows easy navigation of the model's structure."
"coordination of variables' values between panels is expressed in the main interactor. this coordination can either be expressed in terms of invariants or in terms of modal axioms. the use of invariants has already been illustrated when specifying the coordination between the tmt panel and its internal variables. invariants can also be expressed over two or more variables in different panels. to do this, the axioms must be placed in the main interactor. for example, specifying that the variable bd1a must have the same value in the monittmt and monitsin interactors is as follows:"
"(1) using xpath 1.0 expressions for detection patterns faces some expressiveness limitations: in some cases, for instance, regular expressions may be required to identify a web application. we have the option of switching to xpath 2.0 expressions or to add extension functions for this purpose, but we should strive also at keeping a language that is as declarative as possible for optimization purposes."
"to show the update semantics is a refinement of the value semantics, we must exploit the information given by cogent's linear type system. a typical refinement approach to relate the two semantics is to define a correspondence relation between update semantics states and value semantics values, and show that an update semantics evaluation implies a corresponding value semantics evaluation. however, such a statement is not true if aliasing exists, as a destructive update (from, say, put) would result in multiple values being changed in the update semantics but not necessarily in the value semantics. as our type system forbids aliasing of writable references,"
"conceptually, the refinement proof proceeds bottom-up, starting with the leaf functions of the program and ending with the top-level entry points; corres results proved earlier are used to discharge corres assumptions for callees. the corres proof tactic thus follows the call-graph of the input program. currently, the tactic is limited to computing call graphs correctly only for programs containing up to second-order functions. we have not needed any higher orders in our applications, but the tactic can be extended using similar techniques if needed."
"in this section, the proof once again connects deep and shallow embeddings, where the shallow embedding is, this time, a pure function in isabelle/hol. this shallow embedding is still in a-normal form and is produced by the compiler: for each cogent type, the compiler generates a corresponding isabelle/hol type definition, and for each cogent function, a corresponding isabelle/hol constant definition. we drop the linear types and remain in isabelle's simple types, because we have already used the linear types to justify our switch to the value semantics."
"our verification framework achieves this goal through the certifying compiler of cogent: a high-level, pure, polymorphic, functional language with linear types, specifically designed for certain classes of systems code such as file systems. for a given well-typed cogent program, the compiler will produce a high-level shallow embedding of the program's semantics in isabelle/hol [cit], and a proof that connects this shallow embedding to the compiler generated c code: any property proved of the shallow embedding is guaranteed to hold for the generated c."
"cogent's main restrictions are the (purposeful) lack of recursion and iteration and its linear type system. the former ensures totality, which is important for both systems code correctness as well as for a simple shallow representation in higher-order logic. the latter is important for safe memory management and for enabling a transition from an imperative c-style semantics, suitable for code generation, to a functional semantics, suitable for equational reasoning and verification."
"the proof is simple as well. since we can now use equational reasoning with isabelle's powerful rewriter, we just unfold both sides, apply extensionality and the proof is automatic given the right congruence rules and equality theorems for functions lower in the call graph."
"val-rel s v s v ensures that v s and v are of matching types. like the c refinement proof in §4.3, we have fully automated this proof using a specifically designed syntax-directed rule set. fig. 11 depicts the top-level shallow embedding, only mildly polished for presentation, for the cogent example of fig. 2 . as it shows, the isabelle definitions use the same names as the cogent input program and keep the same structure, making it easy for the user to reason about."
"to compare the above classification based on axonal tree morphology to a more common classification 76 based on dendritic tree morphology, we applied an analogous classification approach to the dendritic trees of 77 the same neurons. the resulting f 1 -scores are presented in fig. 1i . the dendritic tree-based classification 78 better detects neurogliaform cells (f 1 -score of 0.83 compared to 0.356-0.547 in other cell types). this result description 1 the number of branches 2 symmetry -mean of the ratio between the number of children in each daughter branch 3 maximum branch order 4 mean branch order 5 difference between maximum and minimum on the x-coordinates (µm) 6 difference between maximum and minimum on the y-coordinates (µm) 7 difference between maximum and minimum on the z-coordinates (µm) 8 sholl analysis -the number of branch intersections at a radius of 100µm from the soma in 3d 9 sholl analysis -the number of branch intersections at a radius of 200µm from the soma in 3d 10 sholl analysis -the number of branch intersections at a radius of 300µm from the soma in 3d 11 the number of branches longer than 200µm 12 the number of branches longer than 300µm 13 the number of branches longer than 400µm 14 maximum path length, the distance from the soma to the farther leaf (µm) 15 minimum path length, the distance from the soma to the closer leaf (µm) 16 mean path length, the distance from the soma to the closer leaf (µm) agrees with the observation that neurogliaform cells are known for their thinness and abundance of radiating 80 dendrites (33) . in fact, of the six cell types, this is the only case in which the dendritic tree-based classification 81 performs better than the axonal tree-based classification (f 1 -score of 0.83 compared to 0.781). interestingly, 82 the dendritic tree-based classification performs poorly on chandelier cells (f 1 -score of 0.356), in contrast to 83 the axonal tree-based classification, that classifies these cells with a very high success rate (f 1 -score of 0.902)."
"the type structure and associated syntax of cogent is presented in fig. 3 . our type system is polymorphic, but we restrict this polymorphism to be rank-1 and predicative, in the style of ml, to permit easy implementation by specialisation with minimal performance penalty."
"we are able to use the existing kind system to handle these safety checks with the inclusion of the e permission, for escapable, which indicates that the type may be safely returned from within a let!. we ensure, via the typing rules of fig. 6, that the left hand side of the binding (ok in the example) has the e permission, which excludes temporarily nonlinear references via bang(·) (see fig. 4 )."
"most rules in the update semantics only differ from the value semantics in that they thread the heap µ through the program evaluation, and so many of those rules have been omitted. however, key differences arise in the treatment of records and of abstract types, which may now be represented as boxed structures, stored on the heap. in particular, note that the rule uput 2 destructively updates the heap, instead of creating a new record value, and the semantics of abstract functions · u may also modify the heap."
"hofmann [cit] proves, in pen and paper, the equivalence of the functional and imperative interpretation of a language with a linear type system. the proof is from a first order functional language to its translation in c, without any pointers or heap allocation. in contrast, cogent is higher order, accommodates heap-allocated data, and its compiler produces a machine checked proof linking a purely functional shallow embedding to its c implementation."
"we presented a framework for dramatically reducing the cost of formal verification of important classes of systems code. it relies on the cogent language, its certifying compiler, their formal definitions and top-level compiler certificate theorem, and the correctness theorems for each compiler stage. cogent targets systems code where data sharing is minimal or can be abstracted, performance and small memory footprint are requirements, and formal verification is the aim."
"the definition states that if the state relation r holds initially, then the monadic computation p m cannot fail and, moreover, for all executions of p m there must exist a corresponding execution under the update semantics of the expression e such that the final states are related by r and val-rel c holds between their results. autocorres proves automatically that:"
"in §4 we define in more detail the relations that formally link the values (and states, when applicable) that these programs evaluate to. steps 3 and 4 are general properties about the language and we therefore prove them manually once and for all. steps 1, 2, 5, and 6 are generated automatically for every program. the proof for step 1 is generated by autocorres. for steps 2 and 5 we define compositional refinement calculi that ease the automation of these proofs."
"the full kinding rules for the types of cogent are given in fig. 4 . basic types such as () or u8, as well as functions, are simply passed by value and do not contain any heap references, so they may be given any kind. kinding for structures and abstract functions is discussed shortly in §3.1.1."
"even in the restricted target domains of cogent, real programs will contain some amount of iteration. this is where cogent's integrated foreign function interface (ffi) comes in: the engineer provides her own verified data types and iterator interfaces in c and uses them seamlessly in cogent, including in formal reasoning. our framework guarantees that the verification of combined ccogent code bases has no room for unsoundness."
"this result allows for a simple kind-checking algorithm, not immediately apparent from the rules. for example, the maximal kind of an unboxed structure with two fields of type τ 1 and τ 2 respectively can be computed by taking the intersection of the computed maximal kinds of τ 1 and τ 2 . this result ensures that this intersection is also a valid kind for τ 1 and τ 2 ."
"on the expression level, the programmer can use let! expressions, in the style of wadler [cit], to temporarily convert variables of linear types to their read-only equivalents, allowing them to be freely shared. in this example, we wish to copy a buffer b 2 onto a buffer b 1 only when b 2 will fit inside b 1 ."
"for any nonlinear record (that is, (1) read-only boxed records, which cannot have linear fields, as well as (2) unboxed records without linear fields) we also allow traditional member syntax e.f for field access. the typing rules for all of these expressions are given in fig. 6."
"to ease implementation, and to eliminate any direct dependency on a heap allocator, we require that all functions be defined on the top-level, disallowing closures. any top-level function can be shared freely, as they cannot capture any local variables, let alone linear ones."
"the top-level theorem is the overall certificate emitted by the compiler. we say a c program correctly implements its cogent shallow embedding if: (i) it terminates with defined execution; and (ii) assuming that the initial c and cogent stores are related and the program inputs are related, then their outputs are related. in other words, the compiler theorem states that a value relation is preserved across evaluation. in §3.3.1, we present a value relation between update and value semantics. at every other refinement stage, we introduce a similar relation between values of the two respective programs. by composing these relations, we get the value relation v between a result v m of a c program p m and a shallow embedding s, that relies on the intermediate update and value semantics results. the relation in §3.3.1 depends on a cogent store µ, which is related to the c state using the state relation r, defined in §4.3."
"lines 3-7 show basic type constructors and declarations of variants, records and tuples using type variables and the primitive type u32. e.g. type cnt is defined as a pair of uarray node and a function type. types in cogent are structural [cit] ], i.e. types with the same structure are equal. moreover, line 16 calls the abstract polymorphic function uarray create(), instantiated with type argument node. the !nd notation temporarily turns a linear object of type node into a read-only one (see §3.2.1). the two basic, non-linear fields to and fr in type node can directly be accessed read-only using projection functions. line 17 and 28 are pat-prim. types"
"the solid arrows on the right-hand side of the figure represent refinement proofs and the arrow labels correspond to the numbers in the following description. the only arrow that is not formally verified is the one crossing from c code into isabelle/hol at the bottom of fig. 1 -this is the c parser [cit] ], which is a mature verification tool used in a number of large-scale verifications [cit] ]. as mentioned, it could be checked by translation validation."
"the refinement proofs state that every behaviour exhibited by the c code can also be exhibited by the cogent code and, furthermore, that the c code is always well-defined, including that e.g. the generated c code never dereferences a null pointer, and never causes signed overflow. it also implies that the generated c code is type-safe and memory-safe, meaning the code will never try to dereference an invalid pointer, or try to dereference two aliasing pointers of incompatible types. in conjunction with the cogent typing proofs, generated by the cogent compiler for the input program, we get additional guarantees that the generated code handles all error cases, is free of memory leaks, and never double-frees a pointer. these points will be formally established in §3 and §4."
"if the monadic code never fails, then the c code is type-and memory-safe, and free of undefined behavior [cit] . we prove non-failure as a side-condition of refinement, basically using cogent's type system to guarantee c memory safety."
"wadler [cit] we use linear types for both of these reasons: we do not require a garbage collector, and we assign to cogent programs an equational, purely functional semantics implemented via mutable state internally. unlike hofmann, our mechanised proof of the correspondence between these two semantics takes heap-allocated data structures into account, using our heap footprint annotation technique ( §3.3.1)."
"user experience we have found that the bilbyfs proofs were significantly easier and less time consuming than direct c proofs; partly because the purely functional shallow embedding enabled a high degree of automation with isabelle/hol, and partly because the linear type system provided free theorems: e.g. operations that do not write to disk represent the disk as a shareable type, which automatically establishes that no writes are performed to it. this illustrates that cogent is practical and usable for writing real-world systems, that the generated c code is efficient, and that reasoning on top of the neat shallow embedding is much simpler than reasoning about c directly."
"theorem 4. given a cogent function f that takes x of type τ as input, let p m be its generated c code, s its shallow embedding, and e its deep embedding. let v m be an argument of p m, and u and v be the update and value semantics arguments, of appropriate type, for f ."
"we only document the core language of cogent in fig. 5 to which the surface syntax is desugared, leaving out the richer surface syntax due to space constraints. fig. 6 shows the typing rules for cogent expressions. many of these are standard for any linear type system. we will discuss here the rules for let!, where we have taken a slightly different approach to established literature, and the rules for the extensions we have made to the type system, such as variants and record types."
"we outline each intermediate theorem, starting with simpl at the bottom. for well-typed cogent programs, we prove: 1 the simpl code produced by the c parser corresponds to a monadic representation of the c code. the proof is generated using an adjusted version of autocorres [cit] . 2 the monadic code terminates and is a refinement of the update semantics of the monomorphic cogent deep embedding [cit] . 3 if a cogent deep embedding evaluates in the update semantics, it evaluates to the same result in the value semantics. we mechanise this consequence of linear type systems, significantly extending known theoretical results to accommodate heap data structures, on a real full-scale language. 4 if a monomorphic cogent deep embedding evaluates in the value semantics then the polymorphic deep embedding evaluates equivalently in the value semantics. 5 if the polymorphic cogent deep embedding evaluates in the value semantics then the cogent shallow embedding evaluates to a corresponding shallow isabelle/hol value. 6 the shallow embedding is (extensionally) equal in isabelle/hol to a neater shallow embedding, more convenient for human reasoning. the neat shallow embedding corresponds to the cogent code before being a-normalised."
"to study signal propagation dynamics, we measured the response to current stimulus pulses injected into the 98 soma at various frequencies along the axonal tree. figure 2 shows an example of simulated neuronal activity 99 along axonal branches of a basket cell. in this example, we used the membrane properties of the 'continuous (fig. 3f ). including the morphology of the 120 dendritic tree as well resulted in an average f 1 -score of 0.94 ( fig 3g) . the classification model results for each 121 neuron are presented in table s1 ."
"to complete the refinement step from the update to the value semantics, the compiler just applies theorem 3. we establish the correctness of the compiler's monomorphisation pass, moving upwards in fig. 1 from a monomorphic to a polymorphic deep embedding in the value semantics."
"imagine writing low-level systems code in a purely functional language and then reasoning about this code equationally and productively in an interactive theorem prover. imagine doing this without the need for a trusted compiler, runtime or garbage collector and letting this code interoperate with native c parts of the system, including your own efficiently implemented and formally verified additional data types and operations."
"note that bitufted cells are better differentiated by the axonal tree, even though their name was coined due to 85 their dendritic tree structure. 86 we next combined axonal and dendritic tree morphology parameters and applied the same classification 87 scheme as before. this resulted in an improved classification performance: the average f 1 -score increased 88 from 0.777 for axonal trees only and 0.488 for dendritic trees only to 0.817 for the two combined ( fig. 1j )."
"cogent is restricted, but it is not specific to the file systems domain. this leads us to believe that our language-based approach for simplifying verification will extend in the near future to other domains, either with cogent directly, or with languages that make different trade-offs suitable for different types of software."
"the state of the art for certifying compilation of functional languages is cakeml [cit], which covers an entire ml dialect. cogent is targeted at a substantially different point in the design space. cakeml includes a verified runtime and garbage collector, while cogent works hard to avoid these so it can be applicable to low-level embedded systems code. cakeml covers full turing-complete ml with complex, stateful semantics, which works well for code written in theorem provers. cogent is a restricted language of total functions with intentionally simple, pure permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org."
"the code for the models and simulations is publicly available on github: table s1 . detailed confusion matrix. type nmo mc bc btc chc dbc ngf martinotti nmo 06140 250 4 0 0 0 0 nmo 36965 194 0 0 56 1 0 nmo 37190 245 2 2 0 3 1 nmo 37290 202 0 38 17 0 0 nmo 37291 256 0 0 0 0 0 nmo 37297 235 0 0 0 0 0 nmo 37301 233 5 0 0 0 0 nmo 37672 269 0 0 0 0 0 nmo 37787 201 3 47 0 0 2 nmo 79457 213 0 0 0 0 0 nmo table s1 : detailed confusion matrix. the results of the classification based on the axonal and dendritic morphology and activity (an elaboration of fig. 3g ). the maximum number in each row is indicated by blue background. 04mean_bo 09sholl200 19total_lng 01brn 24gr2 22max_d 23mean_d 25gr3 26max gr 02symm 07depthz 15min_pl 06depthy 14max_pl 16mean_pl 05depthx 10sholl300 11l200 12l300 13l400 17max_lng 20max_lng_d 18mean_lng 21mean_lng_d nmo_79468 nmo_37625 4 2 0 2 4 martinotti basket bitufted chandelier doublebouquet neurogliaform figure s1 : a heatmap comparing the distributions of morphological parameters. the parameters were transferred to z-score values. the neuromorpho.org ids are indicated for each neuron reconstruction on the right side, and the interneuron subtype is indicated by the 'row color' on the left side. the columns are the 28 morphological parameters organized according to the above dendrogram. the left dendrogram clusters the neurons in an unsupervised manner. figure s3 : comparison between 10 e-types from the bbp. a heatmap showing the similarity between all 10 interneuron e-types under current pulse frequencies of 100hz, 200hz, 300hz, and 400hz in terms of firing pattern, in a basket cell (nmo 06143). in the original experiments conducted by bbp, an elongated current step was induced, resulting in significant differences between these 10 e-types. in our case, the soma was stimulated with strong current pulses, leading to very similar responses in several e-types. hence, we chose to focus on four e-types: cac, bac, bnac, and cnac. the response for stimulus frequency of 300hz. b. 'cnac' (the same as in fig. 3b ), c. 'cac', d. 'bnac', and e. 'bac' e-types. the line color indicates the fraction of spike train that propagates: maroon -1, orange -0.75, deep sky blue -0.66, violet -0.5, navy -0.375, dim gray -0.2, and silver -0. the same neuron as in fig. 2 ."
"we allow linear values to be shared read-only in a limited scope, an idea first explored by wadler [cit] . this is useful for practical programming in a language with linear types, as it makes our types more informative. for example, to write a function to determine the size of a (linear) buffer object, a naive approach would be to write a function:"
"cogent is a pure, total functional language to enable productive equational reasoning in an interactive theorem prover. it is higherorder and polymorphic to increase conciseness. it uses linear types to make memory management bugs compile time errors, and to enable efficient destructive in-place update. it avoids garbage collection and a trusted runtime to reduce footprint. it supports a formally modeled ffi to interoperate with c code and to implement additional data types, iterators and operations. it does all of these with full formal proof of compilation correctness and type-safety in isabelle/hol."
"in order to pattern match on a variant, we provide a case construct that attempts to match against one constructor. if the constructor does not match, it is removed from the type and the reduced type is provided to the else branch. in this way, a traditional multi-way pattern match can be desugared by nesting:"
"the 2 step in fig. 1 is a refinement proof between deeplyembedded cogent and a shallow, monadic embedding of c. to phrase the refinement statement we first define how deeply embedded cogent values and types relate to their corresponding values in the monadic embedding. this value-mapping is captured by a value relation val-rel c, generated in isabelle automatically by the cogent compiler, using ad hoc overloading. we must generate val-rel c separately for each cogent program because the types used in the shallow embedding depend on those used in the deep embedding. for example, c structs are represented directly as isabelle records."
"a variant type c i τ i is a generalised sum type, where each alternative is distinguished by a unique data constructor c i . the order in which the constructors appear in the type is not important. one can create a variant type with a single alternative simply by invoking a constructor, e.g. some 255 might be given the type some u8 . the original value of 255 can be retrieved using the esac construct. the set of alternatives is enlarged by using promote expressions that are automatically inserted by the type-checker of the surface language, which uses subtyping to infer the type of a given variant. a similar trick is used for numeric literals and cast."
"like cogent, hasp's systems language [cit], habit, is a functional language. it has a verified garbage collector [cit] ], but no full formal language semantics yet. ivory [cit] ] is a domain specific systems language embedded in haskell. it generates well-defined, memory safe c code, but unlike cogent it does not prove its correctness."
"recall that cogent may be extended with abstract types, implemented in c, which we write as t τ i m in our formalisation. we allow abstract types to take any number of type parameters τ i, where each specific instance corresponds to a distinct c type. for example, a list abstract type, parameterised by its element type, would correspond to a family of c list types, each one specialised to a particular concrete element type. because the implementations of these types are user supplied, the user is free to specialise implementations based on these type parameters, for example representing an array of boolean values as a bitstring, so long as they can show that every different operation implementation is a refinement of the same user-supplied cdsl semantics for that operation."
"the compiler generates a renaming function r that, for a polymorphic function name f p and types τ, yields the specialised monomorphic function name f m . just as we assume abstract functions are correctly implemented in c, we also assume that their behaviour remains consistent under r."
"for any valid type τ, the kind of bang(τ) will be nonlinear, which means that our size function no longer needs to be encumbered by the extra return value. this kinding result is formally stated as:"
"intuitively, our top-level theorem states that for related input values, all programs in the refinement chain evaluate to related output values. this can of course be used to deduce that there exist intermediate programs through which the c code and its shallow embedding are directly related. the user does not need to care what those intermediate programs are."
"where r is the number of groups (\"species\" according to its original definition), and p i is the normalized 205 number of members in each group. 0 d equals the number of groups, and 1 d converges to the exponent of 206 shanon entropy (equation 3 )."
"if c is so great, why not verify c systems code directly? after all, there is an ever growing list of successes [cit] in this space. the reason is simple: verification of manually written c programs remains expensive. just as high-level languages increase programmer productivity, they should also increase verification productivity. cogent is specifically designed with a verification-friendly high-level semantics. this makes the difference between imperative and functional verification. it is pointer fiddling and undefined behaviour guards in c versus abstract functional objects and equations in cogent. an imperative vcg [cit] for c must overwhelm the prover with detail, while the abstraction and type system of cogent enable the use of far stronger existing automation for high-level proofs."
"as mentioned earlier, we implement parametric polymorphism by specialising code to avoid paying the performance penalties of other approaches such as boxing. this means that polymorphism in our language is restricted to predicative rank-1 quantifiers."
"as we will see in §4.3, proving refinement requires access to the typing judgements for program sub-expressions and not just for the top level, so the cogent compiler instructs isabelle to store all intermediate typing judgements established during type checking. these theorems are stored in a tree structure, isomorphic to the type derivation tree for the cogent program. each node is a typing theorem for a program sub-expression."
"optimisation the current cogent compiler applies few optimisations when generating c code and leaves low-level optimisation to gcc or compcert. involved optimisations in the cogent-to-c stage would complicate our current syntax-directed correspondence approach. cogent-to-cogent optimisations, however, are straightforward. the ease of proving a-normalisation correctness over the shallow embedding via rewriting suggests that this is the right approach in our context. in particular, some of the source-to-source optimisations discussed in chlipala [cit] seem promising for cogent."
"step (2) is embodied in the following refinement theorem, which we prove, once and for all, by rule induction over the value semantics. the specialisation lemma 4 of §3.2.4, is a key ingredient of this proof."
"while autocorres was designed to ease manual reasoning about c, we use it as the foundation for automatically proving correspondence to the cogent input program. one of the main benefits autocorres gives us is a typed memory model. specifically, the state of the autocorres monadic representation contains a set of typed heaps, each of type 32 word ⇒ α, one for each type α used on the heap by the c input program."
"verifying cogent programs our initial case studies [cit] ] demonstrated that writing file systems in cogent makes verification significantly easier when compared to traditional verification methods for c programs, such as in sel4. the verification of two major file system operations never had to deal with proof obligations arising from possible aliasing of pointers, invalidation of memory, or the possibility of undefined behaviour in the program, as cogent statically proves all these obligations. in addition, the isabelle equational simplifier was able to make short work of many proof goals, as the shallow embedding given is just a pure hol function."
"this allows us to specify dynamic objects, such as our value typing relations (see §3.3.1) and our dynamic semantics (see §3.3), in terms of simple monomorphic types, without type variables. thus, in order to evaluate a polymorphic program, each type variable must first be instantiated to a monomorphic type. we show that typing of the instantiated program follows from the typing of the polymorphic program, if the type instantiation used matches the kinds of the type variables."
"our main contribution is the framework for significantly reducing the cost of formal verification for important classes of systems code, using this language-based approach for automatically co-generating code and proofs. specifically, our framework relies on the following technical contributions: a) the cogent language, its formal semantics and its certifying compiler; b) the formal machine-checked proof for switching from imperative update semantics to functional value semantics for a fullfeatured functional language, justified by linear types ( §3"
"semantics that are easy to reason about equationally. cakeml is great for application code; cogent is great for systems code, especially layered systems code with minimal sharing such as the control code of file systems or network protocol stacks. cogent is not designed for systems code with closely-coupled, cross-cutting sharing, such as microkernels."
"cogent as a systems programming language running iozone [iozone] microbenchmarks, we previously demonstrated that the ext2 implementation in cogent exhibits a modest improvement compared to the c version in random-and sequentialwrites throughput, with similar cpu usage at around 10% [cit] ]. for our own file system bilbyfs, the cogent version performs slightly worse than the c version, with 10-20% less throughput and cpu usage of 20% compared to 10%. in the absence of any i/o disturbance, the cogent implementation of ext2 is also slightly slower than native ext2fs when performing random writes to a ram disk. these slight performance overheads are due to the fact that the cogent implementations tend to use data structures that strongly resemble those of the original c, rather than more idiomatic functional data structures, which are handled better by our compiler. for some operations, the c implementation would rely on unsafe features for performance, where the cogent implementation uses slower, but easier to verify techniques. one way to improve this on a language level would be to include specialised constructs for these operations in the language, allowing for the generation of efficient, yet automatically verified code. including support for such constructs is at the top of the priority list for future work on cogent. these case studies are invaluable for identifying common patterns and guide this important next step. occasionally, some overheads are introduced in c code generation, as we rely on the c compiler for low-level optimisation. generated code displays patterns which are uncommon in handwritten code and therefore might not be picked up by the c optimiser, even if they are trivial to optimise. for example, the generated code is already quite close to ssa form used by gcc internally for optimisation, however gcc does not recognize this and optimise accordingly. generating a compiler's ssa representation directly, such as that of llvm [llvm], may eliminate these problems, however this would imply significant changes to our verification tool chain."
"expd ( children, e x t 2 _ f r e e _ b r a n c h _ c l e a n u p ) ) the integration of foreign functions is seamless on the cogent side. it naturally puts requirements on the c code: it must respect the invariants of cogent's linear type system, terminate, and implement the user-supplied semantics that appear in the corresponding shallow embedding of the cogent program in isabelle/hol. ideally the user provides a proof to discharge the corresponding assumption of the compiler certificate. abstract functions can be higher-order and provide the iteration constructs that are intentionally left out from core cogent. e.g. line 20, uarray map no break() implements a map iterator for arrays. we found that for both of our file system implementations, a small common library of iterators for types like arrays and linux's existing red-black tree implementation was sufficient."
"our aim is to significantly reduce the cost of verifying real-world systems software with minimal performance impact. in this section, we evaluate the performance and usability of cogent in two realistic file system implementations, as well as discuss ways in which cogent can be improved in the future."
"linear types have been used in several general purpose imperative languages such as vault [cit] and rust [rust] . paclang [cit] ] uses linear types to guide optimisation of packet processing on network processors. similar substructural type systems, namely uniqueness types, have been integrated into the functional language clean [cit] . however, the intent is only to provide an abstraction over effects, and thus clean still depends on run-time garbage collection."
"to address this problem, we include a type operator bang(·), in the style of wadler's ! operator, which changes all writable modes in a type to read-only ones. the full definition of bang(·) is in fig. 4 . we can therefore write the type of our function as:"
"framing implies that our correspondence relation, for both values and environments, is unaffected by updates to parts of the heap not mentioned in the heap footprint:"
"as discussed in §1, [cit] prove the correctness of the high-level language compiler cakeml. as it depends on run-time garbage collection, it is not suitable for our systems target. furthemore, as it is an unrestricted, turing-complete language with mutable state, its high-level semantics are not amenable to equational reasoning. by contrast, [cit] focus on a compositional approach to compiler verification for a relatively simple functional language, pilsner, to an idealised assembly language. like us, charguéraud [2010 charguéraud [, 2011 generates shallow embeddings to facilitate mechanical proofs, but unlike us they do not prove compilation correctness. [cit] formally show in coq [bertot and castéran 2004] full crash-resilience of fscq. fscq is smaller than ext2 and bilbyfs, and an order of magnitude slower than asynchronous ext4. its implementation relies on generating haskell code from coq, and executing that code with a full haskell runtime in userspace. we focus on bridging high-level specification and lowlevel implementation, on efficiency, and on providing a small trusted computing base, while [cit] assume all these are given and focus on crash resilience. the approaches are complementary, i.e. it would be straightforward to implement crash hoare logic on top of isabelle cogent specifications, enabling a verification in the style of fscq."
"the compilation target is c, because c is the language in which most existing systems code is written, and because with the advent of tools like compcert [cit] [leroy, 2009 1 and gcc translation validation [cit] ], large subsets of c now have a formalised semantics and an existing formal verification infrastructure."
"we are now able to prove refinement between the value and the update semantics. we first prove that the correspondence relation is preserved when both semantics evaluate from corresponding environments. by erasing one semantics, this becomes a type preservation theorem for the other. full details of the proof are available in our isabelle/hol formalisation. to prove refinement, we must show that every evaluation on the concrete update semantics has a corresponding evaluation in the abstract value semantics. while theorem 1 gets us most of the way there, we still need to prove that the value semantics can evaluate whenever the update semantics does. composing this lemma and theorem 1, we can now easily prove our desired refinement statement."
"we write an isabelle function to simulate the compiler monomorphisation phase, and prove that (1) the monomorphised program the isabelle function produced is identical to that produced by the compiler, and (2) the monomorphised program is a correct refinement of the polymorphic one. we define two isabelle functions parameterised by r: m e for monomorphising expressions and m v for monomorphising (function) values."
"(overbar indicates lists, i.e. zero or more) 3), accessing and binding the mbuf field of nd to the name mbuf (punning as in haskell), as well as binding the rest of the record to the name nd t. the linear type system tracks that the field mbuf is logically absent in nd t. it also tracks that nd on line 18 was used, so cannot be accessed again. thus the programmer is safe to bind a new object to the same name nd (on line 25) without worrying about name shadowing. line 25 shows surface syntax for put, the dual to take, which re-establishes the mbuf fields in the example."
"to integrate this type operator with parametric polymorphism, we model our solution on odersky's observer types [cit], and tag type variables that have been made read only, using the syntax α!. whenever a variable α is instantiated to some concrete type τ, we also replace α! with bang(τ). the lemma above ensures that our kinding rule for such tagged variables is sound, and enables us to prove the following:"
"effort and size cogent has been under development for over two years and has continually evolved as we have scaled it to ever larger applications. all up, the combined language development and certifying compiler took five person years. engineering the cogent compiler, excluding 33.5 person months (pm) spent on proof automation and proof framework development, consumed ten pm. the remaining 18 pm were for the design, formalisation and proof of cogent and its properties (e.g. the theorems of §3). the total size of the development in isabelle is 16, 423 loc, which includes the once-and-for-all language proofs plus automated proof tactics to perform the translation validation steps, given appropriate hints from the cogent compiler. the cogent compiler, written in haskell, is 11, 456 sloc. for 9,447 lines of ext2 cogent code the compiler generates 119,167 lines of isabelle/hol proofs and embeddings."
"for iris recognition we used four different types of models like ica, pca, daugman's rubber sheet model & hybrid model. after comparison of these algorithms we may come to know that which better one is. in this system we are going to observe two output one from rfid & other from one of iris recognition system. if both outputs are high then only it will give that person is signal conditioning unit authenticated & for other than these outputs it will give that person is unauthenticated which makes system more secure. these both outputs we are going to apply to arm processor to take faster action depending on rfid & iris recognition system. we are mainly refered the following paper to implement our system1)"
"the application of programmable logic controllers (plc) in control systems of complex production processes makes rigorous correctness requirements for plc programs. any program error is considered to be unacceptable. at the same time, plc programming is an applied region in which the existing deal of groundwork in the field of formal modeling and analysis methods of program systems can have success ful application."
"(vii) the output controller signal to the lift motor \"motion mode up\" is displayed if the lift cabin is arranged at the basement floor and the motor is switched off. this signal is eliminated, i.e., the inverse sig nal \"motion mode down\" is displayed, if the motor is switched off and either the lift cabin is arranged at the second floor or (1) the cabin is at the first floor, (2) the command of motion down from the first floor is accepted, (3) there is no motion command to the second floor from the basement floor, and (4) either there is no call to the second floor or the command to move down from the second floor to the basement floor is accepted."
"the linear time temporal logic (ltl) language is considered as the specification language of the behavioristic properties of the temporal model. the selection of the ltl logic is associated with the fact that the plc program is a classical reactive (reacting) control system, which, being once started, should have correct infinite behavior. it is convenient to specify correctness conditions in the form of templates of properties, which should correspond to correct program executions. in the ltl temporal logics, each formula is such a template in essence."
where firingcond and firingcond' are represented by sets of contacts that are connected with each other in series and/or in parallel. we note that the behavior of the found ld program by the construction will completely satisfy the formulas of the ltl specification.
"previously [cit], an approach to constructing and verifying \"discrete\" plc programs ensuring the possibility of their correctness analysis using the model checking method [cit] or programming and verify ing by the ltl specification was proposed. in this approach, the ltl language of temporal logic is used as the specification language of the program behavior. the correctness analysis of the ltl specification is performed using the cadence smv symbol checking program mean [cit] ."
"in this article, we describe the construction and verification technology of plc ld programs based on the approach to programming and verifying by the ltl specification. the technology is demonstrated by the example of a plc ld program for a library lift control."
"for edge detection we will use sobel operator in which gradient is calculated. the gradient is simply the derivative of the local image values. an edge in the original image would correspond to a higher value in the gradient image. using a gradient image for the hough transform decreases computation time significantly since only points that correspond to actual edges are used in the computation. the sobel operator is often used to compute the gradient image as well. it has distinct advantages, although it is slightly more complex. it is less sensitive to isolated high intensity point variations because it averages points over a larger area. there are two kernels which are convolved with the image: one for each direction. the kernels are applied separately to combined to produce an absolute value [cit] table 1-g x table 2 -g y"
it makes use of filters for image processing and extract iris features from the filtered images. daugman's used gabor wavelets to extract texture phase structure information of the iris to compute a 256-byte binary iris code. the hamming distance measure was used to compare the difference between a pair of iris representations.
"as the cabin enters the specified floor, the lamp of this floor \"floor 2,\" \"floor 1,\" or \"floor 0\" is switched on. the lift cabin does not have its own door and only the shaft doors are there, which are opened and closed manually. sensors \"s2,\" \"s1,\" and \"s0\" serve to determine the position of the doors. the door sensor gives a signal if the door is closed. when the door is opened, the signal is eliminated. the signal from the door sensor is determined visually using the lamp of this sensor."
"in this paper, we present a novel approach for interactive rendering of large terrain datasets, which is designed to prevent limitations of previous algorithms. our approach subdivides the terrain into rectangular patches at different resolutions, and we present efficient algorithms for partitioning meshes into triangle strips, which are stitched together by some strips and commendably resolved the gaps between tiles. by combining carefully designed data structures with the use of gpu based programs, out-of-core; multi-thread and view-dependent rendering technologies are used. we evaluate our technique by applying it to dynamic and real data sets."
"a problem is in writing the program for the plc with ten inputs and 14 outputs, which are intended to control the lift. the interface of the plc for the library lift control is presented in fig. 1 ."
"gabor filters are able to provide optimum conjoint representation of a signal in space and spatial frequency. a gabor filter is constructed by modulating a sine/cosine wave with a gaussian. this is able to provide the optimum conjoint localization in both space and frequency, since a sine wave is perfectly localized in frequency, but not localized in space."
"the memory to be used for data exchange is tuned by an adaptive parameter, which represents the percentage of available memory that can be used after the non adaptive part of the environment is fetched. knowing the amount of memory to be used, the square area is maintained by tracking the viewpoint by fetching/removing tiles to/from memory. gpu based acceleration. during runtime, the quad-tree of tiles is partially held and maintained in main memory, dependent on the movements of the viewer. in each update, tiles inside the view frustum are loaded and gpu based pipeline transform them to the frame buffer, as shown in figure 4 . the rendering pipeline is mapped onto current graphics acceleration hardware such that the input to gpu is in the form of vertices [cit] . these vertices then undergo transformation and per-vertex lighting. at this point in modern gpu pipelines a custom vertex shader program can be used to manipulate the 3d vertices prior to rasterization. once transformed and lit, the vertices undergo clipping and rasterization resulting in fragments. a second custom shader program can then be run on each fragment before the final pixel values are output to the frame buffer for display. the graphics pipeline is well suited to the rendering process because it allows the gpu to function as a stream processor since all vertices and fragments can be thought of as independent [cit] . this allows all stages of the pipeline to be used simultaneously for different vertices or fragments as they work their way through the pipe. in addition to pipelining vertices and fragments, their independence allows graphics processors to use parallel processing units to process multiple vertices or fragments in a single stage of the pipeline at the same time [cit] . recent generations of gpu have allowed the design of algorithms that have a substantial part of their workload performed by the gpu. to reduce memory and bandwidth requirements, data compression employed, which can be decoded directly on the gpu."
the model checking problem consists in determining the performability of the property expressed by the temporal logics formula for a finite program model (in the form of the kripke structure).
"the library lift is controlled using the plc, which receives input signals from sensors and buttons and gives output signals to the lift motor and lamps (fig. 1) ."
"(v) the call command of the cabin to the first floor is accepted, and the lamp \"call 1\" is switched on if the button \"call 1\" is pressed. the command is considered to be fulfilled if the cabin is arranged at the first floor with the lift motor switched off. the call command of the cabin to the second floor is fulfilled similarly."
"since all the variables in the ld language belong to the binary data type, in order to describe the situ ations that lead to an increase in the binary variable v, it is proposed to use the ltl formula of the form (1) which means that, any time when a new value of the variable v turns out larger than its previous value writ ten in the variable _v, it follows from this that the condition of the corresponding external effect firingcond was fulfilled."
"the cadence smv verifier makes it possible to check program models containing up to 59 binary vari ables (variables of other types are represented by the sets of binary variables). substitution variables are not involved into this number; i.e., only register variables and function variables are counted."
"the next stage will be to normalize the iris region in order to counteract imaging inconsistencies such as pupil dilation. an implementation of daugman's polar representation will be used for this purpose, as this is the most documented method for iris normalization. once a normalized iris pattern has been obtained; histogram is obtained in order to extract features."
for pattern matching we have used the same concept that is used in ocr for character recognition i.e we will match the two bits if match is found some score value is added and that goes on increasing accordingly and if not found we start with the negative value of the starting score value. on the basis of threshold score decision of valid iris is made and authentication is done.
"we consider the cadence smv verifier as the program tool for the analysis correctness by the model checking method [cit] . after forming the specification, it is proposed to construct the kripke model in the smv language with the subsequent checking of the performability of the general program properties for this model. if a certain general program property is not fulfilled for the model, then the verifier constructs an example of an incorrect path in the kripke model, by which corrections are introduced into the spec ification. when all the program properties are checked with a positive result, the plc ld program is con structed by the specification."
"(vi) the dispatch command of the cabin from the first floor to the basement floor is accepted and the first floor lamp \"down 1\" is switched on if the lift motor is switched off, the cabin is arranged at the first floor, and either the button \"down 1\" is pressed or the cabin is arranged at the first floor with closed doors for more than ten seconds. the command is considered to be fulfilled if the cabin is arranged at the base ment floor with the lift motor switched off. the command of the cabin dispatch from the second floor to the basement floor is accepted and fulfilled similarly."
"modulation of the sine with a gaussian provides localization in space, though with loss of localization in frequency. decomposition of a signal is accomplished using a quadrature pair of gabor filters, with a real part specified by a cosine modulated by a gaussian, and an imaginary part specified by a sine modulated by a gaussian. the real and imaginary filters are also known as the even symmetric and odd symmetric components respectively. [cit]"
logical (with application of arithmetic operators and comparison operators) expressions over the vari ables of the plc program will be considered as elementary statements of the model.
"this approaches, such as independent component analysis (ica) and principal component analysis (pca), attempt to use classical statistical approaches to extract iris features. the pca is superior in image construction, because it can control construction errors by selecting the cumulative variance. euclidean-distance and nearest-neighborhood (nn) classifier are adopted in these approaches."
"in this case, the variable _v should also be determined in the description section of the variables with the same initialization as for the variable v."
"the floor sensor \"fs,\" which is arranged on the lift cabin, is used to determine the position of the cabin in the shaft. the floor sensor gives a signal only when the cabin is completely arranged on one or another floor reacting on a special metallic plate. oppositely, the signal is eliminated."
"the rubber sheet model remaps each point within the iris region to a pair of polar coordinates. the homogenous rubber sheet model accounts for pupil dilation, imaging distance and non-concentric pupil displacement, it does not compensate for rotational inconsistencies."
"the symbol of leading underlying \"_\" in the notation of the variable _v is convenient to be interpreted as a pseudooperator, which makes it possible to refer to the value of the variable v that it had in the previ ous state. in this case, the pseudooperator can be used only under the effect of the temporal operator x. the condition firingcond is a logical expression over program variables and constants, which are con structed applying comparison operators, logical and arithmetic operators, and the pseudooperator \"_\" (which can be applied only to variables). the expression firingcond describes the situations at which the necessity of varying the value of the variable v appears."
"when constructing the specification, it is important to take into account the order of the arrangement of the temporal formulas that describe the behavior of the variables. a certain variable without the pseu dooperator \"_\" can be used in the behavioristic specification of another variable only if its behavioristic variable is already executed and is situated above in the text."
"for the behavioristic specification of the function variable v, we have a simple circuit of the form each program variable should be determined in the section (local or global) of the description of the variables and initialized according to the specification. for example, we note that all the variables in the medium of the codesys design [cit] are initialized by zero on default."
"it uses local variations, which are characterized by the appearance and disappearance of an important image structure. a bank of spatial filters is constructed for efficient feature extraction and for matching process."
the major goal of this system is to derive a code of iris feature scanned that would improve the recognition accuracy of an individual. it also outlines the current state of iris recognition technology. it begins with a brief overview of iris production as a means of explaining feature extraction.
"(iv) the command \"up to floor 1\" is accepted and the lamp of the basement floor \"up 1\" is switched on if the lift cabin is arranged on the basement floor and the \"up 1\" button is pressed on this floor. the command is considered to be fulfilled and the corresponding lamp is switched off if the cabin is arranged on the first floor with the lift motor being switched off. the command \"up to floor 2\" is fulfilled similarly."
"(iii) the lamps \"floor 2,\" \"floor 1,\" and \"floor 0\" should be switched on when and only when the lift motor is switched off and the lift cabin is arranged exactly at the second, first, and basement floors, respectively."
the condition of varying the variable in only one place of the program facilitates debugging and gives the possibility of simple evaluation of the degree of readiness and the volume of the program text. the stage of writing the program code is finished immediately as the specification is formed for each output and auxiliary variable. we note that the amount and sense of the output variables are determined by the plc interface starting from the problem statement.
"the global program variables are determined by the plc interface. in addition to the necessity to implement the algorithm solving the problem, the introduction of auxiliary variables is in many aspects dictated by the necessity to express the general program properties following from the problem statement using the ltl language of temporal logic. in this section, we consider a series of general program proper ties for the \"library lift\" problem as an example; these series are then specified in the form of ltl formu las with the support on auxiliary variables introduced by necessity."
the kripke structure satisfies the formula (property) ϕ of the ltl logics if ϕ is executed for all paths coming from the initial state s 0 .
"since tiles are loaded and rendered independently, the joints of vertex and texture between neighbouring tiles are key problems for large scale terrain roaming. connection of meshes. normalized meshes that represent the regular tiles and their stitching strips are generated at runtime. the patch hierarchy is used to guide the selection of the various levels of detail based on view-parameters. in each frame the patch hierarchy is traversed in a top-down manner to select a set of active tiles that form an appropriate level of detail. since lod representation of terrain meshes, different resolution of sample will occur between adjacent tiles, named t-connection, as shown in figure 5 . to link all the tiles together, we render a mesh skirt down the surface around every tile. there are two methods to eliminate cracks caused by t-connection. filling method: that inserts new points in the low resolution tiles, and sets the value equal to corresponding points in the high resolution tiles. then add new triangles in the mesh skirt, as shown in figure 5 (c) . omitting method, that ignores redundant points in the high resolution tiles, and then reconstructs triangles between neighbouring terrain meshes, as shown in figure 5 (d) . we introduce the latter, because the resolution of mesh grid change acutely at the boundary of tiles, reduction of triangles will not reduce the quality; nay, it actually relieves the grid changes."
"the library lift, the diagram of which is presented in fig. 1, is intended for lifting books on demand (the filled demand form) from the storage on the basement floor to the first and second library stories and the return of books to the basement floor. the call of the lift cabin to the upper floors and the cabin dispatch from the upper floors to the basement floor are performed by pressing the buttons \"call 2,\" \"call 1,\" \"down 2,\" and \"down 1,\" respectively. the dispatch of the cabin to the upper floors from the basement floor is performed by pressing the buttons \"up 1\" and \"up 2.\" if the command corresponding to the 0 pressed button is accepted, the lamp of this button is switched on; it is switched off after the command is fulfilled."
"these properties are the explicit general program properties in the case with the library lift, which express the obligatory character of the fulfillment of the commands call/dispatch to the floors. before we consider the examples of general program properties, let us construct the constructive specification of the program for the library lift control by the problem description according to the concept of programming and verifying by the ltl specification."
"we note that the approach to programming by the specification, which by essence describes the cause of varying each program variable, looks natural and justified since the plc output signal is the control one, while the change of this control signal usually contains the additional sensual load. for example, it is important to understand why the motor should be switched on/switched off while a certain lamp should be switched on/switched off. therefore, it seems to be evident that each variable should be accompanied by two properties, one for each direction of changes. it is assumed that, if the changing conditions are not fulfilled, then the variable conserves its previous value."
"the edge and the image we have a mixture of black, gray and white values. in order to maintain the feasibility we will consider one threshold value. all the values above that threshold will be considered while and all those below as black. thus a pure black and white image is obtained."
"step 9: match/non match decision is obtained using multiclassifiers. ica is important to blind signal separation and has many practical applications. it is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent."
"the plc is a \"reacting\" system that has a multitude of inputs connected to the control object by means of sensors and a multitude of outputs connected to actuators [cit] . the plc program is performed in a working cycle: reading the inputs, the fulfillment of the program, and the alignment of the outputs."
let us consider the method of construction of ld diagrams by the constructive ltl behavioristic spec ification of program variables. the schematic of the translation of ltl formulas into ld diagrams is the following. the following ld circuits are accepted as corresponding to two temporal formulas for the vari able v (setting the signal and removing the signal):
"the transform is computed by taking the gradient of the original image (in this case, the sobel gradient) and accumulating each non-zero point from the gradient image into every point that is one radius distance away from it. that way, edge points that lie along the outline of a circle of the given radius all contribute to the transform at the center of the circle, and so peaks in the transformed image correspond to the centers of circular features of the given size in the original image. once a peak is detected and a circle 'found' at a particular point, nearby points (within one-half of the original radius) are excluded as possible circle centers to avoid detecting the same circular feature repeatedly."
"we note that the model differs from the starting plc program only by the discrete representation of the operation of timers [cit] . if timers are not used in the program, then the model behavior complete coin cides with the program behavior."
"the kripke model for the plc program is constructed in a natural way. the model state is the state of the program (the vector of the values of all the variables) of the plc after one complete passage of the working cycle. the initial model state is the program state after initializing the values of the variables. thus, the complete execution of the program for one passage of the plc working cycle is considered as the transition from one state of the model into another one (or the same)."
"4. \"switching off the motor.\" each time when the lift motor is switched on, it will be obligatory early or late switched off; i.e., the motor cannot operate infinitely long: g(mtr ⇒ f(¬mtr))."
(ix) the lift motor is stopped if either (1) one of the doors is opened or (2) the cabin in the motion down mode is arranged exactly at the basement floor or (3) the cabin in the motion up mode is arranged exactly at the second floor or (4) the cabin is arranged exactly at the first floor and either the call to the first floor is present but there is no command to move down to the basement floor or there is the command to move up from the basement floor to the first floor.
"5. \"command fulfillment.\" if we consider only those program fulfillments at which the shaft doors will open/close only a finite number of times after the arriving of certain call/dispatch commands to the floor cmd before its fulfillment, then, with arriving of the command cmd, it will be early or late fulfilled, where cmd is flr1, flr2, up01, up02, dwn1, or dwn2: g(cmd ⇒ ¬g(cmd u ¬ds) ⇒ g(cmd ⇒ f(¬cmd)). further, the smv program model is constructed by the constructive specification (see the next sec tion), for which the validity of the general program properties is verified. after verifying the properties, the construction (also by the constructive ltl specification) of the ld plc program is performed, which is presented in figs. 2, 3, and 4."
"normalization is used to transform the iris region so that it has fixed dimensions in order to allow comparisons. the normalization process produce iris regions, which have the same constant dimensions, so that two photographs of the same iris under different conditions will have characteristic features at the same spatial location."
"wavelets can be used to decompose the data in the iris region into components that appear at different resolutions. wavelets have the advantage over traditional fourier transform in that the frequency data is localized, allowing features which occur at the same position and resolution to be matched up. a number of wavelet filters, also called a bank of wavelets, is applied to the 2d iris region, one for each resolution with each wavelet a scaled version of some basis function. the output of applying the wavelets is then encoded in order to provide a compact and discriminating representation of the iris pattern."
"according to the concept of constructing and verifying the plc programs by the ltl specification [cit], the sense of which consists in provision of the possibility for the correctness analysis of plc programs by the model checking method, the value of each variable should vary only in one place of the program and no more than once for one complete program execution when passing the plc working cycle. therefore, the variation in the value of each program variable is described using a pair of ltl formulas. the first ltl formula describes the situation in which the corresponding variable increases, and the second ltl for mula specifies the values that lead to a decrease in the variable value. the ltl formulas considered to specify the behavior of the variables are constructive in that sense that the plc program, which corre sponds to temporal properties expressed by these properties, is constructed by them. thus, plc program ming is reduced to the construction of the ltl specification of the behavior of each program variable. in addition, the smv model, which is then checked for correctness (relative to the general program ltl properties) by the cadence smv symbolic model checking tool, is constructed by the ltl specification."
"since an individual iris region contains features with high degrees of freedom, each iris region will produce a bit-pattern which is independent to that produced by another iris, on the other hand, two iris codes produced from the same iris will be highly correlated."
"even though the homogenous rubber sheet model accounts for pupil dilation, imaging distance and non-concentric pupil displacement, it does not compensate for rotational inconsistencies. in the daugman system, rotation is accounted for during matching by shifting the iris templates in the è direction until two iris templates are aligned."
"the different biometric authentication system available in present like fingerprint, face, and thumb are not highly secured & in that duplication is easily possible. that why to increase the security level we have planned to develop system which is combination of iris recognition unit and rfid unit. a variety of iris recognition approaches were proposed that can be broadly classified in three categories depending on the method used to extract features from the texture. these categories are: (1) texture-based, (2) appearance-based and (3) feature based extraction."
"in this we have to consider a threshold value which is used to generate bit pattern. individual pixels in a grayscale image are marked as \"object\" pixels if their value is greater than some threshold value (assuming an object to be brighter than the background) and as \"background\" pixels otherwise. typically, an object pixel is given a value of \"1\" while a background pixel is given a value of \"0.\""
"linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ica is a special case of noisy ica. nonlinear ica should be considered as a separate case."
"in this paper, we have presented a seamless rendering method for large scale height fields. we have demonstrated the effectiveness of our approach by comparing it to previous terrain simplification and rendering approaches. to future improve the scalability of the proposed terrain rendering pipeline we will investigate the possibility to use differently sized tiles in this approach. in this way we can adapt more flexibly to rendering load that is imposed by regions with different structures."
"the success of segmentation depends on the imaging quality of eye images. persons with darkly pigmented irises will present very low contrast between the pupil and iris region if imaged under natural light, making segmentation more difficult. the segmentation stage is critical to the success of an iris recognition system, since data that is falsely represented as iris pattern data will corrupt the biometric templates generated, resulting in poor recognition rates."
the program specification is divided into two parts: (i) the behavioristic specification of all the program variables (excluding the inputs) and (ii) the specification of the general program properties. the second specification component affects the number and sense of the internal auxiliary variables of the plc pro gram.
(viii) the lift motor is switched on in the established mode if the doors at all floors are closed and either (1) the cabin is not arranged exactly at the floor and there is at least one still not fulfilled command or (2) the cabin is arranged exactly at the basement floor and motion commands up from the basement floor arrived or (3) the cabin is arranged exactly at the basement floor with a closed door for more than ten sec onds and calls from upper floors arrived or (4) the cabin is arranged exactly at the first floor and the com mand to move down from the first floor to the basement floor arrived or (5) the cabin is arranged exactly at the second floor and the command to move down from the second floor to the basement floor arrived.
"based on above two data sets, we give a detail comparison on rendering performance under the same conditions with roam algorithm [cit] . we sample the instant fps every second, with the same reference coordinates, the same roaming track, and the same moving speed. figure 8 gives the fps changing graph of two methods with two data sets respectively. for our method, the average frame rate is 46.75 fps with the first data set, and 44.57 fps with the second data set; for roam algorithm, the average frame rate is 41.14 fps with the first data set, and 39.44 fps with the second data set. by a detailed analysis, we can see that roam takes a lot of time when processing vertex queue and mesh simplification, which reduces the rendering performance; contrarily, our method use the gpu for update and rendering step, which improve the output performance greatly. in figure 8 we can see that our method shows a good capability with large scale terrain, especially for real-time applications. the frame rates are between 50 to 110 fps, the nadir of frame rate is larger than 30 fps even though large data exchange occurs. results from our analyses suggest that our method largely reduce the triangles number at every frame, and frame rates reached the real-time requirement of our travels. although storage of the two data sets has biggish contrast, the changes of frame rate are unnoticeable. that is to say, our method is not sensitive to the data scale."
(ii) the controller outputs for the signalization of the lamps of the position of the doors should pass input signals of the corresponding door sensors.
"the eyelid detection system is used to isolate most occluding eyelid regions. the iris region can be approximated by two circles, one for the iris/sclera boundary and another, interior to the first, for the iris/pupil boundary. the eyelids and eyelashes normally occlude the upper and lower parts of the iris region. also, specular reflections can occur within the iris region corrupting the iris pattern. a technique is required to isolate and exclude these artifacts as well as locating the circular iris region."
"the homogenous rubber sheet model devised by daugman [cit] remaps each point within the iris region to a pair of polar coordinates (r, è) where r is on the interval where, i(x, y) is the iris region image, (x, y) are the original cartesian coordinates, (r,è) are the corresponding normalized polar coordinates, and are the coordinates of the pupil and iris boundaries along the è direction. the rubber sheet model takes into account pupil dilation and size inconsistencies in order to produce a normalized representation with constant dimensions. in this way the iris region is modeled as a flexible rubber sheet anchored at the iris boundary with the pupil centre as the reference point."
"to close this subsection we want to remind the reader that this kind of application is not actually of interest in our research but were included to show the inadequacy of such models for saturated networks. for the rest of the paper, we do not consider the conclusions of this subsection, since this workload forces the network to operate in a state that is not likely to occur in our set-up with our target applications."
"in this paper we propose a collection of models which improve accuracy, comply with cotson restrictions and are lightweight enough to perform fast simulation. these models rely on the idea of reserving resources for the period of time that they are in use, allowing contention to be modelled. we perform an exhaustive evaluation using workloads of different nature: synthetic traffic from independent sources, traces from cache coherence, transactional memory and message passing applications and cache-coherency like synthetic traffic to simulate a 1024-core noc. the wide variety of evaluation scenarios provide insights into the strengths and weaknesses of the different models."
"the use of independent traffic sources is a typical evaluation tool which allows to extract some raw characteristics of a network [cit] . in this study we use it with a different perspective, though. since independent traffic sources allow to adjust network load at will, we can use them to show the behaviour of each model with a wide range of communication needs. we model the traffic sources as independent entities injecting packets randomly along time following an exponential distribution. spatially, packets are distributed uniformly through the network."
"the simplest models and, as discussed before, the most prominent in the literature, do not consider network contention. for instance, there are some models which provide a constant latency for all network accesses regardless of the packet size, the number of hops and so on (e.g. vanilla simics [cit] ). it is clear that disregarding any knowledge about the network makes this model very inaccurate; it can be used to test functionality, but should not be used to evaluate the performance of a large system. in our experiments this model has been denoted as 'fixed' and considers all communications to require 16 cycles to complete."
"the second model, 'path con', considers each link of the topology as a resource. a core has to reserve all the links towards the destination for a period equal to the packet length. resembling the way that packets travel through a network, the end of a reservation will affect the starting timestamp of all subsequent reservations. as we are modelling a virtual cut-through network, a link can be reserved once the header has arrived to a router, in other words we can start reserving # − 1 cycles before the end of the reservation of the previous link. for simplicity we have considered that the packet always follow a xy path, but any routing algorithm could be easily implemented. in principle this is the model whose behaviour most closely mimics the actual network as it emulates packets movement. however as router arbitration is not modelled its behaviour can differ from the actual network. also it is the most complex of the models as it requires modelling lots of the components of a network (all the links)."
"in the case of 'direction con' we find out that the restriction of having a single packet per row or column is exceedingly restrictive, specially with high loads or large systems. for this reason we should discard this model. however, taking into account that it produces reasonably good results with the 32-node systems we may consider an intermediate solution which splits a large network into smaller 'direction con' networks which should provide better results for large systems while still being lightweight enough."
"the next logic step is to implement the most promising of those modules in the cotson simulator we use in our daily research. we have found that a reservation model which resembles network topology seems to be the most accurate model but it may be excessively demanding to provide very fast simulation. a good alternative is a reservation-based model which provides a number of 'pipes' which have to be reserved to submit packets. this also has the advantage of supporting distributed simulation seamlessly, which will simplify simulating the 1000-core chip we are designing."
"the first model has been denoted 'direction con'. it considers each row and each column of the noc as a shared resource in each direction. a core trying to inject a packet will reserve the row and the column as dictated by xy routing. first it will reserve the row for the number of hops required in the x dimension starting in the current moment, the reservation of the column will start after the end of the previous one and will last for the number of hops in the y dimension. to the final latency obtained by reserving the column we will add the packet length. the main limitation of this model is that it only allows one packet travelling in each row/column and direction of the noc, while an actual noc may allow several packets travelling in the same row, provided they are not competing for resources. for example the two packets in fig. 2 do not compete in a real noc, but require one waiting for the other in this model as both of them are using the same row. as we will see later, this will be the reason for this model reaching congestion before the modelled noc."
"to close the paper, authors want to emphasise that these models are intended to accelerate the simulation of manycore processors but should not replace a proper evaluation of the communication infrastructure when it comes to design such systems. our experiments showed that, under scenarios of saturation, none of the models were able to emulate network behaviour in an appropriate way."
"the employed methodology is as follows: the results obtained by the different modules are contrasted with those from the time-accurate network simulator. we will consider the following three figures of merit: 1) simulated execution time of the application. this provides a first approximation to the accuracy of the different models. 2) similarity score metric. a more profound assessment of accuracy. we measured the simulated time to execute every 100 trace events and compute the average difference with the actual simulator. a lower value means that the evolution of the application is closer to simulation, i.e. more accurate. 3) actual running time. this figure gives an insight into simulation speed. to provide fair speed estimates we developed the timing models as stand-alone tools. fig. 3 illustrates the similarity score metric. it shows the evolution of ferret with the simulator, the 'path con' model and the 'no contention' model. the average of the differences between the simulator and a model in each of these points is its similarity score. for instance, we can see how the evolution of the 'path con' model is always closer to the evolution of the simulator than the 'no contention' model. therefore it will have a lower similarity score, meaning that it is more accurate."
"looking at the results, we can see how 'path con' is consistently the most accurate. this is because it is the one having highest resemblance with the actual behaviour of the network. it models the topology and the way physical links are employed during normal operation, but avoids modelling all the complex logic within the router (requesting, vc management, arbitration, qos, congestion control). however, we found that this model cannot scale well to our target of 1024 cores as it may slowdown simulation considerably."
"if we look at the actual computation time, we can see that the high pressure exerted over the network by this kind of applications makes the reservation-based models slower than the other models. this is because the large amount of packets in the network is translated into lots of reservations in the data structures containing our resources. as the resources are implemented as sorted lists, each time a reservation is called, the whole structure has to be browsed which lowers performance. at any rate the reservation-based modules are still noticeably faster than the simulator in this context."
"the results for the different benchmarks are plotted in fig. 7 . we can see how, with this kind of applications, differences between models are more significant than with the previous two. this is because, as discussed, network saturation does appear and persist. in fact most of nas benchmarks are composed of different phases in which computation and communication are alternated. this means that during computation phases, network is barely used but, in turn, during communication phases the network suffers from severe saturation."
"this section is devoted to discussing the different timing models considered in this paper. we will start with some simplistic models that have been used by the community, but that in our opinion will not be appropriate once the number of on-chip cores goes over a few tens. then we will present the reservation-based models we are proposing. finally, we will consider some illustrative statistical models. in all cases we will discuss the strengths and limitations of the models."
based on this data structure we have implemented four different timing modules. the first two models are aware of the network topology -for the purpose of this paper we will consider a mesh -the other two are topology agnostic.
"instead of modelling contention, fist uses load-latency curves obtained from training simulations to estimate packet latency. it has, however, several limitations: the load-latency curves need to be obtained specifically for each traffic pattern, so if an application has a mixture of traffic patterns, or a patternless traffic it cannot be modelled properly. also it requires tracking the load handled by the noc. as instantaneous load tracking would be prohibitive in terms of synchronisation, a sweet-spot would need to be found for how often we calculate/estimate network load. again this means reaching a trade-off between accuracy and speed. at any rate using fist would involve executing a stand-alone network simulator to train the fist estimator, which has a definite impact on the overall time required to perform simulation."
"we propose several models that consider contention for the use of network resources. the basic element in these models is a 'resource', which in general represents a communication channel. to use one of these resources it is necessary to reserve it for a given period of time, forcing other accesses to the same resource to wait until it is freed. given the nocallback limitation of cotson, when a packet wants to reserve a resource which is in use, it then will reserve the first available fig. 1 . example of the reservation data structure. the resource starts with three reservations (top). a new reservation which requires adding a new element-in grey (middle). a new reservation which requires modifying an existing element-in grey-and permits removing an old element-in dark (bottom). period. note that as cotson has to provide the time for executing a complete operation, more than one packet may be needed to transmit trough the noc, and therefore, we may need to make reservations for the future."
"for the sake of completeness we use traces from applications of a diverse nature: directory-based shared memory, transactional memory and message passing. the former two are of interest in our research as they are the 'hot' application models to be run in the manycore systems we investigate. the latter, used in parallel and distributed systems, has a two-fold purpose. on one hand, it provides specific traffic characteristics that are not covered by the previous two models. on the other hand, it allows us to assess whether the proposed models may be used in other design domains such as, for instance, by the cluster computing community. finally we will use synthetically generated coherency-like traffic to evaluate systems composed of 1024 cores. this will provide some insights on the scalability of the evaluated timing models."
"an improvement of this model would consider the distance and the packet size to model the latency, but again without considering any contention in the noc. this model is still very poor as the noc is a common resource that has to be shared by all the cores and, hence, it is not likely that network packets travel through the noc without encountering any contention. in the discussions below we will denote this model 'no contention'. as we are considering cut-trough switching this module will return a latency of + # − 1."
"transactional memory is a novel memory model devised to simplify the development of shared memory applications and, especially, thread synchronisation [cit] . this scheme provides support for transparent atomic execution of instructions in a shared memory system. since this application model is gaining interest in the many-core community and it is an essential characteristic of our design, our study includes some examples of transactional memory applications. we generated 32-core traces of some of the stamp benchmarks [cit] following a procedure similar to the one used for parsec. the parameters for these benchmarks are listed in table ii. the traffic generated by this kind of applications has similar nature to cache-coherency's. however there is a noticeable difference: if the region of the memory which has to be accessed transactionally is small it will be likely located in a single core's cache. this core will then become a 'hot stop' as all the transactions will require traffic to and from this node. for this reason, the spatial patterns of these applications are more likely to have an unbalanced use of the network."
"in this paper we have proposed and evaluated a collection of timing models for the noc in the context of fast simulation of many-core systems. the main novelty of these models is that they implement a reservation scheme that allows modelling network contention without the need for tracking instantaneous network load, or implementing a complex and slow simulation requiring callbacks. our study has found that the proposed models consistently show more accuracy than the no-contention model that we had previously implemented in our set-up and that is widely used by the community."
"finally there exists the possibility to estimating latencies from a real simulation of the network. although very complicated models can be extracted we think that estimations from the average latency in simulation does suffice for the purpose of this paper. we will use an exponential distribution in which the parameter depends on the average delay measured during an actual simulation and the distance to be traversed, so that it provides the same average latency as simulation. we denoted this model 'exponential'."
"when designing new chip architectures it is essential to select appropriate evaluation methodologies. for example in the first phases of the design it is preferable to explore as much of the design space as possible. thus, fast evaluation methodologies such as functional simulation or analytical modelling are favoured in these phases even when they offer limited accuracy. as the final design approaches, we need to assess chip functionality and performance through more detailed simulations-a practice that can be seen as virtual prototyping. the high complexity of these models demands large amounts of computing power to carry out simulation."
"the topology-agnostic models consider the network as a collection of channels or pipes without any particular arrangement. when a core wants to send a packet it randomly selects one of the pipes and reserves it for the time required to perform the communication (# + − 1). this simplifies simulation while still considering contention for the use of resources. we have implemented two different versions of this model. in the first one, 'pipes', all the cores share all the pipes, so the contention in any part of the network will affect all the cores equally. the second one, 'pipes dist' is a distributed implementation in which the system is divided in groups of cores which share a collection of pipes. this way contention in an area of the network does not affect other areas. this model simplifies distributed simulation as there is no need for a shared data structure."
"we will close our study with two timing modules that do not consider contention directly but assume that travelling packets will suffer some extra delay due to other in-transit packets. the first model, 'load estimation', estimates the current network load and approximates the latency either as noncongested in which latency is barely affected, or as congested in which latency is greatly increased. the approximation uses a exponential distribution to select the latency, based on the estimated load and the distance the packet has to travel. in this paper, our estimator for the load is calculated as the number of injected packets divided by the elapsed time. note that this model follows the same idea as fist [cit] but in a simplified way."
"we present the results for these synthetic communications in fig. 8 . in general, all the reservation-based models but 'direction con' consistently provide better accuracy than the other ones. the reason for the low accuracy of 'direction con' is clear: allowing only one packet per each 32-core row (or column) is extremely restrictive, and results in overly pessimistic performance. regarding the computation time, we can see how all models except 'path con' execute more than two orders of magnitude faster than the simulator, which means good scalability levels. in the case of 'path con', it requires using roughly 4,000 'resource' structures and use, on average around 20 of them for each packet. this render this timing module noticeably slower than the others (but still one order of magnitude faster than actual simulations). this may not be acceptable for the simulation of large chips, as it may become an important simulation bottleneck."
"several international projects are pursuing this objective, although with different perspectives and objectives. the atac system, for instance, explores the viability of using a broadcast optical medium as the communication infrastructure within a 1000-core chip [cit] . in contrast, rigel [cit] has been devised as a programmable accelerator comparable to current gpus because of its single-process multiple-data (spmd) execution model. the main reason that such architectures have become so attractive for the scientific community is improved power and thermal characteristics by means of per-core frequency and voltage regulation (or even shutting off idle cores). they are also more resilient to failures due to their greater redundancy."
"the results for the transactional memory experiments are plotted in fig. 6 . in the plot we can appreciate that in general, for this kind of applications, the differences in simulation time are minute. this is because these applications generate very low traffic into the network and therefore there is not much contention. however the similarity score shows that the reservation-based models are more accurate than the other ones. again, the computation times required by all the timing modules remain similar and noticeably faster than simulation."
"another model which is worth mentioning is the 'pipes dist'. it has shown good accuracy and reasonable speed and scalability, but the more interesting property is that it can be used in distributed simulations without the need for a shared resource for simulating the network, which would simplify parallelising simulation as no synchronization would be needed among different parallel instances. these two characteristics make it a good candidate to scale up our simulator up to the 1000-core systems subject of our research."
"finally, although energy estimation is outside of the scope of this paper, it is fair to say that the proposed models should be more accurate than no-contention models because both are aware of the distance travelled by the packets but, in addition, the contention-aware models also provides an estimation of the buffering time employed by the packets, which has a definite impact on noc consumption."
"we have implemented a sorted linked list in which every element in the list represents a period in which the resource is reserved (fig 1) . the only required operation allowed is to reserve the resource for a given period of time; given the timestamp the reservation should begin and the duration (in cycles). this operation searchs for a free period of time that can accommodate the required reservation and will return the timestamp when it ends. therefore when a resource is reserved, the delay can be calculated instantaneously after reserving, just by subtracting the end of the granted period and the current timestamp. as a secondary effect, every time this operation is invoked, it will remove all outdated reservations from the list (those which have already finished). this helps to keep the list in a manageable size independently of the simulation length. to further reduce the number of elements in the list, a new element will be added to the list only if extending an existing one (i.e. increasing the timestamp it ends at) is not possible."
"in general, it is reasonable to state that the proposed models provide more accurate results than simpler no-contention models simply by considering packet interaction in a very simplified way. moreover, these models do not only provide more accuracy when simulating the same amount of traffic but may help to detect when the network becomes a bottleneck. for instance, consider two alternate architectural designs with different communication needs, one of them being communication biased. if we evaluate these two designs using a nocontention model the communication-biased design would be in clear advantage as network contention and saturation do not affect its performance. the same evaluation with a contentionaware model would provide a more sensible evaluation."
"the relentless improvement of electronic miniaturization has provided the possibility of integrating several processing cores into a single chip. most modern general purpose processors have several processing cores and, indeed, we can find processors with over 10 cores offered by several companies, such as the 10-core intel xeon processors [cit], the 12-core amd opteron processors [cit] or the 16-core sparc processors [cit] . even more processing cores are provided by the 48-core larrabee processor from intel [cit] or the 64-core tile64 processor offered by tilera [cit] . in fact, the design and development of new processor architectures able to integrate over one thousand cores in a single chip is a current hot topic [cit] . indeed, some authors speculate that such technologies may become a reality within this decade [cit] ."
"it is worth noticing that in this paper we have considered a simple noc design (mesh topology, cut-through switching and deterministic xy routing). however, modelling other noc designs based on the proposed 'reservation' structure is straightforward. it only requires to organize the pool of resources following the topology of choice, to reserve them as directed by the routing and to chain them as dictated by the switching technology. a proof of concept for different flavours of nocs remains as future work."
"as device scaling continues to track moore's law and with the end of corresponding performance improvements in outof-order processors, multicore systems have become the norm. if current trends continue, a chip with over 1000 [cit] . given manycores' inherent complexity, simulation is essential for exploring the design space of a future architecture. moreover, while simulators have focused on microarchitecture in the past, high level architecture (modelling the on chip network, memory hierarchy and coherence, etc.) is becoming increasingly more important. simulating a processor involves making decisions on trade-offs between simulation speed and accuracy. at one end of the spectrum, purely functional simulation provides little insight into system performance, but allows for fast simulation. at the other end, a cycle-accurate full-system simulation gives reliable estimates of performance, but at the cost of very long running simulations. there exists a range of options between these extremes, some of which are explored in this paper."
"we can see how, in general, all the models are very inaccurate as the simulation times differ greatly from the simulator. this is because, modelling the behaviour of a saturated network is nearly impossible without modelling the whole network. some of the reservation-based models do a relatively good work with some of the benchmarks but fail with the others. for example, as stated before, 'direction con' is severely affected by restriction of only one packet per direction. in the case of bt and sp benchmarks, they use a near neighbour communication pattern which essentially allows 7 packets in each row or column whereas 'direction con' has to deliver the packets sequentially (fig. 2) ."
"the average latencies reported by each model are shown in fig. 4 . in the plot we can see that when the network load is low all the models offer latency figures very similar to those of the real simulation. as network load increases all the models follow a trend similar to the real simulation. the only exceptions are the two simple models: 'fixed' and 'no contention' as they are unaware of the network load. the main difference, though, is when the different models start to behave as saturated, i.e. having very high latency figures. for example the 'direction con' model reaches saturation very early when compared with the real simulation. this is because modelling each row/column as a single shared resource is an extremely restrictive model (see again fig. 2 ). anyhow the network of a many-core system is not likely to suffer from persisting states of saturation when running shared memory or transactional memory applications as the threads requesting the use of the network will commonly stall until the reception of an ack packet indicating that the operation has succeeded. in general we can state that the contention-aware timing modules produce latency figures that resemble the shape of those of a noc. notice that the 'exponential' model practically overlaps with the simulator in this experiment. this is because this model uses the simulation average latency to generate a latency distribution with the same value. however, we will see later that with the real applications it can not capture application dynamics properly, throwing worse results than the reservation-based models."
"in this section we show how the different timing models for the noc behave under different operating conditions and compare them with insee, a lightweight time-accurate network simulator [cit] . for the purpose of this paper we considered minimal nocs: simple mesh topologies using xy routing and a single virtual channel. first we will use synthetic traffic from independent traffic sources which allows us to easily vary the pressure exerted on the communication infrastructure. next we will test them using traces from applications. this will allow us to assess their accuracy for a range of applications of interest. the use of traces of applications simplifies comparing the timing modules because network utilization will remain the same for each workload. if full-system simulation would have been used, each timing module could have had a different set of messages, as the performance of the noc may affect the overall traffic; e.g. in the case of two memory accesses to the same memory address, one for reading and the other for writing, the order in which they arrive to the cache will affect the subsequent communications: if the read arrives first it may require an extra invalidation packet once the write is executed."
"our research group is currently working in the first design phases of a future 1000-core architecture: teraflux [cit] . given that simulation speed is a valuable characteristic for us now, we have selected the cotson simulator [cit] as it provides adequate accuracy while being lightweight enough. to speed-up execution cotson processes a block of events at a time and offers other facilities such as statistical sampling. when modelling a large chip we need to provide timing models that do not slow-down the simulation while still being representative of the execution. how to strike such a balance is the key insight provided by this paper. our focus is on how we can produce timing models for nocs that we can use when considering the architectural design of large many-core chips without slowing down the simulation."
"as well as sharing products of particular individual and subgroup work, what was also noteworthy was the real time sharing of work being done on paper documents and whiteboards. here the video capabilities of the phones were used to reveal work as it was being performed. this involved some collaborative efforts with one person holding the mirrored phone to video the mark-up, gesticulation and talk around the paper in situ such that it could be shared across the two locations."
"we carefully evaluate the performance of the smurf algorithm with a simulation study. we model credit worthiness of customers in the presence of 7 assumed predictors and an interaction effect, based on the case study of gouvêa and gonçalves (2007) . table 4 lists the predictors and their levels."
"computing optimal solutions is intractable for many optimization problems of industrial and scientific importance. in practice, we are usually satisfied with \"good\" solutions, which are obtained by heuristic or metaheuristic algorithms. metaheuristics represent a family of approximate optimization techniques that gained a lot of popularity in the past two decades. they are among the most promising and successful techniques. metaheuristics provide \"acceptable\" solutions in a reasonable time for solving hard and complex problems in science and engineering. this explains the significant growth of interest in metaheuristic domain. unlike exact optimization algorithms, metaheuristics do not guarantee the optimality of the obtained solutions. instead of approximation algorithms, metaheuristics do not define how close the obtained solutions from the optimal ones are."
"test condition and sampling frequency were 896 r/min (loaded 1 kn) and 10 khz, respectively. the sample set is shown in table 2, and the vibration signals under the different health conditions are shown in figure 6 ."
"many problems in ai can be solved in theory by intelligently searching through many possible solutions: reasoning can be reduced to performing a search. for example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. planning algorithms search through trees of goals and sub goals, attempting to find a path to a target goal, a process called meansends analysis. robotics algorithms for moving limbs and grasping objects use local searches in configuration space. many learning algorithms use search algorithms based on optimization [cit] ."
"the term \"clustering\" is used in several research communities to describe methods for grouping of unlabeled data. these communities have different terminologies and assumptions for the components of the clustering process and the contexts in which clustering are used. thus, we face a dilemma regarding the scope of this survey. the production of a truly comprehensive survey would be a monumental task given the sheer mass of literature in this area. the accessibility of the survey might also be questionable given the need to reconcile very different vocabularies and assumptions regarding clustering in the various communities [cit] ."
"in addition to such multi-display and screen mirroring work, additional work looks to consider the potential for incorporating mobile phone camera capabilities into video call and media space set-ups. neustaedter and judge, for example, developed the peek-a-boo [cit] concept that exploited the mobility of the camera phone to link in with a fixed media space display in the home [cf. 3] ."
"the hyperbolic tangent function (''tanh'') is a zero-mean function that is the deformation of the sigmoid function (equation (3)), and its desired activation range is [−1, +1]. in some practical applications, the tanh function gradually replaces the sigmoid function. that ''tanh(x)'' is presented as"
"the prototype is implemented as a desktop application using.net 4.5 and windows presentation foundation. using the lync sdk we developed a bespoke lync client into which we have wrapped key elements of the lumia beamer functionality. the key difference from the standard lync desktop client is in the interface of an active video call. when a call is answered our application intercepts the conversation and presents a full screen mode containing 3 primary elements: the video conversation, a qr code for connecting the beamer application, and an area for displaying the mirrored smartphone screen (see fig. 1 ). to synchronise the mirrored view on both clients connected to a call, a lightweight machine-to-machine communication protocol (mqtt) with a publish/subscribe mechanism is used. whenever a lync call is accepted, each client creates an id unique to the conversation, based on an md5 hash of a sorted list of the lync ids of connected users. each client connects to a mqtt message broker and uses the created id as the subscription topic. when a qr code is scanned the client publishes the session url to the conversation topic and all clients with the same id receive the message and redirect its mirrored view to that session url. each time a beamer mirroring session has been initiated, the client on which the qr code was scanned generates and presents a new unique qr code. by doing this, participants on each side of the video call always have access to start a new beamer mirroring session."
"secondly, we showed in full detail how this algorithm creates sparse models, using varying combinations of lasso-type penalties, and investigating and documenting all possible model choices. the choice of penalty weights and the tuning strategy have a substantial influence on the performance of the estimated model. in our simulation study, the cross-validation approach with one standard error rule and combined adaptive and standardization penalty weights provided the best results. additionally, the re-estimation of coefficients provided good results in this paper, where we used relatively large data sets. however, when dealing with smaller data sets, it might be preferable to work with the original regularized estimates as the performance gain due to the bias reduction might be canceled out by the increase in variance. our implementation of the algorithm is available on gitlab as the r package smurf (https://gitlab.com/treynkens/smurf/) and will be added to the cran repository. we propose several paths for future research."
"in this section, shows experimental results which validate the modified variable neighbourhood search (vns) algorithm as a metahieuristic search and (cs) algorithm as a swarm intelligence search with k-mean clustering. as mentioned above, we developed our database clustering software, and all tests were run in the testing environment."
"clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). the clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. however, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur."
"we studied the general problem of convex optimization with a differentiable loss function and multi-type penalty terms. this setting is highly relevant when the level structure of different predictor types needs to be taken into account in the regularization. our contribution is twofold. first, we developed the smurf algorithm that accurately and effectively solves this general optimization problem. the algorithm extends other proximal gradient algorithms found in the literature for convex, regularized optimization."
"these feature surfaces enable the convolutional layer to determine the output of the neural network's neurons. the neural network extracts different features of the input by convolutional operations. this special structure leads to a characteristic called weight sharing. in the same convolutional layer, the convolutional kernel with the same weight is applied to all data positions. the neurons within a convolutional layer will connect only to a small region of the layer preceding it. therefore, the convolutional layer needs to learn only one feature set and can then promote the reuse of the features to obtain additional abstract features in a high-level expression. this process helps reduce the network's parameters without affecting the runtime of feedforward propagation."
"using a standard procedure from the gradient descent algorithm in, we rewrite the objective function in (1) and replace f by a local linearization around a point β ( * ) including a tikhonov regularization term:"
"based on the partition of j into j l, j r, j li, j ri, j lr and j lir, some binary decision variables (x j, y j, z j ) are fixed. this reduces the number of decision variables and the computation effort to solve the next mathematical model (p 2 ). since the jobs in j l, j r satisfy the following conditions:"
"one of the challenges of clustering in data mining is the dealing with large databases. while many clustering algorithms work only on a small database, one solution is to use samples of the large database and cluster them, the quality of this clustering depends on samples, which may lead to biased results. taking the whole large database will make the execution time too long to complete clustering. the suggested algorithms-that can deal with large database in a scalable manner-does not depend on sampling, thus it will give us real results (not biased). this algorithm generates the cluster point automatically by vns and cs then applies on the k-mean for some iteration, which leads to minimize and stabilize the time of clustering. from the experiment results stated in this paper, the following conclusions can be drawn:"
"the effectiveness of the proposed lower bound has been demonstrated by the results of extensive computational experiments. indeed, the obtained lower bound outperforms the best lower bound from the literature for 915 instances out of 2100 (43.57%). in the other hand, this lower bound requires a moderate cpu time (0.76s) and appears to be reasonably fast despite we are solving a sequence of hard knapsack problems. moreover, we observe that the average difference of the proposed lower bound with the literature lower bound, is significant with an average value of 8.57.. furthermore, the best performance of the proposed lower bound is detected for the large number of centers, where the propagation procedures reveals its efficiency."
"future research efforts need to be provided in terms of development of a more efficient heuristic and exact procedures in order to solve the hfsmt scheduling problem. we expect that the new derived lower bound would be useful for assessing the proposed heuristics in a good way, in addition to helping bounding the optimal solution value, in order to facilitate and speed up the exact solution procedures, which requires further investigation."
"the goal of this paper is to merge and to extend both streams of work into a carefully designed estimation procedure, suitable for regularization with different predictor types and more general loss functions. our solution relies on the theory of proximal operators [cit] for a comprehensive overview) [cit] . this allows us to decompose the optimization procedure with the multi-type penalty into a set of smaller optimization subproblems for each predictor type and its associated penalty. as a second contribution, our proposed estimation procedure applies specialized machine learning algorithms to each subproblem. furthermore, this partition enables the use of distributed or parallel computing for the optimization process. this estimation procedure is provided in the r package smurf, available on gitlab (https://gitlab.com/treynkens/smurf/)."
"however, the sigmoid and tanh functions are saturated nonlinear functions, which may have the disadvantage of encountering problems with local optimization. when input x is excessively large or small, the derivatives are nearly 0 ( figure 3 ). consequently, nearly no parameter will be updated in the gradient descent optimization process."
"interaction salxloan interaction effect between the salary and loan predictors. table 4 : overview of the response, the predictors and their levels used in the simulated data sets."
"an important issue in clustering is how to determine the similarity between two objects, so that clusters can be formed from objects with high similarity within clusters and low similarity between clusters. commonly, to measure similarity or dissimilarity between objects, a distance measure such as euclidean, manhattan and minkowski is used. a distance function returns a lower value for pairs of objects that are more similar to one another [cit] ."
"the subsequent group interview sessions were used to elicit general opinions about the system as well as elaborations on specific behaviours of interest identified by the researcher observing the sessions. these interviews were video recorded and transcribed for later analysis. over and above the in situ observations of the research, video recordings of the task sessions were subsequently revisited to allow a more detailed, reflective and systematic analysis of the unfolding collaborative action of the participants. findings are primarily based on the observations and video recordings, but interview data have been utilised in getting a deeper understanding of particularly interesting events throughout the analysis."
"the naive version of the smurf algorithm can be found in algorithm 1 in the paper. in this section, we discuss several improvements that are used in the implemented smurf algorithm. pseudo code for the full algorithm is given in algorithm a.1 and numeric values for the algorithm parameters are given in table b .1."
"when ρ (l−1) changes, the inverse can easily be recomputed as the eigenvector and eigenvalues are independent of ρ (l−1) . note that the eigenvalue decomposition needs to be computed only once and not at every computation of the proximal operator. therefore, this approach is faster than using a general function to compute the matrix inverse of c simulation study"
"in this paper, we present work that is motivated by the key arguments and limitations of current screen mirroring capabilities discussed above. the work looks to support more ad hoc and casual screen mirroring opportunities within video calls for multiple collocated participants at either end of a remote collaboration. the aim here is to achieve this by facilitating the personal devices of all participants that are not explicitly connected as host devices within the call. furthermore it aims to enable this to be done from wherever in the room by using wireless rather than wired based screen mirroring mechanisms. while our aims and motivations apply to a broader range of personal devices and artefacts within the meeting room ecosystem, we focus in the first instance on wireless smartphone screen mirroring. one of the key reasons for this is to enable us to further exploit the specific mobile camera capabilities of smartphones with a view to open up access to all in the room to participate in distributed sharing of the various physical information artefacts located around the room. before presenting the system, we briefly discuss some related work to further ground the arguments underpinning the system. after presenting the system, we highlight some initial findings from a study of the system in use for a distributed collaborative activity."
"in summary, sigmoid, tanh, and relu functions are the most widely used activation functions in neural networks. their shapes and those of their derivatives are shown in figure 2 and figure 3, respectively."
"for testing purposes, an implementation original version of k-mean algorithm with variant clusters number was done to compare its results in term of the efficiency and performance with the proposed algorithms results."
the implementation of the admm algorithm was done in c++ using the armadillo library [cit] which is called through the r package rcpparmadillo [cit] .
"the mtpl data set contains information on 163,660 [cit] . each policyholder is observed during an insured period ranging from one day to one year, further denoted as the exposure variable expo. during this period the policyholder is exposed to the risk of being involved in an accident and able to file a claim to the insurer. policyholders are further distinguished through a set of personal as well as vehicle characteristics displayed in table 6 . the aforementioned papers remove some predictors from this data set a priori, such as mono, four, sports and payfreq. we keep these in our analysis and use the data-driven smurf algorithm to determine their predictive power. figure 6 shows the histograms and barplots of the response, the exposure, the spatial and the ordinal predictors in the data set. the response nclaims denotes the number of claims filed to the insurer during the exposure period. figure 7 displays the bar plots of the binary and nominal predictors. [cit] ."
"the improved relu-cnns and relu-cnn have smaller losses compared with s-cnn and t-cnn. the initial loss value of s-cnn is 1.95. at the beginning of the iteration process, the values decrease sharply same with those of the other models but remain a steady period around the value of 1.25. then, the loss decreases until after 500 iterations, when the loss of s-cnn increases slightly instead. for t-cnn, after 250 iterations, its loss does not further decrease after reaching approximately 0.75, and its loss value becomes considerably higher than those of the four other models. the average loss of t-cnn is nearly twice times higher than that of relu-cnn. in short, the loss values of s-cnn and t-cnn are considerably higher than that of the improved relu-cnns and the relu-cnn. this phenomenon hinders the generation of a superior model mainly because of the gradient vanishing problem and the local minimum. according to the definition analysis in section ii part c, s-cnn has this loss changing trend that may be due to the vanishing gradient problem. once the gradient vanishes, the parameters will not be updated. thus, the remaining learning process has no meaning. additionally, global optimization is difficult to accomplish after s-cnn or t-cnn acquires their local optimization parameters. consequently, the current local optimization value remains the same."
"however, the sigmoid and tanh functions are saturated nonlinear functions, which may have the disadvantage of encountering problems with local optimization. when input x is excessively large or small, the derivatives are nearly 0 ( figure 3 ). consequently, nearly no parameter will be updated in the gradient descent optimization process."
"po (generalized) fused lasso. no analytic solution exists for the po of the (generalized) fused lasso. to solve (9) for these penalties, we implement the alternating direction methods of multipliers (admm) algorithm of and incorporating some minor adjustments suggested in . the admm algorithm has previously been used to solve similar fused lasso [cit] ) as well as trend filtering [cit] having these efficient solvers available for all pos, we combine them into the smurf algorithm, of which the naive form is given in algorithm 1. we improve the computational efficiency of this naive version using several techniques from optimization theory. appendix a of the supplementary material provides the full implementation details for these improvements. the implementation of smurf is modular, allowing for straightforward extension to new penalties by implementing the solver of the accompanying po. smurf has the same asymptotic properties as the base proximal gradient algorithm [cit] ) which converges to the optimal solution when the number of iterations k goes to infinity."
"over the past few years, the great progress which machine learning especially deep learning achieved make these techniques gain increasingly widely applications in many different areas, such as the rotating machinery industry area [cit] . at same time, modern machines are developing with enhanced complexity, their health conditions become increasingly crucial to ensure the industry system's safety and operation performance. in rotating machinery, bearings and gears are critical components and their fault diagnosis and detection have practical significant influences in industry [cit] . therefore, fault diagnosis based intelligent method has attracted increasing attention in recent years [cit] ."
"minimizing the cost function is the most important part of the optimization algorithm. when the gradient or the derivative is known, the parameters of the cnn can be updated through iteration. the parameter update equation is"
"of significance here was the coordinated organisation of individual and shared aspects of the task. individual work here took place in parallel to shared discussions happening around the shared display. for example, individuals were observed using their smartphones to search for images while not mirrored to the shared display. once the images were located they would then mirror their display to the shared surface in order to take the floor. likewise, design ideas were explored on paper documents in preparation for subsequent sharing via the main screen. such preparation would often happen in parallel with another participant sharing and presenting. some participants would prefer to utilise the live video to share design ideas with the option of quickly switching between different documents laid out on the table, by simply moving the camera around. others would take photos as their work progressed and later share them by mirroring the photo gallery application of the smartphone. in any case, distributing the activities across multiple devices and artefacts enabled a more fluid interleaving of individual, subgroup and full group sharing activities. the preparation work meant that objects of sharing were immediately available to facilitate the social mechanisms and timings by which new ideas could be introduced to take the floor in the discussion."
"with the arrival of big data, many companies and institutions struggle to infer meaningful information from their data sets. we propose a novel estimation framework for sparse regression models that can simultaneously handle: (1) the selection of the most relevant predictors (or: features), and (2) the binning or level fusion of different predictor types, taking into account their structural properties."
"a flowchart of the proposed method is shown in figure 4 . the whole process comprises five stages, including preparing the data, building the model, training the model, and applying the model. the details of the five stages are given as follows:"
"the strategy for the variable neighborhood search involves iterative exploration of larger and larger neighborhoods for a given local optima until an improvement is located after which time the search across expanding neighborhoods is repeated. the strategy is motivated by three principles: [cit] 1) a local minimum for one neighborhood structure may not be a local minimum for a different neighborhood structure, 2) a global minimum is a local minimum for all possible neighborhood structures, and 3) local minima are relatively close to global minima for many problem classes. variable neighborhood search (vns) is a recent metaheuristic, or framework for building heuristics, which exploits systematically the idea of neighborhood change, both in the descent to local minima and in the escape from the valleys which contain them [cit] ."
"1-the proposed algorithms do not need to input the value of number of clusters to it, instead the algorithm needs to pass automatically and randomly a new value in accurate way to determines the closeness between items in the same cluster. so also are the items in the same cluster are very close to each other."
"the mobility of these devices means that real time capture opportunities are available in flexible sites around the distributed locations. as we saw, this enabled this functionality to be moved to the sites of interest allowing physical artefacts and the work around them to be incorporated in the distributed screen mirroring. in extending the mirroring capabilities across multiple personal devices, individuals had an additional resource through which to take control of the floor in the conversation. we saw how this facilitated parallel streams of individual and shared working and the fluid interleaving of these activities. individuals were able to engage in their own preparatory activities with both digital and physical resources before introducing them into a more shared context for discussion. directly mirroring personal devices such as smartphones naturally introduce privacy issues. these are outside the scope of this paper but is an interesting issue for future work. finally, in contrast to some screen mirroring technologies that require an existing mirror connection to be first disconnected, our mechanism enabled participants to override any existing connection. this meant the opportunity to share was always available and negotiable through lightweight social mediation."
"the hole procedure is halted if no infeasibility and nor adjustments are detected. the obtained value of the destructive lower bound is denoted lb d . in addition, the considered initial trial value is lb lit ."
"in this work, the improved relu-cnns was proposed for machinery fault diagnosis. raw vibration signal was directly utilized as input data, and the proposed method can eliminate dependence on manual feature extraction and selection. the improved relu-cnns use lrelu and prelu, which have the advantages of the standard relu activation function and overcome its shortcomings, as activation functions. then, experimental validations on bearing and gearbox datasets were performed to verify the performance of the proposed method. the results showed that the improved relu-cnns has the best performance compared with other established models. the improve relu-cnns can obtain higher diagnosis accuracies on both datasets and outperform the standard relu-cnn in terms of convergence. thus, the proposed method can obtain satisfactory performance in the intelligent fault diagnosis of machinery."
"in the real world, if a cuckoo's egg is very similar to a host's eggs, then this cuckoo's egg is less likely to be discovered, thus the fitness should be related to the difference in solutions depends on the step size value (which it mean the probability to finding the best solution). in the other hand the original k-mean algorithm find the best solution by find the closest items in each cluster only. therefore, it is a good idea to do find a closest items in each cluster by using the fitness measure in cuckoo search algorithm (in this case the cluster means the nests) by increasing the degree of nearest between the items in the clusters, through we discard the weakness nests which have the lower number of items after that regenerate randomly a new nests instead of the discarded nests with re-computing the fitness of all clusters after nests sorted in a matrix by fitness depending on the best solutions. in this way, higher fitness solutions have slight advantage over solutions with lower fitness. this method keeps the selection pressure (the degree to which highly fit solutions are selected) towards better solutions and algorithm should achieve better results."
"and the eventual adjustments of the time window time [r j, d j ] of a job j are done with the new value of s j (t 1, t 2 )."
"re-estimation as with most regularization methods, the finite sample coefficient estimates and predictions obtained with the fitted model will be biased. to reduce this bias, we propose to re-estimate the model without penalties, but with a reduced model matrixx, based on the parameter estimates obtained with smurf. hereto we remove the columns of x for which the coefficients are estimated to be 0, and collapse the columns for which the coefficient estimates are fused. the re-estimated coefficients will thus have the same non-zero and fused coefficients as the regularized estimates, but will not be biased. [cit] ."
"figure 8: parameter estimates for the ageph, power, bm and agec predictors of the mtpl data, centered around 0. the dots and crosses denote the parameter estimates from the smurf algorithm before and after the re-estimation respectively. the full black line corresponds to the gam fit and the dotted lines to the fit ± 2 standard deviations. figure 9 shows the parameter estimates for the binary predictors, and the predictors payfreq and coverage. from the set of binary predictors, only fuel is selected while the others are put to 0, effectively removing them from the model. the parameter estimates obtained with the gam fit confirm this behaviour, as 0 is within the confidence interval for all removed predictors except 4x4. as the levels of 4x4 are highly imbalanced (see figure 7), the influence of this predictor on the negative log-likelihood is minor and the regularized estimates remove the predictor from the model. the expected number of claims rises as payfreq increases up to monthly or triyearly payments, which are fused in the final model obtained with smurf. when a policyholder buys a partial or full omnium, the expected claim frequency decreases compared to the standard third party liability option. the smurf estimates fuse the levels for partial and full omnium. the dots and crosses denote the estimates of the smurf algorithm before and after the re-estimation respectively. the black squares correspond to the parameter estimates obtained with the gam fit and the vertical black lines to the gam fit ± 2 standard deviations. figure 10 illustrates the estimated parameters for the spatial effect, captured by muni. the smurf algorithm estimates 38 unique coefficients for the initial 266 different levels whereas the gam calculates 23.7 degrees of freedom. for both the smurf as the gam estimates, we see a higher expected claim frequency for people living in and around the larger cities in belgium, though this distinction is less clear for the gam estimates. in contrast, the models predict less claims for people living in the rural parts to the south, northeast and west of belgium. similar to the ordinal predictors, the gam estimates are smoother than the smurf estimates and need less degrees of freedom to represent the data. however, the range of the parameter estimates of the smurf algorithm is wider than for the gam, allowing for larger differences in expected claim frequency."
"after the pooling layer, cnn can obtain high-level features. assume that p l is the output of the l th layer, which is a pooling layer. then,"
"the energetic reasoning feasibility test, consists on comparing from one side the minimum amount of time required to be provided by the machines for treating the jobs (total work) and for the other side the capacity of this machines m (t 2 − t 1 ), throughout the time interval [t 1, t 2 ]. in the last sections, the total work has been improved (compared to the classical one ) by adopting new approaches ((p 2 ) and (p 3 ))."
"we use the insights of section 3.1 to build smurf, an algorithm that enables sparse multitype regularized feature modeling. the critical point is to find the solution of the pos in (9) for the different penalties discussed in section 2.1. we briefly sketch our solvers for each penalty type below and provide an overview in table 2 . appendix a of the supplementary material provides all further details on the implementation of smurf."
"where y is the true class label, and p is the predicted probability. for all training samples, calculate the mean value, which acts as the cost function."
"the objective function (7) is the minimizing of the total amount of work on j \\ (j l ∪ j r ∪ j i ). constraints (8a-8d) require that each job should be placed at only one position (left or right or inside), according to the subset it volume 5, 2017 belongs to. constraints (8e -8f) involve that the resource capacity constraints at times t 1 and t 2 are respected, respectively. finally, constraints (8g-8i) state that the decision variables are binary-valued. here after the formulation (7 -8i) is refer to as the exact formulation."
"artificial intelligence (ai) is technology and a branch of computer science that studies and develops intelligent machines and software. major ai researchers and textbooks define the field as \"the study and design of intelligent agents\", where an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. [cit], defines it as \"the science and engineering of making intelligent machines\" [cit] ."
"this section is devoted to the generalization of the concept of the rer to the cumulative scheduling problem. in this context, the following notations are presented."
"a critical feature of these interactions was the nature of mobility enabled by the wireless sharing of these images. this played out in a number of important ways. first of all, we saw how it allowed people to perform sharing activities from wherever they were seated allowing them to fluidly shift from individual to shared activities in the context of their locally assembled artefacts. second, we saw how this mobility enabled movement around and beyond the room. for example, one participant left the meeting room to capture a photo of artwork situated in the atrium of the building. another participant moved from their seat to the whiteboard in the room and proceeded to share a live image of the content from his camera phone while talking about it. finally the micro mobility of the phone was exploited to achieve the fine-grained framing requirements of specific features of the work process and artefacts that were deemed useful to be shared across sites. in essence, mobility allowed participants to accommodate for features of the environment that impacted on the spatial organisation of the work."
"to put it in a simple way, swarm intelligence can be described as the collective behavior emerged from social insects working under very few rules. self-organization is the main theme with limited restrictions from interactions among agents. many famous examples of swarm intelligence come from the world of animals, such as birds flock, fish school and bugs swarm [cit] ."
"is the treated part of job j if it starts (resp finishes) processing at r j (resp d j ) over [t 1, t 2 ]. clearly, a lower bound on the time that must be provided by the resource z, during [t 1, t 2 ] to process a job j, in any feasible schedule, is the work"
"the rules for determining structural hyperparameters (e.g., cnn depth, layer number, or the node number of a fully connected layer) remain uncertain. during the design of a cnn, the most common approach is to set a specific depth and other parameters according to the researcher's experience or according to the recommendation of other studies. several hyperparameters that are important to the cnn model are discussed in this study."
"we remark that center-machine configurations contains a diversified distribution patterns, more precisely: in addition, set b includes three types of instances: type b-1, type b-2, and type b-3. these types are characterized by the way the processing times p kj are generated."
"with the emergence of commercial screen mirroring technologies in smartphones such as miracast, airplay and chromecast, we are beginning to see some explorations of its use among small collocated groups of collaborators. a recent study [cit] showed how multiple collocated users of these technologies self-managed the mirroring of their phones to a main display. while they used wired connections in their study, they highlighted the ways in which participation was better shared among the collaborators reducing dominance by a single person who might otherwise control the mirrored display. in our work we extend these ideas to include remote settings as well as wireless techniques for smartphone screen mirroring."
"cnn is a supervised neural network, and its basic structure comprises an input layer, convolutional layers, pooling layers (subsampling layers), fully connected layers, and an output layer. a single cnn typically has several convolutional layers. meanwhile, the pooling layer is dispensable because its use is determined by a specific model. when a pooling layer is used in a cnn architecture, this layer is commonly connected after a convolutional layer. the basic cnn structure is illustrated in figure 1 [cit] . the basic functionality of each part of cnn can be summarized as follows."
"although it can be proved that the procedure will always terminate, the k-means algorithm does not necessarily find the most optimal configuration, corresponding to the global objective function minimum. the algorithm is also significantly sensitive to the initial randomly selected cluster centers. the k-means algorithm can be run multiple times to reduce this effect [cit] ."
"the overall results are shown in figure 14 . all the models obtain 100% training accuracies. the improved relu-cnns figure 14. accuracy results of different cnn models (gear). volume 8, 2020 obtain the highest testing accuracy of approximately 95%. the testing accuracy of t-cnn is the lowest. the tanh function is a deformation version of the sigmoid function and is supposed to perform better than sigmoid. however, the results fail to support this when the dataset is not large enough."
variable neighborhood search is a metaheuristic and a global optimization technique that manages a local search technique. it is related to the iterative local search algorithm.
"the former and latter convolutional layers extract low-and advanced-level features, respectively. assume that the input of the convolutional layer is a. in the convolutional layer, the activated output of the l th layer can be expressed as"
"in this section, an improvement of the capacity is proposed. the first improvement is based on the resolution of the following knapsack problem: (25) subject to :"
"k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. the procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. the main idea is to define k centroids, one for each cluster. these centroids should be placed in a cunning way because of different location causes different result. so, the better choice is to place them as much as possible far away from each other. the next step is to take each point belonging to a given data set and associate it to the nearest centroid. when no point is pending, the first step is completed and an early group-age is done. at this point we need to re-calculate k new centroids as bary-enters of the clusters resulting from the previous step. after we have these k new centroids, a new binding has to be done between the same data set points and the nearest new centroid. a loop has been generated. as a result of this loop we may notice that the k centroids change their location step by step until no more changes are done. in other words centroids do not move any more [cit] ."
"different cnns were designed for comparison to validate the performance of the improved relu-cnns, except the setting of the activation function, the network structure and experimental set remained the same. all these networks used a similar 6-layers model structure with two convolutional layers, two max-pooling layers (which followed two convolutional layers separately), one fully connected layer, and one output layer."
"since the early days of video mediated communication research, there has been widespread recognition of the importance of sharing the viewing of information artefacts between distributed participants as the basis for ongoing discussion and collaboration [e.g. 2]. these shared artefacts whether physical or digital provide a common ground [cit] that can be drawn attention to in the context of collaborative work. significant research efforts within the domain of computer-supported cooperative work (cscw) have looked to develop ways in which these artefacts can be shared and simultaneously viewed by distributed collaborators. drawing on these efforts, many commercially available video calling solutions (e.g. skype, lync, google hangouts, and webex) offer some form of screen mirroring capability to support shared viewing of digital documents across distributed sites. in this paper the term mirroring refers to the direct duplication of one screen to another, while sharing is used to describe the act of visually sharing activities and digital or physical artefacts among collaborators."
"we also compare the out-of-sample predictive performance of smurf, the glm and the gam on the hold-out dataset. for each model, we sort the predictions such thatŷ (1) refers to the observation with the highest predicted claims frequency. we define the proportion of the sorted population, prop i and the proportion of observed claims, obs i, by:"
"stochastic version of smurf. in current machine learning literature, stochastic versions of gradient descent algorithms exist where only part of the data is used every iteration. this speeds up the calculation per iteration while requiring more iterations for convergence. additionally, due to its stochastic nature, stochastic optimization methods are less prone to get stuck in a local optimum, a useful property in the context of non-convex optimization. therefore it is interesting to adapt smurf into a stochastic variant. this opens up the use of the algorithm with non-convex penalties such as the l 0 norm. section a gives a detailed overview of the implementation of the smurf algorithm. the calculation of the proximal operator (po) for the generalized fused lasso is explained in section b and section c expands on the simulation results."
"the improved relu-cnns perform better than the relu-cnn, although their losses have similar changing trends. among all models, prelu-cnn maintains the lowest value of 0.225 on average. according to the definition of activation functions (section ii part c), relu, lrelu and prelu are all non-saturating rectified linear functions. therefore, to some extent, these functions take advantage of linear functions, and the improved relu-cnns and relu-cnn can calculate the activated values in a straightforward manner instead of performing many complex operations. the learning processes are faster than those in s-cnn and t-cnn while obtaining higher accuracies. however, relu-cnn's accuracy is not as good as that of the improved relu-cnns. given that relu may not fully use neurons during the learning process, relu-cnn cannot exploit all the neurons in one layer. by introducing negative axis values, the improved relu-cnns solve this problem."
"as explained in section 3.1, we use a standard procedure from the gradient descent method to approximate the objective function. instead of using a standard gradient update as in algorithm 1, suggests to use acceleration. here, a new point θ (k) is found by moving along the line determined by β (k−1) and"
"within the sessions, it was observed that multiple people in both locations took the opportunity to share content from their devices. the content shared included images sourced from the web, photographs taken of drawings on paper documents, photographs of whiteboards, photographs of objects captured outside the meeting room and live video images of paper documents and objects as they were being worked on or discussed and pointed to in real time."
"bearing fault signals from a test rig were used to verify the effectiveness of the proposed method. the accelerometer (pcb 352c33) was installed near the testing bearing. the bearing test platform was composed of a drive motor, a healthy bearing, accelerometers, a loading system, and testing bearings. the experimental setup is shown in figure 5 . the test bearings were skf bearings (6205-2rs skf). a simple device based on the thread-and-nut system was assembled in the radial direction to add an adjustable mechanical load. four types of rolling bearing vibration acceleration signal conditions were adopted, namely, normal (norm), inner-race fault (if), rolling ball fault (bf), and outer-race fault (of) conditions, which were collected from the drive end of the test motor. for if, bf, and of conditions, vibration signals of five severity levels (0.2, 0.3, 0.4, 0.5, and 0.6 mm) were collected separately. defects of the bearings were artificially set by a wire-electrode cutting machine. each condition contained 100 samples. a total of 1600 samples were collected, wherein 80% were used for training and 20% for testing."
"the rooms used were standard conference rooms with a large display on the front wall on which the application was presented (see fig. 2 ). each room had various vertical whiteboard surfaces on the walls. we also provided both rooms with coloured pencils, felt-tip pens, post-its, and a4 paper both blank or with pre-printed t-shirt templates. during the introduction each participant was given a nokia lumia phone configured to have access from the start screen to the beamer application, internet explorer, office, camera, photo gallery, and calendar. browser history and photo fig. 2 . room set-up for the study gallery were cleared between sessions. each session was video recorded using a dedicated video camera that was positioned to capture all of the collocated participants in the room and the shared display on the front wall on which the remote video of the single participant room was visible. the single person room was not video recorded but observed, to document events that might not be so apparent through the video conversation view."
"convolutional and pooling layers can be alternately connected to one another multiple times. then, one or two shallow fully connected layers are applied on top of them. each neuron in the fully connected layer is fully connected to all the neurons in the previous layer. these neural layers comprise the basic cnn architecture. the convolutional and pooling layers jointly perform implicit feature extraction, and the fully connected layer flattens the feature from the matrix form into a one-dimensional sequence. the output value of the last fully connected layer is forwarded to an output layer, which further classifies the extracted features. in this study, softmax regression is utilized as the output layer for multiclass classification."
"in conclusion, we see that smurf is competitive with the well established gam approach, both in model complexity as in predictive accuracy, while additionally performing automatic predictor selection, fusion of levels and improving the interpretability."
"therefore, improved relu functions, namely, lrelu and prelu, have been introduced to avoid this disadvantage. lrelu has a leakage correction and is an extended version of the relu function. the coefficient of the leakage correction is 0.01. this small value guarantees a small straight line in the activation function. prelu is a generalization of lrelu, and its leakage correction is parameterized as α. prelu can be learned along with other neural network parameters during the iteration process. the formulas of the improved functions are listed in table 1 . their shapes and derivative shapes are also shown in figures 2 and 3, respectively. the lrelu with leakage correction is an extended version of the standard relu activation function, whereas prelu is a generalization of lrelu. both activation functions can be considered to be generalized versions of the standard relu function. lrelu-cnn and prelu-cnn are used separately to explore the effects of the improved relu-cnns. moreover, lrelu-cnn is a specialist of prelu when α is equal to 0.01."
"activation functions are crucial elements in deep learning neural networks. these functions perform transformations on the input to maintain their values in a manageable range. in a neural network, feedforward propagation is the process of multiplying various input values of a particular neuron by their associated weights, summing the results, and then scaling the values between a given range before forwarding this information to the succeeding layer. activation functions perform scaling operations."
"the main reason for having many clustering techniques (methods) is the fact that the notion of \"cluster\" is not precisely defined [cit] . consequently many clustering methods have been developed, each of which uses a different induction principle [cit] ."
"the convolutional layer is composed of multiple feature surfaces (feature maps), each of which comprises multiple neurons. each neuron connects a local region of the former feature surface through the convolutional kernel."
"the problems (p 5 ) and (p 6 ) are exactly solved since the knapsack problem is an easy one among the hard problems. denoting __ w * 5 and __ w * 6 the optimal solutions of (p 5 ) and (p 6 ), respectively, then the total work w * 1 can be expressed by:"
"both functions are saturated nonlinear functions and face the local optimization problem. rectified linear units (relu) have been developed to replace these functions. although the relu function is easier to optimize than traditional activation functions, its definition makes it contain limited information which contained in negative value inputs. thus, in this research, the improved relu-cnns are proposed for intelligent fault diagnosis of rotating machineries. the proposed method implements the improved relu, which adopts the advantages of the relu function and overcomes its shortcomings, as activation function. bearing and gearbox datasets are analyzed to validate the proposed method. the results prove that the proposed method demonstrates high recognition accuracy with satisfactory convergence speed. the main contributions are listed as follows: (1) an improved rectified linear units function based on a cnn model for machine fault diagnosis is proposed; (2) model's hyperparameters' selection, especially for an activation function, is fully investigated; (3) the proposed model with satisfactory accuracies with an enhanced convergence speed is experimentally validated using two different machinery datasets."
"in the backward process, the stochastic gradient descent (sgd) is used to optimize the model. the sgd algorithm is used to adjust the parameters of the improved relu-cnns. specifically, it calculates the error between the output vector and the actual target vector and obtains the loss function and its gradients. the algorithm then updates the parameters, including the weights and biases in each layer."
"as shown in figure 15 (a) and (b), the accuracies and losses of the five models are quite different. during the beginning iterations, t-cnn rapidly obtains high accuracy and performs much better than s-cnn. however, t-cnn has the biggest loss through the iteration process; its loss curve fluctuates and is always higher than those of the four other models. this loss change indicates that the learning process of t-cnn is not good enough to obtain satisfactory model parameters. the s-cnn's performance increases slowly, and its accuracies nearly do not improve during the first 200 iterations. s-cnn finally obtains an accuracy of approximately 85%, which is 10% higher than that of t-cnn, but it cannot increase further. this finding confirms the previous analysis that once the saturated function meets the local optimization, it tends to stop optimizing. the improved relu-cnns and the standard relu-cnn have similar changing trends for both accuracy and loss. they both obtain higher accuracies and lower losses than s-cnn and t-cnn. compared with the standard relu-cnn, the improved relu-cnns have an overall better performance, as indicated by the faster accuracy increase ratio and the smaller oscillation during learning process. these results are possibly due to the characteristic of the improve relu activation functions. although their activation functions have similar properties, the improved relu-cnns can use additional information contained in the negative input. therefore, the improved relu-cnns demonstrate strong robustness, which is consistent with the analysis in the previous section."
"deep learning is a relatively new machine learning technique [cit] and has made considerable progress in various fields, such as image recognition, speech translation, and activity recognition. typical model structures of deep learning include the autoencoder (ae) and its variants [cit], deep belief network (dbn) [cit], convolutional neural network (cnn) [cit], and recurrent neural network (rnn) [cit] . deep learning aims to learn the general underlying representation of input data [cit], thereby automatically learning essential features from raw data [cit] . this capability promotes the application of deep learning in the field of mechanical fault diagnosis [cit] ."
"the development of cnn can be traced back to the work of krizhevsky and hinton [cit], whose study demonstrated how deep cnn could substantially outperform other image classification methods. cnn can remarkably extract features directly from raw data [cit], thus reducing the dependence on expert knowledge during feature extraction and selection. in summary, cnn can eliminate the dependence on various advanced signal processing techniques and manual feature extraction [cit] ."
"another experiment validation was conducted on a dataset obtained from an automobile transmission gearbox, as shown in figure 12 . the gearbox consisted of one backward speed and five forward speeds. vibration signals were collected by an accelerometer. the third speed gear was used as testing gear for the wearing process. the tooth numbers of the driving and driven gears were 25 and 27, respectively. their corresponding meshing frequency was 500 hz. the loding was around 200 n·m. the rotation speed and the sampling frequency were 1600 r/min and 3000 hz, respectively. the running periods of the gearbox are listed in table 4 . four kinds of health conditions were used, namely, normal signal, slight wear, medium wear, and broken tooth, which were labeled as 1, 2, 3, and 4, respectively. each condition had 100 samples, as shown in table 5 . the waveforms of each condition are presented in figure 13 ."
"2-using metahieuristic methods for optimization solution such as vns proposed algorithm, we could reduce the time needed to complete process from initial state to find the best solution (determine the best number of cluster) to half time or less. 3-the proposed algorithm (vns) it affects the quality of clustering (performance) as it determines the closeness of items in the same cluster. 4-using swarm intelligence methods for optimization solution such as cuckoo search proposed algorithm, reduces the time which is needed to complete process from initial state to finding the best solution (determine the best number of cluster) to half time or less. 5-the proposed algorithm (cs) affects the quality of clustering (performance) since it determines the closeness of items in the same cluster."
"po lasso and group lasso. [cit] show that the po in (9) has an analytic solution for the lasso and group lasso penalties. the po is partitioned per coefficientβ j,i (lasso) or per group of coefficientsβ j (group lasso) and then the (group) soft thresholding operator gives the solution:"
"most of these methods, however, are developed for linear regression and specific data sets where all predictors are of the same type and thus the same type of penalty is applied to all parameters or coefficients. for example, the lasso is originally developed for linear regression with continuous predictors, using one parameter (or: coefficient) per predictor. however, many predictive problems require more general loss functions and rely on various types of predictors requiring different kinds of regularizationf applied to the coefficients. for example, the levels of a continuous or ordinal predictor (e.g. age) have a different structure compared to a spatial predictor (e.g. postal code), where a two-dimensional layout determines the relationship between the levels. this also applies to nominal predictors (e.g. type of industry) where the underlying structure is often predictor-specific. this level structure within a predictor needs to be taken into account when assigning regularization terms to coefficients, leading to different penalties tailored to the specific structure of the corresponding predictor. the use of such penalties enables the analyst not only to select the relevant predictors, but also to fuse levels within a predictor (e.g. clusters of postal codes or industry types). this fusion of levels is often challenging in large data sets where many predictors are present which may consist of many levels. an automatic selection and fusion strategy is then very helpful. [cit] who provide a regularization method which can simultaneously deal with binary, ordered and nominal predictors for linear models. this was later extended to generalized linear models (glms) [cit] . we inherit their formulation where the multi-type penalty acts on the objective function as a sum of subpenalties, one for each predictor type."
group lasso [cit] ]. the group lasso penalty uses an l 2 -norm to encourage the coefficients in β j to be removed from the model in an all-in or all-out approach:
"for each j, (9) is now a classical regularized linear model that only involves one type of penalty. we can then use the available statistical and machine learning literature to solve the different pos efficiently, as explained in section 3.2."
"partitioning methods relocate instances by moving them from one cluster to another, starting from an initial partitioning. such methods typically require that the number of clusters will be pre-set by the user. to achieve global optimality in partitioned-based clustering, an exhaustive enumeration process of all possible partitions is required. because this is not feasible, certain greedy heuristics are used in the form of iterative optimization. namely, a relocation method iteratively relocates points between the k clusters. the following subsections present various types of partitioning methods."
"cuckoo birds attract attention of many scientists around the world because of their unique behaviour. they have many characteristics which differentiate them from other birds, but their main distinguishing feature is aggressive reproduction strategy. some species such as the ani and guira cuckoos lay their eggs in communal nests, though they may remove others' eggs to increase the hatching probability of their own eggs. cuckoos engage brood parasitism. it is a type of parasitism in which a bird (brood parasite) lays and abandons its eggs in the nest of another species. there are three basic types of brood parasitism: intra-specific brood parasitism, cooperative breeding, and nest takeover. [cit] some host birds do not behave friendly against intruders and engage in direct conflict with them. in such situation host bird will throw those alien eggs away. in other situations, more friendly hosts will simply abandon its nest and build a new nest elsewhere. [cit] modified k-mean with cuckoo search algorithm"
"only hidden and output layer neurons possess activation functions. in general, values in the input layers are appropriately scaled. hence, input layers do not require any activation function. however, once these values are multiplied by weights and summed in the first hidden layer, they rapidly become larger than the range of their original scale, where activation functions become necessary. activation functions can force these large values back within the acceptance range and make them useful. consequently, this process affects the succeeding layer's input and thus the computation of new weights. lastly, activation functions affect the distribution of weights, which is back-warded through the entire network. therefore, the final output values of the cnn are influenced. accordingly, activation functions determine the output of the neural network from the given input dataset."
"based on the latter proposition, one can see easily that during the computation of the total work, and according to the new way we have:"
"the paper is structured as follows. section 2 explains the different lasso penalties. section 3 presents our optimization algorithm for a multi-type penalty. our approach is then applied to a simulation study in section 4 and to the motor insurance case-study in section 5. section 6 concludes. coefficients in β j . for simplicity, we assume that β is partitioned per predictor with the coefficients in β j corresponding to the levels used to code predictor j. for a continuous predictor, β j contains a single coefficient while (e.g.) for an ordinal predictor β j comprises as many coefficients as there are levels."
"to perform the testing perfectly, three databases form university of california irvine (uci) machine learning repository website [cit] (car, wine and zoo), in addition to two local databases (human resources and ic3 exams) was used."
"metaheuristics are a branch of optimization in computer science and applied mathematics that are related to algorithms and computational complexity theory. the past 20 years have witnessed the development of numerous metaheuristics in various communities that sit at the intersection of several fields, including artificial intelligence, computational intelligence, soft computing, mathematical programming, and operations research. most of the metaheuristics mimic natural metaphors to solve complex optimization problems (e.g., evolution of species, annealing process, ant colony, particle swarm, immune system, bee colony, and wasp swarm) [cit] ."
"the performance of the five cnn models in fault diagnosis is analyzed in this section. the average training and testing accuracies of the five models are shown in figure 9 . the training accuracies of all five cnn models are 100%. in the aspect of testing accuracy, the improved relu-cnns achieve the best results with values above 90%, which is the standard relu-based cnn's score. it obtains evidently higher testing accuracies than s-cnn, t-cnn, and the standard relu-cnn. [cit] iterations and test accuracy change curves during first 800 iterations of the learning process are drawn in figures 10 and 11, respectively. the overall trends of all five models' loss curves ( figure 10 ) are decreasing, reflecting the optimization of the models."
"the improved relu-cnns and relu-cnn exceed the best performances of s-cnn and t-cnn ( figure 11 ), and the improved relu-cnns yield higher accuracies at a faster speed than relu-cnn. in the beginning, the testing accuracies of these three models simultaneously increase exponentially and obtain the highest value at approximately 300 iterations. after 300 iterations, the accuracies of the improved relu-cnns remain stable with a slightly increasing trend. meanwhile, the accuracy curve of relu-cnn has noticeable oscillations, which may be due to that relu's tendency to easily oscillate or die during iteration. the improved relu-cnns overcome this problem by correcting the data distribution and retaining some information contained in negative axis values."
"the remainder of this paper is organized as follows. in section 2, lower bounds from the literature are presented. in section 3, the concepts of the classical and revisited energetic reasoning are briefly recalled. section 4 is dedicated to the new revisited energetic based lower bound for the hfsmt problem. an extensive computational analysis of the different lower bounds is presented throughout section 5. finally, a summary of our results and some directions for future research are indicated in the conclusion."
"one can choose to use the adaptive or the standardization penalty weights, or combine the objectives of both the adaptive and the standardization weights by multiplying them:"
"from the discussion in section ii part c, although relu can eliminate negative input values, it may discard some important information contained in these values, which then may furtherly generate some problems. when the input is negative, the gradient will be 0. then, the weights of the neural network cannot be updated and remain silent during the rest of the training process. the learning speed of the cnn becomes slow, and some neurons may become ineffective."
"in this paper, we have proposed a new destructive lower bound for the hfsmt, where the feasibility test is based on the concept of the revisited energetic reasoning. first, we have recall the classical energetic reasoning procedure, in addition to the revisited energetic reasoning concept for the parallel machine scheduling problem. after that, we extend the revisited energetic reasoning for the cumulative scheduling problems. this extension is applied to the hfsmt problem iteratively, starting from the first center to the last one, with the propagation of the adjustments for the window time (if found) for the other centers. this procedure allows us to obtain a destructive lower bound."
"we compare the estimated effects as obtained with smurf on the one hand and gam on the other hand in figures 8-10 . the dots and crosses in figures 8 and 9 show the parameter estimates as obtained with smurf before and after re-estimation respectively. the black lines represent the gam estimates for ordinal predictors in figure 8 while the black squares give the gam estimates for the binary, payfreq and coverage predictors in figure 9 . confidence intervals are given as dashed lines or segments respectively. [cit] we centered the smurf parameter estimates to ease the comparison with the gam estimates. smurf leads toβ containing 71 unique coefficients while the gam calculates 64 degrees of freedom, indicating a comparable model complexity. figure 8 (a) illustrates that young, inexperienced drivers report more claims on average and thus represent a higher risk for the insurance company. the riskiness then declines steadily and increases again at older ages. powerful cars (figure 8(b) ) also exhibit increased risk over less powerful cars. similarly, the model predicts a higher expected claim frequency for policyholders in a high bonus malus scale (figure 8(c) ) due to their claims history. [cit], agec is not considered in the analysis but smurf recognizes it to have some predictive power. especially for older cars, such as old timers, the expected claim frequency is lower. the parameter estimates obtained for the fused levels of the ordinal predictors in figure 8 follow nicely the behavior of the gam fit while greatly (and automatically) reducing the dimensionality compared to a standard glm. most parameters are estimated close or within the confidence interval of the gam fit. the coefficients before and after re-estimation are close to each other for wider bins and are relatively farther apart for smaller bins."
swarm intelligence is an important concept in artificial intelligence and computer science with emergent properties. the essential idea of swarm intelligence algorithms is to employ many simple agents applying almost no rule which in turn leads to an emergent global behavior.
"detailed technical details is beyond the scope of this paper, but the following provides an overview of the used technologies and the main components of the implementation. the system we developed draws together capabilities from microsoft's lync communication suite and lumia beamer screen mirroring applications. lync is microsoft's communication suite that supports video conferencing, instant messaging and screen sharing functionalities. while a mobile lync client is available, it does not support distributed screen mirroring capabilities. lumia beamer is an application available on nokia lumia phones that enables users to mirror the screen of their phone to another display through a regular web browser. by using the phone to scan a qr code presented in the browser, the application mirrors the screen of the phone in the web session. in addition, switching to the inbuilt smartphone camera application while mirroring, can effectively make the phone function as a wireless handheld web cam."
"involves that the job j cannot start processing at r j, for any feasible schedule. let h j be the starting time processing j, then the treated part"
"monthly loan payment, in eur, rounded to the nearest 100: 100-3000. figure c .20: zoomed-in boxplot of the auc for the binomial glm with a small ridge penalty and for the different settings settings of the smurf algorithm."
"converting traditional methods to artificial intelligence techniques has become an irreversible trend. intelligent diagnosis techniques are a hot research topic in the machine fault detection area [cit] . different intelligent diagnosis methods, such as artificial neural network (ann) [cit] and support vector machine (svm) [cit], have been widely applied in the past decades. nevertheless, these traditional machine learning methods have limitations in sensitive feature extraction from raw signals."
"fused lasso [cit] ]. to group consecutive levels within a predictor, the fused lasso penalty puts an l 1 -penalty on the differences between subsequent coefficients:"
"parameters and hyperparameters must be determined in cnn. hyperparameters, such as activation functions and structure parameters, may strongly influence cnn's performance. the activation function introduces linear or nonlinear transformation to the neural network. this function is crucial for obtaining successful results for cnn. although the choice of activation function has an important effect on the neural network, fewer studies are available on activation functions compared with neural network structures. usually, the activation functions in cnn are set as the traditional activation functions, such as the sigmoid function and tanh function."
"using this setup, we simulate 100 times a data set of 80,000 observations and a single holdout data set of 20,000 observations to be used for evaluating the performance of the models after the training and tuning process."
"clustering is useful in several exploratory pattern-analysis, grouping, decision making, and machine-learning situations; including data mining, document retrieval, image segmentation, and pattern classification. however, in many such problems, there is little prior information (e.g., statistical models) available about the data, and the decisionmaker must make as few assumptions about the data as possible. it is under these restrictions that clustering methodology is particularly appropriate for the exploration of interrelationships among the data points to make an assessment (perhaps preliminary) of their structure."
"extending smurf to other loss functions and penalties. in the current implementation of smurf, only lasso, group lasso and (generalized) fused lasso are available. this can be straightforwardly extended to the ridge and elastic-net [cit] ) penalties. another extension is the generalized lasso penalty, which replaces the graph structured matrix g(w j ) of the generalized fused lasso by an arbitrary matrix m . by construction, the associated proximal operator can be solved with the same admm algorithm used for the generalized fused lasso. this allows for more elaborate modeling options such as piece-wise polynomial regression or wavelet smoothing. additionally, the current implementation of our algorithm can handle the superposition of the lasso or group lasso with other penalties, all acting on the same subvector β j . examples of these in the literature are the sparse group or the sparse generalized fused lasso. however, these joint penalties need extra tuning parameters, making the model training more difficult. further theoretical work needs to be done to find efficient ways of choosing or tuning these extra parameters. additionally, smurf can be extended to handle other optimization problems such as cox regression, generalized estimating equations or m-estimators."
". together with the second penalty term, this shows us that the proximal operator in (8) is separable and solving it is equivalent to solving the subproblems"
"where c l is the output of the l th layer; a l is the input of this l th convolutional layer, which is also the output of the previous (l − 1) th layer; w l represents the weight vector from the (l − 1) th convolutional layer to the l th layer; b l denotes the bias vector of the (l − 1) th layer; * is the convolutional operation; f () is the activation function, which is discussed in section ii part b. the pooling layer, which is also composed of multiple features, typically follows a convolutional layer and further modifies the output of the layer. each characteristic surface of the pool uniquely corresponds to a feature surface of the former connected convolutional layer. the pooling layer then simply performs down-sampling along the spatial dimension of the given input, thereby further decreasing the number of parameters. the pooling layer generally uses the maxpooling operation, which reports the maximum output within a rectangular neighborhood. the pooling layer serves as the second stage in feature extraction; it reduces the dimension of the feature map and ensures that the scale of the feature remains unchanged."
"consequently, if l i is a lower bound on c max i, then l i is a valid lower bound on c max (i ) . the transformation of the instances via the dffs may improve the value of a given lower bound."
"as a final point we saw how participants were able to successfully negotiate among themselves the fluid transfer of control over the shared display. with the always-present availability of the barcode to control mirroring, participants were observed to vocalise their intention to share just prior to initiating the mirroring process. it was apparent in the timing and nature of these socially mediated requests that participants exhibited sufficient awareness of the ongoing work of the collaborating parties across sites to achieve such transitions relatively smoothly. because there were no explicit mechanics in the application to control the flow of screen mirroring or indicate who was currently sharing content, occasionally situations would occur where multiple participants would for instance try to share content simultaneously. however, keeping negotiation of control as part of the social interaction rather than an explicit function in the application was observed to be a strength rather than a needed feature that could easily complicate frequent switching between participants."
"the remainder of this paper is organized as follows. section ii illustrates the fundamental of cnn, including basic cnn and activation functions. section iii introduces the improved relu-cnns. sections iv and v present the experimental validations and discussions of bearing and gearbox fault diagnosis, respectively. finally, section vi elaborates on the conclusions of the study."
"the standardization penalty weights w (st) j,i−1 adjust for possible level imbalances, where some levels may contain more observations than others. extending (6) to the generalized fused lasso is possible by adding an extra factor, taking into account the number of regularized differences, relative to the fused lasso. for a predictor with p j levels, the fused lasso penalty contains p j − 1 terms. however, for the generalized fused lasso this number is determined by the number of edges r g of the graph g. [cit], we construct a penalty of the same order as the one used in the fused lasso by multiplying the standardization penalty weights in (6) by a factor p j −1 r g, see table 1 . without this factor, applying a generalized fused lasso with large r g would make the penalty artificially larger compared to the fused lasso, only because there are more regularized coefficient differences. the extra factor reduces to 1 for the fused lasso and to 2 p j when all pairwise differences are regularized."
"second, the cnn's depth and number of fully connected layer nodes were explored. the results are shown in figure 8 . cnn models of different layers perform differently. 6-layers cnn and 7-layers cnn both perform well, but 6-layers cnn demonstrates more stability than 7-layers cnn. meanwhile, the performance of 5-layers cnn and 8-layers cnn exhibits some random changes. because a cnn structure that is extremely shallow or deep may not correctly represent the underlying relationship between the input and its corresponding output. for the fully connected layer, choosing 160 nodes is the best selection whether for 6-layers cnn or 7-layers cnn."
"the general algorithm for the destructive lower bound is provided in algorithm 1. since the hfsmt problem is a symmetric one, the same procedure obtaining lb d, is applied to the reverse problem(from z k down to z 1 ) and the resulted lower bound is noted lb"
"while screen mirroring capabilities offer important value in video mediated collaborations, the current set-ups are not without their limitations. for example, it is common in many everyday conferencing situations for one site to use a single host computer to connect to another host computer at a remote site. it is common for multiple collocated participants to be present at either site but in hosting on a single computer, the access to screen mirroring capabilities can be restricted to the person driving the host computer. while it is possible to swap control to another participant, this process is cumbersome and ultimately inhibits more casual and ad hoc artefact sharing [cit] . related to this concern, these collocated participants may be working with a broader ecosystem of mobile devices such as laptops, tablets and smartphones as well as a variety of physical information surfaces like whiteboards and paper. such personal devices have significance in that they may contain information arising from individual work performed prior to the video call as well as supporting any individual or subgroup work being performed in the meeting in parallel to the shared aspects of the work -both of these being potential contributions to subsequent shared activities in the meeting. while there are possibilities for sharing from some of these devices, the mechanisms are again cumbersome and inhibitory. for example, it is possible that these devices actively join the video meeting as an additional participant and then proceed to mirror the screen from there. but this is sufficiently effortful to be a barrier. as argued by mueller-tomfelde and o'hara [cit], such effort also incurs certain social consequences. going through these processes entails \"taking the floor\" in a strong way in which the contributions of the sharing and talk need to be sufficient to justify the interruption. in this sense, these cumbersome processes create a social barrier to more casual sharing."
"where w j is a penalty weighting factor. in contrast to the l 1 -norm, the l 2 -norm is not separable for each coefficient in β j and is only non-differentiable when all β j,i are 0. this penalty is appropriate to determine if β j has adequate predictive power as a whole, because the estimates for β j,i will be either all zero or all non-zero. when β j consists of only one coefficient, the l 2 -norm reduces to the l 1 -norm and the standard lasso penalty is retrieved. this group lasso penalty is particularly useful for selecting ordinal or nominal predictors."
computing the total work according to the new way and the adjustments are refer to as the revisited energetic reasoning for the cumulative scheduling problem.
"α (k+1) depends on the previous iteration and gets larger in every iteration, i.e. bigger leaps are taken. they can lead to faster convergence, however they do not necessarily monotonically decrease the objective function unlike (standard) gradient descent methods. o'donoghue and candès (2015) indicate that this non-monotone behavior occurs when the momentum exceeds its optimal value [cit] . therefore, they propose to restart the momentum after a fixed number of iterations or if the objective function increases. we choose to use the latter restart scheme which is adaptive and easy to implement since the values for the objective function are readily available. o'donoghue and candès (2015) show that using adaptive restarts leads to a convergence rate close to the optimal convergence which is obtained with the problem specific, and hard to calculate, optimal series for α (k) ."
"in this section, the improved relu-cnns are proposed. improved relu-cnns are cnns utilized with improved relu activation functions, which contain leaky relu (lrelu) and parametric relu (prelu). the details are as follows."
"to guarantee the usefulness of the processing values, activation functions must be 1) nonlinear and 2) continuously differentiable or at least nearly completely differentiable. most neural networks are optimized with some form of gradient descent. therefore, a continuously differentiable function is necessary for gradient-based optimization methods and allows efficient backpropagation of errors throughout the network. theoretically, any function can be used as an activation function as long as it meets the above requirements. however, only a few such functions are used in practice. in a traditional cnn, activation functions generally use saturated nonlinear functions (saturating nonlinearity), such as sigmoid and tanh functions. unlike saturated nonlinear functions, unsaturated nonlinear functions (non-saturating nonlinearity) can solve the gradient explosion and disappearance problems of saturated functions. in addition, unsaturated nonlinear functions can enhance the convergence rate and are theoretically suitable for neural networks."
"in this paper, we have presented a system to enable wireless screen mirroring from smartphones to shared displays and across distributed settings. key here is the integration of these mirroring capabilities within a video conferencing application that lends mirroring access mechanisms both across sites and to collocated participants within a site. what we see is how this extends the ecosystem of devices from which ad hoc wireless screen mirroring can be achieved within a video call in ways that exploits their unique affordances. of note here is the lightweight way in which the camera ecosystem of the video call can be extended through the camera capabilities of the smartphone."
"solving directly (p 2 ) is a time consuming procedure since it is an np-hard problem, therefore a relaxed version is proposed. first, the problem (p 2 ) is transformed into an equivalent problem (p 3 ). second, some constraints are relaxed. this relaxation, which refers to as the knapsack based relaxation, yields two independent knapsack problems. firstly, observing that:"
