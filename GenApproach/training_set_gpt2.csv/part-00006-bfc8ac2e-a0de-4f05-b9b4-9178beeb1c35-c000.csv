text
"finally, the algorithm detects zero crossings of the heartbeat signal s hb in order to find heartbeat (r wave) positions necessary for bbi calculation (fig. 4(f) )."
"the flowchart of the proposed algorithm for bbi extraction is presented in fig. 3 . intermediate results of different algorithm stages for the simulated chest wall displacement x(t) defined in section ii-b are illustrated in fig. 4 . the first step is the band pass fir filtering (bpf) of discrete in-phase and quadrature components. this step attenuates low frequency breathing effects and extracts higher heart rate harmonics, as illustrated in section ii-c. an example of i and q channel signals for the simulated chest wall displacement is presented in fig. 4(b) and corresponding filtered outputs are presented in fig. 4(c) . instantaneous powers of filtered i and q signals are summed to represent the influence of both channels (signal p iq in fig. 3 ). the combined instantaneous power is further averaged by applying a moving average filter with a short impulse response (time window) of 0.4 s (signal p ma, 0.4 in fig. 3 ), since the duration of a normal qt segment in the ecg is less than 0.42 s [cit] . the amplitude of the resulting p ma, 0.4 signal varies because of breathing effects, as shown in fig. 4(d), across different subjects or different radar positions. in order to reduce these variations, a simple automatic gain control (agc) block is implemented to obtain the normalized power signal (signal p norm in fig. 3 ). the normalized power signal is calculated as the ratio of signals p ma, 0.4 and p ma, 1.5, where p ma, 1.5 is the moving averaged combined instantaneous power, obtained by a moving average filter with a longer impulse response of 1.5 s. the example of signal p norm is shown in fig. 4(e) ."
"where a t and f are the amplitude and the frequency of the transmitted signal, respectively; θ(t) is the phase noise of the local-oscillator. since the target is moving, the reflected signal r(t) is the phase-modulated signal expressed as"
"the overall delay of the proposed algorithm, defined as the time from the occurrence of a single heartbeat until its detection, is 2.5 s. it originates mainly from the group delay of filters in the processing chain. group delays of individual blocks are given in fig. 3 ."
"in this paper a novel accurate algorithm for the estimation of hrv features using 24 ghz continuous-wave doppler radar is presented. the proposed algorithm combines frequency and time domain analysis to accurately estimate bbis in real-time and to calculate four hrv features. unlike many other approaches, the proposed method directly uses in-phase and quadrature radar signals for the bbi estimation without 74722 volume 7, 2019 any displacement demodulation techniques, hence providing negligible sensitivity to dc offset and i/q imbalance. the algorithm is not intensive computationally, which makes it capable for real-time performance, even on platforms with limited hardware resources. furthermore, the total heartbeat detection delay is small, which is of crucial importance in applications that require fast bbi detection."
"hrv features reflect changes in time intervals between two consecutive heartbeats, the so called beat-to-beat intervals (bbis). conventionally, hrv monitoring systems are based on the usage of contact sensors such as ecg (electrocardiogram) sensors, ppg (photoplethysmogram) sensors,"
"in table 3, results of the proposed algorithm are compared to the results from relevant previously published papers that dealt with hrv measurements and short time window heart rate determination. the approach presented in this paper has a lower error than any other considered method with similar duration of the time window."
"where the sd is the standard deviation of all differences. the analysis shows high agreement between radar-based and ecg-based bbis for subjects with exceedingly various average heart rates (48.2−88.1 bpm), which makes this testing more reliable. table 2 shows the calculated heart rate variability (hrv) features based on the bbis extracted from the doppler radar signals and from ecg signals. fig. 14 shows the bar chart of calculated sdnn, rmssd, lf and hf hrv features, and also the lnlf and the lnhf parameters, calculated as the natural logarithm of lf and hf normalized to 1 ms 2, which can be used in some applications as well [cit] . results show high agreement between hrv features obtained from ecg and doppler radar signals in the case of the bp fb approach for bbi extraction. czt based hrv extraction yields less accurate results than the extraction from the bp fb output signal, especially for the hf parameter. this confirms the significance of the bp fb in the processing chain, even if that is not clearly evident from the results in table 1 . it is also important to notice that test subjects have various hrv features within a wide range of typical values for healthy adults [cit], which once again confirms the reliability of the testing."
"the coarse estimation of the heart rate is performed using the chirp z-transform (czt) algorithm [cit] applied to the autocorrelation of the decimated signal p norm . the signal is decimated by using the anti-aliasing filter and downsampling by a factor of 50 to avoid intensive computing (fig. 3) . the autocorrelation is used in order to highlight the signal's periodicity before the czt frequency estimation. the data block needed for coarse heart rate estimation should include at least two heartbeats in order to detect any periodicity. its duration is empirically set to 3.5 s. the coarse estimation can be used for the bbi extraction, but gives unsatisfactory error in hrv feature extraction."
"in-phase and quadrature components in all three cases. the simulated carrier frequency is set to 24 ghz. the sampling rate is set to 1 khz. frequency analysis of all these sets is performed using the window pre-sum-fft method [cit] in order to reduce spectral leakage. zoomed frequency spectra for simulated in-phase and quadrature components are shown in fig. 2(a) and fig. 2(b), respectively. it can be noticed that there is a part of spectra where higher harmonics of heartbeat frequency components are dominant compared to breathing frequency components. therefore, application of band pass filtering directly on in-phase and quadrature components can be efficient for attenuating the breathing effects. the amplitude frequency response of one such band pass filter is shown in fig. 2(a) and fig. 2(b) ."
"the goal of this research was to enable the usage of simple radar architecture and low power compact radar sensor for real-time and fast single person hrv extraction, which is suitable in applications such as health care monitoring, sleep monitoring or drowsiness detection. even though the cw doppler radar cannot detect absolute distance, the previous research [cit] shows that the absolute distance information is not necessary in the bbi detection."
"after i/q demodulation, in-phase and quadrature components are sampled using analog-to-digital converters (adcs). the last digital signal processing (dsp) stage usually includes the following steps: 1) correction of in-phase and quadrature discrete components (removing dc offsets [cit], i/q amplitude and phase imbalance correction [cit] ), 2) demodulation techniques applied on corrected in-phase and quadrature discrete components for the displacement x(t) extraction (arctangent demodulation [cit] or the extended differentiation and cross-multiplication (dacm) demodulation [cit] ), and 3) estimation of vital signs (respiration rate and heart rate) and their features from the displacement x(t). the first step requires a calibration process and/or real-time correction. the algorithm for hrv feature estimation presented in this paper is applied directly to noncorrected in-phase and quadrature components (expressed in (3) and (4)), which eliminates the need for steps 1) and 2)."
"in this paper, a novel method for the extraction of hrv features using the doppler radar technology is presented. the proposed method is capable for real-time applications and provides a small delay from the occurrence of the heartbeat until its detection (∼ 2.5 s). it uses the frequency domain analysis for coarse estimation of the heart rate frequency and the narrowband band pass filtering for further refinement. moreover, the algorithm is virtually insensitive to the i/q imbalance, since it does not use any demodulation technique for displacement extraction. the method has been validated on a group of ten subjects with a wide range of heart rates. multiple error metrics and bland-altman analysis have shown a high level of agreement between bbis extracted from the doppler radar signals and from the reference ecg signals. the mean relative error for the coarse estimation of bbis in the frequency domain was less than 5.21 %, but with the addition of the band pass filter bank for bbi extraction refinement, the mean relative error became lower than 2.07 % for all tested subjects. the obtained results show that the proposed method outperforms all of the previously published methods considered herein that dealt with the hrv extraction and heart rate extraction in short time intervals. high-accuracy bbi extraction enables high-accuracy estimation of two time-domain (sdnn, rmssd) and two frequency-domain (lf, hf) hrv features."
"the subjects remained in the sitting position and were breathing normally for 3 min. two systems were used for data acquisition: the doppler radar system and smartex wearable wellness system (wws, pisa, italy) as the reference system. smartex wws system consists of a belt with ecg and respiration sensors, and a microcontroller that sends data to a remote pc via bluetooth connection. bbis are extracted from the ecg using the pan-tompkins algorithm [cit] . the doppler radar system was placed in front of the subjects, at a distance of 75 cm, as in fig. 7 ."
"various research works have suggested methods for estimation of average heart rate using doppler radar technology [cit] . however, high-accuracy hrv extraction is much more demanding than an average heart rate extraction and requires better estimation of bbis. the duration of bbis in a healthy subject changes significantly over time, because of the respiratory sinus arrhythmia [cit], emotions of the subject, or their overall physical state [cit] . therefore, algorithms that accurately detect moments in which heartbeats occur are needed for correct hrv feature estimation."
"simulation model, described in section ii-b, showed that it is possible to extract heartbeat information directly from in-phase and quadrature signals. additionally, the model is used for testing the algorithm performance in the presence of noise and i/q imbalance in several different scenarios. all algorithm parameters in those tests are set to values described in the section iii-a. fig. 10 shows the influence of noise on algorithm accuracy. this influence is tested for six different simulated breathing rates (br). the bbi sequence for breathing rate of 8 bpm is shown in fig. 10(a), as an example. the peak-to-peak amplitude of changes in the bbi sequence, which comes from the simulated respiratory sinus arrhythmia, is set to 150 ms for all brs. in order to achieve the worst-case scenario that corresponds to the physical range described in the section ii-b, the peak-to-peak amplitude of the breathing displacement is set to maximal 12 mm, whereas the heartbeat displacement is set to minimal 0.2 mm. fig. 10(b) shows that the minimum snr needed, for less than 3% mean relative error, is 20 db. the mean relative error is larger for larger values of the breathing rate, which can be explained by good coarse estimation of lower heartrates capacity that further influence the total error performance."
"the chest wall displacement x(t) was modeled as a sum of the breathing displacement x b (t) and the much smaller heartbeat displacement x h (t), neglecting the body movement displacement as in"
"a wide range of carrier sine wave frequencies in the cw doppler radars has been used in the literature. lower frequency radar systems allow easier extraction of the displacement from the radar signal using the small angle approximation [cit] . higher frequency radar systems have better sensitivity for displacement estimation, which is crucial in the case of small displacements such as chest wall movements caused by heartbeats. moreover, lower frequency electromagnetic (em) radiation penetrates the exposed tissue significantly [cit], whereas the em radiation with frequencies higher than 10 ghz is absorbed at skin surface [cit] ."
"considering previous assumptions, after testing the algorithm performance on the real and simulated data for multiple combinations of algorithm parameters in order to achieve the best performance, the parameters (bandwidths of all band pass filters, moving average filters duration, duration of data block used in the coarse heartrate estimation, parameter f offset in (9) and number of band pass filters in the band pass filter bank) are chosen empirically."
"four hrv features are extracted from the bbis: standard deviation of bbis (sdnn), the root mean square of successive differences (rmssd), the low frequency power (lf), and the high frequency power (hf) [cit] ."
"high-accuracy monitoring of the heart rate variability (hrv) features is required in numerous applications, such as hospital and in-home health care [cit], stress and emotions recognition [cit], vigilance monitoring [cit], anxiety treatments [cit], training optimization [cit], etc. real-time monitoring is desirable in most of these applications."
"the algorithm is not compute-intensive. the duration of processing 180 s of data in matlab (mathworks, usa) using a moderately powerful pc with the intel core i5-2320 processor and 8 gb of ram is only 0.54 s (averaged for 1000 runs). low computation complexity indicates that the algorithm is suitable for real-time operation."
where n bbi is the number of all detected bbis and t bb is the mean value of all bbis. lf and hf features are calculated from the power spectral density (psd) of the bbi sequence by integrating the psd function within the corresponding frequency band (0.04−0.15 hz for lf and 0.15−0.4 hz for hf). power spectral density is calculated by using the fast fourier transform (fft) approach.
"further in this paper, section ii describes the theoretical background of the doppler radar architecture used in the paper and the adopted simulation model of the chest wall displacement caused by heartbeats and breathing. additionally, simulated doppler radar signals are analyzed in frequency domain. section iii describes the proposed algorithm for hrv monitoring. results obtained from the simulated data and data measured in a human subject study are presented and discussed in section iv. finally, the conclusion is given in section v."
"continuous-wave (cw) doppler radar transmits a single frequency sine wave continuously towards a moving target and receives the reflected wave in order to determine the displacement x(t) of the target. the displacement x(t) summarizes all motions that the target makes (respiration, heartbeat and body motions) [cit] . in the absence of large-scale body movement, the displacement x(t) originates only from the chest wall displacement due to breathing and heartbeats. fig. 1 shows quadrature architecture of the cw doppler radar system for vital signs monitoring. the carrier sine wave is generated by the local-oscillator (lo) and amplified using a power amplifier (pa). the transmitted signal t (t) is expressed as"
"the displacement due to breathing x b (t) is modeled as per [cit], where x b (t) is the low-pass filtered periodic sequence of quadratic inspiration and exponential expiration waveform w(t), expressed as"
"where f coarse is the heart rate frequency calculated in the coarse hr estimator, where f offset is set to 0.9 hz and f is set to 0.1 hz (generally, f depends on the number of filters in the filter bank). small discontinuities may occur when switching from one bp fb output to another. to mitigate these effects, the additional moving average filter is implemented at the bp fb output, producing the so-called heartbeat signal s hb, which can be used for high-accuracy bbi extraction."
"previous investigations in this field have used different approaches for error calculation. in order to compare results of the proposed algorithm with the literature, three different ways for error calculation are implemented:"
"radar technology is one of the most promising methods for non-contact heart rate measurement [cit] . based on their hardware architecture, radars used in vital signs detection field are usually classified into the following classes: continuous-wave (cw) doppler radars [cit], frequency modulated continuous wave (fmcw) radars [cit], and impulse radio ultra-wideband (ir uwb) radars [cit] . ir uwb radars emit short wideband pulses that are reflected from the target back to the receiver. since the transmitted pulses are wideband, the receiver must be capable for very high-speed operation, especially in short-range applications, like in the vital signs detection where the period between the pulse transmission and the pulse reception is extremely short. analog-to-digital converter (adc) used in such applications needs to support high sampling rate. all this leads to high power consumption by the device [cit] . additionally, ir uwb radars are usually not sensitive enough for heartbeat detection [cit] . cw and fmcw radars emit sinusoidal electromagnetic wave, reflected from the target, continuously. cw doppler radars use a single frequency wave modulated by the target's movement, which makes them capable for monitoring relative displacement only. this means that any target's displacement in the radar's field of view (fov) affects the measurement. therefore, doppler radars have difficulties with several simultaneously illuminated targets. on the other hand, fmcw radars can manage both absolute and relative target's displacement detection, based on the frequency and phase difference between the transmitted and reflected chirp signal. as a result, fmcw technology enables detection of vital signs of more than one person [cit] . however, fmcw radars have significantly more complex hardware architecture than the simple cw radars, thus they consume more energy from the power supply, and require more complex signal processing methods for the displacement extraction."
"in future, the plan is to design radar antennas with higher directivity and use them for radars with even higher frequencies than 24 ghz in order to obtain better results. higher directivity antennas would detect fewer random body movements, thus providing better error performance. moreover, the radar system with even higher carrier frequency would be more sensitive to small chest displacements such as heartbeat displacement. if so, the system should be able to detect bbis and hrv features even more accurately."
"in (12), (13), and (14), n bbi is the number of detected bbis, t bb,radar is the bbi extracted from the doppler radar signal, and t bb,ecg is the bbi extracted from the ecg signal. table 1 shows the results of bbi extraction for both approaches presented in section iii: the czt based bbi extraction, used for coarse bbi estimates, and the bp fb based approach. the algorithm parameters are the same for all test subjects. results are compared with those obtained from the reference ecg signal. the mean relative errors for czt and bp fb approaches are 1.83−5.21 % and 1.02−2.07 %, respectively. the error is lower when bp fb is used for all subjects."
"the study included ten healthy subjects (seven male and three female, aged 26.1 ± 5.8, with a height of 178.5 ± 10.9 cm and a weight of 71.6 ± 16.3 kg) who did not suffer any cardiac or other chronic disease. all participants have signed informed consent to the study approved by the local ethical committee."
"the associate editor coordinating the review of this manuscript and approving it for publication was roberto gomez-garcia. piezoelectric or piezoresistive sensors [cit] . however, the usage of contact sensors has several disadvantages such as limited mobility, discomfort caused by sensors placed on the body and by patients' awareness of the measurement being performed, skin irritation and allergic contact reactions (in the case of infants or sensitive skin in adults), inapplicability on damaged skin (painful skin rashes, burns, hives, etc.), inability to measure through clothes or other obstacles. non-contact measurements can overcome these problems."
the advantage of using narrowband bp fb for bbi estimation is illustrated in fig. 6(a) . simulated bbis in a 100 s time interval are compared to bbis extracted using the coarse estimation and bbis extracted using the bp fb output. bbis are simulated using the model described in section ii-b that integrates variability due to respiratory sinus arrhythmia and small variability due to changes of overall physical conditions. the averaging nature of the czt reduces the ability for detecting all bbi changes. the use of bp fb overcomes this issue and provides a more accurate bbi estimation. fig. 6(b) shows the changes of the active bpf index corresponding to fig. 6(a) . it can be observed that high-accuracy bbi estimation benefits from multiple band pass filters with different central frequencies.
"we first updated the wdsp tool, which relies on a wd40-specific position weight matrix (pwm) and psipred [cit] as backends. we have expanded the experimental structures from 33 to 65 for generating the pwm. meanwhile, the psipred has been replaced from v3 to v4. with these improvements, the updated wdsp has outperformed its previous version and other general-purpose domain annotations, as measured by the 5-fold cross-validation f1 score (supplementary material)."
"1. each image contains at least one occluded pedestrian. 2. datasets caviar and ethz are video sequences with high frame rate, e.g. 25 frames per second for caviar. in these datasets, the current frame may be very similar to the next frame. in our dataset, the frame rate is reduced to ensure variation among selected images."
"each pedestrian is labeled with a bounding box and a tag indicating whether the pedestrian is occluded or not. since a lot of occluded pedestrians in datasets like inria, ethz and tub-brussels are not considered positive testing samples, the occluded pedestrians are relabeled in our dataset. occluded pedestrians have been labeled in caltech dataset, their labels are unchanged in our dataset."
"however, to the best of our knowledge, the field of wireless ad hoc networks with nodes that have different transmission ranges in three-dimensional space has not been studied. in this paper, we study the mscdas problem of a wireless ad hoc network with different transmission ranges in three-dimensional space. note that the mscdas problem is also np-hard because the mcds problem in ubg is np-hard and because ubg is a special situation of a ball graph (bg). e remainder of this paper is organized as follows. section 2 introduces some basic definitions and terms. section 3 separately calculates an improved upper bound for maximal independent sets in ubgs and bgs. en, we present an algorithm to construct an scdas in section 4. section 5 provides the simulation results with different parameter settings. finally, in section 6, we summarize this paper."
"let g � (v, e) be a strongly connected ball graph and l x,c be a set of independent nodes in n + (x) and not in c. to obtain a das, we first use a greedy method to find a ds s d . second, we consider another strongly connected ball graph"
"most existing datasets are not specifically designed for evaluating occlusion handling. for example, in the caltech dataset, only 105 out of 4250 images for evaluation have occluded pedestrians. if such datasets are used for evaluation, it is not clear how much improvement comes from occlusion handling or other factors. in order to specifically compare pedestrian detection algorithms under occlusions, we construct the cuhk occlusion dataset that mainly include occluded images. this dataset contains 1063 images from the datasets of caltech, ethz, tud-brussels, inria, caviar and our recorded images from surveillance cameras. the composition of the dataset is shown in table 1 . images are strictly selected according to the following criteria."
"most previous approaches [4, 10, 29, 32 ] rely on the detection score of a part for estimating its visibility. however, part detectors are imperfect and such estimation is inaccu-rate. take the pedestrian in fig. 1 as an example. although the part of left-head-shoulder is visible, its part detection score is relatively low because its visual cue in the image does not fit the part detector well. although the part of leftleg is invisible, its part detector finds a meaningless falsepositive window on the baby carriage with a relatively high detection score. if the detection scores of parts are directly used for estimating visibility, the pedestrian will be wrongly estimated as having left-head-shoulder invisible and left-leg visible."
"finally, although the above discussions focus on occlusions, the proposed framework is also effective to handle abnormal deformations to some extent. if some parts are abnormally deformed and cannot be detected by part detectors, they can be treated as occlusions and removed from the integration of parts."
"wdspdb 2.0 is based on uniprotkb (release july 5, 2017), a more recent version, and an optimized curation pipeline allowing the inclusion of more non-canonical wd40 proteins. as a result, the data coverage is about 10 times of wdspdb 1.0. in brief, it contains 594 319 wd40 proteins with 4 033 034 repeats from 4426 species. among these proteins, 852 295 potential side-chain hydrogen bond networks and 4 963 216 ppi hotspots were predicted. specifically, from clinvar [cit], cosmic [cit], intogen [cit], cbioportal [cit], intact [cit] ), 1000 genomes (1000 [cit] and exac [cit], we have mapped to 252 wd40 proteins 37 184 variants, which are pathogenic, cancer-related, cancer-driver, cancer highly recurrent, ppi-influencing or neutral. wdspdb 2.0 comprises almost all of the entries in wdspdb 1.0, and only a few entries are exclusive in wdspdb 1.0 due to entry merging, removing and renaming in the process of uniprotkb updates (fig. 1b and supplementary material). as expected, the intersection of wdspdb 2.0 and 1.0 mainly belongs to the 'high' confidence category, and most newly added entries are assigned to other confidence categories, since the new pipeline has adopted looser inclusion criteria. many proteins that are widely considered as wd40 proteins but absent in wdspdb 1.0 have been included in wdspdb 2.0, such as lrrk2, palb2 and apaf1. taken together, wdspdb 2.0 is much more comprehensive regarding the record number and annotation information."
"for an undirected graph h � (v 1, e 1 ), when an approximation algorithm for the mcds of h is considered to obtain a cds with a performance ratio with respect to mcds, generally, there are two steps. e first step is to find an mis m in h, which is a domination set of h. e second step is to find some nodes in v 1 − m to connect the nodes in m to obtain a cds. however, for a directed graph g � (v, e), when we design an approximation algorithm for an scdas of g with a performance ratio with respect to mscdas, it is not sufficient that there are only two steps as in the above method for a cds. e main reason for this is that for a directed graph g, an mis is not necessarily a dominating set. in this paper, we use the following three steps to obtain an scdas of g with a better performance ratio to mscdas. e first step is to find a dominating set m for g. e second step is to locate some nodes in v − m and add them to the dominating set m such that it becomes a cds s d for g. e third step is to reverse all edges in g to obtain a new directed ball graph g � (v, e). a similar method to the first and second steps is used to obtain a connected dominating set s a of g, which is a connected absorbing set for g. en, s d ∪ s a, an scdas of g, is obtained. for the sake of convenience, let us first discuss the upper bound of the size of mis for a unit ball graph g � (v, e)."
"object detection is a fundamental problem in computer vision with wide applications such as surveillance, image retrieval, robotics and intelligent vehicles. since pedestrian detection is one of the most important topics in object detection, it has attracted much attention in recent years."
"in this paper, we mainly study the problem of constructing mscdass in a directed strongly connected ball graph with different transmission ranges for its nodes, which is np-hard for obtaining an optimal solution. to obtain a constant factor approximation solution for mscdass in a strongly directed connected ball graph, we proposed an algorithm that produces an scdas by computing a dominating set and an absorbing set. we proved that the dominating set and absorbing set are independent sets. to obtain the ratio of the scdas to mscdas, we first proved that the upper bound of the number of nodes in mis in a directed strongly connected ball graph is ((319/15)k 3 + (116/5)k 2 + (29/5)k)opt + ((29/ 3)k 3 + (116/5)k 2 + (87/5)k + (13/15)). using the upper bound, we derived that the size of the scdas generated by the algorithm proposed in the paper does not exceed ((319/15)k 3 + (116/5)k 2 + (29/5)k)opt + ((29/3)k 3 + (116/5) k 2 + (87/5)k + (13/15))."
"for the sake of convenience, we introduce some background information, including special terms in graph theory, which will be used in the paper. a wireless ad hoc network n, in which each node has a transmission range, can be denoted by a directed graph"
"many results related to the problem of computing the mcds can be found in references [6, [cit] . note that the previous references about mcds are based on the unit disk graph in the two-dimensional plane."
"many classification approaches, features and deformation models have been used for achieving the progress on object detection. the classification approaches widely used include various boosting classifiers [cit], linear svm [cit], histogram intersection kernel svm [cit], latent svm [cit], multiple kernel svm [cit] and structural svm [cit] . the investigation on features includes haar-like features, histogram of gradients (hog), integral histogram, color histogram, gradient histogram, covariance descriptor, local binary pattern, features learned from deep model, depth, segmentation and motion [cit] . recent deformable models for object detection mainly model the translational deformation of parts [cit] ."
"note that for two independent nodes u and v in g, there exist two corresponding packing balls with radius 0.5 and centers u and v, respectively, denoted by b u and b v, in g, and they are not adjacent. in other words, there is space between b u and b v . to derive a better upper bound of the size of the maximal independent set in the connected unit ball graph g, we use a rhombic dodecahedron, which surrounds a ball with radius 0.5 to replace each packing ball with radius 0.5 in g. e reasons are described as follows."
"this paper describes a probabilistic framework for pedestrian detection with occlusion handling. it effectively estimates the visibility of parts at multiple layers and learns their relationship with the proposed discriminative deep model. since it takes the detection scores of parts as input, it is very flexible to incorporate with new features and other deformable part-based models. through extensive experimental comparison on multiple datasets, various schemes of integrating part detectors are investigated. our approach outperforms the state-of-the-arts especially on pedestrian data with occlusions."
"along with the rapid development of wireless radio communication technologies, embedded sensors, and vlsi, the cost of establishing a wireless ad hoc network is decreasing and the performance of wireless ad hoc networks is improving. recently, wireless ad hoc networks have been widely used in many areas, such as disaster rescue, environmental monitoring, military operations, and mobile computing (see [cit] ), where it is difficult to build a physical backbone for the network; it can be expected that wireless ad hoc networks will play an increasingly important role in the communication of future networks."
"hence, the distance between l 1 and l 2 does not exceed two hops. □ lemma 11. after unidtree (algorithm 2) is executed, the following conditions hold:"
"in this section, we introduce an efficient heuristic algorithm to construct an scdas for a directed ball graph g � (v, e). e idea of this algorithm is described as follows. we choose a node s with the largest degree in g and construct two node sets s d and s a by calling a subroutine called uniditree such that s � s d ∪ s a is an scadas for g, where uniditree is a subroutine, which can generate a rooted tree for g."
"the wd40-repeat proteins are a subfamily of b-propellers, and their sequence and structure relationships and association with diseases have been widely studied [cit] . as one of the most popular interactors in protein-protein interaction (ppi) networks, they act as scaffolds to assemble various molecular machineries, and play versatile roles in fundamental biological processes including signal transduction, ubiquitination, cell cycle control, etc. [cit] . obtaining their structural information is the key to revealing their interacting details and thus to understanding their biological functions and to obtaining insights to the underlying pathogenic mechanisms, but available experimental structures are heavily lacked regarding their abundance in eukaryotic proteomes."
"wdspdb 2.0 has incorporated significant improvements. the version 1.0 is confined to typical wd40 proteins only, but users have frequently requested annotations of atypical ones. this update recorded as many as possible putative wd40 proteins with more accurate structure predictions, and has assigned confidence levels to meet requirements of customized usages. the integration of variant data will enable the direct and intuitive exploring of the relationship between variants and featured sites in the structural context. the web interface is also largely enhanced for better browsing, visualization, and downloading. we will regularly update wdspdb to continuously benefit the researchers in the fields of repeat proteins, ppis and genetic variants interpretation."
"according to lemma 11, there exists a directed path p sv � s ⟶ · · · ⟶ v from root node s to node v in g, and another directed path q sv exists from root node s to node v in g. we reverse all edges in q sv, and we obtain a directed path, denoted by q vs, from node v to root node s in g."
"we re-implemented the web interface using django to provide cleaner and more organized browsing experiences. it adopts a powerful table plug-in that enables customized data display and download in multiple formats, and has replaced the visualization tool to ngl viewer [cit] for faster loading and smoother operation. a rest service has also been implemented for downloading the secondary structure annotations. in addition, we deployed the updated wdsp tool with options of parameter tuning (the searching database and the iterative times), which would provide predictions for users' own sequences."
"let d i with the center e i and d j with the center e j be two connected unit balls (see figure 1 ). let d i and d j be two balls with radius 1.5 and centers e i and e j, respectively (see figure 2 ), where d(e i, e j ) � 1. let b 1 � d i ∪ d j and b 2 � d i ∪ d j . e following result is obvious."
"parts model. our parts model consists of 3 layers that have different sizes of parts as shown in fig. 4 . parts at the bottom layer have the smallest size, and those at the top layer have the largest. a part at upper layer is composed of its children in the lower layer. the top layer is the possible occlusion statuses. gray color indicates occlusion. the other two layers are body parts. an occlusion status is obtained by combining one or several parts in the middle layer. the leftmost part, i.e. head-shoulder, appears twice (representing occlusion status at the top layer and part in the middle layer respectively) in this figure because this part itself can generate an occlusion status."
"the training algorithm is to learn the visibility correlation w l, detection score weight g l and bias c l in (7), with two stages. at stage 1, the parameters are trained layer by layer and two adjacent layers are considered as an rbm that has the following distributions:"
"this paper is motivated by the fact that it is more reliable to design overlapping parts at multiple layers and verify the visibility of a part for multiple times at different layers. the detection score of one part provides valuable contextual information for the estimation on its overlapping parts. take the pedestrian in fig. 1 as an example. the left-head-shoulder and head-shoulder are overlapping parts at different layers. similarly for the left-leg and the twolegs. the part of head-shoulder has a high detection score because its visual cue in the image fits the corresponding part detector well. and the part of two-legs has a low detection score because it does not find any visual cue to fit the detector. if the correlation among parts is modeled in a correct way, the detection score of the head-shoulder can be used to recommend the left-head-shoulder as visible and that of the two-legs can be used to recommend the left-leg as invisible. therefore, the major challenges are how to model the relationship of the visibilities of different parts and how to properly combine the results of part detectors according to the estimation of part visibility."
"where b 1.5 e i is the ball with center e i and radius 1.5 and z is the volume of (b rd 14 }, which satisfies the following conditions:"
deep model. the graphical model of the proposed deep model is shown in fig. 3(b) . detailed information is shown in fig. 4 . denote the visibility of p l parts in layer l by
"since rbm is a building block of our deep model introduced in the next section, a brief introduction on rbm is provided. denote the binary visible variables by vector"
"implies that, when opt d � k + 1, the result is also true. according to lemma 8, we have that the number of nodes in an mis of g does not exceed the number representing the maximum packing of balls with radius (1/2)r min in a * . note that for any two independent nodes u and v in g, the two corresponding balls with radius (1/2)r min and centers u and v are contained in an area a * and they are disjoint. in addition, since rhombic dodecahedrons can be used to densely fill a space, we can use a rhombic dodecahedron, which surrounds a ball with radius (1/2)r min, to replace the ball with radius (1/2)r min to fill a * . it is easily determined that the volume of a rhombic dodecahedron surrounding a ball with radius (1/2)r min is 4 � 2 √ ((1/2)r min ) 3 . at the same time, we note that when an independent node w is on the surface of a ball with a center node in opt d, the corresponding rhombic dodecahedron (surrounding the ball) may not be completely contained in an area a * and the volume of the part of the rhombic dodecahedron outside a * may attain (1/7)(4 � 2 √ − (4/3)π)((1/2)r min ) 3 . from the above analysis, we can obtain an upper bound of the size of an mis for g as follows:"
"x forms the visible layer and h forms the hidden layer. there are symmetric connections w between the visible layer and the hidden layer, but no connection for variables within the same layer. the graphical model of rbm is shown in fig. 3(a) . this particular configuration makes it easy to compute the conditional probability distributions:"
"according to lemma 3, we know that the number of independent nodes in a uds is related to the upper bounds for the sphere backing problem. it is worth mentioning that there are a lot of studies being related to the bounds for the sphere backing problem (see [cit] ). now, we derive the main result in the section."
"7071. according to lemma 3, to obtain an upper bound of the size of the maximal independent set in the connected unit ball graph g, we need to calculate the number corresponding to the maximum packing of balls with radius 0.5 in g. since the volume of a ball with radius 0.5 is (4/3)π0.5 3 ≈ 0.5236, we obtain an upper bound of the size of the mis:"
"in wireless ad hoc networks, the power source that each node (sensor) possesses is limited, which results in a limited distance over which the nodes in the wireless ad hoc network can transfer information and the time that the nodes can continue to work. to reduce energy and storage requirements and avoid information conflict and broadcast storms, a backbone-like structure has been proposed [cit], called the virtual backbone (vb). a vb is defined as a subset of wireless ad hoc network nodes. since every operation request between nodes in a wireless ad hoc network can be transformed into a homologous operation on the vb, the routing overhead in the wireless ad hoc network can be significantly reduced by the vb [cit] . when the nodes in the vb perform tasks related to routing, they are interfered to some extent by the equipment in the fixed physical backbones. to reduce the interference from the fixed physical backbones, it is natural to attempt to design a vb in which the number of nodes is minimized. in many wireless ad hoc networks [cit], a vb can be modeled by a connected dominating set of the wireless ad hoc network. in other words, finding the minimum size vb is equal to determining the minimum connected dominating set in the wireless ad hoc network."
"to denote the incoming neighborhood (respectively, the outgoing neighborhood, the closed incoming neighborhood, and the closed outgoing neighborhood) of v. assume that s is a dominating set of g � (v, e);"
e number of independent nodes in b 1 does not exceed the number corresponding to the maximum packing of balls with radius 0.5 in b 2 .
there are two contributions of this paper. 1. a probabilistic framework for pedestrian detection which models the visibility of parts as hidden variables. it is shown that various heuristic occlusion handling approaches (such as linear combination and hard-thresholding) are considered as its special cases but did not fully explore its power in modeling the correlations of different parts.
"proof. let a be the edge length of a rhombic dodecahedron and r be the radius of its inscribed ball (see figure 3 ); then, the volume of the rhombic dodecahedron is v 12 � (16/9) � 3 √ a 3, and the relationship between a and r is r"
"in order to reduce bias of training data and regularize the training process, we enforce that the visibility correlation parameter w to be non-negative. therefore, our training process have used the the prior knowledge that negative correlation among visibility of parts is unreasonable. furthermore, the element w in fig. 4 . in this way, we keep the most important edges based on our knowledge. there are other ways for modeling the connection among parts, e.g. the full-connected part models [cit] . they could be helpful for finding more connections but will increase model complexity, reduce efficiency and need more training samples. they are not dbn, need more complex inference/learning algorithms, and may need pruning edges in training."
"we have then optimized the overall annotation pipeline for more comprehensive inclusion of wd40 proteins and better annotation quality (fig. 1a and supplementary material) . the optimized pipeline is briefly described as follows: (i) retrieve the protein sequences of uniprotkb, including swiss-prot and trembl section. (ii) utilize the hmmsearch to screen all the input sequences based on 54 wd40-related profiles, and retain the sequences with e-value no greater than 10 as wd40 candidates. (iii) employ the updated wdsp to predict the secondary structures and the featured sites. (iv) assign confidence categories ('high', 'middle' and 'low') to each wd40 candidate according to our customized rules. (v) use modeller [cit] to build 3d structures for single-domain wd40s, and integrate missense variants and their associated annotations to wd40s with 'high' confidence from the swiss-prot section."
"independently, tabareau recently proposed a monadic interpretation for execution levels [cit], in which the exception semantics can be plugged by using a monad transformer. it was observed by tabareau that in the standard execution levels calculus, the execution level is a property of the control flow and, as such, is propagated between expression evaluation in a sequential order. this matches the semantics of the state monad, in the case where the state is a natural number. he then defined the execution level monad, and two exception transformers to provide flat (standard exceptions that do not have access to the level of execution), and levelaware [cit] exceptions to execution levels. more importantly, tabareau developed an abstract weaving algorithm that relies on monads to define concrete aspect semantics. such a monadic aspect weaver allows for modular construction of languages ( figure 1 ). using these concepts, he describes a monadic weaving algortihm on top of minaml, a statically typed functional aspect language."
"in the pointcut-advice model of aop, weaving is the fundamental mechanism by which aspects inject their crosscutting behavior in programs. typically, prototype aspect languages reuse the facilities of an existing aspect language, like aspectj [cit] or aspectscheme [cit], as a solid base on which to provide new extensions. however, in some cases it is necessary to maintain (potentially very similar) branches of the base language and the new features under focus of study. this situation is usually tedious, but can be managed with standard version control systems. however, this is but a symptom of a deeper problem: there is a lack of an abstraction mechanism for experimenting with aspect seman- figure 1 . combining a monadic weaver with a monadic aspect semantics to obtain a new aspect language."
"the abstract weaving function (listing 8) is implemented in an untyped module (its polymorphic type is given in comments). it expects a computation fun, a join point stack jp* and an aspect environment aspects containing all aspects that can apply and returns a new woven function. aspects are processed sequentially, and in each step a proceed-fun function is generated (line 5). proceed-fun corresponds either to the next advice in the chain, or to the original fun function. advice is inserted in the resulting function (line 7-11) only when the pointcut matches (lines 8-10). the aspect-pc (line 6) and aspect-adv (line 9) functions are accessors to the internal aspect structure used during aspect deployment. note that the algorithm uses do and return to work in the monadic setting, but is independent of any specific monad."
"in our setting, the aspect semantics of a language is specified by a monad, with the usual return and bind functions, plus a function responsible to create monadic aspects."
"so far, our motivation has been the separation of concerns between a core aspect language and concrete aspect semantics specifications. however, since monads are typically used to control effects in programs, we want to mix monads to control the effects available to base and aspect code. to make the combination possible, it is necessary to provide a lifting between base and aspect monads (in both directions). for instance, we have developed the agnostic execution level (ael) monad, in which no explicit level-shifting is allowed. we can then define a mixed ael+el monad in which levelshifting is forbidden in advices."
"monadic operators. some programs may be interested in querying or modifying the level of execution. to this end, the monadic aspect semantics module exports some functions and syntactic extensions. for instance, the levelshifting operator up is used to move the evaluation of an expression to the level above the current level. its definition is shown in listing 3 (lines 13-17). it uses the lower-level (not user-visible) functions inc-level and dec-level. down is defined similarly. finally, the lookup function reifies the current level as a value (lines 1-3) ."
"as a monadic aspect semantics module, it must adhere to the interface of section 3.2. the transformer takes a monadic aspect semantics to be transformed, and exports the bindings required by monascheme. listing 5 describes the monadic definition of the transformer. this part is similar to the traditional exception monad transformer. an exception type (line 1) is defined as a pair of a constant symbol 'exception (we use a simple version where exceptions contain no extra information) and an integer representing the level at which the exception was raised. we define the type la a (line 2) as an alias for a computation of the original monad with the union type (u a exception), denoting a value of type either a or exception. to lift values into the new setting, la-return (line 4-5) uses the original m-return (line 5) to create a la computation. la-bind (line 7-14) behaves as the usual exception monad transformer binder."
"in this section, we describe in detail the implementation of aspect deployment and weaving in monascheme. the management of the join point stack is as in aspectscheme [cit] ."
"the core monascheme module is responsible for configuring the monadic aspect semantics. it must require (i.e. import) an aspect semantics module in order to typecheck and compile correctly. when a client program requires the core module, the monadic aspect semantics are stored in two racket parameters (i.e. thread-and continuation-safe dynamic bindings). the current configuration can be queried using functions like get-current-monad. for instance, the do form we used above queries the current monad in order to obtain the associated binder. also, the weaver (section 4.3) uses the get-current-makeasp function to construct aspects according to the current configuration. a monadic aspect semantics module must implement a specific interface."
"as any racket module, a monadic aspect semantics module can provide supplementary procedures and syntactic extensions as needed, e.g. up and run in the el monad."
"but there are many more combinations that remain to be explored and are part of ongoing work. for instance, it is possible to support sandboxed aspects. this includes scenarios like providing aspects with e.g. read-only capabilities to the underlying store, or using the state monad so that aspects have their private store. this can be achieved by using two variants of the state monad m base and m asp, with the same underlying type constructor but with different effects: when executing base computation in m base, only effects (such as read or write) on s base can be used, and when executing advice computation in m asp, only effects on s asp are accessible. in that case, lifting a computation from one monad to another is simply given by the identity. the key to specifying the allowed effects lies in the m-make-aspect function."
"we start by describing shortly how to write aspect-oriented programs in monascheme, assuming the semantics given by the execution level (el) monad. monascheme is implemented in typed racket [cit], a typed version of racket designed to ease the transition between untyped and typed programs. consider the program shown in listing 1. the (: ... ) notation is for type annotations in typed racket. the run form (line 6) evaluates a given program providing the initial level of execution (internally defined as 0). a tracing aspect is dynamically deployed using deploy-fluid (line 6). it matches all calls to write and executes trace around those calls. then, when (write 'hello) (line 7) is called, the trace advice executes writing 'tracing to the output, and then proceeds (line [cit] . the end result is that the symbol 'tracinghello is written to the standard output. observe that the advice does not loop when calling write, due to the semantics of execution levels (which we explain in section 3.3). besides type annotations, the only difference between the program of listing 1 and its lascheme equivalent is the use of return and do in monascheme 1 . this is because both base and aspect code are written in monadic style, and all functions must return a computation inside the corresponding monad. in this example, all functions must return an el computation. computations are also needed when deploying aspects, as reflected by the use of (return trace) at line 6. additionally, we borrow the do notation from haskell as a shorthand for using the monadic bind function. it chains operations sequentially and allows the programmer to extract the value of a computation. also, additional bindings can be provided by a specific semantics. in our example, the run form is specific to semantics of the el monad. finally, the forms deploy-fluid, deploy-static and deploy-top allow the deployment of aspects with dynamic, static, and global scope respectively. all these forms are implemented using the racket macro system and are valid in all aspect semantics."
"the relation between aspects and monads has been discussed several times. hofer and ostermann [cit] clarified that they are two different beasts. our work does not attempt at unifying them, but rather at extending monads to aop languages in order to build extensible aspect languages with reasoning support."
"we faced this maintenance problem during the development of an exception handling mechanism for execution levels. tanter proposed execution levels for aop [cit] as a means to structure computation and avoid infinite regression by default, while providing flexibility to the programmer when required. in subsequent work, figueroa and tanter [cit] address the exception conflation problem, that is, the problematic interactions between aspect and base code in presence of an exception handling mechanism. to solve this problem, they propose a level-aware exception handling mechanism in which exceptions are bound to the level at which they are thrown, and are only caught by handlers at those levels. during development of the level-aware exception mechanism, it was necessary to maintain two branches of lascheme [cit] (the original execution levels prototype language). the reason is that the new version introduced additional noise that was unused and unnecessary for the original version to work, so in order to clearly present both implementations we kept them separate. unfortunately, both code bases diverged, requiring extra work to keep them in sync."
"aspect semantics. in execution levels, an aspect is deployed at a level n and can only advise join points emitted at that level. join points are emitted one level above the computation they originated. also, an aspect at level n evaluates its advice at level n + 1. this semantics is specified by the el-make-aspect function (listing 4). given a pointcut pc and an advice adv, the function creates both a level-aware pointcut and advice. in both cases this is done by capturing n (line 3), the level of execution at definition time, using lexical scoping. the new pointcut first performs a level check (line 8), comparing the level of definition with the level of execution, and only if those levels match pc is applied one level above (line 9). the new advice returns a closure which executes adv one level above current computation, using up, wrapping proceed in a level-capturing function (line 13) to ensure that base computation is always executed at its original level. a level-capturing function is bound to a level of execution l. when applied, it is evaluated at level l and then the level goes back to its previous value. lambda-at is an internal expression to construct a level-capturing function bound at level n."
"as a first practical instantiation of the formal development of tabareau, we developed a monadic version of aspectscheme [cit], called monascheme. monascheme is a higher-order functional aspect language with monadic aspect weaving, which supports dynamic aspect deployment with lexically, dynamically, and globally scoped aspects. monascheme is implemented as an embedded language in racket. at the heart of monascheme lies the monadic aspect weaver, which provides management of the join point stack and is parametric with respect to the monadic aspect semantics of a given language. our implementation is available at http://pleiad.cl/research/monascheme."
"the contributions of our work are: first, we validate the work of tabareau by providing a concrete implementation of a monadic aspect weaver for the racket language, called monascheme (section 2). monadic aspect semantics are defined as racket modules, encapsulating the corresponding monad definition as well as supplementary procedures and syntactic extensions as needed. second, we improve on minaml by providing a full-fledged aspect language. third, we exemplify the usefulness of our system by implementing the execution level monad, and combining it with a variant of the exception monad transformer to obtain level-aware exceptions (section 3). we describe aspect deployment and weaving in section 4. finally, we see this development as the first step for a modular framework for controlling aspects with monads (section 5). section 6 discusses related work."
"having established the context in which exceptions propagate adequately, we define (listing 6) the throw and try-with forms, to raise and catch exceptions, respectively. to raise exceptions, we get the current level n (line 3) using the lookup operation provided by m, and return an exception listing 6. throw and try-with forms tag and check the execution level respectively listing 7. deploy-fluid implementation at that level using m-return (line 4). the try-with takes two computations e1 and e2. it extracts the value of e1 into result (line 8) using the binder of the monad m. then, if result is an exception (line 9) it checks if the level of the handler matches that of the exception (line 11-13). if they match, e2 is returned (line 12). otherwise, the exception is propagated (line 13). if result is not an exception, it is returned (line 14)."
"in the wpt system, the dc source is converted into ac source using a convert circuit. in order to make the wpt system simple, the following circuit shown in fig. 2 is as the feeding circuit to connect with the wpt system [cit] . in this circuit, the basic components are dc power supply, resistor r 1, capacitors c 1, c 2, and c 3, inductors l 1 and l 2, together with a bipolar junction transistor (bjt) t."
"wireless power transfer technology has a great application potential in implanted medical devices. different from the previous research, in this paper, the optimization design for the wpt system not only includes the coil system but also considers the influence of the power source. in order to simplify the system, a simple feeding circuit is applied in the wpt system to convert a dc source into an ac source. the property of this feeding circuit is firstly analyzed, and the relationship between the parameters in the circuit is concluded in this paper. then the optimization method is applied to optimize and design the wpt system as a whole. the results indicate that this method can obtain the optimal performance of the wpt system."
"in order to reduce the ac resistance of the wire, the litz wire is used to construct the receiver coil and the transmitter coil is made of copper wire. through analysis, the relationship between the power transfer efficiency and the number of strands and number of turns of the receiver coil is shown in fig. 9 ."
"in the circuit, r 1 is used to limit the amplitude of the current flowing into the bjt to guarantee its proper operation; the capacitors c 1 and c 3 are assist components to ensure that the circuit gives more stable output voltage and current; the characteristic of power switch of bjt is used to carry out the conversion from dc to ac. when the excitation in the feeding circuit is a dc voltage, the output voltage and current in l 2 are shown in fig. 3a and b respectively. it can be seen that the output voltage and current are both a sinusoidal function."
"in order to find the relationship of the values of l 1 and l 2, the frequency of the feeding circuit is fixed. then the output currents in l 2 are obtained by varying with the values of l 1 and l 2 as shown in fig. 7 . it can be seen that the output current will reach its maximum when the values of the two inductors are chosen correctly."
"according to (1), the change of c 2 will determine the frequency of the output current in l 2 . the optimal results in table 2 are based on the 13.56 mhz. the power transfer efficiency curve varied with different frequencies through changes in the value of c 2 is shown in fig. 9 . the power transfer efficiency achieves the maximum value (about 30%) when the frequency is 13.56 mhz."
"the driving frequency of the feeding circuit is determined by the components l 1, l 2, and c 2 . the frequency can be calculated by"
"(1)de algorithm is applied several times to tackle our problem. in the first trial, random initialization is used, while in the following trials, we embed the best found solution in previous trials into the initial population. in this way, de starts in a good position in the subsequent trials. (2)solution revisiting happens when a heuristic algorithm converges to global/local optima. in preventing revisiting and saving simulation time, we use a non-revisiting scheme [cit] . this scheme stores all solutions in a binary space-partitioning tree structure. this scheme requires more computer memory than an algorithm without this scheme; however, saving simulation time is more important than memory consumption in our case."
where p load and p source are the power in the load and the source respectively; i receiver and i transmitter are the currents in the receiver coil and the transmitter coil. the relationship of the power transfer distance and the diameter of the transmitter coil are the constraint condition.
"in this paper, the wpt system applied in the implanted medical devices is optimized and designed. the analysis procedure is shown in fig. 4 . firstly, the feeding circuit as shown in fig. 2 is analyzed to find its optimal parameters and the relationship between the two inductances. the aim of the analysis is to obtain the maximum current in l 2 . then an optimization method is designed with the above results as the restrict conditions. as a whole, finally, the wpt system is optimized according to the determined initial parameters of the receiver coil."
"in this paper, a differential evolution (de) algorithm is used to analyze the model. it is simple to implement, easy to use, and computationally fast. the de algorithm includes the following two equations for producing the candidate solutions:"
"3.4 three-coil wireless power transfer system with the feeding circuit and load circuit according to the above description on the feeding circuit, the wireless power transfer can be analyzed as a three-coil system as shown in fig. 8 . the coils l 1 and l 2 are combined as the transmitter coil while the coil l 3 is the receiver coil. m is the mutual inductance between the two coils."
"in the feeding circuit, the parameter r 1 also impacts output current in l 2 as shown in fig. 6 . it can be seen that the output current in l 2 will reach the maximum value when r 1 is about 3 kohm."
"wireless power transfer (wpt) system has a great application potential, because of the development of the electronic devices. comparing with the traditional method of power transfer using magnetic inductive coupling in achieving wireless and efficient near-field power transfer, there are increasing number of researchers that focus on the magnetic resonant coupling method [cit] ."
"in order to obtain a sensible design, the size of the receiver coil should be determined initially. in this research, it is assumed that the wpt system is designed for use in implanted medical devices. the size of the receiver coil, therefore, should be as small as possible. the frequency is chosen to be 13.56 mhz. based on the above analysis, the wpt system as shown in fig. 8 is optimized as an integrated system. during the optimization, the optimization objective function is the power transfer efficiency which is expressed as:"
"the receiver coil induced voltage through catching the magnetic fluxes generated by the transmitter coil. as shown in fig. 1, the wpt system includes a dc power source, inverter, transmitter, receiver, rectifier circuit, and load. the inverter is applied to invert the dc source into a sinusoidal source to provide a shifting current in the transmitter coil. in the previous research, the works about the wpt system almost focus on the coil design when the system is optimized [cit] . in the system, the inverter circuit is too complicated to be optimized along with the coil system. generally, power transfer efficiency is as the objective function under some constrains, such as the volume of the receiver coil or the power transfer distance. according to the application background, sometimes, the power transfer efficiency and the maximum power transfer are both as the objective functions [cit] . under this condition, the multi-objective optimization method [cit] can be used to design the wpt system. in this paper, the power transfer efficiency is chosen as the objective function. the wpt model is formulated as a constrained optimization problem. it is a simulation-based optimization problem with mixed discrete-continuous variables. metaheuristic approaches are generally appropriate and hence commonly used for solving such optimization problems."
"according to (1), the frequency of the convert circuit is 13.56 mhz after the values of inductance and capacitance are substituted. the voltage and current in l 2 are shown in fig. 3 . it can be seen that the output voltage in l 2 becomes a sinusoidal function with 5-v amplitude from the dc source. the sinusoidal current in l 2 can generate a shifting electromagnetic field which is a key medium in the wpt system."
"according to the electromagnetic theory, the strength of the magnetic field generated by the coil is proportional to its current which can be described as (4) ."
"to seek more efficient and durable power supply way, therefore, become one of the key technologies of the implanted medical devices. after entering the end of the twentieth century, to realize the wireless charging for implanted medical devices provides a safe and reliable method to solve the supply problem. thus wpt technology receives more and more attentions from the researchers in biomedical engineering [cit] ."
"in this paper, a simple convert circuit is used to convert a dc source into an ac source. this circuit is applied into the wpt system for implanted medical devices. thus, the system can be optimized and analyzed as a whole. a differential evolution (de) algorithm is used to analyze the model. it is simple to implement, easy to use, and computationally fast."
"before the wpt system is analyzed, the feeding circuit as shown in fig. 2 is analyzed firstly. during analysis, the parameters in the feeding circuit will be analyzed while it can be seen that, the output current in l 2 reaches the maximum value when c 1 is about 4 pf as shown in fig. 5a . in fig. 5b, it indicates that the value of c 3 has no effect on the output current in l 2 . its role is just to ensure that the bjt circuit has a relative high voltage gain."
"nowadays, there is a growing interest in reliably estimating a vehicle's energy consumption (either fuel or electrical consumption) towards a specific destination. the pre-trip knowledge of the expected energy consumption along a route may affect the decision of selecting a particular route among the available ones considering the constantly rising price of energy, as well as due to ecological reasons [cit] . furthermore, such knowledge is necessary in order to calculate the reachability of a destination before settling towards it. the outcome of this calculation is very important especially in cases of vehicles consuming alternative fuels, which have limited reserves and certain restrictions regarding their refuelling process. for example, the fevs' recharging process is significantly time consuming, while the compressed natural gas (cng) vehicles' refuelling network is very limited. thus, the accuracy and reliability of the energy consumption estimation is of high significance when planning the routing and refuelling strategies of such vehicles."
"under mrsrf(p, q) the trees in p are compared to the trees in q . if p q ≡, all trees are compared to each other. node n i 's submatrix is created in parallel using two mapreduce phases as described below."
"in this section, we first analyze the property of indoor environments and present an efficient approach to acquire the initial pose of robot with only one laser scan. given the initial knowledge of robot's pose, we use a particle filter with accurate proposal distribution to track robot pose in real time."
"the appearance of virtual objects and their integration with the real world is fundamental. a visual inconsistency between the virtual and real objects due to wrong illumination negatively affects the appreciation of the scene. as we have seen, few studies have been made about the estimation of illumination direction in real-time with affordable hardware."
"the us national institutes of health published a request for applications to build an informatic system for extracellular rna that includes the concept of an exrna atlas (78) . if an ev ontology were to be developed, for example, as part of the proposed exrna atlas, it could be added to the network of ontologies used by this and other systems participating in ev research. the unified representation of ev-related information might help disparate computer systems coordinate a response to automated or human-directed query and retrieval tasks. these disparate systems may include existing repositories of ev data (11á13,79). the sum may be a semantic web that accelerates insight into the integrative biology of evs, including their possible role in intercellular and inter-organ communication (80, 81) ."
"the main purpose of this work was the creation and test of a low-cost sensor to detect the illumination direction of the environment. the gathered information is then used to manipulate a virtual light source in an ar application running on an android device. in general, our method generated a coherent ar environment, where the virtual illumination resembles the real illumination."
"mrsrf(p, q) is where all of the computation for the mrsrf algorithm lies. at least one node will require o(t) time to obtain the trees for its p and q sets. the first map phase of the mrsrf(p, q) algorithm, which is based on hashrf's first phase, requires o n p q m"
"since biomedical software applications commonly have complex data models, one may expect it to be helpful to have application authoring systems that can automate the generation of the gui from the data model (70á73). by reducing programmer workload, these extend the possible scope of other programming tasks. however, we find these systems to be inflexible for this application because we need the gui to render differently according to the context. for example, the same data elements may need to be grouped differently, be editable, or displayed at all according to the user's authorizations and the use case. instead, we prefer a system where the gui behaviours may be coded in a set of rules that a programmer can edit. the result, a gui that requires less programmer work to generate than if the client side code were written by hand, yet remains programmable within large bounds. the resulting compromise meets well the needs of developing research biomedical guis, and there is active research in the area of rule-based gui generation (70á73)."
"once the map stage completes, each of the r reducers takes as input a (key, list(value)) pair with bipartition b i as the key and a list of tree id lists as the value. there will be at most m lists of tree ids for each bipartition. each reducer then combines these o(m) lists in a manner such that the trees from file 1 are separated from the trees from file 2 to form a single line in the global hash table. each row of the global hash table represents a unique bipartition among the t trees. continuing with our example from figure 3, the first reducer processes the"
"once, a mapper has finished processing its hash table, it emits its similarity matrix for processing in the reduce stage. the key is the row id and the value is the contents of that row id. thus, in figure 4, the first mapper emits the (key, value) pairs (t0, (1, 1) ), and (t1, (0, 0))."
", where n is the number of taxa and m is the number of mappers. o(n(p + q)) is the total number of bipartitions that must be processed across the p + q trees and inserted into the hash table. suppose b unique bipartitions are found. in the worst case, a bipartition has a length of p + q, which re ects the fact that it appears in all p + q trees. hence, the complexity is"
"overall, the clusterings in figures 8 and 9 suggest that there exist several well-supported partitions of the trees and that each partition should be summarized separately in order to minimize information loss. moreover, the data suggests that the various bayesian runs among the 150 and 567 taxa trees did not converge to the same place in tree space. one of the greatest benefits of convergence is reliability of the trees found by a phylogenetic heuristic. hence, rf matrices could be used as a method to detect convergence between runs."
"insofar as evs may be regarded as fluid-borne biomarker vectors, a biorepository to support ev biomarker science faces informatic challenges beyond those faced by simple tissue biorepositories, commencing with the need to support the benchmarking of ev biomarker assays against solid tissue analyses (5). moreover, since an additional function of a biomarker is to bridge an intrinsic biological state to a predicted clinical endpoint, an informatic system to support ev biomarker science likewise must record linkage between biological and clinical states (6) . the need for biospecimens throughout cancer research has motivated the development of several biorepositories, including offerings by federal governments, academia, and the commercial marketplace (7á9). to unify some of these offerings, the united states national cancer institute maintains a specimen resource locator, which seeks in turn to aggregate catalogues from a number of specimen banks (9) . but the development of the dual services sought of a biomarker, both to be a bridging indicator of biological state and to provide a surrogate measure of clinical outcome, pose informatic and other challenges beyond the focus of the valuable existing biorepository resources."
"privacy, security, and permissioning table iv lists the privacy and security exposures that were encountered by this system with the corresponding protective measures. the system conforms to best practices for protection and storage of medical grade data. this includes written policies, checklists as well as confidentially agreements to be signed by investigatorusers, and related measures to adhere to the privacy requirements of the health insurance portability act of america (hipaa) (52). to adhere to the security requirements of hipaa, the system employs encryption throughout coupled with password policies (53) . the u.s. food and drug administration (fda), in addition to the privacy and security rules in hipaa, has guidance documents pertaining to the integrity of research data (54). this system conforms to those guidance documents. the system releases permission parsimoniously to authenticated users to view and edit subject information on a need to know basis. authorized users may have one of two types of data access scope, site scope and protocol scope. site scope entitles a user to view and edit data that originates from the user's institution. most authorized users have site scope. protocol scope entitles users to view aggregates from across all institutions. only the overall principal investigator and delegates have protocol scope."
"for the development of the web-based gui, we use an open-source space tree visualization toolkit (27) . we augmented the source code to include the additional properties described earlier of unification, of programmatic and data domains, data model introspection, and interactivity. the unified data visualization hypothesis is reified in all use cases since they all are expressed in the gui."
"because the icp results are very accurate, so we can use it to replace odometry readings and get a very accurate motion estimate of robot."
"specimen data are stored in a collection of tables. there is a specimenaccession table that stores information about the acquisition of the specimen such as anatomic site, time, date, geo location, quantity, and related information. this table has a to-many relationship to the specimenfraction table that contains a bar code identifier, geo location including a pointer to an enclosing container, and a custody chain record. there are optional pointers between the specimen, the imaging, and the comprehensive clinical data tables."
"tracking and alignment is required to provide overlay information. to improve the user experience, the system must provide a realistic blending between the virtual objects and the real world. the ability to properly simulate the illumination condition of the real world is still limited, mainly due to the difficulty of estimating the illumination conditions of the real environment."
"to address these hypotheses, we surveyed commercially available laboratory specimenáhandling computer programs, including biological specimen inventory system (information management services, inc., http:// www.bsisystems.com), a virtual inventory system lacking storage of radiological imaging studies, and a resultshandling component, and thus unable to be queried for clinical analytic correlations. however, its web service programming interface provided a useful specimentracking component of a larger system. in review of the application programming interface (api) documents, we judged the programming interface to lack sufficient power and flexibility to meet that desired component role."
"for the 567 taxa trees, the heatmap (in figure 9) shows regions of high similarity among the trees within a run, and regions of dissimilarity across runs. the clustering also shows that runs 0, 6, 10 exhibit trees with high levels of similarity among them. one conclusion is that these runs converged to similar areas of tree space in the phylogenetic search and the trees from those runs can be summarized by a single tree (such as a consensus tree). this is also true for runs 1, 5, 7, and 8. the clustering of the other runs (runs 2 and 3) and (runs 0, 6, 11) have lower levels of similarity."
"we report the initial deployment of a biorepository informatic system whose design is driven by some hypothesized properties of future human ev biomarker research. we hypothesize clinical and basic science research centres collaborating across the world that obligate a coordinated virtual, global specimen inventory management system with global document sharing and social computing features. we hypothesize that novel diagnostic, prognostic, response, toxicity, molecular pathway, and ev biomarker analytics have the best value when correlated with multimodal clinical data, so we include features for multimodal clinical data capture and correlation with experimental ev analysis. we anticipate the need to perform analytics on tightly defined novel patient population cohorts linked by age, tumour appearance on imaging, response or resistance to therapy. we anticipate that ev science may turn in as yet unknown directions, so we include a flexible data model and flexible gui enriched by externally maintained and validated ontologies. from these hypotheses we derived the use cases that the system ought to support and evolve with the science. they are the field use, clinical data, laboratory, specimen inventory, specimen query, specimen transport, data archive and reporting, and collaborative science use cases. the use cases drove the data model design, the software code creation, the external integration selections, and the deployment decisions. this software development process where one hypothesizes needs, builds the system, evaluates its usage, and rebuilds it accordingly, shares features with the software development methodology named behavior driven development (bdd) (64) . in bdd, the challenges recursively are identified first, and design and deployment choices follow. to some extent, the flexibility to handle unknowability was built into the system. one method is by grouping all elements that belong in the gui into a single network graph model that is then rendered visually, we hope to allow complex and possibly evolving elements to be intuited visually. a second method is by employing a rule-based gui programming system, that introspects on the data model and draws gui elements accordingly, we increase the ease with which evolutions in the data model can be incorporated. if over time the challenges change or the current system as designed does not meet them satisfactorily, we aim to re-evaluate and change the system accordingly. data visualization in the web browser is an area of active growth. we selected the infovis library because it excels at the display of network graph data, and as an open-source project, its source code is available for modification to meet the custom needs of this system (27) . among other javascript-based data visualization open-source libraries that offer network graph visualization modules, data driven documents (d3.js) has a broad range of applicability, and has been used in various bioinformatics applications such as gene ontology visualization (65, 66) . the selection here of infovis is a matter of convenience, and other options such as d3.js could be adopted when they meet better the needs as may arise."
"the rest of the paper is organized as follows. section 2 provides background information about previous works. then, in section 3 and 4, we describe the proposed system for real-time calculation of illumination, followed by an experimental application to show the features of the system. we conclude in section 5 with a brief discussion on limitations and advantages of our approach and future work."
"as virtual objects share the same scene with real objects in real lighting conditions, a common issue in ar is the appearance inconsistency between the virtual and real objects due to wrong illumination. the illumination of the virtual objects has to change in the same way as the illumination of the real scene. in order to achieve this consistency, the estimation of the illumination direction is required."
"we selected postgres as the database server software because of its resilience, performance, and because it can be configured to handle maintenance of the data audit trail. however, as a relational database it does not map naturally to network graph data because each node traversal requires an expensive sql join operation (67) . a number of graph databases have been introduced that are optimized for the storage of data richly populated with nodes and edges. these graph databases have efficient traversal of edges during query tasks (19, 67) . a graph database might be programmed more conveniently to coordinate with a network graph oriented gui as we have implemented here."
"in this paper we present an illumination direction estimation method to simulate illumination on mobile ar systems in real-time. a low-cost sensor was built in order to detect the illumination of the real environment. since no image processing method is used, our method is affordable for any smartphone capable of running an ar application. a pc application is required to transmit the information from the sensor to the smartphone."
"the data model is designed for the gathering of clinical data (symptoms, signs, comorbidities, major interventions, clinical laboratory findings, quality of life, and outcomes) and imaging data (from computed tomography and magnetic resonance imaging) as coupled to fluid and tissue specimens (fig. 3) ."
suppose the initial knowledge of robot pose 0 ( ) p x is provided by some appropriate methods. then the global localization turns to be the pose tracking problem. the key idea of particle filter for pose tracking is to estimate the robot trajectories with a sample-based representation.
"here, the input is a similarity matrix row identifier, i, and a list of rows that contain the local similarity scores found by each of the mappers. for a particular key, the number of rows within the list of rows received by a reducer is equal to the number of mappers, m. in figure 4, the first reducer receives the following (key, value) pair for similarity identifier, t0: (t0, (1, 1), (1, 0) ). the reducer sums up the columns of each of the lists to produce (t0, (2, 1)). to produce the rf distance for row t i, each column in the final similarity matrix is subtracted from n -3, the maximum possible rf distance where n represents the number of taxa in the t trees of interest. together, the output from the reduce stage yields a final submatrix. for each node n i, the resulting submatrix is written to a file. these files can then be combined to form a final rf matrix, or be kept in their partitioned form for easier handling."
"augmented reality (ar) is the combination of real and virtual elements to augment the real world, improving people's senses and skills. realistic appearance of virtual objects and their proper interaction with the real world is of high interest in entertainment, design, medicine, education and many other applications areas. in the last years, mobile devices like smartphones and tablets became part of copyright: this article is distributed under the terms of the creative commons license cc-by-nc. our everyday life. due to their computational power and integrated camera, they can act like a window into an augmented real world ( [cit] )."
our results show that mapreduce is a promising paradigm for developing multicore phylogenetic applications. the results also demonstrate that different multi-core configurations must be tested in order to obtain optimum performance. we conclude that rf matrices play a critical role in developing techniques to summarize large collections of trees.
"we selected jvm version 1.7.x for the middle tier because its combination of performance, security, language-type safety, memory management, and rich collection of compatible libraries have led it to be the leading language and platform for server applications (31) . most of the middle-tier code is written in java. we are transitioning to scala version 2.10.x because it is more expressive and offers functional constructs while preserving type safety (33) . it compiles to java byte codes and has proven in this project to be trivially interoperable with the preceding java code. moreover, scala is convenient for data import tasks because it offers deployment as an interactive language (34) . this permits data import scripts to be developed in an interactive fashion."
"a similar functionality was implemented with the open-source portal liferay, where the custom application is supplied as a portlet (per the jsr-268 specification) and the liferay application provides password management, authentication, document sharing, calendar sharing, and social computing services (40, 41) . google apps and liferay both furnish document libraries that may be user for the shared editing and storage of study documents such as protocols, consent forms, grant applications, manuscripts, and institutional human subjects committee or other regulatory documents. however, we switched to google apps as the main option because the users are more familiar with the various google applications than the liferay ones, and because liferay, as a full open-source product, requires one fully to administer the deployment whereas google apps are administered by google. however, the liferay integration remains as an option."
"other tables maintain information on study personnel and study institutions, including documents pertaining to and status of institutional committees for studies involving human subjects. the tables are mapped to java objects by the eof object-relational mapping system included in webobjects (36). bar coding and geo tagging a unique, machine readable bar code is assigned to every specimen, every specimen container, and every location that contains containers. for example, a laboratory freezer maintained at (808c may contain several racks, each rack may contain several drawers, each drawer may contain several boxes with dividers, and each cell amidst the dividers may contain a vial with frozen liquid specimen. in this instance, a unique bar code would be on the freezer, the racks, the drawers, the boxes, and the vials. we employ various methods for bar coding of vials and containers as appropriate for the circumstances. for the small vials to be deep frozen, we selected vials with preprinted bar codes (thermo scientific part #5001-0012). for other bar codes, we employ a bar code printer that prints them on an adhesive tape and then we apply that label to the specimen container. for containers that are already frozen at (808c and thus may not serve as a sticky surface for a bar code sticker we employ vinyl (xpress tags part #tg-2122-wh) or paper tags. the bar code stickers are applied to the tags dry and at room temperature, which are then applied to the frozen container. the bar codes may be read (''gunned'') with a commercial bar code scanner attached via a universal serial bus (usb) computer port (hewlett packard part #ey022aa). the bar code scanner reads the identifier code and places it at the cursor point in the active gui screen."
"the yaw rotation of the light source is calculated as follows. first, the eight ldrs around the sensor are numbered from 0 to 7. then, the ldr detecting more light is considered. for instance, the ldr 3 in figure 4 . as each ldr is separated by 45 degrees, the angle of incidence of the light would be the number of the ldr detecting more light, multiplied by 45 degrees. in figure 4, the angle of incidence of the light would be 135 degrees."
"the virtual biorepository currently includes 3 fixed locations with postal addresses (fig. 2) . it holds now 4,392 physical specimens that were acquired in 1,443 accessions from 611 subjects. of those accessions, 1,065 (73.8%) are fluid and 378 (26.2%) are solid tissue."
"we created an informatics system to support ev biomarker research. this biorepository informatics system represents a hypothesis-driven experiment on the future course of microvesicle research. rigorous methods do not exist to test the performance of this system against the actual needs of the field as it develops, so the question of whether the hypotheses represented here is confirmed by the data will likely be addressed only qualitatively. we hope we have built a system that may evolve with ev science."
"a conventional gui typically contains a text-based menu structure and presents tabular data as text for viewing and editing. however, humans can grasp much greater volumes of rich and complex data when presented visually rather than textually. we hypothesized the need to accommodate these graphic displays when anticipating changes in concepts of molecular gliomagenesis paths and incorporating recent additions to the cancer genome atlas (tcga) related to glioma (15, 16) . thus, graph methods may prove to be convenient and clear ways to represent networked information that consists of nodes and edges such as complex biological data (17á20). examples of such complex biological data include metabolic pathway data (19, 21, 22), ontology networks (23), and database structures that link tables with subject, diagnostic, specimen, and outcomes data. we anticipate the need to accommodate new directions in the science of evs, including speciation of those obtained from in vitro cell lines, in situ patient specimens, and in vivo animal models, subjected to varying forms of centrifugation and column and filtration separation, and then analysed by transmission electron microscopy (tem) imaging, immune-gold tem, nano-particle concentration and size tracking, zeta potential analysis, and lipidomics (24á26). hence, we elected to base the gui on a graph theory paradigm (fig. 2 ). an existing data visualization library for the display of networked data is not sufficient for our purposes, but requires 3 additional properties. first, all elements in the gui must be unified into a single network graph model. this includes programmatic functionality, data query and editing, and meta data maintenance. second, subject, specimen, and ontology networks may change in response to scientific developments. the system must be able to introspect these data models and modify the network graph model accordingly so that it can be rendered on the screen by the data visualization package. third, the network graph gui code must allow the user to query, inspect, and edit data with conventional screen forms, graphic forms, and tabular text formats in the appropriate circumstances."
"the results seem to indicate that this approach is useful in any situation, specially where low monetary and computational cost are required, or the computational core of the system is based on smartphones."
"we report a laboratory and clinical information system that supports this fluid-based biomarker search and validation effort. to accommodate maturation in the ev field, we hypothesized the need for informatics to keep pace with scientific advances. given hypothetical changes in nomenclature, structural distinctions, preparative techniques, proteomics, and collaborative studies, we generated narrative examples of the way in which the system may be used and expressed these examples as use cases. from the use cases we designed the data model and the graphical user interface (gui) screens. in the future, we will compare actual usage with projected usage, and modify the system to support evs science as it evolves."
"before comparing the precision, we use odometry readings and icp algorithm to estimate robot pose respectively. as shown in fig.6, the odometry readings are noisy. after loop closing, the estimated position of robot is far away from the true position. if we use the odometry reading to calculate the proposal distribution, we can only get a suboptimal proposal. as a result, one needs a comparably high number of particles to track the robot pose. however, the laser scan data are significantly precise and reliable. we can use icp algorithm to estimate the robot transformation between two consecutive time steps. compared to odometry reading, the robot poses estimated by icp are more accurate. and the proposal distribution will be more accurate, if the odometry readings are replaced by icp results. although the odometer readings are replaced by icp results, it is essential. in some situations, icp may be failed. at this time, the odometer readings can be adopted to calculate the proposal. because the odometry readings are noisy, the pfo can not get accurate estimated trajectory, even with high number of large particles. fig.7 depicts the estimate trajectory based on pfo with 500 particles. at the same time, 5 particles are used in pfi and pfoi to estimate robot trajectory and the results are also depicted in fig.7 and fig.8 compared with estimated results of pfo, the proposal calculated from icp results can get accurate trajectory, even with low number of particles. since the accurate proposal takes into account the most recent observation, the pfoi can get the most accurate estimated trajectory. to compare pfo and pfoi more deeply, we changed the number of particles for each of them and estimated robot trajectory respectively. as shown in fig.9, when low number of particles is used, the estimated results of pfoi are more accurate than that of pfo. when the number of particles is over 30, they almost have the same precision. usually, the lms can collect 30 scans in 1 second (33ms per scan). if 20 particles are used, the rum time for dealing with each scan is about 20ms in pfoi, which is little than the collected time for each scans. that's mean the proposed approach can localize the robot in grid map in real time."
"google spreadsheets are used in this system as a medium for data exporting and reporting. the google spreadsheets offering includes apps script, a programming language and environment that includes facilities for accessing external web services as well as for adding menu items in the spreadsheet gui (50). we supply a code written in apps script to retrieve anonymized subject and specimen data via encrypted web services with proper authorization from the custom application and to insert that data into google spreadsheets. this functionality is embedded in selected spreadsheets such that the invocation in the spreadsheet menu of a refresh command causes the retrieval of data from the application into that spreadsheet. this offering allows the ad hoc analysis of data by authorized study personnel (fig. 6) . moreover, direct access to the sql tier may be afforded for analysis for research needs not met by data in a spreadsheet format. access to this spread sheet pull capability is compartmented. authorized users with site scope permission at each participating institution may pull data submitted by that institution. authorized users with protocol scope permission may pull data pooled across all institutions."
"the software components described earlier are termed the custom application, but they do not comprise the complete informatic system. the application is provided as integrated feature of a larger, complete web-based computing environment. the application was initially provided as a portlet in a liferay portal deployment (40, 41) . it is now provided as a google gadget that employs the opensocial specification (10, 42) . thus, the biorepository informatic system described in this paper refers to the integration of this custom application with a private google apps for business account."
"a common trend in phylogenetics is encapsulating the result into a single consensus tree, where the assumption is the information discarded is less important than the information retained. however, many of the trees may contain elements of the \"true\" evolutionary tree and their relationships should not be ignored. hence, we show how to use rf matrices to improve the summarization of a phylogenetic analysis. overall, our results provide evidence that large computations involving phylogenetic trees can take advantage of the mapreduce framework to design high-performance phylogenetic applications. since all the lines are independent from each other, all mappers run in parallel."
"in figures 8 and 9, heatmap cell (i, j) is colored according to how similar (lower rf rates) the trees are across runs i and j. hot regions, colored in shades of red, denote highly similar trees. cool regions, colored in shades of green, denote dissimilar trees. cell (run1, run1) in figure 8 shows an average rf rate of about 20% while (run1, run2) show an average rf rate of around 24%. in figure 9, cell (run6, run10) shows an average rf rate of about 18% while these runs compared to themselves (i.e., cells (run6, run6) and cells (run10, run10)) show higher levels of similarity with an average rf rate of around 11%. finally, the histogram in the color key represent the number of cells with a particular rf value."
"we ran our experiments on 20,000 and 33,306 biological tree collections consisting of 150 and 567 taxa, respectively. mrsrf was implemented using phoenix [cit], a mapreduce implementation for shared memory multicore platforms, and openmpi [cit] . our results show that mrsrf is a promising methodology for parallelizing the all-pairs rf distance problem. in our experiments, mrsrf shows good overall speedup. on 8 cores, mrsrf is over 6 times faster than the best-performing sequential algorithm, which is also mrsrf run on a single core. for 32 cores, it is 18 times faster than the serial version of mrsrf. speedup resulted from allowing the underlying mapreduce runtime system to schedule communication on the multi-core system, which greatly simplifies mrsrf's implementation."
"we studied the performance of our mrsrf algorithm on two large biological trees sets consisting of 20,000 trees of 150 taxa each and 33,306 trees of 567 taxa each. our experiments show that mrsrf is a scalable approach reaching a speedup of over 18 on 32 total cores. our results also show that achieving top speedup on a multi-core cluster requires different cluster configurations. finally, we show how to use an rf matrix to summarize collections of phylogenetic trees visually."
"the application consists of an virtual object projected over an ar marker and a virtual directional light source, which rotation depends on the data transmitted by the sensor. the main purpose of the experiment is the comparison between the illumination and shadows projected by a real object and a virtual object. the shadows projected by the real object is produced by the real light source, and the shadow projected by the virtual object is produced by the virtual light source, which orientation depends on the sensed data. figure 5 shows the experiment where the illumination and shadow of the real and virtual objects can be appreciated. as the position of the light source changes, the illumination and shadows of the virtual object also change in real-time."
"most of the existing particle filter application reply on this recursive structure. (3). resampling : particles are drawn with replacement proportional to their importance weight, which means a particle with small weight maybe replaced by a particle with large weight. often, a probabilistic odometry motion model is used as the proposal distribution in the simultaneous localization and mapping (slam) or localization algorithm. but the odometry readings are always noisy and there needs large number of particles to avoid filter divergence. in following section, we will describe a technique that can get the initial pose with only one laser scan and compute a more accurate proposal distribution."
there are tables that maintain administrative and regulatory compliance information. an audit trail is maintained of all data updates and deletions by coupled use of a postgres trigger as described earlier.
in this paper we have described a solution for realtime estimation of illumination direction for an ar application. our approach is based on low-cost sensors and arduino. we presented an experimental test using android as a platform for our ar software and a pc to transmit the illumination information provided by arduino.
we develop a new algorithm called mrsrf (mapreduce speeds up rf) for computing the all-pairs robinsonfoulds distance between t evolutionary trees on multicore computing platforms. the rf distance is a popular measure for computing the differences in evolutionary relationships between t phylogenetic trees of interest. there are several applications for using rf matrices such as visualizing collections of trees [cit] and clustering tree collections [cit] .
"our mrsrf algorithm for multi-core platforms is based on the hashrf algorithm [cit], a fast, sequential algorithm for computing an all-to-all rf matrix to compare t trees on n taxa. for a bipartition b, hashrf uses a global hash"
"overall, our results show that mapreduce is an exciting approach for developing multi-core phylogenetic applications. future work includes studying the performance of mrsrf on larger clusters and tree collections. finally, we intend to design additional mapreduce phylogenetic applications-especially as it relates to reconstructing more accurate phylogenetic trees efficiently."
"since the initial robot pose is acquired, pose tracking algroithm can be used to estimate the robot trajectory. for readability, the following abbreviations are employed to denote the alternative localization approaches. pfo: standard particle filter based pose tracking approach with odometry-based proposal. pfi: standard particle filter based pose tracking approach with icp results to replace the odometry readings. pfoi : our pose tracking approach with icp result and recent observation to calculate the accurate proposal."
"in the data tier, we selected postgres version 9.x as the sql server for its maturity, robustness, and scalability (32) . we leverage its data trigger capabilities to maintain an audit trail. when a record is updated or deleted, the triggers instruct the sql server to attach a time stamp and copy the record to an audit trail data base for later analysis as needed."
"in this paper, we evaluate the applicability of the mapreduce framework for developing multi-core phylogenetic applications. we design a new algorithm called mrsrf for computing the all-to-all rf matrix using the mapreduce framework. an open-source implementation of our mrsrf algorithm is available from the web [cit] . a heatmap illustrating the clustering of the biological trees across mrbayes runs for the 567 taxa and 33,306 tree set."
"since the information of laser range finder (usually, with a sick laser measurement sensor (lms)) equipped on robot is significantly more precise than the motion estimate of robot based on the odometry. it is reasonable to estimate motion of robot by using icp with two adjacent laser scans, which are mostly overlap. suppose the laser scan points at"
"where ( ) c i q is the closest point for i p, r is a rotation matrix and t is a translation vector. icp algorithm solves the least square problem presented in eq. (9) by using two iterative steps:"
"the central nervous system (cns) has multiple anatomic sites for tumour development. likewise, there are multiple neuropathological entities and systemic entities, which may correlate with neurologic status and ev biomarkers. we hypothesized that there is a need to expand the scope and flexibility of the system and thus used externally maintained and validated ontologies to tag tissue sources, pathological entities, and other biological events. we anticipate the need to accommodate surprising scientific insights which may emerge. for example, recent studies have correlated single base pair mutations in the isocitric dehydrogenase genes (idh1/2) with in situ and systemic accumulation of the substrate 2-hydroxyglutaric acid (2hg) as well as tumour-specific promoter methylation. each of these may correlate with predisposition to inherited disorders, sarcoma, cholangiocarcinoma, and acute myelogenous leukaemia (4). moreover, a future ev ontology, perhaps one that builds on existing work, could be adopted (11á13). the united states national center for bio medical ontology (ncbo) maintains a network of ontologies (14). we designed this biorepository system to mirror those ontologies and to use their world-recognized terms to tag data. the ontology tag hypothesis is reified in the field, clinical data, specimen query described above, and data archive and reporting use cases described in the use cases section."
"subjects may be enrolled in phase 2 or 3 clinical trials of investigational agents given according to a defined schedule. this trial information may be of importance in interpreting the results of ev rna or protein analyses. the system maintains a list by subject of clinical trials, with a link to the clinical trial identifier as assigned by clinicaltrials.gov. users may, in the gui, record the duration of enrolment in one or multiple trials. these data is then available for query and analysis."
"the eligible experimental subjects are human patients whose clinical care includes sampling of at least one paired lesion tissue and fluid (blood or cerebrospinal fluid) sample as part of a biopsy or resective procedure. at each participating institution, the research protocol was approved by the institutional committee for human subject research, and the subjects sign informed consent before participating."
"the 3 tiers of the custom application include a structured query language (sql) server in the data tier, a java virtual machine (jvm version 1.7.x) in the middle tier (31), and a client tier written in html version 5 and javascript that is deployed to a web browser. we employ, where available, open-source software libraries (table i) ."
"(1). with the transformation results get in previous iteration k, construct the new correspondence for each point in data sets p from data set q :"
"our major focus is the utility of ev-derived mrnas and non-coding rnas as diagnostic and therapeutic biomarkers of brain tumours. we assume that collaborators with the richard floor biorepository (http://www. floorbiorepository.org/) and with the abc2 brain tumor biomarker consortium (http://abc2.org/) have complementary areas of interest and expertise. global tertiary centres with large clinical volumes may contribute human specimens to research laboratories. when investigators generate a new causative or correlative hypothesis, they may seek to validate it by analysing specimens drawn from archives. so to maximize efficiency, specimens may need to be stored at and transported between many facilities worldwide. the informatic system must offer global specimen inventory tracking. this is reified in this system in the field and laboratory use cases, described further in the methods section."
"in all instances, the tissue for ev and biomarker studies is provided when clinical needs are met, and thus the tissue is viewed as discarded tissue. prior to or after diagnosis, biofluids (blood, csf, urine) may be obtained, aliquoted, and banked for scientific research. standard operating procedures (sops) for sample acquisition are indexed by date and code number for all samples with heavy reliance on the recommendations of the international society of extracellular vesicles (fig. 2) (25, 26, 29) . the virtual biorepository currently includes 3 fixed locations with postal addresses (partially visible on the right hand panel of fig. 2 ) holding a total of 4,392 physical specimens. fig. 2 . main gui screen shot. the portion controlled by the google apps domain is cross hatched. program functionality, data structures, meta data including ontology networks, and asset relationships are unified into a single network graph theory model. this model is rendered with an open-source data visualization library that we extended to offer interactive data queries and data editing (17) . in this example screen shot the mouse hovers over the locations node, causing an overview map of this global virtual biorepository to appear in the sliding window on the right."
"qualified study personnel may be present in the operating room, the clinic, and the laboratory to acquire tissue and biofluid specimens. the system records the field site and provides unique identifiers for subjects and specimens. the identifiers are applied as bar codes to specimen containers. b. the clinical data use case refers to records of clinical presentation, laboratory findings, imaging studies such as magnetic resonance (including the images per se), comorbidities, medications, therapeutic concurrent clinical trials, therapeutic interventions, and outcome metrics, including quality of life, progression-free survival at 6 and 12 months and overall survival. these data are linked to specimens via barcodes to afford biomarker assessment via correlations with tissue and biofluid ev analytic results. c. the laboratory use case provides for preparative techniques prior to storage or analysis of biofluids. these techniques include use of specialized tubes, low-speed and high-speed centrifugation, ultracentrifugation, filtration, addition of anticoagulants, addition of stabilizers such as lithium chloride, and reversal of anticoagulants such as heparin. specimens may need to be prepared differently according to the biofluid, time interval, or analytic requirements according to different protocols. recording is made of processing and storage variables such as concomitant medications, haemolysis, thrombocytopenia, lymphopenia that may influence analytic results. these sops may be contained in documents that are stored in the systems document repository. d. the specimen inventory use case refers to the location tracking of all specimens, specimen containers, and fixed storage facilities distributed globally. to quickly locate a given specimen among potentially millions distributed globally, the location of each container and each specimen needs to be tracked. outer containers and buildings are tracked with respect to their postal addresses and geo coordinates (latitude and longitude). internal containers and specimens are tracked with respect to their external container and the grid coordinate of their slot in the external container. to find a given specimen, the system thus produces a list of container bar codes and grid coordinates. the specimen inventory use case includes aliquots of specimens and the tracking of residual aliquot quantities as they are consumed by laboratory analysis. the system do not have at this time an explicit method to record specimen-handling exceptions such as those might occur with a freezer failure."
"the specimen transport use case thus includes the placement of specimens in a shipping box, recoding the shipper-assigned tracking key, use of the tracking key to monitor the transport of specimens, and the receipt acknowledgement and storage of the specimens by the receiving site. g. the collaborative science use case includes the provision of a common platform for investigators to maintain activity calendars, bulletin boards, special interest announcement lists, video-conference tools, and data sharing. this supports the pooling of documents that separate institutions may require such as example documents for institutional committees for research on human subjects, exemplar consent forms, biofluid-handling sops, and investigator instruction tutorials. h. the clinical data use case refers to records of clinical presentation, laboratory findings, imaging studies (such as magnetic resonance including the images per se), comorbidities, medications, therapeutic concurrent clinical trials, therapeutic interventions, and outcome metrics, including quality of life, progression-free survival at 6 and 12 months and overall survival. these data are linked to specimens via barcodes to afford biomarker assessment via correlations with tissue and biofluid ev analytic results."
"g liomas are the most common type of primary brain tumour in humans. they include 3 types with varying levels of malignancy: astrocytoma, oligodendroglioma, and mixed oligo-astrocytoma (1) . glioblastoma multiforme (gbm) is the most malignant and common form (who grade iv) with an incidence of about 3 cases/100,000 individuals per year and a 5-year survival rate of about 3% (u.s. central brain tumor registry; www.cbtrus.org) (2) . standard-of-care treatment may involve surgical biopsy, partial or gross total resection of the lesion, multiple forms of radiation, and specific protocols for chemotherapy. these therapies are not curative for gbm. treatment decisions are complicated as disease burden is monitored chiefly by magnetic resonance imaging (mri) and assessment of neurologic symptoms. this presents a significant problem, as after therapy there are frequently abnormal neuroimaging findings due to necrosis and other effects of treatment, which may be difficult to distinguish from tumour regrowth per se. the definitive assay remains rebiopsy of the tumour for neuropathological analysis. therefore, there is a need for biomarkers to diagnose tumours, to monitor the status of glioma recurrence following surgery and to monitor therapeutic responses. the identification of valid biomarkers is also important in the management of other types of cancer, including breast, lymphoma, melanoma, lung, pancreas, prostate, and colon. some biomarkers of interest include mutant/ variant mrna of the epidermal growth factor receptor, egfrviii, and isocitric dehydrogenase (idh1/2) mutations preponderantly found in gliomas for which they correlate with survival (3, 4) . the scientific biomarker pursuit entails significant informatics challenges including those of global specimen assignment and the correlation of clinical, imaging, and research extracellular vesicle (ev) data (fig. 1) ."
"to properly align the simulated virtual object with the real world scene and produce a consistent simulation, an ar system must track the position and orientation of the camera respecting to the real world. this is typically performed by using ar markers with known shapes or textures ( [cit] ). recent development using feature extraction and recognition provides a new approach for ar tracking without needing any marker, however, this approach is still under development ( [cit] )."
"a second approach is proposed in order to get a better approximation. in this approach, not only the ldr detecting more light is considered, but also the others immediately next to it. thus, the yaw angle is calculated as an interpolation of those three measures. similarly, the pitch rotation of the light source is calculated as an interpolation between the ldr detecting more light and the ldr on the top."
"the first map stage similarly to hashrf, the first mapreduce phase is responsible for generating the global hash table. that is, every bipartition is given a unique identifier (key). its values are the tree identities (tids) that contain that bipartition. the number of mappers correspond to the number of cores utilized on a particular node. each mapper sends its trees to hashbase to create a local hash table. hashbase is our name for a modification to hashrf that outputs a hash table from its input trees. each line from the hash table that is provided to mrsrf (p, q) from hashbase consists of a bipartition b i and the associated list of tree ids that were found to share it. in addition, all the bipartitions that are found are given a marker to denote which input file created it. this is to ensure that bipartitions shared within a tree file are not compared to each other. the bipartition and its list of tree ids form a (key, value) pair, which is emitted as an intermediate for the reduce stage."
"in the 2d plan, a line can be easily parameterized as a collection of points (, ) x y as follows : cos sin sin cos"
"the various use cases are deployed to the user on a web browser in this system as currently programmed and deployed, but some use cases may be supported better by lighter weight, mobile table-style devices with the application written in a native library rather than for a web browser. for example, the clinical data use case may call for personnel to go to an outpatient clinic setting to capture clinical and outcomes data. the specimen inventory use case may call for personnel to present to a pathology laboratory to acquire tissue not needed for diagnosis. in each of these instances, the use case may be served more efficiently if the study personnel carried a tablet device with which to enter a new specimen and its bar code or to capture more clinical data. we intend thus to observe the usage patterns of the current setup, and as needed create, test and deploy tablet device interfaces."
"virtualized (cloud) deployment allows increased specialization and lowers the cost of deployment (74) . the specialization refers both to hardware and software services. the hardware includes the rental of computer units, input/output bandwidth, and long-term storage. the software services are, among others, the maintenance of passwords and the offering of authentication and authorization services. systems administration tasks are automated by the data centre or are managed by data centre personnel, who have training in and conform to best practice guidelines in the numerous tasks involved in deployment. this lowers the complexity and cost of global, multitiered deployment. this, in turn, allows the developers to focus on design and programming."
"given the quantization precision of the parameter pairs (, ) r , we can build two hough tables [cit] . take each point and vote for all lines that could go through it, we can get two discrete hough transforms (dht) with following relation:"
"efficient collaboration is also advanced by the sharing of documents, calendars, electronic bulletin boards, and by live communication through video conferencing. a human subject has a disease condition affecting an internal organ that may be directly sampled at risk higher than sampling of fluids such as serum. the fluids may have microvesicles whose analysis may shed light when aggregated to other clinical, laboratory and imaging information on the status of the internal organ. the goal is to develop improved inferential models of the state of the internal organ from fluid microvesicle analysis, and use that model better to guide therapy."
"for global scale deployment, the world medical association provides policy guidance for medical research privacy, and there appears to be convergence in the requirements of various nations (55á57)."
"hierarchical clustering, using the hclust function in r, is used to cluster highly similar cells in the heatmap to each other. for the 150 taxa tree collection, the trees from one run are not similar to trees in the other run suggesting that two different summaries are required to encapsulate the evolutionary relationships among the trees (see figure 8 )."
"a biorepository conventionally supplies methods and resources for the collection, processing, storage/inventory, and distribution of biological specimens, all under a quality management program to promote consistence with evolving standards (28) . to serve the particular goal of biomarker identification, this biorepository augments the conventional offering with informatic resources for the acquisition and storage of correlative clinical, laboratory, and imaging data."
"e. the specimen query use case identifies biorepository specimens to be selected by an investigator for research studies. for example, when specimens are needed for studies of glioma pseudo-progression (30), we support a query of specimens from subjects with glioblastoma who have undergone radiation and whose brain mri scans reveal oedema manifested by elevated t2 signal and gadolinium enhancement. the system responds to this query with a list of specimens that meet the criteria along with their locations. these specimens may then be transported to the investigators laboratory. the specimen query, inventory, and transport use cases work in coordination. f. the specimen transport use case allows the biorepository to be virtual and globally distributed. some institutions may specialize in acquiring specimens by virtue of their large clinical practices. other sites may have unique expertise in a particular molecular analytic method. while this coupling enhances efficient science, it requires the transport of specimens."
"the cloud deployment provider selected for this deployment meets security standards much higher than can be met for example with self-managed deployment. the data centres offer multilayered firewalls, intrusion detection, and proactive security policies. the cloud deployment provider selected for this custom application has multiple security certifications and has undergone multiple security and quality audits. these include iso 27001 certification (75), payment card industry (pci) data security standard (dss) level 1 certification, federal information security management act (fisma) moderate authorization and accreditation, and federal information processing standard (fips) publication 140á2 validation (76) . moreover, it hosts hipaacompliant commercial services."
"the application is deployed as two separate instances of an ec2 server each running linux centos 5.x, which includes the security enhancements package selinux contributed by the united states national security agency (46, 58) . each server has 2 gigabytes of random access memory. one server holds the database (postgres version 9.x) and is not web-accessible. there is full disk encryption on the disk that holds the database. there are daily encrypted backups of the database, including the audit trail. the other server runs the middle tier. the java application is deployed in a tomcat 6.x container (59). calls to the application are mediated by apache 2.x httpd and forwarded to the tomcat container (60) . encryption policies are stipulated declaratively in the configuration files of the apache 2.x httpd and tomcat layers declaratively, in addition to in the wrapper (google apps or liferay) and custom application. as per routine, the ec2 server instances are behind a security firewall (fig. 7) ."
"establishing the fastest sequential algorithm we evaluate the performance of mrsrf on our computational platform as we vary the number of cores, the number of nodes, and the problem size of interest. first, we establish the fastest sequential algorithm in order to compute the speedup of our approach. speedup is defined as"
"another important aspect of an ar system is portability ( [cit] ). an ar system with high portability would give the user a greater degree of freedom during the interaction with the augmented environment. based on the positioning of the display, there are two categories of ar systems with high degrees of portability: head-worn ar and hand-held ar ( [cit] ). from these two categories, the hand-held ar systems are currently considered to be the best for introducing ar to the mass market due to its low production cost and ease of use ( [cit] ). the proliferation of smartphones that could be used as hand-held ar systems also helps the ease of adoption by potential users. similarly, headworn ar is a viable option nowadays since there are in this work we present a low-cost approach to detect, in real time, the light source origin using portable devices, like android and arduino. our work allows the ar developer to illuminate virtual objects according to the real illumination of the ambient, improving the integration of the scene."
"below, we describe the biological trees used in this study were obtained from two recent bayesian analysis. http://www.biomedcentral.com/1471-2105/11/s1/s15 with 1,651 aligned sites [cit] . two independent runs consisting of 25 million generations (with trees sampled every 1,000 generations) were performed with four independent chains in mrbayes using the gtr+i+γ model. 2. 33,306 trees obtained from an analysis of a threegene, 567 taxa (560 angiosperms, seven outgroups) dataset with 4,621 aligned characters, which is one of the largest bayesian analysis done to date [cit] . twelve runs, each with four chains, ran for at least 10 million generations in mrbayes using the gtr+i+γ model. trees were sampled every 1,000 generations."
"in this section, we will introduce how to use iterative closest point (icp) [cit] algorithm to improved the motion model. icp algorithm is a popular algorithm in robotics, which can assign correspondences between two sets of points and recover the transformation that maps one points set to the other. it is simple and has computational complexity, while it has accurate results."
"we treat the top levels of the gui in the custom application as a network data visualization problem. in this formulation, the elements of the data model, meta data, and of program functionality are treated as nodes and the relations between them as edges in a graph (e.g. see the left side of fig. 2) . as a rendering library, we chose the spacetree module of the infovis javascript library (27) . since this library is open source, we were able to extend it to meet our needs by attaching event handlers to the document object model (dom) elements that represent nodes as infovis renders them in the network graph. the event handlers respond to mouse clicks on the visual node as appropriate, for example, by opening tabular data editing forms on a sliding side window of the gui. moreover, we created a declarative programming interface between the infovis javascript library and the d2w rule-based gui programming system. we hope this declarative interface facilitates the adaptation to new data structures that may appear in the course of ev science."
"one graph database, bio4j, a bioinformatics-oriented derivative of neo4j, comes prepopulated with data from uniprot kb, gene ontology, uniref, refseq, ncbi taxonomy, and expasy enzyme db (47). at installation, bio4j version 0.8 thus has more than 190 million nodes and more than 1.2 trillion edges (relationships between nodes) (68) . a transition from postgres to bio4j would offer the inclusion of these ontologies for potential use in this system, possibly affording in turn a rich platform for the incorporation of an ev ontology. such an ontology may leverage underway efforts in ev systematics such as exocarta, vesiclepedia, and lipid maps (11á13,69). the combination of bio4j with this system's graph-oriented gui may also offer a convenient way to implement the visualization of ev-related metabolic pathways in relation to other metabolic, proteomic, or genomic pathways."
"the system consists of a custom application built in a 3-tier architecture as commonly employed in contemporary internet commerce. the custom application is integrated primarily via web services with several external systems, to comprise the biorepository informatic system and its aggregate functionality."
"this system integrates with several external informatic and computer systems to enhance its functionality and reduce de novo programming tasks. the external integrations are listed in table iii . the live link to ncbo has been described earlier. there are google integrations that augment functionality in several respects. the name floorbiorepository.org was registered with the internet domain name system (dns), and a google apps for business account was purchased for it. this account functions as the wrapper for this informatics system. by means of administrative screens supplied by google, users of this system are assigned a login name and password by a floorbiorepository. org administrator. authorized users must be assigned to a group within the floorbiorepository.org domain that has authorization to use the various digital resources of this system including the custom application. the custom application described in this paper is deployed in linux (centos 5.x (46)) instances on amazon elastic compute cloud (ec2) (48). it is embedded in the floorbiorepository.org google apps domain using the opensocial and google gadget specifications. when an authorized user logs into the system and navigates to this custom application, a cryptographically signed token is passed from the google container to the embedded application using the oauth (a protocol for the secure exchange of authentication and authorization between cooperative systems) (49). the custom application maintains its own list of access permissions. when it receives a cryptographically signed token that is unique to a user, it allows access to that user to elements of information according to its own policies. all exchanges of information between integrated systems are encrypted. in fig. 2, the cross-hatch area depicts the portion of a typical screen handled by google apps whereas the central non cross-hatched area is the custom application. it maintains a set of access policies separate from the google apps domain. the privacy and security exposures and their respective countermeasures are further described in the privacy, security, and permissioning section."
"the specimen anatomic sources are presented with respect to the preferred ontology name given by the foundational model of anatomy in table v (61,62 ) and the pathological diagnoses of tissue specimens with respect to snomed in table vi (63) ."
"in order to test our system, an ar application running on an android device was implemented. unity3d 2 with vuforia 3 were used to create the application."
"the detection of subtle relationships between biomarkers or between biomarkers and clinical data may be enhanced when information is aggregated from multiple modalities of clinical data, such as history, comorbidities, medication, physical examination, therapeutic interventions, laboratory results, and radiological imaging exams. for example, the elevation of a certain ev biomarker expression when complemented by magnetic resonance perfusion imaging changes might signify glioma progression more reliably than either alone. this multimodal clinical data hypothesis is reified in the field and clinical data use cases described in the use cases section."
"in this section, we outline the experimental results and some analysis. to demonstrate the good performance of the proposed algorithm, it was tested on an opened data set collected by a robot, which is equipped with a lms. and the data set is collected in indoor environment (eliazar, a. & parr, r., 2002) . the gird map of the indoor environment is depicted in fig. 3 . all experiments are tested on a pc with 3.0 ghz dual-core. since the ground truth of robot trajectory is not available, a fixed number of 100,000 particles are used to estimate the approximate trajectory."
"we employ an object-relational mapping library in the middle tier to transport and operate on the data between the data and client tiers. because of the ontological richness of biomedical vocabularies, we sought a library that would facilitate the representation of a complex data model in the gui. with these desiderata, we selected webobjects because it has a mature object-relational mapping system (enterprise objects foundation) and a rule-based way to map data objects to gui objects (direct to web) (35á37). with d2w, many core tasks such as presentation in html of query, list, and edit screens can be programmed by means of editing a gui rules file á significantly reducing the quantity of handwritten computer code that needs to be generated, tested, and maintained. this may simplify the task of modifying the system when the science grows in unexpected directions."
"our results are encouraging and future research must include the development of a pc-free sensor, such as a stand-alone sensor or one physically attached to the mobile device. such device should be able to interact with the ar software without needing any extra hardware. we must continue investigating new hardware in order to achieve a portable sensor, thus maintaining the low cost of the system. finally, the detection of multiple light sources and the light color and distance were not considered for this work. in order to accomplish this, further work is planned to extend and improve the sensor features."
"in this paper, we proposed an improved approach to solve robot global localization in indoor environments. it can acquire the initial knowledge of robot's pose in short time, with only one laser scan data. then a particle filter can be used to track robot pose. in order to calculate accurate proposal distribution, our approach replaces the noisy odometry readings by icp results and takes into account the recent observation. when it estimate robot trajectory, the proposed approach employs a fixed number of particles. it has been tested and evaluated on an opened data acquired with a robot equipped with laser range finder scan. to get the same precise localization results, the number of particles needed in the proposed approach is one order of magnitude lower than that of previous approaches. experiment results show that the proposed approach can accurately localize robot in real time."
"this recursive bar coded geo mapping allows for a global drill down effect in the gui whereby the user can click with a computer mouse on a symbol representing a container on google maps. this opens a sub-window with a geometric model of the container's contents, and the user can proceed until an individual specimen is localized. conversely, a user can identify a specimen based on a query that includes clinical characteristics as described previously, and the system offers a ''bread crumb'' trail of containers leading to the specimen, thereby facilitating the ability to find physically a desired specimen among potentially millions."
"the top level nodes in the main screen of the gui of the custom application are primary data, meta data, workflow, assets, and team (fig. 2) . when a user clicks on a node, child nodes appear. some nodes offer extra options, as cued by small icons in the node. for example, in depth information on the topic of the clicked node may appear on a sliding window on the right side of the fig. 6 . google spreadsheets may be programmed with the apps script programming language and library to make encrypted web service calls to the custom application to retrieve fresh data with which to populate spreadsheets and related analysis and reporting tools. this allows the fresh retrieval of data for analysis as the scientific needs may dictate."
"in order to estimate the light source direction, we need to estimate the angle of incidence from the light source to a certain point in space. hence, we can obtain an approximation of the light source rotation and therefore, estimate its direction. a photo-resistor or light-dependent resistor (ldr) is a very common and low-cost sensor which decreases the current resistance according to the light intensity detected on its surface. a semi-sphere shape sensor was built to sense illumination from different angles. this semisphere has an array of 8 ldrs along its surface in order to estimate the incident angle of the light source in the yaw rotation axis, each one separated by 45 degrees. also, one last ldr is located on the top of the semi-sphere. the location of the ldrs can be seen in figures 1 and 2 . together with the other ldrs, the angle of incidence of the light source in the pitch rotation axis can be estimated. the sensor is shown in figure 3 ."
"this biorepository informatic system is currently in an early phase of implementation. it is chiefly tasked now with the recording of clinical, laboratory, and imaging data, and the geo tracking of specimens. as time passes, the usage patterns will be where the underlying hypotheses are tested. other informatic systems with a narrower purpose may prove to be a better choice for specific tasks covered comprehensively by this system. for example, projects such as openphacts may provide generic tools for data storage and sharing that may obviate this independent offering in this system (77) . other systems may be embedded into the gui of this one if their architecture lends to integration with single sign on services and the cross embedding of html fragments."
"in the client tier, the javascript language and the jquery library are used heavily in html to control use of the bar code capture device, provide the widgets with live links to the web service maintained by the us ncbo, and implement the screens for visual navigation of the program and data model (38,39)."
"after calculating weight for all possible pose, pose with largest weight can be selected as the initial robot pose. in order to eliminate the quantization errors, gradient descent search algorithm can be employed to get more accurate initial robot pose."
"the parameters of s j, n and f j, n describing respectively the standing wave and the radiation wave for each layer can be formulated as"
"consider the scattered wave field generated by a three-layer structure shown in fig. 1 when illuminated by the incident plane wave ∑ where k 0 is the wave number in the surrounding medium, r n represents the scattering coefficient, j n is n-order bessel function and h n (1) is n-order hankel function of the first kind. the wave in each layer can be represented by a sum of standing wave field p j sta and radiation wave field p j rad :"
"it is of great scientific significance and application potentials to pursue an acoustic cloaking structure exempted from spatially-varying parameters and large layer number in acoustic field. we have presented a dual-layer acoustic cloak made up of homogeneous single-negative medium in this work, which cancels the scattered field from a circular object. instead of using the transformation acoustics technique to mathematically derive the desired parameters that are proven to strictly satisfy the complementary condition, here we propose an essentially different scheme of producing non-blind invisibility by straightforwardly eliminating the impedance mismatch responsible for the scattering effect to guide the incident wave into the cloaked region reflectionlessly. we give analytical analysis on the scattering by multilayer concentric circular scatterers and derive the analytical solutions of the scattered field. based on this, a genetic algorithm is used to obtain the optimized material properties for the dual-layer cloak which ensure global minimum for the scattered field. we demonstrate the effectiveness of our scheme via numerical simulations, showing that the cloaked object is able to sense the outside world while causing negligible disturbation to the original incident field. moreover, the cloaking effect is robust against the deviation of the material parameters of cloak from the perfect values. the findings may significantly facilitate the experimental realization of acoustic cloaks and shows many potential applications in a variety of practical situations such as high-precision acoustic measurements in which a reduction of the disturbance caused by acoustic sensor is highly desired."
"where t 3, n is determined by the standing wave. then the parameters of m j, n and r j, n can be expressed as"
"in addition, the perfect restoration of information carried by the incident wave needs the two layers to support the attenuation and amplification of evanescent wave respectively, which means the mass densities of the two layers need to be oppositely signed 15, 16 . it is therefore expectable that in cases (c) and (d) we would not be able to satisfy such a requirement for rebuilding the incident wave in the cloaked region, which has been verified via numerical simulations 16, 17, as is shown in fig. 2(d) and (e). as a consequence, cases (a) and (b) are the only cases in which the designed structures can be expected to exhibit the potential to yield the desired cloaking effect via parameters optimization."
"here a genetic algorithm (ga) is used to find the optimal design parameters for generating the minimized scattered field at the desired frequency 20 . based on charles darwin's theory of evolution 21, 22, the genetic algorithm is an effective optimization method to search for the optimal solution by simulating the process of natural selection. in a genetic algorithm, once the approximate ranges of the parameters are known, the possible solutions can be encoded by a number of binary genes. the best fit genes would be passed on from one generation to the next. after the process of repeated ranking, selection, crossover, mutation in a number of iterations, and the evaluation by the objective fitness function, the global minimum in the search space can be found."
"the optimized structure parameters of our dual-layer cloaks are obtained with genetic algorithm (ga). the algorithm selects the fittest individuals and performs the operations of crossover and mutation to create the next generation. the ga begins with the initial population size being 50 and other various parameters such as mutation rate being 0.2. the initial population is first generated and for every following generation, the individuals are ranked based on the fitness scores to find the survivors to mate for creating the next generation. this process would be repeated until the fitness scores satisfy the stopping criteria."
"numerical simulations are performed by comsol multiphysics software. the background medium is chosen as water, for which the mass density and sound speed are 998 kg/m 3 and 1480 m/s respectively. the structural parameters yielded by the optimization method are used in the simulations for the cloaking media. perfectly matched layers are used to eliminate the reflected waves by the outer boundaries."
"in this article, we propose to produce non-blind acoustic invisibility by using a dual layer structure filled with two homogeneous single-negative media, breaking through the limit in the transformation acoustics-based designs. based on an inherently different scheme that straightforwardly realize a perfect impedance match between an object and the surrounding medium, the proposed scheme enables the ingredients of cloak not to strictly satisfy the complementary condition indispensable for the existing approaches. by developing the analytical formulae to fast calculate the scattering strength from a circular object and then employing genetic algorithm as an efficient method to achieve the optimal configuration for maximal scattering cancellation. as a result, the designed cloak only needs to consist of a negative mass density and a negative bulk modulus media with judiciously designed parameters, which represents the simplest configuration ensuring the fundamental requirement of evanescent wave restoration [cit] . the performance of the resulting cloak is demonstrated numerically via production of non-blind invisibility effect that allows the cloaked object to receive the incident signal undistortedly while suppressing its scattered field to near-zero. quantitative evaluation on the cancellation of scattering field further reveals that in comparison with a transformation acoustics-based non-blind cloak with the same thickness and parameter complexity, our designed cloaks give rise to an invisibility phenomenon with equivalent efficiency despite the full simplification in its parameter distribution. we have also demonstrated the robustness of the cloaking effect against the deviation of the structural parameters of the cloak, which helps to substantially facilitate the fabrication and application of the non-blind cloak considering the avoidable difficulty in exactly achieving the predicted negative parameters in practice."
"the gdeep type system can check only gdeep expressions; it must be paired with an artifact type system to check the artifact expressions. as illustrated in the previous example, however, the artifact language and gdeep are intertwined. in particular, the artifact language may use paths such as x.a (syntactic sugar for x@(x).a) in artifact declarations. paths are the primary interface between the artifact language and gdeep; they allow artifact code to traverse the gdeep module structure. all of the languages that we consider in this paper rely on paths. figure 5 formally defines the simple artifact language that we have been using so far in our examples. the language has two types, int and string, and supports arithmetic expressions and string concatenation. we have deliberately made the language very simple, but it nevertheless demonstrates the key mechanisms by which an artifact language can be integrated into gdeep in a type-safe way."
"in addition, every overriding declaration in a derived module must be a subtype of the original declaration in its parent. in the example above, note that the definition of a within m2 inherits from original x .a. this pattern of inheritance is enforced by gdeep's static type system."
"second, artifact typing must assign a type to expressions of the form m@(n).l. it does so by invoking the gdeep type system. it (1) ensures that m@(n).l is well formed and (2) uses the gdeep subtyping judgment to look up the declaration associated with m@(n).l. third, the artifact language must define subtyping and well-formedness rules for artifact declarations. these rules plug into the gdeep type system; a module is only well formed if all of its declarations are well formed. in this case, an artifact declaration is well formed if it is well typed and if the type of the expression matches the type that was declared for it. one declaration is a subtype of another (i.e., one declaration can be overridden with another) if they have the same declared type."
"usdp can be considered a generic framework for software development, which is instantiated in each specific development project. in between, \"project templates\" can be created 3 which constrain, inherit and extend the process defined by usdp so as to account for specific software engineering concerns. this is the approach we will follow in this paper: show examples of usdp-defined artifacts particularized to the development of accessibility evaluation tools. the artifact templates we propose deal with the different disciplines dealt with by usdp: a business model and a domain model to kick off the requirement elicitation, a use-case model to represent the functional requirements, an analysis model to reflect an intermediate view of the architecture, and a deployment model to describe the physical bindings. these artifacts will still be abstract, in that they do not reflect the conditions of a concrete accessibility evaluation tool, but they offer general directions that can be applied to any of those tools whatsoever. that is why we just talk about guidance, which will still need other inputs from the specific business to come with the concrete artifacts in each case."
"alternatively, the test designer may specify a unique identifier of the test subject, so that the tool may retrieve it when (or before) processing the evaluation. in order for that identifier to work, it must be part of a namespace shared between the test designer and the tool and accessible by both. for instance, paths to the local file system can be used if both are working on the same local node, and have equivalent access permissions. urls are also good candidates, but there is no guarantee that a server will serve the same contents to different clients requesting the same url. a detailed specification may account for context attributes which may determine the result provided by the server, e.g. user agent properties (user agent version and platform, preferred content types and languages, ...), user credentials, and browser state (cookies, cached versions, …). some context attributes are difficult to emulate (e.g. request time, ip address) or even impossible. moreover, the content of the test subject may change since the test is specified until it is executed; hence a potential trade-off may exist between the precision and the simplicity of the specification. to test some dynamic behavior or stateful products, more complex test fixtures can be defined, which may include a set of actions (retrieve several pieces of content in a specific order, generate user interface input events, interpret software scripts), before the test execution. this may even require manual user intervention to lead the evaluated system to the desired status."
"the artifact language in figure 5 is sound only if the type of m@(n).l is preserved under artifact reduction. artifact reduction merely invokes gdeep reduction, so this result follows from the soundness of gdeep."
"bali is a tool for synthesizing program manipulation tools on the basis of extensible grammar specifications [cit] . it allows a programmer to define a grammar and to refine it subsequently, in a similar fashion to class refinements in jak. figure 2 shows a grammar and two grammar refinements that correspond to the jak program above. the base program defines the syntax of arithmetic expressions that involve addition only. we then refine the grammar by adding support for multiplication and variables."
"second, it is possible to use artifact-specific type systems in concert with the gdeep type system. for example, the standard java type system is responsible for assigning types to java expressions, which always occur in atomic modules. the type of an expression in java is the name of a class (or a basic type). we integrate the java type system into gdeep by replacing the default java class lookup mechanism. the gdeep type system becomes responsible for looking up class and method names within features, since such names cross module boundaries (see section 6 for more details). the core theorems of deep ensure that this combination is sound. however, a discussion of deep's core theorems is out of scope, and we refer the reader to hutchins [2006 hutchins [, 2009 ."
"we propose a formal, language-independent model of feature composition and program refinement in the form of a calculus. our goal is twofold: (1) to provide deeper insight into the principles of feature composition, and (2) to develop generic algorithms and tools that can be used to analyze and manipulate various kinds of software artifacts."
"the next section will introduce our vision of accessibility evaluation tools and the process model we will follow; the rest of the paper will provide specific guidance for the different models employed in the process, namely: business process model, domain model, use case model, analysis model and deployment model. we have deliberately omitted references to specific tools, as we want this guidance to be as broadly applicable as possible."
"modules in gdeep are similar in some respects to formal models of objects, for example, by abadi and cardelli [cit] and boudol [cit] . boudol in particular shows how extensible objects, classes, and even mixins can be implemented by using generating functions-functions that take \"self \" as their first argument. modules in gdeep are essentially generating functions, which are applied using delegation: m@(n).l. the gdeep type system, however, is completely different from that used by boudol, because gdeep relies exclusively on subtyping, whereas boudol uses record types with row-variables."
"type errors will be detected before composition. examples of type errors are: referring to a slot (or calling a method) that does not exist, calling a function with an invalid argument, or overriding a declaration with a value that is not a subtype of the original. the following example illustrates these errors, using a simple artifact language that supports integer arithmetic and strings:"
"we have presented the formal syntax, operational semantics, and type system of gdeep and illustrated what a language needs to provide when it is plugged into gdeep. we have demonstrated that a wide variety of very different artifact languages can be used with gdeep in order to enhance them with feature composition capabilities. in an accompanying technical report, we explain how we adapted and developed formalizations of java, bali, and xml and plugged them into gdeep [cit] ."
"pattern-based specification allows test designers to select at once all those test subjects whose identifiers match some kind of pattern (e.g. a regular expression). patterns may be used to select contents of a specific type (based on its file extension), on a specific directory, etc. pattern application requires an underlying query mechanism to be available which guarantees that all contents matching a pattern may be retrieved. distributed systems such as the web do not usually provide these functionalities, so pattern matching is usually combined with link traversal, effectively crawling web sites as a heuristic way to retrieve all their contents (same as what search engines do when indexing web sites). even after the pattern is applied, the tool may come up with a large amount of potential test subjects. filtering and sampling functionalities, either automatic or assisted by the test designers may help in creating a manageable set of subjects."
"an intermediate case is that of evaluation tools which run on the server side, but do not interact with the client. for instance, a tool may be a module of a content management system that validates the accessibility of the contents it stores. the tool runs on an application server, but is only invoked from within."
"a complete software system does not just involve java code. it also involves many non-code artifacts. for example, the simple expression evaluator in figure 1 may be paired with a grammar specification that provides a concrete syntax for expressions."
"after an evaluation has been performed, results will be presented to a test consumer, who may or may not be the same person who ran the evaluation. for instance, test engineers may design and run evaluation tests, and then return the results to the development team so that they can fix any issues. or an accessibility consultant may present the results of an accessibility evaluation to their clients. or a company may be presented with the results of the accessibility evaluation of a corporate site developed by a contractor. or a user association could get the report of a third-party site created by an external auditor."
"the expression problem illustrates a broader issue called the \"tyranny of the dominant decomposition\" [cit] . in most programming languages, it is possible to extend either the data type or the set of operations, but not both at the same time. the data type and the operations are two different concern dimensions. although both dimensions are conceptually of equal importance, existing languages require code to be factored in such a way that one is prioritized over the other. in an object-oriented language, the interpreter design pattern prioritizes the data type, and thus allows new variants to be easily added, whereas the visitor design pattern prioritizes the operations. no matter how the code is factored, concerns belonging not to the \"dominant\" dimension cut across the implementations of concerns belonging to other dimensions [cit] . the implementation of such a \"crosscutting concern\" is scattered throughout the code, where it is difficult trace for the programmer and difficult to extend with standard programming language constructs."
"bali is a simple artifact language, because it does not have any compound elements, that is, there are no nested grammar production rules [cit] . as a result, it is very easy to plug it into gdeep. the gdeep calculus provides a hierarchical module system, and all expressions in the artifact language become atomic declarations. the module calculus and the artifact language are almost completely orthogonal."
"strictly speaking, the accessibility of a product can only be stated regarding specific user needs, goals and contexts of use 2 . if a certain user can access it under those conditions, it will be accessible for him or her. it is unreasonable to test this in each possible context of use (real or emulated), so a set of practical criteria have been agreed, based on community experience, as a reference to determine whether a product is accessible or not. even then, it is not viable to manually test each and every criterion. thus, accessibility evaluation software tools have come to relieve the evaluator of that burden, by automatically analyzing a product and issuing judgments regarding its compliance with accessibility criteria. these tools greatly simplify the process, but cannot provide a definite replacement for human judgment. this is why they are sometimes called semi-automatic evaluation tools. for instance, a software tool may detect whether an image is served with an alternative text, but cannot decide if both convey an equivalent meaning, yet it can point the evaluators towards the image to get their expert assessment."
"features in gdeep are represented as either modules or functions over modules. a base feature is an ordinary module, whereas a feature that performs a refinement is a function that transforms a module. modules have a named hierarchical structure; a module may contain submodules, subsubmodules, and so on."
"missing implementations are not automatically detected by gdeep, because missing implementations are artifact-specific (e.g., missing implementations cannot occur in the simple artifact language shown in figure 5 ). instead, missing implementations will be detected after feature composition, when the artifact program is compiled."
"xak explicitly establishes a module structure for xml by associating a module name with a particular xml element using the attribute xak:module. we use the same strategy for gxak. figure 8 shows an except of the previous xml code, written in gdeep. this example divides the xml document explicitly into three named submodules: contents, operations, and main. note that, unlike the original definitions, contents and operations are no longer subelements of main. instead, main refers to contents and operations by means of the paths x.contents and x.operations. gxak differs from xak because it uses paths rather than element annotations to establish a module structure. however, the net effect is the same. figure 9 shows a simple implementation of the expression evaluator, supporting addition, evaluation, and multiplication, written haskell and gdeep. haskell supports a style of programming that is not found in java; data types and functions over those types are defined by means of cases."
"once the test criteria and subjects have been specified, the proper evaluation process may start. the execution use case may be split into several steps: triggering, retrieval, execution, and reporting."
"from the perspective of software engineering, the accessibility of a software product can be regarded as a category of quality properties 4 or requirements, which ensure that it can be used in equivalent ways (considering aspects such as comfort, security or cost) by people in the broadest range of contexts of use, specially accounting for users with different abilities. thus, an organization normally uses accessibility evaluation tools in the activities dealing with requirements (e.g. specification, testing). the workflow followed by that organization may be captured by usdp in a business process model, which shows the different business actors and the activities they undertake to deal with work units and internal process data. these may be specific to the strategic goals pursued by the entity which commissioned the tool, yet we can define a set of activities where the tool will be typically used."
"other usdp features may be leveraged to implement the large amount of fine-grained techniques generic evaluation tools should deal with. packages may be defined for related techniques (e.g. dealing with the same content type, or implementing the same criterion), successive iterations may implement different techniques, etc."
"a name clash occurs when a derived module attempts to add a new declaration, but a declaration named already exists in the parent. for example:"
"same as any other software product, the development of an evaluation tool is usually structured as a project involving a set of activities, worker roles and artifacts. the organization of this project may respond to different process models, one of the most widespread being the unified software development process (usdp). usdp 1 is characterized by being: driven by use cases, which are used as a tool to capture user requirements and intertwine the subsequent design, implementation and testing activities; centered on the architecture, which represents the internal form the product will take; and incremental and iterative, so that risks are mitigated early at a lower cost."
"jak, xak, and bali are each designed to work with a particular kind of software artifact. ahead brings these separate tools together into a system that can handle many different kinds of software artifacts."
"delegation, written using the syntax m@(n)., projects the declaration named from the module m. any occurrences of the self-variable x will be bound to n."
"a feature refines the content of a base program either by adding new elements or by modifying and extending existing elements. mathematically, we treat features as functions that transform their input in a well-defined way."
"in ahead, a piece of software is represented as a directory of files. composing two directories together will merge subdirectories and files with the same name. ahead will select different composition tools for different kinds of files. merging java files will invoke jak to refine the classes, whereas merging xml files will invoke xak to combine the xml documents, and so on."
"instead of trying to map java inheritance onto gdeep inheritance, we simply encode the extends and implements clauses as artifact-specific annotations, the final part of the translation is to map global class names like bar onto paths of the form x.bar. in java, all class names are treated as global identifiers. in fop, classes are not global-a class definition is local to the feature in which it is defined. the translation from global names to local paths ensures that classes are virtual and use late binding, which is the key to making feature composition work. figure 6 shows an excerpt of the earlier java/jak example encoded in gdeep"
"in addition, we would like to know that types in the artifact language are preserved by feature composition, that is, by gdeep reduction. there are two gdeep reduction rules. β-reduction substitutes one term for another and module composition overrides one slot with another. the soundness result for gdeep guarantees that subtyping and well-formedness are preserved under substitution, and the subtyping rule for artifact declarations ensures that overriding declarations have the same type. thus, feature composition preserves artifact types."
"the idea behind ahead is to unify approaches of fop and scale them to programming in the large. first, ahead generalizes the operations that are performed when a feature is composed with a base program. a base program is modeled as a collection of named program elements, which are organized in a hierarchical namespace. a feature encapsulates a program refinement, which is a set of changes to a base program. such changes include the introduction of new program elements and the modification of existing program elements. the hierarchical namespace allows a refinement to target any element at any depth of the hierarchy."
"a detailed presentation includes all the results of the evaluation in a human-consumable format. apart from a linear view of the results (displaying one assertion after another), navigability mechanisms should be provided for human consumption. tabular structures may ease the orientation through the different results. sorting, grouping, querying and filtering mechanisms ease locating the assertions with specific criteria, conformance levels or result values. collapsible trees hide any unwanted details while allowing users go deeper into results of their interest, etc."
"java is the most difficult to handle, because java and gdeep are not orthogonal. java defines different kinds of compound elements: packages contain named subpackages, classes, and interfaces, whereas classes contain named inner classes, methods, and fields, and so on. hence, we map packages, classes, and interfaces onto modules in gdeep. methods and fields have no named substructure, so we represent them as atomic declarations."
"a feature encapsulates a slice of program behavior. applying a feature to a program extends the functionality of the program in some way. this leads us to two typing laws for features. if f is a feature and a and b are programs, then:"
"as a side note, we wish to point out that fop is slightly less flexible than the family polymorphism in gbeta [cit] ]. in gbeta, a family of classes is encapsulated within an object, which exists at run-time. in contrast, jak performs feature composition at compile-time. features are \"erased\" from the final compiled code, and are not reified as run-time objects. the advantage of the erasure semantics is that it allows fop to be used with languages, like java, that do not have built-in support for features. the disadvantage is that, unlike gbeta, it is not possible to instantiate new families at run-time."
"object-oriented inheritance creates a new class with a different name. thus, in order to implement feature eval using standard object-oriented inheritance, we would have to create three new classes: expreval, valeval, and addeval."
"first, deep provides a formal model of deep mixin composition [cit] ], which is related to a number of techniques in the literature, including higher-order hierarchies [cit] ], virtual classes [cit], nested inheritance [cit], and superimposition [cit] . the \"deep\" part of composition scales object-oriented inheritance to programming in the large. unlike inheritance in java or c#, which support only the overriding of virtual methods, the deep calculus supports the refinement (i.e., overriding) of nested classes and submodules within a module hierarchy. the \"mixin\" part of composition allows separate refinements to be combined together, in a fashion that is somewhat analogous to multiple inheritance [cit] ."
"declarations in gdeep are similar to methods in object-oriented languages. the body of a declaration may refer to \"self \" using the self-variable of the module. in most object-oriented languages, including java, \"self \" is treated as a hidden argument, which is passed implicitly during a method call. in gdeep, the \"self \" argument is not hidden; it is passed explicitly."
"reporting: the results of each evaluation process execution may be represented as assertions, and stored following standardized formats (e.g. earl). although the initial test subject specification may be quite generic, detailed information about the effectively evaluated subjects may be included in the report. for instance, the specification might just establish that all the html documents in the web site www.example.com will be evaluated, while the report would provide a detailed list of the html files that were effectively evaluated, and potentially the exact http requests that were issued to retrieve them. reports may include aggregate measures, such as conformance to larger families, or comprehensive numeric accessibility measurements. numeric scores should account for potential barriers 7 that are correctly tackled (i.e. did not materialize), rather than simply focusing on the number of criteria that pass or fail. care should be taken to avoid presenting e.g. a 50% success rate as a \"decent\" accessibility (while it truly means that many users will encounter one or another accessibility barrier). nonlinear and weighted scoring algorithms can be applied to avoid that. although scores do not offer a total order metric, they should be monotonous regarding the (dis-)appearance of potential barriers. different scores can also be provided for different user ability profiles, kinds of criteria, content types, groups or families, etc. reports may also include information about causes, symptoms and remedies 8 . going beyond evaluation, the tool output may automatically modify the contents so as to correct accessibility barriers, becoming what is known as a repair tool."
"in summary, the process template we propose could be applied to the development of any (semi-) automatic accessibility evaluation tool -\"auxiliary\" evaluation tools which do not emit judgments at all lay out of this work."
"feature composition tools like xak require the software artifact to have a named hierarchical structure. as far as documentation is concerned, fop works well for semistructured text such as websites, reference manuals, and api documentation [cit] c] . fop is less useful for narrative documents like novels, because features may not have proper points to \"hook in\" and feature composition may disrupt the flow of text by inserting sections."
"gdeep provides two main \"hooks\" for integrating an artifact language. first, declarations in the artifact language are denoted by d (cf. figure 4) . in the case of java, d represents field and method declarations; in the case of xml, d represents xml elements, and so on. artifact declarations are completely opaque; gdeep does not interpret them."
"a test criterion specifies the test that was applied. again, test criteria may range from high-level abstract goals to tailored techniques only applicable in specific contexts. several terms are usually applied to different kinds of criteria. principles are abstract foundations which must be satisfied by any accessible product of some type, e.g. principles of universal design, wcag's (web content accessibility guidelines) pour principles (perceivable, operable, understandable and robust). guidelines define coarse-grained criteria. success criteria or checkpoints refine them into assessable requirements against which an objective judgment can be issued, yet without binding to any specific technology. techniques are applied to a specific content type or in a specific situation, and define detailed features which accessible contents must (or should, or cannot) exhibit. indeed, a tool will only support the techniques for the content types it deals with. tests or test scripts refer to a specific procedure (set of instructions) to be executed when checking the criterion. they are also called \"test cases\", although this strictly refers to a set of conditions or variables under which a determined behavior is expected. test criteria can be aggregated into families, usually compiled together and subject to standardization processes. different families apply to different technologies (e.g. wcag for web contents and applications) and can be considered test criteria per se as well."
"static type checking ensures that classes, objects, methods, etc. are properly declared and used in a way that is consistent with their declaration. linking (phases 2 and 3) ensures that every declaration is implemented and that implementations do not conflict."
"in order for an accessibility evaluation to be run, test designers first need to specify what is exactly meant by 'evaluation'. observing that assertions associate test subjects and test criteria, accessibility evaluation tools will at least need to allow test designers to specify (configure) which subjects and criteria will be used in the evaluation."
"retrieval: test subjects must be retrieved from an external storage system, according to the previously configured specification. if identification was provided for test subjects, a transport mechanism must be used to retrieve them. in order to retrieve subject contents, all the context and test fixtures defined during configuration must be first set up. if link traversal had been specified, contents must first be analyzed to find (and resolve) links."
"execution: each test criterion is applied to all the test subjects where it is pertinent. for each test subject, a typical structure is: 1) applicability rules define which points of the subject may validate / break a criterion; 2) a test procedure is applied to each point; 3) the results are compared with expected values that determine the potential test results. semi-automated criteria may need (sometimes) test requester help regarding the points, procedures and/or conditions. their involvement may include interactive interventions, postponed until the evaluation has ended, or be simply ignored and an \"unknown\" value be emitted. heuristic procedures may be applied which provide a somehow trustworthy yet not definite answer."
"to define the business process for accessibility evaluation, we place it in the context of the business process of software development. fortunately, usdp is itself described using the business process modeling technique, so we just need to identify the activities where accessibility evaluation is involved. readers should note that more finegrained detail could be provided for specific evaluation contexts. likewise, some activities may not appear in all the scenarios, but we aim at providing a generic framework that may be as reusable as possible. figure 1 shows an activity diagram where the different workers (business actors) are represented by different swim lanes, which show the workflow between the activities they perform, together with the entities shared to pass work units along. activities related to accessibility evaluation are highlighted. we must remark this business process depicts the tool usage, although it takes place itself within a larger process: the development of the product under evaluation."
"second, compound modules of the artifact language such as java classes or xak modules are represented as compound modules in gdeep. gdeep's modules can be annotated with a domain-specific construct, named ξ . ξ is defined to hold any information about a compound module that is required by the artifact language, but that gdeep does not include. for example, in our encoding of java [cit], we use it to hold the class constructor along with the extends clause (see section 6). using this information, it is possible to create a one-to-one mapping between java classes and gdeep modules. in section 6 and section 7, we explain in detail how java, bali, xml and haskell are plugged into gdeep."
"modules are composed by recursively composing their constituent declarations. the composition of two modules a and b will have all the declarations of a and all the declarations of b. if a and b both have a declaration with the same name, then those two declarations will be composed recursively. composition thus descends into the module hierarchy and recursively merges submodules together until it reaches the atomic declarations, which form the leaves of the hierarchy."
"in addition to providing an operational semantics for feature composition, the gdeep calculus also defines a language-independent type system for features based on the type system of deep. gdeep's type system provides type judgments at the module/feature level that are universal over all supported artifact types. the gdeep type system has two main capabilities:"
"second, the inheritance requirements imposed by gdeep are slightly different than those imposed by java. in particular, constructors in gdeep are treated like virtual methods; they are inherited by derived modules. constructors must be inherited in order for class refinement to work properly; if the refinement of a class was to alter constructor signatures, it would break code in other features that attempted to instantiate the class."
"rules in bali can refer to other rules in the same grammar by name. in gbali, such references are expressed as paths of the form x.l. they can also override rules of a base grammar and refer to the original definitions via original x .l. both of these terms are syntax sugar for standard gdeep delegation-m@(n).l. figure 7 shows how the earlier bali example is encoded in gdeep."
"test criteria may as well be specified in bulk or on a case-by-case basis. evaluation tools must offer test designers a set of pre-configured test criteria that reflect industry-wide accepted criteria families. on the one hand, this simplifies the activity of specification; on the other, tools can foster the adoption of these widely recognized principles that distil the acquired knowledge of the community -rather than ad hoc criteria the test designers may decide on their own. some families define different levels of conformance, which account for the fulfillment of larger (stricter) or smaller (looser) sets of criteria. tools should enable test designers to select the appropriate level of conformance against which the tool will evaluate the test subjects."
"semistructured program documentation is another example of a non-code artifact. xak is a language and tool for composing various kinds of xml documents [cit] ]. it enhances xml by a module structure useful for refinement. this way, a broad spectrum of software artifacts can be refinedà la fop, e.g., uml diagrams (xmi), build scripts (ant), service interfaces (wsdl), server pages (jsp), or xhtml. figure 3 depicts an xhtml document that contains documentation for our expression evaluator. the base documentation file describes addition only, but we refine it in a mixin style to add a description of evaluation, multiplication, and variables as well. the tag xak:module labels a particular xml element with a name that allows the element to be refined by subsequent features. the tag xak:extends overrides an element that has been named previously, and the tag xak:super refers to the original definition of the named element, just like the keyword super in jak and bali."
"both of these errors correspond exactly to link errors in other programming languages. for example, source files in c are type checked and compiled separately. the compilation phase guarantees that every routine is declared before it is used and the use of a routine matches the type that was declared for it. however, it not possible to determine, at compile-time, that every declared routine is actually implemented, because the compiler does not have access to all source files. indeed, the whole point of separate compilation is that the compiler should not require access to all source files. as a result, inter-file dependencies are resolved by the linker, rather than by the compiler. the c linker will generate an error if there are any missing implementations or if there are two implementations with the same name (i.e., a name clash)."
"there are several reasons why we have not pursued this approach. the most important reason is that the purpose of gdeep is to provide a model that is consistent with existing feature composition tools such as jak, xak, and bali. in these tools, the end result of feature composition is a program in the original artifact language that does not contain features. for example, jak produces straight java code that can be handed off to a standard java compiler. views are not a part of standard java, so our semantics must treat name clashes as errors."
"the names of declarations in a gdeep program are used to establish a module structure. in contrast, the tags of an xml document are just markup-they may or may not have anything to do with modular structure. moreover, names of xml elements do not need to be unique in the scope of an enclosing tag, for example, as in the case of xhtml. gdeep, on the other hand, requires that names are unique."
"beside composition, further algorithms and tools can be developed on top of the calculus to provide a seamless infrastructure for developing, analyzing, composing, and validating features in different representations."
we have developed gdeep as a core calculus for feature-oriented programming (fop) that encapsulates the essence of feature composition and validation. it abstracts from artifact-specific details and treats many different kinds of software artifact in a uniform way. gdeep provides three basic concepts for constructing features which are largely orthogonal: (1) a module allows a mutually recursive set of named definitions; (2) refinement statements allow a module to be extended; (3) monotone functions allow separate extensions to be composed.
"recently, following the philosophy of ahead, the featurehouse tool suite has been developed that allows programmers to enhance given languages rapidly with support for fop, for example, c#, c, javacc, haskell, alloy, and uml [cit] c ]."
"a significant body of work has explored the concept of mixin composition, for example, bracha and cook [cit], findler and flatt [cit], and kamina and tamai [cit] . gdeep builds on this work and implements deep mixin composition [cit] ] in a language-independent manner. [cit] have presented a formal system for modular linking in the presence of mixins. they aim similarly at language independence but with focus on object orientation."
"it is usual that test subjects are not specified in isolation, but aggregated as part of a larger subject. two typical ways to specify test subjects in bulk are link traversal and pattern-defined identifiers (both may be combined)."
"feature-oriented programming tools, such as jak, provide a mechanism that allows crosscutting concerns to be defined separately and then mixed together. figure 1 follows the interpreter design pattern, in which variants are classes, with a method for each operation. it splits the functionality into four separate features."
"-modules may contain both type-level definitions (e.g., classes) and objectlevel definitions (e.g., methods and data members). -modules are recursive. one definition within a module can refer to other definitions by name, in mutually recursive ways. module-level recursion can be used to define both recursive types (e.g., classes like list) and recursive objects (e.g., recursive methods). -modules are extensible and use late binding. unlike mainstream languages, late binding applies to both type and object members. it is possible to define both virtual methods and virtual types or virtual classes [cit] . -module refinement resembles object-oriented inheritance; there is a subtype relationship defined between modules. -modules can be higher-order: they can be parameterized by other modules."
"the goal of feature-oriented programming (fop) is to modularize software systems in terms of features [cit] . a feature is a unit of functionality of a software system that satisfies a requirement, represents a design decision, and provides a potential configuration option . ahead (algebraic hierarchical equations for application design) is an architectural model for large-scale fop [cit] ."
"not all errors can be detected before feature composition. in particular, there are two kinds of error that must be detected after composition: name clashes and missing implementations."
missing implementations arise when gdeep is paired with an artifact language that allows program elements to be declared without an implementation. the canonical example is an abstract method in java; such a method has a type signature but no implementation. figure 1 shows an example of missing implementations. assume feature expr is implemented as a module and eval and mult are defined as functions that extend their input. then eval(mult(expr)) has a missing implementation-class mult does not implement the eval method.
"features can be composed with other features by function composition, or composed with a base program by applying the function to yield another program. the order in which features are applied is important; earlier features in the sequence may add elements that are refined by later features. existing fop tools perform feature composition at compile-time, although this is not a requirement."
"the gdeep module system is a generalization of the deep module system and thus has all of the properties mentioned above. gdeep provides a module system that supports feature composition. it does not handle the syntax, evaluation, or typing of expressions in the target artifact language (java, bali, etc.); these must be provided by the \"sister calculus\" that it is paired with. figure 4 shows the syntax and the operational semantics of gdeep. the syntax is divided into two parts. the first part, shown on the left, is a calculus for features. this part represents the core calculus, and it is the only part which is common across all applications of gdeep."
"the second part of the calculus, shown on the right, is artifact-specific. these terms serve as placeholders for the particular language to which gdeep is being applied-the target artifact language or target language. when support for features is added to java, these terms are \"filled in\" with java constructs. when support for features is added to xml, they are xml trees, and so on (see section 4.8)."
"many test subjects include (hyper-)links to other pieces of data: web documents that link to one another other, source or object code files which import other software modules, software controllers described in imperative languages which load declarative models or view descriptions, etc. it is customary that test designers will be interested in evaluating not only a test subject, but also all those linked from it. this process may be applied recursively, which requires in turn specifying termination conditions that prevent an unbounded recursion: e.g. select only those test subjects hosted in a specific realm (e.g. a local host, a specific web host, an organization dns domain), traverse only those links to inline or embedded contents, follow links just to a depth of n levels, etc."
"test results can also be rendered together with the original content. for instance, an icon representing a test result can be superimposed wherever a test was applied, so that consumers may easily locate the points that have yielded failed or inconclusive results. these icons must comply themselves with accessibility criteria and may link to detailed explanations. likewise, annotations may decorate the source code on a development environment, so that accessibility violations are treated in the same way as other software construction errors or warnings."
"(3) gdeep's type system factors out the portion of typing that is concerned with feature composition. rather than designing separate and incompatible type systems to handle features in each artifact language, we can use a single type system. the type system for features integrates with the type system of each artifact language in a uniform way. (4) tools can operate directly on a concrete representation of gdeep. this way, tools for composition, validation, and analysis of features can be reused for various types of artifacts."
"multimethods have been proposed to problems of single method dispatch [cit], for example, the binary method problem. multiple method dispatch allows a programmer to extend a given object subsequently without changing existing code or introducing type errors, which is also possible with gdeep. additionally, gdeep structures the name space hierarchically in order to encapsulate and scale the extensions a feature can apply, which has been shown useful in fop [cit] ."
"typing. the classes add and mult have data members of type expr. operations such as eval and replacevars are defined recursively on these members. thus, these additional operations must be defined within class expr itself in order for the implementation to be well-typed; they cannot be defined within derived classes."
(1) it enables us to reason about the properties and procedures of program refinement and feature composition in a formal way. (2) software artifacts of different types can be plugged into the calculus and treated equally by the algorithms for feature composition and validation. the algorithms can be expressed in a uniform and language-independent way.
"when representing a software artifact in gdeep, all of the structural elements in the artifact language must be mapped onto the two kinds of declaration in gdeep."
"once a software artifact has been translated to gdeep, we can use the calculus to compose features in a type-safe and language-independent way. other languages such as javacc, c++, c#, or c are plugged into gdeep in similar ways."
"second, deep supports not only virtual types, but also higher-order subtyping [cit] with bounded quantification over such types [cit] ]. higher-order subtyping is crucial to our treatment of feature composition, but it interacts with recursion and virtual types in subtle ways [cit] ]. to our knowledge, deep is the only calculus that is capable of handling this combination."
"a feature which can be applied to a base program of type a is written as a function that refines an argument of type a, for example, [cit] . gdeep extends this theory by using bounded quantification [cit] ] exclusively to establish type constraints on formal arguments. the application m(n) is valid only if n is a subtype of the argument type of m. argument types in gdeep are invariant rather than contravariant in subtypes; this avoids the well-known problem with contravariance [cit] ]."
"formal models of java type checking, such as featherweight java [cit] or classicjava ], make use of a global class lookup table. type judgments then use this table to find the type signatures of methods and constructors."
"unlike other tools for grammar specification, the grammar rules in bali use late binding: the name expr is not resolved until after feature composition. bali is also similar to jak in its use of keyword super: expression super.oper refers to the original definition of oper."
"all of the features above are well formed. the feature f can be applied to any subtype of a, and b is a subtype of a, so f(b) is well formed. however, evaluating f(b) will cause a name clash, because both f and b have declarations named b."
"finally, test results can also be modified by human testers (evaluators), which may provide additional data for test criteria that require manual intervention (as indicated by the test results), or even to rectify wrongly assessed criteria. the tool must keep provenance data that help tracking the responsible for each assertion. the tool output (e.g. reports, scores) must be recomputed after modified test results are saved."
"we provide only monotone functions in gdeep because we are interested in encoding features. the full deep calculus includes general-purpose (i.e., nonmonotone) functions as well, but such functions are not needed for fop, so we have omitted them in the interest of simplicity."
"type checking in java is largely unaffected by the translation into gdeep, with one exception: the fact that class names are translated into local paths."
"triggering: evaluation may be triggered either directly by the test requester or by an authoring tool he or she is using (e.g. a content management system, an integrated development editor)."
"in the sake of convenience for the test designer, the evaluation tool may integrate with the apis of the user agent the test designer employs to retrieve contents, so that they just need to instruct the tool to get the \"currently displayed content\" as test subject. user agent apis can also be used to upload locally hosted files to remote tools."
"we now illustrate how further artifact languages can be used with gdeep. we begin with the bali and xhtml examples from section 2 and proceed with a further example written in haskell. for bali and xml, we have developed formalizations, called gbali and gxak, that we have integrated with the gdeep calculus. a comprehensive description of the syntax and evaluation rules of gbali and gxak can be found elsewhere [cit] ."
"test subjects may be specified either individually or in bulk. in the first case, the test designer may in turn define the individual test subject in several ways. the most precise way consists in directly providing the tool with a binary or textual serialization of the content or product under evaluation. in that case, the evaluation tool must provide the test designer with interfaces to input that serialization (e.g. an editable field, an http-post endpoint). it is up to him to get the content serialized, for which he may first resort to external tools if needed."
the module system of gdeep is a generalization of deep. the deep calculus is described elsewhere in greater detail [cit] ]; we begin with a brief overview of deep and proceed with gdeep's syntax and semantics.
"first, there is a subtype relation defined over features and feature compositions. features which require other features can express that dependency by means of subtyping (see section 4.6). the type checker will ensure that all dependencies are properly declared as requirements and that all requirements of a composition are satisfied."
"late binding allows a base module to defer implementation details to derived modules. all references to \"self \" within the base module are mapped to the derived module when features are composed. delegation, in turn, allows a derived module to refer back to the base module. module inheritance thus establishes a two-way communications link between parent and child."
"current feature composition tools, such as jak, do not support modular type checking; all type checking must be done after composition. by plugging the artifact type system into gdeep, it is possible to do type checking before composition."
"there is one remaining subtlety. notice that we have replaced an equality in type lookup with an inequality. java type checking uses the fact that class lookup is an equality to determine whether methods are overriding or not. this situation is very similar to the name clashes discussed in section 5.2, and we can resolve it in the same way. we recognize the fact that composition may introduce a name clash and signal an error if it does."
"the usdp defines many other artifacts and activities that will need to be considered when developing an accessibility evaluation tool. e.g. the specification activities will need to count with: a broad glossary that goes beyond the domain model; user interface prototypes that help communicate the user perspective to the client since the beginning; and the specification of non-functional requirements. regarding the last point, it is of paramount relevance to make the evaluation tool accessible itself, although other aspects should not be dismissed, such as the performance of tools which provide developers with real-time feedback, or the localization of presentations. in any case, workers will be needed during specification who can grasp the knowledge of the specific business and users."
"together, these two laws allow us to derive subtyping rules for feature compositions. if f 1 ..f n is a sequence of features, and g 1 ..g m is a sequence of features, then (expr)))."
"a domain model captures the entities appearing in the context of the system and their relationships. although each business will contribute its own entities to the domain model, we can distil a subset that will appear in all accessibility testing tools. for that, we have departed from the working draft of the evaluation and report language (earl) 5, which defines an rdf vocabulary to be used for accessibility test reporting purposes, though it synthesizes cross-cutting concepts common to the whole accessibility testing process. stripping out representation format details, we get the uml view of the core domain model of accessibility evaluation tools shown on figure 2 ."
"code in the artifact language can be represented in one of two ways: compound declarations are modules, which means they have a named substructure. for example, a java class is compound because it contains named methods and nested classes. atomic declarations do not have a particular substructure which gdeep can interpret; an atomic declaration can be any arbitrary term in the artifact language. examples of atomic declarations are java methods and fields, ordinary xml declarations, and bali grammar rules."
"second, deep provides a static type system that can handle deep mixin composition. this is not so important for untyped artifacts such as xml, but it is important for typed artifacts such as bali and java, since it addresses a crucial weakness of existing feature-oriented tools. in existing tools, all type checking must be done after composition. using the deep type system, we can type check features before composition. note that we do not provide proofs or cover the metatheory of deep in this paper; readers interested in technical details should refer to hutchins' ph.d. dissertation [cit] ]."
"the original deep calculus was conceived as a formal model of a stand-alone programming language. the main contribution of this article is to demonstrate that the module system of deep can be used for a wide variety of different artifact languages. the gdeep calculus extends deep with hooks that allow us to embed java programs, xml documents, and so on within deep modules."
"test designers may have a legitimate interest to only test the compliance of specific test criteria. tools may provide them with the possibility to choose test criteria one by one, or just those applicable in a specific context of use (e.g. for a user with a specific disability profile), but they must avoid misleading designers by clarifying that meeting single accessibility criteria does not imply the product being accessible at all. test designers may also wish to provide their own test script implementations, which may be supported by tools that define a test criteria provider interface that may be extended by third-party developed implementations which act as plug-ins for the main tool. this may be the case when tools want to e.g. support new technologies or account for specific deployment contexts or corporate policies. new tests may also be specified following a declarative format, providing, e.g. query expressions and conditions their results must abide by to meet accessibility requirements. nonetheless, tools should encourage that any newly defined criteria preferably fits within existent success criteria, guidelines and principles, in that order, so as to discourage the definition of \"unusual\" approaches to accessibility."
"a concept related to mixins is the concept of a trait. a trait is a reusable unit of behavior that, in its original proposal, has no state [cit] . experiences from practical fop show that features often have state, or to phrase it differently, add fields to existing classes. although there is a recent proposal for stateful traits [cit], we favored the mixin concept because of it is widely used in fop tools. another difference between traits and mixins lies in the treatment of name clashes. traits explicitly require the composer to handle conflicts. anyway, this paper does not focus on these issues but on the principle of uniformity, which is also applicable to the work on traits."
"an assertor represents the issuer of the assertion: it can refer to the software tool, or the identity of the person or organization on whose behalf the tool is working."
"the main goal of an accessibility evaluation tool is to automate (at least partially) the process of evaluating the accessibility of a product, service or content. thus its core use case is the processing of the evaluation upon request of a test requester. in the simplest scenarios, this use case includes as well the other two (specification and presentation), while in more advanced workflows, different actors will independently invoke each use case at different moments. these actors may show different features depending on the specific scenario where the evaluation tool will be used. actors may also represent other software systems, e.g. a business rule engine automatically invoking a tool, or an auditing system into which evaluation results are fed. the evaluation results may also be later used for other purposes, which may include repairing the errors detected. analysts will thus to account for all these aspects, and tightly cooperate with the client or business units in order to define the specific requirements of their evaluation tool, defining and prioritizing a required subset of the tool features and use cases."
"specification may also configure other properties: events triggering the evaluation process, evaluation schedule, routing of the results, etc. the specification might also be split between two phases: first a system analyst would provide a partial specification for test subjects (e.g. a template defining content types under evaluation), and then a developer would specify the specific software artifacts to be evaluated after they have been developed."
"remote services, on the contrary, expose a functionality to be accessed from different hosts, with which they interact to pass input and output data back and forth. for instance, following a service oriented architecture (soa), the tool may provide a web services or a restful interface to offer its functionalities. input data (test criteria and subject configuration) are passed through this interface and output data (test results) are returned through it as well. remote services may also include a presentation layer that retrieves the configuration from an html form and returns the results as web pages to be rendered by the user's browser (making what is usually known as a web application). conversely, invocations to remote services may be triggered from controls added to the user's browser (e.g. scriptlets, favelets, menu add-ons), which dynamically generate the necessary configuration based on the current browsing context."
"the composition of two atomic declarations works much like standard objectoriented method overriding. one declaration overrides the content of the other. however, the overriding declaration may use the metavariable original (much like java's super or jak's super) to refer to the overridden content. by using keyword original it is possible to combine the content of two atomic declarations together without needing to know details about them. the calculus assumes only that the artifact language is defined using terms and that a substitution operator is available over terms."
"in gdeep, refinement is a computation that is performed at composition time. evaluating b will merge the definitions in b with the definitions in a."
"we illustrate the role of gdeep's type system informally by example; the type system of gdeep is defined formally in appendix a. the type system was designed to support separate compilation and type checking of features, something that existing tools such as jak cannot do."
"delegation is similar to the use of keyword super in java or super in jak. when a derived module inherits from a base module, it can delegate some behavior to its parent. delegation allows a feature to \"refine\" its parent by transforming existing declarations, rather than just overriding these declarations outright [cit] b] . note that although we refer to this mechanism as \"delegation\", it is actually done statically, when features are composed."
"xml elements may have subelements that are subject to refinement. xak explicitly establishes a module structure for xml by associating a module name with a particular xml element using the attribute xak:module [cit] ]. thus, the module structure defined by xak is largely independent of the structure of the xml document itself. xml elements that are tagged with xak:module become modules, whereas ordinary elements become atomic declarations. once again, the module language and artifact language are largely orthogonal."
"to integrate java with gdeep, we must define a bijective (i.e., one-to-one and onto) mapping between java programs and gdeep modules. our basic strategy will use this mapping to translate java features to gdeep, use gdeep to perform feature composition, and then translate the resulting program back to java. we will write the mapping function, which translates java to gdeep, as . member variables, constructors, and method declarations in java are translated verbatim into atomic declarations in gdeep. for member variables, we use the name of the variable as the name of the declaration. for methods and constructors, we use a name-mangling scheme to ensure that overloaded methods are assigned unique names, for example, classes and interfaces are translated into modules in gdeep. since gdeep defines its own notion of inheritance, we could attempt to map java inheritance onto gdeep inheritance. there are two problems with this approach. first, we would like features to be a minimally invasive extension. replacing java inheritance with gdeep inheritance would significantly alter the structure of the language."
"analysis activities provide a preliminary model of the internal classes that will implement the functionalities of the different use cases. it is richer than the domain model, including classes that reflect technical concepts and architectural features; but is not yet so detailed as the design model. our domain model already anticipated some analysis classes: a real client would have usually preferred terms from their domain (e.g. multimedia content) to deal with the problem realm, while accessibility specific terms (e.g. test subject) would have only arisen later. figure 4 presents our analysis class model for evaluation tools, using three class stereotypes: entity, boundary and control (akin to those in the model-view-controller pattern). we may note how the tool must provide different user interfaces (boundary classes) for each of the supported use cases. two entities model the configuration parts (test subject and test criteria). while the configured criteria are directly fed to the test executor, the test subject configuration needs first to pass through a retrieval task that obtains the contents of all the resources to be evaluated (according to the test subject configuration). control classes are defined for each of the internal activities defined in the use cases: content retrieval, evaluation execution, and reporting."
"first, type and class definitions within a module can be virtual, which means that they can be overridden by derived modules. virtual classes are not a new idea; they first appeared almost 20 years ago in the beta language [cit] . however, designing a type system that can handle the simultaneous refinement of a system of mutually recursive classes has proved to be very difficult, and it was not formalized until recently [cit] ."
"we handle compound elements of the artifact language by defining a translation function. in the case of java, the translation function provides a bijective mapping between java classes and gdeep modules. in the case of xml, the translation function provides a bijective mapping between xak modules and gdeep modules. since the translation function is bijective, any manipulations performed within gdeep can be translated back to the artifact language."
"second, ahead scales program refinement to arbitrary kinds of software artifacts. a feature typically includes changes not only to the source code, but also to other supporting documents, for example, html documentation, ant scripts, and uml diagrams. the principle of uniformity that underlies ahead can be stated as follows: features are implemented by a diverse selection of software artifacts, and any kind of software artifact can be the subject of subsequent refinement [cit] ."
"the operational semantics of deep is simple, but the type system (in particular the metatheory) is quite complex. there are three aspects of the type system that deserve special mention."
"a test result determines the outcome of applying the test criterion. if we draw a parallel between test criteria and logic propositions, test results will be usually expressed as boolean truth values (e.g. pass / fail). results may be extended to other values, such as 'unknown', 'not applicable' or a certainty value continuously ranged between 0 and 1. these values would respectively appear in other logical systems such as kleene's three-valued logic, description logics in e.g. the semantic web stack, and fuzzy or subjective logics. test results may be enriched with additional information, such as pointers to the region of the test subject where the accessibility criterion was applied (and succeeded or failed), human-readable help to explain why the criterion has succeeded or failed, suggested remedies or repair actions, etc."
"we call our calculus gdeep, which is short for generalized deep. as its name might suggest, gdeep is based on the deep calculus. the main concepts of deep were initially presented in an earlier paper [cit], and are explored in detail in the ph.d. dissertation of hutchins [cit] . the deep calculus provides two capabilities that are relevant to fop."
"ξ is an artifact-specific annotation. it can be used by the artifact language for the composition of artifact-specific details, but is otherwise ignored by the core calculus (see section 4.8)."
"results are reported and stored following standardized formats. but their presentation can be arranged in different ways depending on the consumer needs. in any case, the result format must match the consumer's context of use, e.g. html or pdf will be used for human consumers, or standardized formal languages for software tools."
"an xml document contains a set of elements, which are organized in a hierarchical structure. one might assume that, because xml documents already have a named hierarchical structure, we might attempt to define a translation function that maps xml elements onto gdeep modules, in much the same way as we did for classes in java. however, such a mapping is inappropriate for xml."
"feature var copies an expression by calling the constructors of val and add. if these constructors referred to the original class definitions, then copying would produce an expression that did not support any of the operations defined in other features, such as eval. when composing features with jak, this problem does not occur; notice that the main routine calls replacevars followed by eval."
"based on our experience with existent evaluation tools, we aim at providing some insight to non-accessibilityexperts on how to develop accessibility evaluation tools, following a standardized software development process. the work herein presented has taken into special consideration the guidance on the development of web accessibility evaluation tools (working title), a draft document which is currently being composed by the w3c/wai evaluation and report working group (ert wg). (further information on that document and its evolution may be found at http://www.w3.org/wai/act/deliverables#eval_tools.)"
"a test subject is the product, service or (piece of) content to which the test is applied. the tool puts some specific subject into evaluation, and the assertion captures the result of applying the tests to that subject. it can be as fine-or coarse-grained as needed. following the previous examples, a whole website, a document, an application, an image, a user interface input control, or an object attribute can all be regarded as test subjects."
"gdeep uses a linking mechanism that is similar to c, except that it supports a hierarchical namespace based on features. evaluating a gdeep expression will compose features together. if composition generates a name clash, then the result will be an error, as defined by rule (e-error)."
"usdp deals with deployment planning as part of the design activities. a deployment model defines the physical architecture, identifying the active classes (processes) that will be created in the system, the physical nodes (hosts) where they will be running and the communications mechanisms among them. evaluation tools follow varied deployment alternatives, thus a particular reference model is of little use. nonetheless, most can be subsumed under two major deployment alternatives: either local applications or remote/distributed services. the chosen deployment model may also impact other aspects such as exploitation, scalability, etc. local applications are stored and run on the same device the test requestor directly operates. although they may access external services to retrieve remote data (e.g. test subject contents), all the operations directly related to the evaluation process are performed locally, including the presentation of the results. local applications need to abide by the platform requirements imposed the target environment (operating system, component dependencies, user permissions). they may either be executed as standalone software or require a previous installation. a special case for local applications is that of browser add-ons: their target environment is a sandboxed platform provided by the browser. browser add-ons have their control user interface tightly integrated within the browser, and may delegate on it some functions such as content retrieval or result rendering."
"more specific material exists which may apply these principles to different realms. regarding web accessibility evaluation tools, we provide supplementary material available online, with a detailed list of useful documents 9 ."
"it is not possible to prove that gdeep can be combined with any artifact language in a way that is sound. in most cases, however, the gdeep type system and the artifact type system are largely orthogonal. the type safety result for gdeep provides some basic guarantees that can be reused with many different artifact languages to prove that features are safe for use in those languages."
"a development process starts with the workers in charge of requirement capture and elicitation specifying the system requirements, including accessibility requirements as well. for that, they will usually interact with the client teams. requirements are then conveyed to the workers responsible for the bulk of the development work (system analysis, design and implementation). further on, these may use the tool to validate that the products they create abide by the requirements previously defined by the analysts. later, test engineers will perform tests in the large (e.g. integration tests) to the system version, using once more the evaluation tool to validate accessibility conformance. after internal tests pass, the system is released to the client, who may perform their own (acceptance) tests; and to external auditors, who may perform their own accessibility tests to issue conformance certificates."
"it is worth examining briefly why the features in figure 1 cannot simply be implemented with standard object-oriented inheritance. the most obvious problem is that feature composition would require multiple inheritance, which java does not have. however, there is a more important and more subtle issue involved."
"like object-oriented languages, gdeep uses late binding. variable x is not assigned a value until b is actually projected from m1, for example, because of late binding, refining the definition of a will automatically affect the definition of b. m2 inherits b from m1. however, when the expression m2.b is evaluated, x will be bound to m2 rather than m1, for example, this example also illustrates a more complex use of delegation. notice that the module a is overridden with a version that inherits from original x .a. the \"original\" module in this case is m1, so original x .a is syntax sugar for m1@(x).a. the expression m1@(x).a means: \"extract the declaration named a from m1, but pass x, which is the self-variable for m2, as self.\""
"moreover, beyond the abstract functional specification we provide, the process template does not enforce any additional technological or architectural constraints. although inspiration mainly came from web accessibility evaluation, it is defined in a technology-agnostic fashion and can be applied to other information and communication technology realms. it can also be applied in different business and exploitation scenarios."
"usdp employs use cases as a tool to define the functionality of a system. each use case defines a sequence of actions between an external actor and the system, which provides some value to the actor or obtains value from it. different evaluation tools may define their own actors and use cases, but at least some of them will extend the abstract use cases we define herein. apart from the experience with evaluation tools, we have derived these use cases from generic software testing standards 6 . the three main uses cases of an evaluation tool ( figure 3 ) deal with specification, processing and presentation of the evaluation; extensions include revision of the results."
"the refines m clause denotes the parent of the module, which is much like a superclass in java. a module that has no parent can declare top to be the parent. a module extends its parent by adding new declarations or by overriding existing declarations. declarations that override existing ones must be declared with keyword override."
"as before, we must define a one-to-one function that maps haskell code into gdeep declarations. our mapping for haskell translates both data types and function definitions into atomic declarations. we represent all of the cases of a function by a single cases clause (lines 4 and 9 on the right), which can reference previously defined cases by means of the original keyword. as with our java translation, haskell typing judgments are preserved by feature composition. a more detailed encoding that demonstrates type safety for variant data types can be found in hutchins [cit] . practical experiences with haskell and fop are discussed elsewhere [cit] b ]."
"a bali grammar contains a set of production rules. bali does not define its own compound modules; each grammar consists of a single top-level module, without any nested modules. because bali does not have nested modules, there are no bali-specific module annotations and thus there is no need for a translation function. gdeep declarations and modules can be used as-is."
"an assertion can be enriched with further information detailing the context where it was obtained: a description of tool features, the mode by which the results were obtained (manual, automatic, semi-automatic), a timestamp…"
"creating new classes with different names would break the implementation of other features, because all references to the original class names (e.g., expr and add) would refer to the old definitions instead of the new ones. classes are referred to by name in two places: (1) when constructing instances of the class and (2) in static type information."
"the definition of c is an error because the slot a does not exist. d attempts to override a with new definition that has a different type, and h calls f with an invalid argument."
"jak is an implementation of a composition operator for java artifacts [cit] . figure 1 depicts the jak code of an expression evaluator, which is a feature-oriented solution to the well-known \"expression problem\"."
"third, subtyping is defined directly over modules, rather than classes or types. in fact, the deep calculus does not even have a typing relation; the type system is based entirely on subtyping. this is perhaps the most controversial part of the calculus, but it simplifies the theory in various ways [cit] ]."
"imagine you are a technology consultant who has just been commissioned by a large web provider to develop a module for a web application platform, so that non-technology-savvy content and service creators can be assisted in determining whether their creations are equally accessible for people with diverse abilities. that is, you need to develop an automatic accessibility evaluation tool, which will be introduced in a specific workflow and will need to abide by some business constraints. maybe you are already acquainted with software development, but this is the first time you deal with this kind of tool. or you have used them before in other projects, but have never tackled the development of your own evaluation tool. which functionalities are expected? what is exactly meant by \"evaluating accessibility\"? how are the evaluation results to be handled? what should you start with? although a definite answer to these questions depends on the varied requirements of the specific evaluation tool to be developed, dozens of such tools already exist, which allow extracting useful guidance on how the process of developing an automatic accessibility evaluation tool can be undertaken."
"in addition, views are semantically complex, and there are easier ways to achieve a similar effect. our preferred way is to use name mangling, which is commonly used in other mixin-based languages such as gbeta [cit] . each feature is assigned a unique name, and each non-overriding slot is renamed to one that incorporates the name of its container. overriding slots are declared as such and are renamed to the name of the declaration that they override. ambiguous overrides and ambiguous paths are flagged as errors during the name mangling phase, which precedes the type checking phase."
"we first investigate a biped with a hip and two prismatic spring legs (illustrated in a with parameters given in table 1 ). we assume that a lower piston fits into an upper cylinder on each leg (with a spring in between). points a and f are the robot's point feet (massless), c and d are the centres of mass (coms) of the cylinders, b and e are the coms of the respective pistons, and h is the hip point (with a point mass). we assume that the terrain slopes at a constant angle φ and our cartesian x,y coordinates describe absolute horizontal and vertical directions (relative to the earth). we look only at running gaits, consisting of a stance phase (with one stance leg touching the ground and one swing leg) and a flight phase. a rotational motor in the hip provides actuation torque u. the prismatic joints are assumed to have actuators that can only lock or unlock the prismatic motion -locked just after take-off (once the leg has reached its nominal length) and unlocked just before, or at, touch-down. thus, the prismatic joints are passive."
"the passive and active open-loop gaits found in the previous section are unstable; even tiny disturbances, like those due to truncated numerical calculations in simulations, will cause the joint trajectories to drift and result in the robot falling after a few steps. to investigate gait stability, consider the linearized poincare map around the fixed point (26) or (27) as:"
c. van leeuwen formulated a radically opposite point of view [cit] : neither high-dimensional linear models nor low-dimensional non-linear models have serious relations to the brain.
"after dimensionality reduction, we can perform whitening of data and apply the stochastic separation theorems. this requires a hypothesis about the distribution of data: sets of a relatively small volume should not have a high probability, and there should be no 'heavy tails'. unfortunately, this assumption is not always true in the practice of big data analysis. (we are grateful to g. hinton and v. kůrková for this comment.)"
"in which k is the number of steps. any fixed point of the poincare map indicates a periodic orbit of the overall dynamic model and provides a valid initial condition for a passive periodic running gait. to find the fixed point of the poincare map, find the zero of the function:"
"during the last two decades, the curse of dimensionality in data analysis was complemented by the blessing of dimensionality: if a dataset is essentially high-dimensional then, surprisingly, some problems get easier and can be solved by simple and robust old methods. the curse and the blessing of dimensionality are closely related, like two sides of the same coin. the research landscape of these phenomena is gradually becoming more complex and rich. new theoretical achievements and applications provide a new context for old results. the single-cell revolution in neuroscience, phenomena of grandmother cells and sparse coding discovered in the human brain meet the new mathematical 'blessing of dimensionality' ideas. in this mini-review, we aim to provide a short guide to new results on the blessing of dimensionality and to highlight the path from the curse of dimensionality to the blessing of dimensionality. the selection of material and angle of view is based on our own experience. we are not trying to cover everything in the subject of review, but rather fill in the gaps in existing tutorials and surveys. r. bellman [cit] in the preface to his book, discussed the computational difficulties of multidimensional optimization and summarized them under the heading \"curse of dimensionality\". he proposed to re-examine the situation, not as a mathematician, but as a \"practical man\" [cit], and concluded that the price of excessive dimensionality \"arises from a demand for too much information\". dynamic concentrated near its surface. moreover, a high-dimensional sphere is concentrated near any equator (waist concentration; the general theory of such phenomena was elaborated by m. gromov [cit] ). p. lévy [cit] analysed these effects and proved the first general concentration theorem. modern measure concentration theory is a mature mathematical discipline with many deep results, comprehensive reviews [cit], books [cit], advanced textbooks [cit], and even elementary geometric introductions [cit] . nevertheless, surprising counterintuitive results continue to appear and push new achievements in machine learning, artificial intelligence (ai), and neuroscience."
"note that γ exponentially tends to zero with n increase. small γ means 'good' clustering. if γ 1 then the probability to find a data point in the intersection of the balls (the 'area of confusion' between clusters) is negligible for uniform distributions in balls, isotropic gaussian distributions and always when small volume implies small probability. therefore, the clustering of mistakes for correction of high-dimensional machine learning systems gives good results even if clusters are not very good in the standard measures, and correction of clustered mistakes requires much fewer correctors for the same or even better accuracy [cit] ."
"based on previous works [cit], and using the web ontology language (owl) [cit], an etl pattern based ontology was developed to support the necessary requirements and to describe each pattern configuration, enabling its mapping to physical models that can be executed in practice [cit] . basically, an intermediate layer is provided to separate technical knowledge, typically used in commercial tools, from the domain knowledge used by decision-makers [cit] . due the complexity of the knowledge involved and the application of each pattern to specific contexts [cit], etl processes can suffer from inconsistencies and misunderstandings about communication problems that can result from different meanings or architectural contradictions. ontologies can be used to provide the contextual data necessary to describe each pattern according to its structural properties [cit] ."
"as future work, a set of tests will conducted to study the feasibility of our approach as well as to extend it, improving and enriching the ontology in order to cover more coordination and communication aspects, essentially."
"the introduced measure of dimension performs competitively with other state-of-the-art measures for simple i.i.d. data situated on manifolds [cit] . it was shown to perform better in the case of noisy samples and allows estimation of the intrinsic dimension in situations where the intrinsic manifold, regular distribution and i.i.d. assumptions are not valid [cit] ."
"the multidimensional brain is the most puzzling example of the 'heresy of unheard-of simplicity', but the same phenomenon has been observed in social sciences and in many other disciplines [cit] ."
"such a separability is important for the solution of a technological problem of fast, robust and non-damaging correction of ai mistakes [cit] . ai systems make mistakes and will make mistakes in the future. if a mistake is detected, then it should be corrected. the complete re-training of the system requires too much resource and is rarely applicable to the correction of a single mistake. we proposed to use additional simple machine learning systems, correctors, for separation of the situations with higher risk of mistake from the situations with normal functioning [cit] (figure 1 ). the decision rules should be changed for situations with higher risk. inputs for correctors are: the inputs of the original ai systems, the outputs of this system and (some) internal signals of this system [cit] . the construction of correctors for ai systems is crucial in the development of the future ai ecosystems."
for sufficiently high-dimensional sets of input signals a simple enough functional neuronal model with hebbian learning (the generalized oja rule [cit] ) is capable of explaining the following phenomena:
"v. kreinovich [cit] summarised the impression from the effective ai correctors based on fisher's discriminant in high dimensions as \"the heresy of unheard-of simplicity\" using quotation of the famous pasternak poetry. such a simplicity appears also in brain functioning. despite our expectation that complex intellectual phenomena is a result of a perfectly orchestrated collaboration between many different cells, there is a phenomenon of sparse coding, concept cells, or so-called 'grandmother cells' which selectively react to the specific concepts like a grandmother or a well-known actress ('jennifer aniston cells') [cit] . these experimental results continue the single neuron revolution in sensory psychology [cit] ."
"note that this passive limit cycle is unstable; the phase diagram diverges due to the (disturbance) error accumulation in numerical calculations, causing the biped model to fall down after running five steps (the red line exiting to the left in figure 4b ). considering that points p and r correspond to the touch-downs in the limit cycle of figure 3, one can see that there is no discontinuity in the velocities; therefore, in this gait touch-down occurs with no energy losses. however, in the last step before failure, point p changes to a vertical line with an instantaneous velocity change in touch-down, which causes energy dissipation. this small vertical line is recognizable in the figure by a blue point just beneath the red point in p. an energy-conserving touch-down occurs for biped models with massless springy feet when the velocity of the touchdown foot is in the direction of the touch-down leg [cit] . this condition has been satisfied automatically because the calculated fixed point is passive and energy conserving. however, energy dissipation is inevitable at touch-down with biped robots with foot mass. the thick lines in figure 5 representing this gait show that the total mechanical energy of the robot remains constant during the gait, serving to verify the validity of our dynamic equations and their solutions."
"the modern point of view on the single-cell revolution was briefly summarised recently by r. quian quiroga [cit] . he mentioned that the 'grandmother cells' were invented by lettvin \"to ridicule the idea that single neurons can encode specific concepts\". later discoveries changed the situation and added more meaning and detail to these ideas. the idea of concept cells was evolved during decades. according to quian quiroga, these cells are not involved in identifying a particular stimulus or concept. they are rather involved in creating and retrieving associations and can be seen as the \"building blocks of episodic memory\". many recent discoveries used data received from intracranial electrodes implanted in the medial temporal lobe (mtl; the hippocampus and surrounding cortex) for patients medications."
"assuming the 100% efficiency of the motors, the energy expenditure in one step is calculated: the cost of transport (cot) -i.e., the consumed energy per total weight per distance travelled -provides a useful measure for energy expenditure comparisons:"
"the principle of virtual work provides generalized impact modelling. in the following equations, i q denotes the i-th component of f q . the position of the touch-down foot in terms of flight coordinates is: ."
"there is a fundamental difference and complementarity between analysis of essentially high-dimensional datasets, where simple linear methods are applicable, and reducible datasets for which non-linear methods are needed, both for reduction and analysis [cit] . this alternative in neuroscience was described as high-dimensional 'brainland' versus low-dimensional 'flatland' [cit] . the specific multidimensional effects of the 'blessing of dimensionality' can be considered as the deepest reason for the discovery of small groups of neurons that control important physiological phenomena. on the other hand, even low dimensional data live often in a higher-dimensional space and the dynamics of low-dimensional models should be naturally embedded into the high-dimensional 'brainland'. thus, a \"crucial problem nowadays is the 'game' of moving from 'brainland' to 'flatland' and backward\" [cit] ."
"consider a finite data set y without any hypothesis about the probability distribution. let ( ·, · ) be the standard inner product in r n . let us define fisher's separability following [cit] ."
"it is desirable that there are small deviations of motor torques from the fixed point values. in these simulations, each component of the initial condition state vector is 1% deviated from the fixed point vector for the gait on horizontal terrain and 3% for sloped terrain. more deviations lead to divergence from the limit cycle and failure. to generate an efficient running gait, the springs' rates should be adjusted for that speed. we found a set of values by trial and error that can produce a natural-looking gait with a speed of 10 m/s when compared to the fastest human running speed of 12.4 m/s [cit] . a stick diagram of the generated gait on sloped terrain with an angle of 5° appears in figure 12 . the red curves indicate the trajectory of the com during flight, demonstrating a considerable range in flight. although unstable, these gaits can continue open-loop running for about nine steps before falling down, appearing to have better stability characteristics than the telescoping leg biped model. the linearized poincare map (31) for the gait on horizontal terrain has a maximum eigenvalue of 2.92 and for sloped terrain 3.63, quite suitable for the application of a linear state feedback controller (compared to 13.8 for the prismatic springy leg model). the use of the dlqr method produces matrix ��� ��� and the control law (32) places the closed-loop maximum poles of the systems at 0.4034 and 0.333 + 0.182i for horizontal and sloped terrains respectively. this control law stabilizes the nonlinear hybrid system and generates a stable periodic running motion. when the initial condition is coincident with the fixed point of the poincare map, the phase diagram of the closed-loop system remains on the limit cycle. perturbing each element of the starting state by 2% from the fixed point, we observe initial deviations of the phase from the limit cycle with ultimate convergence within five steps ( figure 14 and figure 16 for the horizontal and sloped terrains respectively). if we perturb only one component of the initial state vector by 10%, the closed-loop system will converge on the limit cycle as well (larger perturbations cause divergence in both cases). the figures demonstrate a local basin of attraction. the vertical green lines p and r correspond to touch-down with instantaneous velocity changes. notice that the gait on the slope has a greater velocity discontinuity and larger energy losses than the horizontal gait. furthermore, figure 15 and figure 17 show that bigger torque magnitudes are needed on the sloped terrain. due to the disturbance in the initial conditions, the control torques have some fluctuations and converge on * u after a few steps. these torques remain constant during each stance and flight phase (labelled in accordance with table 2 )."
"we begin by finding a passive solution, where the torque at the hip remains zero at all times. the poincare map for a passive gait is:"
"the devil is in the detail. first of all, the preprocessing is always needed to extract the relevant features. the linear method of choice is pca. various versions of non-linear pca can be also useful [cit] . after that, nobody has a guarantee that the dataset is either essentially high-dimensional or reducible. it can be a mixture of both alternatives, therefore both extraction of reducible lower-dimensional subset for nonlinear analysis and linear analysis of the high dimensional residuals could be needed together."
"an ontology for describing etl patterns behavior all the etl patterns proposed, having the ability to express the construction rules for a language we built previously to support the configuration of each etl pattern. the ontology also describes the main operational components of each pattern, covering the main properties and restrictions that can be used to support its usage. thus, enriching each pattern definition using in the referred language, we can use all the main components to its posteriorly the mapping of execution primitives. recognizing the value and the abilities of the frameworks offered by commercial migration tools, we can develop specific transformation templates to translate each etl pattern configuration to a corresponding format that can be interpreted directly by an etl implementation tool. all this provides pattern reusability across several systems and contributes to system robustness, since patterns are independent elements on every etl applications."
single false positives were corrected successfully without any increase of the true positive rates. we removed more than 10 false positives at no cost to true positive detections in the street video data (nottingham) by the use of a single linear function. further increasing the number of corrected false positives demonstrated that a single-neuron corrector could result in gradual deterioration of the true positive rates.
"to test the control algorithm we use a gait with 0.01 s stance time step size. again the poincare map is linearized around the fixed point and with the control law of (32) for the linear discrete system (31), and matrix ��� �������� is found using dlqr method. controller (32), when applied to the main system, generates control command for one step using the feedback of stance state in the post contact instance. starting from a state 6% disturbed from the fixed point, the phase diagram of one leg of closed-loop running for 20 steps is shown in figure 21 which shows convergence on a steady stable gait, and the control effort for ten steps is depicted in figure 22 ."
"we implemented the correctors with separation of clustered false-positive mistakes from the set of true positive and tested them on the classical face detection task [cit] . the legacy object detector was an opencv implementation of the haar face detector. it has been applied to video footage capturing traffic and pedestrians on the streets of montreal. the powerful mtcnn face detector was used to generate ground truth data. the total number of true positives was 21896, and the total number of false positives was 9372. the training set contained randomly chosen 50% of positives and false positives. pca was used for dimensionality reduction with 200 principal components retained. single-cluster corrector allows one to filter 90% of all errors at the cost of missing 5% percent of true positives. in dimension 200, a cluster of errors is sufficiently well-separated from the true positives. a significant classification performance gain was observed with more clusters, up to 100."
"the stick diagram of the resulting actuated (non-passive) gait running up a slope angle of � � 5° and a horizontal velocity of 0.85 m/s is shown in figure 4 . for points p and r in this figure, the superscript '-' represents the precontact state and '+' represents post-contact. note the discontinuities in velocities with resulting energy losses at touch-down. due to the uphill motion, the total mechanical energy of the robot increases with each step ( figure 5, thin lines). figure 6 shows the control effort for the generated gait, remaining constant in each phase. the cot for running gaits increases with the terrain incline in a linear manner (figure 8 )."
"because the feet have mass in this model, no passive running gait exists on horizontal terrain. again, the post contact state vector becomes a poincare section. an active fixed point of the poincare map:"
"for each positive and false positive we extracted the second to last fully connected layer from cnn. these extracted feature vectors have dimension 4096. we applied pca to reduce the dimension and analyzed how the effectiveness of the correctors depends on the number of principal components retained. this number varied in our experiments from 50 [cit] . the 25 false positives, taken from the testing set, were chosen at random to model single mistakes of the legacy classifier. several such samples were chosen. for data projected on more than the first 87 principal components one neuron with weights selected by the fisher linear discriminant formula corrected 25 errors without doing any damage to classification capabilities (original skills) of the legacy ai system on the training set. for 50 or less principal components this separation is not perfect."
"where subscripts s and f represent stance and flight respectively, and superscripts -and + indicate time instants just before and after the event, respectively, which results in:"
"re-training large ensembles of neurons is extremely time and resources consuming both in the brain and in machine learning. it is, in fact, impossible to realize such a re-training in many real-life situations and applications. \"the existence of high discriminative units and a hierarchical organization for error correction are fundamental for effective information encoding, processing and execution, also relevant for fast learning and to optimize memory capacity\" [cit] ."
"foot f receives an impulse force, one component of which is transferred along the axis of the prismatic joint to the hip. using angular momentum conservation for the whole robot around f, linear momentum conservation of ahd in the direction of leg fh, and angular momentum conservation of leg ah around h, then the expressions below are conserved in terms of f"
pca gives us a tool for estimating the linear dimension of the dataset. dimensionality reduction is achieved by using only the first few principal components. several heuristics are used for evaluation of how many principal components should be retained:
"to detect a touch-down event during numerical simulation, we have to find the first intersection of the trajectory of foot point d and the line y � xtanφ . at touch-down, the velocities change instantaneously because of an inelastic impact. the angles do not have any instantaneous change, but our coordinate notation does change due to the other leg becoming the stance leg (b):"
"where l denotes the linear momentum and h denotes the angular momentum. the three equations (7-9) have three unknowns s + q , providing a re-initialization of velocities."
"the maximum deviation of the initial conditions from the fixed point for which the controller of constant torque gait converges on the limit cycle is 2% while it is 6% for the gait with variable torque. so, the variable torque has the advantages of lower cot and better controllability, while its drawback is that it needs a greater number of calculations to find the fixed point and it can be more difficult to implement on a real robot than the event-based controller. 1.01 -"
"the correction methods were tested on various ai applications for videostream processing: detection of faces for security applications and detection of pedestrians [cit], translation of sign language into text for communication between deaf-mute people [cit], knowledge transfer between ai systems [cit], medical image analysis, scanning and classifying archaeological artifacts [cit], etc., and even to some industrial systems with relatively high level of errors [cit] ."
"will help generate gaits on horizontal or sloped terrains. three motors provide torque -one in the hip and two in the knees. again, the motor torques remain constant during each continuous time phase and, therefore, vector u has three components for the stance phase and three for the flight."
"of course, if an ai system made too many mistakes then their correctors could conflict. in such a case, re-training is needed with the inclusion of new samples."
"a take-off event consists of an instantaneous transition from stance to flight, occurring when the ground reaction force reaches zero. this event is detected when the spring of the stance leg reaches its free length or the ground reaction force reaches zero. the generalized coordinates f q in flight use the same 1 q and 2 q as in stance, but include 3 q and 4 q as the cartesian x and y coordinates of the hip point respectively . in order to find the initial coordinates in the subsequent flight phase, we use:"
"nowadays, sharing and reusing knowledge it is a crucial activity for software development. many specific frameworks appeared with the goal to define a new kind of software programming for taking advantages of previous expertise and allowing for its reuse on new applications in different application scenarios and domains. usually, these frameworks are composed by collections of software patterns representing a set of instructions or activities, which can be configured and applied to more specific needs. concerning the specificities of an etl environment, patterns can be characterized using a set of pre-established tasks grouped based on a specific configuration related to the context in which are used. creating these reconfigurable components avoid the need to rewrite some of the most repetitive tasks that are used regularly. several tasks, such as surrogate key process generation, lookup operations, data aggregation, data quality filters or slowly changing dimensions, are just some few examples of usual tasks used in any dws. instead of using repetitive tasks to solve the same problems over and over again, conceptual models can be used to simplify etl representation. this way, users focus on more general requirements, leaving the complexity of its implementation on other development steps. consequently, users only need to provide configuration metadata to the conversion engine that will be responsible to generate the correspondent physical model. owl, a language based on web semantic technology, is often used to describe domain specific meta-models in order to represent properties and relationships between domain concepts (i.e., patterns). owl is a w3c standard (w3. [cit] ) that was developed to provide a simple form to process and use semantic data across applications in the web. with owl, classes or concepts can be described and arranged to form taxonomic hierarchies, properties describing the composition in terms of attributes of each concept and restrictions over the relationship between the concepts presented. thus, etl patterns can be syntactically expressed using classes, data properties and object properties, providing the basic structure to support the development of a specific language to pattern instantiation. figure 1 shows an excerpt of the breakdown among the different levels of the etl patterns taxonomy proposed. the 'pattern' class represents the most general concept used, while 'extraction', 'transform' and 'load' are the three types of patterns that are intrinsically associated to each typical phase of an etl process. instances of 'extraction' are used to extract data from information system using a specific data object (e.g., a table or file), representing typical data extraction processes and algorithms applied over specific data structures. three types instances are commonly referred for the concept 'extraction', namely: a) full extraction patterns that are used to extract all data from a specific data source without any criteria, i.e., all data currently available. b) differential extraction patterns that are used to identify new data since the last successful extraction. for this data extraction type, all data from source and target repository is compared to identify new data. c) incremental extraction patterns: used to extract data from data sources since the last successful extraction but based on specific criteria and using specific cdc (change data capture) techniques to identify and track the data that has been changed in all the data warehouse sources. the 'transformation' class represents patterns that are used in etl transformation phase for the application of a set of cleaning or conforming tasks [cit], in order to align source data structures to the requirements of the target schema of a data warehouse. this class represents a large variety of procedures that are often applied in dws, such as patterns responsible to apply the well-known policies related to scd techniques, patterns for surrogate key generation, or patterns to support the conciliation and integration of data from many data sources. for example, a dqe pattern can be specialized to a 'normalization' class, which represents the set of tasks needed whenever it is necessary to standardize or correct data according to a given set of mapping rules stored in mapping tables. with these classes, all the most frequent etl patterns can be represented along with all its operational stages. using the ontology hierarchy to support etl patterns meta-model, patterns can be changed or even new patterns can be added without compromising the whole pattern structure. finally, the 'load' class represents patterns that are used to load data to the target dw repository, representing efficient algorithms for data loading or index creation and maintenance for loading procedures. the 'intensive data loading' (idl) subclass should load data to a target dw schema considering the model restrictions used."
"we have to utilize the conservation of angular and linear momentum in order to describe the impact map. we assume leg ah has a fixed length during the impact, whereas leg fh is free to compact (compressing the spring). assuming a fully plastic contact, point f becomes an ideal pivot after contact."
"this model (figure 2 ) consists of a point mass in the hip and two legs with a thigh and a shank, both of which have mass and moment of inertia. the model's lengths and masses model a typical human (). inspired by the fact that human muscles change their stiffness during running [cit], we discovered that changing the knee stiffness between stance and flight produces more efficient running gaits -the gait requires more torque in stance than in flight. we define the free angles of the torsion the stance phase's generalized coordinates comprises (the absolute angle of thigh bh), (the angle of link ab relative to bh), (the absolute angle of thigh dh) and (the angle of link cd relative to dh), as shown in figure 2a . the positive direction of the angles is counterclockwise. by utilizing the lagrange method, the stance phase dynamic equations become:"
"thus, after a brief exposure of some related work (section 2), we describe our ontology approach to support etl patterns, providing a specific taxonomy of the most used etl techniques and the main components that support the configuration of each pattern (section 3). next, a set of necessary formalisms to create a pattern-based language and how to use them to generate physical models is presented (section 4). finally, we discuss the experiments done so far, analyzing results and presenting some conclusions and future work (section 5)."
"the separability properties can be affected by various violations of i.i.d. structure of data, inhomogeneity of data, small clusters and fine-grained lumping, and other peculiarities [cit] . therefore, the notion of dimension should be revisited. we proposed to use the fisher separability of data to estimate the dimension [cit] . for regular probability distributions, this estimate will give a standard geometric dimension, whereas, for complex (and often more realistic) cases, it will provide a more useful dimension characteristic. this approach was tested [cit] for many bioinformatic datasets."
"the maximum eigenvalue of matrix a corresponding to the passive gait on horizontal terrain is 13.8, and for the active gait on 5° sloped terrain is 5.8+2.7i. so, although both of these gaits are unstable, the magnitudes of the eigenvalues are small enough that stabilization using discrete-time state feedback control constitutes a viable strategy."
"the generalized coordinates of the flight phase f q have 4 components, where 1 2, q q are the same as in the stance phase and 3 4, q q indicate the position of the hip point. the generalized forces during the flight phase are"
"countless fixed points exist, each corresponding to a running speed. we find a fixed point that minimizes energy expenditure subject to the constraint of the swing leg remaining clear of the ground. the energy expenditure index will be introduced in the next section."
"the kinetic and potential energies follow from the geometric relationships between the positions of the robot points and their symbolic time derivatives. the governing equations of the stance phase, derived by symbolic software, are:"
"this mini-review focuses on several novel results: stochastic separation theorems and evaluation of goodness of clustering in high dimensions, and on their applications to corrections of ai errors. several possible applications to the dynamics of selective memory in the real brain and 'simplicity revolution in neuroscience' are also briefly discussed."
"the generalized coordinates of the stance phase � � have � � as the angle of the stance leg with respect to vertical � � as the angle of the swing leg with respect to the stance leg, and � � as the length of the stance leg spring. the swing leg (quickly) shortens by length s just after touch-down, and moves (quickly) back to the original length just after take-off."
"a gait designed for an uphill run will require torques (i.e., an active gait). moreover, we shall see that active gaits may also provide a better starting point (linearization) for our closed-loop control strategy. to find such active running gaits, we find a fixed point of the active poincare map:"
"is the subtraction of the kinetic and potential energy. the kinetic energy consists of the links' translational and rotational energies, and the potential energy comprises the gravitational and elastic energies. the joints are assumed to be frictionless. the generalized forces i q are found using virtual work:"
"assuming fully plastic contact, the post-contact position of the hip produced by differentiating the post-contact stance coordinates is: constitute the touch-down map. these equations, together with the stance and flight phase and take-off map, form the hybrid dynamic model."
"at least one fixed point exists for any given running speed. we measure the running speed as the horizontal velocity of the com at the touch-down instance, and then add its difference from the desired velocity to the norm of the error function in the optimization routine."
"corrector decision about correction figure 1 . a scheme of corrector. corrector receives some input, internal, and output signals from the legacy artificial intelligence (ai) system and classifies the situation as 'high risk' or 'normal' one. for a high-risk situation, it sends the corrected output to users following the correction rules. the high risk/normal situation classifier is prepared by supervised training on situations with diagnosed errors (universal construction). the online training algorithm could be very simple like fisher's linear discriminants or their ensembles [cit] . correction rules for high-risk situations are specific to a particular problem."
"for analysis of fisher's separability and related estimation of dimensionality for general distribution and empirical datasets, an auxiliary random variable is used [cit] . this is the probability that a randomly chosen point x is not fisher-separable with threshold α from a given data point y by the discriminant (1):"
"then all sample points with a probability greater than 1 − ψ are fischer-separable from a given point y with a threshold α. similarly, if"
"after the taxonomy definition, the meta-model should be enriched to support the basic rules for the development of well-formed etl patterns. for that, each class should be defined through the use of properties. for example, the 'extraction' class representing all 'extraction' patterns is composed by some datatype properties such as patternid and patternname (inherited from 'pattern' class), and object properties such as periodliterals that refers to extraction interval used (hour, daily, month) (oneof property) and the metadata related to the repository connection (input object: data class and fields used: field class). each subclass can also include additional properties. for example, the 'incremental' class uses a date type property to identify new or changed records. each property should be described based on its cardinality, value, domain and range. the domain links a property to a class, while the range links a property to a class or data range. this allows for the association between classes and data types, and provides a way to establish restrictions. for example, all patterns should have (hasinput object property) at least one (mincardinality restriction) and at most two (maxcardinality restriction) 'data' class association for pattern input and only one (functionalproperty constraint) 'data' class association for pattern output. generally, patterns only have one source for input and one target repository as output. however, the dci (data conciliation and integration) pattern uses more than one data source as input (using subpropertyof axiom) due being responsible to integrate data extracted from two data sources from the same data object. in figure 2 we can see a brief resume of some classes and its data or object properties."
"in the controller (30) for the linear system (29) investigated in the previous sections, the torques were constant during each stance and each flight phase; reducing the number of calculations needed yet restricting the energy efficiency of the gait. in this section, the control method is generalized for running gaits with variable motor torques to show the generality of the control strategy. here, the motor torques are discretized into smaller time steps, but remain constant during each step. due to the variation of motor torques, the stance and flight times will vary, requiring us to choose a number of steps that cover more time than the entire phase."
"the nonlinear optimization needed to minimize (22) presents some practical difficulties. since this complicated function contains the hybrid dynamic model with continuous and discontinuous time phases, in practice the optimization algorithm will often settle in a local minimum. to solve this problem, after finding each solution we re-initialize the optimization algorithm by rounding the last result, and repeat this procedure until reaching the desired tolerance. the presence of events in the dynamic model causes challenges in choosing an integration step size; we use a relatively large time step, with a maximum step size of 10 ms for the continuoustime phases, but a much smaller time step around the events, with the maximum step size in the order of 0.1 ms."
"v. kůrková [cit] emphasized that many attractive measure concentration results are formulated for i.i.d. samples from very simple distributions (gaussian, uniform, etc.), whereas the reality of big data is very different: the data are not i.i.d. samples from simple distributions. the machine learning theory based on the i.i.d. assumption should be revised, indeed [cit] . in the theorems above two main restrictions were employed: the probability of a set occupying relatively small volume could not be large (3), and the support of the distribution is bounded. the requirement of identical distribution of different points is not needed. the independence of the data points can be relaxed [cit] . the boundedness of the support of distribution can be transformed to the 'not-too-heavy-tail' condition. the condition 'sets of relatively small volume should not have large probability' remains in most generalisations. it can be considered as 'smeared absolute continuity' because absolute continuity means that the sets of zero volume have zero probability. theorems 1 and 2 have numerous generalisations [cit] . let us briefly list some of them:"
"we investigated in the computational experiments if it is possible to take one of cutting edge cnns and train a one-neuron corrector to eliminate all the false positives produced. we also look at what effect, this corrector had on true positive numbers."
"for simplicity and to compare the results with [cit], we assume massless feet and place the leg masses at points c and d. the only event that should dissipate (significant) energy during a running cycle is touch-down. however, massless feet make it possible to touch down without energy losses, resulting in passive periodic orbits. thus, gait generation will consist of finding a set of initial conditions and control commands that can produce a periodic orbit for running. a poincare map of one complete running step will serve as a convenient method of describing a periodic orbit. a complete running step includes stance phase, take-off, a flight phase and touchdown. we choose the post-contact state vector as the poincare section, which is also the initial condition of the stance phase. thus, the state vector at the beginning of a stance phase gives the poincare map its input, and it outputs the next state vector at the beginning of the next stance phase (the next step). although s x has six components, at the very beginning of the stance phase the spring starts at zero compression and the initial length of the leg is the same every time, such that our poincare map state vector x consists of a five-dimensional vector."
"the activity of dozens of neurons can be recorded while patients perform different tasks. neurons with high selectivity and invariance were found. in particular, one neuron fired to the presentation of seven different pictures of jennifer aniston and her spoken and written name, but not to 80 pictures of other persons. emergence of associations between images was also discovered."
". now, instead of a hybrid nonlinear dynamic system (2, 4, 5, 6-9), we have a linear digital system (31) with an equilibrium point at the origin. this system will be stable, if all of the eigenvalues of matrix a are located inside the unit circle. a state feedback control:"
"the sum_duration aggregator pattern (transform.aggregator) presents three main blocks derived from the object properties applied to the pattern class. the source describes input metadata, target describes output metadata and fields block describes the fields will be used as output to the target repository. these three blocks correspond to hasinput, hasoutput and hasfields object properties, respectively. for input block, a csv file was used for data extraction based on delimiter ':' (a composite statement is used due the existence of a data property describing the delimiter rule for the csv class), and the pattern output will store correspondent data into a specific relational table. details such as database name or server was omitted since they can be configured in further steps. after fields identification (separated by comma), the keyword options is used to specify each configuration parameter (derived from properties applied to the aggregator class) associated to aggregator class: function to identify the aggregation function applied, functionfield to specify the field that should be used by the function, renamefield to apply the alias to the new field generated and the groupfields used to specify the group by clause."
"studies on the energetics and kinematics of a spring-mass model show that compliant elements in the legs of a biped robot take an important role in both walking and running [cit] . although human legs have very complicated muscle-tendon neural control systems, they exhibit simple spring-like behaviour in running and some walking speeds [cit] . a one-legged hopper with a springy passive knee achieves a stable hopping motion using a harmonic input for the hip motor [cit] . to model a human-like compliant leg, we can use hill-type muscles in the biped model, but it makes the model unnecessarily complicated. in order to simulate muscle compliance with a minimalistic model, we propose a biped model in which each joint comes equipped with a rotational spring parallel to a torque motor."
"with this pattern-based approach, a new abstraction layer for developing etl processes is proposed. patterns can be used to create an etl conceptual model without focusing in very detailed tasks. however, to produce physical models based on conceptual primitives we need to provide two independent components: patterns configuration meta data that is supported by the domain language provided, and workflow coordination data that describes the process flow. only for demonstration purposes, the bpmn language was used to create the etl conceptual models we used. in several works, bpmn has proven that is quite suitable to represent several workflow operational components of etl systems, both at conceptual and physical primitives [cit] . in recent works [cit] we also proposed the use of bpmn as visual layer to support etl conceptual models, representing patterns using bpmn elements. the experimental tool developed has the ability to interpret the configuration language and provide the generation of a physical model, making it possible to be executed by commercial tools such as kettle from pentaho [cit] . for that and based on the ontology presented, a specific meta model can be generated and used to support pattern instantiation and configuration. using protégé editor to create the ontology it's possible to generate java code based on the ontology definition. this feature allows for manipulating the ontology and, at the same time, provides the necessary contracts to control and implement the necessary models to support pattern interpretation and manipulation. the final step covers the generation of physical models using the architecture and philosophy followed by each commercial tool. a set of standard transformation skeletons was built to encapsulate the logic of the conversion process, providing the meanings to transform each pattern internal structure to a specific serialization format. figure 4 summarizes all the phases of the development process that are needed to support the physical representation of an etl process using patterns, from the ontology definition to the generation of physical model."
"vector u contains the data of the control effort of one step. since we wish to use discrete-time control methods, we require controls to be constant over discrete-time intervals. we use one constant motor torque during each phase (i.e., a constant value in stance and a different constant value in flight). since the system has only one (hip) motor, the control vector for this biped model consists of a two-dimensional vector:"
"stabilizes system (31). linear control techniques can produce an appropriate control gain matrix k. in pole placement, the poles of a closed-loop system a-bk are simply placed inside the unit circle. with the discrete linear quadratic regulator (dlqr) method, a matrix k is found that minimizes the cost function:"
"graph cut based active contour (gcbac) is active contour segmentation of an object using graph cut as the optimization tool [cit] . in each iteration, the algorithm deforms the initial contour to find the new contour, which is the minimum cut in favor of a shorter boundary. the objective of gcbac, similar to the classical active contour, is to find the boundary of the object. the classical active contour, also called snakes [cit], has several disadvantages. for instance, snakes uses a number of control points which may lead to unequal spacing or selfcrossing during contour deformation. in addition, several parameters to control the snakes algorithm are difficult to determine. gcbac is much more practical to find a contour in a desired space or contour neighborhood (cn)."
"the aorta is the largest vessel in the human body. it is tubular in shape, emanating from the heart in the chest and passing into the abdomen to supply blood to all organs in the body. aortic aneurysm is a dilatation of the aorta, which can cause rupture leading to death. epidemiological studies in the usa have revealed that abdominal aortic aneurysm (aaa) is a disease of increasing incidence and mortality, with major risk factors being hypertension, high cholesterol and smoking [cit] . more than 10,000 people die from rupture each year [cit] . while aortic aneurysm can occur worapan kusakunniran worapan k@hotmail.com thanongchai siriapisith thanongchai@gmail.com peter haddawy haddawy@gmail.com 1 in the thorax or abdominal area, rupture is more common in the abdominal region [cit] ."
"to address this problem, we modify the traditional graph cut by adding a probability density function to discard or partition the pixels which have intensity significantly higher or lower than the mean value. the probability density function is used to model the pixel intensity of possible foregrounds. the graph g(v, e) is constructed with pixels in the region inside the initial contour (r i ). the edge weight to the source node w(u, s) is assigned according to the probability of the pixel intensity of node u to be a source node using the probability density function. this is shown in fig. 5 ."
"we experiment with the images in ava that are associated with the textual tags listed in figure 1 . these images were split into 5 folds, with images being evenly distributed over the folds according to their semantic tags (training, validation and test lists will be made available on-line for those interested in reproducing our results). three folds were used for training, one fold was used for validation, and one fold was used for testing. the models were trained 5 times, with folds being switched in a round-robin fashion so that every fold was used as the validation and the test fold exactly once. the results we present are the average over the five folds."
"a limitation of our study is that the neighboring structure with strong edge gradient (spine, bowel) induces the segmentation result to slightly overestimated. for example, in the case of outer wall connecting to the spine, the spine is included in the search space and the segmentation result is stopped at the edge of the spine. another limitation is at the transition zone between non-aneurysmal and aneurysmal portions of aorta, there is too steep on surface of aneurysm that produces very low edge gradient between outer wall and adjacent structures. the segmentation result is infrequently underestimated in this region."
"in addition, because of slow and turbulent flow, blood clots or thrombus are commonly found in aneurysms separating outer wall from inner wall. during aneurysm growth, the outer wall expands and the thrombus formation progressively increases non-uniformly. for this reason, the inner wall grows in non-uniform proportion to the outer wall, making the outer wall difficult to segment."
"this paper introduces a new method for detection of the outer wall of aaa in ct angiography (cta), which is currently the most widely used imaging modality in patients with suspected aortic disease, particularly in emergency situations. grayscale medical images are difficult to be segmented with a single segmentation technique because the pixel similarity between foreground and background objects is very high. in the real situation, many difficult cases were found, which are irregular shaped and abundant soft tissue structures contacting to outer wall. the segmentation of difficult cases is yet an open problem. thus, this paper focuses on the segmentation of outer wall of aaa in both easy and difficult cases. our solution is to use twolayer iterative segmentation, alternating between intensitybased and contour-based segmentation techniques. the proposed approach of iteratively combining two different segmentation methods is developed based on the concept of variable neighborhood search (vns) [cit] . the basic idea of vns is to change the neighborhood structure when one algorithm gets trapped in its own local minimum. the second algorithm changes the neighborhood search space to help escape from the local minimum. if the algorithms are properly chosen, a local minimum of one is not a local minimum for the other. in our case, we combine two greedy search algorithms that are both seeking the same solution but are searching in different spaces. by interleaving the two segmentations, they are able to move beyond the local minima and may reach the globally optimal solution. this paper is the first to apply the concept of vns to solve the problem of grayscale medical image segmentation."
"examples of initial contours and segmentation results for easy and difficult cases are shown in fig. 9 . the segmentation terminated after only a few iterations with the resulting contour successfully reaching to the outer wall in easy and difficult cases as shown in fig. 10 . the input, updated, and output contours in each loop of the iterative segmentation progressively converged to the outer wall (fig. 11) . for most of the cases, the contour converged to the outer wall within 10 iterations, which is fast compared with classical active contour alone. the accuracy of the segmentation result was quantitatively evaluated by comparing the segmentation result with ground truth. ground truth segmentations were obtained by manual segmentation by an experienced cardiovascular radiologist using the gnu image manipulation program (gimp version 2.8.18). the segmentation of contiguous image slices was obtained in two easy and two difficult cases. the sub-selected slices were from the level of renal to iliac arteries using original 1-or 1.25-mm slice thickness (150-200 slices for each case). 3d reconstructions were generated for visualization and data correlation, as shown in fig. 14 . ground truth segmentations were also obtained by manual segmentation with the 3d slicer version 4.7.0."
"the semantic parameters α c are learned by minimizing the (regularized) negative log-likelihood of the data on the model, which leads to the traditional logistic regression formulation:"
"where a is a binary image and b is a structuring element in elliptical shape. the initial contour is mandatory input in the next step. from the binary image of the aortic lumen, this region is dilated to be the initial contour for the next process. because the outer wall is located beyond the inner wall, the inner wall contour must be dilated to be the initial contour. the size of the dilatation is a function of the lumen size, which has been segmented in the preprocessing step. then the outer boundary of the dilated label d is the initial contour for the next segmentation."
"this study has introduced a fully automated process for detection of the outer wall of aaa by using the new concept of vns that iteratively combines two different segmentation techniques, with one searching through intensity space and the other searching through gradient space. the interleaving combination of gcpdf and gcbac showed high accuracy for outer wall segmentation in both easy and difficult cases. this novel proposed method for the outer wall segmentation is a highly promising tool to reduce the time and effort to evaluate aaa."
"semantic retrieval is currently perceived by users as a commoditized feature of multimedia search engines. this is confirmed by a recent user evaluation [cit] performed to determine the key differentiating factors of an image search engine. the top five factors were reported to be: \"high-quality\" (13%), \"colorful\" (10%), \"semantic relevance\" (8%), \"topically clear\" (7%) and \"appealing\" (5%). semantic relevance is only ranked as the third factor, whereas features related to the quality and aesthetics rank first and second."
"the aorta is composed of inner and outer walls. the normal diameter of the aorta is typically not larger than 30 mm, with a patient considered to have an aneurysm when the absolute diameter is increased more than 1.5 times of the normal diameter. in general, the maximum cross section diameter (outer to outer wall) is related to risk of rupture due to increased wall tension with increased radial growth. the risk of rupture is defined to be high when the maximum diameter is greater than 5.5 cm and the rate of increase is more than 1 cm/year [cit] . the peak wall stress has shown more reliability to predict rupture and is highly dependent on the aneurysm shape [cit] . a 3d model of aneurysm can be used to calculate peak wall stress. in the literature, such models have been built using the segmented aneurysm in medical images via manual, automatic, or semi-automatic processes. however, manual delineation is a time consuming process that requires more than an hour for each case."
"measures of performance. we report the normalized discounted cumulative gain (ndcg), precision and mean average precision (map). we focus on ndcg and precision at 10, 20 and 50 as, in a real world application, it is more important to have accurate results among the top ranked images (typically the ones fitting in the first two or three pages of a search engine result). we also plot map calculated on the whole image ranking. we report ndcg@k averaged over all semantic tags. ndcg@k was computed as:"
"segmentation of the outer wall of aaa is particularly challenging due to three main factors. first, in most of the cases, the outer wall is not symmetric around the inner wall. since only hard structure is in the back of aorta (spine), the aneurysm grows non-uniformly with turbulent flow inside. some areas have slower flow and develop thrombus lining along the inner wall in that region. this is the reason the outer wall is not always symmetric around the inner wall. second, the aorta may not always be oval shaped due to the tortuosity and severity of disease, as shown in fig. 1 . third, there is a lack of color information and a high degree of similarity between foreground (aorta) and background (surrounding tissue)."
where a is a binary image and b is a structuring element in elliptical shape. the dilation of a by b is given by the expression [cit]
independent ranking model (irm). the simplest strategy one can think of to model aesthetic and semantic information is the irm of figure 4 . it consists of training a set of semantic classifiers (one per class) and a single class-independent aesthetic ranker capable of learning differences in quality between pairs of images.
"the classical graph cut can partially detect the outer wall, but other similar intensity pixels in the background are also included. it has dsc and jsc of 69.48±16.87 and 55.77±20.02%, respectively. the result of gcbac alone has dsc and jsc of 64.53±16.49 and 49.75±17.76%, respectively. the result of gcpdf alone is similar to the conventional graph cut and gcbac, with dsc and jsc of 64.80±11.80 and 49.06±13.13%, respectively. among the existing segmentation methods, the conventional graph cut shows the best result. this is because the pixel intensity is useful information for the grayscale segmentation. however, none of the individual segmentation methods is sufficiently accurate in detecting the outer wall for clinical diagnostic purposes."
"in table 1, we shows precisions at differing ks with and without rebalancing for jrm. it is not completely surprising that jrm without rebalancing performs similarly to a semantic classifier. in fact, pairs showing the ranker differences between high and low quality images are very rare. most pairs train the ranker to discriminate between the various semantic classes. with rebalancing we greatly improve the performance since aesthetically relevant pairs are given more importance. these results will serve as a baseline for the two models we introduce in the next subsection."
"in this work, we investigate three strategies to rank images by taking into account semantic relevance and aesthetic quality. in particular, we improve state-of-the-art approaches that attempt to learn aesthetic and semantic information jointly. we perform a quantitative and qualitative analysis on a large scale-dataset containing aesthetic and semantic labels. we show that content-dependent rankers combined with semantic classifiers provide the best results, and that data rebalancing is important for improving the ranking performance."
"where n t is the total amount of training images and n u i,n u j the number of images with relevance level u i and u j . at iteration t of the sgd optimization, the w i, j weight for the sample pair is applied to the update term and suppresses the amount by which the model is updated, for frequently-occuring pairs. with this weighting, highly probable relevance pairs, such as (0, 2), are strongly penalized."
"as a rule of thumb, the logistic loss gives results which are similar to the hinge loss of the svm but the former option has the advantage that it provides directly a probability estimate. figure 4 : the three learning models we evaluate. jrm models semantics and aesthetics jointly, whereas irm and drm learn two separate models with different dependence assumptions."
"a key aspect of our work is the study of methods that reuse existing corpora with aesthetic and semantic annotations. recently, a large scale database (ava, aesthetic visual analysis [cit] ) containing such annotations was published. ava was derived from the website www.dpchallenge.com, where photography hobbyists and professionals submit images in response to photographic challenges, defined by textual descriptions. the submitted images are then scored in terms of their aesthetics, taking into account the challenge description. ava provides almost 1,500 such challenges. for each image, a score distribution is available to characterize its aesthetic quality. on average, each image is described with 200 votes between 1 and 10."
"we believe that a major weakness of the jrm is that it confounds both sources of variability: semantics and aesthetics. this makes the task of the linear svm ranker more difficult. instead, we advocate models which treat semantic and aesthetic separately."
"in figure 6 we present a breakdown of the results (ndcg@20) for each semantic tag in order to understand where content-dependence is most beneficial. from this graph we can draw some conclusions. first, drm provides the best results for 15 semantic tags. for most of the other tags it is outperformed only by a small margin. second, content dependence seems to help more for the semantic tags that are easier for the semantic classifier to learn. data-rebalancing experiments were also performed for irm and drm but no significant difference was found. this is expected because for irm and drm, separate aesthetic ranking models are trained using only relevance levels 1,2 and 3 which are much less unbalanced."
"where x and y are regions in an image, which is output from automatic segmentation and ground truth, respectively. the performance of the proposed method is shown in tables 1 and 2 . for easy cases, the average dsc and jsc are 94.69±3.54 and 90.11±5.97%, respectively. for difficult cases, the average dsc and jsc are 92.71±5.49 and 86.84±8.60%, respectively. the proposed method was applied to all slices in each case. all methods shown in tables 1 and 2 were tested on our dataset, so direct the left column is the input contour. the middle column is output contour of the first layer segmentation: graph cut with probability density function (gcpdf). the right column is output contour of the second layer segmentation: graph cut based active contour (gcbac). each row represents each loop of the iteration comparison can be made. it can be seen that the proposed method significantly outperforms the other existing methods. in addition, for the 3d dataset segmentation of the easy cases, the proposed method achieves average dsc and jsc of 92.47 and 86.00%, respectively. while for the 3d dataset segmentation of the difficult cases, the proposed method achieves average dsc and jsc of 90.11 and 82.03%, respectively. the average number of iterations used for each image in the easy and difficult cases was 8.96 and 10.82, respectively. the minimum number of iterations for any image was 4 and the maximum was 20. the average running time for segmenting each image was only 0.51 and 0.54 seconds (intel xeon e5-2697 v3 2.6ghz, 128 gb ram) for the easy and difficult cases, respectively."
"in addition, table 3 shows performance comparisons on the outer wall segmentation of aaa. the methods shown in table 3 were tested on different datasets. the performance of the proposed method is shown to be promising when compared with the existing methods. the proposed method is a fully automatic, while the others [cit] are semiautomatic. also, the proposed method is shown to work effectively for difficult cases, while the others do not."
"to remove small debris after thresholding segmentation, the mathematical morphological operations by minkowski operation [cit] including erosion and dilation are applied. the output is the binary image of the aortic lumen. the erosion of a by b is given by the expression [cit]"
"in fact, the classical active contour or gcbac works well if the initial contour is close to the object, but this is not possible to achieve in practice. the addition of gcpdf solves this problem since the gcpdf is able to remove unwanted pixel intensity by using its simple probability model. gcpdf brings the contour close enough to the object (fig. 6) that gcbac is then able to find the boundary (fig. 8) . our proposed method was also successfully implemented in contiguous ct slices for 3d model reconstruction. the branching iliac arteries are not a limitation for the proposed segmentation technique (fig. 14) ."
"in the past few years, the computer vision community has demonstrated a growing interest in the data-driven analysis of image aesthetics. particular emphasis was given to the extraction of features which would suitably describe the aesthetic properties of an image. several works in this vein proposed features which would mimic good photographic practices such as the rule of thirds [cit] . in a recent work, it was shown that generic image descriptors, i.e. descriptors which were not specifically designed for aesthetic image analysis, could yield state-of-the-art results [cit] . extracted features are used to train statistical models to discriminate between \"high quality\" and \"low quality\" [cit] . the copyright of this document resides with its authors. it may be distributed unchanged freely in print or electronic forms. [cit], to predict the aesthetic score of an image [cit], or to rank images by their aesthetic quality [cit] . these encouraging results have lead to the development of several prototypes for assessing and improving image aesthetics [cit] . one such system, acquine [cit], predicts for a given image a corresponding aesthetic score . another system, oscar [cit], may be deployed to a mobile device such as a smart-phone and offers on-line feedback to help the user improve the composition or colorfulness of an image."
"in the research conducted in this paper, an it2fsvm is introduced which has been able to show a high level of proficiency in dealing with complicated classification and recognition problems. the it2fsvm merges the svm and it2fis to create a hybrid classifier with improved performance when compared to the some traditional classifiers. the it2fsvm classifier has been applied to the epilepsy phase classification problem. the results obtained from the simulations that were carried out show that the it2fsvm has a better performance than the traditional knn and naive bayes and svm method when the classifier is subjected to the original and uncontaminated input data. the input data was then contaminated with noise in order to test the robustness of the classification methods. the it2fsvm proved to have a significant level of robustness to noise in the data as there was a relatively smaller impact on the recognition accuracy of the it2fsvm under noisy data when compared to other classification methods. further work on this research direction will involve investigating better ways to optimise the membership function, also trying out other it2fsvm architectures in an effort to improve the overall recognition accuracy."
"the fuzzy forecasting methods can forecast the data with linguistic values. fuzzy time series do not need to turn a non-stationary series into a stationary series and do not require more historical data along with some assumptions like normality postulates. although fuzzy forecasting methods are suitable for incomplete data situations, their performance is not always satisfactory [cit] ."
"the application being considered in this paper is the recognition of the phases involved in the onset of an epileptic seizure, the epilepsy signals obtained from the electroencephalograph (eeg) using real clinical data is subjected to the novel classification technique [cit] . the fact that there are multiple features and also the susceptibility of the eeg data to noise results in a very challenging classification problem [cit] . the classification technique is designed to differentiate between the potentially life-saving application/research field and this is a major motivation for the research being carried out in this paper. the accurate classification/differentiation between the 3 seizure phases would give doctors and other healthcare professionals ample time to be able to prepare for the oncoming seizure. an interval type-2 fuzzy support vector machine (it2fsvm) is being proposed to deal with this problem. the it2fsvm will be utilized to differentiate between the 3 seizure phases. the fsvm is proposed due to its superior ability at dealing with uncertainties and unbalanced data [cit], this would therefore prove to provide a higher level of recognition accuracy than the traditional svm and forms the basis for the implementation of this classifier. the classification performance of the it2fsvm technique will be compared to some traditional classifiers like the knn technique, svm and naive bayes classifier. this paper is organized as follows. section ii reviews the svm theory. section iii reviews the interval type-2 fuzzy inference system (it2fis). section iv proposes the it2fsvm structure with a detailed schematic to illustrate how it functions. section v introduces epilepsy, data collection and feature extraction. section vi presents the classification method to deal with the epilepsy seizure phase classification problem. section vii contains the experimental results obtained from the application of the it2fsvm method to the epilepsy seizure phase classification problem with a comparison to other existing methods followed by a discussion of the results obtained. section viii draws a conclusion."
"a problem that arises from these computations would result in a large vector which would be difficult to classify. this is solved by implementing principal component analysis (pca) to reduce the number of dimensions in the feature vector. after this dimensionality reduction method has been implemented, we finally have 45 points which form the feature vector. this feature vector is then applied to the pre-determined classifiers."
"a relatively recent classification method is based on fuzzy logic [cit] which is the theory of fuzzy sets used to handle fuzziness/imprecision in datasets. this is done by assigning each variable with membership functions with respect to its relative distance to the class [cit] . there are two main types namely type-1 and type-2 fuzzy sets [cit] . in type-1 fuzzy sets, the membership values are precise numbers in the range of 0 and 1 whilst the membership grades of a type-2 fuzzy set is a type-1 fuzzy set due to the imprecision in assigning a membership grade. as a result, type-2 fuzzy sets are useful as they offer an opportunity for the modeling of higher level uncertainty in the human decision making process when compared to the type-1 fuzzy set where the membership grade is distinct."
"at the beginning, and on the border of the grid when the vehicle moves, the initial state of the ground is unknown. to initialize the em process, the ground mean state is set to 0 height, with a null slope. the covariance matrix is initialized with great diagonal coefficients, so that these initialization values have few impact on the final result."
"b. spectral classification using the simulated data in the conventional acoustic matching framework such as dtw, for any pair of speech events, spectrums are directly compared between them. so if one want to calculate the distances between the dialects of two speakers based on the spectral comparison, the following formula can be used: using the original dialect data and the simulated versions, a classification experiment was carried out by calculating the spectral distances between them using d 2 . the result is shown by fig. 4 . the speaker ids and the colors have the same meanings as in table ii, while an id with a top bar means a simulated taller speaker and an id with a bottom bar means a simulated shorter speaker. in this figure, we can find that speakers are classified into three big sub-trees according to their body heights. and in each sub-tree, the classification is affected by the speaker features greatly and many speakers are not classified by their dialects."
"in fact, nobody can speak all the chinese dialects. but an experienced dialectologist can label the dialect data with ipa symbols and then read every transcript by looking at the symbols and listening to the original utterance at the same time. at last, the second author finished this challenging work. then, the new version of data was checked at least twice by different linguists. by listening to the original utterance and the corresponding new one, they were ensured to be the same linguistically."
"in this paper was presented a 3d point cloud ground segmentation system, based on a dynamic estimation of local ground elevation and slope. the system models the ground as a spatio-temporal conditional random field, dividing the surrounding into interconnected elevation cells, affected by local observations and spatio-temporal dependencies. ground elevation parameters are estimated in parallel in each cell, using an interconnected expectation maximization algorithm variant. the computations are accelerated using gpu technologies, allowing real-time performances on embedded devices and experimental platforms. experiments on various environments (city, countryside, mountain roads,...) using different sensors (velodyne-64, ibeo-lux) show favorable results. if qualitative analysis of these results are promising, a quantitative analysis and rigorous comparison to other methods are to be defined and performed. future works will consist in those, integration of dynamic knowledge on moving objects in the model and automatic generation of occupancy grids using those results, defining a smart sensor model. fig. 8 . scene where the road is higher than the ground on the right, in front of the inria building. top right is the result at a given time t, accurately describing the form of the ground. at the bottom are depicted the ground estimation at the next time step t+1, without temporal conditioning and with. in yellow-circled areas can be observed the ground figure conservation in non-visible regions (obstructed by another car) when temporal constraints are added. fig. 9 . results of our approach on kitti raw dataset in an urban environment with vehicle traffic."
", representing the value of node at previous time step, to the state of node g i at current time step. in this paper an identity matrix will be used."
"for the experiments, a renault zoe car (figure 4) has been equipped with a velodyne hdl64 on the top, covering a 360-degree field of view (fov), 3 ibeo lux lidars on the front covering 160-degree fov and one on the back covering 85-degree fov. xsens gps and imu provide vehicle velocity and orientation. two ids cameras, on the front and on the back, are also mounted."
"referring to equation (5), table iii and fig. 5 shows the results and visualizations of pfz with the amount of catches in that cluster areas are equal to or greater than 5 at three differents temporal intervals."
"proposed model: we introduce a proposed fuzzy time series model depend on fuzzy clustering. most of authors in fuzzy time series field took the same path according to processes of the fuzzy time-series, which are presented by song and chissom (1993a), but we introduce this novel model to solve the problem in which the membership values are assumed as song and chissom model and this membership values have an important role in the forecasting values. proposed model employed seven main procedures in timeinvariant fuzzy time-series and time-variant fuzzy time series models as follows:"
"in fig. 5, it is found that all the speakers are classified by their dialects and the simulated tall and short speakers are classified near to their corresponding original speaker separately: yue and min speakers are classified into a sub-tree; xiang speakers are classified into a sub-tree together with g3 and h3; the left gan and hakka speakers are classified into one sub-tree. if we just focus on the dialects of the speakers, we can find this classification result is exactly the same as the result in fig. 3 which is obtained only using the original dialect data."
2) match the weights ω i with the corresponding x i and reassign the labels to match with the new x i which are now in ascending order.
"in order to analyze the pronunciation of speakers from different dialects using the structural representation, comparable dialect structures have to be built using their dialect utterances of the same set of some linguistic units. considering that although there are many grammatical and lexical differences and the inventory of the phonological units changes from dialect to dialect, all the chinese dialects share the same written characters and every character is pronounced as a mono-syllable, the utterances of syllable units (characters) become the best choice to build the comparable structures."
"in fuzzy logic, classification rules are specified by the user instead of being inherently decided upon by the machine learning method like in the svm or nn, this means that it is not a black-box method and the decision rules are clearly visible. fuzzy logic has been combined with the nn and svm and this gave birth to the neural fuzzy network (nfn) and fuzzy support vector machine (fsvm) [cit] . the nfn works well when the sample data provided is sufficient but suffers from a significantly reduced generalization performance when the sample data is limited, the fsvm however works well even when the sample data is limited and is proven to provide higher generalization performance [cit] ."
"the decision tree method is a prime example of the logic based classification method. classification is carried out by categorizing the inputs based on the feature values in the input [cit] . a drawback of this method is that once the splitting rule makes a wrong decision, it is impossible to return to the correct path and this would therefore result in an accumulation of errors. bayesian classifier is based on the assumption that equal prior probabilities exists for all classes [cit] . the main limitation of the bayesian classifier is that the posterior probabilities cannot be determined directly [cit] . an example of the instance-based method is the k-nearest neighbour (knn) [cit] technique which is based on the principle that objects in a data set will generally exist in the neighbourhood of other objects with similar properties. the technique finds the k nearest objects to the particular input and determines its class by looking for the most frequent class label."
"referring to table iv, it can be seen that the it2fsvm classifier outperforms other classifiers in terms of average recognition accuracy for testing dataset, which shows that the it2fsvm demonstrates an outstanding generalization ability dealing with unseen data. compared with other classifiers, the average testing recognition accuracy is 10% to 21% higher. it is interestingly observed that the naive bayes classifier performs the worst to the testing data, which shows that it is very sensitive to unseen dataset."
the proposed fuzzy time series model is introduced to handle forecasting problems and improving forecasting accuracy. each value (observation) is represented by a fuzzy set. the transition between consecutive values is taken into account in order to model the time series data.
"is the ratio between compensated densities and coefficient as shown in equation (3). it is a criteria to define the minimum number of objects that should exist in a cluster. 6) clustering. firstly, each object that has a density greater than dt is labeled as a cluster. then, for each object are checked with the other objects in neighboring cells. if the other object in the neighboring cells has a greater density than dt and its distance less than r, then that two clusters that contain that objects are merged into one cluster. 7) removing noise. from those clusters obtained, clusters with the average density less than dt considered as a noise and will be removed. this study implement the agrid+ algorithm for clustering fish catch data which is the spatio-temporal data that has the information of fish catch coordinates as spatial dimension and fishing time as temporal dimension."
"this spatio-temporal clustering method can be implemented either by conducting a single step clustering in the 3-dimensional data or do the clustering in two phases, i.e. get spatial information first and then the temporal information. each type of clustering give a slightly different results, because each type give emphasis on certain dimensions, whether in spatial dimension or in temporal dimension [cit] ."
"one of the approaches in spatio-temporal data mining is a spatio-temporal clustering where the process is to analyze the data object without knowing the label of its class. spatio temporal clustering itsef is widely used in many fields, e.g. in medical application areas, security, environment, biology, pathology, health, fisheries, and others [cit] ."
"when considering a real world application of the svm, it is important to account for the difficulty in obtaining a precise measurement of the input data. one of the major disadvantages of the svm technique was its sensitivity to outliers and noise in the input data, this is due to the fact that the svm assigns the same penalty cost to each data point, the fsvm is able to solve this problem by assigning membership functions to each data point which vary according to the relative importance of this data point, this therefore helps in reducing the impact of outliers in the input dataset [cit] ."
"provides superior class recognition for the pre-seizure phase in the training, noise-free testing and noise testing of both classifiers. this is a critical difference between these classifiers. the it2fsvm however shows a superior overall/average recognition accuracy when compared to the svm and this"
"in this paper, several experiments are carried out and show that the structural representation of chinese dialect pronunciations can extract the speaker-invariant linguistic features and classify speakers based on their dialects. at the beginning, a dialect-based speaker classification experiment is carried out using the utterances of 19 dialect speakers. then the original utterances spoken by different speakers are read linguistically by one experienced dialectologist in her own voice and a new corpus with minimum speaker differences are built. using the mimicked data, classification experiment is carried out and the result is very similar to the result obtained using original dialect data. after that, data sets with maximum speaker differences are built using high-quality voice morphing techniques and several verification experiments are carried out using our structural comparison and the conventional spectral comparison. by these results, our proposal is shown again that it can extract the purely linguistic features and the classification result are not affected by the speaker features."
"where w 2 is a partition function, β a scalar parameter representing the confidence in the elevation estimates of the neighboring cells and f ij a matrix depending on the relative positions of two nodes:"
"a classification problem can be best illustrated when an object or group of objects have to be assigned into a pre-defined group or class where the assignment is made based on a number of observed features/attributes pertaining to that particular object. classification is a very important field of research due to the advantageous nature that a classifier of high generalization ability would have in the economical, industrial and medical field [cit] just to name a few. as a result of this, extensive research has been carried out over the years and this has resulted in a large number of applications e.g., classification of different investment or lending opportunities as acceptable or unacceptable risk [cit], hand-writing recognition [cit], image classification [cit], medical engineering [cit] and speech recognition [cit] ."
a classifier based on the proposed it2fsvm structure has been implemented for the classification of the 3 seizure phases with the aid of the feature vectors obtained from the feature extraction method applied in the preceding section. the structure of the it2fsvm consists of 3 it2 svm blocks that are used to distinguish between the 3 seizure phases. fig. 3 shows the overall structure of the fsvm classifier which consists of 18 45-input-single-output svms (6 for each of the it2 svm blocks). the need for 3 sets of svm machines to distinguish between 3 classes of data stems from the fact that the svm can only separate between 2 classes at any given time.
"the proposed it2fsvm classifier is used to classify between the 3 epilepsy seizure phases using the feature vector that has been obtained by the method detailed in section v-a. for comparison purposes, compared with the knn classifier). this however is not an indication of the knn being a superior classifier as we see that it suffers from a significant reduction in its average recognition performance when exposed to unseen test data with and without noise as seen in column 3 of table. iv and column 2 of tables v-viii . the 100% average training accuracy seen in table. iii is reduced to 56.6667%"
"where z is the feature space vector, ω is the weight vector and b is the scalar threshold (bias). the set s can be said to be linearly separable if there exists a combination of ω and b that satisfy the following inequalities for all elements of the set s."
"after the length of intervals for each dimension, then the partition is done by assigning each object to the cell according to the value of its features. next step, compute the distance between an object a in a cell and the objects in its neighboring cells, and its density is the count of objects that are close to object a. determination of the neighbor is based on the ith-order neighbor. [cit] defined the ith-order neighbor as a cell in d-dimensional space which shares ( − ) dimensional facet with cell, where cell is a cell in the d-dimensional space that contain an object a, and q is an integer between 0 and d. the 0th-order neighbors of is itself. examples of ith-order neighbors in a 2d space are shown in fig. 1 ."
"in this section, the it2fsvm classifier is introduced. the standard svm classifier is used for this hybrid classification mechanism which involves the merging of an it2fis with an svm to form the the overall it2fsvm architecture is shown in fig. 3 . as the hyperplane can only separate between 2 classes, multiple svms will be necessary in a case where there are more than 2 classes in a classification problem. for the application in this paper which is to differentiate between the epileptic seizure stages, multiple svms will be needed as there are three classes (seizure-free, pre-seizure and seizure). there are three it2 svm blocks in the diagram which are used to individually separate between the seizure phases. it2 svm 1 separates between the seizure-free and pre-seizure phases with the label \"−1\""
"in fact, the dialect regions of hakka, gan and min are very near to each other geographically, genetically, phonologically. it is found that several sub-dialects of hakka are located at the middle of gan dialect region and the xiang dialect region are also very close to gan dialect region geographically [cit] . and about speaker g3 and h2, before the experiment, their data were checked by a dialectologist. it is found that the dialects of g3 and h2 are most different to other gan speakers and hakka speakers, respectively, and their three pronunciations of some characters are not very steady. meanwhile, it is also considered that the linguistic distances of these dialects are different to their acoustic distances because traditional linguists classify chinese dialects not only according to their acoustic features. thus, the acoustic distances of these dialects and the linguistic distances of them cannot be compared directly. so we design a new experiment to prove our approach can extract the purely speaker-invariant linguistic or dialect features."
"indicating the input data belongs to the seizure-free class and label \"1\" indicating the input data belongs to the pre-seizure class. it2 svm 2 separates between the seizure-free and seizure phase with the label \"−1\" indicating the input data belongs to the seizure-free class and label \"1\" indicating the input data belongs to the seizure class. it2 svm 3 separates between the pre-seizure and seizure phase with the label \"−1\" indicating the input data belongs to the pre-seizure class and label \"1\" indicating the input data belongs to the seizure class. the output labels of the three it2 svm blocks are presented in output 1 to output 3 which are then subjected to a rule-based class determiner in order to determine what the final classification would be."
"where w 3 is a partition function, γ a scalar parameter representing the confidence in the elevation estimates of the cells at previous time step and q i a transition matrix that transforms g"
"then, to approximate the full distribution over g i and c i, an iterative expectation-maximization (em) [cit] algorithm is used. the expectation (e) step estimates the probability distribution over the point classification c i while the maximization (m) step uses this distribution to estimate the ground state distribution. those 2 steps are alternatively repeated in a loop over a number of iterations."
"in brief, the current situation of chinese dialects is becoming more and more complicated. strictly speaking, every speaker has his/her own dialect, and the pronunciations of two speakers of the same dialect often show different sub-dialect features because they may belong to different sub-sub-dialects."
"the m-step estimates the ground state distribution for each node given the measurement values, the classification and the neighborhood interactions. to approximate the distribution, the information vector x i and information matrix p i are updated for each node i as follows:"
"step 4: partition universal of discourse u into equal intervals: according to this step, the proposed model, partition the universe of discourse into c intervals."
"we utilize the relevant channels for classification (i.e channel 1, 2, 3, 4, 5, 6, 11, 12, 13, 14) for the simulations carried out in this paper. for each of the channels, a feature vector containing the timedomain and frequency-domain components of the dataset is created [cit] . the first part of the feature vector comprises of computations in the time-domain such as the standard deviation, second order norm, third order norm, fourth order norm, absolute sum, maximum value and minimum value of the 100 sample points from each channel. the second part is comprised of computations in the frequency domain such as the mean frequency, maximum frequency, minimum frequency, standard deviation of frequency, windowing filtered mean frequency and windowing filtered maximum frequency of each chosen channel will form the second part of the feature vector."
"the final output is obtained by combining a number of svms with the aid of fuzzy rules (which determine the number of svms) and membership grades or weights (which depict the impact that a particular fuzzy rule would have on the final output). there is no limit to the number of fuzzy rules that can be applied in this instance but an increase in the number of fuzzy rules would lead to a slower convergence of training and also a higher computational cost of the system. in this paper, there are 3 fuzzy rules employed to implement the it2fsvm. the membership grade is obtained from the membership function which is defined by the user and the shape of the membership function is a triangle as shown in fig. 1 . the shape of the membership function is represented by the points p 1 to p 7 which are then optimized with the aid of ga."
"the svm theory is reviewed in this section, which provides the theoretical background to the development of it2fsvm. the main objective of the svm is to create a separating hyperplane such that the distance between the hyperplane and the nearest data point in each class is maximized."
"the rule based class determiner system for selecting the final classification output for the it2fsvm table. i. the final class is a whole number between 1 and 3 where \"1\" representing the seizure-free phase, \"2\" representing the pre-seizure phase and \"3\" representing the seizure phase."
"potential fishing zone can be determined from spatio-temporal fish catches data using data mining approach. agrid+, a grid-density based clustering, can be used to perform clustering data. the clustering results are used as a reference for determining pfz using thresholding techniques."
"where a ij and b ij mean the (i, j) element of the bd-based distance matrices a and b, respectively. m means the number of the syllables."
the most prevalent of these is an algorithm developed by karnik and mendel [cit] known as the karnik-mendel (km) algorithm which is iterative and very fast in achieving a state of convergence.
the future works are to do an adaptation of agrid+ algorithm especially for spatio-temporal data only. we also planned to do a fish forecasting by adding features of sea surface temperature (sst) and chlorophyll-a.
a very important theorem for the svm theory is the karush-kuhn-tucker theorem [cit] which states that the solution α i to (9) satisfies the following conditions:
is the kernel function which is used for the mapping onto a higher dimensional feature space. the kernel functions may be linear or nonlinear. the nonlinear separating hyperplane can then be found by solving the following equation
"where, is the interval length for each cells at dth-dimension, is the feature of the data at dth-dimension, and is the number of cells at dth-dimension. we have to do features normalization before the clustering process due to the difference in scale between spatial features and temporal feature."
"generally speaking, the existing traditional methods for classification can be categorized into logic based (e.g decision trees) [cit], statistical approach (e.g bayesian classification) [cit], instance-based (e.g. nearest neighbor algorithm [cit] ), perceptron based (e.g single layer perceptrons and neural networks [cit], and support vector machine (svm) classification [cit] ."
"referring to the worst individual class testing recognition accuracy, it2fsvm can maintain 70% while other drops to around 23% to 50%. tables v to viii in general, the recognition accuracy drops for all classifiers when the noise level increases. in most of the cases, the average testing recognition of it2fsvm and naive bayes classifiers offer the best result. however, when it is down to the individual class recognition accuracy, especially for higher noise levels (0.1, 0.2 and 0.5), the it2fsvm performs more stable giving the lowest class recognition accuracy of 40% while other classifiers give the lowest class recognition accuracy ranging from 15% to 36%. similar to the comment concerning the knn and its poor performance in accurately classifying the pre-seizure phase (class 2), it is important to also note that the nave bayes classifier exhibits a relatively poor ability to classify the pre-seziure phase as we see that the svm and it2fsvm"
"step 5: fuzzify the historical data: in this step, proposed model fuzzufy historical data, where the proposed model determine the best fuzzy cluster to each actual data."
"in the agrid+ algorithm, firstly each dimension is divided into many intervals and data is partitioned into hyper-rectangular cells, the 3d-rectangular cells used for fish catch spatio-temporal data. the interval value is calculated based on the number of 3d-rectangular cells to be formed. interval values for each dimension will be different according with the number of rectangular in its dimension. the length of cell's interval in each dimension obtained from the dimension's range divided by the number of cells in that dimension. the computation of interval length can be formulated in equation (1)."
"there are 3 fuzzy rules for each of the it2 svm blocks. the parameters of the triangular membership functions, i.e., p 1 to p 7, as shown in fig. 1 are optimized with the aid of ga which has the ability to influence the shape of the membership functions. the ga optimization is performed to maximize the recognition accuracy using 70% of dataset as the training samples. the rest 30% of dataset are used as the test samples. the lower and upper membership functions for svm block 1 to 3 after training are shown in figs. 4 to 6. the membership grade is represented on the y-axis and the normalized inputs are represented on the x-axis. the normalized input denoted as x norm is calculated as follows:"
"to find the estimate of a ground node at previous time step g t−1 i, the previous grid is transformed into the current reference frame, using the transform computed from the vehicle displacement and orientation change, calculated from the fusion of imu, gps and odometry data. for each node g i the value at previous time is then interpolated. in the areas corresponding to newly-discovered regions, the values at previous time step are set undefined, while taken out areas can simply be forgotten, or stored in a long-term map."
a. [cit] . the geographic coordinates for this location area ranging from latitude 16.56 -2 s and longitude 100.49 -140 e. this data obtained from fish catch data by the shipping company pt. perikanan nusantara indonesia.
"in our previous study, a structural pronunciation representation was proposed to extract speaker-invariant speech contrasts or dynamics [cit] and applied to speaker-independent automatic speech recognition (asr) [cit], speech synthesis [cit] and computer aided language learning (call) [cit] . then, this approach was further applied to chinese dialect analysis [cit] and dialect-based speaker classification [cit] with satisfactory results were achieved."
"determination of the area that stated as pfz can be done through thresholding technique. this threshold is obtained from the opiniion of a fishing company, pt. perikanan nusantara indonesia, which states that an area categorized to be potential if the amount of the catch is equal to or grater than 5 in a single fishing trip. this number is related to the profits calculation with the minimum number of catches."
"the feature extraction procedure is very vital in the classification process as it obtains the relevant characteristics and information from a large dataset (eeg signals in this instance), this has the knockon effect of simplifying the dataset and also reducing the effect of redundant data points that have little or no effect in the classification of the dataset. this is a very important step in improving the performance of the classifier as classification is easier when the classifier is subject to fewer data points."
"agrid+ algorithm is consists of seven steps, partitioning, computing distance threshold, calculating densities, compensating densities, calculating density threshold, clustering and removing noise. 1) partitioning. the whole data is partitioned into cells according to the number of cells in each dimension (m). every non-empty objects are inserted into the cells with corresponding coordinates. the coordinates are computed based on the interval in each dimension (l)."
"here, every speech event, such as the pronunciation of one syllable, is captured as a distribution and event-to-event distances are calculated as bhattacharyya distance (bd),"
2) match the weights ω i with the corresponding x i and reassign the labels to match with the new x i which are now in ascending order.
"in china, there are hundreds kinds of dialects. traditionally, they are classified into 7 major dialect regions [cit] and every dialect region has many sub-dialects and sub-sub-dialects [cit] . therefore, in order to process all the sub-dialects of one dialect region by training different models for different subdialects, dozens of models must be built sometimes. further, because of the popularization of mandarin and population movement across different dialect regions, the dialects of many speakers are also changing. so two speakers from the same dialect region may speak different sub-dialects and it is a very challenging work to build dozens of sub-dialect models for one dialect region by collecting the data of many speakers from the same sub-dialect region."
"epilepsy, which is characterized with its ability to instantiate recurrent seizures (an interruption of normal brain functions) which are unforeseen in nature is a very common and significant neurological disorder caused by a sudden discharge of cortical neurons [cit] . epileptic seizures are classified as either partial (involving focal brain regions) or generalized (where it involves a widespread region of the brain across both hemispheres) [cit] . the length of time for the seizure occurrence varies from a few seconds up to a minute with some of the effects including momentary lapse of consciousness for the sufferer of the seizure [cit] . a complete loss of consciousness occurs when the epileptic activity involves both the cortical and subcortical structures of the brain and this occurrence is known as an absence seizure."
"abstract-in our previous works, a structural pronunciation representation was proposed to extract the linguistic features from dialect pronunciation and classify speakers based on their dialects. in this paper, in order to prove that the structural method can extract the purely speaker-invariant dialectal features, several new experiments are carried out. first, using the data of 19 speakers from different dialect and sub-dialect regions, a dialect-based speaker classification experiment is carried out and satisfactory result is achieved. then, one chinese dialectologist transcribes all the data and reads the linguistic content of each original utterance in her voice through looking at the transcript and listening to the original utterance. so a new data set with minimum speaker differences (fixed speaker identity) is created. using the new data, similar classification experiment is carried out and the result is very similar to the result of last experiment. it means that our method can extract the purely speaker-invariant dialectal features and classify speakers based on their dialects very well. after that, for the original and mimicked data sets, data sets with maximum speaker differences are simulated using high-quality voice morphing techniques. using the original dialect data and the simulated versions together, classification experiments are carried out based two criteria, spectral comparison and structural comparison. by comparing these results, we can find that unlike the method of spectral comparison, the structural method can purely classify speakers based on their dialects, which shows the proposed dialect structures are speaker-independent and linguistic enough features."
"generally speaking, dialect means a variety of a language that is used by a particular group of that language's speakers. among dialects, there are always some phonetic, grammatical, and lexical differences to different degrees. in modern speech processing technologies, segmental features of speech are usually represented acoustically by spectrum, which contains not only linguistic information but also extralinguistic information corresponding to age, gender, speaker, and so on. therefore, in order to process different dialects in conventional spectrum-based dialect processing frameworks, dialect-dependent but speaker-independent models were always trained by collecting utterances from many different speakers of one dialect. however, this approach may not work well especially in chinese dialect processing."
"in order to simplify the inference, a gaussian model is chosen for the ground state. the distribution over each node state g i is represented by a mean vectorg i and a covariance matrix m i . in the following, to simplify the formula, the information vector x i and information matrices p i will be used, defined as follows:"
"define the universe of discourse u: define the universe of discourse for the observations. according to the issue domain, the universe of discourse for observations is defined as:"
"define the linguistic terms: each linguistic observation, a k can be defined by the intervals u 1, u 2,...,u n, as follows:"
"using d 1, the distance between the dialects of two speakers is calculated as the distance between the pronunciation structures of them. then these speakers are classified and the result is shown by fig. 2, where the structure of every speaker is represented by the speaker id in table ii and different colors show different dialect regions. in this figure, the result is shown by a bottom-up clustering method, ward's clustering method."
the support vector machine (svm) [cit] as a machine learning model that went on to be applied to various supervised and unsupervised learning applications [cit] . the svm approach can be split into support vector classification (svc) which are used for task such as pattern recognition and support vector regression (svr) which is mainly applicable to time-series applications [cit] . the main concept of the svms is that of the hyperplane which is used to separate two data classes. the aim of the svm is to maximize the margin between the hyperplane and the input samples which is being separated by it thereby reducing the generalization error. data that is difficult to separate on the input space is mapped into a higher dimensional feature space for ease of separation. computations on the higher dimensional feature space are possible with the use of a kernel function [cit] . this feature illustrates a very important trait of the svm which is its ability to perform well in a high dimensional feature space [cit] .
"in china, there are hundreds kinds of dialects and they are traditionally classified into 7 major dialect regions (guanhua, wu, xiang, gan, kejia, yue and min) [cit] . moreover, most of the major dialects also have many different sub-dialects and sub-sub-dialects. for example, there are 8 sub-dialects and 42 sub-sub-dialects in guanhua dialect region. all the dialects are developed from the same root and they have inherited a lot of common features. they are sharing the same written characters, similar sound systems, the same phonological structure and similar phonetic features, etc. for example, every written character is pronounced as a monosyllable which is combined by an initial, a final and a tone. however, due to many historical or geographic reasons, there are still many differences among these dialects grammatically, lexically, phonologically and phonetically. take the finals as example, there are 38 finals in mandarin but 53 finals in cantonese and 32 finals in shanghainese."
silhouette index is used to evaluate the clustering results. the values of silhouette index for each object indicate the representation of how well each object lies within its cluster. it was first described by peter j. rousseeuw [cit] . silhouette index is computed according to the similarity between an object and the other objects of the cluster it belongs to compared with the similarity between an object with the other objects of each of the other clusters. equation (4) is the formula to compute silhouette index.
"the forecast accuracy is compared by using normalized root mean square error (nrmse). the normalized root mean square error (nrmse), in statistic is the square root of the sum of the squared deviations between actual and predicted values divided by the sum of the square of actual values:"
"the goal of the processing is to estimate the unknown variables g and c, given the observed variables z and the random distribution of variables g at previous time step g t−1 i ."
euclidean norm) and c is known as the regularization constant. it is the only free parameter in the svm formulation and can be tuned to find a balance between margin maximization and classification violation. the optimal hyperplane can be found by constructing a lagrangian multiplier and obtaining the dual formation:
the unexpected nature of these seizures has proven to have an adverse effect on the quality of life for those who are suffering from them. the impact is most prevalent in the formative stages of a childs life as we see an increase in the requirements for special education and also a higher incidence of below-average school performance [cit] . it also proves life-threatening in situations where the sufferer is isolated at the time of its occurrence and there is no experienced or medical help on hand to alleviate the situation. therefore having an accurate understanding or predictive model for the pre-seizure phase (the transition towards an absence seizure occurrence) is a very vital task as it would provide the sufferers and their carers enough notice of the upcoming seizure so they could prepare themselves and dampen the impact of the seizure occurrence.
the equalities (10) and (11) suggest that it is only the nonzero values α i in (8) that satisfy the constraints in (6) . the values of x i that corresponds with the solution α i are known as support vectors.
"using the mimicked data and the warped utterances, their dialect pronunciation structures are built and speakers are classified based on the distances between them and the result is shown in fig. 5 . in this result, the speaker ids and the colors are the same as those in table ii, while an id with a top bar means a simulated taller speaker and one with a bottom bar means a simulated shorter speaker."
"the second step of ouput processing occurs after type-reduction. in the case of the km algorithm being used as a type-reducer, the type-reduced set is always confined to a finite interval of numbers, the deffuzifier then obtains the defuzzified value (which is a crisp output) by calculating the average of the upper and lower bounds of this interval."
"the single layer perceptron can be simply described as a component that computes the sum of weighted inputs and then feeds this to the output of the system. a major limitation of the single layer perceptron is that it can only learn linearly separable problems and is therefore incompatible when considering non-linear problems [cit] . this problem is solved by the introduction of the neural network (nn). the neural network can be divided into 3 distinct segments. the input units which have the primary responsibility of receiving information, the hidden units which contain neurons carry out the input-output mapping and the output units which store the processed results [cit] . by determining properly the connection weights and transfer functions, nn can be regarded as a universal approximator [cit] which is able to approximate any continuous functions (e.g., hyperplanes) to any arbitrary precision in a compact domain."
"in this paper, the dialect pronunciation structure is applied to extracting the purely linguistic features from chinese dialect to classify speakers based on their dialects and the speakerinvariance of this approach is examined. in section 2, the current situation and fundamentals of chinese dialects are introduced. then the method for building comparable dialect pronunciation structures and calculating the distance between them is described in section 3. in section 4, dialect-based speaker classification experiment is carried out using the dialect data of 19 speakers. in section 5, this proposal is verified by classification experiment using data with minimum speaker differences. in section 6, this proposal is further verified by classification experiment using data with maximum speaker differences and the result is compared with the classification based on spectral comparison. at last, this paper is concluded in section 7."
"the rules are responsible for the mapping of an input space x to an output space y . experimentation has shown that the general t2fis model has high complexity and large computational costs. this has resulted in the development of the it2fis which makes the computation simplified. the membership grades for interval fuzzy sets can be portrayed by their lower and upper membership grades of the fou. the output of the firing strength for an it2fis ω i is represented by a lower and upper bound i.e.,"
"on the illustrations, the point clouds are segmented into ground-related points in green, and obstacle-related points in pink. on most of them is also represented a red mesh, partly representing the ground elevation model : to every point of the mesh corresponds the average estimated height of the ground in the node at this location. it is worth noting that in long-term dataless areas, while the average height tends towards 0, the variance tends towards infinity, so that the systems knows those values are meaningless. on figure 1 is depicted the simplest example of ground segmentation, as the ground is flat and obstacles are high, with sharp edges with the ground. on figure 5, the scene is a bit more complex, as some low-height obstacles and many with less-sharp edges with the ground could have deteriorated the estimation. the figures 6 and 7 show results on rural mountain roads, with important slope variations, with different sensor inputs. if velodyne point segmentation confirms the method efficiency in these difficult environment, the most interesting results are to be seen with the ibeo data : with few data inputs, the system is able to correctly assess navigable space and real obstacles, thanks to the spatiotemporal conditioning. figure 8 shows the concrete effect of temporal filtering on the model, especially the conservation of the estimated ground profile in case of momentary obstacle interposition. figure 9 shows latest results in city environments, using the kitti dataset. not only those results show persuasive outputs, but the use of these partially labeled data (in terms of ground/obstacle segmentation and ground elevation), widely used in the domain, will lead to more quantitative evaluations and comparisons."
2) computing distance threshold. the distance threshold (r) is a criteria to define whether two objects are close enough or not. it is computed according to the interval lengths of every dimensions using equation (2). 3) calculating densities. the density for each object is counted according to the number of objects both in its neighborhood and in its neighboring cells using ith-order neighbor concept. 4) compensating densities. the density compensation is the product of its original density and ratio of the volume of the neighborhood to that of the considered part of neighborhood. 5) calculating density threshold. the density threshold (dt)
"for the new experiment, we tried to build a new corpus that the features of speaker differences are removed manually. in fact, if there is a chinese dialectologist who can speak all these dialects, the dialect utterances recorded above can be repeated linguistically by him/her, which gives us the dialect utterances only with a fixed speaker identity."
"using the mimicked data, the classification experiment is carried out and the result is shown in fig. 3, while the ids and colors are the same as those in table ii . by comparing this result with fig. 2, it was found that they are very similar to each other. in both of these results, all the speakers are classified into four large sub-trees and each has the same speakers: speakers from yue and min are classified into their individual sub-trees; speakers from xiang are also classified into a large sub-tree and speaker h2, g3 are also classified into this sub-tree as well; the left gan speakers and hakka speakers were clustered into a large sub-tree, which itself has two sub-sub-trees corresponding to gan and hakka separately. by focusing on the speakers (gan, hakka and xiang), we can find their positions are exactly the same in the two results. so it means that our approach can be utilized to extract the speaker-invariant purely linguistic features from the dialect pronunciation of every speaker."
"fuzzy inference systems are mainly used to represent the relationship between the input and output variables in systems which are governed by a selection of if-then rules which utilize linguistic labels for the expression of rules and facts. an it2fis is a fuzzy logic system where the uncertainty of the membership functions are incorporated into fuzzy set theory, in the circumstance where no uncertainty exists, a type-2 fuzzy set would reduce to a type-1 fuzzy set, and this is identical to the concept of probability reducing to the determinism when the unpredictability is eradicated [cit] . in order to distinguish between a type-1 and type-2 fuzzy set, a tilde symbol is placed above the symbol for the fuzzy set, in this case, a would represent a type-1 fuzzy set andã would represent a type-2 fuzzy set [cit] . type-2 fuzzy sets are seen to be more prevalent than type-1 fuzzy sets in rule-based fuzzy logic systems as they have a higher level of non-linearity and therefore have the ability to model uncertainties better than the type-1 fuzzy sets with less number of rules. the structure of the it2fis detailing the input-output relationship is shown in fig. 2 . the it2fis consists of 5 major components [cit] : fuzzifier, fuzzy rules, inference engine, type-reducer and defuzzifier. the crisp input is first transformed into fuzzy sets in the fuzzifier block as the rule base is activated by fuzzy sets and not numbers. in the fuzzification stage, when the measurements are perfect the input is modelled as a crisp data set, when the measurements are noisy but stationary it is modelled as an interval type-2 fuzzy set. after the input is fuzzified, the fuzzy input set is then mapped onto the fuzzy output set with the aid of the inference block. this is achieved by quantifying each rule using fuzzy set theory and then using the mathematics behind fuzzy set theory to obtain an output for each rule. the output of the fuzzy inference block would then contain one or more fuzzy output sets. the fuzzy output sets are then converted into a crisp output with the aid of the output processing unit. in an it2fis the output processing unit consists of two blocks: the type-reducer and the defuzzifier blocks. in the first step, the it2 fuzzy output set is reduced to an interval-valued type-1 fuzzy set in a process known as type-reduction."
"based on the fish catch area from clustering results, the determination of potential fishing zone (pfz) can be done by computing the mean of fish catches for each area. then, compare each of that means with the threshold. if it is greater the threshold, that area can be categorized as potential area. the threshold is from the expert opinion, i.e. the shipping company pt. perikanan nusantara indonesia, which states that a catch point is categorized as potential if its number of fish catches is equal or greater than 5 in a single trip."
"the svm performs structural risk minimization (srm) which aims to balance the complexity of the model with its ability to accurately fit the input data [cit] . this therefore gives the svm good generalization ability for classification problems as it can simultaneously minimize the empirical risk [cit] . the srm principle is grounded on the fact that the generalization error of the model is bounded by the sum of the empirical error and a confidence interval which is based on the vapnik-chervonenkis (vc) dimension [cit], a higher classification performance is achieved by minimizing this bound. the svm also provides a global optimization solution to the problem at hand and therefore provides a more credible output when compared to the neural network which provides a local optimization solution [cit] . one of the drawbacks of the svm method is its sensitivity to outliers that may exist in the input data, this stems from the fact that the same penalty weight is assigned to each data point and an outlier would therefore significantly distort the representation of the input signal and therefore affect the classification performance. another drawback is seen in the instance when the svm is applied to a classification problem with an imbalanced data set (where negative data significantly outweighs the positive data) the optimum separating hyperplane in this case can be skewed towards the positive with the consequence being that the svm could be very ineffective in identifying targets that should be mapped to the positive class [cit] ."
"when speech is represented acoustically by spectrum, the inevitable extra-linguistic features can be approximately modeled as two kinds of distortions according to their behaviors: convolutional and linear transformational distortions. convolutional distortions are caused by extra-linguistic factors such as different recording microphones, and vocal tract length differences are the typical reason for linear transformational distortions [cit] . if a speech event is represented by a cepstrum"
step 5: if: step 2. song and chissom (1993b) presented the concept of fuzzy time series based on the historical enrollments of the university of alabama. fuzzy time series used to handle forecasting problems. they presented the time-invariant fuzzy time series model and the time-variant fuzzy time series model based on the fuzzy set theory for forecasting the enrollments of the university of alabama. the definitions and processes of the fuzzy time-series presented by song and chissom (1993a) are described as follows [cit] .
"which is called the silhouette width of the object, where ( ) is the mean distance of ith-object to the other objects of the clusters it belongs to, and ( ) is the smallest value of the mean distance of ith-object to the objects in other clusters. the value of silhouette index is between -1 and 1. a value near 1 indicates that the object is affected to the right cluster. otherwise a value near -1 indicates that the point should be affected to another cluster."
"using the mimicked data of multi-dialects but fixed speaker identity, a verification classification experiment can be carried out. if the classification result is similar to the result in fig. 2, it will mean that our approach can extract the purely linguistic features by canceling the features of speaker differences."
presented the concept of fuzzy time series based on the historical enrollments of the university of alabama. they presented the timeinvariant fuzzy time series model and the time-variant fuzzy time series model based on the fuzzy set theory for forecasting the enrollments of the university of alabama.
"where p 1 (c), p 2 (c) mean the distributions of two speech events. with multiple events, we can obtain a distance matrix by calculating bds between any pair of them. since bd is invariant with respect to affine transformations, the obtained matrix is invariant to extra-linguistic factors. as a distance matrix can determine uniquely a geometric shape, we refer to the matrix as a pronunciation structure. therefore, with the utterances of dialect speakers, we can build dialect pronunciation structures which are invariant to extra-linguistic factors."
"in this figure, we can focus on the speakers from yue and min dialect regions first, who are classified into a sub-tree on the right of this figure. further, the speakers from yue dialect and those from min dialect are clustered to their subsub-trees. meanwhile, about speakers from hakka, gan and xiang, after checking the sub-dialect information of them in table ii, we can find that the speakers from the same subdialect are all classified near to each other in the result. but looking at speakers g3 and h2, we have to admit that all the speakers are not completely clustered into different sub-subtrees by their dialects. then, a question will still be asked that whether there is any problem with our approach or just these speakers should be classified in that way according to their acoustic features."
the simulations that have been conducted with the aid of the matlab software. the control parameters of the ga are shown in table ii . different combinations of kernel functions we utilized
empirical study: previous studies on fuzzy time series often used the enrollments data at the university of alabama as the forecasting target in many forecasting studies.
"from this raw sensor data, most perception approaches rely, at some point, on a classification between obstacle data, relevant for occupancy grid generation or object detectionand-tracking, and the data relative to the ground. if a simple height thresholding can be used in very simple situations, a much more sophisticated approach needs to be adopted in most cases. many methods rely on previously-generated maps, and concentrate on precise localization in those maps, thus extracting the local ground profile, but such maps, although being impressively developing in terms of accuracy and coverage, are not always available, nor really necessary in local perception. other approaches focus on pure data point analysis, and then have to deal with undefined ground form, and often sparse data, in particular in high-speed vehicle perception. in many cases, as the one presented in this paper, those methods are designed to be embedded with real-time performances on computationally and energetically limited systems, shared by a full perception chain, which can include important computingpower consuming components (systems of advanced situation awareness, motion planning, etc.). computational efficiency is then a critical aspect."
"in brief, we can find the structural method still works very well even using the dialect data with minimum speaker differences (mimicked data) and maximum speaker differences (simulated data) together. unlike the classification using conventional spectral comparison, these speakers are classified by their dialects and the result is not affected by speaker features at all. it is further proved that our structural method can extract the purely dialect features from speech."
"as we have no knowledge of the higher dimensional feature space ϕ(·), carrying out the computation in (8) and (13) would be rendered impossible due to its complicated nature. an advantageous characteristic of the svm is that it is not necessary to know about the ϕ(·). the problem is alleviated with the aid of a kernel function which has the ability to compute the dot product of the data points in the feature space of z, it is however obligatory for these functions to satisfy mercer's theorem [cit] before they can be used for computing the dot product [cit] ."
"fully autonomous driving is an important but challenging goal, for which a reliable perception of the local environment is crucial [cit] . accurate detection of the navigable space and classification of obstacles are key tasks for intelligent vehicles, to be able to estimate the risks on its path and the surrounding motion. many sensor outputs consist in raw impact point clouds, from which must be differentiated the points related to actual obstacles and the ones related to clear areas. laser range scanners (3d-lidars), for example, generate high resolution 3d point clouds of the environment while remaining unaffected by varying illumination, making them popular and widely used in robotics. because of their relative cost in comparison to other systems (camera, radar, ultrasonic,...), which is still prohibitive for many constructors, lidar sensors have not yet reached consumer market vehicle constraints, but recent key technological progress (solid state lidars) should make them more affordable."
"step 3: define the universe of discourse u: in this step, the proposed model defines the universe of discourse as song and chissom (1993b) were defined it as eq. 6."
"in table. iv when the classifier is subjected to the test data. another significant impact of this is that the knn now has an individual testing recognition accuracy of 23.3333% as seen in column 5 of table. iv when classifying the pre-seizure phase (class 2), this is of significance because the accurate classification of the pre-seizure phase is a core objective in addressing the problem of epilepsy seizure phase classification as this would give the patients the advance warning and therefore sufficient time to prepare for the onset of the seizure. the svm and naive bayes come in the third and fourth places."
"step 2: determine membership values for each cluster: in this step, membership values is determining after doing fuzzy cluster. the proposed model, use this membership values according to the forecasting values."
s i d ij r l l l ordinal scale s i d i d l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l ordinal scale ordinal scale neg l l neg l l neg l l i i i w min max neg d r max neg l l max neg l l max neg l l max neg l l min max neg l l max neg l l max neg l l max neg l l max l l max l l max l min l max l l max l l max l l max l l max l l min l l l l l l ls i d ij r l l l l l l l l l l ordinal scale s i d i d l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l ordinal scale ordinal scale neg l l neg l l neg l l neg l l neg l l neg l l neg l l neg l l neg l l negs i d i d l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l ordinal scale ordinal scale neg l l neg l l neg l l neg l l negscale ordinal scale ij r l l l l l ordinal scale s i d i d l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l ordinal
"the emispher virtual medical university the formation and operation of the emispher virtual medical university (evmu) for elearning (teleteaching) is one of the main efforts in the project. the evmu uses real-time broadcast of lectures, live surgical operations and pre-recorded video sequences etc., as well as web-based e-learning applications. the target population of the evmu is comprised of medical students (both undergraduate and postgraduate) hospital staff, general practitioners and specialists, health officers and citizens. each of the leading medical centres provides didactical material and modules for synchronous and asynchronous e-learning in their medical specialties. the central gateway to evmu is the project's website: www.emispher.org. some of the multimedia teaching material needs to be presented in real-time. live transmission of surgical operations from operating theatres, lectures, etc. from one site to one or several sites simultaneously (point-to-point or multipoint) are possible in the network between the 10 partners (fig. 8) ."
with a document camera analogous patient data can be captured and digitised by winvicos as a document. for example x-ray or ct-images can be captured from an illumination board and displayed locally and transmitted using this document camera function.
"additionally the signal of a room camera (e.g. for telementoring; insert right top) as well as further medical data (e.g. x-ray, ct, etc.; insert left bottom) can be transmitted live to the rh. during the surgery the video data stream of a camera integrated in the centre of the surgical light can be transmitted live to the experts in the rh using wotesa / winvicos (insert left bottom). in this way these experts receive a live video of the situs of surgery (inserts right bottom) and can advise the colleague in the mfh."
"telemedicine describes the use of information and communication technologies (ict) for the delivery of medical services. it aims at equal access to medical expertise irrespective of the geographical location of the person in need. new developments in ict have enabled the transmission of medical images in sufficiently high quality that allows for a reliable diagnosis to be determined by the expert at the receiving site [cit] . through telemedicine patients can get access to medical expertise that may not be available at the patients' site. networks for telemedicine enable the integration of distributed medical competence and contribute to the improvement of the quality of medical care, to the cost-effective use of medical resources and to quick and reliable decisions. for optimal performance of telemedical applications, the networks and communication tools used must be optimised for medical applications, both with respect to the quality-of-service (qos, a set of parameters characterising the performance of the communication channel per se, such as transmission bandwidth, delay, jitter, data loss, etc.) as well as to the class-ofservice (cos; a set of terms specifying the medical services offered in the network, like telesurgery, telepathology, telesonography, tele-teaching, -training & -education, etc.) . the use of specifically designed networks for telemedicine (distributed medical intelligence) contributes to the continuous improvement of patient care. experience over the last decade has shown that the goals of telemedicine are not automatically reached by the introduction and use of singular new technologies per se, but rather require the implementation of integral services. at the same time, however, these innovative developments in ict over the last decade bear the risk of creating and amplifying a digital divide in the world, creating a disparity in the quality of life, e.g. between the northern and the southern euro-mediterranean area [cit] . in recent years different projects have demonstrated how the digital divide is only one part of a more complex"
"mobile teams are deployed on the disaster site for search, identification, triage and evacuation of victims. they communicate with the coordination-and medical-teams located in permanent centre or mobile field hospital via low-rate (globalstar, 9.6 kbps) and medium-rate (inmarsat, 64 kbps) satellite telecommunication systems. their positions are tracked via the established global positioning system (gps satellite system)."
"the reference hospital(s) (rh), located outside of the disaster area, acts as an expert centre by providing telemedical services to the mfh using the high-bandwidth satellite link (vsat, 2 mbps). these services consist of off-line and on-line telediagnosis, access to external medical databases, as well as real-time interactive telemedical services such as live teleconsultations, live telesonography, intraoperative virtual reality simulation and interactive telemicrobiology (see figs. 3 and 4) . statistics show that in cases of disaster emergency medicine, approx. 40% more amputations are performed, as compared to normal situation. one of the aims of providing live second opinion by remote experts is to reduce this number of unneeded amputations, manipulations and subsequent complications substantially, by expert support during triage, diagnosis and medical treatment. these interactive telemedical services between the mfh and the rh are realised using a dedicated wotesa (workstation for telemedical applications via satellite) with the communication software winvicos (wavelet-based interactive video communication system). wotesa/winvicos combines the user-friendliness and flexibility of ip-based communication protocols with the security and sufficiently-high quality of the live video transmission at a satellite bandwidth of only up to 2 mbps. medical experts at the rh support the medical treatments in the mfh and enable a quick and reliable decision concerning treatment and/or evacuation of the patient/victim. in this way, the quality of the provided medical service during and after disaster emergencies is strongly improved."
"overall point-to-point connections were successful with normal delays (~650 ms). realised maximum bandwidths for symmetric point-to-point connections are 512-640 kbps in each direction. up to these bandwidths the video and audio streaming is smooth. when going beyond these settings frozen images and audio problems start occurring. for asymmetric video bandwidth allocation (for example for e-learning applications) the maximum bandwidth settings for good operation are 768 kbps in one direction and 192 kbps in the other direction. for multipoint connections between three sites the maximum bandwidth settings for good operation are 384, 384 and 512 kbps. if the results presented above are compared with the nominal bandwidths required for the various services, it is clear that the emispher platform provides sufficient bandwidth for most of the telemedicine applications with smooth video and audio streams in sufficient resolution. during the first months of full operation of the network continuous evaluations have been performed and user-feedback was collected. this resulted in the upgrade of the winvicos telemedical communication software as well as various upgrades of the medsky software platform. the valuable user feedbacks were taken into account and have lead to even improved functionalities of the system and thus improved quality of medical work. this demonstrates that the satellite platform with wotesa/winvicos telemedical stations is very suited for the envisioned variety of medical services and is easy to learn for a wide range of medical professionals from the various countries."
 it is difficult to predict the breakeven level of services and therefore to determine the minimum level of investment required for breakeven of the services.  ip levels of connectivity are also required as a minimum.  all the services need to be fully integrated into one platform.
design and implementation of satellite-based networks and services for ubiquitous access to healthcare 123 injured part of the patient's body which he needs to see for his consultation. thus a very realistic and effective live communication is possible.
 establishing and building confidence with physicians takes time.  it still seems that it will be a further 1-2 years before full deployment and therefore a critical mass can be built.
"the permanent centre is located outside the disaster area. the permanent centre constitutes a new element in the architecture of support systems for disaster emergencies and is unique to the deltass system. in conventional set-ups the mobile teams at the disaster site are coordinated and supported by the staff of a mobile field hospital deployed at or close to the disaster site. however, complete deployment of such a mobile field hospital takes at least ~6 hours, usually ~12 hours, and consequently the activities of mobile teams in these first, highly critical hours, are ill-coordinated and far from optimal. to improve this bottleneck, deltass has a designated permanent centre that is in control of coordination and medical support to the mobile teams from time zero on. the permanent centre is equipped with terrestrial gateways to the globalstar and inmarsat satellite systems through which it receives all data from the mobile teams. it coordinates all actions of the mobile teams and manages all medical and logistic data, thus assuring efficient operation during the first critical phase. all data received at the permanent centre are processed, appropriate reference hospitals (rh; see below) are identified and the logistic and medical data are transferred to these rh via terrestrial telecommunication links."
"satellite-based networks and services are crucial to support ubiquitous access to healthcare. appropriate enabling technologies must be deployed and interconnected via networks with an appropriate design. in this chapter we have presented wotesa/winvicos as a flexible high-end module for real-time interactive telemedical services. besides video communication in medically expedient quality, the provision of interactivity for the remote control of medical equipment is indispensable. both video communication and interactivity require a (nearly) real-time mode of bi-directional interactions. various examples have been given of particular networks and services that have been deployed, each to support medical telepresence in specific functional scenarios (galenos, deltass medaship and emispher). however, despite substantial improvements that have been realised, these developments bear the risk of creating and amplifying digital divides in the world. to avoid and counteract this risk and to fulfill the promise of telemedicine, namely ubiquitous access to high-level healthcare for everyone, anytime, anywhere (so-called ubiquitous healthcare or u-health) a real integration of both the various platforms (providing the \"quality-ofservice\", qos) and the various services (providing the \"class-of-service\", cos) is required [cit] b; [cit] . a virtual combination of applications serves as the basic concept for the virtualisation of hospitals. virtualisation of hospitals supports the creation of ubiquitous organisations for healthcare, which amplify the attributes of physical organisations by extending its power and reach. instead of people having to come to the physical hospital for information and services the virtual hospital comes to them whenever they need it. the creation of virtual hospitals (vh) can bring us closer to the ultimate target of u-health [cit] b) . the methodologies of vh should be medical-needs-driven, rather than technology-driven. moreover, they should also supply new management tools for virtual medical communities (e.g. to support trust-building in virtual communities). vh provide a modular architecture for integration of different telemedical solutions in one platform (see fig. 10 ). due to the distributed character of vh, data, computing resources as well as the need for these are distributed over many sites in the virtual hospital. therefore, grid infrastructures and services become useful for successful deployment of services like acquisition and processing of medical images (3d patient models), data storage, archiving and retrieval, as well as data mining, especially for evidence-based medicine [cit] c) . the possibility to get support from external experts, the improvement of the precision of the medical treatment by means of a real medical telepresence, as well as online documentation and hence improved analysis of the available data of a patient, all contribute to an improvement in treatment and care of patients in all circumstances, thus supporting our progress from e-health and telemedicine towards real u-health. the technologies of vh (providing the \"quality-of-service\", qos) like satellite-terrestrial links, grid technologies, etc. will be implemented as a transparent layer, so that the various user groups can access a variety of services (providing the \"class-of-service\", cos) such as expert advice, e-learning, etc. on top of it, not bothering with the technological details and constraints."
the s-video output of the us equipment is directly connected to the osprey video capture board. satellite transmission tests have shown that not only still images can be transferred but also live ultrasound investigations can be transmitted at 500-700 kbps (see fig. 6 ).
"emispher is dedicated to establish an equal access for most of the countries of the euromediterranean area to real-time and on-line services for healthcare in the required quality of service (see www.emispher.org). in the project an integrated internet-satellite platform has been set up on which three main areas of work have been realised: virtual medical university, real-time telemedicine, and medical assistance [cit] . the platform includes a bi-directional satellite network (up to 2 mbps) between 10 centres of excellence in the euro-mediterranean region (morocco, algeria, tunisia, egypt, cyprus, turkey, greece, italy, france and germany; see fig. 7 ). for dissemination of the achieved results and for maximising its impact, emispher has organised international conferences at each of the mediterranean partner sites."
"the third field of service operated in emispher is medical assistance. as tourism constitutes a substantial economical factor in the mediterranean region and because of the increasing mobility of the population, continuity of care through improved medical assistance is of major importance for improved healthcare in the euro-mediterranean region. introduction of standardised procedures, integration of the platform with the various local communication systems and training of the medical and non-medical staff involved in the medical assistance chain allow for shared management of files related to medical assistance (medical images, diagnosis, workflow, financial management, etc.) and thus for improved care for travellers and expatriates."
emispher has set up a satellite-based network using the combined wotesa and winvicos modules for real-time telemedicine. in the field of real-time telemedicine the following www.intechopen.com
the ecg system is connected to wotesa on board the ship and can be controlled by the physician from this workstation. via application sharing software also the expert can control the ecg system from the land-based workstation. the main menu that includes all the functions of the ecg as well as the patient's ecg is transmitted to the expert. thus the expert and the physician on board can jointly acquire and analyse the ecg report.
"two types of evaluation were performed for the emispher satellite-internet platform and the services delivered from the platform. firstly, the evaluation of all technical aspects of the platform and connecting services (winvicos and medsky) and secondly the evaluation of the operational elements of the system meeting the medical user requirements."
"the main objective of the service developed by the medaship project is to supply integrated solutions for medical consultations on-board of ships [cit] b) . the satellite-based telemedicine services address both passenger ships and merchant vessels and are intended to provide passengers and crew members with an effective medical assistance in cases of emergency and in all those cases where the on board medical staff requires second opinion. during the validation phase the service was tested on board of three ships with the possibility to have it connected to three land medical centres. (fig. 5) . in addition to the standard medical equipment aboard the ships, two video cameras, an electrocardiograph (ecg) and an ultrasound (us) equipment are used. with this equipment the following telemedical services have been realised using satellite transmission at a bandwidth of 512 kbps up to 1 mbps offering the required high quality of images and video transmission:"
"although the market is promising and the technology ready (or nearly) to be used, the takeup and commercialisation of the telemedicine services is still uncertain due to a number of barriers: external  decision-making in health is fragmented with respect to procurement policies, which hinders progression towards real integration."
"the costs for emergency interventions for removing a passenger from the ship and hospitalisation abroad are not to be undervalued. the removal of a passenger in the carribean can cost up to $ 11.000 and the cost for hospitalisation can range from 500-1000 € per day. consequently, market trends force passenger shipping lines to offer services that help to improve the response to on-board clinical emergencies, improve the customer satisfaction and the companies' image."
 awareness of telemedicine and tele-health as integral part of medical practice;  the need for common standards and policies;  the need for specific legislation.
"categories of applications are offered: second opinion, teleteaching & teletraining (demonstration and spread of new techniques), telementoring (enhancement of staff qualification), and undergraduate teaching courses and optimisation of the learning curve. the leading medical centres in the project provide expertise in the following medical fields: open and minimally-invasive surgery, multi-organ transplantation, endoscopy, pathology, radiology, interventional imaging, neurology, infectious diseases, oncology, gynaecology and obstetrics, reproductive medicine, etc. these real-time telemedical applications contribute to improved quality of patient care and to accelerated qualification of medical doctors in their respective specialty. the main target audience are specialist doctors (fig. 9) . fig. 9 . interactive multipoint radiological teleconsultation in emispher. experts from different centres interactively discuss radiology results from a particular patient case."
"in cooperation with: eutelsat (f), nortel dasa, (d), alcatel space industries (f), telespazio (i), medsat (f), ncsr demokritos (gr); financially supported by the european union (eu). in the galenos project a european competence network via satellite, dedicated to telemedical applications, has been realised [cit] ). owing to the participation of industrial partners in integrating the communication network and getting satellite transmission capacity available, several telemedical services (e.g. offline access to archived data, live consultation of experts, tele-teaching, etc.) have become available in a unified and low-cost technology. this possibility to get support from external experts, the improvement of the precision of the surgical treatment, and last but not least, online documentation and hence improved analysis of the available data on a patient, contributes to a continuous improvement in treatment and care of patients. the trans-european communication network over satellite provides various collaborative services and applications, avoiding the use of specific technologies, thus promoting a wide availability. furthermore, the network exhibits inter-operability with previously existing telecommunication networks. the network offers up to 2 mbit/s interfaces with satellite link and covers a total of 14 clinics in bulgaria, france, germany, greece, italy and tunisia. access to the local area network (lan) at each site is also provided for the exchange of patient data, medical images, videoconferencing, etc. substantial reactivity and technical flexibility allow treatment protocols, adapted to the patient's pathology, in the shortest time. the galenos network enables physicians not only to receive virtual \"classical\" hands-on medical training, but also trains them in using the state-of-the-art communication-, videoand computer-technologies needed for collaborative work in a network of distributed medical intelligence (interactive teleteaching). such distributed medical intelligence in the competence network enables e.g. local physicians or local hospitals that are confronted with unexpected test results to get online advice from the nearest academic hospital (intraoperative radiological teleconsultation). it also becomes possible to perform intraoperative telepathology where, after a biopsy has been taken and a slice has been prepared, this sample is put under a remotely-controllable, camera-equipped microscope and the diagnosis is then formulated by a remote expert. or intraoperative teleradiology, where anytime during the operation, the surgeon can access all the preoperative radiological data (like x-ray, ct, mrt, etc.) and discuss it with the radiologist in real-time. in the deltass-project [cit] a disaster scenario was analysed and an appropriate telecommunication system for effective rescue measures for the victims was set up and evaluated. satellite-based systems are well suited for these circumstances, where generally ground infrastructures are partly or even totally destroyed. in such situations, even on a large geographic area or isolated area, space-based services can be easily and quickly deployed. deltass demonstrates operational performance of various services, covering the different aspects/phases of disaster emergency medicine. according to these phases, the deltass system is made up of the various corresponding subsystems ( fig. 2"
the live camera on-board of the ship can be used to transmit the image of the doctor who is leading the examination on-board of the ship or the image of the patient when being questioned by the land-based expert. it can also be used to show the land-based expert an www.intechopen.com
"in various pilot projects, the technology has been put in place, the necessary applications have been developed and it had been proved that it can be used successfully and meet the needs of its end users. yet numerous trials and demonstrations carried out during the projects have also highlighted a certain number of issues that could potentially hinder the commercialisation process of e-services in the medical sector. in fact, various market analysis reports prepared for the european commission also insist on the fact that the telemedicine market growth will be dependent upon a number of vital conditions and enabling factors already noted above. it is generally accepted that four key action lines should be initiated in parallel: consolidation on the supply side, technical integration, investment on the demand side and \"accompanying measures\" that could be the enabling factors required to allow the health telematics market to achieve substantial and exponential growth."
"a mobile field hospital (mfh), which will be deployed at or close to the disaster site, provides all activities related to the co-ordination of the mobile teams on the disaster site, the victims' medical triage, reception, first aid treatment, conditioning for transportation, further medical expertise for some patients by teleconsultations between mfh and reference hospital(s)."
"the solutions to such 'barriers' to commercialisation do not lie within the scope of the technology and appear to be generic to those 'up and running' tele-health activities worldwide. it is only in north america that new legislation has been introduced to respond to the particular needs of this practice and this legislation are not yet being applied on a federal level but rather on a state by state basis. however, the organisational and cultural aspects that must accompany any new form of practice need the input of other actors both on the governmental, legal and political scale. tele-health is not a simple extension of current health systems and cannot be perceived as such. solutions to the \"barriers\" can be consolidated under the following three main categories:"
"here we demonstrate that the learnt model can indeed capture face-specific features. to visualise the projection matrix w, we make use of the fact that each gmm component corresponds to a part of the fisher vector and, in turn, to a group of columns in w . this makes it possible to evaluate how important certain gaussians are for comparing human face images by computing the energy (euclidean norm) of the corresponding column group. in fig. 2 we show the gmm components which correspond to the groups of columns with the highest and lowest energy. each gaussian captures joint appearance-location statistics (sect. 2), but here we only visualise the location as an ellipse with the centre and radii set to the mean and variances of the spatial components. as can be seen from fig. 2-d, the 50 gaussians corresponding to the columns with the highest energy match the facial features without being explicitly trained to do so. they have small spatial variances and are finely localised on the image plane. on the contrary, fig. 2 -e shows how the 50 gaussians corresponding to the columns with the lowest energy cover the background areas. these clusters are deemed as the least meaningful by our projection learning; note that their spatial variances are large."
"our results in both unrestricted and restricted settings confirm that the proposed face descriptor can be used in both small-scale and large-scale learning scenarios, and is robust with respect to the face alignment and cropping technique."
"diagonal \"metric\" learning. apart from the low-rank mahalanobis metric learning (sect. 3), we also consider diagonal metric learning on the full-dimensional fisher vectors. it is carried out using a conventional linear svm formulation, where features are the vectors of squared differences between the corresponding components of the two compared fvs. we did not observe any improvement by enforcing the positivity of the learnt weights, so it was omitted in practice (i.e. the learnt function is not strictly a metric)."
"it is easy to verify that the above construction is in ptime. we next verify that this is indeed a reduction from the x3c instance, i.e., there is a 1-1 p-hom mapping from g1 to g2 if and only if there is an exact cover for the x3c instance."
"in addition to dsp at the receiver side, much work has been done in the design of suitable modulators at the transmitter side. recently, many experimental studies have been focusing on the 16-ary quadrature amplitude modulation (16-qam) format to increase the spectral efficiency of the system since each of its symbols carries 4 bits. in order to analyze the performance of a coherent optical system in a laboratory setting, the transmitter can be realized in several different ways, including using i) multi-level driving signals and a single i/q modulator (iqm) [cit], ii) an integrated structure with several optical modulators in parallel and binary electrical signals [cit], or iii) a cascade of more than one optical modulator [cit] . in the setup for an experimental study, we chose to work according to the first suggestion and, for hardware simplicity, a single pseudorandom binary sequence (prbs) was used. combining delayed copies of this signal, a 16-qam signal could be generated. however, for some chosen values of the delays, we noticed that the cma failed to converge, which lead us to investigate the phenomenon more closely."
"in this section we explain how a high-dimensional fv encoding (sect. 2) is compressed to a small discriminative representation. the compression is carried out using a linear projection, which serves two purposes: (i) it dramatically reduces the dimensionality of the face descriptors, making them applicable to large-scale datasets; and (ii) it improves the recognition performance by projection onto a subspace with a discriminative euclidean distance."
"the hardness is verified by a certain reduction from the maximum weighted independent set problem (wis). in a graph, an independent set is a set of mutually non-adjacent nodes. given a graph with a positive weight associated with each node, wis is to find an independent set such that the sum of the weights of the nodes in the set is maximum. it is known that wis is np-complete, and is hard to approximate: it is not approximable within o(1/n 1−ǫ ) for any constant ǫ, where n is the number of nodes [cit] ."
"approximation hardness. in light of corollary 4.2, the best we can hope for are efficient heuristic algorithms for finding (1-1) phom mappings, with performance guarantees on match quality. unfortunately, cph, cph 1−1, sph and sph 1−1 are all hard to approximate. indeed, there exist no ptime algorithms for finding (1-1) p-hom mappings such that the quality of each mapping found is guaranteed to be within o(1/n 1−ǫ ) of its optimal counterpart. are not approximable within o(1/n 1−ǫ ) for any constant ǫ, where n is the number of nodes in g1 of input graphs g1 and g2."
"face identification, i.e. the problem of inferring the identity of people from pictures of their face, is a key area of research in image understanding. beyond its scientific interest, this problem has numerous and important applications in surveillance, access control, and search. automatic face verification (afv) is a formulation of the face identification problem where the task is to determine whether two images depict the same person or not. in the past few years, the dataset \"labeled faces in the wild\" (lfw) [cit] has become the de-facto evaluation benchmark for afv, promoting the rapid development of new and significantly improved afv methods. recent efforts, in particular, have focused on developing new image representations and combination of features specific to afv to surpass standard representations such as sift [cit] . the question that this paper addresses is what happens if, instead of developing yet another face-specific image representation, one applies off-the-shelf object recognition representations to afv."
"it is easy to verify that the above construction is in ptime. we next verify that this is indeed a reduction from the 3sat instance, i.e., there is a p-hom mapping from g1 to g2 if and only if the 3sat instance φ is satisfiable."
"for gp and g of fig. 1, the p-hom mapping given in example 3.1 is also a 1-1 p-hom mapping, i.e., gp 1−1 (e,p) g. as another example, consider g1 and g2 of fig. 2 . while g1 (e,p) g2, g1 1−1 (e,p) g2. in particular, the p-hom mapping given in example 3.1 is not injective, since it maps both a nodes in g1 to the same a node in g2. similarly, while g5 (e,p) g6, g5 1−1 (e,p) g6 as a p-hom mapping has to map both b nodes in g5 to the b node in g6, which is not allowed by a 1-1 mapping."
"(b) for each truth assignment ρ of xp j1, xp j2 and xp j3 that makes cj true, e ′ 2 consists of the following edges:"
"two evaluation measures are considered. the first one is the receiving operating characteristic equal error rate (roc-eer), i.e. the false positive rate at the roc operating point where the false positive and false negative rates are equal [cit] . this measure reflects the quality of the ranking obtained by scoring image pairs and, as such, is independent on the bias learnt in (2) . roc-eer is used to compare the different stages of the proposed framework. in order to allow a direct comparison with published results, however, our final classification performance is also reported in terms of the classification accuracy (percentage of image pairs correctly classified) -in this case the bias is important."
"putting all these together, we have shown that compmaxcard indeed simulates isremoval, i.e., given the same input, they always produce the the same output. 2"
"(1) real-life data. the real-life data was taken from the stanford webbase project [cit], in three categories: web sites for online stores, international organizations and online newspapers, denoted by sites 1, 2 and 3, respectively. for each web site, we found an archive that maintained different versions of the same site."
"on the other hand, larger values of ∆k 1 and ∆k 2 lead to lower ser, as a result of equalizer convergence. exceptions to this are when"
"on all graphs in skeletons 1, cdkmcs did not run to completion. while compmaxcard and compmaxsim found more than 50% of matches, sf found no more than 40%. on skeletons 2, all of our algorithms found more matches than cdkmcs. in particular, on site 3 cdkmcs found no matches at all. in contrast, our algorithms found up to 60% of matches on the same data. compared with sf, all of our algorithms performed better on sites 1 and 2, whereas sf did better on site 3. however, when the size of web sites increased, the performance of sf deteriorated rapidly."
"to show the approximation bound, we need to use approximation factor preserving reduction (afp-reduction) [cit] . let π1 and π2 be two maximization problems. an afp-reduction from π1 to π2 is a pair of ptime functions (f, g) such that"
"consider graphs g5 and g6 shown in fig. 2 . there are two nodes labeled b in g1, indicated by v1 and v2, respectively. a similarity matrix mat0() is given as follows:"
"(b) procedure trimmatching (fig. 4) algorithm compmaxcard can be readily converted to approximation algorithms for cph 1−1, sph and sph 1−1, as follows."
"figure 5(c) shows that the accuracy of our approximation algorithms is not very sensitive to ξ, with accuracy above 70% in all the cases. when ξ is between 0.6 and 0.8, the accuracy is relatively lower. this is because (a) when ξ is low ([0.5, 0.6]), it is relatively easy for a node in g1 to find its matching nodes in g2; (b) when ξ is high (above 0.8), the chances for each node in g1 to find its copy in g2 are higher, by the construction of g2. figure 6 (c) tells us that the scalability of all these algorithms is indifferent to ξ."
"we note that other modulation formats can be generated in a similar way, using either more delays (e.g., 3 delays for 64-qam) or fewer delays (e.g., a single delay for qpsk)."
"(1) we introduce p-homomorphism (p-hom) and 1-1 p-hom in section 3. these notions extend graph homomorphism and subgraph isomorphism, respectively, by (a) incorporates similarity metrics to measure the similarity of nodes, as opposed to node label equality; and (b) mapping edges in a graph to paths in another, rather than edge-to-edge mappings. in contrast to previous extensions, one can use node similarity to assure, e.g., that two web pages are matched only when they have similar contents [cit] or play a similar role (as a hub or authority [cit] ). edge-to-path mappings allow us to match graphs that have similar navigational structures but cannot be identified by the conventional notions of graph matching. in addition, these notions can be readily extended to deciding whether two graphs are similar to each other in a symmetric fashion."
"from the above, it is rather difficult to generalize the choice of the delays which guarantees cma convergence. but to gain a deeper insight into how the different values of ∆k 1 and ∆k 2 affect the correlation between data symbols, we have examined the autocorrelation of the 16-qam signal using a single infinitely long i.i.d. bit sequence. the autocorrelation is obtained as"
"where w(nt s ) are i.i.d. complex awgn samples with variance n 0, and h(t) is the overall equivalent electrical channel. the goal of the blind equalizer is to recover the data symbols without knowledge of h(t). cma is a well-known blind equalizer and operates as follows. to recover symbol a k, we consider a window of observations r k of length l, where l is approximately 1/t s times the duration of the support of h(t). the equalizer output at time k is given by"
"we expect that p-hom and 1-1 p-hom will find applications in web site classification [cit], complex object identification, plagiarism [cit] and spam detection [cit], among other things."
"we propose several notions to capture graph structural similarity that encompass the previous extensions, and provide a full treatment of these notions for graph matching."
"it is easy to verify that these problems are in np. we next show that there exists a reduction from the p-hom problem to mcp (msp) for p-hom, and the reduction from the 1-1 p-hom problem to mcp (msp) for 1-1 p-hom is identical."
"restricted setting method mean acc. v1-like/mkl [cit] 0.7935 ± 0.0055 pem sift [cit] 0.8138 ± 0.0098 apem fusion [cit] 0.8408 ± 0.0120 our method 0.8747 ± 0.0149 table 2 : left: face verification accuracy in the unrestricted setting. using a single type of local features, our method achieves the new state-of-the-art among the non-commercial methods, and is competitive compared to the best commercial systems. right: face verification accuracy in the restricted setting (no outside training data). our method achieves the new state-of-the-art in this strict setting."
"a strong requirement to guarantee cma convergence is that the transmitted symbols are independent. we have shown that generating 16-qam data from a single prbs to drive the i/q modulator in laboratory experiments potentially leads to cma misconvergence, due to correlation in adjacent symbols. we have evaluated the performance of cma for various delays in a specific choice of modulator and found that by correct selection of these delays, proper operation of the cma can be achieved. however, care needs to be taken as there are specific values for the delays, dependent on the particular prbs sequence, that cause incorrect constellations to be generated. further study should be conducted to find good values for delays when generating higher-order constellation (e.g., 64-qam) from a single prbs."
"we investigated (a) the accuracy of our four algorithms, and (b) the efficiency of these algorithms and graphsimulation. we do not show the accuracy of graphsimulation as it found 0% of matches in all the cases. we evaluated the effects of the following parameters on the performance: the number of nodes m in g1, the noise ratio noise% and the node similarity threshold ξ. in each setting, the accuracy was measured by the percentage of matches found between g1 and a set of 15 graphs (g2) as mentioned above."
"when graph homomorphism or subgraph isomorphism is used to measure graph similarity, g does not match gp. indeed, (a) nodes in g may not find a node in g with the same label, e.g., audio; and worse still, (b) there exists no sensible mapping from vp to v that maps edges in gp to edges in g accordingly."
"first, we explore how the different parameters of the method affect its performance. the experiments were carried out in the unrestricted setting using unaligned lfw images and a simple alignment procedure described in sect. 4. we explore the following settings: sift density (the step between the centres of two consecutive descriptors), the number of gaussians in the gmm, the effect of spatial augmentation, dimensionality reduction, distance function, and horizontal flipping. the results of the comparison are given in table 1 . as can be seen, the performance increases with denser sampling and more clusters in the gmm. spatial augmentation boosts the performance with only a moderate increase in dimensionality (caused by the addition of the (x, y) coordinates to 64-d pca-sift). our dimensionality reduction to 128-d achieves 528-fold compression and further improves the performance. we found that using projection to higher-dimensional spaces (e.g. 256-d) does not improve the performance, which can be caused by overfitting."
"it may be too expensive to compute vertex similarity matrix on large graphs or to match those graphs. to cope with this we may use \"skeletons\" of the graphs instead, namely, subgraphs induced from \"important\" nodes such as hubs, authorities and nodes with a large degree. indeed, approximate matching is commonly accepted in practice [cit] . we compute mat() for such nodes only."
"we next present an experimental study of our matching methods in web mirror detection. using real-life and synthetic data, we conducted two sets of experiments to evaluate the ability and scalability of our methods for matching similar web sites vs. (a) conventional graph simulation [cit] and subgraph isomorphism [cit], and (b) vertex similarity based on similarity flooding [cit] ."
"we can replace each clique in g + 2 with a single node with a selfloop, whose label is the bag of all node labels in the clique. we denote the compressed graph by g * by capitalizing on bags of labels, our algorithms can be modified such that any strong (1-1) p-hom mapping they find from a subgraph of g1 to g + 2 is also a strong (1-1) p-hom mapping from a subgraph of g1 to g2, with the same quality. by compressing g2 to g + 2, the performance of the algorithms is significantly improved. the compressing process incurs little extra cost since sccs of g2 can be identified during the computation of g + 2 [cit] ."
"the experimental setup of the 16-qam modulator using a single prbs is seen in fig. 1 . the output of the pattern generator is a real continuous-time signal, b, driven by the binary prbs. this signal is added to an inverted, 6 db attenuated, and ∆k 1 symbol slots delayed copy of b. the resulting signal can take on four different values, and this electrical signal is used to generate the real part, i, of the constellation. the imaginary part, q, is constructed by delaying the i signal ∆k 2 symbol slots. these two signals are fed to the iqm, with the optical input connected to an external cavity laser (ecl), resulting in the 16-qam symbols. the output of the iqm is directly fed to the receiver."
"lfw specifies a number of evaluation protocols, two of which are considered here. in the \"restricted setting\", only the pre-defined image pairs for each of the splits (fixed by the lfw creators) can be used for training. instead, in the \"unrestricted setting\" one is given the identities of the people within each split and is allowed to form an arbitrary number, in practice much larger, of positive and negative pairs for training."
cph maximum cardinality for p-hom cph 1−1 maximum cardinality for 1-1 p-hom sph maximum overall similarity for p-hom sph 1−1 maximum overall similarity for 1-1 p-hom
mean acc. ldml-mknn [cit] 0.8750 ± 0.0040 combined multishot [cit] 0.8950 ± 0.0051 combined plda [cit] 0.9007 ± 0.0051 face.com [cit] 0.9130 ± 0.0030 cmd + slbp [cit] 0.9258 ± 0.0136 lbp multishot [cit] 0.8517 ± 0.0061 lbp plda [cit] 0.8733 ± 0.0055 slbp [cit] 0.9000 ± 0.0133 cmd [cit] 0.9170 ± 0.0110 high-dim sift [cit] 0.9177 ± n/a high-dim lbp [cit] 0.9318 ± 0.0107 our method 0.9303 ± 0.0105
"(4) nevertheless, we provide in section 5 approximation algorithms for finding mappings with the maximum cardinality or the maximum similarity, for p-hom and 1-1 p-hom. these algorithms possess performance guarantees on match quality: for any graphs g1 and g2, the solutions found by the algorithms are provable to be within a polynomial o(log 2 (n1n2)/(n1n2)) of the optimal solutions, where n1 (resp. n2) is the number of nodes in g1 (resp. g2)."
"from the results we can see the following: our algorithms (1) perform well on both the accuracy and efficiency on different types of web sites, (2) find more matches than cdkmcs and sf, and (3) are much more efficient and robust than the other two methods."
"referring the x3c instance given above, the dag g2 is shown in fig. 8 (g2) . (3) the similarity matrix mat() is defined as follows:"
"our framework is evaluated on the popular \"labeled faces in the wild dataset\" (lfw) [cit] . the dataset contains 13233 images of 5749 people downloaded from the web and is considered the de-facto standard benchmark for automatic face verification. for evaluation, the data is divided into 10 disjoint splits, which contain different identities and come with a list of 600 pre-defined image pairs for evaluation (as well as training as explained below). of these, 300 are \"positive\" pairs portraying the same person and the remaining 300 are \"negative\" pairs portraying different people."
"we follow the recommended evaluation procedure [cit] and measure the performance of our method by performing a 10 fold cross validation, training the model on 9 splits, and testing it on the remaining split. all aspects of our method that involve learning, including pca projections for sift, gaussian mixture models, and the discriminative fisher vector projections, were trained independently for each fold."
"(e,p) g2. these problems are already np-hard when both g1 and g2 are acyclic directed graphs (dags). it is np-hard for 1-1 p-hom when g1 is a tree and g2 is a dag. 2"
"the remainder of the paper is organized as follows. section ii presents the system model where we describe how the multi-level signalling generation is done, the receiver 978-1-4577-0454-3/11/$26.00 ©2011 ieee the 16-qam modulator setup using a single prbs. the tunable delays, marked ∆t, were set to an integer number of symbol slots. these are denoted by ∆k1 and ∆k2, respectively. used, and how the cma algorithm works. in section iii, we analyze the performance of the system when a prbs sequence is used to generate the 16-qam signal, as well as in the presence of an independent and identically distributed sequence. then the autocorrelation properties of the sequence generated is presented where conclusions are drawn to avoid the misconvergence of cma."
"it should be noted that the proposed system is based upon a single feature type. in our future work, we are planning to investigate multi-feature image representations, which can be readily incorporated into our framework."
"(2) the p-hom algorithms found more matches than the 1-1 p-hom ones since the latter pose stronger requirements than the former. (3) all algorithms found more matches on sites 1 and 2 than site 3 since a typical feature of site 3 (online news papers) is its timeliness, reflected by the rapid changing of its contents and structures."
"(2) the matching-lists h + and h − correspond to n ([v, u] ) and n ([v, u]), respectively, where nodes v, u come from line 2 of greedymatch. since computing the neighbors or non-neighbors of a node on graphs is trivial, it is not explicitly addressed in ramsey. in greedymatch, however, we need to distinguish neighbors from non-neighbors in the matching-list h, instead of the product graph directly. procedure trimmatching in fig. 4 is thus introduced to solve this problem. indeed, it is trimmatching that makes it possible to operate on the product graph directly."
k . the encoding describes how the distribution of features of a particular image differs from the distribution fitted to the features of all training images.
"these highlight the need for revising the conventional notions of graph matching. in response to these, several extensions of the conventional notions have been studied for graph matching [cit] . however, a formal analysis of these extensions is not yet in place, from complexity bounds to approximation algorithms."
"initially, the experimental investigation focused on the backto-back performance, from which we identified the overall channel impulse response, which lasted around 2 symbol slots. the received signal was oversampled at 10 samples per symbol and processed offline. offline processing consisted of cma equalization, downsampling to the symbol rate, carrier phase compensation, frame synchronization, and data detection. an 80 taps cma equalizer (corresponding to 8 symbols) was used, while data detection was performed according to the maximum likelihood criterion."
"we next develop more efficient algorithms that operate directly on the input graphs instead of on their product graph, retaining the same approximation bound. we first present an algorithm for cph, and then extend the algorithm to cph 1−1, sph and sph 1−1 ."
"from the experimental results we find the following. (a) the notions of (1-1) p-hom are able to identify a large number of similar web sites that are not matched by graph simulation, subgraph isomorphism and vertex similarity. on a set of organization sites, the accuracy of all of our algorithms is above 80%, as opposed to 0%, 0% and 30% by graphsimulation, cdkmcs and sf, respectively. (b) our algorithms scale well with the sizes of the graphs, noise rates, and similarity threshold. they seldom demonstrated their worst-case complexity. even for g1 of 800 nodes and g2 [cit] nodes, all of our algorithms took less than two minutes."
"we have proposed several notions for capturing graph similarity, namely, p-hom, 1-1 p-hom, and quantitative metrics by maximizing either the number of nodes matched or the overall similarity. these notions support edge-to-path mappings and node similarity. we have established the intractability and the hardness to approximate for these problems. despite the hardness, we have developed approximation algorithms for these problems, with provable guarantees on match quality. we have verified the effectiveness of our techniques using web site matching as a testbed. our experimental results have shown our methods are able to identify a number of similar web sites that cannot be matched either by the conventional notions of graph matching or by vertex similarity alone."
"the minimiser of (4) is found using a stochastic sub-gradient method. at each iteration t, the algorithm samples a single pair of face images (i, j) (sampling with equal frequency positive and negative labels y i j ) and performs the following update of the projection matrix:"
"t is the outer product of the difference vectors, and γ is a constant learning rate, determined on the validation set. note that the projection matrix w t is left unchanged if the constraint (2) is not violated, which speed-ups learning (due to the large size of w, performing matrix operations at each iteration is costly). we choose not to regularise w explicitly; rather, the algorithm stops after a fixed number of learning iterations (1m in our case). finally, note that the objective (4) is not convex in w, so initialisation is important. in practice, we initialise w to extract the p largest pca dimensions. furthermore, differently from standard pca, we equalise the magnitude of the dominant eigenvalues (whitening) as the less frequent modes of variation tend to be amongst the most discriminative. it is important to note that pca-whitening is only used to initialise the learning process, and the learnt metric substantially improves over its initialisation (sect. 5). in particular, this is not the same as learning a metric on the low-dimensional pca-whitened data (p 2 parameters); instead, a projection w on the original descriptors is learnt (pd p 2 parameters), which allows us to fully exploit the available supervision."
"given an instance φ of the 3sat problem, we construct two dags g1, g2 and a similarity matrix mat() such that g1 (e,p) g2 if and only if φ is satisfiable. the similarity threshold ξ is set to 1."
"since each set of the graphs represents different versions (snapshots) of the same web site, they should match each other. based on this, we evaluated the accuracy of our algorithms. more specifically, after ta was generated, we sorted the 11 graphs based on their timestamp to get a web graph sequence [cit] . we treated the oldest one as pattern g1, and tested whether various approaches could match the 10 later versions to g1. we used the percentage of matches found as the accuracy measure for all the algorithms."
"face descriptor computation. for dense sift computation and fisher vector encoding, we utilised publicly available packages [cit] . dimensionality reduction learning is implemented in matlab and takes a few hours to compute on a single core (for each split). given an aligned and cropped face image, our mexified matlab implementation takes 0.6s to compute a descriptor on a single cpu core (in the case of 2 pixel sift density)."
"the algorithm works as follows. it first constructs the adjacency list h1 and the matching list h for g1 (lines 1-4, fig. 3), where"
"in this paper, we have shown that an off-the-shelf image representation based on dense sift features and fisher vector encoding achieves state-of-the-art performance on the challenging \"labeled faces in the wild\" dataset. the use of dense features allowed us to avoid applying a large number of sophisticated face landmark detectors. also, we have presented a largemargin dimensionality reduction framework, well suited for high-dimensional fisher vector representations. as a result, we obtain an effective and efficient face descriptor computation pipeline, which can be readily applied to large-scale face image repositories."
"these conventional notions are, however, often too restrictive for graph matching in emerging applications. in a nutshell, graph matching is to decide whether a graph g matches another graph gp, i.e., whether g has a structure similar to that of gp, although not necessarily identical. the need for this is evident in, e.g., web anomaly detection [cit], search result classification [cit], plagiarism detection [cit] and spam detection [cit] . in these contexts, identical label matching is often an overkill, and edge-to-edge mappings only allow strikingly similar graphs to be matched."
"our algorithms took less than 4 seconds in all these cases, while cdkmcs took 180 seconds even for graphs with only 20 nodes. note that although sites 2 and 3 are about the same size, the running times of cdkmcs on them are not comparable. while the running time of sf was comparable to our algorithms on small web sites (skeleton 2), it took much longer on large sites (skeleton 1)."
".minus for each node v ′ in h other than v. the extra step changes neither the worst-case complexity nor the performance guarantee of compmaxcard. this yields an approximation algorithm for cph 1−1, referred to as compmaxcard 1−1 ."
"in this work, we report the observation of cma misconvergence for a specific choice of the 16-qam modulator. in this way, we identify a potential pitfall when generating a multi-level modulation format from correlated data streams. by close investigation of the problem and the autocorrelation properties of the generated symbols, we propose a way to avoid misconvergence of the cma equalizer."
"no matter how desirable, it is intractable to determine whether a graph is p-hom or 1-1 p-hom to another. we remark that while graph homomorphism is special case of p-hom, there is no immediate reduction from the former to the latter, and vice versa; similarly for subgraph isomorphism and 1-1 p-hom."
"despite theorem 4.3, we next provide approximation algorithms for each of the maximum cardinality problems (cph, cph 1−1 ) and the maximum overall similarity problems (sph, sph 1−1 ). optimization techniques are presented in appendix b."
"fisher vectors. the fv encoding aggregates a large set of vectors (e.g. the dense sift features just extracted) into a high-dimensional vector representation. in general, this is done by fitting a parametric generative model, e.g. the gaussian mixture model (gmm), to the features, and then encoding the derivatives of the log-likelihood of the model with respect to its parameters [cit] . following [cit], we train a gmm with diagonal covariances, and only consider the derivatives with respect to the gaussian mean and variances. this leads to the representation which captures the average first and second order differences between the (dense) features and each of the gmm centres:"
"as far as the choice of the fv distance function is concerned, a low-rank mahalanobis metric outperforms both full-rank diagonal metric and unsupervised pca-whitening, but is somewhat worse than the function obtained by the joint large-margin learning of the mahalanobis metric and inner product. it should be noted that the latter comes at the cost of slower learning and the necessity to keep two projection matrices instead of one. finally, using horizontal flipping consistently improves the performance. in terms or the roc-eer measure, our best result is 93.13%."
"(5) using web site matching as a testbed, we experimentally evaluate our similarity measures in section 6. we compare p-hom and 1-1 p-hom with three other methods: graph simulation [cit], subgraph isomorphism [cit] and vertex similarity matrix [cit] . using real-life web sites and synthetic graphs, we show that our methods outperform those three methods in both match quality and efficiency."
", where s xy is the (pca-sift) descriptor of a patch centred at (x, y), and w and h are the width and height of the face image. the resulting fv dimensionality is thus 67584. fig. 2 illustrates how gaussian mixture components are spatially distributed over a face when learnt for a face verification task."
"(1) g1 (e,p) g2. a p-hom mapping is defined by mapping both a nodes in g1 to the a node in g2, the node b in g1 to the b node in g2, and the node c in g1 to any of the two c nodes in g2."
"remark. for g1 (e,p) g2 (g1 1−1 (e,p) g2) we require an edgeto-path mapping from g1 to g2 when g1 is a pattern for a data graph g2 to match. nevertheless, (1-1) p-hom can be readily made symmetric that maps paths between g1 and g2. indeed, one only need to compute g"
"horizontal flipping. following [cit], we considered the augmentation of the test set by taking the horizontal reflections of the two compared images, and averaging the distances between the four possible combinations of the original and reflected images."
"using the web data we generated our graphs as follows. we randomly chose a web site a in each category. we then produced a set ta of web graphs, using data from the archive for a. in each graph, each node was labeled with the content of the page. the similarity between two nodes was measured by the textual similarity of their contents based on shingles [cit] ."
"rapidly growing industry of graphics processing units (gpus) driven by fast-growing video game market provides new dimension in computational resources: a current example is the nvidia tesla k40 that can reach about 5 trillion floating-point operations per second, while a new released intel core i7-5960k (xenon haswell) processor can only reach 350 billion floating-point operations per second. this makes gpu very attractive for scientific computation [cit] and many traditional scientific methods including md, mc, finite element analysis are adapting for gpu. as a result, gpu versions of the codes are accelerated by factors from 10 to 100 compared to single core cpus [cit] . 1 nevertheless, the adapted parallel versions of traditional methods cannot use full advantage of gpu architecture, because they were designed conceptually for sequential implementation and thus, contain large portions of nonparallilizible parts of the code and inter-connections that require communication between the cores, for example, to update the list of nearest neighbors. thus, there is a need in the development of new methods that are specially designed for modern highly parallel architecture."
"in order to compare all methods under equal conditions, as well as present a more challenging and realistic scenarios, we perform a cross dataset experiment. specifically, an allagainst-one strategy is adopted for our method and hpe, where a model using all images in four standard datasets except one are used for training and the fifth dataset is used for testing. this is repeated generating a different model for all possible combinations. this experiment also aims to validate the previous conclusions and results and ensure that the validity of our approach is not the result of overfitting. the pretrained approaches drmf and openface are added in the comparison using the same test sets but this time the comparison is fair (even if the training sets are different) since no methods have seen the testing type of images. table 3 and 4 shows the average result over the iterations as well as dispersion for the cases that the full image or the face crops are used, respectively. it can be notice how all reported errors for hpe and our method increases due to the most challenging problem, getting closer results to the pretrained methods. for the full image experiment, our method still reports the best results in all cases. for the face crop experiment, results are not so clear and hpe provides in many cases similar or better results. however, our approach is still providing the best pitch estimation without having a significantly lower yaw estimation, and without restrictions regarding the training data capture, such as the number of classes or the angular resolution."
"the rest of the paper is organized as follows: section ii explains the procedure of the moving average control chart. in section iii the performance of the proposed chart with respect to the arl values of different shift levels have been discussed. the simulation study is given in section iv. a real example is given in section v. in the end, the concluding remarks and the future suggestions have been given."
"in a second ablation experiment, the discriminant subspace is generated using the well-known linear discriminant analysis (lda) [cit] instead of our proposed gdcv. results in table 7 indicate that gdcv is a technique better suited for the head pose estimation problem, able to produce a more discriminative embedding space."
"the paper is organized as follows. after brief description of static mc methods in section 2, we describe the gpu implementation of rosenbluth sampling method in section 3. comparative examples between serial and parallel implementation of the rosenbluth method for polymer chains up to 64 monomers are presented in section 4. we summarize our results in section 5."
"once the hog features (x hog ) are calculated, our proposed system aims to find a linear mapping or projection onto a feature manifold where the correspondence between the input image and their angular pose is easier to be estimated than in the original space. while many dimensionality reduction methods such as principal component analysis (pca) [cit] or linear discriminant analysis (lda) [cit] can be applied to calculate this embedding (see section iii-c), two particular characteristics of the head pose estimation problem should be taken into consideration. first, poses corresponding to the same or very close angles should be kept together after the projection, while poses with very different angles should be separate as much as possible, in order to achieve an effective regression later. second, the method should be able to cope with the large dimensionality of the face image data (and the even larger hog feature dimensionality) in comparison with the available number of samples in the training set, which leads to the well-known small sample size (sss) problem [cit] that produces singular matrices during computation."
"both drmf and openface methods provide very poor result, some of which can be considered almost random, since they have not been trained on the testing image types. this is therefore a not fair or conclusive comparison but they have been added here to illustrate the difficulty of generate useful models in real life applications, as well as the limitations of current methods. next subsection makes emphasis in this problem and fair comparison. volume 6, 2018 table 2. average error in degrees for the intra-dataset experiment using the face crop as input."
"in order to justify and validate our pipeline, we repeat the previous cross-dataset experiment in table 5 but removing or replacing with conventional approaches some of the modules in our pipeline."
the extension of the null space of s x w (which implies restricting the corresponding range space) is done from the eigendecomposition of s x w .
"1) the proposed control statistic is plotted and the run length is computed for a specified a and k. 2) the step 1 is repeated 10000 times and arl is computed. if the the computed arl is equal to the specified value, stop. otherwise, step 1 and step 2 are repeated again with different choice of a and k. 3) using the determined a and k for a shift process the proposed control statistics are computed and plotted against the same limits as of in control process. 4) the arl is computed for the shifted process."
"in this experiment, the method is trained and tested in different partitions of the same dataset. specifically, all five standard datasets are used, and a model for each dataset under consideration is generated (training) with the 50% of the samples and tested with the remain 50%, cross validation is applied as evaluation protocol to avoid bias to a particular training/testing split, where each experiment is run 10 times with different random training/testing sample choices. our approach is compared against hpe in the same training/testing setup. drmf and openface are also added to the comparison for reference according to their results in the testing partitions, but using the best trained model provided by the authors."
"several relevant conclusions can be achieved from these results. first, our proposed approach provides the best results in all cases for both possible inputs, improving greatly the next best result, hpe. as it could be expected, since less distracters are present in the image, all methods behave significantly better when using only the face crop (except in a couple of cases whose difference is not significant). however, this assumes that a face detector/segmentation algorithm is available with almost perfect performance, which is challenging and unrealistic in real-life scenarios. in these situation, it may be easier to provide the full head image. while hpe increases the error between 0 and 37 degrees for the yaw and up to 25 degrees for the pitch, depending on the dataset, our approach only increases between 0 and 9 degrees for the yaw and up to 20 degrees for the pitch. it can also be noticed that pitch angle seems more difficult to be estimated, although it is likely that this is the result of having less training examples since not all datasets have images with varying pitch. an unusually large error value of the pitch in the caspeal-2 is given by our method, which is caused by the limited information contained on low resolution face crops."
"for relatively long chains, we propose data flow shown in figure 2b where all coordinates are stored in global memory instead of shared memory. although the data transfer between global memory is much slower than shared memory, global memory is larger thus can accomodate larger molecules. a detailed comparison of the efficiency 8 tables 1,2 . in such case, we find that although the bandwidth of global memory is still quite slow compared with shared memory, it will not affect the number of blocks running simultaneously. as a result, the global memory implementation will be faster for long chains that do not fit in shared memory."
"in a parallel perspective, for each block on gpu, d denotes the dimension of the space, blockdim.x is the block dimension defined by user and chainlength is the total number of conformations to be generated. with this, d arrays 3 with dimensions blockdim.x*chainlength are allocated in the shared memory to store the coordinates. same numbers of arrays with dimensions blockdim.x are allocated in the shared memory to store a temporary position of the subunit during molecule generation. the variables, such as rosenbluth weight, the increment counter of overlaps depend on the thread index, and are stored in the shared memory; each with the size of blockdim.x to distinguish between different configurations. the full computational task of parallel random monomer selection and random monomer placement can be programmed at once in a single gpu kernel. this will give opportunity to use gpu parallelization to maximum extent, limited only by hardware restrictions: gpu memory restrictions on coalesced reads and writes, and more importantly on register use. each polymer conformation is generated in a separate cuda thread."
"whether a represents a good estimate for a depends on the total number γ of configurations used and, for a given γ, on the choice of p s (r n ), which, in turn, should approximate exp −βu r n as closely as possible to obtain meaningful results from mc simulations. mc simulations can be static or dynamic. in this paper we mainly focus on the static mc, where a sequence of statistically independent configuration-space points from the distribution p s (r n ) is generated as a basic sampling."
"the basic idea of rosenbluth sampling is to avoid self-intersections by only sampling steps leading to self-avoiding configurations. hence the algorithm will terminate only when the walk is trapped in a dead end and cannot continue growing. although this still happens exponentially often for long chain, rosenbluth sampling can produce substantially longer configurations than simple sampling. during rosenbluth generation process [cit], a monomer can be placed to adjacent sites which can be selected with a probability p. the weight of the generated configuration is multiplied by 1/p. thus, n-step walk grown by rosenbluth sampling has a weight"
"end for 27: end function table 3 one can see from tables 1,2 that for shared memory method implementation, the increasing chain length leads to increase of consumption of shared memory. thus, the number of blocks running simultaneously will be reduced to meet the increasing shared memory consumption since the total shared memory size for each gpu is fixed."
"in this paper, we propose a novel appearance based approach for both yaw and pitch estimation that combines an advance manifold embedding with regression in order to achieve state-of-art performance. furthermore, histogram of oriented gradients (hog) are extracted from the image as preliminary feature extraction step. our approach can be applied to both full head images or to face crops in combination with a face detector. our system is thoroughly evaluated in 6 different datasets and compared against state-of-art methods hpe [cit], drmf [cit] and openface [cit] . the main contributions of our paper are the proposal of a manifold embedding based on discriminative common vectors that allows a better modelling of the face image subspace, and a fully continuous regression model that allows continuous angle estimation, including extreme angles. this paper is structured as follows: section i-a briefly introduces the related works in this field. section ii introduces the method proposed. section iii describes the empirical validation and presents the results and the analysis of the proposed approach as well as its comparison against the state of the art. finally, section iv summarizes the main conclusions and results."
1) compute gradients in the cell region to be described 2) put them in bins according to orientation 3) group the cells into large blocks 4) normalize each block
in equilibrium statistical mechanics thermodynamic properties are represented by the ensemble averages of the observable a over all coordinates of n particles r n .
"any sample x i can be projected in the discriminative subspace, for an easier classification, by using the projection matrix w gdcv, according to"
"finally, in order to provide our best possible system for its application in real scenario, a final experiment is designed where the model is trained using taiwan and prima datasets. this is due to them having the best pitch and yaw resolution. face crops is used as input due to its superior performance demonstrated in previous experiments. this model is tested in all remaining datasets, including the drivface [cit] dataset which contains real variations such as illumination changes, vibrations and imperfect face crops, and compared against all other competitors (hpe with the same training and drmf using the best training provided by the authors). table 5 shows the comparative among all methods. it can be seen how our approach with a carefully selected training provides the best performance in almost all cases (expect for cmu-pie, the smallest set where hpe gives the best result), with errors ranging between 9 and 16 degrees for the yaw and around 10 degrees for the pitch, which are acceptable for most applications."
"in the classical literature, two types of charts have been used in monitoring the production process. when the quality characteristic of interest is measurable, we use variable control charts such as the x-bar chart and s-chart or the r-chart, while attribute (count data) charts such as the p-chart, npchart are used when the quality characteristic is classified as good or bad. the attribute control charts are particularly important in non-manufacturing quality improvement efforts in which the targeted quality characteristics are impossible to measure on a numerical scale (montgomery [cit] ). the number of defects in an item produced by a manufacturing process is common to monitor for improving the quality of the product."
", due to the high degree of freedom depending on n, it is in general not possible to sample the entire original distribution, one can use a similar but smaller sampling distribution p s (r n ) to replace the original distribution then correcting for the corresponding error. such technique is known as representative sampling [cit] . a is then estimated by"
"in the case of off-lattice implementation, the chains are represented as a sequence of beads which can be placed randomly in 3d space according to saw."
"given the usual bias of the training set to certain angles, since datasets are usually recorded at regular intervals, we exploit this feature in our advantage to reduce noise in the projection in those cases. specifically, the previous projected sample i is refined to the location of the closest discriminative common vector j if this distance d i,j is below a small threshold (see eq. 4). otherwise, the projection remains unchanged as given by eq. 3."
rosenbluth chain generation on a cubic lattice require: blockdim.x is the block dimension defined by user require: seed is a random number seed chosen by the user require: chainlength is the polymer chain length chosen by the user require: function rngonlattice generates a random position for the next monomer require: function distance calculates the distance between two monomers 1
"experimental results are provided for both cases when all the head image (and surrounded background) is used as input by the system for or when only the face crop is provided (with the exception of openface whose software requires the full head to fit their 3d model). table 1 shows the average result over the iterations as well as dispersion for the full head image variation, and table 2 the same results when using the face crop only."
"appearance based methods have historically considered the head estimation problem as a discrete problem -i.e. as a classification problem-, or as a continuous problem -i.e as a regression problem. classification-based methods [cit] suffer from granularity of the estimated angles given the difficulty to train two classes whose angles are very close. in contrast, regression based methods provide a fully continuous estimation, resulting on a higher proliferation of these approaches in the literature. these approaches are mainly composed of two main stages: a first stage where a feature set is obtained from the raw image, and a second stage where linear/nonlinear regression methods make use of a labelled training set to create a mapping from images/features space to their corresponding poses."
"a direct comparison between cpu and gpu of both lattice and off-lattice version is presented in tables 1 and 2 . we define a speed-up factor as follows: tcpu is the execution time on a single cpu core and tgpu is the runtime on the gpu. during the run one million of linear polymer conformations of different lengths is generated on nvidia tesla k80 gpu or nvidia gtx 1080 and intel i5-3320m cpu. with growing length, the efficiency gradually decreases. the decrease comes mainly from the increasing consumption of the shared memory of each block. [cit] monomers gpu outperform single core cpu as a factor of 67.2."
"w ) the projection basis fulfilling the above conditions for a given value of α can be obtained through u r, such that r is reassigned. the gdcv method is presented in algorithm 1."
benchmarks on different gpus are summarized in table 3 and the comparison of performance is presented in figure 3 . the performance is evaluated as the number of chains generated per second.
there exists a large class of sampling algorithms based on mc methods. rosenbluth sampling is a mc method for generating correctly distributed self avoiding walk (saw) by means of weights calculated on the fly. rosenbluth sampling method have been widely applied and used due to its efficiency and simplicity of implementation.
"this section is limited to the most relevant literature to our work, the appearance-based methods, i.e. those methods that use the full raw image as input due to their advantages for real unconstrained environments. a complete description of all the methods available is out of the scope of this article so we refer the reader to the survey [cit], the paper [cit] and the book [cit], although theses do not include methods based on depth learning due to their recent appearance."
"the performance is larger for smaller system sizes due to limitations of size of shared memory and number of registers in existing gpu architectures. with growing computational capacities of gpus one can expect further increase of efficiency allowing for more and more practical applications of the presented method. thus, this method could potentially find applications in many other fields such as robotics,artificial intelligence, big data analysis."
"accurate head pose estimation is a challenging problem in itself due to the variability introduced by multiple factors such as illumination, identity and expression, to name a few. during the last decade there has been an increasing interest in developing head pose estimation methods for different applications such as security and surveillance systems [cit], human-robot interaction [cit], meeting rooms [cit], intelligent wheelchair systems [cit], and driving monitoring [cit] . head pose is typically expressed by three angles (yaw, pitch and roll) that describe the orientation with respect to a headcentered frame, being yaw and pitch the angles that are more related with the gaze and attention of the subject under consideration."
"our head pose estimation framework is composed of three main components: an initial feature extraction based on the computation of histogram of oriented gradients, a manifold embedding projection based on generalized discriminative common vectors (gdcv), and a continuous regression composed of spline fitting and multivariate local regression. this pipeline, as well as the resulting subspaces involved at each step, are depicted in figure 1 ."
"after the manifold has been created, regression in such discriminative embedding space (w t gdcv x hog ) is learned to generate the final pose estimation. this regression consists of two parts. first, a b-splines is used to construct a curve y that has the best fit to the project samples, where the control points are the x gdcv j . this spline allows explicitly introducing the continuous and smooth transition between classes, inherent to the nature of the angular problem under consideration. second, a multiple linear regression to estimate the relationships between the previous curve y and the final angle(s) to be estimated z is calculated."
"the proper choice of the data structure is critical for implementation performance. in the present work, all the coordinates of each monomer are stored in a shared memory during the saw process, the coordinates of each conformation and corresponding rosenbluth weight are flushed to the global memory when the chain is successfully generated."
"statistical methods and computer simulations play major role in theoretical understanding of many-body interactions in physics and chemistry [cit] . in particular, molecular dynamics (md) and monte carlo (mc) methods [cit] are the main theoretical tools used to describe physical and chemical processes at the molecular level. increasing computer power and availability of computational recourses contribute in growing popularity of computational methods. even computationally expensive ab-initio calculations become feasible nowadays: the length-scales and time-scales of atomistic simulations increased more than 10 times in a decade [cit] ."
"a block diagram illustrating a the generation process is shown in figure 2a . computation is performed by sets of streaming multiprocessors, each containing several computer units. code is executed as a block of threads on a particular multiprocessor. blocks of threads are grouped in a grid. each multiprocessor contains a small shared memory store that can be accessed by all threads in a given block. for current implementation we define the size of the block. the total number of blocks or the grid will automatically be defined as the total number of chains dividing the block dimension. on a single block, each thread simultaneously and synchronously generates its own statistically uncorrelated conformation of the molecule using the rosenbluth algorithm. all the data are stored in the shared memory during the generation process. the data in the shared memory can also be used on the fly: data is processed directly and then discarded. alternatively, it can also be saved to the global memory on gpu and later copied back to cpu. however, since all coordinates are stored in shared memory during the saw process, high consumption of shared memory may greatly affect the occupancy of the program."
"where the proportionality constant a depends on the structure and on external conditions such as the solvent used in the chemical solution or temperature; critical exponent ν is universal, and depends only on the dimension of space."
"in this paper, we propose a novel appearance-based head estimation system for both yaw and pitch estimation. our system combines hog feature extraction with an gdcv manifold embedding, that takes the granular high dimensional nature of the problem, and multivariate regression, that considers the continuous and smooth continuity of the estimated angles by applying splines. our system demonstrates flexibility to work with raw head images, wildly available in real conditions, or more refined facial crops, assuming a good face detector is available. our approach achieves stateof-art performance in an exhaustive experimental validation comprising six different datasets and both intra-set and crossdataset experiments. the final performance surpasses the other three methods in the comparison, including cnn-based methods, with angular errors between 9 and 17 degrees, and was evaluated in a realistic datasets for autonomous driving."
"we initialize the rng with a single random number seed but a different sequence number for every thread. to initialize the mersenne twister generator [cit], it is necessary to create a rng [cit] status for every thread and pass this status to the curand init [cit] function with a seed but different sequence number. the distance between the first elements in successive sequence for mersenne twister is 2 67, so that it is unlikely that two sequences will overlap even in extensive simulation. in our implementation, we initialize the mersenne twister once and use the updated rng status for the entire calculation. once the rng is initialized, a normally distributed pseudo-random numbers can be generated for all individual threads. in current implementation, the initial seed was given before the starting of the sampling kernel."
"first, hog features are extracted to enhance the discriminative information in the image [cit] before the final feature embedding space is calculated. the underlying idea is that local object appearance and shape are well characterized by the distribution of local intensity gradients and edge directions while being less sensitive to illumination changes and cluttered and changing background. while the manifold embedding could be generated directly using the raw image as direct input (see section iii-c), the resulting space using hog feature enhances the discrimination between pose orientations."
"we developed highly efficient parallel gpu implementation of the rosenbluth algorithm of generation of selfavoiding random walks on a lattice and in real space, which scales almost linear with the number of cuda cores. both versions of the code have the same accuracy compared with a single core cpu implementation, but give huge performance improvement in simulation efficiency making the generation process almost perfectly parallelizable [cit] . the implementation breaks the performance bottleneck of existing molecular conformation generating methods, significantly improving parallel performance, and has broad application prospects in the static mc simulations. in molecular simulations, the whole configurational sampling of short lipids, surfactants or peptide sequences can be generated directly."
"for lattice implementation, the coordinates of the monomer therefore are stored as numerical integer type, which can benefit from half-precision introduced in new architecture pascal. for real-space implementation, the coordinates of the monomer are stored as single-precision or double precision floating-point type. however, double precision greatly increases the shared memory consumption leading to push the gpu into the occupancy limitation and increasing bank conflicts, thus decreasing the performance. that is why all calculations are performed using single-precision floating-point operations. we expect that the performance of gpu with double-precision environment will be improved in the future chip generations. the length of the chains in our calculations is limited due to limitations of shared memory in existing gpu chips. the memory architecture and shared memory size may greatly improve in future gpu architectures thus allowing for increased performance for longer chains."
"in order to compare the performance of our gpu implementation, we implemented a sequential cpu version of the same algorithm. similar to gpu realization, we use the single precision for the calculations."
"automatic head pose estimation has been approached from different points of view, from appearance based methods, such as manifold embedding, regression or classification approaches, to model based methods, which includes deformable and geometric models. while model-based methods exhibit excellent performance, specially in frontal images or small angles, they require detecting/tracking facial features with high precision and they are significantly affected by partial occlusions of such facial landmarks or by illumination changes, common in real environments. on the contrary, appearance based methods are less sensitive to partial occlusions and extreme angular views since these approaches use the full image of the head, but at the cost of higher computational cost."
"with new nvidia pascal architecture, the shared memory is increased due to the larger number of stream multiprocessor (sm) count, and aggregate shared memory bandwidth is effectively more than doubled. a higher ratio of shared memory, registers, and warps per sm is introduced to the new benchmark gp100 which allows the sm to more efficiently execute code. the bandwidth per-thread to shared memory is also increased. all these features will increase the performance of our implementation."
"first, hog feature extraction is removed and image raw pixels are given to gdcv as direct input. the comparison is shown in table 6 . it can be observed how the use of hog features help to obtain a more discriminative subspace and a better performance, reducing the angular error between 12 and 31 degrees, depending the dataset. this is particularly noticeable in realistic conditions (drivface), where illumination changes are frequent and can affect greatly the raw pixel values."
"where µ is a model dependent connectivity constant and γ is the universal entropic exponent. consider a sample of ℵ configurations of polymers with length n, (s 1, s 2, ..., s ℵ ) and corresponding rosenbluth weights (w(s i )). then z n is estimated as"
"average error in degrees for the cross-dataset experiment using the face crop as input. n/a is reported for hpe in some cases due to the limitation of the algorithm to run, such as a lower number of classes in training than in testing."
"generation of a representative sampling conformations of polymers require random numbers with long periods and good statistical properties. generating pseudo-random numbers on a cpu is a well-studied topic [cit], in a gpu single instruction multiple thread (simt) environment, many approaches have been used for the generation of random numbers in different types of applications [cit] . the description of random number generators (rngs) can be found in the literature for single stream computations [cit], or parallel implementations [cit] . any rng chosen should guarantee that random numbers to be generated and immediately consumed by user kernels without requiring the random numbers to be written to and then read from global memory. it also guarantees that each thread generates their own random number at the same time. in our code we chose the mersenne twister [cit], which guarantees uncorrelated random number streams of each thread. a detailed implementation of mersenne twister on gpu can be found, for example, in the sdk library from nvidia [cit] ."
"here, the control constant k should be determined by considering the target in-control arl. it is noted that the control limits may be flexibly constructed according to the target in-control arl and the specified test time. the test time constant a is involved in the failure probability of p 0, so it should be determined at the same time."
"open access this chapter is licensed under the terms of the creative commons attribution 4.0 international license (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the creative commons license and indicate if changes were made."
"once we have this information about transitions, we can identify specific states in the lts where there is a choice in the lts that directly affects the compliance with the property. we call these states and the transitions incoming to/outgoing from those states neighbourhoods."
"more precisely, the clear visualizer supports the visualization of tagged ltss enriched with neighbourhoods. these techniques have been developed using javascript, the angularjs framework, the bootstrap css framework, and the 3d force graph library. these 3d visualization techniques make use of different colors to distinguish correct (green), incorrect (red) and neutral (black) transitions on the one hand, and all kinds of neighbourhoods (represented with different shades of yellow) on the other hand. the tool also provides several functionalities in order to explore tagged ltss for debugging purposes, the main one being the step-by-step animation starting from the initial state or from any chosen state in the lts. this animation keeps track of the already traversed states/transitions and it is possible to move backward in that trace. beyond visualizing the whole erroneous lts, another functionality allows one to focus on one specific counterexample and rely on the animation features introduced beforehand for exploring the details of that counterexample (correct/incorrect transitions and neighbourhoods). figure 3 gives a screenshot of the clear visualizer. the legend on the left hand side of this figure depicts the different elements and colors used in the lts visualization. all functionalities appear in the bottom part. when the lts is loaded, one can also load a counterexample. on the right hand side, there is the name of the file and the list of states/transitions of the current animation. note that transitions labels are not shown, they are only displayed through mouseover. this choice allows the tool to provide a clearer view of the lts. from a methodological point of view, it is adviced to use first the clear visualizer during the debugging process for taking a global look at the erroneous part of the lts and possibly notice interesting structures in that lts that may guide the developer to specific kinds of bug."
"we also carried out an empirical study to validate our approach. we asked 17 developers, with different degrees of expertise, to find bugs on two test cases by taking advantage of the abstracted counterexample techniques. the developers were divided in two groups, in order to evaluate both test cases with and without the abstracted counterexample. the developers were asked to discover the bug and measure the total time spent in debugging each test case. we measured the results in terms of time, comparing for both test cases the time spent with and without the abstracted counterexample. we observed a gain of about 25% of the total average time spent in finding the bug for the group using our approach. we finally asked developers' opinion about the benefit given by our method in detecting the bug. most of them agreed considering our approach helpful."
"step-by-step animation is also helpful for focusing on specific traces and for looking more carefully at some transitions and neighbourhoods on those traces. if the developer does not identify the bug using these visualization techniques, (s)he can make use of the clear abstraction techniques presented in the next section."
"in this paper, we have presented the clear toolset for simplifying the comprehension of erroneous behavioural specifications under validation using model checking techniques. to do so, we are able to detect the choices in the model (neighbourhood) that may lead to a correct or incorrect behaviour, and generate a tagged lts as result. the clear visualizer takes as input a tagged lts and provides visualization techniques of the whole erroneous part of the model as well as animation techniques that help the developer to navigate in the model for better understanding what is going on and hopefully detect the source of the bug. the counterexample abstraction techniques are finally helpful for building abstractions from counterexamples by keeping only relevant actions from a debugging perspective. the experiments we carried out show that our approach is useful in practice to help the designer in finding the source of the bug(s)."
"the clear tools ( fig. 1 ) aims at simplifying the debugging of concurrent systems whose specification compiles into a behavioural model. to do so, we propose a novel approach for improving the comprehension of counterexamples by highlighting some of the states in the counterexample that are of prime importance because from those states the specification can reach a correct part of the model or an incorrect one. these states correspond to decisions or choices that are particularly interesting because they usually provide an explanation of the source of the bug. the first component of the clear toolset computes these specific states from a given lts (aut format) and a temporal property (mcl logic [cit] ). second, visualization techniques are provided in order to graphically observe the whole model and see how those states are distributed over that model. third, explanations of the bug are built by abstracting away irrelevant parts of the counterexample, which results in a simplified counterexample. the clear toolset has been developed mainly in java and consists of more than 10k lines of code. all source files and several case studies are available online [cit] . clear has been applied to many examples and the results turn out to be quite positive as presented in an empirical evaluation which is also available online."
"the images or other third party material in this chapter are included in the chapter's creative commons license, unless indicated otherwise in a credit line to the material. if material is not included in the chapter's creative commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder."
"the clear visualizer provides support for visualizing the erroneous part of the lts and emphasizes all the states (a.k.a. neighbourhoods) where a choice makes the specification either head to correct or incorrect behaviour. this visualization is very useful from a debugging perspective to have a global point of view and not only to focus on a specific erroneous trace (that is, a counterexample)."
"there are four kinds of neighbourhoods, which differ by looking at their outgoing transitions ( fig. 2 from left to right): (1) with at least one correct transition (and no incorrect transition), (2) with at least one incorrect transition (and no correct transition), (3) with at least one correct transition and one incorrect transition, but no neutral transition, (4) with at least one correct transition, one incorrect transition and one neutral transition. the transitions contained in neighbourhood of type (1) highlight a choice that can lead to behaviours that always satisfy the property. note that neighbourhoods with only correct outgoing transitions are not possible, since they would not correspond to a problematic choice. consequently, this type of neighbourhood always presents at least one outgoing neutral transition. the transitions contained in neighbourhood of type (2), (3) or (4) highlight a choice that can lead to behaviours that always violate the property. it is worth noting that both visualization and counterexample abstraction techniques share the computation of the tagged lts (correct/incorrect/neutral transitions) and of the neighbourhoods."
the rest of this paper is organised as follows. section 2 overviews the lts and property manipulations in order to compute annotated or tagged ltss. sections 3 and 4 present successively our techniques for visualizing tagged models and for abstracting counterexamples with the final objective in both cases to simplify the debugging steps. section 5 describes experiments we carried out for validating our approach on case studies. section 6 concludes the paper.
understanding this counterexample for debugging the specification is a complicated task for several reasons: (i) the counterexample may consist of many actions; (ii) the debugging task is mostly achieved manually (satisfactory automatic debugging techniques do not yet exist); (iii) the counterexample does not explicitly point out the source of the bug that is hidden in the model; (iv) the most relevant actions are not highlighted in the counterexample; (v) the counterexample does not give a global view of the problem.
"in this section, once the lts has been tagged using algorithms overviewed in sect. 2, the developer can use abstraction techniques that aim at simplifying a counterexample produced from the lts and a given property. to do so we make a joint analysis of the counterexample and of the lts enriched with neighbourhoods computed previously. this analysis can be used for obtaining different kinds of simplifications, such as: (i) an abstracted counterexample, that allows one to remove from a counterexample actions that do not belong to neighbourhoods (and thus represent noise); (ii) a shortest path to a neighbourhood, which retrieves the shortest sequence of actions that leads to a neighbourhood; (iii) improved versions of (i) and (ii), where the developer provides a pattern representing a sequence of non-contiguous actions, in order to allow the developer to focus on a specific part of the model; (iv) techniques focusing on a notion of distance to the bug in terms of neighbourhoods. for the sake of space, we focus on the abstracted counterexample in this paper. abstracted counterexample. this technique takes as input an lts where neighbourhoods have been identified and a counterexample. then, it removes all the actions in the counterexample that do not represent incoming or outgoing transitions of neighbourhoods. figure 4 shows an example of a counterexample where two neighbourhoods, highlighted on the right side, have been detected and allow us to identify actions that are preserved in the abstracted counterexample."
"we carried out experiments on about 100 examples. for each one, we use as input a process algebraic specification that was compiled into an lts model, and a temporal property. as far as computation time is concerned, the time is quite low for small examples (a few seconds), while it tends to increase w.r.t. the size of the lts when we deal with examples with hundreds of thousands of transitions and states (a few minutes). in this case, it is mainly due to the computation of tagged ltss, which is quite costly because it is based on several graph traversals. visualization techniques allowed us to identify several examples of typical bugs with their corresponding visual models. this showed that the visualizations exhibit specific structures that characterize the bug and are helpful for supporting the developer during his/her debugging tasks. as for abstraction techniques, we observed some clear gain in length (up to 90%) between the original counterexample and the abstracted one, which keeps only relevant actions using our approach and thus facilitates the debugging task for the developer."
"application programming interface (api) is a set of protocols and standards that define the communication between software applications through internet. cloud apis are used at all the infrastructure, platform and software service levels to communicate with other services. infrastructure as a service (iaas) apis are used to access and manage infrastructure resources including network and vms, platform as a service (paas) apis provide access to the cloud services such as storage and software as a service (saas) apis connect software applications with the cloud infrastructure. the security of various cloud services depends on the apis security. weak set of apis and interfaces can result in many security issues in cloud. cloud providers generally offer their apis to third party to give services to customers. however, weak apis can lead to the third party having access to security keys and critical information in cloud. with the security keys, the encrypted customer data in cloud can be read resulting in loss of data integrity, confidentiality and availability. moreover, authentication and access control principles can also be violated through insecure apis."
"cloud computing offers the provisioning of services by sharing of infrastructure, platform and software. however, different components such as cpus, and gpus may not offer cloud security requirements such as perfect isolation. moreover, some applications may be designed without using trusted computing practices due to which threats of shared technology arise that can be exploited in multiple ways. in recent years, shared technology vulnerabilities have been used by attackers to launch attacks on cloud. one such attack is gaining access to the hypervisor to run malicious code, get unauthorized access to the cloud resources, vms, and customers data."
"identity and access management should also be implemented properly to avoid access to credentials. to avoid account hijacking threats, multi factor authentication for remote access using at least two credentials can be used. a technique that uses multi-level authentication at different levels through passwords was made to access the cloud services. first the user is authenticated by the cloud access password and in the next level the service access password of user is verified [cit] . moreover, user access to cloud services and applications should be approved by cloud management. the auditing of all the privileged activities of the user along with information security events generated from it should also be done to avoid these threats [cit] ."
"many business level security policies, standards, and practices cannot be implemented in cloud due to which different security risks arise. although cloud security has been a focused area of research in the last decade, there are still open challenges in achieving it. to control the security risks in cloud, it is crucial for researchers, developers, service providers, and users to understand them so that they can take maximum precautions, deploy existing security techniques or develop new ones. in this paper, the top security threats for cloud computing presented by cloud security alliance (csa) [cit] have been analyzed. the csa guide [cit] presents the security threats for cloud in the order of their severity and provides controls that can be followed by the service providers to avoid these threats. however, these threats and the controls to avoid them are very mentioned specifically to meet the requirements of industry. therefore, there is a need to survey the security threats for cloud and their solutions from the research perspective. in this paper we define these threats, describe the ways they can be launched in cloud, the possible ways to exploit these threats and their effects on cloud entities. we have comprehensively analyzed and presented the security solutions for the prevention of these threats from literature. moreover, we have classified these security issues into three categories which are data security, network security and cloud environment security (that includes issues specific to cloud environment). this paper is composed as follows: section ii describes the most critical threats for cloud computing and their effects on cloud entities. section iii describes the security solutions to avoid these threats, and section iv gives the conclusion of paper."
"in this section the major threats for cloud computing are explored. these are: i) data threats including data breaches and data loss, ii) network threats including account or service hijacking, and denial of service, and iii) cloud environment specific threats including insecure interfaces and apis, malicious insiders, abuse of cloud services, insufficient due diligence, and shared technology vulnerabilities."
"a malicious insider is someone who is an employee in the cloud organization, or a business partner with an access to cloud network, applications, services, or data, and misuses his access to do unprivileged activities. cloud administrators are responsible for managing, governing, and maintaining the complete environment. they have access to most data and resources, and might end up using their access to leak that data. other categories of malicious insiders involve hobbyist hackers who are administrators that want to get unauthorized sensitive information just for fun, and corporate espionage that involves stealing secret information of business for corporate purposes that might be sponsored by national governments."
"data is considered to be one the most important valuable resource of any organization and the number of customers shifting their data to cloud is increasing every day. data life cycle in cloud comprises of data creation, transit, execution, storage and destruction. data may be created in client or server in cloud, transferred in cloud through network and stored in cloud storage. when required data is shifted to execution environment where it can be processed. data can be deleted by its owner to complete its destruction."
"account hijacking involves the stealing of user credentials to get an access to his account, data or other computing services. these stolen credentials can be used to access and compromise cloud services. the network attacks including phishing, fraud, cross site scripting (xss), botnets, and software vulnerabilities such as buffer overflow result in account or service hijacking. this can lead to the compromise of user privacy as the attacker can eavesdrop on all his operations, modify data, and redirect his network traffic. 1n 2009 a legitimate service was purchased from amazon's ec2, and compromised to act as zeus botnet [cit] ."
"in cloud architecture, hypervisor is responsible for mediating interactions of virtual machines and the physical hardware. therefore, hypervisor must be secured to ensure proper functioning of other virtualization components, and implementing isolation between vms. moreover, to avoid shared technology threats in cloud a strategy must be developed and implemented for all the service models that includes infrastructure, platform, software, and user security. the baseline requirements for all cloud components must be created, and employed in design of cloud architecture. the service provider should also monitor the vulnerabilities in the cloud environment, and release patches to fix those vulnerabilities regularly [cit] ."
"network plays an important part in deciding how efficiently the cloud services operate and communicate with users. in developing most cloud solutions, network security is not considered as an important factor by some organizations. not having enough network security creates attacks vectors for the malicious users and outsiders resulting in different network threats. most critical network threats in cloud are account or service hijacking, and denial of service attacks."
"the data security properties that must be maintained in cloud are confidentiality, integrity, authorization, availability and privacy. however, many data issues arise due to improper handling of data by the cloud provider. the major data security threats include data breaches, data loss, unauthorized access, and integrity violations. all of these issues occur frequently on cloud data. in this paper, we focus on data breaches and data loss that are described as the two most severe threats to cloud computing by csa [cit] . this may happen accidently due to flaws in infrastructure, application designing, operational issues, insufficiency of authentication, authorization, and audit controls [cit] . moreover, it can also occur due to other reasons such as the attacks by malicious users who have a virtual machine (vm) on the same physical system as the one they want to access in unauthorized way."
"b. network security 1) protection from account or service hijacking: account or service hijacking can be avoided by adopting different security features on cloud network. these include employing intrusion detection systems (ids) in cloud to monitor network traffic and nodes for detecting malicious activities. intrusion detection and other network security systems must be designed by considering the cloud efficiency, compatibility and virtualization based context [cit] . an ids system for cloud was designed by combining system level virtualization and virtual machine monitor (responsible for managing vms) techniques [cit] . in this architecture, the idss are based on vms and the sensor connectors on snort which is a well-known ids [cit] . vm status and their workload are monitored by ids and they can be started, stopped and recovered at any time by management system of ids."
"2) denial of service: denial of service (dos) attacks are done to prevent the legitimate users from accessing cloud network, storage, data, and other services. dos attacks have been on rise in cloud computing in past 5 years and 81 percent customers consider it as a significant threat in cloud [cit] . they are usually done by compromising a service that can be used to consume most cloud resources such as computation power, memory, and network bandwidth. this causes a delay in cloud operations, and sometimes cloud is unable to respond to other users and services."
"2) data loss: data loss is the second most important issue related to cloud security. like data breach, data loss is a sensitive matter for any organization and can have a devastating effect on its business. data loss mostly occurs due to malicious attackers, data deletion, data corruption, loss of data encryption key, faults in storage system, or natural disasters. 44 [cit] that resulted in data loss and data leakage [cit] . similarly, malware attacks have also been targeted at cloud applications resulting in data destruction."
"the term due diligence refers to individuals or customers having the complete information for assessments of risks associate with a business prior to using its services. cloud computing offers exciting opportunities of unlimited computing resources, and fast access due which number of businesses shift to cloud without assessing the risks associated with it."
"a. data security 1) protection from data breaches: various security measures and techniques have been proposed to avoid the data breach in cloud. one of these is to encrypt data before storage on cloud, and in the network. this will need efficient key management algorithm, and the protection of key in cloud. some measures that must be taken to avoid data breaches in cloud are to implement proper isolation among vms to prevent information leakage, implement proper access controls to prevent unauthorized access, and to make a risk assessment of the cloud environment to know the storage of sensitive data and its transmission between various services and networks."
"it is important for organizations to fully understand the scope of risks associated with cloud before shifting their business and critical assets such as data to it. the service providers must disclose the applicable logs, infrastructure such as firewall to consumers to take measures for securing their applications and data [cit] . moreover, the provider must setup requirements for implementing cloud applications, and services using industry standards. cloud provider should also perform risk assessment using qualitative and quantitative methods after certain intervals to check the storage, flow, and processing of data."
"distributed denial of service (ddos) attack is a form of dos attacks in which multiple network sources are used by the attacker to send a large number of requests to the cloud for consuming its resources. it can be launched by exploiting the vulnerabilities in web server, databases, and applications resulting in unavailability of resources."
"cloud computing is getting widely adopted in businesses around the world. however, there are different security issues associated with it. in order to maintain the trust of customers, security should be considered as an integral part of cloud. in this paper we have focused on most severe threats on cloud computing that are considered relevant by most users and businesses. we have divided these threats into categories of data threats, networks threats, and cloud environment specific threats. the impact of these threats on cloud users and providers has been illustrated in the paper. moreover, we also discuss the security techniques that can be adopted to avoid these threats."
"cloud computing offers many advantages such as increased utilization of hardware resources, scalability, reduced costs, and easy deployment. as a result, all the major companies including microsoft, google and amazon are using cloud computing. moreover, the number of customers moving their data to cloud services such as icloud, google drive, dropbox, facebook and linkedin are increasing every day."
"due to the complex architecture of cloud, some of organization security policies cannot be applied using cloud. moreover, the cloud customers have no idea about the internal security procedures, auditing, logging, data storage, data access which results in creating unknown risk profiles in cloud. in some cases, the developers and designers of applications maybe unaware of their effects from deployment on cloud that can result in operational and architectural issues."
in this section the security methods to avoid the exploitation of threats mentioned in section ii have been discussed. we describe the implementation of these security techniques at different levels to secure cloud from threats.
"the protection from these threats can be achieved by limiting the hardware and infrastructure access only to the authorized personnel. the service provider must implement strong access control, and segregation of duties in the management layer to restrict administrator access to only his authorized data and software. auditing on the employees should also be implemented to check for their suspicious behaviour. moreover, the employee behaviour requirements should be made part of legal contract, and action should be taken against anyone involved in malicious activities [cit] . to prevent data from malicious insiders encryption can also be implemented in storage, and public networks."
"the biggest challenge in achieving cloud computing security is to keep data secure. the major issues that arise with the transfer of data to cloud are that the customers don't have the visibility of their data and neither do they know its location. they need to depend on the service provider to ensure that the platform is secure, and it implements necessary security properties to keep their data safe."
"c. cloud environment security 1) protection from insecure interfaces and apis: to protect the cloud from insecure api threats it is important for the developers to design these apis by following the principles of trusted computing. cloud providers must also ensure that all the all the apis implemented in cloud are designed securely, and check them before deployment for possible flaws. strong authentication mechanisms and access controls must also be implemented to secure data and services from insecure interfaces and apis. the open web application security project (owasp) [cit] provides standards and guidelines to develop secure applications that can help in avoiding such application threats. moreover, it is the responsibility of customers to analyze the interfaces and apis of cloud provider before moving their data to cloud."
"over the years, different attacks have been launched through cloud by the malicious users. for example, amazon's ec2 [cit] . famous cloud services such as twitter, google and facebook as a command and control servers for launching trojans and botnets. other attacks that have been launched using cloud are brute force for password cracking of encryption, phishing, performing dos attack against a web service at specific host, cross site scripting and sql injection attacks."
"xen platform is an open source solution used to offer cloud services. xen hypervisors code creates local privilege escalation (in which a user can have rights of another user) vulnerability that can be launch guest to host vm escape attack. later, xen updated the code base of its hypervisor to fix that vulnerability. other companies such as microsoft, oracle and suse linux that were based on xen also released updates of their software to fix the local privilege escalation vulnerability. [cit] showed the usage of vmware to run code from guests to hosts showing the possible ways to launch attacks."
"2) protection from denial of service: to avoid dos attacks it is important to identify and implement all the basic security requirements of cloud network, applications, databases, and other services. applications should be tested after designing to verify that they have no loop holes that can be exploited by the attackers."
"3) abuse of cloud services: the term abuse of cloud services refers to the misuse of cloud services by the consumers. it is mostly used to describe the actions of cloud users that are illegal, unethical, or violate their contract with the service provider. [cit], and different measures were taken to prevent it. however, 84 percent of cloud users still consider it as a relevant threat [cit] . research has shown that some cloud providers are unable to detect attacks launched from their networks, due to which they are unable to generate alerts or block any attacks. the abuse of cloud services is a more serious threat to the service provider than service users. for instance, the use of cloud network addresses for spam by malicious users has resulted in blacklisting of all network addresses, thus the service provider must ensure all possible measures for preventing these threats."
"the ngs approach is generating extensive data, and the number of micrornas will continue to rise in the near future. these new data will be integrated and the database regularly updated (at least twice a year) so that it remains exhaustive. identity with human or mouse micrornas will be checked each time that a new release of mirbase becomes available. the three ruminant species probably contain a comparable number of micrornas as the difference between the numbers of bovine (2887), caprine (2733) and ovine (5095) micrornas could be due to the more intense efforts assigned to ovine sequencing. however, the numbers of human or mouse micrornas in the latest release of mirbase are 2654 [cit], respectively. microrna detection in ruminant species, as compared to humans and mice, may have reached saturation. in addition, the numbers will probably evolve as the genome assemblies are updated and the same quality of contiguity and annotation is attained as for human and mouse species."
"the rumimir database contains an exhaustive list of all micrornas described in publications pertaining to three livestock species: cattle, goats and sheep. this database supplements mirbase, one of the most widely used microrna databases, by including various features mentioned in the literature, which are important in the context of animal production and dairy traits, notably the breeds of animals studied or the tissues in which the micrornas were described. rumimir can be used to retrieve specific micrornas and thus provide additional information about the livestock species and a clearer understanding of the context in which these micrornas were discovered. the database will be regularly updated and will continue to be exhaustive. rumimir, by standardizing and centralizing information from a large number of publications, constitutes a unique tool that presents and describes all known micrornas in ruminants."
"all micrornas, and most of the features described in the literature, have therefore been documented in the rumimir database. all the microrna sequences are noted, as is the presence (or not) of isomirs (microrna sequences at the same genomic position, with almost identical sequences). these almost identical sequences are highlighted in a specific column ('isomirs') in rumimir and also if the microrna sequence belongs to a known microrna family. the database also contains details of the number of animals studied, their breeds, ages and lactation stage, as well as the tissue of origin, if this was mentioned in the publication."
"one potential extension for rumimir might be to add genetic variants of ruminant micrornas ('mirsnps' for microrna and single nucleotide polymorphisms). first, those linked with dairy qtl, and then those associated with health or meat qtl. publications on these genetic microrna variations are indeed increasingly numerous. for example, in humans, the msdd database has been created exclusively for mirsnps linked to human diseases (10). in the same way, the rumimir database could be completed by including ruminant mirsnps. other features and filters will be added depending on the features described in the literature and the needs of the scientific community. evolution of the reference genomes will also be considered, to take into account the genomic positions of micrornas in the latest versions of the reference genomes. the rumimir database could also be extended to include other livestock species."
"the web interface is user friendly, allows visualization of all the micrornas described in bovine, caprine and ovine species and provides at minimum the genomic position (chromosome, start and end) of each microrna. some or all the columns can be selected to obtain the sequence, name, tissue or other features of the study and of the micrornas, by clicking on the appropriate name in the 'show/hide columns' box ( figure 7) ."
"the rumimir database is extensive because it lists all the features mentioned in the publications (unlike mirbase and other databases), the data are homogenized (with blast on the same reference genomes for example), and because information is included about the identity, small rnas etc. in addition, the rumimir database combines the results of a multitude of studies in a single location."
"users can also visualize those micrornas of solely personal interest by selecting the corresponding column (sequence and/or tissue and/or breed etc.) and then using the 'search' box to define different options such as 'by position' (chromosome, start and end), 'by source' (species, breed and tissue) or 'by feature' (conditions, method and software; figure 7 ). the above-mentioned criteria can also be applied to filter all the mature micrornas listed. a sequence or key word can also be entered in the 'search' box to extract a list of the relevant data in the results table. the name of each microrna in the 'name' column is the one given in the original publication. if specific requests are selected, the appropriate filtered data appear and can be downloaded in several formats (excel, csv, fasta or gff). users can thus obtain all the data that they require, appropriately filtered and rapidly. they can also download the entire database without applying any filters."
"age was mentioned in 38% of the publications, and more than 30 different ages have been listed. some studies described the differential expression of micrornas throughout development, and at different ages, which is why several ages might be considered in a single publication. the same was true of lactation stages. all these figures will increase as the database is updated. the numbers of micrornas common to all three ruminant species, or sequences common to these species and to human and mouse sequences, or specific to each species, are presented in figure 6 . between 80.24% and 88.82% are speciesspecific micrornas."
"studies in cattle, goats and sheep have often focused on production traits such as dairy and meat products (15) (16) (17), health [mastitis resistance (18, 19) ] or reproduction [fertility and fecundity (20) (21) (22) ]. due to genome conservation between the three species, the mirnomes (all micrornas expressed in a tissue or cell type) are relatively similar. however, there are some differences, as well as those between breeds (23). this explains the importance of generating a database that includes all three species, together with specific information on the breed and physiological status of the animals, as this is frequently lacking in the most commonly employed databases."
"the features available for the various micrornas, such as the sequence, related publications, name, isomirs, family, tissue origin, breed, condition studied and others are listed in the 29 columns shown in table 1 ."
"the importance of each microrna is evaluated from the false-positive score. around 50% of the micrornas (56.1% in bovine, 51.2% in caprine and 49.8% in ovine species) have a score with a low risk of being false positive (code containing 0 or 1 '1'). moreover, almost every microrna has a code containing 0-2 '1' (98.3% in bovine, 92.8% in caprine and 99.4% in ovine species; figure 3 and supplementary figure 3 ). each score is represented with the corresponding encoding: for example, the scores of 1 are due to the absence of identity with a mouse and human microrna, in the three species. in fact, when other elements appeared (i.e. small rna or multi-mapping), they are often associated with another false positivity element, as seen in figure 3 ."
"most of the micrornas listed in rumimir have the expected length (20-24 nt) (1), with 99.27% having 17-25 nt. the microrna length distributions in bovine and caprine species are similar ( figure 4a ) with 91% of bovines and 84% in caprines having 20-24 nt. the length distribution in ovine species is unusual (55% having 20-24 nt). the numbers of micrornas with lengths of 17-18 nt and those with 21-22 nt are similar. the proportion differs if only those micrornas with weak false-positive score (code containing 0 and 1 '1') are considered, but the percentage of micrornas with lengths of 17-18 nt is still high (∼25%; figure 4b ). most of the ovine sequences with lengths of 17 and 18 nt were obtained from a single paper (4) . the authors explained, in their discussion, that they had discovered a large number of new micrornas because they had not applied the commonly used restricting prescreening. all sequences detected at a low-count level or with a low-sample frequency were considered. most of the ovine sequences with lengths of 17 and 18 nt correspond to new micrornas."
rumimir provides a single access portal to an integrated database containing all the information currently published on ruminant micrornas. rumimir is freely available online at the following url: http://rumimir.sigenae.org/.
"micrornas are small, highly conserved, non-coding rnas ∼22 nt in length (1) that participate in the post-transcriptional regulation of genes through their impact on targeted messenger rnas (mrnas). this phenomenon can lead to translation repression or degradation of the targeted mrnas and depends on the base-pair binding of the micrornas to their target via a recognition site, the 'seed' sequence (2, 3) ."
"this database offers an exhaustive list of bovine, caprine and ovine micrornas, collected from the literature. pertinent information, notably in the context of dairy production, has also been added to the description of each mature microrna, and a filtering option for all these data is included. the entire database (and data filtered by applying one or more filters) can be downloaded in different formats. moreover, past database versions are traceable and can be downloaded."
"many studies currently use next-generation sequencing (ngs) technology to explore the transcriptome, notably in a context of microrna discovery. a significant amount of data is thus being generated, which can attain more than 1000 microrna sequences in a single publication (4, 5) . regularly updated tools for data exploration are therefore required and essential. animal, plant and virus micrornas are already listed in several databases, such as mirnest 2.0 (covering more than 400 different species) (6) or mirortho, which contains 46 animal genomes (7) . some databases are restricted to human micrornas in a disease context, such as epimirbase (micrornas associated with epilepsy) (8), mircancer (micrornas and cancer) (9) or the microrna snp disease database (msdd; genetic variants affecting micrornas in a disease context) (10) . the recent mircarta database (11), which lists micrornas for 148 species, is solely based on the prediction of novel micrornas. a database portal, mirtools-gallery, which contains more than 1000 tools for studying, identifying or predicting the targets of micrornas, has recently been set up (12) . [cit] by griffiths-jones and collaborators at the university of manchester (13) and lists micrornas for 271 species (14) ."
the 'release history' box on the website shows the history of the various rumimir database versions and allows the user to download entire datasets from current and past versions.
"rumimir is limited to three ruminant species, unlike mirbase, which is the most widely used and complete database in terms of the number of species covered (plants, animals and viruses). however, this restriction enabled us to generate a more detailed database containing all the micrornas described in different publications as well as numerous features. six times more micrornas are listed in the rumimir database for bovine, caprine and ovine species than in mirbase: 10 715 in rumimir versus 1614 in mirbase. although the latest mirbase release (v22) is quite recent [cit], it included only 5% of the data available on deep sequencing of small rna (14) . the risk of assembling all micrornas mentioned in the literature is still to include false-positive micrornas, which is why this risk has been evaluated for each sequence. thus, rumimir offers a complete and more precise microrna database for cattle, goats and sheep and should therefore be of value to scientists working on livestock species."
"the rumimir website was built using html5 technology (https://dev.w3.org/html5), the bootstrap frontend framework (https://getbootstrap.com− −v4.0.0) with additional jquery user interface elements (http://jquery. com− −v3.2.1) and datatables jquery plug-in for the data table (https://datatables.net− −v1.10.16), and the plots were implemented by using the highcharts javascript library (http://www.highcharts.com− −v6.2.0). rumimir has been successfully tested on chrome (version 49 and later) and firefox (version 57 and later). the data, in json format, were provided to the datatables jquery plug-in by setting the ajax option to the address of the json data source. all statistics, charts and drop-down lists have been built into the data-based fly to make any updates as easy as possible. the blast tool was developed using perl-cgi."
"rumimir contains data collected from all publications describing ruminant micrornas, which corresponds to the 78 publications cited in pubmed (https://www.ncbi.nlm. nih.gov/pubmed). the references for all these publications are listed in supplementary data ( supplementary figure 1 ). titles and authors, with a hyperlink to the publication, are provided. rumimir includes all known microrna sequences, as well as those described as 'novel' in the publications, which are not always found in the mirbase database. all microrna sequences were obtained from publications (usually from the text, figures or supplementary data). the microrna sequences included in rumimir database are processed and filtered by the authors of each publication. the rumimir data were standardized by aligning the sequences to a unique reference genome for each species. for this purpose, blast was implemented using the ncbi tool (24), based on the latest available versions of the reference genome (umd3.1.1 for bovine, ars1 for caprine and oar v4.0 for ovine). the parameters used for the blast analyses were the ncbi blastn tool ('somewhat similar sequences') and general default parameters (short queries; expected threshold, 1000; word size, 7) (24). in the event of multiple alignments, with a whole query cover and 100% identity on the genome, the position of the microrna sequence could be identified from a blast of the precursor sequence. multiple positions were obtained for 344 sequences, even after applying the precursor sequence, when this was available in the publication. these micrornas have therefore been listed in the rumimir database without their position, but the number of positions on the genome is shown in the 'multi-mapping' column (de facto the chromosome, start and end columns are therefore blank). characterization of a new microrna was refined by comparing its sequence with small nucleolar rna (snorna), transfer rna (trna), ribosomal rna (rrna) and small nuclear rna (snrna) sequences. indeed, part of the snorna structure, the stem loop, is almost identical to that of microrna (25), and trnas with three hairpin loops might also be confused with microrna. to prevent this, the sequences were therefore compared with all species snorna and bovine trna present in snornabase v3 (26) and gtrnadb 2.0 (27), respectively, and with the bovine, caprine and ovine rrna and snrna sequences present in biomart, an ensembl tool (28) . when a microrna displayed 100% identity with a snorna, trna, rrna or snrna over its entire length, it was retained in the rumimir database but this information is indicated in the 'small rna' column. a home-made python script was applied, which compared the microrna sequences with those extracted from the above-mentioned database (supplementary figure 2 ). finally, each microrna was assigned a rumimir identification number: 'rum-species id-xxxxx'. the species ids for cattle, goats and sheep are bta, chi or oar, respectively, and xxxxx is an incremental five-digit number. to add information about a microrna listed in the rumimir database, a search was done to determine its identity with human or mouse microrna. if it had a 100% sequence match with a known microrna over the full length of the shorter sequence, using the mirbase database (release 22), it was considered identical (14) . these identities were verified by applying a python script to compare the sequences with other microrna sequences and those extracted from mirbase (supplementary figure 2) and to indicate any 'strict' homology with human or mouse microrna. this step was included because of the large number of publications, and hence details, available for human and mouse micrornas. all these processes for adding micrornas to the rumimir database ( figure 1, blue box) were used to create a fourdigit false-positive code representing four positions, each with a value of '0' or '1' (0000 to 1111). a '1' in first position means that the microrna has multiple genomic locations, and a '1' in second position means that the microrna is not already listed in the rumimir database (a microrna occurring in different publications is more likely to be a true microrna). a '1' in third position means that the microrna has homology with small rnas, and a '1' in fourth position means that the microrna has no identity with either human or mouse microrna present in mirbase. one or more values of this four-digit code can be filtered out. a microrna with a code of 1111 is most likely to be a false positive. this indication enables rumimir users to select the micrornas with acceptable false-positive scores."
"the majority of micrornas in the database were taken from studies based on differences between breeds, developmental stages (mainly in a context of meat production) or immune response (comparison of healthy animals with those suffering from mastitis, a widely-studied pathology in ruminants; figure 5 ). the different issues addressed in the publications were implemented as filtering options in the rumimir database. thus, 40 different breeds (17 bovine, 14 caprine and 9 ovine breeds, both meat and dairy) are represented along with about 30 tissues and body fluids such as milk (29, 30), adipose tissue (4), mammary gland (31) (32) (33) and ovaries (22, 34, 35) . around half of the micrornas described in the rumimir database are associated with a single tissue (these 'specific' micrornas are highlighted by the symbol ' '). while the probability of a microrna being a false positive decreases if it is described in several studies, some of the micrornas described only once can be specific to a breed, tissue, physiological stage or particular condition."
"for clarity, some statistics and graphs, which summarize the data presented in the database ( supplementary figure 4), are provided in the online version. these include, among others, the date of the latest update, the number of sequences listed and the number of publications involved. the distribution of micrornas by species and by breed is presented graphically, as is the distribution by tissue origin."
"the rumimir database currently contains 10 715 different micrornas for 3 ruminant species: 2887 for cattle, 2733 for goats and 5095 for sheep. the average numbers of micrornas described per publication for bovine, caprine and ovine species are 66, 136 and 340, respectively. the microrna data were collected from 44 publications for bovines, 19 for caprines and 15 for ovines ( figure 2) . the difference in numbers of micrornas reported for cattle and goats, as compared to sheep, is due to the wider use of ngs technologies in ovine species. more precisely, 16 041 sequences, corresponding to 10 715 different mature micrornas, are present in the rumimir database, the difference being due to the presence of isomirs. a total of 344 microrna sequences are indicated as having multiple locations (multi-mapping). the micrornas listed in rumimir have identities with 889 human or mouse micrornas: 11.78% of the described micrornas show sequence identities with both human and mouse micrornas and 4.66% present sequence identity with only 1 human or 1 mouse microrna. comparisons with snorna, trna, rrna and snrna sequences revealed that 48 micrornas displayed sequence identities with part of a trna, 47 with part of a snorna, 34 with part of an rrna and 9 with part of an snrna."
"when the training target is 0.001, figure 1 shows that the pca-ga-bp output error is 0.00072166 and the training steps number is 71. figure 2 shows that the ga-bp output error is 0.00089814 and the training steps number is 889. figure 3 shows the pca output error is 0.00097956 and the training steps number is 3880. by comparison, the ga-bp convergence speed is faster than bp and the convergence speed of pca-ga-bp convergence speed is faster than ga-bp."
"1) encoding and initializing population: using the floating point number encoding, each individual contains all the weights and threshold. r is the nodes number in the input layer. s 1 is the nodes number in the hidden layer. s 2 is the nodes number of the output layer. s is the individual length."
"the bp neural network uses a gradient descent method to change the sample i/o problem into a nonlinear optimization problem [cit] . bp is a typical supervised learning algorithm [cit] . through learning the neural networks weights and thresholds repeatedly to obtain the output error minimum value. the specific process is divided into two steps: forward propagation: the data pass the input layer, hidden layer, and output layer. the actual output and expected output are compared at the output layer. if the actual output error is not reached the expected output error, the network enter the back propagation."
"in the previous section we showed the general benefits that could be achieved by the combination of semantics and crowdsourcing. in this section, we detail some specific use cases (cf. table i) of research developments that try to overcome big data integration and analysis challenges by taking advantage of this combination."
"big data is characterized by three basic properties (3 vs.): volume, velocity and variety [cit] . some additional features like variability and complexity [cit], as well as value and veracity [cit] are also associated to the concept of big data. another element to take into consideration is the fact that the value of data increases exponentially when it is linked and fused with other data. hence addressing the data integration challenge is critical to realizing the promise of big data [cit], and unfortunately existing data warehousing techniques are inefficient to handle such integration [cit] . indeed, traditional data warehouses integrate structured, transactional data that is contained within relational databases. in contrast, unstructured data, which comes in the form of emails, social media, blogs, documents, images, and videos need a novel methodology for the integration process. in recent years, a new concept of data lakes has appeared that stores a vast amount of raw data in its native format and support flexible \"schema-onread\" with the help of metadata descriptions [cit] . nevertheless, preparing, organizing, exploring, and querying a data lake is often strenuous. in particular, \"on-demand integration\" does not take into account the need for data quality or schema understanding [cit] ."
"the cumulative contribution rate of the first three principal components is 99.448%. according to the rule, the original eight variables are replaced by the first three principal components. they are 1 i, 2 i and 3 i respectively. the principal component load matrix is shown in table 4 ."
"vi. conclusion in this survey, we explored the steps needed for merging semantic and crowdsourcing technologies in order to enhance big data integration process and tackle analysis needs and challenges. moreover, we clarified how the choice of a suitable approach for data integration depends on the types of datasets used. in addition, we presented a conceptualization of the highlevel data integration workflow summarized in a flow diagram (fig 5), where, for each step, we showed how extending the automated tasks with crowdsourcing would improve the quality of integrated data."
the rest of the paper discusses the benefits of associating a semantic approach along with crowdsourcing in order to improve data integration despite its variety. in section ii we describe the benefits of such a combination. in section iii we detail some use cases that can be described as success stories for such an association. in section iv we analyze and compare the methods and techniques used for data integration in the use cases we exposed in the previous section. in section v we propose a generic workflow for data integration based on merging these crowdsourcing and linked data techniques. finally section vi concludes this paper and presents a research proposal that would benefit from the hereby described combination.
"back propagation: the error signal transmits from the output layer, then passes the hidden layer and finally reaches the input layer. during this process, every neuron's weight in each hidden layer is corrected according to the negative gradient direction of the error function, and the error signal is continuously reduced to make the actual output near the desired output."
"ga simulates the evolutionary principle in the biological field [cit], which can search data in parallel and randomly, regarding the problem as the biological evolution process. ga selects individuals and generates new individuals repeatedly with selection-crossover-mutation operation until the condition is satisfied [cit] . the specific steps of ga to optimize the bp are as follows:"
"2) evaluation function: inputting the training sample and calculating its error function value. regarding the reciprocal of the error function value as the fitness. if the error is smaller, the fitness is greater, as in (8) . e is the sum of squared errors between the predicted output and the expected output. δ is a positive minimum amount."
"the pca-processed data can be used as the input data of the ga-optimized bp network to achieve rapid convergence. at the same time, because pca greatly reduces the complexity of input data and the complexity of neural network training, it can improve the accuracy of prediction. ga can overcome the shortcomings of the slow convergence rate of bp network and falling into local minimum easily. combined with the three algorithms, they can exert their respective advantages, reducing input, accelerating the convergence speed, and searching journal of computer and communications for global optimal values. at the same time, they have good nonlinear modeling capabilities and overall improve the performance of the network."
"the opening price, closing price, highest price, lowest price, change amount, change rate, trading volume and turnover of gold prices are the combined results of various macro factors and micro factors. a large number of potential laws and factor indicators determine the changes about the gold price. through the study of these data, we can grasp the trend of gold prices to a certain extent. therefore, using historical market data to study gold prices have certain significance. after pca selects principal components with some rules, the original multidimensional data can be simplified, the relevance of network input data can be eliminated, redundant information can be eliminated, the input data of network can be reduced, and the main information of the original data can be retained."
"4) determine the principal component, calculate the principal component contribution rate and cumulative contribution rate. select the first m principal components with the cumulative contribution rate is not less than 85%. the component contribution rate of kth principal as in (4):"
2) standardized the raw data. normalize each raw data with (1) to eliminate the magnitude and dimension difference between variables. the standardized data is shown in table 2. 3) calculating the correlation coefficient matrix r with (2).
"we found that by jointly using semantic and crowdsourcing technologies we have the possibility to address the big data variety issue. indeed, these technologies provide necessary mechanisms to enable integration of different big data sets while ensuring quality, correctness and consistency. in our opinion, future work in this area must focus particularly on providing a generic framework based on the synergy between the semantic technologies and crowdsourcing to solve problems of big data effectively and efficiently. in fact, this study will be the used in support to the implementation of such a framework, in the context of relations extraction and validation from \"a semantic data lake\", in order to improve the quality of data integration and gain insights from various data sources."
"is simply a photo coupling game. eight photos of pois are displayed; four of these photos must come from a trusted source. two must be taken from the set of candidate links, and thus are uncertain. two are distractors in order to check the reliability of players. in this paper, authors succeeded to prove that mobile gaming applications can be successfully employed to consume, create and improve urban-related linked data, creating high-quality links between existing datasets, (namely openstreetmap, linkedgeodata and flickr). as a conclusion we can see that different datasets can be created or improved via human computation approaches in the case of trade services, tourism, traffic optimization, environmental sustainability."
"in this section, we detail the general workflow (see fig. 5 ) that could solve the problem of integration of heterogeneous big data by using semantic technology and crowdsourcing. the implementation of this workflow encompasses the following main steps: data preparation, resource selection, data modeling, data instantiation and finally data linking. to address the quality issues of this automated process, the workflow is extended with a parallel crowdsourcing process. in short, semantic and crowdsourcing analysis are applied to the heterogeneous data at the entry of the workflow and the resulting extracted entities are stored in a knowledge base, inter-linked with linked open data resources. in more detail, the steps are: 1) data preparation: raw data may be dirty, inconsistent, or incomplete. thus, data preparation is the process of preparing (or pre-processing) data sources into refined information, which can be used effectively in data integration process. the data in this step need to be explored, organized, cleaned and augmented. 2) resource selection: choosing a target linked dataset to link with, requires extensive research work and a good analysis of the data source as well as the selected target. 3) semantic data modeling: the main common goal in developing an ontology is to share the domain knowledge of the structure of information. a lot of work has been put into the investigation of ontology alignment. in general, this topic can best be treated under three headings: ontology mapping, ontology merging, and ontology matching. all of them are significant methodologies in managing semantic heterogeneity. 4) semantic data instances assessment: semantic data instances may enclose logical inconsistencies, and syntax errors. in other cases, these data instances can be incomplete and need to be enriched with additional information. 5) semantic data linking: linking data with lod brings down the fences between various sources, but these links need to be created and validated. as we can see these tasks may not be fully automated. therefore, the support of crowdsourcing at each level can lead to an undeniable improvement of the quality of data and consequently of the quality of data integration outcome. more details at the conceptual as well as technical level will be elaborated in our future studies and implementations, primarily in investigating recent researches which merge crowdsourcing and nlp techniques."
"urban-related information and geographic data such as interesting facets of cities, street topology, road traffic conditions, business activities, points of interest, etc, are more and more studied in the linked data community. in paper [cit], the authors introduce the urbanmatch mobile locationaware game with a purpose, which tries to alleviate main drawbacks hampering a larger adoption of linked data in smart cities scenarios. the two main drawbacks are according to the authors: the doubtful quality of the available information thus making people distrust linked data content, and the lack of user-centered tools preventing people from contributing. urbanmatch engages players to provide information specific to the city of milano. it is aimed at linking points of interests in the city with the most representative photos retrieved from social media web sites and to rank those links, so to identify the most characteristic ones and to discard the others, thus improving the quality. the input data come from available web sources (cf. fig.3 ). points of interest in milano were collected and chosen among those available from openstreetmap; an rdf description of those pois is also available in linkedgeodata, the linked data version of openstreetmap. for each of the 34 pois of this set, 5-6 photos depicting them were manually selected. in this way, a trusted set of 196 links which relates the pois with their respective images was built. a higher number of photos of milano pois were collected from wikimedia commons and from flickr. this second set of information consists of more than 37,000 candidate links that relate the pois with the images that potentially depict them. this link-set is considered uncertain or untrusted. those candidate links are expressed as rdf links using the foaf:depiction predicate and are further annotated with a confidence value that expresses the lack of certainty about their trustworthiness (e.g., the initial confidence of links to wikimedia images is set to 60%, links to flickr to 40%). the game urbanmatch fig. 3 . input data sources and output links in urbanmatch [cit] ."
"after training, neural network grasps the relationship between input variables and output variables. finally the output could be predicted according to the input variables base on the trained model [cit] ."
3) use roulette algorithm as the selection operator. i p is the probability that the individual i is selected. i f is the fitness value of individual i. n is the population size.
"it's clear that linked open data can solve some of the problems of big data integration and reduce heterogeneity aspect of big data. however, the case studies demonstrate that the integration of linked open data is challenged by the several quality issues and problems that the linked data paradigm is facing. many of these quality issues require crowdsourcing techniques to be solved. as a result, all these case studies combine the use of lod as a semantic web technology with different crowdsourcing techniques to solve the big data variety. we can see that:"
"this determines that the grey prediction has a strong inertia without considering the randomness of the system. it is also not sensitive about the fluctuation trends and is more suitable for predictions data with certain characteristic features. in addition, it also includes far model [cit], garch model [cit] and so on. but none of these methods are suitable for the prediction of gold prices."
"input the last 10 days data to the three neural network models respectively that have been trained. their closing price prediction curve is shown by figure 4 and the relative error curve is shown by figure 5 . the detailed results of the predictive closing price and the predictive error are shown in table 7 . table 7 can be used to calculate average relative error about the three models and the specific results are shown in table 8 . from figure 4 and figure 5, the pca-ga-bp model that predicts gold price is more accurate than ga-bp and bp. table 8 shows that the pca-ga-bp model average relative error of prediction is only 1.637%, which is less than the ga-bp result which is 3.124% and bp result which is 5.018%. the pca-ga-bp prediction result is closer to the actual value."
"pca is a concept in statistics. through the analysis of original data to obtain the cumulative contribution rate, and then get the main component. that the reconstructed data retain the primary information of the original data, thus achieving the goal of reducing the correlation between the original data and reducing the data dimension [cit] . the original data are represented by the matrix x (n*p). the specific calculation steps are as follows:"
"selecting 110 [cit] /5/23 [cit] /11/3. the opening price, closing price, highest price, lowest price, change amount, change rate, trading volume and turnover as eight input variables."
"the algorithm proposed in this paper combines pca, ga and bp neural network. pca can simplify the network structure and reduce the dimension of input data. the genetic algorithm optimizes the weights and thresholds of the bp neural network, and overcomes the shortcoming that the bp neural network is easy to fall into the local minimum. bp neural network can predict the nonlinear relationship of gold price. the pca-ga-bp model could predict the price of gold accurately, which has certain reference significance in the financial field. in the next step, we will continue to improve the bp network on the basis of this research, and combine other algorithms to further improve the accuracy of the forecast price."
"the projects that were detailed in iii, and summarized in table i, are analyzed and compared according to the following characteristics: 1) domain: the problems of data integration that have been exposed in the current study arise each in a very distinctive domain e.g. geoscience research, sensor data, urban environments and biomedical image. 2) dataset: each of these projects treats very diverse data sources, ranging from structured (public transport timetables, professional society membership, job density,) semi-structured (yale image finder, openstreetmap,) and unstructured (flicker, conference abstracts, social media,). 3) semantic techniques: among these projects, the lod is a common technique that is used to integrate and access data at various levels of complexities. additional techniques are also used:"
"matrix r shows that the correlation coefficient between the first variable and the second variable is 0.9591, and the correlation coefficient between the second variable and the fifth variable is 0.9798. this shows that the correlation between these data is strong and the correlation needs to be reduced. table 3 ."
"the size of the population has a great influence on the global search performance of the genetic algorithm. therefore, the size of the population must be selected according to the specific problem. initial population size is 50."
"however, pca cannot obtain the non-linear relationship about data. bp neural network is a feed forward neural network [cit] . the bp neural network could be used as a good model for the gold price prediction due to its simple structure and easy operation, especially the ability of self-learning to realize any complex nonlinear mapping. when using bp network, it is not necessary to establish a specific mathematical model, and could find the optimal solution with iterativing the input and output data is directly. however, the bp network has the disadvantages of slow convergence rate, easily falling into local minimum, and oscillation near-optimal solution. ga can globally optimize the weights and thresholds of the neural network, obtain the approximate solution of the optimal solution. then the bp neural network can obtain the optimal solution, achieve the goal of global optimization. ga can solve the problem of bp neural network."
"we use the vehicle trajectory generator vanetmobisim to simulate the vehicle trajectory. vanetmobisim is an extension of canumobisim for vehicle mobile application, it can define vehicle trajectory and generate vehicle trajectory trace files needed for vmit simulation. the final simulation result of the experiment is the average of 100 test results. among them, the comparison algorithm introduced in this paper is a high mobility vehicle mobile internet of things (ldpr) based on improved path duration proposed by oliveira [cit] and an adaptive vehicle mobile internet of things based on adaptation method (aid) proposed by bakhouya [cit] . and the simulation conditions of all the algorithms are consistent in the experiment. the ldpr algorithm improves the communication duration of the path, reduces the probability of communication interruption, and improves the network communication efficiency by improving the network topology. the aid algorithm uses an adaptive data broadcasting method to reduce data transmission delay, reduce redundant data, and improve communication efficiency. the method of this article is to improve network communication efficiency by covering enhancements and solving the best communication durations. the ldpr algorithm is similar to the research method in this paper, and they all pay attention to the issue of communication duration. and the aid algorithm is different from the method of this paper, but all have the purpose of improving vmit network communication efficiency. so in the experiment, we use the ldpr algorithm and the aid algorithm to compare with our algorithm respectively, more to highlight the performance advantages of our algorithm. figure 8 (a) and (b) shows the average packet loss rate during vmit simulation under different numbers of vehicles. as can be seen from the figure, as the number of vehicles increases, the average packet loss rate gradually decreases. combined with data analysis in the figure, the vmit network has higher communication interruption probability than other networks due to the high-speed mobility and the moving direction of the vmit network. when the number of vehicles increases, the number of candidate next-hop nodes that can be selected within the communication range of vehicle nodes increases, they can choose to transmit data packets to the nodes closer to them to increase the communication duration, thus reducing the probability of interruption. the probabilistic analysis method adopted by panm can effectively calculate the relationship between the speed and the communication duration of the source vehicle and the adjacent vehicles so that the source vehicle can select the next-hop node that meets the requirement of the communication duration as much as possible to transmit data and reduce the communication interruption probability. ldpr algorithm adopts the principle of recent communication when the vehicle density is small and adopts the principle of optimal direction when the density is large. although the current path of the selected next-hop node and the source node is optimal, it lacks the correlation analysis between the speed and the communication duration. because the variability of the speed of the next-hop node, it cannot guarantee that the communication time can reach the required time for successful data transmission. aid algorithm has no specific analysis of the communication interruption time, and the packet loss rate caused by communication interruption is greater. ) shows the data transmission delay under the different number of vehicles. as can be seen from the figure, the delay time gradually decreases as the number of vehicles increases. when the number of vehicles is small, if there are no adjacent vehicles in the communication range of the source node, it needs to travel a certain distance, and data can be forwarded only when there is an adjacent vehicle. in this case, the transmission delay time is relatively large. and when the number of vehicles increases, the number of neighbor nodes in each vehicle node's communication range will increase, thus reducing the time to find neighbor nodes and reducing the delay time. when the vehicle reaches a certain number, each vehicle node at the same time has a neighbor node. at this point, even if the number of vehicles continues to grow, the delay time is no longer affected by the time of finding the neighbor nodes, which mainly depends on the routing strategy adopted by the algorithm. when adopting the routing strategy, the probability of communication interruption is large, the time consumed in data retransmission will increase, and the delay time will also increase, therefore, with the data in the figure, it can be seen that each algorithm is affected by the increasing number of vehicles; the delay time of vehicle communication is reduced. among them, the panm algorithm has a better communication persistence mechanism, with fewer delays and fewer retransmissions caused by interrupts. figure 10 (a) and (b) shows the total number of data packets received by the target vehicle under different vehicle numbers. after the target node receives the data of the source node during the simulation, it randomly selects the source node and the target node again until the end of the simulation. as can be seen from the figure, panm algorithm receives more packets in total. this is because the panm algorithm minimizes the overlapping ratio of the sensing range of the nodes by using the coverage enhancement method while ensuring the communication duration so that more data can be received than the ldpr and aid algorithms."
. (1) where a and b denote two subaperture images.ā andb denote the average of a and b respectively. the energy calculate algorithm is as follows:
"as showed in table 2, not all correlations of inter-view are higher than inter-frame in these test materials. we came to realize that for the residual videos the correlation calculation volume 8, 2020 may not be accurate. on one hand, the residual matrix contains more zero coefficients or coefficients close to zero. on the other hand, the correlation calculation algorithm starts to deteriorate when the difference between the frames is relatively large. the second problem also exists in central view videos, but it may not cause huge influence for it is the whole video. the inter-frames correlation of flowers, matryoshka and trees are very small. according to fig. 5, among all test materials, flowers, matryoshka and trees have relatively large difference in temporal domain. the correlation value is not accurate for these test materials under this circumstance. further analysis is needed to determine which coding method is better for the residual videos."
"in the semantic layer, we perform tasks such as filtering of content from social media, followed by a grouping of semantically similar sentences in a reddit post and a question from a questionnaire. we use two background knowledge sources to assess the entity-based semantic similarity of sentences and questions. for this task, we create four suiciderelated semantic clusters namely indicator, ideation, behavior, and attempt; each semantic cluster (or contextual cluster) represents different states(characteristics) of suicide or suicidal"
"my little brother possibly killed himself and let me tell you, its been months and i havent gone a day without sobbing and considering suicide and feeling like my ribs were splitting apart."
the performance of the proposed technique is compared with other existing techniques like histogram equalization and adaptive histogram equalization. fig. 4 shows comparison of the proposed technique with other techniques. table i & ii shows values of performance parameters using proposed technique against other existing techniques. fig. 4 shows enhancement results on input image using proposed dsr-based technique and other existing enhancement techniques table 1 comparative performance of the proposed technique with various existing techniques using two performance metrics f and pqm on two grayscale input images fig. 1(a) fig. 1(c
"without a semantic clustering scheme, where semantically similar sentences and questions are paired based on background knowledge, extracting an answer for a question from an entire reddit post using mere lexical matches leads to inaccurate responses [cit] . our novel approach of dividing a post into sentences provides better results because 1) sentences in a post that don't convey relevant information are removed improving focus in response extraction and 2) better responses are generated for a question if answer is looked for in the most relevant part of a reddit post. the figure 4 summarizes the statistics of the semantic clusters and intermediate dataset generated at this phase. it can be inferred from figure 4 that semantic cluster indicator contains the largest number of sentence-to-question pairs ( 30k) followed by ideation. semantic cluster attempt has the least number of sentence-to-question pairs. while c-ssrs does not include a section for indicator, our results can be used to expand the questions in c-ssrs and address suicidal indicator signs in individuals. this addresses issues of questionnaires such as c-ssrs where reliability and comprehensiveness concerns are raised [cit] ."
"further dsm-5 classes mentioned in table ii encompass various mental disorders [cit] . mental disorders such as borderline personality disorder (bpd) are categorized under class dicd. similarly, bipolar disorders are categorized under class depressive disorder."
"mathematically it has been proved that if the signal -to-noise ratio (snr) is maximized, it enhances the signal. benzi\"s double well theory suggests two states of image contrast, i.e. low and high also it is assumed that the particles are analogous to state of coefficient magnitude and particle\"s oscillation corresponds to number of iterations applied on dsr equation which is described as following [cit]"
"in order to analyze whether s-mvc is more efficient than t-mvc in most cases, we calculated the correlations of interviews and inter-frames for the central view videos of several test materials by using the method mentioned above. all test materials are showed in fig. 5 . the results are showed in table 1 . as showed in table 1, we can notice that correlation of inter-views is generally higher than inter-frames for central view videos. correlation of inter-views and inter-frames represent spatial correlation and temporal correlation respectively. according to this analysis result, we think s-mvc is more efficient for central view videos. relevant experiments are implemented to prove in experimental part. in order to find out the better coding method for the residual videos, correlations of inter-views and inter-frames for residual videos are calculated. the results are showed in table 2 ."
"in order to make the vehicle mobile internet of things enhance the network coverage while ensuring the continuity of communication, we propose the panm algorithm by analyzing the relationship between vehicle speed status, communication duration and network coverage under two common vehicle communication models. the panm algorithm first analyzes the vehicle spacing distance through a variety of motion conditions between the source vehicle and the target vehicle in communication and obtains the communication duration function. secondly, under the premise of ensuring the required communication time, we analyze the network coverage of the communication model. we use the method of increasing the distance between the nodes and adjusting the direction of vehicle sensors to improve the network coverage. in the simulation experiment, it can be seen from the comparison of the cases that the panm algorithm is used or not, respectively, we can see that the panm has a certain effect in enhancing the coverage of the vehicle mobile internet of things. [cit], she has been a lecturer with the department of information science, xinhua college of sun yat-sen university. she has published seven papers and holds one patent. her research interests include machine learning and intelligent algorithms."
"according to the content-based analysis conducted in section iii, we think s-mvc is more efficient when coding a plenoptic video directly. thus, s-mvc is always better than t-mvc for central view videos, this argument has been proved by the experimental results of this section. for the residual videos, s-mvc or t-mvc is selected adaptively by calculating the correlation and residual energy, similarly, the efficiency has been proved. therefore, the proposed method selects best method for both central view videos and residual videos, which make the proposed method outperforms traditional methods."
"in case the input is a grayscale image, omit the steps of color conversion, remaining steps are same for grayscale image. however this algorithm does not enhance bright image so it needs to be modify to enhance bright image by incorporating adaptive local neighborhood processing."
"the average packet loss rate is an important indicator to measure the data transmission efficiency of the vehicular network. the smaller the packet loss rate is, the higher the reliability and effectiveness of the in-vehicle network data transmission [cit] . the average packet loss rate γ is defined as: m packet represents the total number of packets sent by the source vehicle, and k packet represents the total number of packets that the target vehicle has successfully received."
"the time delay is a universal problem in the vehicular network, due to the high mobility of vehicle network, the degree of delay is more serious than that of other static networks [cit] . the smaller the time delay, the vehicle network is of higher value in the application. transmission delay t packet is defined as the current time at which the target node received the data minus the initial time at which the source node sent data."
"the existing methods for plenoptic video coding can be classified into two categories: methods based on predictive coding and multiview-based methods. many different methods based on predictive coding exist in the first category, they take advantage of different characteristics: displacement intra prediction mode [cit], disparity-based compensation mode [cit] and self-similarity compensated prediction [cit] . these methods can be used for both plenoptic video and image coding, however, they cannot fully exploit the optical imaging correlations among micro-images. some other work focused on the geometric relation among subaperture images and proposed homography transformation based methods [cit] . volume 8, 2020 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ these methods increase computational complexity, although they provide higher efficiency compared with the methods that coding plenoptic content directly using spatial coding tools in a video encoder like hevc [cit] . many multiview-based methods [cit] for plenoptic video compression in the second category have been proposed. in these methods, plenoptic video is decomposed into several viewpoint sequences, that is, a multiview video. some methods focused on prediction structures. in a prediction structure [cit], ipp or ibp structures are applied to each horizontal line and only the first central column of the views array. the number of available vertical inter-view predictions is very limited in such structures. in another structure [cit], additional vertical inter-view prediction is introduced, views are divided into different types based on its references. multiview video coding (mvc) can fully exploit both of the spatial correlation and temporal correlation among the subapertures. thus, it improves the coding efficiency greatly but also increases huge computational complexity at the same time, especially when the number of views is large."
"the second section introduces the vehicle iot network communication model, mainly introduces two kinds of on-board sensor radiation models, namely omnidirectional radiation and fan-shaped radiation models. the third section introduces the panm algorithm, which proposes a communication duration function equation to analyzes the moving speed and acceleration of the communication vehicle without interruption of the communication link, and proposes the network coverage enhancement method under the omnidirectional radiation model and the fan radiation model is proposed. section 4 conducted experimental simulations and data analysis."
"in h.265, the nal header contains three fixed length bit regions: nalu(t), nal-reference-idc(r) and hidden bits (f). the nalu type uses 5 bits to represent the 32 different types of nalu. types 1-12 are defined by h.265, types 24-31 are used outside of h.265. other values are reserved by h.265, which means types 13-23 are available. therefore, we take the type 13 as the flag bit with no extra overheads introduced."
"the popular \"suicidewatch\" subreddit has been widely used among individuals who experience suicidal thoughts, and provides significant cues for suicidality. the timeliness in sharing thoughts, the flexibility in describing feelings, and the interoperability in using medical terminologies make reddit an important platform to be utilized as a complementary tool to the conventional healthcare system. as mhps develop an implicit weighting scheme over the questionnaire (i.e., c-ssrs) to assess suicide risk severity, creating a relative weighting scheme for answers to be automatically generated to the questions in the questionnaire poses as a key challenge."
"where g l is transmitter antenna gain, g r is receiver antenna gain, h l is transmitter antenna height, h r is receiver antenna height. we assume that the maximum loss threshold that the link can tolerate in the communication model is tl, and the probability distribution function of the link loss allowed in the communication is:"
"for our semantic clustering task, we use background knowledge sources built from snomed-ct [cit], icd-10 [cit], umls [cit], and datamed [cit] to determine the classification of a sentence in a reddit post and the questions in our questionnaire. the background knowledge sources we used as a part of this study are suicide severity lexicons 4 (see footnote) and dsm-5 lexicons [cit] . the key contributions of this study are: 1) an automated social media content elicitation framework based on semantic clustering and background knowledge; 2) a cohort of 16 questions developed by practicing clinical psychiatrists to assess suicide risk on social media; and 3) a gold standard dataset of 4992 reddit suicide posts and their suicide risk level."
"in this paper, an image enhancement technique using dsr is presented. the dark as well as bright images are enhanced using dsr. the results of the proposed technique are quite satisfactory as compared to other techniques in terms of enhancement and visual information."
"it is assumed that vehicles on the traffic lane need to communicate with each other, to obtain relevant data information of the other vehicle. figure 1 shows a vehicle model with randomly distributed vehicles when the radiation methods of the vehicular sensor is an omnidirectional radiation. and figure 2 shows a vehicle model with randomly distributed vehicles when the radiation method of the vehicular sensor is a fan-shaped radiation. in both models, r represents the sensor's sensing radius, and l represents the spacing distance between two vehicles. assuming that the vehicle network communication in the model adopts the dsrcs standard, the maximum communication distance is 300m. when the distance between vehicles exceeds 300m, the communication link is immediately interrupted [cit] . in the model, the shaded area indicates the overlap area between the sensor sensing range of the vehicle. is divided into n intervals between the source vehicle and the target vehicle."
"proof: lemma 1 can prove that t can be expressed as the square root function of l 0, lemma 2 can prove that the square root function of l 0 submits to the lognormal distribution, so lemma 3 holds."
we conducted our study by dividing the problem into three phases as shown in figure 1 . this architecture is inspired by 6 table ii : dsm-5 lexicon the notion of web-based intervention where digital markers from social media are used for suicide prevention.
"all possible s-mvc and t-mvc combinations are compared in table 5 . first, a conclusion can be drawn that s-mvc is more efficient for the central view videos by comparing the csrs and ctrs, csrt and ctrt. csrs and csrt cannot be compared because different prediction structures (psb for csrs, lf-mvc for csrt) are adopted for their residual videos. in order to solve this problem, an extra method, csrs(lf-mvc), is added. thus, csrs(lf-mvc), csrt and the proposed method share the same prediction structure for both central view videos and residual videos. by comparing the results of these three methods, it can be found that the proposed method always chooses the more efficient method from s-mvc and t-mvc adaptively in all test materials, which proved the efficiency of the proposed method. overall, the proposed method outperforms psb by an average of 25.06% bitrate saving."
1. an estimate of relative contrast enhancement factor (f) can be obtained by computing ratio of values of quality index post-enhancement (qb) and pre-enhancement (qa). therefore
"however, the research work for enhancing coverage area of the vehicle network under the condition of guaranteeing the normal communication of vehicle internet of things area, which is still very rare. and with the continuous development of intelligent transportation system, the application demand of vehicle network is indispensable [cit] . how to increase the network coverage to expand the scope of data collection and reduce the overlapping area to save network energy will become an important issue for vehicle network research [cit] . as the more mature land-based wireless sensor network technology, the problem of network coverage enhancement has also become an important aspect of the study of terrestrial wireless sensor networks [cit] ."
"reddit is an invaluable resource of symptom markers (e.g. signs of hopelessness, anger, anxiety) that can be used to augment the passive assessment of patients by mhps [cit] . reddit has been used by individuals to freely share their symptoms, disorders, and experiences with no thoughts of social stereotypes. the fact users anonymously and candidly disclose their feelings provide the convenience of having timely healthcare signals [cit] . one of these signals shared on reddit is related to suicidal thoughts. this makes the data more reflective of the suicidal marks the user has experienced at the specified point in time making intervention effective. thus, reddit is an untapped resource for monitoring and studying suicide at a community level, offering benefits related to enforcing policies at a large scale. a framework that uses and analyzes data from reddit enables mhps to quantitatively assess individuals' level of suicide risk. we address the challenge of rating the suicide risk of reddit online posts as a question answering problem using sets of questions developed by practicing clinical psychiatrists. since c-ssrs is developed for an mhp interacting with a patient and is limited in scope [cit], we had our clinical psychiatrist collaborators produce a novel questionnaire that is complete and linguistically adaptable with reddit content [cit] . we approach the reddit posts elicitation study using our suicide risk assessment questionnaire as text skimming problem using our novel semantic clustering technique. we build four semantic clusters where semantically similar sentences in a reddit post and questions in a questionnaire are grouped together. the semantic clusters we build are inspired by the scope of the suicide lexicons used in this study (suicide indicator, suicide ideation, suicide behavior, suicide attempt). we weight our semantic clusters based on their suicide grades which are used in assessing the aggregate suicide risk severity of a reddit post."
"there is a commonly used method in video coding to measure the residue, which is to calculate the residual energy. as showed in table 3, we adopt (2) to calculate the energy of residue with the same method described in correlation calculation. theoretically speaking, the higher the correlation, the smaller the residual energy. the results of table 2 and table 3 are perfect match except for flowers, cube and trees. so, we think the correlation of inter-frames can be a sign: when its value is super small, it presents low redundancy in temporal domain. s-mvc is better for residual videos under this circumstance. for test materials that have relatively more redundancy in temporal domain, choosing coding method based on residual energy is more reasonable."
"in this paper, an efficient content adaptive plenoptic video coding method is proposed. based on the spatial correlations among subapertures, the plenoptic video is divided into two subaperture groups: the central view videos and the residual videos. the central view videos are encoded by s-mvc and the residual videos is encoded by s-mvc or t-mvc adaptively based on correlation and residual energy analysis. experimental results demonstrate its superior improvement in compression efficiency relative to traditional mvc methods. the future work will focus on decreasing the number of views in the central view videos to further improve the compression performance."
"two plenoptic video compression methods exist at present, we define them as s-mvc(multiview coding method based on spatial correlation) and t-mvc(multiview coding method based on temporal correlation), as depicted in fig. 1 and the multiview video is encoded as input by standard mvc. in terms of spatial domain, a problem exists in t-mvc is that coding efficiency begins to reduce significantly when the video content moves a lot. however, this problem has no effect on s-mvc, because there is only a difference in perspective between subaperture images, and the spatial correlation is relatively stable. it is this problem that makes s-mvc more efficient in most instances."
image enhancement is an approach to improve the quality of images in terms of human visual perception. it is the process of manipulating an image so that the result is more suitable than the original for a specific application [cit] . it is one of the most important area of digital image processing. large number of techniques are used for enhancement of images in the spatial domain. dynamic stochastic resonance (dsr) technique can also be used for image enhancement in spatial domain.
"plenoptic camera inserts a microlens array between the main lens and the sensor, allowing the sensor to capture intensity and angular information of the light rays. after passing through the main lens and microlens, the light rays in different direction from one object point are recorded by the sensor as a group of pixels [cit], called macropixel or an elemental image (ei). the number of macro pixels is related to the number of lenses in the microlens array. subaperture images are extracted from plenoptic image according to ng's optical analysis [cit] and light field decoding [cit] . repeating the extract process for each frame of plenoptic video, a fixed number of subaperture images are generated in each frame. in mvc, a subaperture image represents a view. thus, a complete multiview video is generated by concatenating every view of the same position in each frame into a sequence respectively. in traditional mvc methods, the complete multiview plenoptic video is used as input for encoding directly. in this paper, several different ways for rearranging plenoptic video are used, more details are described in experimental part."
"to demonstrate the efficiency of the proposed compression method, several sets of experiments have been conducted in this section. first, the efficiency of s-mvc and t-mvc, psb and lf-mvc are compared. then, the efficiency of a special method, which is combination of hevc and mvc method using subaperture projection-based grouping [cit], is explored. last, the efficiency of proposed method is proved by comparing all possible s-mvc and t-mvc combinations."
"subaperture images are extracted from plenoptic image using light field decoding toolbox [cit] . the color space is converted from rgb to ycbcr 4:4:4 in the subaperture extraction process. according to the standard recommended by mpeg [cit], 165 views in s single plenoptic image are used for compression. 25 subaperture images are classified as central view images and 140 subaperture images are classified as adjacent images according to subaperture projection-based grouping [cit] . accordingly, 140 residual subaperture images are obtained in a plenoptic image. two mvc prediction structures, pseudo-sequence-based (psb) [cit] and ''lf-mvc'' [cit], are adopted in this paper."
"where α, β, γ1, γ2 and γ3 are model parameters that were estimated with the subjective test data [cit] b is the average blockiness, estimated as the average differences across block boundaries for horizontally and vertically. a and z constitute the activity of the signal."
"in this interdisciplinary study, we position our approach towards a solution for an automated suicide risk-elicitation framework through a novel question answering mechanism. our two-fold approach benefits from using: 1) semantic clustering, and 2) sequence-to-sequence (seq2seq) models . we also generate a gold standard dataset of suicide posts with their risk levels. this work forms a basis for the next step of building conversational agents that elicit suicide-related natural conversation based on questions."
"when the vehicular network carries on the collection of road environment data, the total amount of data packets that the target node finally gets is an important index to evaluate the efficiency of the vehicle network task execution [cit] . the total number of received packets p total indicates the number of packets received by all destination nodes after the simulation."
"2, or c-ssrs 3 are widely used by the clinical community to assess suicide risk, they can be prone to subjective bias among mhps. a commonly used suicide risk assessment scale, c-ssrs, does not adequately address completeness and relative weighting of questions which can lead to inaccurate risk evaluation [cit] . mhps use their personal experience to understand and give different levels of severity to patient responses to questions in their suicide risk evaluation task. further, the public social stigma on the subject of suicide deters patients from disclosing their symptoms where a questionnaire is used in a direct mhp-to-patient interaction."
"step1:-convert rgb image to hsv color space step2:-apply dsr step3:-compute rgb image after each iteration step4:-calculate the performance metrics f(n), pqm(n) and cef(n) after each iteration. the iterative process is continued till f (n) + cef (n) becomes maximum within the constraint that pqm is as close as possible to value 10."
"abstract-mental health america designed ten questionnaires that are used to determine the risk of mental disorders. they are also commonly used by mental health professionals (mhps) to assess suicidality. specifically, the columbia suicide severity rating scale (c-ssrs), a widely used suicide assessment questionnaire, helps mhps determine the severity of suicide risk and offer an appropriate treatment. a major challenge in suicide treatment is the social stigma wherein the patient feels reluctance in discussing his/her conditions with an mhp, which leads to inaccurate assessment and treatment of patients. on the other hand, the same patient is comfortable freely discussing his/her mental health condition on social media due to the anonymity of platforms such as reddit, and the ability to control what, when and how to share."
"in this paper, according to the communication distance limit under the dsrcs standard in the actual application scenario, we obtain a function of the vehicle communication duration by analyzing the vehicle speed relationship within the communication critical distance. in order to improve the robustness, this function includes the relationship between the distance and the duration of the vehicle in the variable speed state. when the communication between the vehicles needs to satisfy a certain time, the current distance between vehicles can be adjusted according to the function to ensure that the communication link will not be interrupted. in order to increase the coverage rate of vehicle mobile internet of things, we introduce the concept of vehicle network coverage overlap ratio and analyze the coverage overlap ratio of vehicle mobile internet of things under omnidirectional radiation and fan-shaped radiation communication models respectively. we derive the distance between the vehicles and the orientation angle of the sensors at the minimum overlap ratio of the two communication models and give a proof of this."
"to verify the performance of the proposed technique, dynamic stochastic resonance (dsr) is applied on some low contrast images. fig. 1(a) shows original dark input image on which dynamic stochastic resonance (dsr) is applied to enhance it and resultant enhanced output is shown in fig. 1(b) . similarly, fig. 1(c) shows original low contrast input image on which dynamic stochastic resonance (dsr) is applied to enhance it and resultant enhanced output is shown in fig. 1(d) . in fig. 2(a), one color image is taken on which dynamic stochastic resonance (dsr) is applied and its enhanced output is shown in fig. 2(b) . in this paper, the proposed technique is also applied on bright images. fig. 3(a) and fig. 3(c) show bright images and fig. 3(b) and fig. 3(d) show respective enhanced output."
"all test materials are captured by lytro camera. as depicted in fig. 5, there are 30 frames in each plenoptic video. row 1, 2 and 3 represent the 1st, 15th and 30th frame of each video, respectively. the motion modes of test materials are very different: the content of flowers moves down in the both vertical direction and horizontal direction; the content of auto race is a toy car that moves diagonally; the cube captures a rotating cube; the content of matryoshka is a doll that moves down in the horizontal vertical direction, same with the trees. the content of toys is a combination of auto race and cube. different contents and different change mode of content ensure the sufficiency and rationality of the experiment."
"in this position paper, we begin with motivating scenarios where challenges to process social media text are discussed through examples on suicide and related mental disorders (section ii). we then briefly go through previous studies conducted that align with our work as stated in the related work (section iii). we proceed onto a detailed description of the data used in our two-fold approach (section iv). finally, we lay out the methodology to address the challenges stated in the motivating scenario (section v). we conclude with the broader implications of our study (section vi)."
"the remainder of this paper is organized as follows. section ii introduces the generation process of plenoptic video and two widely used methods for encoding plenoptic video. content-based analysis of plenoptic video, the workflow and details of proposed plenoptic video compression method are introduced in section iii. in section iv, we conducted experiment to explore its compression efficiency, we also did some comparative experimental analysis. section v concludes the paper."
"compared with videos captured by traditional cameras, plenoptic videos captured by plenoptic photographic devices record not only intensity information of light rays from the associate editor coordinating the review of this manuscript and approving it for publication was xinfeng zhang. different direction but also the intensity variation in temporal domain. therefore, plenoptic videos have huge data volume, which brings great difficulty to transmission and storage. in addition, complex subaperture relationship and huge data volume brings great challenges to compression. thus, an efficient compression method designed for plenoptic video is highly desired."
"given that l i is a random variable with log-normal distribution, therefore a + √ b + cl 0 obeys a lognormal distribution. lemma 3: communication duration t submits to lognormal distribution."
"noise is generally considered as nuisance i.e. it is unwanted signal. in past, the emphasis was how to reduce or eliminate noise .some researchers recently discovered that there exists a potential function when extracting useful signal from noise which increases the signal intensity ratio significantly. this is stochastic resonance [cit] . it is a phenomenon in which noise can be used to enhance rather than hinder the system performance. therefore, noise can play a constructive role in enhancing weak signals. in the proposed work, the internal noise of an image has been utilized to produce a noise-induced transition of an image from a state of low contrast to that of high contrast. dynamic stochastic resonance (dsr) has been used for enhancement of dark images previously [cit] . however, in this paper, dynamic stochastic resonance (dsr) is used for enhancement of both dark as well as bright images."
"index terms-semantic social computing, suicide risk assessment, reddit, c-ssrs, the diagnostic and statistical manual of mental disorders -fifth edition (dsm-5), web-based intervention, semantic machine learning"
"lemma 4: for an arc length and the radius of the ends forming the perimeter of fan-shapeda, when the rotation range of the radiation direction of the sensor b is limited to the left end radius ofa, as shown in figure 5 (b), s(h 2 ) is the minimum coverage overlap area that can be obtained in this case."
"having generated the intermediate dataset, we use conceptnet 7 numberbatch embeddings which consists of 417193 concepts to represent a sentence in a post and a question. we generate a 300-dimensional embedding for each term in a sentence and take the average of the embeddings to generate the 300-dimensional embedding for the sentence. we did the same for a question where a 300-dimensional embedding for each term in a question is generated and took the average of the individual terms to compute the embedding representation of the question. we used the same pre-trained embedding model for both the sentences and the questions. after generating 300-dimensional embeddings for a sentence and a question, we performed dot-product of the embeddings representing a sentence and a question to generate a combined 300-dimensional embedding which is a representation of a sentence-question pair. this 300-dimensional embedding representation later is used as an input feature for the learning phase which we build."
"several groups of methods are compared in table 5 . first, as shown in the column hss, mvc method is quite more efficient for hss presents an average of 80.04% increase in bitrate. then, s-mvc is more efficient when coding a complete plenoptic video directly according to the column t-anchor. however, things are different in chrs and chrt because of the similar average results, 17.35% and 17.74%. in fact, t-mvc is more efficient in test materials (c) and (e). in addition, lf-mvc is more efficient than psb(anchor) in 5 of 6 test materials."
"the decoding process of our proposed method is shown in fig. 4 . the central view videos bitstream is decoded by s-mvc. for the residual videos bitstream, the flag bit is checked first. s-mvc or t-mvc is chose adaptively for the residual videos bitstream based on the flag bit. by plus the decompressed central view videos to the decompressed residual videos, the complete decompressed plenoptic video is obtained."
"link loss is represented by loss (l). as vehicular sensor uses wireless communication technology, when eliminating external interference, the attenuation of signal mainly considers the influence of link distance [cit] :"
"we assume that there are j kinds of cases in which the vertical o has not been passing through the overlapped area. taking figure 7 as an example, supposing the vertical o and the area b are separated by an angle φ, the area covering the overlapping area is:"
"in the experimental part, we use the network simulation tool ns-2 to build a simulation scene, the driving data in the experiment process adopts the method of random extraction. for example, in this experiment, we randomly selected the driving data of xiaogang area and shayuan area of guangzhou, china. the road scene has a large traffic volume, a large number of forked sections, and a complex vehicle environment, which has a certain representative value as a test scenario for vehicle network simulation. vehicle information statistics are as follows:"
"in this study, we present an automated solution that will complement the traditional diagnosis and treatment procedures followed by mhps for assessing a patient's suicide risk. we highlight the contribution of rich content of reddit posts in understanding of suicide risk for patients. we further explain the technical challenges in processing and analyzing reddit posts. as we present an automated suicide-risk elicitation framework for reddit, we plan to apply our approach over other social media platforms such as twitter as our future work. furthermore, our framework can be extended to solutions for broader healthcare problems involving multi-modal data, and can also be integrated with intelligent conversational agents or chatbots to reduce patient's suicide risk level [cit] ."
"once a sentence-question pair representation is generated, the next task is to build a model that can give responses to 7 http://conceptnet.io/ the individual pairs. we run a series of machine learning and deep learning experiments and evaluate the performance of each model against a truth dataset of the same posts annotated by practicing clinical psychiatrists. since our study deals with sequential stream of text data, where a sentence in a reddit post and a question should both be accounted for the sequences of words, our seq2seq models perform better based on the evaluation metric we defined. we perform ranking of responses to individual sentence-question pairs to generate a response to the parent reddit post."
"the encoding architecture is proposed in fig. 3 . as depicted in the architecture, the plenoptic video is divided into the central view videos and the adjacent videos according to the subaperture projection-based grouping [cit] . first, the central view videos are encoded by s-mvc. then, the residual videos are generated by the difference between the reconstructed central subaperture videos and the adjacent subaperture videos. encoding methods between s-mvc or t-mvc are chose adaptively for the residual videos according to correlation and residual energy calculation. a flag bit is set to record the encoding method selected by the residual videos. the adaptive selection process is as follows: correlation analysis is performed first, s-mvc is adopted when interframe correlation is smaller than an empirical threshold. otherwise, residual energy analysis is implemented. then, residual energy among frames and views are compared, t-mvc is adopted when inter-frame energy is smaller and s-mvc is adopted in the opposite case. in addition, ''s-residual'' video is transformed to ''t-residual'' video by exchanging the spatial domain and temporal domain before encoded by t-mvc. finally, a central view videos bitstream and a residual videos bitstream are obtained."
"where e denotes energy between a and b, a and b denote two residual subapertures. we calculate the correlation between views and frames respectively according to (1) . taking a plenoptic video that containing 30 plenoptic images for example, there are 25 central views in each plenoptic image after grouping. calculating the correlation between all views and frames will introduce a huge amount of computational complexity. in fact, we calculated the correlation between all views and frames of several different materials, finding that correlation between the views in each plenoptic image and correlation between frames in each view are very close. therefore, selecting several plenoptic images or views randomly can be used as a representative."
"if i had a gun id be dead already.a good shotgun or high-caliber handgun would do the job, and id do it without hesitation. killing myself would be far, far easier than facing, let alone actually dealing with, any of the bullshit."
"rapid growth of wireless technology has increased the wide popularity of mobile ad hoc networks. however, frequent disconnections, limited energy reserves and bandwidth constraints, makes efficient data access in ad hoc networks a challenging task. connection refusals due to lack of resources occurs when the server workload is increased. these problems are exacerbated by the dynamic topology of ad hoc networks. data caching is recognized as feasible approach to improve the performance in many traditional systems. caching is the process of pre-fetching the needed data and storing it closer to the source. in mobile ad hoc networks, caching can lead to significant bandwidth savings, perceived network latency reduction and ensures higher data availability. data caching in ad hoc networks are mainly proposed as cooperative caching [cit] . cooperative caching aims to reduce the redundant data transfer using a mechanism which enables the local cache of different mobile clients to be shared in a cooperative manner. in cooperative caching the mobile clients are configured to request the data object from its local set of data items, if not found queries its neighboring nodes. when there is a cache miss in the neighboring nodes queried, the data item is retrieved directly from the server and this procedure continues recursively."
"our evaluation considers the algorithms to derive intrinsic images introduced by weiss [cit], land and mccann [cit], and blake [cit] . because all of the algorithms are based on the concept of intrinsic images, we review it first. then, we describe in some detail each of the algorithms evaluated. lastly, because the algorithms were primarily created to compute intrinsic images, we illustrate how they could in principle be used to detect shadows."
"(2) if the support predicate indicates passive voice(e.g. 'doi'), then the nominal functions as the subject of the predicate. so, we insert a subject case particle --'i' or 'ga'."
"an alternative elaboration of this technique attempts to retain original text meaning, and proposes to replace words in the cover text with synonyms [cit] . however, there is deterioration in documents in which importance is attached to any delicate semantic nuance when synonyms are substituted. there are also cases where incorrect words are selected as synonyms among many synonym candidate words. moreover, the method requires a large synonymy dictionary and a huge collocation database [cit] ."
"in cooperative caching if data replacement decision is made by individual nodes by considering only their local cache, the performance is degraded because the data may be present in the neighboring nodes. in order to cache more distinct data, new data item fetched from adjacent nodes are not cached. when the cache is full, appropriate data from the cache have to be evicted to make room for the incoming data. the replacement policy proposed here considers the number of references for a particular data item and gives more emphasis data items that are referenced more than once. if we have data items referenced only once then that set is given priority for replacement. for this lru policy is used. if an item is referenced more than once the inter arrival time between the recent two references is considered for eviction."
"note that korean, as an agglutinative language, differs significantly from indo-european languages such as english, because one word consists of several morphemes. for this reason, we believe that korean and other agglutinative languages provide a good basis for text watermarking based on division of a word by the characteristics of its morphemes. in addition, most languages permit the free order of a syntactic adverbial within its clause boundary. we exploit these two characteristics for text watermarking."
"we propose natural language watermarking for korean based on morphological division and syntactic displacement. by using the characteristics that a word in the agglutinative language usually consists of several morphemes, we divide the word that has two content morphemes into two new words, and a new function morpheme is inserted for the first new word that does not have a function morpheme. we also use the property that syntactic adverbials can be displaced within their clause boundaries. to improve watermarking capacity, we adopt a sentence weight value and make the weight carry a watermark bit. the experimental results show that the coverage of our method is 82.17%, the average edit rate is 7.70%, and the watermarking capacity is 0.86 bit/sentence, outperforming previous systems."
the value of ranges between zero for uniform distribution and one for strict zipf distribution. in our study is set to 0.8. the data request is processed in fcfs manner at the server. an infinite queue is used to buffer the request when the data center is busy. each miss in the cooperate cache will incur a delay of 8 ms to retrieve data from the data center.
"although the korean language admits relatively free word order, the boundaries over which a word can move is limited. we can displace a word within the clause that it belongs to. adverbial, rather than other constituents, can move more freely in a sentence without semantic distortion [cit] . therefore, as a target node for movement, we choose an adverbial constituent from a syntactic tree. we consider the displacement position from among the positions of the nodes at the same level hierarchy of the adverbial node in a clause. two kinds of displacement directions exist: left and right. in this study, we only consider moving to the right position. we choose the first syntactic adverbial that can be moved to the right position in a sentence, and move the adverbial to the right nearest position. in fig. 3, 'mi-sul-gwan-e-neun' is an adverbial, and it moves to the right nearest position. finally, from the modified syntactic tree, we generate a marked sentence as shown in the bottom of fig. 3 ."
"here, we propose a text watermarking method that combines morphological division and syntactic displacement, to improve coverage of watermarking. to improve system security, we assign a sentence weight for each sentence, and the watermark bit is hidden in the weight value."
"so far, optimal network structures for synchronization have been studied mainly by using monte carlo methods [cit] or gradient-based learning strategies [cit] . these are based on the use of some objective function for synchronization (as for example the order parameter) which is used to find the optimal network whose structural properties are then surveyed. the monte carlo approach is a generic and powerful strategy but it is typically time-consuming, and increasingly cumbersome to apply to large-scale networks. gradient-based methods assume some constraints to derive the evolution rule of the coupling strengths and the rules are often not local, in the sense that some global information on the entire network is used. also, it has been shown that adaptive networks can yield the emergence of modular and scalefree structures, while enhancing synchronization [cit] ."
"we measured the following performance characteristics. table 1 shows the rate of unsuitable sentences among marked sentences and that among untransformed sentences. it is also interesting to note that sentences which have not transformed have also received edit hits at a rate of 8.07%, implying that the edit hits between marked sentences and non-marked sentences are not so different. the average edit rate is 7.70%, which shows worse result than that of h. m. [cit] as shown in table 3 . however, the editing rate is very subjective, and we should also consider that the language and length of sentences are different between two methods."
"for algorithm comparison, we used two data sets, one synthetic and one real (see figure 1 ). the synthetic data set consists of two sequences and has been used for quantitative evaluation; meanwhile for the qualitative evaluation, we used real and synthetic sequences. to build the data sets, we considered only daytime, when the sun is by far the main source of light. it is important to mention that in scenarios lighted with other sources, such as fluorescent or street lights, the shadows cast by static objects will be always static and perhaps only changes in intensity will be perceived. in the next subsections we present details regarding the generation of these data sets."
"wherer andl are the reflectance and illumination time-varying reconstructed images, f m is the inverse filter of f m, and g is the filter which satisfies the equation:"
"a serious limitation in the systematic evaluation of algorithms to detect shadows cast by static objects during extended periods of time is the lack of a standard data set with annotated ground truth. based on this fact, we used two synthetic sequences that simulate the changes in the sun's position over a long period of time (days) for a particular geographical position. the advantage of using synthetic images is that the ground truth is automatically generated."
system model we assume a mobile ad hoc network with a set of nodes which are able to communicate with each other. the transmission radius r determines the maximum communication range of each node and is equal for all nodes in the network. two nodes in the network are neighbors if the euclidean distance between their coordinates in the network is at most r. the euclidean distance between the nodes are estimated based on the relative position of nodes. we assume that each node knows its current location precisely with the availability of global positioning system (gps).
"when a node wants to access data, it checks in its own local cache. if the requested data is not cached, the node checks whether the data is present in the neighboring nodes. if we are not able to find the data from the neighbor list the request is given to the data center."
"we conclude that our watermarking method based on morpheme division, syntactic displacement, and the adoption of a sentence weight value is useful in watermarking of korean text. of course further experiments will confirm this further, and we will try to apply our method to other agglutinative languages to demonstrate that this method is effective on other languages."
"in fig. 2, we can see that a subject case particle is inserted in the first new word because the support predicate('doi') exhibits a passive voice."
"the remainder of the paper is structured as follows. section ii reviews the related works in cache resolution, section iii presents the system architecture of the proposed cooperative caching scheme, section iv describes simulation model and metrics, section v gives the experimental results and section vi concludes the paper."
"initially, the mobile nodes are randomly distributed in the simulation area. after that each node randomly chooses its destination with a speed s which is uniformly distributed u (v min,v max ) and travels with that constant speed s. when the node reaches destination, it pause for 200 seconds. after that it moves to the new destination with speed s'. for the time out mechanism each node maintains two time out period for the primary and secondary zone. the details of the simulation parameters are given in table 1. mean query generation time 10s table. 1 simulation parameters b. metrices the performance metrics evaluated includes cache hit ratio, message overhead and average query delay. the evaluation of these parameters are done by varying the number of cache locations with respect to number of nodes and the behavior of cache hit ratio for different cache sizes. the hit ratio is defined as the percentage of requests that can be served from previously cached data. since the replacement algorithm decides whether to cache the data or not, it affects the cache hits of future requests. the query delay is the time interval between the query sent and the data transmitted back to the requester. average query delay is the query delay averaged over all queries. message overhead is the overhead messages needed to manage the cache discovery process in cooperative cache."
"our results clearly show the role of node heterogeneity in inducing functional structures using an evolutionary strategy for network synchronization. in particular, differences in the node dynamics do influence the evolution of the network determining a differentiation in the link activation probabilities that is instrumental to obtain minimal structures with relatively high values of the order parameter. also, hubs tend to emerge there where the distance from the average natural frequency is highest. further simulations also confirmed that a similar structure of the emergent network can be induced by using a power-law rather than a normal distribution when selecting the heterogeneous natural frequencies of the oscillators (data not shown)."
"in this section, a quantitative assessment of the methods to detect shadows cast by urban infrastructure, based on the derived illumination image, is presented. to evaluate the performance of the methods systematically, we used the receiver operating characteristic (roc) analysis. for our evaluation, we used forty synthetic images (samples of these images are illustrated in fourth, fifth and sixth rows in figure 5 ). the ground truth was computed automatically by subtracting the shadows from the shadowless image. first, the intrinsic image derivation methods were used to compute the reflectance and the illumination component of each frame. then, a segmentation process based on the histogram analysis was applied. the curves in figure 6 represent the false positive shadow detection rate on the horizontal axis and the shadow detection rate on the vertical one. figure 5 . qualitative results in the synthetic data set. the first and second rows are input and ground truth of shadows. the third, fourth, fifth, and sixth rows are the results of the weiss [cit], gray [cit], and color retinex methods [cit], respectively. figure 6 . roc curves for the weiss [cit], gray [cit], and color retinex methods [cit] . the vertical lines define the four regions where the algorithms are compared."
"korean words usually consist of a content morpheme and a function morpheme. however, predicate nominals are exceptional and typically consist of two content morphemes-nominal and predicate-together with one function morpheme. a predicate nominal is simply a predicate that is derived from a nominal. so, this paper proposes text watermarking by dividing a predicate nominal into these two components: a nominal and a predicate. we also try to transform a sentence by displacing syntactic adverbials. we embed this style of watermark in an original text, creating a ciphertext based on morphological division or syntactic displacement according to the choice of the watermark selector, which preserves the meaning. to make the system even more secure, we use a sentence weight and make the weight value carry a watermark bit."
"(1) morphological and syntactic analyses are performed for all the sentences in the whole text, during which watermarking applicability is determined for each sentence. (2) the weight value wj for a sentence sj is computed as follows. sentence weight w for a sentence s is defined by:"
"in this paper, we propose the use of an evolutionary strategy to find a functional structure for synchronization in a network of heterogeneous oscillators. in so doing we will show that heterogeneity in the nodes is instrumental in determining the properties of the resulting network. the goal of the strategy is to identify, over all possible unweighted network configurations, the structure with a minimal number of links, which guarantees frequency synchronization of its nodes. while the fundamental aim of our study is similar to that of the literature [cit], the approach we propose is completely different. indeed, our strategy uses adaptive schemes which are completely local and do not rely on any global synchronization measure. moreover such schemes are deployed in a novel evolutionary manner."
obtained from an exhaustive search and a monte carlo based method [cit] maximizing the value of r with the constraint that the total number of edges m is equal to 7. we notice that the two networks share the same links.
"a primary goal of many computer vision algorithms is to attenuate the effects caused by shadows. due to several factors, the problem of shadow detection is a complex and open research field. in this paper, we presented an evaluation of several intrinsic image base methods to detect shadows cast by static objects in outdoor locations."
"we tuned netevo to find an optimal structure, given an initial condition (the same used in the procedure for finding the minimal structure). simulated annealing tends to avoid local minima (or maxima), so we could start the optimization from any random connected network structure. however, we decided to start \"near\" the minimal structure, to facilitate the optimization (by near, we mean a structure obtained from the minimal structure, after rewiring about 10% of its edges)."
"a shadow is the result of an opaque object obstructing light which otherwise would directly illuminate a surface. shadows are present in almost every computer vision application, where they may give rise to undesired effects in methods including segmentation [cit], recognition [cit], and tracking [cit] . the main problem is that their existence may alter our interpretation of the scene, making our models drift-sometimes up to the point of failure. consequently, it is desirable to detect them and to attenuate as much as possible their negative effects [cit] . however, in some situations their presence is attractive as they may help to obtain 3d information for scene reconstruction [cit], for instance."
here we suggest that heterogeneity in the nodes is a driving force behind the evolution of the network structure that determines its properties and function. to test this ansatz we take as a representative example the problem of evolving the network structure to achieve synchronization of coupled oscillators. this is one of the best understood and most widely studied type of collective behavior on networks [cit] .
"where g(ω) is the probability density function of a given distribution. it should be noted that for a large network, we performed our simulation taking the natural frequencies randomly from a distribution and the obtained results are qualitatively the same. next, we investigate how the evolution of the network is affected by tuning the heterogeneity in the nodes. to this aim we use the edge snapping strategy described above in a novel evolutionary manner (see fig. 1 ) as explained in the next section."
"to evaluate the performance of the proposed cache discovery and cache replacement scheme, we compared the performance of our new cooperative caching scheme (ccn), with neighbor caching (nc), a caching scheme which uses broadcasting and lru for cache replacement. fig 1 shows the message overhead for two schemes, as a function of different node densities. the figure shows that ccn outperforms nc at all node densities. as the node density increases, the difference become more significant, this implies that ccn can benefit from larger node densities. fig 2 depicts the comparison of message overhead for different cache sizes. again we can see that the message overhead is significantly less for ccn. from figure 3 we can see that the cache hit ratio for ccn is greater than nc for different cache sizes. the relative performance of cache hit ratio remains relatively stable for higher cache sizes. from figure 4 we can see that the query delay decreases with increased cache size. in this paper we addressed the problem of data discovery and cache management policies for cooperative caching in ad hoc networks. the objective of our problem was to minimize the number of messages flooded in to the network, which in turn reduces the communication cost and bandwidth utilization. we designed a data discovery process based on the position of the neighboring nodes. the cache replacement policy proposed increases the cache hit ratio compared to that of the most commonly used lru scheme. experimental results show that the proposed approach can significantly improve the performance in terms of message over head, cache hit ratio and average query delay."
"our paper presents two main contributions. first, we introduce the use of synthetic images for which ground truth can be generated automatically, avoiding the tedious effort of manual annotation. in the process, we generated a custom database based on two image data sets (these data sets are publicly available for download at http://imagenes.cicataqro.ipn.mx/shadows/), one real and one synthetic. the real data set was acquired outdoors during several days using a fixed camera, which was overlooking a quiet area without moving objects. the synthetic data set was created using a rendering software. secondly, we perform a quantitative and a qualitative evaluation of several algorithms for shadow detection based on the intrinsic image (the concept of intrinsic images was introduced by barrow and tenenbaum [cit] as a way to describe an image in terms of characteristics such as range, orientation, reflectance, color, texture, and incident light) representation [cit], all of which uses the reflectance component. the quantitative evaluation was done for the synthetic data set, while the qualitative one was done with both data sets."
"evolution is a fundamental force driving the organization and structure of natural systems. it is based on two key ingredients: variation and natural selection [cit] . the first ensures the necessary mutation and recombination generating new species while the second determines the survival of the fittest to perform a certain function. networks in nature have been subject to the same powerful mechanisms that ultimately determined their structure, properties and functionality. the resulting networks have heterogenous topological structures, which researchers have been interested in together with their effects on dynamical processes [cit] . examples include epidemic spreading, opinion formation, and synchronization [cit] . often there is also heterogeneity in the nodes of a network. for example, in social networks, individuals have different personalities, which will have great impacts on their social relationships; or, in manufacturing, industrial products are slightly different from one other, affecting their impact and market shares. the relationship between the heterogeneity of the nodes and the structural properties of a network is little understood, particularly when the network evolution is state-dependent."
"typically, one word consists of a content morpheme and a function morpheme. however, some words exist that have more than one content morpheme. so, we try to find word types that have more than a content morpheme, and divide those words into two new words. in korean, a predicate nominal has two content morphemes -a nominal and a predicate. we choose the first predicate nominal in a sentence for division. we divide each predicate nominal into two new words, and insert a function morpheme for the first new word that does not have a function morpheme. an example is in fig. 2 . a predicate nominal consists of \"first content morpheme(nominal) + second content morpheme (support predicate) + function morpheme\". as shown in fig. 2, the first new word consists of only a nominal, and the second new word consists of a predicate and a function morpheme -in fig. 2, a coordinate ending is used as a function morpheme. subsequently, the first new word does not have a function morpheme. so, we insert a relevant function morpheme based on the relationship between the nominal and predicate, determined as follows."
"we conclude that our natural language watermarking based on morpheme division and syntactic displacement shows reasonable performance without much semantic and stylistic distortion, and this method also shows good coverage. we also show improved watermarking capacity and we use sentence weight value to make our system more secure."
"the general problem of shadow detection can be classified depending on whether the objects casting the shadow are static [cit] or moving [cit] . however, a very important factor when considering this categorization is the scale of time. for instance, in outdoors the shadows cast by objects such as buildings, lamp posts, and trees during daylight can be interpreted as static if we consider in our analysis a temporal window of a few seconds. in this case, no significant changes will be perceived in the scene and existing techniques for moving cast shadow detection [cit] cannot be applied. on the other hand, if we consider a temporal window of a few hours, the same shadow could be interpreted as a moving object. although the problem of detecting moving shadows has been extensively studied [cit], the problem of detecting static shadows outdoors, over long periods of time, such as days, has received little attention [cit] ."
"the purpose of the experiments is twofold. on one hand, we show how the intrinsic image methods are used to detect shadows. on the other hand, we report quantitative and qualitative results based on the illumination images computed with the intrinsic derivation methods described in the previous section."
a syntactic dependency parser is used to determine the syntactic relation between words in a sentence. we use a korean syntactic parser of m. y. [cit] . fig. 3 shows an example of a syntactic dependency tree.
"netevo is a computational framework designed to help understand the evolution of dynamical complex networks [cit] . it provides flexible tools for the simulation of dynamical processes on networks and methods for the evolution of underlying topological structures. to bring together simulation and evolution in a coherent way, the framework uses the idea of a supervisor, illustrated in fig. 7 . evolution of the system is performed by the supervisor which can be viewed as a form of optimiser. this takes as input an initial topology, simulated output from the system and user defined constraints, and aims to return an optimal or enhanced topology. changes to the system are assessed by using the performance measure -r (the opposite of the order parameter), with smaller values representing an improved performance. by default, netevo provides a supervisor that uses a simulated annealing meta-heuristic to search for near optimal configurations. this method has been shown to perform well for a wide range of problems with an unknown prior structure."
"the process of cache discovery is triggered when there is a local cache miss. we use a location aided discovery to reduce the message overhead. the goal of this scheme is to reduce the average number of messages among the cooperative caches while maintaining high cache hit ratio. in this algorithm, the decision to forward the data request is based solely on the location of itself and its neighboring nodes. when the requested data is not found in the local cache the request is forwarded to the nearest neighbors present in the primary zone. after sending the request the node waits for the reply .if the node doesn't receive a positive reply after a period of time t1, which is a predefined threshold, the node searches the data in the secondary zone. the nodes in the secondary zone will be at greater distance compared to the primary zone. the time out period is set to t2, which is higher than time out set for the primary zone. the process of cache discovery is fully distributed and runs in all the nodes in the network."
"we have used the 3,000 declarative sentence set in the corpus of matec99 (morphological analyzer and tagger evaluation contest in korean). as shown in table 1, the average number of words/sentence is 17.95."
"as mentioned, because korean is an agglutinative language, single word consists of several morphemes. inserting a functional word to the first word (1) chul-pan (common noun) + i(subject case particle) (2) doi (support predicate) + go (coordinate ending)"
"if a sentence can be changed based on morphological division, the watermark selector determines the sentence will be marked using morphological division, and the watermark bit and quadratic residue function value for the sentence is not equal, then we will get a marked sentence based on morphological division."
"in addition, this edge-snapping strategy has two parameters b and d, which can be used to control network evolution. indeed the barrier of the potential between the two wells, acts as a constraint. as explained above, if the driving force is not strong enough, the edge, after a transient, will remain in the well corresponding to the absence of link. the height of the barrier can be tuned varying the parameter b in the expression of the potential v . the higher the barrier b, the stronger the constraint."
future work will focus on the exploration of alternatives to obtain intrinsic images without the texture information remaining in the illumination component or the shadow information remaining in the reflectance image.
"where n is the number of words in s, and l(p) is the length of (p+1)th word. then, compare its quadratic residue function value with the watermarking bit mi. if they are equal, no change is made; otherwise, let the watermark selector choose the method for watermarking and obtain a marked sentence until qr(key,wj) equals mi. if all transformations fail, no change is made to wj."
"text watermarking is an emerging technique in the intersection of natural language processing and the technologies of security. text watermarking aims at embedding additional information in the text itself with the goals of subliminal encoding of information, of content and authorship authentication, and finally of enriching the text with metadata [cit] . these kinds of watermarking techniques have been explored extensively for multimedia documents in the last decade [cit] . in contrast, the studies on natural language watermarking are more recent, emerging in the last several years."
"the rest of the paper is organized as follows. in section 2, we present a survey of the existing research literature about shadow detection algorithms. then in section 3, we describe the synthetic and real data sets used in our evaluation. in section 4, we recall the definition of intrinsic images and then present the algorithms used in our evaluation study. in section 5, we present quantitative and qualitative results to compare the methods. finally, section 6 contains our conclusion and our ideas for future work."
"in mobile ad hoc networks, caching can improve mobile client perception in three ways. first, retrieving data from a remote data center involves wireless media network transfers and there is a chance of data loss due to the wireless link characteristics. second, when the data is served locally, the network latency is hidden from the user. the data center processes the data request only when there is a local and cooperative cache miss. by doing this the server load is balanced, which consequently reduces the latency in serving client request. thirdly, frequent disconnections which occur in ad hoc networks can be hidden from users, making the network more reliable."
"to measure the synchronization performance of a es network, we consider an ensemble of phase oscillators connected by that network and evaluate kuramoto order parameter as re"
"we propose two watermarking schemes: one is morphological division, and the other is syntactic displacement. a watermark selector phase chooses sentences that are applicable for watermarking, and then one of the two methods. if both of the methods can be applied to a sentence, we use a round-robin selection of the two methods to balance the result."
"we measure subjective rate by human as suggested by h.m. [cit] . the evaluation method uses human evaluation of the text, and records their reactions by editing attempts. the subjects are given marked text and asked to edit them for improved intelligibility and style. this is a blind test because the subjects are not aware that text watermarking has taken place. three humans have checked the sentences."
"we input the possibly marked sentence and obtain morphological and syntactic analysis results. the watermark extraction process is similar to the embedding process, except that we just calculate quadratic residue function value with the prime key with the sentence weights as the input arguments."
"for convenience, weiss worked in the log domain. in what follows, we will represent variables in the log domain using lower-case letters, e.g., i(x, t) to represent the logarithm of i(x, t). according to weiss' method, a reflectance edge image is computed by taking the median along the time axis of the convolution between the derivative filter and a given image:"
"if a sentence can be changed based on syntactic displacement, the watermark selector decides that the sentence will be marked using syntactic displacement,: if the watermark bit and quadratic residue function value for the sentence are not equal, then we obtain a marked sentence using adverbial displacement."
"for a better analysis of figure 6, we can consider the plots consisting of four regions. the first region r 1, according to the false positive rate, is between 0 and 0.01; the second region r 2 between 0.01 and 0.1; the third r 3 between 0.1 and 0.23, and the fourth one r 4 with larger values than the previous one (vertical lines figure 6 )."
we model the evolutionary pressures to reach synchronization by considering state-dependent second-order step 1 (variation): computation of link activation probabilities by running the edge-snapping strategy from many different random initial conditions. step 2 (selection): selection of those links whose activation probability is above some threshold value p * .
"when there is a local miss the data item is fetched either from the neighboring nodes or from the server. the cache placement module is triggered when the data item is brought in, to decide whether to cache or not the incoming data. in order to cache more distinct data the caching decision is done based on two parameters, size and distance. we set a threshold value t, which is 50 % of the cache size for a data item to be admitted to cache. the data coming from the neighboring nodes are also not cached in order to increase the data accessibility."
"cooperative caches have different functions: cache discovery, placement and replacement, consistency maintenance and data dissemination. discovery refers to how a mobile node locates the cached data. placement is the processes of selecting appropriate nodes as cache locations and replacement is the strategy used for evicting data when the cache is full. consistency maintenance is maintaining consistency among the source data and cached copies. our ultimate goal is to design a cooperative caching technique which minimizes the cache overhead and the average response time of data retrievals. the proposed algorithm investigates cache discovery and cache replacement in cooperative caching for mobile ad hoc networks."
"(1) if the support predicate indicates active voice (e.g.'ha', 'siki'), then the nominal functions as the object of the predicate. so, we insert an object case particle--'eul' or 'reul'."
"we demonstrate that morpheme and syntax-based combined text watermarking can be effective. it improves the rate of embedding, and the coverage of the system better. it also helps retain the meaning and naturalness of a sentence. in the next section, we describe our method in detail."
"while we have described two key user scenarios previously, our approach is not limited in this case and can be applied to multiple scenarios. these include smart-home living situations (for example, personalised instructions on how to initially set up a complex coffee machine, or how to use a washing machine). while we have only included two key scenarios for the purposes of explaining the functionality of the components, the application is built to be versatile for other areas proposed in line with the internet of things."
"within the user ontology model, user profiles for each participant were added, specifying the differing media preferences, health conditions and personal information. this is shown in table 3 . table 4 presents a description of the initial results from the hod experiments. these highlight the impact of the wifi or 3g/gsm speed on the accuracy and efficiency of results to the enduser and also show which media type was returned, based on user profile information and the use of ontological reasoning. the correct requests were retrieved via the user enabling the 'scan nfc' functionality, where the correct media feedback file was sent to the application upon each request. the user tests showed that the media feedback delivery type was highly accurate, displayed the appropriate media to each individual user according to set preferences as stored within the ontology profile model. users 1, 2 and 3 requested different media types (audio, text and video) and each of the returned types matched their specific request (via the haspreferredmediatype property within the model)."
"applications such as personalised mobile guides [cit] provide rich, contextual information delivery to users and enable assistance via navigational services, multimedia feedback and adaptive user interfaces. other existing 'help-on-demand' applications include those created from the apsis4all-project [cit] or the ask-it project [cit] . these projects focus on the development of smart-phone technologies to personalise the interactivity of user activities with public digital terminals (pdt) via the use of user profiles and adaptive interfaces. these are effectively used to leverage the power of mobile computing to enable the provisioning of adaptive services and mobile 'learning' of contextual user preferences."
"two of the core aspects within the personalisation of service consist of the user models and the personalisation mechanisms. there are two main categories of approaches in the area of user modelling -namely knowledge driven approaches and machine-learning approaches. ontology-based modelling has attracted increasing attention within the area of user modelling in context-aware applications. this is largely due to its interoperability facets and ability to enable knowledge sharing and reuse across several application domains [cit] developed the cobra-ont ontology as part of the context broker architecture, which provides knowledge sharing and context-reasoning support for context-aware applications. this architecture was successfully used to enable sensors, devices and agents to share contextual knowledge and to provide relevant information to users based on their contextual needs. [cit] presented work on user modelling with a generic ontology-based architecture called ontobum."
"though there are different methods for user modelling, we have adopted the use of semantic technologies to model, represent and reason about user profiles. semantic technologies enable interoperability across different platforms, are highly expressive when modelling complex relationships, they support semantic reasoning and have the ability to reuse information from several application domains [cit] . semantic technologies enable us to reason about various data, that is, to draw inferences from existing knowledge about a particular area for the purposes of creating new knowledge. the use of semantic data is also beneficial when integrating information from various sources. by developing semantic 'mappings' between different schemas of information across various applications, it is possible to create interoperable systems. at the heart of semantic-based technologies is the use of ontologies."
"within the current study, implementation has been undertaken in six key stages, as shown in figure 3, namely 1) the conceptualisation and creation of an ontological user model, 2) the design of the required swrl rules, 3) the development of the user ontology model, 4) the development of the personalisation reasoning component and 5) the creation of the hod assistive application in the context of a (6) real-life application scenario. in stage 1, ontological user models are created based on the specific scenario of providing assistive services via personalised media feedback to travelling users. nevertheless, the model can be tailored to suit other scenarios through easy manipulation of the ontology concepts via proté gé . in stage 2, we follow the rule design method in section 5.1 and use the swrl editor within proté gé to design a set of rules for the application scenario. when swrl rules are created, they can be tested and checked for inconsistencies using proté gé . the rules are then reasoned where the results are shown as new individuals grouped into classes or if the rules are inaccurate, proté gé will highlight where and why inconsistencies occur. figure 5 shows an extract of the rule creation process, where specific rules are associated with individual classes, instances of classes or object/data properties in the ontology. table 2 presents a subset of the swrl rules developed in this study for the outlined application scenario. these include: rules to restrict the language of the feedback sent to the user (rules 1 and 2), the type of feedback sent to the user and which format should be sent (rule 3), and the content sent to the user and the appropriate personalisation (or formatting) depending on their preferences or contextual information (rules 4 -7). stage 3 focuses on the implementation of personalisation reasoning. in this study, we use an existing open-source semantic reasoning engine called pellet [cit] for personalisation reasoning. stage 4 and 5 of the implementation process involves a service-oriented development and deployment for the presented system architecture, as shown in figure 6, which was used to enable the personalisation mechanism for the specified application scenario. the system was developed using java and the owl-api [cit] . the hod services run on the server-side, which provides the storage and usage of the ontology model (.owl file), a set of pre-defined swrl rules, the reasoner, and the application server (apache tomcat)."
"this paper details the conceptualisation and development of a novel user profile ontology model and a personalisation component. the service provides an extensible model capable of integrating itself into a number of context-aware assistive applications. the component and model can be adapted to numerous scenarios through the use of personalised semantic rulebased reasoning. to demonstrate the utility of the service, a personalised hod service is presented through a smart-phone application. emphasis is placed on the use of semantic web rules used to enable a more accurate and efficient personalised service for the user. swrl further enhances the personalisation capabilities of the component expressing additional concepts that cannot be directly inferred from the existing ontology language."
we now analyze and compare the performance of the pf algorithms described in the previous sections. all required parameters are shown in table i .
"a full-scale user evaluation focusing on usability and acceptance of the technological solution rather than the technologies themselves will be conducted within mobilesage. the usability, functionality and impact of the application of real-life users across different user groups will be tested and assessed."
"in recent years, ontological user modelling has emerged as an important enabling technology for personalisation. ontological user models have been developed for use within personalised web information retrieval systems, adaptive user interface design and for public services such as digital museum guides or electronic, customized libraries [cit] . nevertheless, these models have not been adopted to implement the personalisation of assistive services for mobile users. existing methodologies that are used to personalise a service include work within the area of case-based reasoning in context-aware applications [cit], rule-based reasoning for situation inference [cit] and collaborative filtering (cf) techniques [cit] . in particular, rule-based reasoning approaches are highlighted, where current work exploits the specified rules to infer information about user context [cit] or within the area of activity monitoring and recognition [cit] ."
"users within pervasive environments normally have some form of smart device, for example, a smartphone for interacting with hod services. the user interface of the device can be adapted to user preferences as part of the service personalisation. alternatively, the device can be used to communicate with smart objects to activate service requests or control objects in terms of returned service instructions. though the system architecture is motivated and discussed based on the typical application scenario, the rationale and principle can be applied to any application domain in pervasive environments. the following section presents the technical details of the core services of the architecture in further detail."
"personalisation refers to the manner in which an application provides the 'right' information for the 'right' user at the 'right' time and in the 'right' way [cit] . a personalised service can be described as one that is able to provide evolving, tailored assistance to a user based on their unique preferences, needs or desires [cit] . a key enabler of personalisation is the knowledge of the context that is used to drive such services. in a world where information is not only growing every day, however, is available in a variety of formats and through various media channels, users find it difficult to find the information that they require in a way that suits their individual needs or wants at a particular time [cit] . context is most commonly defined using the definition by dey and abowd [cit] as information that is used to characterise people, places or objects when a user interacts with an application within a particular environment. context-aware applications therefore interpret contextual information based on changes in environments for the purpose of providing a particular service to a user [cit] ."
"the other category of approach to user modelling relies on probability-based analysis, which has a key advantage of handling the uncertainty of user behaviour. some existing approaches in this area include the use of a bayesian network [cit] or the dempster-schafer theory of evidence [cit] . in general, current research relating to user modelling and personalisation has focused more so on improving aspects such as a user's experience via information retrieval methods [cit] or interactions via personalised user interfaces [cit] ."
"rule-based reasoning enables a more expressive method of inference when reasoning about user profile information such as changing preferences. while techniques such as cf or casebased reasoning (cbr) make good use of past user interactions or feedback to personalise future services, they fail to provide a complete model of the user and can be inconsistent. the use of production rules is a powerful way to represent additional attributes that cannot naturally be inferred using traditional ontological models."
"the hod service contains various assistive services that can be used by users to help manage daily activities. example services include monitoring and analysing wellbeing, fall detection, medication reminders, navigation and hod for travelling. such services can be deployed in the cloud and accessed via standard service interfaces (such as http). hod services take as inputs the personalised requests from personalisation services and provide, after processing based on specific business logic, personalised assistance to the user. this component makes use of the specified user rules reasoned information and user information (plus the original help request) to send information back to the user front-end in the form of personalised assistance."
"within this section the process for the implementation of the existing hod application is detailed. the core aspects of the application's functionality are highlighted, with testing followed thereafter."
"personalisation involves multiple entities, such as the users, their situated environments, the application contexts and services and the interactions and causal relationships between these entities. a typical real-world application scenario is presented to contextualise the discussion of the system architecture. the scenario is based on the research context of the eu aal funded research project mobilesage [cit], capturing the unique characteristics and requirements of service personalisation in pervasive environments. mobilesage aims to provide elderly people with personalised assistive tools, which will allow them to undertake everyday tasks and enhance their overall quality of lifestyle. to do this, the project provides older people with context-aware, personalised services that allow them to perform everyday routine tasks with minimal effort."
"jane is a 63-year-old retired schoolteacher who now enjoys travelling a lot. jane prefers to take the train as opposed to planes and as such spends a lot of time moving to and from train stations. jane has diminished eyesight and finds it difficult to make out small details and font, especially on screens. as such, her user profile has a preferred feedback mode of text with the largest font size. when she arrives at her destination jane enters the location of local hotels into her hod application. jane's search query is sent to the cms for information retrieval. in addition to this, her user profile is reasoned and the content she has requested, directions and her preferred method of feedback are returned to her device."
"the personalisation service consists of a rule base and a reasoning engine to enable inferences. the rule base contains a set of rules defining the causal relationships between the user profile, environmental and application context and service outputs. after the rules are created, they can be tested by a reasoner tool. the reasoning engine will take as inputs user profiles, context and user requests of assistance to reason against the rules to decide how ondemand services provide assistance for the user. the personalised service requests will then be sent to hod services for processing [cit] ."
"nevertheless, should a user specify a preference in the preference class to always have video, and then the ontology can infer this information and deliver the appropriate format. by including a series of generic user concepts, the model can be easily used across different application domains. for example, the model can be tailored to suit a user travelling between places or it could be used in a user's home to provide personalised assistance with adls. by formally defining the classes using semantics, the ontology can be used as a key component to infer what services/assistance the user needs at any moment in time. while the use of ontological modelling is beneficial within this area, rule-based reasoning to enable personalisation is another important feature that is used to infer additional logical concepts concerning a user."
"the remainder of the paper is organised as follows: section 2 discusses existing related work within the area of ontological user profile modelling, user personalisation and contextawareness. this section also highlights the key knowledge contributions from this study of work. section 3 provides a detailed description of the overall system architecture for the proposed hod application. section 4 focuses on the use of ontological user profile modelling for the purposes of user personalisation. section 5 introduces the area of rule-based personalisation, where a novel personalisation component is described. section 6 discusses the system implementation, testing and evaluation of the hod service, preceded by two case studies presenting the potential of the ontology model and personalisation service. section 7 concludes the paper and provides a summary of future work."
"we use real measurements obtained in a basement tunnel of linköping university. the measurement setup consisted of a vector network analyzer (vna), two uwb omni-directional antennas, coaxial cables and a pc. we used a swept-frequency sinusoidal signal to characterize the channel between 2.5 and 4.5 ghz. the power level was set to 12 dbm. the frequency responses were transferred to the pc where a bandpass filter was used to reduce the out-of-band noise. finally, by applying the inverse fast fourier transform, the complex impulse responses (cir) are estimated, from which we can extract the desired features. a more detailed description of the measurement campaign is available in [6, section iii] ."
"as previously highlighted, user profile modelling is central to service personalisation. it involves the creation of a data structure that can hold the characteristic attributes of specific types of users. this data structure is usually referred to as a user model that serves as a template for generating specific user profiles for different individuals. these user profiles can be viewed as digital representations of data associated with a particular person [cit] ."
"the process of ontological user modelling can be summarised as the following steps, namely to 1) analyse users' characteristics and needs in context-aware environments; 2) establish interrelationships among core entities of the domain, namely, users, environments, application scenarios and services; 3) identify and define key concepts that can model and represent these entities and further define properties that can be used to describe these concepts; 4) classify the concepts and properties into hierarchical structures; and 5) use ontology development tools to encode these concepts and interrelationships and represent them in a formal ontology language. typically, the ontological user modelling process follows a traditional ontological engineering process [cit] and as such, can be classed as a generic process. nevertheless, while the process follows a typical engineering style through the conceptualisation/specification stages, we have adopted a unique format for our user ontology model and took inspiration from related ontology models within this domain. in this study user focus groups [cit] from three countries (namely spain, romania and norway) were formed within the mobilesage project and various questionnaire and interview techniques were used to extract and analyse users' characteristics, preferences and habits. based on this analysis we have been able to specify concepts, interrelationships, properties, and a set of commonly shared vocabularies."
"as can be seen in figure 2, the user ontologies describe the key concepts related to a user's activity and interaction with an assistive context-aware service and enable the personalisation of the content provided to them. every user has one unique userprofile and an associated context or locale, which details their location, and nearby wirelessconnections. within the ontology, each activity is linked to a specific type of request that the user invokes. the two main requests are triggered via nfc tags or qr codes. every assistance object (e.g., ticketmachine, touristinfomachine) is linked via properties to a specific nfc_id or a qrcode_id. depending on the information associated with the userprofile, once a request is triggered it is sent to the user via some form of media delivery format or mediacontent format. for example, if a user has a health condition stating that they are blind, then this is stored in the healthcondition class of the user ontology and according to the relationships and restrictions placed in the model, only certain media delivery types are sent to the user. in this case, the media content may be in the format of audio only due to their condition."
"rule-based reasoning enables a more functional representation of the user and allows for the creation of a highly expressive personalisation component. in particular, the application detailed within the paper facilitates the application's potential within smart, changing environments using smart-phone technologies and context-based reasoning. the ontology model has been adopted by the mobilesage research project for providing personalised hod services. initial evaluation results indicate that the current application, utilising the personalisation mechanism and the ontology model, provides a quick and accurate response to the test users within the study. furthermore, the study has further highlighted the utility of personalisation components for use within context-aware assistive applications."
"to adapt to the changing needs of a specific user within different settings, a system needs to understand the specific aspects of the user profile particularly relating to its situated environment. when modelling human users, their individual characteristics can be broken down into various levels of granularity. an example of this would be the concept of a 'food' preference, which could contain the sub-preferences 'asian', 'european', 'western' or 'italian'. this in turn suggests that the user profile should also be broken down into various levels to enable the development of a more complete and extensive user model. when modelling the user, temporal and environmental contexts should be taken into account, in order to provide a more comprehensive model of dynamic user attributes that change as they move between mobile environments. for example, if the location of the user is modelled at different times, it can be determined if they want to go to sleep or go shopping at some specific location."
"finally, we provide a summary of the accuracy and complexity of all considered methods in table ii . as we can see, the complexity off all methods, except toa+gpr, is quite similar. toa+gpr is about 16 times slower due to the use of the training samples, but it provides the best performance. 5 in case of more general target trajectory, we expect that toa+gpr would be the best for all percentiles. depending on the desired trade-off between the complexity and the performance, it can be determined which of the two methods (toa+mitig or toa+gpr) should be used."
"where and 2 represent, empirically estimated, mean and the variance of the los ranging error. note that here we do not have any information about the nlos error statistics, so this approach will typically lead to positively biased range estimates."
"the emergence and rapid, continuous development of mobile technologies and adaptable user interfaces has sparked research into enhancing user-based applications that are personalised to suit the changes in user needs over time. this increase in smart, mobile-based technologies has led to an increase in user dependence upon such technologies; an increase in dependence fuels the need to develop new methods of user personalisation to cater for the demand. consequently, this has enabled an increase in both the demand and need for user modelling for service personalisation. user modelling also needs to address the dynamic lifestyles of users within different environments."
"user modelling is related to the user's behaviour and the purposes of user models in the specific application context. for context-aware service personalisation in changing pervasive environments, user modelling presents several novel aspects. people in different environments normally exhibit different behaviours. for example, in a supermarket people are concerned with shopping while in a residential setting people mainly carry out adls."
"rules only establish cause-effect relationships. a reasoning engine is required to decide if the pre-conditions of a rule are met, thus leading to a consequence. in addition, the reasoning engine also needs to decide if the consequence of a previously fired rule results in another rule being fired. once the pre-conditions of the antecedent are met, then the rule is fired and the consequence is held. swrl rule reasoning supports both forward chaining or backward chaining reasoning. forward chaining reasoning starts with a series of facts about involved entities and then looks for rules to apply to such facts. then the generated consequences can be used as facts to activate other rules and this process continues until no rules can be fired. the consequences of the last fired rule will be the ultimate results of the forward chaining reasoning. conversely, backward chaining reasoning begins with a goal and then searches for rules that can be applied to that goal until a conclusion is reached at the end."
"a traditional top-down design approach was implemented where high-level, generic user concepts are selected (e.g., capability, health_conditions) and further broken down into a series of specialised concepts (e.g., capability_level, healthcondition_type). within the model, each defined parent class may or may not contain further sub-classes forming a hierarchy of related information about one user. to build the ontology, we firstly identified the key terms for describing a user and modelled these as ontological classes and sub-classes as shown in table 1 . table 1 . extract of the key ontology classes of the conceptual user models."
we evaluated the functionality of the personalised services by deploying the application onto three android-enabled smart-phones and presented the application to three separate users. the two case studies discussed below were used to highlight the utility of the proposed personalised assistive services for users and to test our proof of concept hod application.
"video, text only, audio and text). furthermore, user experience was quantified in terms of the speed and accuracy at which the appropriate media feedback was sent back to each user."
"for using the qr code option, the scenarios that we have presented targets those who have more minor visual impairments (such as partial blindness or long-sightedness). applying this hod application to those with full blindness, for example, is out of the scope of our work at this time. for a user with a minor vision problem, the smart-phone can be adapted to make instructions larger, the qr code/nfc tags can be enlarged, braille can be used as an additional aid and audio instructions on how to use the application can be played back to the user."
"jack is a 46 year old it project manager. he is often required to travel abroad for project meetings but also enjoys travelling in his spare time. jack has a partial vision impairment and diminished hearing and as a result finds it difficult to read small text and hear things, which poses a number of everyday challenges. he has been able to adapt the preference settings of his smartphone so that applications are displayed in a larger font and use inbuilt audio commands where possible. the parameters related to these preferences are stored in a user profile. the user profile is stored on an application server in addition to the ontology model of information."
"rule design is a practice of knowledge modelling, which involves the analysis of the application domain, to identify application processes and related core entities, and further establish interrelationships among these entities. each rule defines a cause-effect relation among these entities, e.g. how services are delivered under a specific context with a specific user. the swrl language specifies a rule in the following format: each rule is made up of a body (known as an antecedent) and a head (the consequent) [cit] . the syntax of a swrl rule may appear simple in nature, however, the formulation and design of these rules overall is a complex procedure. the designer should normally envision every possible personalisation scenario that the rules would cover and design each swrl rule with a different purpose to cover all possible service personalisation outcomes."
"we assume that there are static beacons with known 2-dimensional (2d) positions and one mobile target with an unknown position. both the beacons and the target are equipped with uwb radios, and the target is also equipped with an imu. the goal is to track the target using available uwb measurements from all beacons. the tracking algorithm is executed either by a central unit or the target itself. 1 for this problem, we use the following state-space model:"
"we have described the process of personalising the application upon scanning the appropriate nfc tags or qr codes; however, we did not discuss how to support a visually impaired person to initially use the application. if we were to target those with severe visual impairments, we could include aids to help such users locate the required nfc tag or qr code on the object in question. aids such as the use of braille (for example, attaching braille to various ticket machines), or the use of tag magnifiers to make the tags appear larger can potentially be used to help overcome the problem of initially accessing the hod service. the smart-phone itself will also contain built-in user accessibility options (these are found within the settings of any android-enabled smart-phone) where the user can tailor specific settings to suit their abilities."
"the pervasive environments component refers to various intelligent environments, which contain sensors, smart objects, intelligent communication and interaction devices. examples of such environments and technologies include smart homes equipped with radio frequency identification (rfid), nfc tags, qr codes, smart phones, and tablets. a pervasive environment usually contains context management middleware, which can monitor users' activities and their interactions with environments and situated objects. context middleware can provide contextual information at a specific point in time to support context-aware applications. within the system architecture, the context of the user is captured and processed to derive high-level contextual information from low-level information. this middleware is used to query the current location, radio connectivity levels (wi-fi/3g/gsm) and surrounding environment of the user at any point in time. in this work, the context-aware service provided via the hod application listens for new contextual information that is sent by the context providers."
"rule-based reasoning approaches have several key advantages within this domain over the use of other techniques, such as cf. one advantage of using rule-based reasoning is that it reduces the effect of the 'cold-start' issue with cf techniques. data scarcity is a challenging issue within this domain and rule-based approaches do not need to worry about basing their services on previous information (for example, basing personalisation on previous user information, in our case, a unique user data model is set up initially to overcome this problem). secondly, rule-based reasoning has an element of uniformity, where all knowledge within the system is expressed in exactly the same format. cf techniques can be inconsistent and reliability is always a factor as the techniques rely on the accessibility and accuracy of previous user interactions and/or information."
"in recent years there has been a significant increase in user reliance upon smart technologies related to the area of aal. consequently, this has enabled the emergence of several research projects focusing on the development of pervasive solutions to enhance overall quality of living for individuals. the aal joint programme is a funding body that aims to create better condition of life for the older adults and to strengthen the industrial opportunities in europe through the use of ict (e.g. mobilesage [cit] ). the eu framework programme [cit] outlines an ict research agenda, which has funded and aided the development of both the universaal [cit] and evaal [cit] projects. universaal proposes an open platform that provides a standardised approach to develop aal solutions for users in their homes. evaal is an initiative proposed by the universaal fp7 project that aims to enable the comparison of aal solutions by establishing various evaluation metrics and benchmarks to help improve standards."
"consequently, the required instruction, detailing how jack should proceed with his purchase is delivered to his smart-phone in video with closed caption format and with the required font size jack requires. jack's interaction with the hod application enables the system to understand what stage of the booking process that he is current at. consequently, jack is able to follow the onscreen instructions in his own language (english) while also receiving video support. figure 7d presents a screen shot of the feedback presented to jack in this scenario."
"the retrieval of specific media formats; based on an individual's user profile was subsequently evaluated. specifically, the time taken to retrieve information from the hod service in addition to the effectiveness of the personalisation service, in terms of matching media type to a user profile. each user tested the utility of the case studies presented previously by using the hod application to (1) purchase a train ticket from an automated ticket machine and to (2) enable personalised route directions from a specified location to another place of interest. users 1 and 2 tested the first scenario and user 3 tested the second scenario. upon testing via these case studies, the aim of the hod application was to retrieve the appropriate help feedback, in terms of the content sent back and what format it was in (i.e."
"future work will aim to further develop the range of personalisation services used to incorporate a wider user audience and extending the model to incorporate a more comprehensive set of user concepts. a comprehensive user evaluation will take place in-line with mobilesage. mobilesage will be conducting user trials testing both the efficiency and impact of the personalisation and adaptation components. user trials will take place, where the case scenarios detailed within this paper will be used to highlight and test the functionality of the hod services."
"the userprofile class denotes the central concept within the ontology, from which all of the relevant information concerning the user's preferences, locale, personal information, demographics, activities and health conditions. relationships between the classes are set-up for the purpose of facilitating more logical inferences when reasoning about the user concepts. the assistanceobject class contains several sub-classes relating to the type of object involved in the interaction between the smart-phone and the helpdelivery class (for example, this could be a bus ticket machine or an airport self-check-in machine). the assistanceobject is the object in which the user must interact with to perform some task and is key to enabling the personalisation of the help request to the server. within the preference class, there are several sub-classes (such as preferredmediatype or preferredlanguage) and these are linked via appropriate data and object properties. for example, the preferredlanguage class is linked to the userprofile via the object property of haspreferredlanguage, which can also be linked to the user class. by including such relationships and different levels of granularity, services/tasks can be tailored to suit an individual's own specified needs or wants."
"the pre-conditions for a help-on-demand service to be delivered in a user-specific way can be defined in a set of logical expressions. each expression is constructed based on logical operators such as variables representing user profiles and service requests. in the case that a service request is initiated, the conditions expressed within these logical expressions will be evaluated to be true or false. if all pre-conditions are evaluated to be true, it will then lead to a consequence. the consequence could be anything pertaining to the application, e.g. a specific way or format to present a service. this cause-effect relation is commonly modelled as a set of rules. the next section describes an approach to creating personalisation rules by making use of semantic web rule language and ontological user models. a personalisation mechanism that can perform production rule reasoning to derive personalised services based on specified requests."
"realistically, this is both a time consuming and tedious task but if there is a solid base rule set that can be generically applied to several applications, the reasoning engine can then be used to infer additional rule sets over time. this means that the designer does not need to develop rules to cater for every scenario and works more efficiency as a result. swrl rules are stored as a series of owl individuals (instances) within the ontology model."
"while this may improve user waiting times (and therefore make the help-on-demand service much more efficient), there would be a compromise on the quality of content sent (i.e. file size, aesthetic quality). at the moment both the audio and video media content are streamed via the web, with the link to each file stored in the content management database. we could also provide dialog boxes to show on-screen if the user is waiting any longer than 3 seconds for content to show. these dialogs would tell the user that the content is coming soon."
"the main aim of any context-aware system is to be adaptable and therefore change its services or content to suit an individual's preferences. this need for personalisation is intensified by the rapid development of mobile-based technologies and user-adapted services. this area, known as user modeling, has sparked the development of a variety of personalised applications to include user targeted mobile advertising, user recommendation systems, personalised help-on-demand systems and adaptive user interfaces [cit] . challenges currently exist which focus on how to provide the best quality of service to the user. as a result, issues such as these have directed research into the areas of hci, user-adapted modelling and context-aware personalisation."
"where (a), (a) represent the estimate of the mean and standard deviation of the distance 3 and a is the normalized 4 measurement vector (in this case, 8-dimensional feature) obtained from uwb signal. they are given by:"
"where the parameters and represent the decay rates, and are estimated from los and nlos samples, respectively. then, the probability of los condition is given by:"
"in addressing the aforementioned issues, this paper makes a number of key knowledge contributions. firstly, a systematic approach and associated service-oriented distributed system architecture that supports service personalisation for a wide range of complex application scenarios is presented. these include the changing environment surrounding users, multiple assistive services, evolving user behaviours over time and development of user habits, and dynamic contexts. secondly, the development of a flexible ontological user model to represent the specific user characteristics of the application domain. these models can be used to enable the personalisation of context-aware services, due to their use of semantic reasoning to allow the models to infer additional assistive services for different users within changing environments. thirdly, we developed a unique personalisation mechanism using a combination of semantic and rule based reasoning techniques to enable context-aware services to be tailored to suit the changes in user needs over time."
"on the client-side, the user is presented with a smart-phone application tailored for android (version 4.1 or above) os. the smart-phone will take contextual information as it is input to enable the discovery of machines within a certain parameter and to determine the nearby wifi connectivity level and strength. when a user scans a nearby nfc or qr tag, the application is triggered and automatically sends an http help request to the application server. a link is also made between the web servlet and the server via a specified socket connection. the http request is sent to the server and the personalisation service handles how that request is managed. the personalised service manages the request using a combination of the ontology reasoning and rules to determine which user profile is associated with which rule set. the user has their unique user profile that has associated with it a set of individual swrl rules that are used to tailor services to that person. all of this information is stored in the model itself and the java application is used to implement and enable the functionality of the reasoner and rules to provide this personalisation. in the following section, we present two case studies that demonstrate the personalisation aspects of the proposed system in the context of the hod services."
"the miniaturisation of technology has become increasingly prevalent in recent years, where various mobile-based 'smart' technologies are rapidly being developed. these include the development of smart-phones, tablets or wireless sensors that are continuously being used to realise pervasive environments [cit] . consequently, people are becoming more dependent on the use of these technologies as they become an integral part of ambient assisted living (aal) -an emerging technology-driven solution for assisting with activities of daily living (adls). with increases in the use of mobile and sensor technologies, research within the area of pervasive computing [cit] has shifted from low-level hardware-related technologies for sensing and communication, towards middle level intelligent data processing and further towards high-level context-aware applications. pervasive computing systems are built upon the fact that relevant contextual information is used to adapt to various user traits or behaviours over a period of time [cit] . this vision of pervasive computing includes the availability of personalised, adaptable information that can meet user needs at different times, based on their context [cit] . in order to achieve this, context-awareness alone is insufficient, highlighting the need for service personalisation amongst pervasive applications."
"in this paper, we compare four different pf variants, that differ in terms of how nlos measurements are handled. more specifically, we use four different models for the likelihood function: i) direct toa-based ranging, ii) toa-based ranging with the rejection of nlos measurements, iii) toa-based ranging with mitigation of nlos bias, and iv) toa-based ranging with gaussian process regression (gpr) based estimation for the nlos case. we compare these methods using uwb measurements from a basement tunnel at linköping university [cit] . according to our results, the latter two methods provide the best performance since they make use of multiple features from the uwb impulse response from both los and nlos measurements. the other two options are not good choices due to the accumulation of error over time."
"the antecedent of the rule can be constructed to represent a conjunction of various user preferences. for example, in the example rule1 the first line constrains any individuals from the userprofile class that have a health condition (via the healthcondition property). in this case, the variable name '?up' is assigned to the class userprofile. in this rule, the user must be 'blind', where this individual name is assigned to the hashealthcondition property. in the rule consequent, it specifies that if the individual meets all these aforementioned constraints, then the helpdelivery class is affected. the audio class within the helpdelivery class will have two major property changes. the mediatype will be set to audio as default and the mediavolume will be set to level 5. the hasmediatype property is linked within the ontology to the helpdelivery class (detailed as an object property). the hasmediavolumelevel is also linked to this class, but its range is set to vollevel5 and its domain is set to playaudio."
"a typical application scenario can be described as follows to illustrate the use of hod. a person travels to a foreign country and needs to use a local train ticket machine. nevertheless, the person does not understand the language displaying the instructions, and also has a vision problem. consider that the ticket machine is a smart object with embedded sensing and interaction capabilities, such as near field communication (nfc) or quick response (qr) code. the person also holds a smart device, such as a smart phone or tablet, which has sensing capabilities (through built-in gps or accelerometers) and contains service front-ends of on-demand services deployed in the cloud. in this case the person can start relevant hod services on the smart device and use the device to interact with the smart object under concern. the hod service could then infer the type of assistance the user requires and deliver this assistance to them in terms of that particular user's profile, environmental circumstances and the object involved. figure 1 presents the architecture of the service orientated distributed system. it consists of four components, each interacting with each other to achieve hod service personalisation [cit] . the main objective of the architecture is to provide a clear flow of information between the hod application (within pervasive environments) to the user via the use of personalisation and user profile services this architecture highlights four core elements that focus on the application scenarios (pervasive environments), the user profile services, the personalisation services and the hod services. the user profile service contains user models, user profile repositories and service interfaces. user models are data structures or templates composed of generic properties that provide a computational representation of users or a specific class of users [cit] . for each individual, their user profile can be created by instantiating a user model with the individual's personal attributes, for example, their preferences, capabilities and interests. the generated user profiles can be saved thus forming the user profile repositories (which are stored alongside the user models within the user profile services). within the pervasive environments component, the users are in different situations (different contextual surroundings) and interact with the hod application front end on their smart-phone to send various help requests to both the user profile services and the personalisaton services. a mixture of contextual information, user profile information and the specific requests made are sent to these two key components. the user models and profiles are generally created through knowledge engineering processes and the resulting user profiles can be used by personalisation services to tailor specific services. the service interfaces within the user profile service provides standard apis for other services to access to, share and reuse user profiles."
"service personalisation, particularly for mobile users in pervasive environments, presents a range of unique challenges that need to be addressed. in order to personalise a service, both the functionality and presentation of the service should be adapted to suit the user's unique preferences [cit] . these challenges include (1) how to infer and analyse relevant information about a user and determine which information is suited to personalisation for a certain service, (2) how to ensure a high quality and accuracy of services is delivered to users and (3) how to know where the relevant user information is stored and how it can be distributed to use in the service personalisation."
"in this case, we keep using toa for los case, but apply a nonparametric bayesian machine learning method to estimate the range from nlos samples. more specifically, we use gaussian process regression (gpr) [cit], which is able to model arbitrary nonlinear functions, given a measurement, and a set of training samples obtained in the considered environment. the likelihood is, as in the previous case, given in a mixture form:"
"ontology-based modelling involves specifying a number of concepts related to a particular user, along with any number of properties or relationships associated with those concepts. in essence, ontologies provide a 'representation vocabulary', where these user concepts are structured in a taxonomy based on various user aspects. ontological models can be used by logic reasoning mechanisms to deduce high-level information from raw data and have the ability to enable the reuse of system knowledge. this is particularly important when modelling user aspects that can be remembered and reused later [cit] ."
"jane is presented with the directions to the nearest hotel, in a text based step-by-step list. the font size reflects her requirements as defined in the user profile to address her diminished eyesight. due to jane's eyesight problems, directions overlaid on a map would be difficult for her to see. figure 7c shows the directions to the nearest hotel on jane's smart-phone."
"this novel personalisation mechanism makes use of user-specified semantic rules and a dedicated reasoning service to provide help-on-demand services to users within pervasive environments via smart-phone technologies. the mechanism differs from existing work as it is combined with ontological user modelling and semantic web-based rules/reasoning, and is linked to pervasive mobile services to provide on-demand assistance to different users."
"any existing health conditions associated with a user that may affect the media content type or delivery feedback for a particular service. categorised into cognitive conditions and physical conditions. qualityscale this refers to the level of quality of media output to the user, defined as 'high', 'medium' or 'low.' uiformatting this is the formatting placed on the user interface of the smart-phone device running the application. the two entities that can be formatted include 'screenbrightness' and 'screencontrast.'"
"the hod application was used to simulate the two motivating case scenarios described within section 6.2.1, within the smart environments research group (university of ulster, uk) lab, to test the technical functions of the system. the application was installed on three smartphones and presented to three different users, each with different user profiles. these experimental 'set-ups' were designed and conducted to test the efficiency and correctness of the underpinning technologies, e.g. user profile models, reasoning or rule capabilities. the application was tested to determine how well the personalisation service worked when presented with different test user profiles and/or scenarios. the personalisation service enables the appropriate assistance (media feedback) to send back the required help response in the user's designated language."
"these initial results from table 4 show that the ontological and reasoning aspects of the service function accurately. the reasoning service worked efficiently by inferring the preferences of all users when they made a help request. this service was able to correctly infer which specific media formats to send back to the user application upon execution of associated rules in the ontology model, with user 1 inhabiting a health condition of partial blindness, allowing the model to infer that they would require audio by default. the application was tested by all users with both wi-fi and 3g/gsm connections. initial results show that the performance was statistically significantly higher when the smart-phone was connected through wifi, due to obvious bandwidth increases. this increase in speed provided a better overall user experience, as stated by the users upon testing the hod application using the two case study scenarios, the initial results indicate that text content was delivered in just over 2 seconds, a much quicker rate than audio or video content. this was due to content size and bandwidth available. while the correct media was sent to the users, the waiting time could be improved on the server-side. both forms of audio and video content (4.7mb and 8.7mb of data) were sent within 6 seconds to the smart-phones (via wi-fi). these times could be improved further through the use of media optimisation. according to a usability research study conducted within the area of user waiting time for web technologies [cit], an acceptable standard waiting time for web-page downloads would be between 3 -5 seconds. this depends on the content being retrieved and will differ for smart-phone communications. our testing shows that the speeds are above average; however, we aim to improve this further at future user evaluations with the hod service. another method to enable the optimisation of the delivery speed could be to reduce the file size of the media or change the formatting of the media to make it more suitable for mobile-phone delivery."
"this approach may work if there are at least 3 los measurements, and if the rise-time can provide us certain information about the nlos condition (i.e., if, is not close to 1/2). however, this may not be the case in many scenarios."
"within proté gé [cit], several instances of the 'user profile' class were defined, each of which holds specific attributes or properties concerning a particular individual. in this work, an instance of the user profile class would be created, where all of the concepts regarding the user's personal information, health history, preferences and characteristics are held and linked via various object and data properties."
"on-demand service personalisation in pervasive environments is based on forward chaining reasoning, and the process can be described as follows. as presented in figure 1, the personalisation services take as inputs: the rules from the rule base, user profiles from the profile services, contextual information from pervasive environments and user requests from the front-end of on-demand services. the reasoning engine has an in-memory working space within which a copy of rules and user profiles are imported. other inputs such as user service requests and contextual information are dynamically captured and imported into the working space where logical operations of the forward chaining reasoning take place. when a user makes a request through the front-end of the on-demand services, the request is passed onto the personalisation services, which is then used as a variable in the logical operation of the antecedent of rules. meanwhile, the contextual information within the pervasive environment is captured and used in a similar way to the service request as variables to be bound to the atom logic expressions of antecedents."
"at this stage, the reasoning engine will check if the antecedent of any rule in the rule base is met prior to firing the appropriate rule. the consequence of the fired rule is then used for firing other rules. in this way, the forward chaining reasoning can take into account user preferences, environmental and application context and the requested services to provide personalised services based on domain knowledge and heuristics."
"v. conclusions we provided an experimental study of uwb indoor tracking based on pf. we considered four different likelihood functions that differ in terms of how nlos measurements are handled. according to our results, the best performance is achieved with toa-based ranging with gpr-based estimation for nlos measurements, but this approach has the highest computational complexity. toa-based ranging with mitigation of nlos bias also provides good performance and it has a reasonably low complexity. the other two methods that we considered are not appropriate for indoor tracking due to the accumulation of error. for the future work, we will try to develop a sparse variant of the gpr-based method in order to reduce its complexity. moreover, we will try to develop a one-step tracking method in which the range estimates are not required."
"while ontological user profile models provide the computational representations of user needs, reasoning mechanisms are required to infer the best way a service should be delivered in terms of the user model and application context. there are currently three categories of personalisation reasoning techniques that can be categorised as cbr, cf and rule-based reasoning. cbr is a known method of solving a new problem by analysing previous solutions or similar problems within the same area [cit] . it is concerned with adapting services to new situations through remembering previous situations or cases."
"to address the growing need for service personalisation in pervasive environments, this paper proposes a novel approach based on a service-oriented distributed system architecture. this approach makes use of semantic web technologies for the purposes of user modelling and reasoning for personalisation. we analyse user behaviours and needs for the purposes of enabling a more effective context-aware service. the paper aims to enhance existing approaches to personalisation by focusing on the dedicated user profile model for the purpose of providing a 'help-on-demand' (hod) service. specifically, it proposes a method that represents user needs and provides a personalised service component to suit the changes in these needs. we aim to provide personalised assistance to healthy users that may or may not have minor vision or hearing impairments."
"while for (⋅) it is reasonable to assume a 2d zero-mean gaussian distribution (with covariance matrix σ ), the model for (⋅) is expected to be more complex due to the presence of nlos conditions. this problem will be considered in the next section."
"the deployment of the transmitters and the receivers in considered area is shown in fig. 1 . for the tracking problem, we used 6 tx positions (3 los and 3 nlos) representing the beacons, and 30 rx positions representing the moving target (we assumed a constant velocity of 1 m/s in the horizontal direction). the nlos condition was caused by tunnel walls built of concrete blocks with steel reinforcement. for each txrx pair, we obtained 10 cirs, so we obtained 1800 cirs in total (900 los and 900 nlos). half of these samples will be used as a training data for likelihood parameters. note that we did not perform the measurements of velocity, so we will generate the synthetic data for dynamic model."
another limitation of the current application includes the inability to determine how wi-fi or 3g/gsm signal strength affects the quality or speed of content transmission from server to smart-phone.
"there has also been an emergence of rule-based approaches to user personalisation. rulebased techniques make use of domain knowledge and heuristics to define causal relationships between user profiles and service output through a set of rules. while such approaches have been used in various sectors such as the area of e-commerce and web personalisation, or mobile tourism, these techniques are a relatively new method for help-on-demand personalisation in context-aware, mobile computing."
"the remainder of this paper is organized as follows. in section ii, we describe the system model and the tracking algorithm based on pf. then, in section iii, we define four different likelihood models for range estimation. the experimental results, based on uwb measurements from a basement tunnel, are provided in section iv. finally, in section v, we summarize our conclusions and discuss future directions."
"jack is required to travel from the uk to germany and when he arrives, he attempts to purchase a train ticket from an automated machine. the default language on the machine is german and the text size is too small to read. instead, jack scans the nfc tag on the ticket machine using his smart phone. this activates his travel application, which passes the nfc_id of the ticket machine to the cms (content management system), located on the server. the cms reasons (using pellet and the associated swrl rule-set) on the nfc_id received, taking into consideration jack's current location (using gps) and his available connectivity and compares this to jack's user profile. in this case, jack's user profile highlights that he always requires assistance in a video format with english closed captions."
"knhanes is the nationwide program to evaluate koreans' health and nutritional status. it consists of 3 parts: health examination, health interview and nutrition survey. [cit] . the knhanes datasets are released for public use within one year of the end of each survey year [cit] . after removing missing valued rows, a total of 25,990 records include 11,317 men, and 14,673 women were used in our experiment. there are 12,915 records of high-risk people and 13,075 records of low-risk people in the dataset. selecting valuable features from the experimental dataset plays an important role in building accurate prediction model. it not only reduces the space and computation complexity but also improves the performance of the classification model. we selected 14 features that influenced in chd from the total number of 423 features of a health interview, health examination, and nutrition survey using an extremely randomized tree classifier in the scikit-learn machine learning package in python on all variables and chi-square test on categorical variables. the general descriptions of the selected features are shown in table 1 ."
"where is a large positive constant and 0 1 h h is a match index which measures how closely the radio access rates of small base stations match the backhaul rates of small base stations which is given by (11). then, 1 h represents the punishment function for violating the constraint c5 in (8). based on the ioa, the detailed steps of the user association considering backhaul rates algorithm are described in algorithm 1. (10)-(14)."
"in this paper, we focus on jointly optimizing the ps ratio and the pa ratio for a power-constrained, two-way relay network with a wireless powered relay that employs the df protocol, a problem that has not been studied yet, to our knowledge. the major contributions of our works are summarized as follows. we form a jointly optimizing ps ratio and pa ratio problem to minimize the system outage probability in the context of a two-way energy-harvesting relay network that is subject to a total transmit power constraint. since this original optimization problem is very difficult to solve, if not impossible, a two-step solution is developed to obtain a closed-form solution efficiently. simulation results are obtained to verify the proposed algorithm and to assess the impact of various network parameters on system performance. the performances of three existing schemes are also simulated and compared with that of the proposed scheme."
"to cope with the above two problems and maximize the overall throughput of all small cells, a joint scheme of users association and resource allocation is proposed. due to the combinatorial and nonconvex features of original optimization problem, it was divided into two subproblems. the first one is user association and the boresight angles optimization considering backhaul rates, which maximizes sum access rate per hz of all small cells as well as makes the radio access rates of small cells match their backhaul rates, so that the bandwidth allocation ratios of all small stations are as close as possible to the optimal bandwidth allocation ratio of the network. immune optimization algorithm (ioa) is adopted to obtain the association variables and the boresight angles of users and base stations. after that, the access rates of small cells are obtained. then, the second one is the dynamic optimal backhaul and access bandwidth allocation, which can be solved by differentiating the general expression of overall throughput. the bandwidth allocation ratio between radio access and backhaul is updated dynamically to adapt to the environment variation during each transmission time interval (tti)."
"where re is the reconstruction error vector, and n is the number of elements in re vector. reconstruction error based deep neural networks for coronary heart disease risk prediction"
"rf. the random forest is the one kind of ensemble algorithm [cit] . it creates several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting [cit] . in this research work, we have adjusted the number of trees in the forest between 10 and 150. svm. the support vector machine is a supervised learning method, and it has shown promising empirical results in many practical applications [cit] . it finds a separator line called the hyper-plane that differentiate the classes very well and learns by minimizing the classification error and maximizing the margin. svm can be extended to non-linearly separable data using kernel function. we have built the svm model by using kernel functions such as linear, poly, rbf, and sigmoid."
"our goal is to provide insights into the optimal ps at the relay and the optimal pa at each source node for the proposed swipt-twr network to minimize the network outage probability derived in equation (7). that is, the optimization problem is formulated as:"
the optimal backhaul bandwidth and access bandwidth for the n-the small cell is ra bh n n bh ra n n r b b r r (15) and ra bh n n b b b
"where c1 and c2 are the boresight angles constraints of users and base stations, respectively. c3 and c4 are the user association constraints, and c4 means one user can be only associated to one small station at most. it can be easily seen that the optimization problem in (7) is nonconvex and combination optimization. it is very difficult to obtain the optimal solution. in the following section, we will divide the problem into two subproblems and solve them one by one."
"in this paper, we presented the dnn based chd prediction model. nn is a blackbox model, it does not give importance of each feature, and any insights on the structure explicitly. therefore, we trained several dnn models by removing each features one by one from all and then ranked all features by how they affect the dnn model. first, we trained a baseline model with dnn based on all 14 features. then, features were ranked depends on the difference of accuracy between the baseline model and the newly trained model that eliminated a feature, shown in table 2 . from the table, all features affect the effectiveness of prediction results because all accuracy was decreased when get rid of a particular feature."
ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi 1 n
"the experiment was done on a computer with i5-8500 cpu, nvidia geforce gtx 1070 ti graphics card, and 32gb ram. we have compared our proposed approach to other machine learning-based algorithms by experimenting with total 6-years data that integrated knha-nes-v and knhanes-vi datasets. all compared algorithms were implemented in python with keras, which is a high-level neural networks api, written in python and capable of running on top of tensorflow. for ae-dnns, two dnn classifiers were configured as same as each other. also, the learning rate to minimize the mean squared error was 0.001, and the adamax optimizer was used [cit] . the batch size was 32, and the number of epochs was 5000. before train prediction models, we have normalized our dataset. standardization is a widely used normalization method in machine learning algorithms, and it calculates mean and standard deviation for each attribute of a dataset. then each value is subtracted by the mean, and the subtracted result is divided by the standard deviation."
", is the noise generated at source s i . for the noise components in the source and relay nodes, it is reasonable to assume"
"this paper presents a clustering technique for massive numeric datasets. the clustering algorithm is based on density approach and can detect global as well as embedded clusters. experimental results are reported to establish the superiority of the algorithm in light of several synthetic data sets. in this project we have only considered two-dimensional objects. but, spatial databases also contain extended objects such as polygons. therefore, there is scope for scaling the proposed algorithm to detect clusters in such datasets with minor modifications, research of which is in progress. from a proper analysis of the designed technique, it can be safely concluded that the algorithm developed is working properly to a great extent."
"considering a downlink cellular network consisting of a single macro cell with n small cells (see fig.1 ). assuming there are u users (ues) randomly located in n small cells. the wireless backhaul traffic of small cells is transmitted from the macro base station (mbs) by millimeter wave communication links while the mbs is connected to the core network by and backhaul (uab) networks utilize a c/u split architecture where the control-plan is managed by the macro station through low frequency bands while the user-plane is processed by small stations through high frequency bands. for the sake of making it simple, the control-plan is not depicted in fig.1 . the radio access links and backhaul links share the whole bandwidth. in this paper, the frequency division is adopted in which the bandwidth allocation ratios of all small stations are the same in order to avoid frequency band overlap between access and backhaul from other small stations."
"5: 2) evaluate the goodness of antibodies jointly taking affinity and concentration into consider-6: ation, and obtain the expected propagation probability according to (9), and record the best 7: individual. 8: 3) form the parent population and update the memory pool."
"in this paper, we present a joint user association and resource allocation algorithm to maximize the overall throughput of small cells for selfbackhaul ultra-dense networks, in which association indicators and the boresight angles of the users and base stations are optimized by ioa firstly, and then the optimal bandwidth allocation is calculated by differentiating the general expression of overall throughput. the convergence and effectiveness of the proposed scheme are validated by extensive simulations. simulation results show that the proposed algorithm makes the access rates of small cells match their backhaul rates,and the overall throughput of small cells are improved"
"by solving equation (24b), one obtains the minimum of [ in this case, λ(α) is generated from equation (19), which is a three-segment continuous function. since f 3 and g 2 (α) have a relationship with the size of the channel gains k 1 and k 2, the same as for case 1, case 2 has three subcases, as well."
"in this study, we used dae-general and dae-risky two deep ae models, showing in fig 1. the dae-risky model learns from the only high-risk subset of the training dataset for the re based feature extraction. the dae-general model is used to choose an appropriate classifier in chd risk prediction module that trains on the whole training dataset. in this section, we will describe how the dae-risky model is employed in the feature extraction process. it is possible to identify well which differentiation of re is risky or normal by training the ae model on the only risky dataset. in other words, if we give a person's data who has low chd risk as an input of dae-risky model, the re will tend to be higher because the model did not learn from the low risk dataset. algorithm 1 shows the steps of how to do feature extraction, one of two fundamental functions of our proposed approach. algorithm 1 feature extraction. first, all risky subset is selected from the n number of training dataset and then the daerisky model is trained on the selected subset. according to algorithm 1, the re based new feature is calculated by a squared difference between initial input and its reconstructed output. the equation of the proposed deep ae neural network can be written in the vectorized form: where tanh and relu are the activation functions, w l and b l are the weight matrix and the bias vector for each layer, l, and x is the input vector."
"we have compared the proposed ae-dnns to the following supervised machine learning techniques. for the compared algorithms, we have chosen optimal values of input parameters by changing their values until decreasing the performance."
"nb. the naïve bayes is probability-based classification algorithm. it computes the posterior probability for each class label and picks the class label that has the highest probability. in nb, it does not calculate the probability based on combined features, instead, considering all features separately; it is called conditional independence [cit] ."
"in this paper, we have proposed a deep learning-based model to predict the risk of developing chd and evaluated it in the korean population. in the proposed method, two fully connected dnn classification models are combined with a deep ae models successfully. generally, ae is used for dimensionality reduction. however, we did not use ae for dimensionality reduction purposes; it was employed as providing re by projecting back reduced dimension into its original space. two ae models named dae-general and dae-risky learned from the whole training dataset and the high-risk datasets, individually. first, re based feature was extracted from the dae-risky model, and it was used to feed the dnn model with other risk factors. then, based on the re of the dae-general model, the whole training dataset was partitioned into two different subsets. finally, two independent dnn classifiers were trained on each group; each group consists of the chd risk factors and re based newly created feature. in the prediction process, we compared the re on the dae-general model of each testing data to the previously determined threshold value and chose an appropriate classifier from these two dnn classifiers. by using two dnn classifiers with re based feature, we improved the performance of single nn classifier on the whole dataset. experimental results showed that the proposed ae-dnns outperformed all the compared classifiers with accuracy, f-measure, and auc score of 86.33%, 86.91%, and 86.65%, respectively."
"knn. the k-nearest neighbor algorithm is used for the classification task. in the classification phase, a user defines the value of the k parameter, and an unlabeled instance is labeled by the most frequently occurred class label among the k number of nearest training samples. first, it calculates distances between the unlabeled instance and each training data to find the nearest neighbors. belongs to the k number of nearest neighbors, a majority voted class label will be assigned to the output label. we have configured the value of the k between 2 and 20."
"in this section, the overall throughput optimization problem is divided into two subproblems. the first one is user association considering backhaul rates to maximize sum access rates per hz of all small cells, where we obtain the binary association variables and the boresight angles of users and base stations by immune optimization algorithm (ioa). the second one is dynamically bandwidth allocation for all the small stations, where we find the optimal backhaul bandwidth and radio access bandwidth."
"to support the exponentially increasing demand on mobile data traffic, it is necessary for 5g wireless networks to boost network throughput by more than 1000 [cit] . current cellular systems could not cope with such traffic growth in an economical and ecological way. researchers have been seeking solutions in order to meet such traffic growth. one of the solutions for such traffic growth is to deploy low-cost and low-power small cells, which is called ultra-dense networks (udns) [cit] . udns are expected to increase both spectrum another solution is so-called millimeter wave (mmwave) technology [cit] . extremely wide available bandwidth is the most appealing feature of mmwave. the high attenuation is another main feature of millimeter-wave communications, which promotes mmwave frequencies to ideal candidates for udns with the aid of beamforming offering very high data rates. thus, udns combined with mmwave communications are expected to provide high spatial multiplexing gain and wide bandwidths for multi-gigabit peak data rates to meet the exponentially increasing demand on mobile data due to the network densification, it is unfeasible for a large number of small base stations (sbss) to connect to macro stations by fiber links because of expensive cost [cit] . the wireless self-backhaul solution is becoming an attractive alternative for udns, where the same radio spectrum is used for both access and backhaul transport [cit] . there are two options for mmwave spectrum resource reuse between radio access links and backhaul links. one is the full sharing by full-duplex (fd) communication [cit], which implies simultaneous transmission and reception of information in the same frequency at the same time. however, the gains of full sharing are limited by the overwhelming nature of self-interference (si) generated by the transmitter to its own collocated receiver. another is partially sharing by separating the radio access links and the backhaul links in time or in frequency resource to mitigate interference between them. since time-division scheme requires all small cells to perform either access or backhauling transmission simultaneously and is particularly troublesome for sbss, as they cannot appropriately balance two-hop link [cit] . hence, frequency-division scheme between access and backhaul may be one possible choice."
"assuming the overall bandwidth is bh ra b b b, where is b bh the backhaul bandwidth and is the radio access bandwidth. in this paper, we mainly focus on the data transmissions in mmwave frequency band. for the n-th small station, the"
"in the chd risk prediction process, first, test data is given as an input of the trained daegeneral model, and its re is calculated. if the re exceeds the threshold, the dnn model 1 that trained on data group with high re will be used; otherwise, the dnn model 2 that trained on data group with lower re will be used to predict the class label, as shown"
"for analytical tractability, the actual antenna patterns are approximated by a sectored antenna model [cit] . this simple model captures directivity gains, the front-to-back ratio, and the half-power beamwidth, which are considered the most important features of an antenna pattern. in ideal sector antenna pattern, the gains are a constant for all angels in the main lobe, and equal to a smaller constant in the side lobe. let"
"the major drawback in such clustering approaches is the processing time. the time to scan the whole database and cluster accordingly is a major area of concern. this project is motivated by the density-based clustering approaches to discover nested clusters and clusters of arbitrary shapes. fig. 2 displays a nested cluster and a multi-density cluster. the basic idea is to reduce the processing time and enhance the efficiency henceforth. for the fulfillment of the requirements of the undertaken research the sampling technique is proposed. it is used as a data reduction technique because it allows a large data set to be represented by a much smaller random sample (or subset) of the data. an advantage of sampling for data reduction is that the cost of obtaining a sample is proportional to the size of the sample, as opposed to the whole data set size. other data reduction techniques can require at least one complete pass through. sampling is a natural choice for progressive refinement of a reduced data set. such a set can be further refined by simply increasing the sample size. this technique of clustering can be efficiently used in a spatial database, where we can choose to define clusters geographically based on how closely different areas are located. better cluster quality and more acceptable complexity are the major features of the proposed approach."
"after threshold estimation, we partitioned the training dataset into two parts; each part consists of subsets labeled by high-risk and low-risk. the first part contained a dataset with high re that exceeds the threshold value and the second part consisted of the rest of the dataset. subsequently, dnn classifiers are trained on each group separately using chd risk factors with re based newly created feature. each of nn classifiers is the same structure that has four hidden layers with neurons 10, 7, 5, and 3, respectively. the input layer consists of 15 neurons, including the selected chd risk factors, and the re based feature to predict target variable y."
"in this case,λ(α) is a continuous two-segment piecewise function generated from equation (18), with the size of k 1 and k 2,λ(α) and op3 have three subcases, which are analyzed as follows."
"reducing the difference between inputs and outputs [cit] . thus, the number of input neurons equal to the number of output neurons in ae. in general, the structure of ae is similar to nn with one hidden layer, at least, whereas ae is distinguished from the nn for predicting output label by the aim of input reconstruction. as shown in fig 2, ae consists of the encoder and decoder parts; firstly, it projects input x to a lower dimension that works in encoder part, then it reconstructs output x' from the low dimensional projection that works in decoder part. in other words, the learning process of ae is that it compresses the input into a lower-dimensional space called latent space and uncompresses back the compressed data into the output that closely matches the original data. then, it calculates a difference between the input and reconstructed output and changes the weights of the network to reduce this difference. ae takes an input vector x and maps it into a hidden representation h, then the hidden representation h, sometimes called the latent space, is mapped back to a reconstructed vector x'. the following equation describes an ae:"
"3) almost all clustering algorithms require input parameters, accurate determination of which are very difficult, especially for real world data sets containing high dimensional objects. moreover, the algorithms are highly sensitive to those parameters. 4) none of the techniques discussed above, is capable in handling multi-density datasets as well as multiple intrinsic or nested clusters over massive datasets qualitatively. e. motivation: most partitioning methods cluster objects based on the distance between them. such methods can find only spherical-shaped clusters and encounter difficulty at discovering clusters of arbitrary shapes. so other clustering methods have been developed based on the notion of density. the general idea is to continue growing the given cluster as long as the density (number of objects or data points) in the \"neighborhood\" exceeds some threshold; this is, for each data point within a given cluster, the neighborhood of a given radius has to contain at least a minimum number of points. such a method can be used to filter out noise (outliers) and discover cluster of arbitrary shapes. these typically regard clusters as dense regions of objects in the data space that are separated by regions of low density (representing noise). dbscan grow regions with sufficiently high density into clusters and discover clusters of arbitrary shape in spatial database with noise according to a density-based connectivity analysis. it defines a cluster as a maximal set of density-connected points."
"for the static scene, it can be seen that the overall throughput of the proposed scheme is increased by 40% at most compared to the association based on minimum distance when there are 100 small cells, and the overall throughputs will decrease with the increasing of the number of small cells, which is caused by the increase of the interference, but it can also improve the overall throughputs by 10% when the number of small cells increase to 200."
"in this section, the simulations are presented to verify the performance of the proposed joint user association and resource allocation algorithm in self-backhaul ultra-dense networks. assuming there are four ues randomly located in the coverage area of a small cell. the main simulation parameters [cit] are listed in table i."
"the power splitter at r then splits the received signal y r into two portions ρ : (1−ρ), of which √ ρy r is utilized for energy harvesting, and the remaining portion in the amount of 1−ρy r is used for information decoding. let the energy conversion efficiency be η. the harvested energy at the relay is expressed as:"
"proof. the convexity of this case can be verified using a similar analysis as lemma 1. given α 12 as an initial value, it is easy to see that g 1 (α) decreases as α decreases from this initial value; however, g 2 (α) increases as α decreases from this initial value. this characteristic shows that the optimal value of λ(α) occurs in the range of [0, α 12 ], where"
"represents the set of virtual base stations(bss). p i is the transmission power spectrum density of bs i. n 0 is the background noise power spectrum density., c i j g is the channel gain between bs i and ue j, capturing both path-loss and shadowing effects. note that if bs i has n i rf chains (analog beams), bs i includes n i bss located at the same position, each having one rf chain."
"in this subcase, g 2 (α) is an increasing function, and f 3 does not affectλ(α). through analysis, lemma 1 can be obtained as follows."
"author contributions: the main contributions of c.p. and g.w. were to create the main ideas and execute the performance evaluations by theoretical analysis and simulation, while f.l. and h.l. worked as the advisors of c.p. to discuss, create and advise on the main ideas and performance evaluations together."
the rest of this paper is organized as follows. section ii introduces the system model and the problem formulation for maximizing the overall throughput of all the small cells. section iii analyzes a joint scheme of users association and resource allocation. section iv evaluations are provided in section v. conclusions are drawn in section vi.
starting from the densest triangle we traverse the neighboring triangles in both directions. first we can start in clockwise direction and when completed we may go in anti-clockwise direction. the basic purpose of traversing is to find the ratio between two adjacent triangles which share the same edge among them (neighbor triangles). fig. 8 given below shows the possible ways of traversal. figure 8 -traversal of neighbor triangles from maximum dense triangle e. merging two triangles: a triangle can be merged with its neighbor triangle if the ratio of densities between them is greater than a threshold set by the user i.e. the triangle satisfies the density confidence condition w.r.t its neighbor. thus these reachable triangles can be considered as the same cluster. this process is performed iteratively till no more triangles of an octagon can be merged in either direction. fig. 9 shows a case in which two neighboring triangles are merged into one cluster satisfying the density confidence condition.
"to evaluate the technique in terms of quality of clustering, the algorithm was also applied on the chameleon t4.8k.dat, t5.8k.dat, t8.8k.dat and t7.10k.dat datasets [cit] . fig. 12(a),12(b),12(c) & 12(d) shows the result of clustering in chameleon t4.8k.dat, t7.10k.dat, t8.8k.dat and t5.8k.dat datasets respectively. the results obtained are shown in fig. 15(a) from the experimental results given above, we can conclude that tdct is highly capable of detecting intrinsic as well as multi-density clusters qualitatively. however, using the idea of grid-based clustering along with tdct can result in more accurate results which is again a future scope of research."
"the remainder of this paper is organized as follows. in section 2, we describe the model of the two-way relay network being considered and formulate the joint resource allocation problem, aiming to minimize the outage probability. transformation of the joint resource allocation problem to a two-step optimization problem is described in section 3. numerical results are presented in section 4, and conclusions are given in section 5."
"where sinr n is the signal to interference and noise ratio between small station n and macro station. sinr un is the signal to interference and noise ratio between small station n and user u n is the set of the users associated to small station n. x un is the binary association variable. for the n-th small station, its throughput is limited by the minimum value of the two throughputs, that is min(, ) bh ra n n n t t t (6) in order to maximize the overall throughputs of all the small stations, the resource allocation problem formulation jointly considering user association and dynamically bandwidth allocation can be given as"
"experimental results show that we can use the ae-dnns method for chd risk prediction because it has given higher performance than other methods. in accordance with tables 2 and 3, reconstruction error based deep neural networks for coronary heart disease risk prediction the rf algorithm had the highest accuracy, f-measure, and auc scores from compared algorithms except for the proposed method, 84.42%, 84.99%, and 84.30% respectively. however, our proposed ae-dnns has made these performances 86.33%, 86.91%, and 86.65%. also, we changed ae modules by pca modules, and the result of pca-dnns was higher than compared algorithms, but not well than ae-dnns."
"in this paper, the technique of data clustering has been examined, which is a particular kind of data mining problem. the process of grouping a set of physical or abstract objects into classes of similar objects is called clustering. a cluster is a collection of data objects that are similar to one another within the same cluster and are dissimilar to the objects in other clusters [cit] . given a large set of data points (data objects); the data space is usually not uniformly occupied. data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. besides, the derived clusters can be visualized more efficiently and effectively than the original dataset. mining knowledge from large amounts of spatial data is known as spatial data mining. it becomes a highly demanding field because huge amounts of spatial data have been collected in various applications ranging from geo-spatial data to bio-medical knowledge. the amount of spatial data being collected is increasing exponentially and has far exceeded human's ability to analyze them. recently, clustering has been recognized as a primary data mining method for knowledge discovery in spatial database. the development of clustering algorithms has received a lot of attention in the last few years and new clustering algorithms are proposed."
"major clustering techniques have been classified into partitional, hierarchical, density-based, grid-based and model-based. among these techniques, the density-based approach is famous for its capability of discovering arbitrary shaped clusters of good quality even in noisy datasets [cit] . density based spatial clustering of applications with noise (dbscan) and ordering points to identify the clustering structure (optics) are two of the most popular density based clustering algorithms. in density-based clustering algorithms, a cluster is defined as a high-density region partitioned by low-density regions in data space. they can find out the clusters of different shapes and sizes from the large amount of data containing noise and outliers. fig. 1 depicts the formation of clusters of similar data. this project, which is primarily motivated by density based clustering, aims at proposing a new density based clustering technique for efficient clustering of spatial data. the rest of the paper is organized as follows. section 2 provides a selected literary review on density based, grid based and other multi-density as well as variable density data clustering techniques. section 3 illustrates the background of the proposed work and section 4 gives the final proposed algorithm. in section 5, we present the experimental results and the performance analysis of the work following the complexity analysis in section 6. lastly, we conclude with a summary and conclusion in section 7. section 8 acknowledges the organizations that guided us through this research."
"for the low speed scene, as can be seen from the fig.8, the throughputs of the proposed algorithm and the association scheme based on minimum distance decrease by about 0.9% compared to the static scene, which means the mobility of the users has an effect on both of the above two algorithms at the low speed scene. but the proposed algorithm still outperforms the association scheme based on minimum distance in terms of overall throughput of all small cells."
"to solve equation (11) effectively, we adopt a successive approach to transform the original problem into two subproblems. first, an optimal ps ratio is obtained by fixing α. then, the resulting ps ratio is substituted into equation (11) to derive a closed-form solution of the optimal pa ratio through case studies. this kind of method, which transforms a complex original problem into a series of easily-solved convex problems, is a typical non-convex problem solution, which has been widely used in solving mathematical problems [cit] and solving formulated optimization problems applied in communication scenarios [cit] ."
"assuming the data rates of the backhaul links between any small station and macro station are fixed. because the backhaul transmission rates are usually determined by the path loss between the two stations [cit], this assumption is appropriate for the actual situation. in order to make the radio access rate of the small station match its backhaul transmission rate, we introduce the constraint:"
"pca-dnns. the proposed ae-dnns uses the re for two kinds of purposes. the first one is re based feature extraction by the ae model that trained on the high-risk subset of the training dataset. the second one is used to arrange training dataset into two groups based on their re divergence by the ae model that trained on the whole training dataset. subsequently, two dnn models learn from these groups. in the prediction process, the second ae model is also employed to choose a proper classification model. therefore, we have used principal component analysis (pca) in place of ae for calculating re. pca is a dimension reduction technique that the direction with the largest projected variance is called the first principal component. the orthogonal direction that captures the second largest projected variance is called the second principal component, and so on [cit] . we can estimate re by projecting back a low dimension to original space using pca."
"where w and w' are the weight matrices, b and b' are the bias vectors, and a and a' are the activation functions. the parameters of the ae model are optimized to minimize the average re, as shown in eq 2:"
"we show the result of the ae-dnns by comparing with machine learning-based nb, rf, knn, dt, and svm algorithms. the design of the experimental study for the proposed method is shown in"
"since λ is a complex minimization function of two variables α and ρ, it is very difficult, if not impossible, to solve for a solution directly based on the optimization formed in (11), but λ(α, ρ) in equation (9) is observed to be a concave function of ρ for a fixed α. this observation leads to a two-step approach to solve this joint resource allocation problem, which is developed in the next section."
"we have derived a joint optimal resource allocation design for df-twr networks with swipt to minimize its system outage probability. the optimization of such a network is a very complex problem. to make it easy to tackle, a two-step method is proposed. with the two-step method, the optimal ps ratio for a given pa ratio is derived first, from which it is found that it is a function of the pa ratio. then, the obtained ps ratio is substituted back into the main optimization problem to determine the closed-form of the optimal pa ratio. simulation results matched the analytical results well and confirmed that the optimized system achieved a lower outage probability than existing schemes, especially in asymmetric channel conditions."
"with the available instantaneous channel state information (csi), it is more desirable to formulate an equivalent joint optimization problem, which aims at maximizing the normalized snr λ shown in equation (9) . this transformed optimization problem is expressed as:"
"equation (17) shows that the optimal α o is determined by g 1 (α) and g 2 (α). define the three components of g 1 (α) in equation (14a) as 13 and α 23 be, respectively, the points of intersection of f 1 and f 2, f 1 and f 3 and f 2 and f 3 . the analytical expression ofλ(α) can be classified as two cases, which is shown in the following theorem 1."
"the features such as age, knee joint pain status, lifetime smoking status, waist circumference, neutral fat, body mass index, weight change in one year status, systolic blood pressure, total cholesterol, obesity status, frequency of eating out, high-density lipoprotein cholesterol, marital status, and diabetes were used as risk factors of chd prediction model. hypertension, dyslipidemia, stroke, myocardial infarction, angina, and hyperlipidemia were used to identify class labels (high-risk or low-risk). in other words, in case one of these 6 disorders is identified, the individual will be considered to have high chd risk."
"here, we introduce some definitions which are used in the proposed algorithm: definition 1 triangle density: the number of spatial point objects within a particular triangle of a particular polygon (octagon). definition 2 useful triangle: only those triangles which are populated i.e., which contain data points will be treated as useful triangle. definition 3 neighbor triangle: those triangles which have a common edge (edge neighbors) to the current triangle are the neighbors of the current triangle. fig. 3 shows the neighbor triangles (in grey shade) of the current triangle 1."
"in this study, we proposed two nn classifiers trained on different groups of a dataset. in practically, a dataset can include a subset which is higher variance than the most dataset and that highly biased dataset degrades the performance of classification techniques. therefore, we isolated a highly biased dataset from the common dataset using the dae-general model that learned from the whole training dataset. the data that is different from most dataset gives higher re than common data on the dae-general model. as shown in fig 1, we used two independent deep ae models. the dae-general is used for data grouping and and selection of the chd risk prediction model. the dae-risky is used for feature extraction. the only difference is that dae-risky trained on risky subset while the dae-general trained on the whole training dataset. for estimating the data splitting threshold, first, we calculated reconstruction errors of the training dataset by the squared difference between the input and output. the threshold value is estimated by the mean and standard deviation of the reconstruction errors; it can be described as:"
the distribution of data in a data set is not uniform in general. some portions of the data space are highly dense while some portions are sparse. the problem associated here is efficient clustering of spatial data points to discover nested clusters and clusters of arbitrary shapes using density based clustering technique.
"the decision tree classifier is a simple algorithm that has been widely used so far [cit] . the goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data. classification and regression trees (cart) [cit] and very similar to c4.5, but the main difference is that it supports numerical target value (class label). it builds both classification and regression trees [cit] . the classification tree construction by cart is based on the binary splitting of the attributes. we have used \"gini\" for the gini impurity and \"entropy\" for the information gain to measure the quality of a split."
"where c1-c4 have the same means with the above formula (7) . c5 ensures that the radio access rates ratios of any two small stations are proportional to their backhaul rates ratios. the optimal resource allocation problem in (8) is a mixed combinatorial and nonconvex optimization. the combinatorial feature is caused by the constraints c3 and c4, and the nonconvexity nature comes from proportional constraint and the directivity gain determined by solution for the resource allocation problem in (8) . thus, in this paper, ioa is adopted to obtain the resource allocation and user association policies. ioa is a well-known optimization algorithm for nonconvexity optimization problems and nonlinear multi-objective optimization problems [cit] . ioa has four important parts, i.e., antibody, concentration, clone, fitness. an antibody in population is a real-value attribute string in the euclidean shape-space, and the concentration represents a measure of diversity. clone states that offspring antibodies are identical to the parent antibody, and fitness function is adopted to measure the goodness of antibody. each antibody represents a possible solution of the optimization problem. in this paper, each antibody represents a possible joint scheme of users association, the boresight angles of users and base stations. note that when we obtain the integer association variable x . the detailed antibody encoding is described in fig.3 . in the antibody population, the expected propagation probability for the v-th antibody p v is determined"
"where the output layer uses the sigmoid activation function, and all of the hidden layers use the relu activation function, w l and b l are the weight matrix and the bias vector for each layer, l, and x is the input vector."
"two consecutive phases are involved to complete each round of information transmission: a multiple access (ma) phase and a broadcast (bc) phase. during the ma phase, nodes s 1 and s 2 broadcast their signals simultaneously to relay r. the received signal at r in this phase is expressed as:"
"for a fixed α, the optimization problem reduces to a one-dimensional problem related to the power splitting ratio ρ. for ∀α, the optimization problem can be expressed as:"
"it is easy to notice that equation (13) is a convex function of ρ for a given α. thus, the optimal ρ derived from equation (13) is obtained when ("
"note that power splitting is done before the received signal is converted from passband to baseband; hence, the signal in the information decoding (id) receiver can be expressed as:"
"nn is mostly used to predict output labels from the input, consisting of an input layer, hidden layer, and output layer [cit] . the input layer is composed of neurons that describe input features whereas neurons in hidden and output layers receive a result of activation function that converts the weighted summation of the neurons of the previous layer, as shown in fig 3. nn learns by changing the weights of each neuron to reduce an error between target class y and predicted class y'."
"however, two problems still exist when the frequency-division scheme is adopted between access links and backhaul links. first, in order to avoid frequency band overlap between access and backhaul from other small stations, the bandwidth allocation ratios of all small stations should be the same in frequency-division scheme [cit], which are not optimal for all small cells to maximize their own throughputs. second, to maximize actual end-to-end throughput, some dynamic bandwidth allocation schemes should be available to balance two-hop link resources for adapting to the environment variation."
"the results shown in figure 6 [cit] . for a short well, the behaviour of the currents is independent of conductivity, so, as long as the borehole is approximated by a sufficiently conductive target, the behaviour of the fields and fluxes will be representative of the fine-scale model. however, as the length of the well increases, the cross-sectional conductance of the well, becomes relevant as it controls the rate of decay of the currents in the well and thus the rate that currents leak into the formation. a similar result holds when a line of charges is used to approximate the well as a dc source; a uniform charge is suitable for a sufficiently short or sufficiently conductive well, whereas a distribution of charge which decays exponentially with depth needs to be considered for longer wells. thus, when attempting to replace a fine-scale model of a well with a coarse-scale model, either with a conductivity structure or by some form of \"equivalent source\", validations should be performed on models that have the same length-scale as the experiment to ensure that both behaviors are being accurately modeled."
"although we do not have a solid mathematical expression to explain these results, the important point to be made here is that ability to simulate fine scale structure and plot the resultant charges and fields enabled us to interrogate these phenomena."
"additionally, we compare these results to the 3d [cit] . the mesh in the ubc simulation included 5 011 924 cells, with the finest cells being equal to the width of the casing;"
"the additional complications that are introduced are: (1) the periodic boundary condition introduced on boundary faces and edges in the azimuthal direction, (2) the removal of radial faces and azimuthal edges along the axis of symmetry, and (3) the elimination of the degrees of freedom of the nodes and edges at the boundary and as well as the nodes and vertical edges along the axis of symmetry. the implementation of the 3d cylindrical mesh is provided as a part of the discretize package (http://discretize.simpeg.xyz), which is an open-source python package that 8 contains finite volume operators and utilities for a variety of mesh-types. discretize is a part of the larger simpeg ecosystem . all differential operators are tested for second order convergence and for preservation of mimetic properties [cit] . one of the benefits of simpeg for forward simulations is that values of the fields and fluxes are readily computed and visualized, which enables researchers to not only simulate data but also examine the physics. this is particularly powerful when combined with the interactive jupyter environment [cit] ."
"it is important to note that although the product of the conductivity and permeability is identical for these wells, the geometry of the well and inducing fields results in different couplings for each of the parameters. for a vertical magnetic dipole source, the electric fields are purely rotational while the magnetic fields are primarily vertical. an approximation we can use to understand the implications of these geometric difference is to assume the inducing fields are uniform (e.g. the radius of the source loop is infinite) and to examine the conductance and permeance of the pipe."
"the software implementation is included as a part of the simpeg ecosystem. simpeg also includes finite volume simulations on 3d tensor and octree meshes as well as machinery for solving inverse problems. this means that the cylindrical codes can be readily connected to an inversion and additionally, simulations and inversions of more complex 3d geologic settings can be achieved by coupling the cylindrical simulation with a 3d tensor or octree mesh using a primary-secondary approach (e.g. example 3 [cit] ). beyond modelling steel cased wells, we envision that the 3d cylindrical mesh could prove to be useful in conducting 3d [cit], is adopted."
"in the intermediate zone, kaufman discusses a number of interesting aspects with respect to the behavior of the electric fields and currents which we can compare with the observed behavior in figure 5 . among them, he shows that the electric field within the borehole and casing is directed along the vertical axis; as a result no charges accumulate on the inner casing wall. charges do, however, accumulate on the outer surface of the casing; these generate radiallydirected electric fields and currents, often referred to as leakage currents, within the formation. at each depth slice through the casing and borehole, the electric field is uniform, however, due to the high conductivity of the casing, most of the current flows within the casing. the vertical extent of the intermediate zone depends on the resistivity contrast between the casing and the surrounding formation and extends beyond several hundred meters before transitioning to the far zone, where the influence of the casing disappears [cit] )."
"to discretize maxwell's equations in time (equation 1) or frequency (2), we invoke the constitutive relations to formulate our system in terms of a single field and a single flux, giving us a system in either the electric field and magnetic flux (e-b formulation), or the magnetic field and the current density (h-j formulation). for example, in the frequency domain, the e-b formulation is"
"the medium can have variable electrical conductivity and magnetic permeability. the 2d solution is especially computationally efficient and has a large number of practical applications. when cylindrical symmetry is not valid, the 3d solution can be implemented; a judicious design of the mesh can often generate a problem with fewer cells than would be required with a tensor or octree mesh. we demonstrated the versatility of the codes by modelling the electromagnetic fields that result when a highly conductive and permeable casing is embedded in the earth. this application was chosen because it is of current interest in the geophysical community and because the large contrasts in physical properties and length scales make it a numerically challenging problem."
"of critical importance was the ability to plot the charges, fields, and fluxes in the simulations. this is valuable for understanding the responses obtained from the experiment and it is a solid foundation for designing a field survey."
"of our numerical tools, we require the ability to simulate large electrical conductivity contrasts, include magnetic permeability, and solve maxwells equations at dc, in frequency and in time in a computationally tractable manner."
"when employing a cylindrical mesh, the distinction between where the electric and magnetic contributions are discretized in each formulation has important implications. if we consider the cylindrically symmetric mesh ( figure 1b ) and a magnetic dipole source positioned along the axis of symmetry (sometimes referred to as the te mode), we must use the e-b formulation of maxwell's equation to simulate the resulting toroidal magnetic flux and rotational electric fields. if instead, a vertical current dipole is positioned along the axis of symmetry (also referred to as the tm mode), then the h-j formulation of maxwell's equations must be used in order to simulate toroidal currents and rotational magnetic fields. the advantage of a fully 3d cylindrical mesh provides additional degrees of freedom, with the discretization in the azimuthal direction, allowing us to simulate more complex responses. however, in order to avoid the need for very fine discretization in the azimuthal direction, we should select the most natural formulation of maxwell's equations given the source geometry being considered. for a vertical steel cased well and a grounded source, we expect the majority of the currents to flow vertically and radially, thus the more natural discretization to employ is the h-j formulation of maxwell's equations."
"these experiments revealed some insights into the complexity of the fields within the pipe and illustrated the role of permeability in the character of the responses at low frequency. next, we move to larger scales and examine the role of conductivity and permeability in the responses we observe in the borehole."
"there are also a range of scenarios where the footprint of the survey is primarily cylindrical, but 2d or 3d variations in the physical property model may be present. for example if we consider a single sounding in an airborne em inversion, the primary electric fields are rotational and the magnetic fields are poloidal, but the physical property model may have lateral variations or compact targets. more flexibility is required from the discretization to capture these features. in this case, a 3d cylindrical geometry, which incorporates azimuthal discretization may be advantageous. it allows finer discretization near the source where we have the most sensitivity and the fields are changing rapidly. far from the source, the discretization is coarser, but it still conforms to the primary behaviour of the em fields and fluxes and captures the rotational electric fields and poloidal magnetic flux."
"for deeper targets in this experiment, the passing image current has diffused significantly, and thus it appears that the wire location has less impact on the magnitude of the current density with location. however, it is possible that increasing the wire-length could be beneficial. this extension is straightforward and could be examined with the provided script. there may also be added benefit by having the target positioned along the same line as the source wire, as at later times, the direction of current reverses, changing the excitation of the target.. the final point to note from this example is that although this is a simple model, the behavior of the currents is not intuitive; visualizations of the currents, fields and fluxes, particularly when connected with the interactive jupyter computing environment [cit], allow researchers to explore the basic physics and prompts new questions. such simulations and visualizations have proved valuable in the context of geoscience education and can be a useful tool for understanding the physical processes that contribute to the data we observe."
"as the length-scale, l, is larger than the circumference of the pipe (2πr) the geometric contribution to the conductance is larger than that to the permeance."
"simpeg and all of the further developments described in this paper are open source and freely available; all of the examples can be accessed at https://github.com/simpeg-research/ [cit] emcyl. the examples have been provided as jupyter notebooks. this not only allows all of the figures in the paper to be reproduced, but provides an avenue by which the reader can ask questions, change parameters, and use resultant images to confirm (or not) his or her presumed outcome. we hope that our efforts to make the software and examples accessible promotes the utility of this work for the wider community."
"we start by considering a 1km long well (10 6 s/m) in a whole space (10 −2 s/m), with the conductivity of the material inside the borehole equal to that of the whole space. for modelling, we will use a cylindrically symmetric mesh. the positive electrode is positioned on the borehole axis in the mid-point of a 1km long well; a distant return electrode is positioned 1km away at the same depth."
"conducting a similar experiment in the time-domain, we can compare the responses as a function of time. for this experiment, a step-off waveform is employed and data are measured after shut-off, the nsf is plotted in figure 13 . note here that the secondary field is in the same direction as the primary, so after the source has been shut off, the secondary field is oriented upwards, as shown in figure 14 . shortly after shut-off, the rate of increase in the secondary field is the same for both the conductive and the conductive, permeable wells. a maximum normalized field strength of approximately 1 is reached for both cases. the responses begin to differ at 10 −3 s where the conductive well maintains a nfs ∼ 1 for approximately 1ms longer than the permeable well before the fields decay away."
"in figure 6 (b), we have plotted the charges along the length of the well. in the short-well regime, the borehole is approximately an equipotential surface and the charges are uniformly distributed; in the long well the charges decay with depth. what was surprising to us was the noticeable increase in charge accumulation that occurs near the bottom of the well. this is especially evident for the short well. initially, we were suspicious and thought this might be due to problems with our numerical simulation; there was no obvious physical explanation that we were aware of."
"this is in agreement with figure 3b [cit] . there is however, a significant discrepancy between our numerical simulations and the scale model for the semi-infinite pipe when the source and receiver lie in the same plane (figure 8a) . [cit] observed a static shielding effect for both the infinite and semi-infinite scenarios, whereas we observe a static shielding for the infinite scenario, but a significant static enhancement for the semi-infinite case. to examine what might be the cause of this, we will examine the magnetic flux density in this region of the pipe."
"where e, b, h, j are vectors of the discrete em fields and fluxes; s m and s e are the discrete magnetic and electric source terms, respectively; c is the edge curl operator, and the matrices m e, f"
"at the zero-frequency limit, each formulation has a complementary discretization for the dc equations, for the e-b formulation the discretization leads to a nodal discretization of the electric potential φ, giving"
"capturing the fine-scale features of a conductive, permeable, steel cased well in a 3d electromagnetic simulation is currently in the high-performance computing realm. this means that for a researcher, the tools necessary to investigate the physical behaviour of electromagnetic fields and fluxes, for example, to assess the impact of magnetic permeability or to examine strategies for reducing computational load by making approximations in the forward simulation, are not readily accessible. our aim in this paper is to bridge that gap."
"within the near-zone, the total charge is dominated by the large positive charge at the current electrode location and negative charges that exist along the casing wall where current is moving from a resistive region inside the borehole into a conductor. the extent of the negative charges along the inner casing wall is more evident when we look at the secondary charge, which is obtained by subtracting the charge that would be observed in a uniform half-space from the total charge (figure 5b) . inside the casing, we can see the transition from near-zone behavior to intermediate zone behavior approximately 0.5 m above and below the source; that is equal to 10 borehole radii from the source location, which agrees with kaufman's conclusion."
"testing for the dc, tdem, and fdem implementations includes comparison with analytic solutions for a dipole in a whole-space. these examples are included as supplementary examples in https://github.com/simpegresearch/ [cit] emcyl."
"in figure 4, we show the absolute value of the radial electric field sampled at fives stations; each of the different line colors is associated with a different location, and offsets are with respect to the location of the well. the 3d (2015) . there is a difference in amplitude and position of the zero-crossing (the v-shape visible in the blue and orange curves) between the commer solutions and the simpeg / ubc solutions at the shortest two offsets in the early times. at such short offsets from a highly conductive target, details of the simulation such as averaging, interpolation and discretization become significant; this likely accounts for the discrepancies but a detailed code-comparison is beyond the scope of this paper. our aim with this comparison is to provide evidence that our numerical simulation is performing as expected, and we deem that our overall agreement with commers and ubcs results is confirmation of this."
154 time steps were taken and 10 different step-lengths were used (requiring 10 different matrix factorizations). this simulation took 57 minutes to run on a single intel xeon x5660 processor (2.80ghz).
"a receiver measuring the z-component of the magnetic flux density is positioned 500 m below the transmitter loop, along the axis of the well. we will examine both time-domain and frequency-domain responses."
"the advantage of cylindrical meshes is that they capture rotational fields and poloidal fluxes characteristic of dipolar em responses. when the source and physical property model are axisymmetric, cylindrically symmetric meshes, which reduce the dimensionality of the problem, may be employed. a common example of this is when a dipole source is positioned along the axis of a casing in a 1d layered-earth. under such conditions, simulations which use a fine discretization of the casing can still be solved with modest computational resources. however, for sources offset from the well or for more complex geologic backgrounds, the problem is fully 3d and accurate modelling must allow for azimuthal variations in the fields and fluxes. additionally, for applications such as using electromagnetics for diagnosing the integrity of the casing, one may wish to model partial corrosion through the casing, making the steel-cased well an inherently 3d target. to allow such scenarios to be considered, we discretize our computational domain in cylindrical coordinates and include an azimuthal discretization."
"where g is the nodal gradient operator, and q is the source term, defined on nodes. note that the nodal gradient takes the discrete derivative of nodal variables, and thus the output is on edges. the h-j formulation leads naturally to a cell centered discretization of the electric potential"
"kaufman discusses the behavior of the electric field by dividing the response into three zones: a near zone, an intermediate zone and a far zone [cit] . in the near zone, the electric field has both radial and vertical components, negative charges are present on the inside of the casing, and positive charges are present on the outside of the casing. the near zone is quite localized and typically, its vertical extent is no more than ∼ 10 borehole radii away from the electrode. to examine these features in our numerical simulation, we have plotted in figure 5 : (a) the total charge, (b) secondary charges, (c) electric field, and (d) current density in a portion of the model near the source. the behaviours expected by kaufman are consistent with our numerical results."
"the pipe to the background needs to be continuous both in the radial and vertical directions at the end of the pipe as does the tangential component of the fields ( e, h). the interplay of these two constraints at the end of the pipe results in more complexity in the resultant fields and fluxes. within the span of a few centimeters we transition from static enhancement at the top of the pipe to a static shielding further down. an error as small as a few centimeters in the position of the magnetometer causes a reversal in behavior; in figure 10, we have plotted the fsr for a magnetometer positioned 3cm beneath the plane of the source, and the static-shielding behavior observed for the semi-infinite pipe is much more aligned with that observed in figure 3a [cit] ."
"however, investigation into the literature revealed that the increase in charge density at the ends of a cylinder is a real physical effect, but an exact theoretical solution does still not appear to exist [cit] ) (see figure 4, in particular)."
"our numerical examples emulated experiments that have previously been published; this allowed us not only to verify statements made in those papers, but also to build upon them by further investigating the em phenomena."
"where d is the face divergence operator, v is a diagonal matrix of the cell volumes, q is the source term, which is defined at cell centers as is φ. here, the face divergence takes the discrete derivative from faces to cell centers, thus its transpose takes a variable from cell centers to faces. for a tutorial on the finite volume discretization of the dc equations, see [cit] ."
"to represent a set of partial differential equations on the mesh, we use a staggered-grid approach [cit] and discretize fields on edges, fluxes on faces, and physical properties at cell centers, as shown in figure 1 . scalar potentials can be discretized at cell centers or nodes. traditionally, a cartesian coordinate system is considered and rectangular cells used for the mesh. as the geometry of steel cased well is cylindrical, we will instead adopt a cylindrical coordinate system. we consider both cylindrically symmetric meshes ( figure 1b ) and fully 3d cylindrical meshes, which include a discretization in the azimuthal direction ( figure 1c )."
"in electromagnetics, it is often the product of permeability and conductivity that we consider to be the main controlling factor on the em responses. to examine the contribution of each to the measured responses, we will examine two scenarios. in the first, the well has a conductivity of 10 8 s/m and a relative permeability of 1, and in the second, the well has a conductivity of 10 6 s/m and a relative permeability of 100; thus the product of conductivity and permeability is equivalent for both wells."
"the radially directed fields from the casing, and the length of the intermediate zone, have practical implications in the context of well-logging because they delineate the region in which measurements can be made to acquire information about the formation resistivity outside the well. within the intermediate zone, fields behave like those due to a transmission line [cit], and multiple authors have adopted modelling strategies that approximate the well and surrounding medium as a transmission line [cit] . we will extend this analysis in the next example and discuss how the length of the well impacts the behavior of the charges, fields, and fluxes."
"the authors thank michael commer and christoph schwarzbach for providing the simulation results shown in figure 4 and for permission to distribute them. we also thank thibaut astic and dikun yang for their suggestions and input on the early draft of this paper. finally, we are grateful to rowan cockett, seogi kang and the rest of the simpeg community for their discussion and efforts on the development of the simpeg."
"although our numerical results were in accordance with those in the published literature, we found some results that are not generally talked about or possibly known. for instance, in the dc problem, when a current electrode is attached to the well-head, there is an unexpected increase in charge density very close to the end of the well. for a conductive and permeable casing, excited by a circular current source, there is a complicated magnetic field that occurs in the top few centimeters of the pipe. both of these phenomena might be due to the complexities of fields that result when a change in geometry causes a discontinuity in the definition of the normal component of the surface."
"in this paper, we introduce an approach and associated open-source software implementation for simulating maxwells equations over conductive, permeable models. the simulation domain is discretized in cylindrical coordinates and includes an azimuthal discretization so that non-axisymmetric survey geometries may be considered."
"where i is the magnitude of the source current density, r s + and r s − are the locations of the current electrodes, and φ is the scalar electric potential."
"some similarities to what we observed in figure 6b, where we saw a build up of charge near the end of the pipe in the dc scenario. at the end of the pipe, we encounter the situation where the normal component of the flux ( j, b) from at -4.5m and the pipe extends from 0m to -9m, (b) a \"semi-infinite\" pipe, where the source is located at 0m and the pipe extends to -9m. in (c), we zoom in to the top 5cm of the \"semi-infinite\" pipe, and (d) shows the 5cm at the top-end of the \"infinite\" pipe."
"where e is the electric field, b is the magnetic flux density, h is the magnetic field, j is the current density and s e is the source current density. maxwell's equations can also be formulated in the frequency domain, using the e iωt fourier"
we consider a 2km long well with an outer diameter of 10cm and thickness of 1cm in a whole-space which has a resistivity of 10 4 ωm. a loop with radius 100m is coaxial with the well and positioned at the top-end of the well.
"where t is the thickness of the casing, r is the radius of the casing and l is the length-scale of the pipe segment contributing to the signal. for vertical magnetic fields, the permeance is"
"an important take-away from this example is that the contributions of conductivity and permeability to the observed em signals are not simply governed by their product. the geometry of the source fields plays an important role in how each contributes. thus to accurately model conductive, permeable pipes, over a range of frequencies or times, a numerical code must allow both variable conductivity and variable permeability to be considered."
"one main dimension over which these approaches can be classified is their scope. on a first level we can distinguish the closed domain approaches, whose scope is limited to one (or a set of) a-priori selected domain(s) at a time. as we have seen, ontology-based qa systems, which give meaning to the queries expressed by a user with respect to the domain of the underlying ontology, although portable, their scope is limited to the amount of knowledge encoded in one ontology (they are brittle). as such, they are closer to nlidb, focused on the exploitations of unambiguous structured data in closeddomain scenarios to retrieve precise answers to questions, than to qa over a document collection or free text. while these approaches have proved to work well when a pre-defined domain ontology is used to provide an homogenous encoding of the data, none of them can handle complex questions by combining domain specific information typically expressed in different heterogeneous sources."
"here we present a short survey of related work on qa targeted to different types of sources: structured databases, unstructured free text and precompiled semantic kbs."
"mash-ups [cit] are able to aggregate data coming from heterogeneous repositories and semantic search engines, such as sindice, however these systems do not differentiate among different interpretations of the query terms, and disambiguation has to be done manually by the user."
"− there is a gap between users and the sw: it is difficult for end-users to understand the complexity of the logic-based sw. solutions that can allow the typical web user to profit from the expressive power of sw data-models, while hiding the complexity behind them, are of crucial importance. − the processes of searching and querying content that is massive in scale and highly heterogeneous have become increasingly challenging: current approaches to querying semantic data have difficulties to scale their models successfully to cope with the increasing amount of distributed semantic data available online. hence, there is a need for user-friendly interfaces that can scale up to the web of data, to support end users in querying this heterogeneous information space."
"however, all these approaches still need an intensive configuration procedure. to reduce the formal complexity of creating underlying grammars for different domains, [cit], and most recently c-phrase [cit] ) present a state-of-the-art authoring system for nlidb. the author builds the semantic grammar through a series of naming, tailoring and defining operations within a web-based gui, as such the nli can be configured by non-specialized, web based technical teams. in that system queries are represented as expressions in an extended version of codd's tuple calculus, which may be directly mapped to sql queries or first-order logic expressions. higher-order predicates are also used to support ranking and superlatives."
"as such, the next key step towards the realization of qa on the sw is to move beyond domain specific semantic qa to robust open domain semantic qa over structured and distributed semantic data. in this direction the poweraqua system provides a single nl access approach for all the diverse online resources, stored in multiple collections, opening the possibility of searching and combining answers from all the resources together. nonetheless, as seen in (lopez, nikolov at al., 2009), it is often the case that queries can only be solved by composing information derived from multiple and autonomous information sources, hence, portability alone is not enough and openness is required. qa systems able to draw precise, focused answers by locating and integrating information, which can be distributed across heterogeneous and distributed semantic sources, are required to go beyond the state of the art in interfaces to query the sw."
"the interface that has the least restrictive and most natural query language, nlp-reduce [cit], allows almost any nl input (from ungrammatical inputs, like keywords and sentence fragments, to full english sentences). it processes nl queries as bags of words, employing only two basic nlp techniques: stemming and synonym expansion. essentially, it attempts to match the parsed question words to the synonym-enhanced triples stored in the lexicon (the lexicon is generated from a kb and expanded with wordnet synonyms), and generates sparql statements for those matches. it retrieves all those triples for which at least one of the question words occur as an object property or literal, favouring triples which cover most words and with best matches, and joins the resultant triples to cover the query."
"qacid ) relies on the ontology, a collection of user queries, and an entailment engine that associates new queries to a cluster of existing queries. each query is considered as a bag of words, the mapping between words in nl queries to instances in a kb is done through string distance metrics [cit] and an ontological lexicon. prior to launching the corresponding sparql query for the cluster, the sparql generator replaces the ontology concepts with the instances mapped for the original nl query. this system is at the domainspecific end of the spectrum because the performance depends on the variety of questions collected in the domain, the process is domain-dependent, costly and can only be applied to domains with limited coverage."
"moving into the systems that do not necessitate any customization effort or previous pre-processing, [cit] presented four different ontology-independent query interfaces with the purpose of studying the usability of nli for casual end-users. these four systems lie at different positions of what they call the formality continuum, where the freedom of a full nl and the structuredness of a formal query language are at opposite ends of the continuum. the first two interfaces, nlpreduce and querix allow users to pose questions in full or slightly controlled english. the third interface ginseng / gino offers query formulation in a controlled language akin to english. therefore, the first three interfaces lie on the nl end of the formality continuum towards its middle. as such, they analyze a user query, match it to the content of a kb, and translate these matches into statements of a formal query language (i.e., sparql) in order to execute it. the last interface, semantic crystal, belongs to the formal approaches, as it exhibits a graphical query language. the guided and controlled entry overcomes the habitability problem of nl systems (providing a trade-off between structuredness and freedom) and ensuring all queries make sense in the context of the loaded kb. however, as stated in this usability study \"users favor query languages that impose some structure but do not overly restrict them\", thus, from the four systems, querix was the interface preferred by the users, which query language (full english) was perceived as a natural, not formal, guiding structure."
"we seek a comprehensive perspective on this novel area by analyzing the key dimensions in the formulations of the qa problem in section 2. we classify a qa system, or any approach to query the sw content, according to four dimensions based on the type of questions (input), the sources (unstructured data such as documents, or structured data in a semantic or non-semantic space), the scope (domain-specific, open-domain), and the traditional intrinsic problems derived from the search environment and scope of the system. to start with, we introduce in section 3 the general background and history of the qa research field, from the influential works in the early days of research on architectures for natural language interfaces to databases (nlidb) in the 70s (section 3.1), through the approaches to open domain qa over text (section 3.2), to the latest proprietary (commercial) semantic qa systems, based on data that is by and large manually coded and homogeneous (section 3.3). then, in section 4 we discuss the state of the art in ontology-based qa systems (section 4.1), in particular analyzing their drawbacks (restricted domain) when considering the sw in the large (section 4.2). we then review the latest trends in open domain qa interfaces for the sw (section 4.3) and look at the evaluations that have been conducted to test them (section 4.4). we finish this section with a discussion on the competences of these systems in the qa scenario (section 4.5), highlighting the open issues (section 4.6). in section 5, we focus on approaches developed in the last decade, that have attempted to support end users in querying the sw data in the large, from early global-view information systems (section 5.1) and restricted domain semantic search (section 5.2), to the latest works on open domain large scale semantic search and linked data [cit] ) interfaces (section 5.3). in section 6, we argue that this new ontology-based search paradigm based on natural language qa, is a promising direction towards the realization of user-friendly interfaces for all the analyzed dimensions, as it allows users to express arbitrarily complex information needs in an intuitive fashion. we conclude in section 7 with an outlook for this research area, in particular, our view on the potential directions ahead to realize its ultimate goal: to retrieve and combine answers from multiple, heterogeneous and automatically discovered semantic sources."
"a wide-ranging example is tap [cit], one of the first keyword-based semantic search systems, which presented a view of the search space where documents and concepts are seen as nodes in a semantic network. in tap the first step is to map the search term to one or more nodes of the sw. a term is searched by using its rdfs:label, or one of the other properties indexed by the search interface. in ambiguous cases it chooses a search term based on the popularity of the term (frequency of occurrence in a text corpus), the user profile, the search context, or by letting the user pick the right denotation. the nodes that express the selected denotation of the search term provide a starting point to collect and cluster all triples in their vicinity (the intuition being that proximity in the graph reflects mutual relevance between nodes)."
"the development of e-commerce brings the boom of b2c electronic commerce company. accordingly, on the one hand, the competitions between b2c electronic commerce companies become more intensive; on the other hand, customers have more choices when they are purchasing from b2c electronic commerce companies. hence, the evaluation of customer value in b2c electronic commerce companies becomes a necessary process for b2c electronic commerce companies to maintain the valuable customers. a proper and effective evaluation requires the decision maker to analyze a lot of data and consider many factors. however, the evaluation of customer value in b2c electronic commerce companies is a multiple-criteria decision making (mcdm) problem with many quantitative and qualitative attributes. because human decision-making process usually contains fuzziness and vagueness, the fuzzy analytical hierarchy process (fahp) approach is adopted to solve the problem. and it is an effective and practical approach. the aim of this paper is to propose fahp to evaluate customer value of b2c electronic commerce companies. in addition, a case study is presented to make our approach more understandable."
"in order to make our approach more understandable, we assume that there are three representative b2c companies. company 1 has relative advantages in customer information value. company 2 has relative advantages in customer loyalty value. company 3 has relative advantages in customer contribution value."
"the e-librarian [cit] understands the sense of the user query to retrieve multimedia resources from a kb. first, the nl query is preprocessed into its linguistic classes, in the form of triples, and translated into an unambiguous logical form, by mapping the query to an ontology to solve ambiguities. if a query is composed of several linguistic clauses, each one is translated separately and the logical concatenation depends on the conjunction words used in the question. the system relies on simple, string-based comparison methods (e.g., edit distance metrics) and a domain dictionary to look up lexically related words (synonyms) because generalpurpose dictionaries like wordnet are often not appropriate for specific domains. regarding portability, the creation of this dictionary is costly, as it has to be created for each domain, but the strong advantage of this is that it provides very high performance, which is difficult to obtain with general-purpose dictionaries (from 229 user queries, 97% were correctly answered in the evaluation). the e-librarian does not return the answer to the user's question, but it retrieves the most pertinent document(s) in which the user finds the answer to her question."
"b. calculation steps of fahp first, fahp decompose a complex decision into a hierarchy with an overall goal, criteria, sub-criteria and alternatives. fahp calculation process can be divided into four steps after the establishment of structural hierarchy."
"ontological semantic systems can exploit the power of ontologies as a model of knowledge to give precise, focused answers, where multiple pieces of information (that may come from different sources) can be inferred and combined together. in contrast, qa systems over free text cannot do so, as they retrieve pre-written paragraphs of text or answer strings (typically nps or named entities) extracted verbatim from relevant text [cit] ."
"furthermore, besides scaling up to the sw in its entirety to reach the full potential of the sw, we still have to bridge the gap between the semantic data and unstructured textual information available on the web. we believe, that as the number of annotated sites increases, the answers to a question extracted in the form of lists of entities from the sw, can be used as a valuable resource for discovering web content that is related to the answers given as ontological entities. ultimately, complementing the structured answers from the sw with web pages will enhance the expressivity and performance of traditional search engines with semantic information."
"finally, google itself is also evolving into a nl search engine, providing precise answers to some specific factual queries, together with the web pages from which the answers have been obtained. however, it does not yet distinguish between queries such as \"where barack obama was born\" or \"when barack obama was born\" [cit] ."
"initially, the overall goal of the decision, evaluating the b2c companies which one has the largest customer value, is presented in the top level of the hierarchy. the second level consists of three major criteria that are identified to achieve the overall goal. specifically, the three major criteria are customer information value, customer contribution value and customer loyalty value. the third level contains sub-criteria of three major criteria in second level. these sub-criteria play a role of recommendation for committee to carry out pair-wise comparison of three major criteria. the coding for the criteria and sub-criteria are given in table iii . the fourth level of the hierarchy represents the alternative b2c companies."
"in the previous sections, we have seen that qa systems have proven to be ontology independent or easily adaptable to new domains, while keeping their efficiency and retrieval performance even when shallow nlp techniques are used. by opening up to the sw scenario, these systems can reach their full potential and enhance or complement traditional forms of qa. in this section we broaden our scope and look at user-friendly semantic search systems and linked data querying interfaces, in search for models, beyond nl qa systems, that can in principle scale enough to open up, and even integrate, heterogeneous data sources on the web of data."
"…, in all synthetic comparison matrices. then the weight vector of each matrix can be attained by formulae (10) . after normalization, the relative weights vector of each matrix is also shown in table iv and table 5 . after all relative weights vectors have been computed, we use formulae (11)"
"w b is the global weight vector of n elements in the third level. hence, under the above procedures, the global weight vector of the final alternatives will be obtained by calculating from top level to lowest level."
"5. calculate global weights of final alternatives after participants calculate the priorities and check the consistency of judgments for each matrix in each level, the next step is to calculate the global weight vector of the final alternatives. the calculation of global weight was conducted by synthesizing the priorities of all matrices from the top level to the lowest level in the hierarchy."
"the next generation of nlidbs used an intermediate representation language, which expressed the meaning of the user's question in terms of high-level concepts, independently of the database's structure [cit] . thus, separating the (domain-independent) linguistic process from the (domain-dependent) mapping process into the database, to improve the portability of the front end [cit] ."
"while the latest open linked data and semantic search applications shown in 5.3 present a much wider scope, scaling to the large amounts of available semantic data, they perform a shallow exploitation of this information: 1) they do not perform semantic disambiguation, but need users to select among possible query interpretations, 2) they do not generally provide knowledge fusion and ranking mechanisms to improve the accuracy of the information retrieved, and 3) they do not discover mappings between data sources on the fly, but need to pre-compute them beforehand."
"analytic hierarchy process (ahp) has wide application in multi-criteria decisions but it has its drawbacks. [cit] noted that ahp applied upon specific (not fuzzy) decision-making and that ahp cannot include uncertainty factors of people toward objects. in real world, fuzzy phenomenon problems and vague human thoughts limit the application of ahp. professor l.a. [cit] introduced fuzzy set theory, trying to solve fuzzy and vague problem. fuzzy analytic hierarchy process that combines fuzzy set theory with ahp provides an approach to solve the fuzzy phenomenon problems in real world.fahp can also be used in evaluating various kinds of mcdm problems in both academic researches and practices. the fahp-based decision-making method could provide the managers of b2c companies with a valuable reference for evaluating the customer value."
". commercial systems include powerset, which tries to match the meaning of a query with the meaning of a sentence in wikipedia. powerset not only works on the query side of the search (converting the nl queries into database understandable queries, and then highlighting the relevant passage of the document), but it also reads every word of every (wikipedia) page to extract the semantic meaning. it does so by compiling factzs -similar to triples, from pages across wikipedia, together with the wikipedia page locations and sentences that support each factz and http://www.wolframalpha.com/index.html, and http://www.trueknowledge.com/ 4 www.opencyc.org, http://www.freebase.com using freebase and its semantic resources to annotate them. the wolfram alpha knowledge inference engine builds a broad trusted kb about the world by ingesting massive amounts of information (approx. 10tbs, still a tiny fraction of the web), while true knowledge relies on users to add and curate information."
"− natural language interfaces to structured data on databases (nlidb traced back to the late sixties [cit] ). − qa over semi-structured data (e.g., health records, yellow pages, wikipedia infoboxes). − open qa over free text, fostered by the opendomain qa track introduced by trec (http://trec.nist.gov) [cit] (trec-8). − qa over structured semantic data, where the semantics contained in ontologies provide the context needed to solve ambiguities, interpret and answer the user query. another distinction between qa systems is whether they are domain-specific (closed domain) or domainindependent (open domain). ontology-based qa emerged as a combination of ideas of two different research areas -it enhances the scope of closed nlidb over structured data, by being agnostic to the domain of the ontology that it exploits; and also presents complementary affordances to open qa over free text (trec), the advantage being that it can help with answering questions requiring situationspecific answers, where multiple pieces of information (from one or several sources) need to be assembled to infer the answers at run time. nonetheless, most ontology-based qa systems are akin to nlidb in the sense that they are able to extract precise answers from structured data in a specific domain scenario, instead of retrieving relevant paragraphs of text in an open scenario. latest proprietary qa systems over structured data, such as trueknowledge and powerset (detailed in section 3.3), are open domain but restricted to their own proprietary sources."
"since the steady growth of the sw and the emergence of large-scale semantics the necessity of nli to ontology-based repositories has become more acute, re-igniting interest in nl front ends. this trend has also been supported by usability studies [cit], which show that casual users, typically overwhelmed by the formal logic of the sw, prefer to use a nl interface to query an ontology. hence, in the past few years there has been much interest in ontology based qa systems, where the power of ontologies as a model of knowledge is directly exploited for the query analysis and translation, thus providing a new twist on the old issues of nlidb, by focusing on portability and performance, and replacing the costly domain specific nlp techniques with shallow but domain-independent ones. a wide range of off-the-shelf components, including triple stores (e.g., sesame 5 ) or text retrieval engines (e.g., lucene 6 ), domain-independent linguistic resources, such as wordnet and framenet 7 ontology-based qa systems vary on two main aspects: (1) the degree of domain customization they require, which correlates with their retrieval perfor-, and nlp parsers, such as stanford parser [cit], support the evolution of these new nli. mance, and (2) the subset of nl they are able to understand (full grammar-based nl, controlled or guided nl, pattern based), in order to reduce both complexity and the habitability problem, pointed out as the main issue that hampers the successful use of nli [cit] ."
"in this paper, we present a survey of ontologybased qa systems and other related work. we look at the promises of this novel research area from two perspectives. first, its contributions to the area of qa systems in general; and second, its potential to go beyond the current state of the art in sw interfaces for end-users, thus, helping to bridge the gap between the user and the sw."
"orakel ) is a nl interface that translates factual wh-queries into f-logic or sparql and evaluates them with respect to a given kb. the main feature is that it makes use of a compositional semantic construction approach thus being able to handle questions involving quantification, conjunction and negation. in order to translate factual wh-queries it uses an underlying syntactic theory built on a variant of a lexicalized tree adjoining grammar (ltag), extended to include ontological information. the parser makes use of two different lexicons: the general lexicon and the domain lexicon. the general or domain independent lexicon includes closed-class words such as determiners, i.e., a, the, every, etc., as well as question pronouns, i.e., who, which, etc. the domain lexicon, in which natural expressions, verbs, adjectives and relational nouns, are mapped to corresponding relations specified in the domain ontology, varies from application to application and, for each application, this lexicon has to be partially generated by a domain expert. the semantic representation of the words in the domain independent lexicon makes reference to domain independent categories, as given for example by a foundational ontology such as dolce. this assumes that the domain ontology is somehow aligned to the foundational categories provided by the foundational ontology. therefore, the domain expert is only involved in the creation of the domain specific lexicon, which is actually the most important lexicon as it is the one containing the mapping of linguistic expressions to domain-specific predicates. the domain expert has to instantiate subcategorization frames, which represent linguistic structures (e.g., verbs with their arguments), and maps these to domain-specific relations in the ontology. wordnet is used with the purpose to suggest synonyms (in the most frequent sense of the word) for the verb or noun currently edited. the approach is independent of the target language, which only requires a declarative description in prolog of the transformation from the logical form to the target language."
", aimed at stimulating the creation of novel demonstrators that have the capability to scale and deal with heterogeneous data crawled from the web. examples include searchwebdb [cit], which offers a keyword-based interface to integrated data sources available in the btc datasets. however, as keywords express the user needs imprecisely, the user needs to be asked to select among all possible interpretations. in this system the mappings between any pairs of data sources at the schema or data levels are computed a priori and stored in several indexes: the keyword index, the structure index and the mapping index. the disadvantage being that, in a highly dynamic environment, static mappings and complex structural indexes are difficult to maintain, and the data quickly becomes outdated."
"in this scenario, qa over semantic data distributed across multiple sources has been introduced as a new paradigm, which integrates ideas of traditional qa research into scalable sw tools. in our view, there is great potential for open qa approaches in the sw. as shown in table 7 .1 semantic open qa has tackled more problems than other methods for many of the analyzed criteria. in an attempt to overcome the limitations of search approaches, that restrict their scope to homogenous or domain-specific content, or perform a shallow exploitation of it, current qa systems have developed syntactic, semantic and contextual information processing mechanisms that allow a deep exploitation of the semantic information space."
"through the formulae, we could obtain a synthetic matrix of relative rankings for all elements in level k. then, the synthetic weight of each element in level k is calculated by the formulae:"
"thus, it is necessary to provide guidelines for b2c companies through a study on the components of customer value [cit] . so the aim of the paper is studying and evaluating customer value in b2c companies, and attempts to find out the relationship among the factors of the customer value. additionally, lingyun fang [cit] set up an evaluation system of customer value, in which there contains five main factors such as customer information value, customer credit value, customer purchase value, customer public praise value and customer loyalty value. all of these and related researches show the fact that the issue of customer value of e-commerce is a complex topic and involves multiple criteria. based on the above literature, this paper uses three main factors (customer information value, customer contribution value and customer loyalty value)."
"qacid has been tested with an owl ontology in the cinema domain, where 50 users were asked to generate 500 queries in total for the given ontologies. from these queries, 348 queries were automatically annotated by an entity annotator and queries with the same ontological concepts were grouped together, generating 54 clusters that were manually associated to sparql queries. the results reported in an onfield evaluation, where 10 users were asked to formulate spontaneous queries about the cinema domain (a total of 100 queries), show an 80% of precision."
step 2: calculating synthetic weight to obtain synthetic matrix. assume that there are k n elements in level k which are sub-elements of an element in the immediately higher level k-1 and there are t experts take part in the evaluation on criteria.
refers to the fuzzy number which represents the comparison weight of i-th criterion to j-th criterion in level k evaluated by t-th expert. the fuzzy number of the comparison weight evaluated by all experts for i-th criterion to j-th criterion in level k is calculated by the formulae:
"ri is a known random consistency index obtained from a large number of simulation runs and varies depending upon the order of matrix. table ii shows the value of the random consistency index (ri) for matrices of order 1 to 10 obtained by approximating random indices using a sample size of 500 [cit] . the acceptable cr range varies according to the size of matrix, i.e., 0.05 for a 3 by 3 matrix, 0.08 for a 4 by 4 matrix and 0.1 for all larger matrices [cit] . if the value of cr is acceptable, it implies that the evaluation within the matrix is acceptable or indicates a good level of consistency in the comparative judgments represented in that matrix. in contrast, if the value of cr is not acceptable, it implies that the evaluation within the matrix is not acceptable or indicates inconsistency of judgments within that matrix. then, the evaluation process should be reviewed, reconsidered and improved until the value of cr is acceptable."
"− ontology independence: later nlidb systems [cit] ) use intermediate representations to have a portable front end with general purpose grammars, while the back end is dependent on a particular database. as a result, long configuration times are normally required to port the system to a new domain. ontologybased qa systems have successfully solved the portability problem, as the knowledge encoded in the ontology, together with (often shallow) domain-independent syntactic parsing, are the primary sources for understanding the user query, without the need to encode specific domain-dependent rules. hence, these systems are practically ontology independent, less costly to produce, and require little effort to bring in new sources (aqualog, panto, querix, questio, freya). optionally, on these systems manual configuration or automatic learning mechanisms based on user feedback can optimize performance. − able to handle unknown vocabulary in the user query: nlidb systems, such as precise [cit], require all the tokens in a query to be distinct and questions with unknown words are not semantically tractable. in ontology-based qa if a query term is lexically dissimilar from the vocabulary used by the ontology, and it does not appear in any manually or automatically created lexicon, studying the ontology \"neighborhood\" of the other terms in the query may lead to the value of the term or relation we are looking for. in many cases this would be all the information needed to interpret a query. − deal with ambiguities: when ontologies are directly used to give meaning to the queries expressed by the user and retrieve answers, the main advantage is the possibility to link words to obtain their meaning based on the ontological taxonomy and inherit relationships, and thus, to deal with ambiguities more efficiently. summing up, the main benefits of ontology-based qa systems are that they make use of the semantic information to interpret and provide precise answers to questions posed in nl and are able to cope with ambiguities in a way that makes the system highly portable."
"an overview of related work shows a wide range of approaches that have attempted to support end users in querying and exploring the publicly available sw information. it is not our intention to exhaustively cover all existing approaches, but to look at the state of the art and applications to figure out the capabilities of the different approaches, considering each of the querying dimensions presented in section 2 (sources, scope, search environment and input), to identify promising directions towards overcoming their limitations and filling the research gaps."
"apart from the benefits that can be obtained as more semantic data is published on the web, the emergence and continued growth of a large scale sw poses some challenges and drawbacks:"
the committee suggests us to construct a four levels hierarchy (see fig.3 ) to help in solving the evaluation of customer value in b2c companies with fahp.
"1 http://clef.isti.cnr.it tabases or resources (mollá [cit] ) . however, their success has been typically overshadowed by both the brittleness and habitability problems [cit], defined as the mismatch between the user expectations and the capabilities of the system with respect to its nl understanding and what it knows about (users do not know what it is possible to ask). as stated in iterative and exploratory search modes are important to the usability of all search systems, to support the user in understanding what is the knowledge of the system and what subset of nl is possible to ask about. systems also should be able to provide justifications for an answer in an intuitive way (nl generation), suggest the presence of unrequested but related information, and actively help the user by recommending searches or proposing alternate paths of exploration. for example, view based search and forms can help the user to explore the search space better than keyword-based or nl querying systems, but they become tedious to use in large spaces and impossible in heterogeneous ones."
"other approaches are based on statistical or semantic similarities. for example, faq finder [cit] ) is a nl qa system that uses files of faqs as its kb; it uses two metrics to match questions to answers: statistical similarity and semantic similarity. for shorter answers over limited structured data, nlp-based systems have generally performed better than statistical based ones, which need a lot of domain specific training and long documents with large quantities of data containing enough words for statistical comparisons to be considered meaningful. semantic similarity scores rely on finding connections through wordnet between the user's question and the answer. the main problem here is the inability to cope with words that are not explicitly found in the kb. gurevych's [cit] ) approach tries to identify semantically equivalent questions, which are paraphrases of user queries, already answered in social q&a sites, such as yahoo!answers. looks up the user's question in its database and returns a list of matching questions that it knows how to answer, the user selects the most appropriate entry in the list, and he is taken to the web pages where the answer can be found. askjeeves relies on human editors to match question templates with authoritative sites."
"− to bridge the gap between the end-user and the real sw by providing a nl qa interface that can scale up to the web of data. − to take advantage of the structured information distributed on the sw to retrieve aggregate answers to factual queries that extend beyond the coverage of single datasets and are built across multiple ontological statements obtained from different sources. consequently, smoothing the habitability and brittleness problems intrinsic to closed domain kb systems. the ultimate goal for a nl qa system in the sw is to answers queries by locating and combining information, which can be massively distributed across heterogeneous semantic resources, without imposing any pre-selection or pre-construction of semantic knowledge, but rather locating and exploring the increasing number of multiple, heterogeneous sources currently available on the web."
"in this section we look at ontology-based semantic qa systems (also referred in this paper as semantic qa systems), which take queries expressed in nl and a given ontology as input, and return answers drawn from one or more kbs that subscribe to the ontology. therefore, they do not require the user to learn the vocabulary or structure of the ontology to be queried."
"faceted views have been widely adopted for many rdf datasets, including large linked data datasets such as dbpedia, by using the neofonie es grows. the ranking of predicates to identify important facets is obtained from text and entity frequency, while semantics associated with the links is not explored."
"qa systems over the web have the same three main components as qa systems designed to extract answers to factual questions by consulting a repository of documents (trec): (1) a query formulation mechanism that translates the nl queries into the required ir queries, (2) a search engine over the web, instead of an ir engine searching the documents, and (3) the answer extraction module that extracts answers from the retrieved documents. a technique commonly shared in web and trec-systems, is to use wordnet or ne tagging to classify the type of the answer."
"as the ahp approach is a subjective methodology [cit], information and the relative weights of elements could be obtained from decision makers or experts by using direct questioning or a questionnaire."
"questio was tested on a locally produced ontology, generated from annotated postings in the gate mailing list, with 22 real user queries that could be answered in the ontology and a travel guides ontology with an unreported number of queries, to demonstrate portability. the initialization time of questio with the travel guides ontology (containing 3194 resources in total) was reported to be 10 times longer, which raises some concerns in terms of scalability. a query is considered correctly answered if the appropriate serql query is generated (71.8% success)."
"− balancing relatively easy design and accuracy: as seen in section 3.2 the current state of the art open systems to query documents on the web require sophisticated syntactic, semantic and contextual processing to construct an answer, including ne recognition [cit] . these open qa systems classify queries using hierarchies of question types based on the types of answers sought (e.g., person, location, date, etc.) and filter small text fragments that contain strings with the same type as the expected answers [cit] . in ontology-based qa there is no need to build complex hierarchies, to manually map specific answer types to wordnet conceptual hierarchies or to build heuristics to recognize named entities, as the semantic information needed to determine the type of an answer is in the publicly available ontology (ies). as argued in (mollá [cit] ) a major difference between open-domain qa and ontology-based qa is the existence of domain-dependent information that can be used to improve the accuracy of the system. − exploiting relationships for query translation: ne recognition and ie are powerful tools for free-text qa (section 3.2.1), although these methods scale well discovering relationships between entities is a crucial problem [cit] . ie methods do not often capture enough semantics, answers hidden in a form not recognized but the patterns expected by the system could be easily disregarded, and one cannot always rely on wordnet coverage to determine the answer type or the type of the object of the verb in the question [cit] . on the contrary, qa systems over semantic data can benefit from exploiting the explicit ontological relationships and the semantics of the ontology schema (e.g., type, subclassof, domain and range), to understand and disambiguate a query. wordnet is only used for query expansion, to bridge the gap between the vocabulary of the user and the ontology terminology through lexically related words (such as synonyms). − handling queries in which the answer type is unknown: what queries, in which the type of the expected answer is unknown, are harder than other types of queries when querying free text [cit] . however, the ontology simplifies handling what-is queries because the possible answer types are constrained by the types of the possible relations in the ontology. − structured answers are constructed from ontological facts: arbitrary query concepts are mapped to existing ontology entities, answers are then obtained by extracting the list of semantic entities that comply with the facts, or fulfill the ontological triples or sparql queries. the approach to answer extraction in text-based qa requires first identifying entities matching the expected answer in text, e.g., using the wordnet mapping approach. second, the answers within these relevant passages are selected using a set of proximity-based heuristics, whose weights are set by a machine-learning algorithm [cit] . although ir methods scale well, valid answers in documents that do not follow the syntactic patterns expected by the qa system can be easily disregarded."
"for instance, mulder (kwok al., 2001 ) is a qa system for factual questions over the web, which relies on multiple queries sent to the search engine google. to form the right queries for the search engine, the query is classified using wordnet to determine the type of the object of the verb in the question (numerical, nominal, temporal), then a reformulation module converts a question into a set of keyword queries by using different strategies: extracting the most important keywords, quoting partial sentences (detecting noun phrases), conjugating the verb, or performing query expansion with wordnet. in mulder, an answer is extracted from the snippets or summaries returned by google, which is less expensive than extracting answers directly from a web page. then, to reduce the noise or incorrect information typically found on the web and improve accuracy, mulder clusters similar answers together and picks the best answer with a voting procedure. mulder takes advantage of google ranking algorithms base on pagerank, the proximity or frequency of the words, and the wider coverage provided by google: \"with a large collection there is a higher probability of finding target sentences\". an evaluation using the trec-8 questions, based on the web, instead of the trec document collection, showed that mulder's recall is more than a factor of three higher than askjeeves."
"finally, the expressivity of the user query is defined by the input the system is able to understand. as shown in table 7 .1, keyword-based systems lack the expressivity to precisely describe the user's intent, as a result ranking can at best put the query intentions of the majority on top. most approaches look at expressivity at the level of relationships (factoids), however, different systems provide different support for complex queries, from including reasoning services to understand comparisons, quantifications and negations, to the most complex systems (out of the scope of this review) that go beyond factoids and are able to understand anaphora resolution and dialogs [cit] . ontologies are a powerful tool to provide semantics, and in particular, they can be used to move beyond single facts to enable answers built from multiple sources. however, regarding the input, ontologies have limited capability to reason about temporal and spatial queries and do not typically store time dependent information. hence, there is a serious research challenge in determining how to handle temporal data and causality across ontologies. in a search system for the open sw we cannot expect complex reasoning over very expressive ontologies, because this requires detailed knowledge of ontology structure. complex ontology-dependent reasoning is substituted by the ability to deal with and find connections across large amounts of heterogeneous data."
"there is a boom of e-commerce in recent decade. strides in information technology and improvements in networking technology have set the pace of the rapid growth in new application of e-commerce in a variety of settings. [cit] which is issued by cnnic, the number of internet purchasers crowd in china reached, and the number of purchasers accounted for 34% of the number of the whole country's [cit] . the statistic shows that no enterprises can afford to ignore the tremendous potential of e-commerce in creating, processing and distributing the value of business. b2b, b2c, c2b and c2c are the most important business channels that have reshaped the marketplaces. b2c electronic commerce company is a sort of b2c business channel. in china, dangdang.com, joyo.com and 360buy.com are the most famous b2c electronic commerce companies and user penetration of 10.4%, 4.5% and 3.5% respectively. furthermore, female customers, whose age range from 18 to 30 years old, are the main group."
"the main clear advantage of the use of nl query tools is the easy interaction for non-expert users. as the sw is gaining momentum, it provides the basis for qa applications to exploit and reuse the structured knowledge available on the sw. beyond the commonalities between all forms of qa (in particular for the question analysis), in this section, we analyze the competencies of ontology-based qa with respect to the main traditional forms of qa."
"after the establishment of structural hierarchy, participants determine the priorities of all elements at each level in hierarchy. in doing this, participants construct comparison matrices. the structure of hierarchy notes that an element of a higher level takes several subelements, just as the overall goal takes several criteria and as criteria in a higher level follows several sub-criteria. a comparison matrix of elements in a level of the hierarchy with respect to an element of the immediately higher level is constructed to prioritize and convert individual comparative importance into ratio scale measurements. the preferences are quantified by using a nine-point scale. the meaning of each scale measurement is explained in table i ."
"a challenge for domain-independent systems comes from the search environment that can be characterized by large scale, heterogeneity, openness and multilinguality. the search environment influences to what level semantic systems perform a deep exploitation of the semantic data. in order to take full advantage of the inherent characteristics of the semantic information space to extract the most accurate answers for the users, qa systems need to tackle various traditional intrinsic problems derived from the search environment, such as:"
"− heterogeneity and openness: the high ambiguity in the sources means that it is not always possible to have enough context to focus on precision when, because of heterogeneity, there are many alternative translations and interpretations to a query. for example, the main issue for poweraqua is to keep real time performance in a scenario of perpetual change and growth, in particular when both very large heterogeneous sources from the linked data cloud, or thousands of small rdf sources from crawled data from watson are added [cit] . − dealing with scalability as well as knowledge incompleteness: filtering and ranking techniques are required to scale to large amounts of data. there are often a huge number (from hundreds to thousands in many cases) of potential ontological hits with different meanings (domains), across and within the same dataset, that can syntactically map the terms in a user query. it is unfeasible to explore all possible solutions to obtain semantically sound mappings, however, filtering and domain-coverage heuristics to shift focus onto precision require making certain assumptions about quality of sources. if filtering heuristics are too strict, recall is affected in a noisy environment, where sources contain redundant and duplicated terms and incomplete information, either because not all ontological elements are populated at the level of instances or because of a lack of schema information (no domain and range for properties, or type for classes, difficult to parse literals, etc.)."
"the main drawback of these early nlidb systems is that they were built having a particular database in mind, thus they could not be easily modified to be used with different databases and were difficult to port to different application domains. configuration phases were tedious and required a long time, because of domain-specific grammars, hard-wired knowledge or hand-written mapping rules that had to be developed by domain experts."
"automatic disambiguation (point 1) can only be performed if the user query is expressive enough to grasp the conceptualizations and content meanings involved in the query. in other words, the context of the query is used to choose the correct interpretation. if the query is not expressive enough, the only alternative is to call the user to disambiguate, or to rank the different meanings based on the popularity of the answers."
"these performance evaluations share in common the pattern of being ad-hoc, user-driven and using unambiguous, relatively small and good quality semantic data. although they test the feasibility of developing portable nlis with high retrieval performance, these evaluations also highlight that the nlis with better performance usually tend to require a degree of expensive customization or training. as already pointed out in, to bridge the gap between the two extremes, domain independency and performance, the quality of the semantic data have to be very high, to ensure a good lexicalization of the ontology and kbs and a good coverage of the vocabulary. nonetheless, as previously reported in aqualog, and recently evaluated in freya, the inclusion of a learning mechanism offers a good trade-off between user interaction and performance, ensuring an increase in performance over time by closing the lexical gap between users and ontologies, without compromising portability. large ontologies pose additional challenges with respect to usability, as well as performance. the ontologies used in the previous evaluations are relatively small; allowing to carry out all processing operations in memory, thus, scalability is not evaluated."
"thus, in factual qa systems over distributed semantic data the lack of very complex reasoning is substituted by the ability to deal and find connections in large amounts of heterogeneous data and to provide coherent answers within a specific context or task. as a consequence, exploiting the sw is by and large about discovering interesting connections between items. we believe that in those large scale semantic systems, intelligence becomes a side effect of a system's ability to operate with large amounts of data from heterogeneous sources in a meaningful way rather than being primarily defined by their reasoning ability to carry out complex tasks. in any case this is unlikely to provide a major limitation given that, most of the large datasets published in linked data are light-weight."
"− mapping the terminology and information needs of the user into the terminology used by the sources, in such a form that: (1) it can be evaluated using standard query processing and inferencing techniques, (2) it does not affect portability or adaptability of the systems to new domains, and (3) it leads to the correct answer. − disambiguating between all possible interpretations of a user query. independently of the type of query, any non-trivial nl qa system has to deal with ambiguity. furthermore, in an open scenario, ambiguity cannot be solved by means of an internal unambiguous knowledge representation, as in domain-restricted scenarios. in open-domain scenarios, systems face the problem of polysemous words, with different meanings according to different domains. − because answers may come from different sources, and different sources have varying levels of quality and trust, knowledge fusion and ranking measures should be applied to select the better sources, fuse similar answers together, and rank the answers across sources. − with regards to scalability, in general terms, there is a trade-off between the complexity of the querying process and the amount of data systems can use in response to a user demand in a reasonable time. multilinguality issues, the ability to answer a question posed in one language using an answer corpus in another language, fostered by the multilingual question answering track at the cross language evaluation forum (clef) 1 nl interfaces are an often-proposed solution in the literature for casual users [cit], are not reviewed in this survey. this is because in the context of qa in the open sw, challenges such as scalability and heterogeneity need to be tackled first to obtain answers across sources."
"we have shown through this paper that ontologies are a powerful source to provide semantics and background knowledge about a wide range of domains, providing a new important context for qa systems."
"while semantic search technologies have been proven to work well in specific domains still have to confront many challenges to scale up to the web in its entirety. the latest approaches to exploit the massive amount of distributed sw data represent a considerable advance with respect to previous systems, which restrict their scope to a fraction of the publicly available sw content or rely on their own semantic resources. these approaches are ultimately directed by the potential capabilities of the sw to provide accurate responses to nl user queries, but are nl qa approaches fit for the sw?."
"for instance, in lasso [cit] ) a question type hierarchy was constructed from the analysis of the trec-8 training data, and a score of 55.5% for short answers and 64.5% for long answers was achieved. given a question, lasso can find automatically (a) the type of the question (what, why, who, how, where), (b) the type of the answer (person, location, etc.), (c) the focus of the question, defined as the \"main information required by the interrogation\" (useful for \"what\" questions, which usually leave implicit the type of the answer which is sought), (d) the relevant keywords from the question. occasionally, some words of the question do not occur in the answer (for example, the focus \"day of the week\" is very unlikely to appear in the answer). therefore, lasso implements ne recognition heuristics for locating the possible answers."
"new technologies have been developed to manipulate large sets of semantic metadata available online. search engines for the sw collect and index large amounts of semantic data to provide an efficient keyword-based access point and gateway for other applications to access and exploit the growing sw. falcons [cit] allows concept (classes and properties) and object (instance) search. the system recommends ontologies on the basis of a combination of the tf-idf technique and popularity for concept search, or the type of objects the user is likely to be interested in for object search. falcons indexes 7 million of well-formed rdf documents and 4,400 ontologies [cit] . swoogle [cit] indexes over 10,000 ontologies, swoogle claims to adopt a web view on the sw by using a modified version of the pagerank popularity algorithm, and by and large ignoring the semantic particularities of the data that it indexes. later search engines such as sindice [cit] index large amounts of semantic data, over 10 billion pieces of rdf, but it only provides a look-up service that allows applications and users to locate semantic documents. watson [cit] collects the available semantic content from the web, indexing over 8,300 ontologies, and also offers an api to query and discover semantic associations in ontologies at run time, e.g., searching for relationships in specific ontological entities. indeed out of these four ontology search engines, only watson allows the user to exploit the reasoning capabilities of the semantic data, without the need to process these documents locally. the other engines support keyword search but fail to exploit the semantic nature of the content they store and therefore, are still rather limited in their ability to support systems which aim to exploit online ontologies in a dynamic way [cit] ."
"although most of the state-of-the-art of ontologybased qa still presumes that the knowledge needed is encoded in one ontology in a closed domain scenario, we envision ontology-based qa to move towards an open sw scenario, to become complementary to freetext open qa. while the first targets the open, structured sw to give precise answers, the second targets unstructured documents on the web. under such a perspective, a document search space is replaced by a semantic search space composed of a set of ontologies and kbs, providing a new context in which the results from traditional open qa can be applied. although linguistic and ambiguity problems are common in most kinds of nl understanding systems, building a qa system over the sw has the following advantages:"
"other notable exceptions to this limited-domain approach include search applications demonstrated in the semantic web challenge competitions, and more recently the billion triples challenge (btc) 17 the erdf infrastructure (gueret at al., 2009 ) explores the web of data by querying distributed datasets in live sparql endpoints. the potential of the infrastructure was shown through a prototype web application. given a keyword, it retrieves the first result in sindice to launch a set of sparql queries in all sparql end points, by applying an evolutionary anytime query algorithm, based on substitutions of possible candidate variables for these sparql queries. as such, it retrieves all entities related to the original entity (because they have the same type or a shared relationships to the same entity, for example wendy hall and tim berners lee both hold a professorship at the university of southampton)."
"similarly, in questio ) nl queries are translated into formal queries but the system is reliant on the use of gazetteers initialized for the domain ontology. in questio users can enter queries of any length and form. questio works by recognizing concepts inside the query through the gazetteers, without relying on other words in the query. it analyzes potential relations between concept pairs and ranks them according to string similarity measures, the specifity of the property or distance between terms. questio supports conjunction and disjunction."
"the emerging semantic web (sw) [cit] ) offers a wealth of semantic data about a wide range of topics, representing real community agreement. we are quickly reaching the critical mass required to enable a true vision of a large scale, distributed sw with real-world datasets, leading to new research possibilities that can benefit from exploiting and reusing this vast resources, unprecedented in the history of computer science. hence, there is now a renewed interest in the search engine market towards the introduction of semantics in order to improve over current keyword search technologies [cit] ."
"− sparseness: the potential is overshadowed by the sparseness and incompleteness of the sw when compared to the web [cit] . during the search process, it may happen that a) there are no available ontologies that cover the query, or b) there are ontologies that cover the domain of the query but only contain parts of the answer."
"in this section, we present a case to make our approach more understandable. first, we arrange an evaluation committee consisting of e-commerce experts, experienced consumers. the committee advice us to construct the following model to help in solving the evaluation of customer value in b2c company with fahp based approach. the model is a hierarchy with four levels, shown in figure 3 ."
"many approaches exist to translate user queries into formal queries. semantic search, a broader area than semantic qa, faces similar challenges to those tackled by qa systems when dealing with heterogeneous data sources on the sw. here, we look at the solutions proposed in the literature for semantic search and how they address semantic heterogeneity from early information systems to the latest approaches to searching the sw. we further discuss how all qa approaches presented till now and the sw user-friendly querying models presented in this section are compared according to the criteria presented in section 2, and how both research directions can converge into large scale open ontology-based qa for the sw, to solve the bottlenecks and limitations of both."
"matrix. triangular fuzzy numbers are used to indicate the relative weight of each pair of elements in the same level. through pair-wise comparison, the fuzzy judgment matrix a is constructed, where the entry ij a is a triangular fuzzy number."
"with regards to on the fly mappings (point 2), most sw systems analyzed here perform mappings on the fly given a user task, and some of them are able to select the relevant sources on the fly. there are three different mechanisms which are employed: (1) through search engines (mash-ups, semantic search, open ontology-based qa); (2) by accessing various distributed online sparql end-points providing full text search capabilities (semantic search, facets); (3) by indexing multiple online repositories (open ontology-based qa, semantic search). state of the art open ontology-based qa and semantic search systems perform better by indexing multiple online repositories for its own purposes. when a search engine such as watson, which provides enough functionality (api) to query and perform a deep analysis of the sources, is used the performance is just acceptable from a research point of view demo [cit] . more work is needed to achieve real time performance -beyond prototypes, for ontology-based qa to directly catch and query the relevant sources from a search engine that crawls and indexes the semantic sources."
"most current work on qa, which has been rekindled largely by the trec text retrieval conference (sponsored by the american national institute, nist, and the defense advanced research projects agency, darpa) and by the cross-lingual qa track at clef, is somewhat different in nature from querying structured data. these campaigns enable research in qa from the ir perspective, where the task consists in finding the text that contains the answer to the question and extracting the answer. the arda's advanced question answering for intelligence funded the aquaint program, a multi-project effort to improve the performance of qa systems over free large heterogeneous collections of structured and unstructured text or media. given the large, uncontrolled text files and the very weak world knowledge available from wordnet and gazetteers, these systems have performed surprisingly well. for example, the lcc system [cit] ) that uses a deep linguistic analysis and iterative strategy obtained a score of 0.856 by answering correctly 415 questions out of 500 in trec-11 (2002) ."
a new layer of complexity arises when moving from a classic kb system to an open and dynamic search environment. if an application wishes to use data from multiple sources the integration effort is non-trivial.
"consistent with the role played by ontologies in structuring semantic information on the web, recent years have witnessed the rise of ontology-based question answering (qa) as a new paradigm of research, to exploit the expressive power of ontologies and go beyond the relatively impoverished representation of user information needs in keyword-based queries. qa systems have been investigated by several communities [cit] ), e.g., information retrieval (ir), artificial intelligence and database communities. traditionally, qa approaches have largely been focused on retrieving answers from raw text, with the emphasis on using ontologies to mark-up web resources and improve retrieval by using query expansion [cit] . the novelty of this trend of ontology-based qa is to exploit the sw information for making sense of, and answering, user queries."
"masque/sql [cit] ) is a portable nl front end to sql databases. it first translates the nl query into an intermediate logic representation, and then translates the logic query into sql. the semi-automatic configuration procedure uses a built-in domain editor, which helps the user to describe the entity types to which the database refers, using an is-a hierarchy, and then to declare the words expected to appear in the nl questions and to define their meaning in terms of a logic predicate that is linked to a database table/view."
"it is costly to produce the large amounts of domain background knowledge, which are required by the proprietary open domain approaches described in section 3.3. although based on semantics, these systems do not reuse or take fully advantage of the freely available structured information on the sw. this is a key difference as they impose an internal structure on their knowledge and claim ownership of a trusted and curated homogeneous kb, rather than supporting the user in exploring the increasing number of distributed knowledge sources available on the web."
"aqualog allows the user to choose an ontology and then ask nl queries with respect to the universe of discourse covered by the ontology. aqualog is ontology independent because the configuration time required to customize the system for a particular ontology is negligible. the reason for this is that the architecture of the system and the reasoning methods are completely domainindependent, relying on the semantics of the ontology, and the use of generic lexical resources, such as wordnet. in a first step, the linguistic component uses the gate infrastructure and resources [cit] to obtain a set of linguistic annotations associated with the input query. the set of annotations is extended by the use of jape grammars 8 8 jape is a language for creating regular expressions applied to linguistic annotations in a text corpus to identify terms, relations, question indicators (who, what, etc.), features (voice and tense) and to classify the query into a category. knowing the category and gate annotations for the query, the linguistic component creates the linguistic triples or query-triples. then, these query-triples are further processed and interpreted by the relation similarity service, which maps the query-triples to ontologycompliant onto-triples, from which an answer is derived. aqualog identifies ontology mappings for all the terms and relations in the query-triples by means of string based comparison methods and wordnet. in addition, aqualog's interactive relation similarity service uses the ontology taxonomy and relationships to disambiguate between the alternative representations of the user query. when the ambiguity cannot be resolved by domain knowledge the user is asked to choose between the alternative readings. aqualog includes a learning component to automatically obtain domain-dependent knowledge by creating a lexicon, which ensures that the performance of the system improves over time, in response to the particular community jargon (vocabulary) used by end users. aqualog uses generalization rules to learn novel associations between the nl relations used by the users and the ontology structure. once the question is entirely mapped to the underlying ontological structure the corresponding instances are obtained as an answer."
"on a second level, and enhancing the scope embraced by closed domain models, we can distinguish those approaches restricted to their own semantic resources. while successful nl search interfaces to structured knowledge in an open domain scenario exist (popular examples are powerset or trueknowledge), they are restricted to the use of their own semi-automatically built and comprehensive factual knowledge bases. this is the most expensive scenario as they are typically based on data that are by and large manually coded and homogeneous."
"the first step in ahp is to construct a hierarchy. in doing this, participants decompose a complex decision into a hierarchy with an overall goal, criteria, sub-criteria and decision alternatives. a hierarchy is a system of ranking and organizing goal, criteria and alternatives on purpose, where each element in the system, except the top one, is subordinate to one or more other elements. a hierarchy can be constructed by creative thinking, recollection, and using people's perspectives and there is no set of procedures for generating the levels in the hierarchy [cit] . the structure of the hierarchy will depends not only on the nature or type of problem at hand, but also on the knowledge, judgments, values, opinions, needs, wants, etc. of the participants in the process [cit] . hence, the hierarchical representation of a system may vary from one person to another. 3 moderately experience and judgment slightly favor one activity over another."
"linked data initiatives are producing a critical mass of semantic data, adding a new layer of complexity in the sw scenario, from the exploitation of small domain specific ontologies to large generic open domain data sources containing noisy and incomplete data. thus, two main user-centric evaluations have been conducted to test poweraqua: before and after using linked data, to investigate whether it can be used to exploit the data offered by linked data. in the first evaluation, poweraqua was evaluated with a total of 69 queries, generated by 7 users, that were covered by at least one ontology in the semantic information space (consisting in more than 130 sesame repositories, containing more than 700 ontological documents). poweraqua successfully answered 48 of these questions (69.5%). the second evaluation was focused on scalability and performance when introducing into the previous evaluation setup one of the largest and most heterogeneous datasets in linked data, dbpedia [cit] . the time needed to answer a query depends on two main factors: (1) the total number of (sparql-like) calls send to the ontologies to explore relevant connections between the mappings, which depends directly on the number of semantic sources and mappings that take part in the answering process, and (2) the response times to these calls, which depends on the complexity of the (sparql) queries and the size of the ontology. poweraqua algorithms were optimized by introducing heuristics to balance precision and recall, thus to analyze the most likely solutions first (iteratively refining candidates only as needed). these heuristics reduced by 40% in average the number of queries sent to the ontologies, however the response times to answer a query increased from 32 to 48 secs. initial experiments using a different back-end for large-scale sources, i.e. virtuoso instead of sesame, reduced the average time to 20 secs. [cit], focused on the usability aspects of different search tools (in particular keyword-based, form-based and nl) within a controlled user study using the mooney geography dataset. of the systems tested, poweraqua was the system with better usability results, evaluated as \"good\" by the users."
"although ontology-based qa can use the context of the query to disambiguate the user query, it still faces difficulties to scale up to large-scale and heterogeneous environments. the complexity arises because of its \"openness\", as argued in (mollá [cit] ), qa systems in restricted domains can attack the answer-retrieval problem by means of an internal unambiguous knowledge representation, however, in open-domain scenarios, or when using open-domain ontologies, as is the case of dbpedia or wordnet that map words to concepts, systems face the problem of polysemous words, which are usually unambiguous in restricted domains. at the same time, open-domain qa can benefit from the size of the corpus: as the size increases it becomes more likely that the answer to a specific question can be found without requiring a complex language model. as such, in a large-scale open scenario the complexity of the tools will be a function of their ability to make sense of the heterogeneity of the data to perform a deep exploitation beyond simple lookup and mash-up services. moreover, ranking techniques are crucial to scale to large-scale sources or multiple sources."
"this paper proposes a model to evaluate customer value of b2c electronic commerce companies, using analytical hierarchy process based approach, fuzzy ahp. a case study is presented to demonstrate how the model can help in solving such problems. the results show that fahp have the capability to be flexible and apply to evaluate customer value. the final relative weight at the last level of the hierarchy will lead to a recommended best option. it can be concluded that the approach we proposed could facilitate decision making."
", each represents the fuzzy comparison weights of three alternative b2c companies with respect to customer information value, customer contribution value and customer loyalty value, shown in table v ."
"as we have seen in the previous subsections, large-scale, open-domain qa has been stimulated in the last decade [cit] by the trec qa track evaluations. the current trend is to introduce semantics to search for web pages based on the meaning of the words in the query, rather than just matching keywords and ranking pages by popularity. within this context, there are also approaches that focus on directly obtaining structured answers to user queries from pre-compiled semantic information, which is used to understand and disambiguate the intended meaning and relationships of the words in the query."
"we asked the committee members to make a fuzzy evaluation respectively on the all elements in the proposed hierarchy. followed by calculation steps of fahp, we began with the constitution of original fuzzy comparison matrices, and use the committee's judgments. then, formulae (8) and (9) .matrix a shown in table iv represents the fuzzy comparison weights of the three major criteria in the second level with respect to the overall goal in the top level. matrix ( )"
"in the middle of the formality continuum, gin-seng [cit] ) controls a user's input via a fixed vocabulary and predefined sentence structures through menu-based options, as such it falls into the category of guided input nl interfaces, similar to lingologic (thompson et a., 2005) . these systems do not try to understand nl queries but they use menus to specify nl queries in small and specific domains. ginseng uses a small static grammar that is dynamically extended with elements from the loaded ontologies and allows an easy adaptation to new ontologies, without using any predefined lexicon beyond the vocabulary that is defined in the static sentence grammar and provided by the loaded ontologies. when the user enters a sentence, an incremental parser relies on the grammar to constantly (1) propose possible continuations to the sentence, and (2) prevent entries that would not be grammatically interpretable."
"to scale poweraqua model to an open web environment, exploiting the increasingly available semantic metadata in order to provide a good coverage of topics, poweraqua is coupled with: a) the watson sw gateway, which collects and provides fast access to the increasing amount of online available semantic data, and b) its own internal mechanism to index and query selected online ontological stores, as an alternative way to manage large repositories, like those offered by the linked data community, often not available in watson due to their size and format (rdf dumps available as compressed files)."
"the system that reports the highest performance is the e-librarian: in an evaluation with 229 user queries 97% were correctly answered, and in nearly half of the questions only one answer, the best one, was retrieved. two prototypes were used: a computer history expert system and a mathematics expert system. the higher precision performance of e-librarian with respect to a system like panto reflects the difficulty with precision performance on completely portable systems."
"in table 7 .1 we compare how the different approaches to query the sw, tackle these traditional intrinsic problems derived from the openness of the search environment (automatic disambiguation of user needs, ranking, portability, heterogeneity and fusion across sources)."
"with regards to fusion (point 2) only mash-ups and open ontology-based qa systems aggregate answers across sources. however, so far, mash-ups do not attempt to disambiguate between the different interpretations of a user keyword."
"to cope with the slower pace of increase in new knowledge in semantic repositories, in compassion with non-semantic web repositories, semanticqa [cit] makes it possible to complete partial answers from a given ontology with web documents. semanticqa assists the users in constructing an input question as they type, by presenting valid suggestions in the universe of discourse of the selected ontology, whose content has been previously indexed with lucene. the matching of the question to the ontology is performed by exhaustively matching all word combinations in the question to ontology entities. if a match is not found, wordnet is also used. then all generated ontological triples are combined into a single sparql query. if the sparql query fails, indicating that some triples have no answers in the ontology, the system attempts to answer the query by searching in the snippets returned by google. the collection of keywords passed to google is gathered from the labels of the ontological entities plus wordnet. the answers are ranked using a semantic answer score, based on the expected type (extracted from the ontology) and the distance between all terms in the keyword set. to avoid ambiguity it allows restricting the document search to a single domain (e.g., pubmed if the user is looking for bio-chemical information). a small scale ad-hoc test was performed with only eight samples of simple factoid questions using the lehigh university benchmark ontology 10 one can conclude that the techniques used to solve the lexical gap between the users and the structured knowledge are largely comparable across all systems: off-the-shelf parsers and shallow parsing are used to create a triple-based representation of the user query, while string distance metrics, wordnet, and heuristics rules are used to match and rank the possible ontological representations."
"performance and scalability issues still remain open. balancing the complexity of the querying process in an open-domain scenario (i.e., the ability to handle complex questions requiring making deductions on open-domain knowledge, capture the interpretation of domain-specific adjectives, e.g., \"big\", \"small\", and in consequence superlatives, e.g., \"largest\", \"smallest\" [cit], or combining domain specific information typically expressed in different sources) and the amount of semantic data is still an open problem. the major challenge is, in our opinion, the combination of scale with the considerably heterogeneity and noise intrinsic to the sw. moreover, information on the sw originates from a large variety of sources and exhibits differences in granularity and quality, and therefore, as the data is not centrally managed or produced in a controlled environment, quality and trust become an issue. publishing errors and inconsistencies arise naturally in an open environment like the web [cit] . thus, imperfections (gaps in coverage, redundant data with multiple identifiers for the same resource, conflicting data, undefined classes, properties without a formal schema description, invalid datatypes, etc.) can be seen as an inherent property of the web of data. as such, the strength of the sw will be more a by-product of its size than its absolute quality."
"lessons and remaining open issues: these systems have the capability to deal with the heterogeneous data crawled from the web. however, they have limited reasoning capabilities: mappings are either found and stored a priori (searchwebdb), or disambiguation between different interpretations is not performed (erdf). the scale and diversity of the data put forward many challenges, imposing a trade-off between the complexity of the querying and reasoning process and the amount of data that can be used. expressivity is also limited compared to the one obtained by using query languages, which hinders the widespread exploitation of the data web for nonexpert users. finally, in both facets and mash-ups, the burden to formulate queries is shifted from the system to the user. furthermore, they do not perform a semantic fusion or ranking of answers across sources."
"as such, we believe that open semantic qa is a promising research area that goes beyond the state of the art in user-friendly interfaces to support users in querying and exploring the heterogeneous sw content. in particular:"
"on a third level, we can highlight the latest open semantic search approaches. these systems are not limited by closed-domain scenarios, neither by their own resources, but provide a much wider scope, attempting to cover and reuse the majority of publicly available semantic knowledge. we have seen examples of these different approaches: a) using linked data sources, i.e., dbpedia, for a query completion component on the yahoo search engine, b) keywordbased query interfaces to data sources available in the billion triple challenge datasets and live sparql endpoints, c) mash-ups able to aggregate heterogeneous data obtained from the search engine sindice from a given keyword, d) open linked data facets, which allow the user to filter objects according to properties or range of values, and e) nl qa system over multiple heterogeneous semantic repositories, including large linked data sources (i.e. dbpedia) and (with some decrease in performance) the search engine watson."
all rssi values of the moving sensor nodes measured in the scenarios above by the anchor nodes are recorded and sent to the central computer by the network. the results analyzed in many ways by the mat lab software are described as follows: the relationship between rssi of the moving sensor node moving at the different fixed speed and the distance a) we gathered 500 rssi values of the moving sensor nodes that kept static one minute at least at each place on the path. the rssi data with noises are shown in fig. 2 .
"after detecting an attack the set p contains the fixed headers of the attacker. from this set it is easy to craft a flow rule that matches the fields from p while treating the others as wildcards. if one header or more headers is in the set with different values, multiple rules have to be installed. for this the existing rule with the singular headers from p has to be copied and for each value of one header a rule has to be created. this could for example be the case if the attacker leverages a bot net and as a result more than one ip-source address has to be blocked. when installed as a low prioritized dropping rule in the switch it is now able to handle further attacking packets at line rate and without additional interaction with the controller."
control plane data plane the logically centralized sdn controller represents a potential single-point-of-failure and many researchers have developed comprehensive approaches to deal with these threats [cit] . in this work we focus on a much less investigated area of sdn security -the detection and mitigation of denial of service attacks against the data plane.
"the rest of the paper is organized as follows: the rssi-d model that is established based on the offline rssi data and its arguments estimated by em are introduced in section ii. based on rssi-d, the distance between sensor nodes can be estimated in section iii. simulation results with comparison of the performances of rssi-d and log-d are presented in section iv where the influence of the number of simple points on accuracy of rssi is discussed. the conclusion of the paper is shown in section v."
"the road in the mine is always straight as a beeline though having a cross. the length of the road may be several hundred meters or even more, but the width is always smaller than 4 meters and the height of the road space is about 3 meters only. in the scenario, the distance between sensor nodes can be equal to the distance in x-direction."
"the general idea for the detection of dos attacks against the data plane aims at localizing the fixed header fields of the attacking flow. these impose a regularity that is not observed in normal traffic since packetin events are just seen once upon flow establishment. the approach uses a table of counters with the different header fields as columns. the table is regularly, i.e. in fixed time intervals, inspected statistically and the maximum entry is abnormally large in case of an attack. to normalize the table size the header fields are hashed by a uniformly dispersing function with fixed output size. the digest of an input determines the row where to increment the counter. during an attack the entries which correspond to fixed fields of the attacking flow grow very fast and are used for the detection. table i shows a simplified example with three header fields a, b and c where the latter is varied. the columns represent the different header fields, while the rows are accessed using the hashed values of the particular header field. after a couple of packetins the fixed fields of the attacking flow are clearly distinguishable from the varied fields."
the conclusion is that the arguments of rssi-d that are estimated by em have good accuracy when n is larger than 15000. the contributions of this paper are concluded here: a) a novel method that is used to increase the accuracy of rssi techniques is proposed in this paper based on rssi-d that is a type of gmm and established with the offline rssi values and their statistic patterns.
"every incoming packetin is unwrapped and the included packet is handled by the method shown in algorithm 1. for each field its value is hashed. the hash sum is now used as an index in the table where the counter is incremented. additionally, the corresponding fields of the most recent packets are stored with the counter for later usage by the mitigation routine upon a detected attack. the employed fnv-1a hash function is designed to have a low collision rate. thus we can assume that storing of one field is enough in practice."
"a few open issues of the ml-esm that we aim to address in the near future include: 1) detailed theoretical and comparative experimental evaluation of the model using different kinds of reservoir kernels and benchmark datasets; 2) derivation of multiobjective optimization criteria that can help select the appropriate number of layers, number of neurons in each reservoir, learning, adaptation, etc. for a particular task; and 3) adaptation of multiple layer reservoir network in an unsupervised fashion for appropriate tasks and evaluating the role of topological organization of reservoirs inside an ml-esm."
where: d is the distance that corresponds to the kth model; m is the number of the selected models whose probability is larger than the threshold; k is the total number of the models.
"in general, the larger is the number of sample points, and the more accurately is the distance estimated by rssi-d, however the more complex is the calculating. meanwhile, n is large so that rssi-d is hard to update and to be used in reality. in order to estimate a proper value of n, we set n as different values such as5000, 15000, 25000 and 30000with a proper k, which can be assigned to 50 based on the result analyzed above, to compare the accuracy each other. the result is shown in fig. 6 and table ii as follows:"
"d) in simulation, the accuracy of rssi estimated by rssi-d is better than that of log-d obviously. meanwhile, k and n are analyzed in the simulation and set properly to have a good accuracy and decrease the calculating complexity as much as possible."
"the results shown in figure 4 support this theoretic model, the simulation fits the theoretical results very well, i.e. the analytical expected value matches the simulated mean."
"based on the established rssi-d and combining the posterior probability in bayesian statistics, the distance between sensor nodes can be estimated by (12) shown as follows:"
"1) the information of the sensor nodes a) seven anchor nodes are located with known coordinates manually at a segment of a crossroad in the mine, of which the width is 3.8 meters, the height is 3.0 meters, the length in x-direction is 500 meters and the length in y-direction is 200 meters. all anchor nodes are located at a fixed height as 2.5 meters, and the un-anchor sensor nodes are distributed randomly. the scenario above is shown as fig. 1 . c) for simplicity and ease of presentation, we limited that all moving sensor nodes wore on the workers were at the height of one meter."
"in order to collect more rssi data and avoid the local optimization problem because the sample capacity of the rssi values are small, we collected the rssi data of moving sensor nodes in two scenarios described in details in next section."
"the reuters corpus contains 21 578 documents grouped into 135 clusters. it is very unbalanced, with some large clusters more than 300 times larger than some small ones. we have considered the modeapte version of this corpus which discards documents with multiple category labels, and only selects the categories with more than ten documents. this left 8123 documents in total of 65 categories."
"although if the product of window time and rate of legitimate hosts is big we have a very broad detection range, i.e. we have a big region where we can choose a good threshold while sacrificing the detection speed. on the other hand for a small product and consequently a small threshold the system is very sensitive for changes in the arrival rate of the users as this can cause false positives."
"it's easier to find the relationship between rssi and the distance than to establish an accurate mathematical system, so based on the statistical property of the test data, a novel method in this paper is proposed to improve the accuracy of the rssi techniques. in this paper, a gaussian mixture model (gmm) that describes the relationship between the rssi values and the distance is established with the offline rssi values measured by anchor sensor nodes, which is called the rssi-d model. and then, the em algorithm that is an iterative method for finding maximum likelihood or maximum a posteriori (map) estimate of parameters in statistical models where the model depends on unobserved latent variables, is used to estimate the arguments of rssi-d with a few of the offline rssi values. based on the established rssi-d model with the estimating arguments, the probability of each sub model of rssi-d, which the online rssi value belongs to, can be calculated by the posterior probability in bayesian statistics. finally, the distance between sensor nodes is estimated by the weighted corresponding distance to each sub model, since the probability that the online rssi value belongs to the sub model is larger than the set threshold and regarded as the weight of each corresponding distance."
d) the accuracy of rssi estimated by rssi-d is much better than that of log-d when the distance between the moving sensor node and the anchor node is in the range [cit] . the reason is that the pattern of the rssi values can be described more accurate by rssi-d than by log-d that is only a curve.
"in accordance with the configurations of table i for this dataset, the comparison of the proposed ml-esm with the standard state-of-the-art esn using both linear and ridge regression techniques and the svesm method is also regulated. here too the ml-esm outperformed the standard esn by improving the predicted accuracy and reducing the error. svesm's performance in this particular experiment as shown in table viii was not bad but computationally was very expensive in comparison to other esn-based approaches."
"b) it's impossible to collect all offline rssi values, so the em algorithm is used to estimate the arguments of rssi-din this paper, where the model depends on unobserved latent variables. c) based on the accurate rssi-d model, the distance between sensor nodes can be estimated in two steps. firstly, sub models of rssi-d that the rssi values obey in large probability, which is calculated by the posterior probability in bayesian statistics and larger than a threshold set by test are selected; secondly, the distance between sensor nodes is equal to the weighted value ofthe distance segments corresponding to the sub models of rssi-d, whose weight is equal to the probability above."
"after rssi-d established accurately, the distance corresponding to the rssi value can be estimated by the kth model with the maximum probability calculated by (2), because the kth model is established based on the rssi values that are measured at the distance segment k*r/k, where the result of r/k means that the moving node moves at a fixed step at once. so, based on the kth model that the online rssi may obey in the maximum possibility, the distance between the sensor nodes can be estimated with rssi finally. this procession can be described by the posterior probability in bayesian statistics, and the probability of each sub model that the online rssi values may obey can be calculated by (10):"
the mackey-glass delay differential equation has provided classical benchmark tasks for time series modeling. the mackey-glass delay differential equation in a discrete time setting is approximated as
"based on the conclusions discussed above, the model that describesthe relationship between the rssi values of moving or static sensor nodes measured by any anchor node and the distance can be established."
"where input sequence x is an array of size equivalent to twice the sequence length. output sequence y i is an array of size equivalent to the sequence length. the length of the sequence in our experiment was 1000. the constant variable a and b were initialized with 0.7 and 0.1 which were used to generate a linear sequence. the nrmse and mean square error (mse) of the esn-based models are shown in table iv . again ml-esm with linear and ridge regression techniques outperformed the standard linear and ridge regression-based esn as well as the svesm model. fig. 5 further shows the comparison of single layer esn with the proposed multiple layer esn using the same range of data samples. fig. 5(left) shows the results by using linear regression technique whereas fig. 5(right) demonstrates the results using ridge regression. it can be clearly seen in fig. 5(left) and (right) that the error first maximally decreases using two-layer with the proposed multiple layer esn in comparison to the single layer esn; and then remain steady in the following layers from 3 to 5 with a slight and steady increase on addition of each layer. it can be clearly seen in table iv and fig. 5 that multiple layer esn produced best results with two and three layers where it significantly outperformed the standard esn using both linear and ridge regression techniques. svesm model like with henon map dataset in section iii-c did not perform well with this narma sequence too. this is basically due to the poor prediction performance of the svesm method at some time points, which adversely affects the average method performance."
"the conclusions discussed above are suitable to any anchor node though it may measure the different rssi values at the same scenario. b) we selected the rssi values of one moving sensor node, which made round trips to the areas at the different fixed speed, measured by one anchor node to observe the influence of the speed of moving nodes on the relationship between rssi and the distance. after filtering the very large and very small rssi values, the relationship between the average values of the remainder rssi values and the distance between the moving sensor nodes and the anchor nodes is shown as fig. 3 ."
"second, fig. 6(b) and (e) again shows the output of error of the ml-esm using linear and ridge regression techniques. this time the effect on error is observed by varying the number of neurons inside each reservoir and the number of layers of the proposed method. the spectral radius is held fixed equal to 1.25 throughout the experiment for both techniques. first, it is observed that for both linear and ridge regression techniques, lesser number of neurons inside each reservoir produces lesser error using multiple layers of reservoir inside an esm. second, the highest variation in error from low to high is observed using two layers inside the ml-esm. the variation of error is observed much greater in linear regression compared to ridge regression technique. lastly, for both techniques higher the neurons inside each reservoir larger was the error produced with smaller number of layers rather than larger number of layers."
"in this paper, we proposed a new multiple layer esn modeling of sequential data, namely ml-esm. on a number of widely used time series benchmarks of different characteristics and origins as well as by conducting a theoretical analysis we have shown the following. 1) a multiple layers connected cyclic topology is often sufficient for obtaining better performance comparable to those of simple standard cyclic topology."
the conclusion that can be proposed is that the phenomenon that the rssi values decrease with the distance increasing is always perfect though the moving sensor node moves at the different speed.
"so, the key point establishing the model is how to estimate the arguments expect k as accurately as possible with a few of sample data."
"proof: \"state contracting ⇒ echo state\": assume that ml-esm has no echo states along multiple layers, that is (14) where d + be the set of all identical pairs ("
"in this section, we consider a sequence of narma model. the sequence at the beginning includes a ramp-up transient. the output of the narma sequence model depend on past and present values of the input as well as the output"
the impact on the network is negligible since only regular hosts falling in the detected flow characteristics are affected by the mitigation which is considered unlikely concerning our attack model.
"the third section demonstrates the comparative analysis of the proposed method with the standard state-ofthe-art benchmark approaches, considering mackey-glass series dataset, henon map, nonlinear autoregressive moving average (narma) sequence, an artificially generated figure 8 dataset, 15 classification problems, reuter-21578 and finally predicting human motion using carnegie mellon university (cmu) mocap dataset."
"as the conclusions mentioned in section ii.b, the k value of rssi-d is decided by the segmented distance mode. for example, when a moving sensor node moved with the speed one meter per second, the distance segment is r divided by one, while r is the anchor node's communication distance. so, the more are the distance segments, the larger is the k and the better is the model because of the degree of the distance subdivision, however, the more complex is the model to calculate."
we used the 32bit fnv-1a hashing function (see [cit] ) folded to an output size of 16bit by applying an xor operation of the upper half to the lower half of the hash sum. the chosen hash function is designed to be fast while having a low collision rate which is evenly distributed itself. this results in 2 16 rows in the counter
"in this paper, the rssi-darguments are estimated by the expectation maximization (em) algorithm that is an iterative method for finding maximum likelihood or maximum a posteriori (map) estimates of parameters in statistical models, where the model depends on unobserved latent variables."
"the uprise of software defined networking (sdn) as a paradigm that separates the data from the control plane introduces new challenges in network security. especially, in modern cloud environments where an attacker can get access to the network by simply renting a virtual machine, denial of service (dos) attacks pose a serious threat."
"in our experiments, the analyzed esn-based models, were trained using the first 1000 samples of the henon map. the initial transient was washed out by employing a reservoir warm-up time of 100 steps. [cit] samples of the henon map, with the employed reservoirs being teacher-driven for the first 100 time points. the performance of the analyzed models in terms of the obtained nae 84, nae 120, nae 400, and nae t metrics is depicted in table iii. as we observe, the ml-esm model using both ridge and linear regression technique performs better than the considered alternatives. this happened due to the externally connected structures temporally creating a long term memory cycle. this approved in reducing the error compared to the standard state-of-the-art time series learning approaches. it is worth noting that svesm is the worst performing method in this experiment."
"very early on, minsky and papert [cit] left open the possibility that multilayer networks may be capable of better performance. recently, this idea has virtually exploded with impressive successes across a wide variety of applications [cit] ."
"in the future we want to enhance the detection with additional metrics such as the difference between two subsequent maxima. finally, we want to further extend the mitigation of the attack. the mitigation could also lead to a relatively big set of table entries and deteriorate performance, though fewer rules than without our approach are used. especially, possible side effects on the legitimate network traffic should be investigated."
"in this section, we provide a thorough experimental evaluation of ml-esm model, considering: 1) mackey-glass series dataset, which provides the most classical benchmark tasks for time series modeling; 2) henon map, which is a discrete-time dynamical system exhibiting a characteristic chaotic behavior; 3) narma sequence which generates a sequence using an narma model; 4) an artificially generated figure 8 dataset with the points of the figure moving around very quickly, and each cycle comprising only few points; 5) robust evaluation of the proposed method compared with the standard benchmark techniques on 15 classification problems (see table vi ); 6) reuter-21578, a popular dataset mostly used for evaluating text mining algorithms; and 7) finally, predicting human motion using cmu mocap dataset [cit] . in all the experimental evaluations, we consider reservoirs comprising analog neurons, with tanh transfer functions. to demonstrate the advantages of the proposed ml-esm, we also evaluate linear regression-based esns, ridge regression-based esns, and svesm models with -insensitive loss functions [cit], using the same reservoirs as the evaluated multiple layer esn model."
"this section describes the abstracted simulation method which was used to validate the detection algorithm. besides the simulation results, an analytic evaluation of the false positive and false negative probabilities is provided which can be used to determine the correct parameters of the detection algorithm."
"in this work we focus on a dos attack class against the data plane that is based on a reactive sdn which is the widely accepted standard behavior of openflow switches. as previously seen in figure 1, upon the incoming of an unknown flow the switch typically consults the controller for further decisions by encapsulating the first packet of the flow into a packetin message. then, the controller can inspect the packet, make a forwarding decision, and set new flow rules in the switch using flowmod messages if necessary. there are different ways for a controller to handle this decision making, especially regarding the granularity of flow definitions. for instance, flows could be setup using the quintuple source ip, destination ip, source port, destination port, protocol or just layer-2 addresses. an attacker knowing about the controller's decision making and flow rule setting behavior is able to craft packets that trigger those flowmods. for the switch these packets appear as new flows and therefore, are handled by the controller resulting in an increasing number of flow rules in the switching tables as depicted in figure 2 . the concrete impact of a switch with full switching tables is not generally defined and highly depends on the model. typical behavior includes the dropping of older switching entries or ignoring new rule setting requests."
"the evaluated model was trained using a sequence of 600 data points from the figure 8 trajectory, and no reservoir warm up was employed. on the sequel the trained models were evaluated over 600 time steps. in fig. 4, we provide the trajectories produced by the evaluated methods. as we observe, the ml-esm using both linear and ridge regression technique works considerably better than the svesm and the linear regressionbased esn, and slightly better than the ridge regression-based esn. specifically the ridge regression-based esn yields a normalized root mean square error (nrmse) equal to 0.0085, whereas the ml-esm using ridge regression technique yields an nrmse equal to 0.00002588."
"in this section, the 15 multivariate benchmark datasets are considered from uci machine learning repositories [cit] (see table vii ). the performance of all the methods including the ml-esm and other state-of-the-art rnn-based time series learning approaches were inconclusive. this is due to the time series learning nature of the proposed method and other state-of-the-art approaches which proved more useful in other time series chaotic predictive task demonstrated in this paper. however, still the proposed ml-esm using both linear and ridge regression outperformed compared to the other standard benchmark techniques. this happened only due to the externally connected transition structures sequentially creating a long term memory cycle which further helped in reducing the error compared to the standard state-of-the-art time series learning approaches."
"a number of methods have been proposed recently to improve the accuracy of the rssi techniques. some have focused on obtaining good rssi values based on eliminating noise, such as kalman filter (kf) [cit], extended kalman filter (ekf) [cit] and particle filter (pf) [cit] . these methods that can get more smooth rssi values than the original data but need to establish the accurate mathematical system models are hard to be used in the reality due to their complex. other approaches have been studied on the statistical property which represents the relationship between rssi and the distance between sensor nodes. these methods based on establishing the injective function between the rssi values and the distance have been proposed in the [cit] . however, the injective function above can't be established accurately as one rssi value can be measured at different distance between sensor nodes because of the multipath interference or the reflection."
our evaluation shows that the algorithm can detect attacks reliably and with low false positive probability with the correct parameters. using the proposed formulas it is analytically possible to determine a good choice of these parameters.
"the standard esms are state-space models with fixed state transition structure (\"the reservoir\") and an adaptable readout form for the state space. adding more connected layers inside the state space of fixed state transition structures are expected to improve the overall performance of the model. sequentially connecting each fixed state transition structure externally with other transition structures creates a long term memory cycle for each. this gives the multiple layer esm (ml-esm) proposed in this paper, the capability to approximate with better accuracy, compared to the state-of-the-art esn-based approaches."
"the probability distribution of the rssi values with different distances between the anchor sensor nodes and the moving sensor nodes is the gauss distribution with different mean values and variances as well. meanwhile, the mean values and the variance are becoming smaller with the distance decreasing."
"\"state contracting ⇒ state forgetting\": assume the multiple layer network is not state forgetting. this implies that there exist a left-infinite input sequenceū −∞, a strictly growing index sequence"
b) another scenario is that the moving sensor nodes make a round trip to the areas where rssi of moving nodes can be measured by the anchor nodes at a random speed.
"over the last decade, the esn has been recognized as the most efficient network structure for training rnns. it was invented independently in the seminal works of jaeger [cit], who termed these rnns \"esns.\" [cit] developed a similar approach for spiking neural networks and termed the derived model: \"liquid state machine.\" these two pioneering methodologies have given rise to the novel paradigm of reservoir computing (rc) [cit] ."
"this paper is organized as follows. the next section of this paper explains how multiple layer echo machine can be trained in a supervised manner. the natural approach here is to adapt only the weights of the ml-esm to the output connections. essentially, this trains readout functions which transform the multiple layer esn into the desired output signal. this section also defines multiple layer echo states and provides equivalent characterizations."
"third, fig. 6 (c) and (f) shows the output for both ml-esm using again linear and ridge regression techniques. in these two subfigures, the effect on the error is analyzed by sequentially changing the number of neurons inside each reservoir and the spectral radius of the weight matrices. it can be again clearly seen in these two subfigures that higher number of neurons inside each reservoir increases the error compared to where lesser number of neurons are used. the proposed method with ridge regression came out overall slightly better compared to the other. further this means that multiple layers of the proposed method produces best approximation with lesser number of neurons inside each reservoir compared to the standard state-of-the-art which is normally observed better worker with larger number of neurons."
"the un-anchor sensor nodes located in the mine can be classified into two categories, as the moving sensor nodes and the static nodes. the static nodes are usually used to collect the arguments of the circumstances of the mine, and the moving nodes that are wore on the workers or the harvesters are used to get the location information. in the mine, the maximum speed of the moving sensor nodes wore on the workers is lower than three meters per second, and that of the harvesters is smaller than eight meters per second."
"the smaller is the distance between anchor sensor nodes and the moving sensor nodes, the more the rssi values will change. on the contrary, the larger is the distance, the more stationary are the rssi values."
"as the conditions of the test have been described in the section ii.a, the simulation with the collected test data is described in three manners."
"our approach is more comprehensive than exiting works, as it covers all header fields. additionally we show a novel analytic approach for the dimensioning of the system and the expected detection rates."
note that in general the attacker is not able to arbitrarily craft the packets since all attack packets need to be routed via the target switch. cloud environments like for example openstack could also prohibit source ip spoofing with packet filters. a rule matching the exact characteristics of the attacking flow and applying a standard behavior like dropping the attacker's packets would be able to completely shut down the attack.
"in our experiments, the weights of the input u(t), stored in the matrix w in, the reservoir weight matrix w and the external reservoir weight matrix w ex in terms of multiple layer esm, are drawn randomly with a uniform distribution over (−0.1, 0.1). the results provided in the remainder of this section are averages over five different random reservoir initializations. finally, training of all esn-based methods was conducted using fivefold cross validation."
"2) the testing ways a) a scenario is that the moving sensor nodes move far from or close to anchor nodes with a fixed step each time such as one meter a step, meanwhile, the moving nodes remain stationary with a long time at each place of the moving paths."
"block all packets that match headers in p end if reset t end for as seen in algorithm 2, the detection routine runs independently of the book keeping on a regular basis and statistically evaluates the counter values. the table is evaluated every fixed time interval t w . the counter is evaluated for every header h and every hash value s, i.e. for every field in the table. if the value of a counter is higher than a predefined threshold θ m, an attack is indicated and the corresponding field v h,p is appended to list p ."
"in this experiment, we evaluate the effectiveness of our multiple layer esn model in learning complex sequential patterns. for this purpose, we consider an artificially generated figure 8 dataset with the points of the figure moving around very quickly, and each cycle comprising only few points. to obtain this signal, we use the artificially created function figure8_dataset which generates the figure 8 whose circles are centered at (384, 302) and (384, 722), with a radius of 210 and a channel width of 79, i.e., 1.5 cm."
"forwarding tables have limited memory capacities. typically, switches rely on content addressable memory (cam) that performs table lookups at line rate. especially, ternary cam (tcam) is expensive and therefore very small, ranging in the region of 1k to 2k entries. but also, regular and cheaper binary cam (bcam) is limited to only a couple of 100k entries. an attacker with the ability to remotely trigger flow handling modifications that add entries to the flow tables can cause a dos by exhausting the switches memory. some works 978-1-5090-6008-5/17/$31.00 [cit] ieee extend the available table size by using a combination of software and hardware flow tables [cit] . if the attacked devices make use of such techniques, the severity of the dos is reduced, as software tables allow far more entries. although, also software tables suffer from performance penalties for big table sizes, e.g. open vswitch uses a linear search to handle wildcard rules [cit] . to deal with this threat we propose a tailored statistical approach for the detection of such an attack. therefore, our main contributions are:"
"in this work we introduced a novel detection and mitigation algorithm for denial of service attacks in software defined networks. our work concentrates on attacks which aim to overflow the hardware tables of sdn switches in cloud environments. the attacker causes a high number of packetin messages by changing header fields. our proposed detection approach is based on the observation that an attacker cannot change all header fields. this allows us to identify the attack. for example, in order to keep the connectivity with the network for openstack spoofing the source ip address is not possible. our approach uses a table with the header fields as columns and hashes of the header fields as rows. if an attack occurs the fields corresponding to the unchanged fields grow tremendously. after identifying the attack, we propose to use an openflow rule which drops further attack packets."
"in reactive setups, on the other hand, a table miss results in a query to the controller. in the controller, the networking applications can make a decision based on a global view of the network's state. then, they are able to enforce a network policy, e.g. routing, by individually forwarding packets, sending out packets, or setting up forwarding rules. the most prominent sdn protocol allowing both modes of operation is openflow (see [cit] ) which also enables any hybrid approach with proactive and reactive elements. figure 1 shows the typical behavior of a reactive sdn setup: 1) a host sends a packet for a new connection which reaches an sdn switch. then, the switch performs a lookup in its forwarding table."
"one drawback is, that our approach cannot detect attackers, that can change all header fields simultaneously. this could be the case for an attacker that controls a big bot-net."
"in this experiment, we train the evaluated method using four walking sequences, one running sequence, one bending sequence, one washing sequence and one dancing sequence from the cmu mocap dataset [cit] . the considered training sequence, corresponding to five different subjects, are obtained from the cmu database files: 35_02, 02_03, 16_21, 02_06, 02_10, 05_02, 03_01, and 06_01. in the sequel, we use 50% of each considered video as training set of the evaluated algorithms and the remaining fifty percent as the testing set. in table v, we provide the nrmses obtained by each of the considered method. similarly in figs. 8 and 9, we have shown the selected frame from the testing dataset of each considered video to show the difference between the actual testing frame and the predicted frame using linear and ridge regression with the proposed ml-esm. it can be clearly seen from figs. 8 and 9 that the two right columns which show the predicted output of ml-esm using both linear and ridge regression technique; predict human motion almost correctly compared with the actual frame with slight vibration/noise to be seen on the moving part and very clear on the remaining part. the nrmse shown in table v clearly reflects the predicted output of frames visualized in figs. 8 and 9 . this enable us to conclude that adding another layer in ml-esm predict comparatively better compared to predictions of svesm and single layer esm using both linear and ridge regression techniques. further we tested the performance of our proposed method from 2-5 layers and compared it with single layer esn using both linear and ridge regression shown in fig. 7 . fig. 7(left) shows the output using linear regression whereas fig. 7(right) shows the output using ridge regression technique. there is always a slight decrease in error noted by adding more than one layers compared with single layer esn in every video file. the decrease mostly occurs in layer 2 or 3 whereas in the remaining layers the error is seen mostly steady. bearing also in mind that running these experiments took around 45 s for the ml-esm, and more than 1 h in the case of svesm, it becomes apparent that ml-esm is a favorable alternative over svesm in this application."
"3) the state forgetting, state contracting, and input forgetting properties are all equivalent to the multiple layer network having echo state property along each layer. 4) the null state input sequence along all the layers of the ml-esm is compatible with the null state sequence. 5) the addition of multiple layers of reservoir provide a more robust alternative to conventional rc networks. additionally, with respect to the standard single layer esn methodologies for sequential data modeling, we observe that our method is overwhelmingly more accurate in terms of reducing the error and computationally competitive as well. similarly with respect to existing support vector machinebased esn methodologies, such as svesm [cit] . our method is computationally more efficient, especially in cases where large data corpora have to be processed."
"the rssi values are variable because of the noises so as that the injective function from rssi to the distance can't be established in reality as well. hence, the distance calculated by (1)is very unreliable."
"we exactly followed the modeapte split of training and testing documents which provides 5946 training documents and 2347 testing documents. overall after preprocessing, this corpus contains 18 933 distinct terms. the nae of testing dataset using svesm, the standard state-of-the-art esn-based techniques and the proposed multiple layer esn using both linear and ridge regression are shown in table viii . further since the weight matrices of the ml-esm in the beginning are normally initialized randomly. to test their effect on the output we initialized the weight matrices including the input weight matrix, internal weight matrices within each reservoir and external weight matrices between reservoirs of each layer with different spectral radius. the spectral radius of the weight matrices co-determines: 1) the effective time constant of the ml-esm (larger spectral radius implies slower decay of impulse response) and 2) the amount of nonlinear interaction of input components through time (larger spectral radius implies longer range interactions). the gradual effect on nae due to the change in spectral radius of the weight matrices and the number of reservoir layers is demonstrated in fig. 6 . fig. 6(a) shows the output of ml-esm using linear regression and fig. 6(d) shows the output using ridge regression technique. first, it is observed that while using linear regression technique, the use of less number of layers with the gradual increase in spectral radius slightly increases the error. whereas using ridge regression technique decreases the error with less number of layers and the gradual increase in the spectral radius. this implies that particularly on this dataset, longer range interaction between the input components specifically while using linear regression do not proved useful in improving the performance of the ml-esm. further using both linear and ridge regression technique, with the increase in the number of layers and the gradual decrease in the spectral radius of the weight matrices significantly decreases the error in the beginning whereas the difference became lesser reaching to constant in the end."
"the model is established based on the offline rssi values that were collected by the sensor nodes deployed in the iron mine. as a special application scenario, the special features of the topology structure of wsn are described as follows:"
"the remainder of this work is structured as follows. in section ii we give an overview of the related work in the field of dos attacks in sdn followed by a detailed description of the attack model and our detection algorithm in section iii. finally, in section iv we provide our evaluation."
"2) since the packet belongs to a new flow which is unknown to the switch, the packet is encapsulated into a packetin message and sent to the controller. 3) in the controller the packetin is processed by an sdn application which provides networking functionality like switching or routing. the applications decision may include sending a flowmod message which installs a rule in the forwarding table. also, the switch may be instructed to forward the original packet through any of its ports. 4) if the controller application has set up a rule in the switch to handle the flow, further packets belonging to that flow will be processed in the fast forwarding hardware without the need for additional communication with the controller."
"where y i, x i, and η i are the output, input, and noise, respectively. m x denote the output and input memory orders. the narma sequence equation is approximated as"
"the anchor sensor nodes whose locations are known and accurate are deployed to cover the areas which the un-anchor sensor nodes may reside in. meanwhile, the method proposed in this paper does not need depending on neighboring sensor nodes communication."
"as mentioned above, k is decided by the number of the distance segments that is equal to the anchor's communication distance divided by the sampling interval. in order to analyze the influence of k on the errors, we set the sampling interval, which means the distance that the moving sensor node moves one step, as 0.5m, 1m and 2m. meanwhile, keeping n always equal to 25000 in each test scenario. that's to say, k is 100, 50 and 25 corresponding to the intervals. the error between the estimated distance and the reality distance is shown in fig. 5 and table i. the conclusions can be proposed as follows: a) the larger is k that means the number of the distance segments and also means the number of the sub models of rssi-d, and the smaller are the errors that include max errors, min errors and average errors. however, the calculating complexity increases with k in a power."
b) the errors have little difference though the k values are striking difference when the distance between the moving sensor node and the anchor node is larger than 30 meters. the reason is same to what has been concluded in previous section.
"networking elements, like switches, process traffic according to the entries in their forwarding tables which are set by a logically centralized controller. sdn offers two major modes of operation -proactive and reactive. in the former case the controller presets all forwarding rules according to the configuration of the networking applications which provide the networking functionality, e.g. switching or routing. packets that do not match any entry in the forwarding table are dropped by the networking element."
"with the help of this probability we can also get the false positive (fp) probability for a given threshold θ m, as this is the probability to reach a value of more than θ m :"
"nowadays, radio frequency identification (rfid) technology is playing a promising role for new generation non-invasive rfid sensor applications. the tag antenna is a key element of the rfid system. the read range is highly dependent on the tag antenna itself. a flexible tag antenna with an omnidirectional radiation pattern is essential for smooth rfid sensor application in various fields, like child tracking in childcare centers, patient tracking in hospital management systems, internet of things (iot) rfid sensors, epidermal sensors, etc. [cit] . the major challenges that could potentially hinder the practical implementation of rfid are cost-effectiveness, reliability, and flexibility of materials. several studies have been performed to address these challenges [cit] . inkjet printing technology is a promising technique for the low-cost fabrication of electronics and radio frequency (rf) circuit. refs. [cit] using engineered conductive inks made from silver nanoparticles, carbon nanotubes, or organometallic particles [cit] . at the beginning this technology required thermal sintering at a high temperature, but now, due to developments in material science, new types of conductive ink have been developed which dry instantly at room temperature [cit] ."
"author contributions: t.a. and m.t.i. conceptualized the proposed antenna, participated in revising the article critically for important intellectual contents and m.t.i. supervised the whole study. i.y. made significant contribution in addressing reviewer response and participated in revising the manuscript. m.c. participated in revising the article critically for important intellectual contents."
"in this paper, a passive printed tag antenna with an omnidirectional radiation pattern and a 15 ft (4.57 m) long measured read range is presented. the antenna encompasses the universal uhf rfid band (860-960 mhz). it exhibits a low-profile, compact, and flexible structure that makes it appropriate for rfid sensor applications."
"in this paper, a passive printed tag antenna with an omnidirectional radiation pattern and a 15 ft (4.57 m) long measured read range is presented. the antenna encompasses the universal uhf rfid band (860-960 mhz). it exhibits a low-profile, compact, and flexible structure that makes it appropriate for rfid sensor applications."
"this paper presents a paper-based flexible uhf rfid tag antenna for sensor applications. the design used a semi-circular shaped feed network with a meander line radiating element, printed using inkjet printing technology. the antenna is printed on photo paper using silver nanoparticle conductive ink. the proposed tag antenna offers eco-friendly low-cost rfid tag service for sensor modules. the read range of the proposed antenna has been validated using an rfid reader module and a read range of about 4.57 m is found when 2.0 w effective radiated power (erp) power is applied to the reader antenna. moreover, the flexibility and antenna lifetime performance have also been studied and displayed a stable performance."
"4) robust synchronization control (rsc): this algorithm has the same control structure as the other three ones. however, only robust feedback control terms are employed to handle the disturbances and faults in the drhas, and the feedback control gains are the same as those chosen by daftsc."
"3) disturbance-estimation based synchronization control (dsc): this control algorithm is similar to daftsc but without fault tolerance capability, and the value of the parameterθ i in observer (32) is set to zero by default. the other parameters setting in the algorithm are the same as those in daftsc."
"the remaining of the paper is organized as follows: a nonlinear model for the drhas and a formulation of the control problem are given in section ii. the internal leakage faults estimation, the eso-based disturbances estimation, the controller design and the stability analysis for the closedloop control system are presented in section iii. comparative simulation studies are shown in section iv. finally, a conclusion is given in section v."
two linear esos which are denoted as dob11 and dob21 are constructed to estimate the unmatched disturbance in ha1 and ha2 respectively. define an extended state vector as
"where c ii is the normal internal leakage coefficient, and c ti is the unknown fault internal leakage coefficient need to be estimated. according to (1)- (10), define the state variables as"
suppose the drhas is working on a/a mode and the outputs of the two has are summed by force. then the dynamics of the control surface can be represented as
"in this section, a daftsc algorithm is proposed to fulfill the aforementioned control objectives. the magnitude of the internal leakage fault in each ha channel is obtained by online adaptation, and the matched and unmatched disturbances in the system are estimated by constructing four linear esos [cit] through full state feedback. a nonlinear controller employing the estimated information is synthesized by backstepping technology. a structure diagram of the proposed algorithm is illustrated in figure 2 ."
"in this paper, a fault-tolerant synchronization control algorithm based on adaptive backstepping technology and disturbances estimation is proposed to handle the control problem of a drhas on a/a mode in the face of internal leakage faults, large disturbances and force fighting problem. to achieve positon tracking and force outputs synchronization simultaneously, two reference trajectories are introduced and a novel nonlinear model of the drhas is developed. based on the backstepping method, a nonlinear controller incorporating the adaptive control and the observer-based disturbance rejection control is proposed. in which, the internal leakage faults are accommodated by a simple reconfiguration strategy based on faulty parameter online adaptation, and the matched and unmatched disturbances are estimated by four esos and are compensated in a feedforward way. comparative simulation results finally demonstrate the effectiveness of the proposed control algorithm."
"where α 2a is a model compensation term,α 2s functions as a nominal stabilizing feedback, and the feedback gain matrix 22 ) is a diagonal positive definite matrix. substituting (39), (40) into (38), then we havė"
"during the fault-transient time, the position tracking performance degradation can be further aggravated by large disturbances, especially for the system without disturbance compensation. as shown in figure 3 and table 2, under the same fault condition, the maximum position tracking error for rsc can reach 0.0076 rad, larger than the other three algorithms. this means that the algorithm with only robust feedback control design cannot accommodate the faults and large disturbances anymore. in comparison, by utilizing the disturbance compensation design, the algorithms daftsc and dsc can suppress the deterioration in position tracking performance more effectively than the other algorithms."
"the redundant hydraulic actuation systems (rhas) have been universally adopted to drive the primary control surfaces of the modern airplanes. for example, both ailerons of the airbus a320 are driven by a dual redundant hydraulic actuation system (drhas), and the rudder of the plane is driven by a triply redundant hydraulic actuation system (trhas) [cit] ."
"the structure of a drhas operating on a/a mode is shown in figure 1 . two has powered by different hydraulic power sources (marked in blue and green, respectively) are connected to a common control surface, and each ha mainly consists of an electro-hydraulic servovalve and a single-rod hydraulic cylinder."
"can guarantee that all the closed-loop system signals are bounded, the fault estimation error, the disturbance estimation error and the system tracking error converge to zero asymptotically if the following matrix is positive definite by selecting suitable feedback gain matrices k 1, k 2, k 3 and proper positive design parameters ω 2, ω 3, µ 1 and µ 2 ."
"3) the matched and unmatched disturbances in both ha channels are estimated by constructing four linear esos, and the effects of the disturbances on position tracking performance of the system are greatly attenuated by employing a feedforward compensation strategy. 4) a daftsc controller which incorporates adaptive control and eso-based adrc into backstepping design is proposed to deal with the control problem of the drhas in the present of internal leakage faults, force fighting and large disturbances."
"remark 7: according to equations (74), (75), (7) and (8), it can be observed that the matched disturbances q 1 and q 2 inserted in this simulation test vary with the changes of the chamber pressures p hi1 and p hi2 . since the internal leakage faults can cause significant changes in chamber pressures of the has, the amplitudes of the matched disturbances will change after the occurrence of the faults."
"after experiencing a fault transient time, the faulty system enters into the steady state. during this time period, the position tracking accuracy of the closed-loop system is mainly affected by the faults. therefore, the algorithms with fault estimation and compensation ability exhibit better position tracking performance. according to table 2, the average position tracking errors of the algorithms daftsc and aftsc are 5.2185e − 005 rad and 4.7411e −004 rad respectively, less than the other two algorithms. moreover, the daftsc algorithm outperforms the aftsc algorithm for all positon tracking performance indexes due to its disturbance rejection ability. as shown in table 3, similar results can be found in the aspect of the synchronization performance of the faulty system."
"in practice, the positon tracking accuracy of the rhas is affected not only by faults or force fighting between ha channels but also by matched and unmatched disturbances, such as the modeling error of the pressure dynamics and the unmolded load force of the cylinder dynamics."
"since the proposed control algorithm is designed based on full-state feedback, future study will focus on the development of an output-feedback ftsc algorithm for the drhas under the condition that only the displacement signals in both has are measurable. moreover, how to design a control scheme to handle the fault-tolerant synchronization problem for the drhas in the present of time-varying disturbances to achieve asymptotic stability needs to be investigated."
"remark 4: it should be noted that the esos in (27) are driven by position signals rather than by velocity signals, which can greatly reduce the effect of the measurement noise in the disturbance estimations."
"therefore, for the drhas (21) subject to internal leakage faults θ, matched disturbance h q and unmatched disturbance h d, the aforementioned control objectives can be transformed into the following new ones:"
"a lyapunov function v is defined as (64). substituting (37), (41), (43), (45), (54), (59), (66) and (68) into (64), the time derivative of v can be expressed bẏ"
"by employing the control strategies mentioned above, the force fighting problem was proved to be reduced effectively. however, most of the existing results have neglected the impact of faults on the performance of the rhas. a fault with greatest concerns is the internal leakage in hydraulic cylinder, which can impair the system performance or even make the system unstable if they are not compensated properly [cit] . on the other hand, the internal leakage faults occurring in one or more ha channels can cause changes on dynamic response of actuators and aggravate the non-synchronous force outputs, which eventually results in more serious force fighting problem. since internal leakage faults are inevitable, it is very meaningful to develop a control scheme that takes into account both control problems, i.e., the force equalization control and the fault-tolerant control (ftc)."
"then, by introducing the linear transformation t for the first two equations of (13), a new state-space model for the drhas can be obtained as"
"the estimation results of the matched and unmatched disturbances in two ha channels for the algorithms dsc and daftsc are presented in figure 6 and figure 7, respectively. according to (21), the faults appear in the same equation as the matched disturbances and thus mainly affect the estimation accuracy of the matched disturbances. as shown in the second and fourth subgraphs of figure 6, the matched disturbance estimations of the dsc algorithm deviate from their true values immediately after the occurrence of the faults, for the disturbance observers (32) employed in this algorithm do not have parameters updating online. in comparison, the disturbance estimations of the daftsc algorithm gradually converge to their real values with the convergence of the faulty parameter estimations."
"to evaluate the control performance of the proposed algorithm in the face of internal leakage faults, large disturbances and force fighting problem, the following four algorithms with similar control structure (shown as figure 2 2) adaptive fault-tolerant synchronization control (aftsc): this control algorithm is similar to the proposed daftsc algorithm but without consideration of disturbance compensation. to achieve a fair comparison between these two algorithms, the parameters of the control algorithm, the parameters for the reference trajectory x r1 design and the fault adaptation rates are the same as those chosen by daftsc."
"the associate editor coordinating the review of this manuscript and approving it for publication was jianyong yao. in a typical rhas configuration, several parallelly connected hydraulic actuators (ha) operate on a/a mode to deflect the control surface, and their outputs are summed by force [cit] . a major issue that affects the position tracking accuracy of such system is the force fighting between ha channels, namely the has output non-synchronous forces due to reasons such as manufacturing tolerances and individual nonlinear property, and then they fight against each other to find an equilibrium point to drive the common control surface [cit] . to reduce force fighting and realize highaccuracy positon tracking control for rhas, some force equalization control methods such as pressure differential equalization control [cit], decoupling control [cit], and motion state synchronization control [cit] have been proposed in recent years."
"to achieve the control objectives, based on the system model (21), a daftsc controller is presented in this section. the design procedure of the controller is presented as follows:"
"similar results can be found in the aspect of the synchronization performance of the control system. note that the magnitudes of the faults in two ha channels are not the same and this may lead to a more serious force fighting problem. moreover, the existence of large disturbances will make the problem worse. therefore, the algorithm which incorporates faults and disturbances compensation will improve the synchronization performance of the system. as shown in figure 4 and table 3, during the fault-transient time, the maximum fighting force for daftsc is 4.1716e+003n, the smallest of the four algorithms."
"2) synchronized force outputs for has can be maintained to reduce the force fighting effectively no matter in faultfree or faulty cases. to achieve the above control objectives and simplify the nonlinear controller design, two reference trajectories for the controller are introduced [cit] ."
"for the drhas (13) and (16), the goal of this paper is to design a fault-tolerant synchronization control scheme to ensure that 1) the output control surface deflection angle θ d can track the reference trajectory θ r accurately, especially in the present of internal leakage faults, matched and unmatched disturbances."
"although, color histogram is robust against small camera motion, succeeds in redundancy elimination, and its computational cost is very low due to its simplicity, it is sensitive to noise."
"in this paper, a suitable multi-source inverter topology for hess was proposed. the main advantage of this topology is that it does not add any additional stages between the grid/motor and battery. this novel multisource connection results in improved power demand fulfillment of the load, thereby improving the efficiency of electric vehicles. also, with a multi-source inverter, smooth current sharing and lower average currents are achieved. on the other hand, the battery can directly drive a motor without any boost operation, as is done with a dc/dc converter, thereby reducing the overall cost of the converter and also increasing the ems efficiency. active power and energy sharing between multiple sources is possible during dynamic load demands obtained using a svpwm-based control strategy, which improves the stability of the load as an induction motor. finally, the performance of a multisource inverter topology is studied with a scaled-down prototype. from experimentation, it is observed that with multisource topology, a higher driving range is achieved with a reduction in the average current of 27% compared to that drawn from a battery during conventional modes of ems control with greater thermal stability, a reduction in overall size, and an enhancement of the life time of an energy storage system. author contributions: y.m. wrote the original draft; presented the methodology, simulation, and hardware environment with technical data and practical information to make this study applicable for industry; and investigated the results from a technical point of view. k.v. reviewed and edited the manuscript and also provided supervision."
"in this case ( figure 37 ) the load is fulfilled through individual ultracapacitor, battery and ultracapacitor together, and individual battery for 0.5ms each. for mode-3, the battery current is 0.04 a, for mode-2 it is 1.6 a and for mode-1 it is 2.8 a. the average battery current is 1.48 a. the above waveforms (figure 38 ) depict the load shared by the battery alone, i.e., mode-1 (2.4a), and the load shared when the battery and ultra-capacitor, i.e., mode-2, are used together (1.2 a) . the average battery current obtained with these modes (mode-2 and mode-3) is 1.8 a."
"the drawback of this category is the presence of unimportant event in the video abstract because each segment includes combination of significant and non-significant events [cit] . on the contrary, video summaries are more flexible in organizing keyframes since there is not any restriction on time synchronization compared with video skims that are restricted on sequential display. however, watching a video skim is more interesting and amusing for the user rather than keyframes showing in the video summary (static video abstraction) [cit] ."
"the experimental work was carried out using a small prototype of an energy management system consisting of a battery, ultra-capacitors, a control board, a power supply, and a digital storage oscilloscope with a differential probe. as an ultracapacitor is used to supply power demands of short duration, mode-3 cannot be used independently to meet long-term energy requirements."
"as discussed earlier, vsumm used k-means for clustering frames and selecting keyframes. the number of cluster in vsumm is easily estimated by increasing the cluster counter if the differences of two successive frames (pairwise euclidean distances) is greater than pre-defined threshold."
"the purpose of this work is twofold: firstly to review and discuss various video abstraction techniques applicable in surveillance videos, and secondly to propose a surveillance video abstract technique as pre-processing step in video surveillance indexing and retrieval framework, which can mitigate the aforementioned problems."
"static video abstraction, also called video summary, stillimage abstraction, representative frames, r-frames, or static storyboard, is the simplest way for providing an abstracted video by extracting and selecting the representative frames, keyframes, from original video. whereas, determining the proper and informative keyframes are not easy tasks [cit] . it becomes more problematic in surveillance videos due to the importance of their content, which can be referred as evidence in case of crime occurrence. although, static video abstraction effectively reduces the consuming time for browsing and retrieving surveillance video, it loses some important information in the abstracted video."
"2) feature extraction is the second step of video analyzing module. the feature selection and extraction are critical steps and directly influence the efficiency of the clustering algorithm. feature selection selects dominant characteristic, simple enough, and fast calculated features that describe unstructured data, while feature extraction transform these input data from high-dimensional space into lower dimension. most of the developed video abstraction approaches used more than one feature to cover efficiently the visual content of video frames. on the other hand, using the mixture of global features ( such as color, shape, texture, edge, motion) and local features (like sift and surf) outperforms the performance of the approaches using only global features. however, the perfect features should be calculated fast, robust against noise, and easy to extract. therefore, we extracted three global features including color, texture, and energy features along with speeded up robust features (surf) as a local feature from each pre-sampled frame to enhance the performance of our clustering algorithm."
"in mode-3, fast dynamics of the ultracapacitor is observed. we can use the ultracapacitor as a source with respect to motor connection [cit] . sharing of power during operation is observed with these characteristics (figure 20) . at 0.3 s the load reference is increased, which initializes the power sharing of ultracapacitor and the battery. as per the simulation results, (figure 21 and 22), it is observed that mode-2 reduces the stress on the battery with simultaneous use of an ultracapacitor. validation is carried out with a small prototype using a combination of the battery and ultracapacitor controlled with a novel hybrid-type as per the simulation results, (figure 21 and 22), it is observed that mode-2 reduces the stress on the battery with simultaneous use of an ultracapacitor. validation is carried out with a small prototype using a combination of the battery and ultracapacitor controlled with a novel hybrid-type inverter. in the next section the design, operation, experimentation, and results are elaborated upon."
"where n cf is number of candidate frames, n tf defines total number of frames in original video, and c f is a set of candidate frames f."
"as a case study, a comparative analysis that clarifies the effectiveness of the hybridization of sources is carried out. an energy management system is designed based on the peak power requirements of an electric vehicle. a conventional energy management system consists of batteries that deliver both peak power and energy to the vehicle. the ragone chart gives an idea of the power and energy densities of various energy and power sources. some researchers [cit] carried out a comparative analysis by using a battery alone, an ultracapacitor alone and combining the two sources, i.e., the battery and ultracapacitor. on the basis of different driving cycles, peak power requirements are considered 20% in the entire driving cycle. consider a vehicle as having 150 kw as the maximum power requirement, from which 30 kw is the peak power requirement and 120 kw is the average power requirement."
"case-2: by using latch enable signals, the mode can be changed. this is the proposed mode (figure 36 ) where findings and conclusions are analyzed and interpreted by applying a combination of different sources simultaneously (mode-2, mode-3)."
"each video sequence is composed of various local and global visual features such as color, edge, motion, audio etc. therefore, various video abstraction techniques have been developed to detect video shots (video sequences) by measuring the similarity degree of frames' features within each shot or scene. color and motion are two commonly used features in developing surveillance video abstraction approaches."
"mode-1: slow constant speed in this mode, the converter power is always greater than the demand power, hence only the battery provides power to the three-phase motor through a dc/dc converter, i.e., boost operation, as shown in figure 5 . in this mode (figure 6 ), the demand power is greater than the converter power; therefore, the control switch is forward biased and the battery supplies power directly to the motor. in this mode, a dc/dc converter is not used. in constant-speed mode, the uc voltage is higher than the battery voltage and hence in this mode uc does not absorb or provide power to the three-phase motor [cit] ."
"in mode-3, fast dynamics of the ultracapacitor is observed. we can use the ultracapacitor as a source with respect to motor connection [cit] . sharing of power during operation is observed with these characteristics (figure 20) . at 0.3 s the load reference is increased, which initializes the power sharing of ultracapacitor and the battery."
"in conventional methods the rechargeable battery or uc combination is used. batteries were charged through a plug-in charging system while the vehicle was idle; there is a requirement for large dc/dc converters. however, these converters are expensive and make the system bulky. also, the use of these converters adversely affects the efficiency of the electric vehicle [cit] . figure 2 shows a dc/dc converter, which aims to use a basic combined rechargeable battery and uc to obtain constant voltage and power output and utilizes uc power to meet peak current demand."
"in this design, the combination of a battery and ultracapacitor is directly connected to the load without using a power electronic converter; hence, this design is cheap and compact as compared to other designs because of the absence of a dc/dc converter [cit] . as the charging and discharging dynamics of uc is fast compared to a battery, uc gets discharges in load before battery and it frequently takes a current to maintain the terminal voltage; thus, the battery suffered from frequent charges."
"case-4 this case involves the load being fulfilled through a battery and ultracapacitor together (mode-2), and individual battery (mode-1), for 0.5 ms each. the above waveforms (figure 38 ) depict the load shared by the battery alone, i.e., mode-1 (2.4a), and the load shared when the battery and ultra-capacitor, i.e., mode-2, are used together (1.2 a). the average battery current obtained with these modes (mode-2 and mode-3) is 1.8 a."
"a 12 v battery voltage (vb) with 50 ah is selected with a 12 v, 20 farad ultracapacitor. three resistive loads with time delays are connected for loading purposes and low charge protection is provided with control switches. the results (figure 12 ) are obtained from the simscope tool in matlab along with simulink. the total simulation time is 1000 s."
"case-1: in this case, the battery acts as a source and provides power to the resistive load (mode-1). here, inverter voltage and current for case-1, for svpwm-based outputs, are obtained from the inverter. to reduce the computation time of the controller, a lookup table with 864 states is included; thus, we obtained a two-level inverter output. buffer (74hc244) is used to select the modes."
"this work focuses on the video summary due to its suitability in video indexing, retrieval and browsing on surveillance archives [cit] . in addition, it is also much faster than video skimming since only visual information is used for generating video abstraction."
"energy storage system of plug-in ev as an active power source [cit] : a hybridized source (battery + ultracapacitor) is used to fulfill the dynamic requirements of the motor. the dynamic power demands can be fulfilled using a combination of a battery and an ultracapacitor in an energy storage system of electric vehicle (ev). real power (p) and reactive power (q) can be injected or absorbed from the load. when this vehicle is used as a plug-in vehicle, this phenomenon of power control enhances the smartness of the grid. during ev working, power from an energy storage system is injected into the electrical propulsion system (here a three-phase voltage source inverter-fed motor is used). driving conditions associated with torque reflect on the motor current. the driving cycle is simulated by using load as induction motor drive. energy sharing between the battery and ultracapacitor is observed using the psim software environment."
"in this mode ( figure 6 ), the demand power is greater than the converter power; therefore, the control switch is forward biased and the battery supplies power directly to the motor. in this mode, a dc/dc converter is not used. in constant-speed mode, the uc voltage is higher than the battery voltage and hence in this mode uc does not absorb or provide power to the three-phase motor [cit] ."
"moreover, we have proposed dbsva as a novel static video abstraction approach by considering the limitation of existing approaches. we used an adapted denclue clustering-based method to abstract the video utilizing both global features (color, texture, and energy) and local feature (surf). in the future work, we will evaluate our proposed approach performing various experiments and conduct some comparison with similar approaches using benchmark dataset."
"this simulation shows p-q control in load (induction motor). considering inductive load, the power factor of the motor is lagging. the simulation results indicate the active and reactive power requirements of the load."
"dynamic video abstraction, also known as moving-image abstraction, moving storyboard, summary sequence or video skimming, consists of a set of video segments along with their corresponding audio and text information [cit] . in fact, video skimming is a brief representation of the original video by optimally eliminating both spatial and temporal redundancies while preserving the main information [cit] . the most important issue in video skimming is to select the representative segments that appropriately concatenated with its proper audio length considering the corresponding boundaries of spoken sentence."
"from the waveform (figures 17 and 18), it is evident that the ultracapacitor gets discharged in load at 0.3 s. current stress on the battery is shared by the ultracapacitor. reference currents in figure 19 generated from the abc reference frame are converted to a dq reference frame transformation, which decides the power sharing. the ultracapacitor and battery both provide power to the load (induction motor/grid) during the simulation period. reference currents in figure 19 generated from the abc reference frame are converted to a dq reference frame transformation, which decides the power sharing. the ultracapacitor and battery both provide power to the load (induction motor/grid) during the simulation period."
"26. operation of circuit under mode-3. the hardware consists of the buffer ic (74hc244) to control modes with an adjustment of the duty cycle using the enable signal. en1 is used to control upper switches su1, su2, su3. en2 controls sl1, sl2, sl3, whereas en1* and en2* are used to control su11, su12, su13, and su21, su22, su23, respectively. a low state (digital zero) of latch allows input at latch output"
"sabbar and their colleagues applied an agglomerative hierarchical clustering in each segmented shots to extract keyframes [cit] . in their approach, numbers of keyframes depended on movements and variations within each shot. they estimated the local motion in frames using the co-occurrence matrix in order to compute the dissimilarity among frames and to consider the motion that exist in the shot. the complexity of their approach is quite high and this makes it too slow for bigsized video."
"this is an appropriate technique to apply in dynamic environment especially where camera is fixed and background scene is also static (does not change). therefore, the identification of moving object's behavior is the key element to abstract a video. trajectory-based techniques enable to simultaneously eliminate both spatial and temporal video frames redundancy. objects trajectories are done by tracking the moving objects, and obtaining the temporal history of their movements over time. essentially, the keyframes are extracted by analyzing these trajectories and detecting their critical points, which describe the related object behaviors [cit] ."
"in conventional methods the rechargeable battery or uc combination is used. batteries were charged through a plug-in charging system while the vehicle was idle; there is a requirement for large dc/dc converters. however, these converters are expensive and make the system bulky. also, the use of these converters adversely affects the efficiency of the electric vehicle [cit] . figure 2 shows a dc/dc converter, which aims to use a basic combined rechargeable battery and uc to obtain constant voltage and power output and utilizes uc power to meet peak current demand. in this design, the combination of a battery and ultracapacitor is directly connected to the load without using a power electronic converter; hence, this design is cheap and compact as compared to other designs because of the absence of a dc/dc converter [cit] . as the charging and discharging dynamics of uc is fast compared to a battery, uc gets discharges in load before battery and it frequently takes a current to maintain the terminal voltage; thus, the battery suffered from frequent charges."
"in this mode, the converter power is always greater than the demand power, hence only the battery provides power to the three-phase motor through a dc/dc converter, i.e., boost operation, as shown in figure 5 ."
"method is the easiest and practically basic version of the clustering by grouping each object (frame) from a given data set (video/shot) into k partitions (clusters) such that each cluster should have at least one object. the clusters are organized by satisfying partitioning criterion like dissimilarity function on the basis of distance, in which each object of a cluster is similar to other objects and dissimilar to objects of other cluster. k-means and k-medoids are two commonly used partitioning methods [cit] . k-means is an easy and practical method of clustering for grouping keyframes in video abstraction process. its main goal is to partition a set of n samples into k (predefined number) classes/clusters and then selects the closest samples to each cluster's centroid."
"in this case ( figure 37 ) the load is fulfilled through individual ultracapacitor, battery and ultracapacitor together, and individual battery for 0.5ms each. for mode-3, the battery current is 0.04 a, for mode-2 it is 1.6 a and for mode-1 it is 2.8 a. the average battery current is 1.48 a."
"at the inception of acceleration (figure 7 ), the uc voltage is greater than the battery voltage and the demand power is greater than the converter power. therefore, both uc and battery via the converter. as the rate of acceleration remains constant, the uc voltage drops to the same level as the battery voltage; consequently, the battery and uc directly combine through a control switch and provide power to the three-phase motor [cit] . at the inception of acceleration (figure 7 ), the uc voltage is greater than the battery voltage and the demand power is greater than the converter power. therefore, both uc and battery via the converter. as the rate of acceleration remains constant, the uc voltage drops to the same level as the battery voltage; consequently, the battery and uc directly combine through a control switch and provide power to the three-phase motor [cit] . this mode (figure 8 ) is also called regenerative braking because in this mode the uc starts to charge, and the dc/dc converter is in boost operation (or there is no operation). converter operation is totally dependent upon the target uc voltage (vuc__target). at the inception of acceleration (figure 7 ), the uc voltage is greater than the battery voltage and the demand power is greater than the converter power. therefore, both uc and battery via the converter. as the rate of acceleration remains constant, the uc voltage drops to the same level as the battery voltage; consequently, the battery and uc directly combine through a control switch and provide power to the three-phase motor [cit] . this mode (figure 8 ) is also called regenerative braking because in this mode the uc starts to charge, and the dc/dc converter is in boost operation (or there is no operation). converter operation is totally dependent upon the target uc voltage (vuc__target). in continuous decelerating mode ( figure 9 ) the dc/dc converter goes into buck operation, thereby transferring the energy from the uc to the battery [cit] . during regenerative braking, the ultracapacitor recovers energy from the motor in a negligible time on the order of 0.2-3 s. the dc/dc converter transfers this energy recovered by uc with a controlled rate of charging of the battery. at the inception of acceleration ( figure 7 ), the uc voltage is greater than the battery voltage and the demand power is greater than the converter power. therefore, both uc and battery via the converter. as the rate of acceleration remains constant, the uc voltage drops to the same level as the battery voltage; consequently, the battery and uc directly combine through a control switch and provide power to the three-phase motor [cit] . this mode (figure 8 ) is also called regenerative braking because in this mode the uc starts to charge, and the dc/dc converter is in boost operation (or there is no operation). converter operation is totally dependent upon the target uc voltage (vuc__target). at the uc side. the outputs of both these converters are the same as the dc link [cit] . in this design, it is possible to maintain a voltage level below the dc link voltage. the main advantage of this design is that the uc is utilized to the best possible extent. on the other hand, the drawback of this design is that it requires two full bridge converters, thus resulting in an increase in the overall size and cost of ess [cit] ."
"the circuit consists of two separate inverters connected to the battery and ultracapacitor. svpwm (space vector pulse width modulation) is used to control the various circuit parameters. ahess uses a multi-source inverter to connect the battery and ultracapacitor directly to the three-phase smart grid when the vehicle is idle, and the same system connects to the load during prolusion without adding a dc/dc converter. elimination of the dc/dc converter enhances the compactness. the battery life depends on the peak power demand of the load. this peak power requirement is in turn fulfilled by connecting the ultracapacitor along with a battery. here the specific energy demand and the peak power demand for hev are provided by the battery and the ultracapacitor, respectively. in addition, the benefits of this new battery/ultracapacitor topology are a reduction in the size and hence in the overall cost of the energy storage system, along with improved life, endurance, and efficiency of the system. the hess system was studied in depth and analyzed using a psim environment-based simulation, the results of which are discussed below."
"two approaches of texture feature extraction are spatial domain analysis (like tamura feature, standard deviation, mean, and edge histogram) and spectral domain analysis such as gabor filters, wavelet transform, curvelet transform, and cosine transform. spectral approaches are commonly utilized because of their robustness against noise compared with spatial approaches [cit] . therefore, we used haar wavelet transform to compute texture feature, which is fast in computation and practically accurate. we extracted energy because events are main source of energy and indicates changes over a sequence of frames. energy feature was extracted from hue component due to its dominant spectral representation and also human eyes are more sensitive to this component. in addition to the above three global features we extracted one local feature; because local features are more stable than global features [cit] . we used surf descriptor because it can be used in frame clustering due to its invariance to frames intensity, location, scale, rotation and its robustness against affine transformation, which make its representation distinctive."
"hardware is implemented using six igbts, which constitute the three-phase inverter, supplying battery power to the ac link and an ultracapacitor bank connected to ac link through six igbts. thus, a total 12 igbts are used in the implementation."
"the circuit consists of two separate inverters connected to the battery and ultracapacitor. svpwm (space vector pulse width modulation) is used to control the various circuit parameters. ahess uses a multi-source inverter to connect the battery and ultracapacitor directly to the three-phase smart grid when the vehicle is idle, and the same system connects to the load during prolusion without adding a dc/dc converter. elimination of the dc/dc converter enhances the compactness. the battery life depends on the peak power demand of the load. this peak power requirement is in turn fulfilled by connecting the ultracapacitor along with a battery. here the specific energy demand and the peak power demand for hev are provided by the battery and the ultracapacitor, respectively. in addition, the benefits of this new battery/ultracapacitor topology are a reduction in the size and hence in the overall cost of the energy storage system, along with improved life, endurance, and efficiency of the system. the hess system was studied in depth and analyzed using a psim environment-based simulation, the results of which are discussed below."
"the motion-based techniques are inaccurate on videos with high level of motion or motionless. but, they are good candidate to perform accurately in surveillance videos since they are videos with medium level of motion."
"case-1: in this case, the battery acts as a source and provides power to the resistive load (mode-1). here, inverter voltage and current for case-1, for svpwm-based outputs, are obtained from the inverter."
"dbscan gives an arbitrary point (p) groups all points with high-density from p (those points close to p) into a cluster considering two global parameters, euclidean distance of neighborhood of p (eps) and minimum number of points (minpts) in eps-neighborhood of p. those point with lowdensity region (those points lie alone or are too far from p) are retrieved as outlier [cit] . the eps (radius of the cluster) and minpts (the threshold of neighborhood density) can be set either manually or calculated according to the heuristics."
"the hardware (figure 39 ) was tested for different cases via mode-1, mode-2, and mode-3. the hardware (figure 39 ) was tested for different cases via mode-1, mode-2, and mode-3."
"1) pre-sampling is the fist step of video analyzing,which selects some candidate frames for further processing and hence reduces the computational burden. choosing improper sampling rate leads to poor video abstract (low sampling rate) or shorten the length of video abstract (high sampling rate). therefore, we performed our pre-sampling step, to reduce redundancy disregard the video content, as follows:"
"2) hierarchical method aims to construct a hierarchy/tree of clusters that includes objects (frames) based on distance, density or continuity. the agglomerative and divisive are two approaches to form this hierarchy. the advantage of this method over the partitioning method is its independency of the pre-defined clusters number."
"in this design, the combination of a battery and ultracapacitor is directly connected to the load without using a power electronic converter; hence, this design is cheap and compact as compared to other designs because of the absence of a dc/dc converter [cit] . as the charging and discharging dynamics of uc is fast compared to a battery, uc gets discharges in load before battery and it frequently takes a current to maintain the terminal voltage; thus, the battery suffered from frequent charges."
"as per the simulation results, (figures 21 and 22), it is observed that mode-2 reduces the stress on the battery with simultaneous use of an ultracapacitor. validation is carried out with a small prototype using a combination of the battery and ultracapacitor controlled with a novel hybrid-type inverter. in the next section the design, operation, experimentation, and results are elaborated upon."
"basic topologies are implemented using multisource connected to load through a dc/dc converter. in the proposed topology, an inverter-fed ac link is used to connect the battery and the ultracapacitor via the space vector pulse width modulation technique ( figure 11 ). the design reduces the size and weight of the system."
"the most effective, expressive, and simple feature of each frame/image is the color due to its insensitivity and stability against any change in the direction and size [cit] . according to this characteristic many video abstraction techniques used this feature. color histogram has been widely used to generate abstracted video by computing the similarity among frames to select the representative keyframes [cit] ."
"the output with input for case-1 (figure 35 ), where the battery supplies power to the load, is as follows. readings of battery current are noted by a variation in the three phase load with continuous intermittent switching of 10 s. in this case, the total energy requirement is fulfilled by the battery alone. the current drawn from the battery in this case is 4 a for 5 s and 2 a for next 5 s interval, which gives an average battery current of 3 a."
"color and texture are two important characteristics of visual content of each image. color histogram, color moments, color correlation, scalable color descriptor (scd), and color structure descriptor (csd) are some common color feature extraction methods [cit] . we extracted color feature computing color histogram in hsv color space (which is close to human visualization). next, we quantized the calculated histogram to 8 bins for reducing the dimensionality without losing information."
"in hardware implementation, the battery and ultracapacitor drive the load (motor/grid/rl load); we observe the power and energy sharing during dynamic loading situations. the observations are carried out for different combinations of three modes (i.e., mode-1: battery only as a source; mode-2: both sources acting simultaneously, handling the dynamic power requirement situation; and mode-3: ultracapacitor as a source)."
"in continuous decelerating mode ( figure 9 ) the dc/dc converter goes into buck operation, thereby transferring the energy from the uc to the battery [cit] . during regenerative braking, the ultracapacitor recovers energy from the motor in a negligible time on the order of 0.2-3s. the dc/dc converter transfers this energy recovered by uc with a controlled rate of charging of the battery. the battery and uc are connected through separate converters ( figure 10 ); one is current-controlled and connected at the output side of the battery, whereas the other is voltage-controlled and connected at the uc side. the outputs of both these converters are the same as the dc link [cit] . in this design, it is possible to maintain a voltage level below the dc link voltage. the main advantage of this design is that the uc is utilized to the best possible extent. on the other hand, the drawback of this design is that it requires two full bridge converters, thus resulting in an increase in the overall size and cost of ess [cit] ."
"the main purpose of this topology (figure 23 ) is to cascade several dc sources to the three-phase ac motor. in this case, two dc sources, namely the battery (v b ) and ultra-capacitor (v u ), are connected. the main advantage of this topology is that it does not add any additional stages between the motor and the sources, which results in an improved response of electric vehicle by enhancing energy and power demand fulfilment. in the proposed control strategy, the source current is controlled according to the torque requirements of the driving cycle. in a closed-loop system three switching modes are selected according to the power requirements of the grid or the acceleration, cruising, and braking of an electric vehicle [cit] . here the modes are selected with open-loop control and to observe the characteristics during the sharing of sources. there are three operating modes corresponding to the switching states:"
"this design is configured by simply switchingthe positions of the uc and battery in the above design. here ( figure 4 ) the battery is connected to the dc link via a dc/dc bidirectional converter, whereas the uc is connected directly to the dc link. here, the main objective of using the converter is to supply a specific constant power from the battery to the load by controlling the source current. on the other hand, the converter is much smaller than that used in uc/battery design and the main advantage of this design is the wide power range of uc [cit] . in order to understand the operation using battery/uc design, there are four operating modes as follows:"
"in continuous decelerating mode ( figure 9 ) the dc/dc converter goes into buck operation, thereby transferring the energy from the uc to the battery [cit] . during regenerative braking, the ultracapacitor recovers energy from the motor in a negligible time on the order of 0.2-3s. the dc/dc converter transfers this energy recovered by uc with a controlled rate of charging of the battery. the battery and uc are connected through separate converters ( figure 10 ); one is current-controlled and connected at the output side of the battery, whereas the other is voltage-controlled and connected at the uc side. the outputs of both these converters are the same as the dc link [cit] . in this design, it is possible to maintain a voltage level below the dc link voltage. the main advantage of this design is that the uc is utilized to the best possible extent. on the other hand, the drawback of this design is that it requires two full bridge converters, thus resulting in an increase in the overall size and cost of ess [cit] ."
"blink led ( to check controller running ) display \"welcome\" msg and name of project on lcd in hardware implementation, the battery and ultracapacitor drive the load (motor/grid/rl load); we observe the power and energy sharing during dynamic loading situations. the observations are carried out for different combinations of three modes (i.e., mode-1: battery only as a source; mode-2: both sources acting simultaneously, handling the dynamic power requirement situation; and mode-3: ultracapacitor as a source)."
"by adjusting the duty cycle of source utilization, the prototype is tested for different combinations of sources. the duty cycle adjustment provision is incorporated in the hardware using a potentiometer. the duty cycle is set for three different combinations and the average current of the battery is calculated for a constant load. a battery alone and the battery ultra-capacitor combination are used for 0.5 ms throughout the cycle. the battery current for mode-2 is 1.60 a; for mode-3, it is 0.04 a. the average battery current obtained for 1 ms with the two modes concerned was found to be 0.82 a."
"in this paper, a suitable multi-source inverter topology for hess was proposed. the main advantage of this topology is that it does not add any additional stages between the grid/motor and battery. this novel multisource connection results in improved power demand fulfillment of the load, thereby improving the efficiency of electric vehicles. also, with a multi-source inverter, smooth current sharing and lower average currents are achieved. on the other hand, the battery can directly drive a motor without any boost operation, as is done with a dc/dc converter, thereby reducing the overall cost of the converter and also increasing the ems efficiency. active power and energy sharing between multiple sources is possible during dynamic load demands obtained using a svpwm-based control strategy, which improves the stability of the load as an induction motor. finally, the performance of a multisource inverter topology is studied with a scaled-down prototype. from experimentation, it is observed that with multisource topology, a higher driving range is achieved with a reduction in the average current of 27% compared to that drawn from a battery during conventional modes of ems control with greater thermal stability, a reduction in overall size, and an enhancement of the life time of an energy storage system. author contributions: y.m. wrote the original draft; presented the methodology, simulation, and hardware environment with technical data and practical information to make this study applicable for industry; and investigated the results from a technical point of view. k.v. reviewed and edited the manuscript and also provided supervision."
"2) keyframes extraction is responsible to extract keyframes, which are representative frames among all clustered frames. this selection is varied based on the utilized methods. here, we selected the middle core frames from each constructed denclue cluster."
"vscan is a static video abstraction approach which applied color and texture features as two input datasets in dbscan in order to cluster video frames [cit] . this approach utilized color histogram and discrete haar wavelet transforms to compute color and texture features in hsv color space, respectively. vscan generated video abstract based on the following five steps. in the first step, it performed video presampling (one frame per second) to reduce processing cost by eliminating some frames. in the second and third steps, it computed color features using color histogram and texture features applying haar wavelet transform, both in hsv color space. in the fourth step, vscan applied a modified dbscan algorithm to cluster video frames. finally, in the fifth step, it selected the middle core frame of each cluster as keyframes. the drawback of this approach is the possibility of losing the informative frames during pre-sampling stage and the weakness of dbscan in handling high-dimensional feature space and large-scale dataset."
"the switches su1, su2, su3 and su11, su12, su13 enable the battery (vb) to supply the motor (rl load) with discharging ultracapacitor (vu) (figure 25 ). there are three operating modes corresponding to the switching states:"
"topology. the uc ensured proper utilization of energy and also controlled thermal management and peak power demand, thus leading to an increase in the efficiency of the vehicle as a whole [cit] ."
"basic topologies are implemented using multisource connected to load through a dc/dc converter. in the proposed topology, an inverter-fed ac link is used to connect the battery and the ultracapacitor via the space vector pulse width modulation technique ( figure 11 ). the design reduces the size and weight of the system. hardware for the uc/battery design simulated and implemented to validate the design."
"this design is configured by simply switchingthe positions of the uc and battery in the above design. here ( figure 4 ) the battery is connected to the dc link via a dc/dc bidirectional converter, whereas the uc is connected directly to the dc link. here, the main objective of using the converter is to supply a specific constant power from the battery to the load by controlling the source current. on the other hand, the converter is much smaller than that used in uc/battery design and the main advantage of this design is the wide power range of uc [cit] ."
"the tasks of searching an event of interest by using conventional playback device are time-consuming, tedious, and costly. these include the abilities such as backward/forward playing, and access to specific frame or performing desired analyzes within the huge surveillance videos archive. therefore, video abstraction can play crucial rule in this regard by eliminating the redundant and irrelevant information and representing shorter and precise contents of the original surveillance video, which includes all the meaningful and important activities and objects [cit] . moreover, video abstraction is an important technology in video processing especially for efficient and effective retrieval, browsing, and managing video archives. the main goal of video abstraction is to make a short-length and concise summary from a long-period video, while preserving its essential semantic content, in order to give the user a general idea about the content. the two fundamental categories of video abstraction are static and dynamic video abstraction [cit] ."
"this design is more complex than a passive parallel one. in the figure 3 below, the battery is connected directly to a dc link and the uc is connected to the dc link via a dc/dc converter."
"the clustering-based technique partitions a group of frames/shots into several clusters so that the frames/shots within a cluster are very similar to each other in term of characteristic or activity, while having high dissimilarity to the frames/shots of other clusters. afterward, some certain numbers of frames (usually one) in each cluster are selected as keyframes. this technique uses different feature (such as luminance, color histogram, and motion vector) and clustering algorithm (for instance, hierarchical, and k-means) [cit] . the four basic categories of clustering techniques based on their applicability and capability are: partitioning, hierarchical, density-based, and grid-based methods [cit] . here, we discuss only the first three techniques."
"in the meaningful segment selection module firstly, similar frames according to their extracted features and using some criteria for similarity measurement were clustered, and then the representative frames were selected from each cluster."
"surveillance video abstraction facilitates the fast and efficient browsing of video content for the security purpose by generating a brief representation of the original video. however, the effectiveness of video abstraction techniques directly depends on the methods employed for grouping similar frames, the utilized features, and selecting keyframes (static video abstraction) or video segment with related audio (dynamic video abstraction), which are the most informative and representative part of video. our given overview can assist researchers to quickly learn different surveillance video abstraction techniques and choose the appropriate and applicable one according to their requirements."
"techniques used in video abstraction can be classified in six main classes according to their overall process and mechanisms [cit] . these six categories include: (a) featurebased, (b) cluster-based, (c) shot selection-based, (d) eventbased, (e) trajectory-based, and (f) mosaic-based. furthermore, feature-based, cluster-based, and trajectorybased techniques have their own categories [cit] . among these various video abstraction techniques, the most applicable techniques in the surveillance area are feature-based, eventbased, cluster-based, and trajectory-based techniques, which shall be further discussed in this paper. fig. 1 illustrates the hierarchical classification of these techniques."
"conventional battery-based energy management systems are designed for 150 kw, so the system becomes bulky (100 kg). as per the datasheet and the ragone chart, an ultracapacitor provides peak power demand 6800w/kg, but the energy rating is 4.1 wh/kg, so this system (22.05 kg) cannot fulfill the continuous energy demands of a vehicle. as a solution, batteries can be used with an ultracapacitor (hybrid ems) having a high power-delivering capability (6800 w/kg) and energy delivering capability of 4.1 wh/kg. this effectively minimizes the average w/kg of the system by reducing cost, weight with enhancing life (due to reduction in peak currents), and performance of vehicle due to fast dynamics of ultracapacitor [cit] . using the market costs of batteries and ultra-capacitors, a comparative analysis is carried out as follows in tables 2 and 3 . hybrid ems weight and cost decreased by 15.59% and 15.53%, respectively. penetration of the ultra-capacitor leads to a savings in terms of maintenance and ems replacement costs."
"mode-1: slow constant speed in this mode, the converter power is always greater than the demand power, hence only the battery provides power to the three-phase motor through a dc/dc converter, i.e., boost operation, as shown in figure 5 . in this mode (figure 6 ), the demand power is greater than the converter power; therefore, the control switch is forward biased and the battery supplies power directly to the motor. in this mode, a dc/dc converter is not used. in constant-speed mode, the uc voltage is higher than the battery voltage and hence in this mode uc does not absorb or provide power to the three-phase motor [cit] ."
"this simulation shows p-q control in load (induction motor). considering inductive load, the power factor of the motor is lagging. the simulation results indicate the active and reactive power figure 14 . simulation of a hybrid inverter with load (induction motor) [cit] ."
"to understand the charging and discharging behavior of the ultracapacitor, a simulation is carried out in the matlab environment as follows. hardware for the uc/battery design simulated and implemented to validate the design."
"the main feature of this simulation is the use of a space vector analogy using a psim script file. using script control block, control signals are obtained. current and torque feedbacks are taken from the sensors. the two dc sources are designed using an ideal battery and ultracapacitor source. a multisource inverter is designed with igbts. the load is designed by a three-phase induction motor. as the induction motor itself is the balanced load, the phase current of one phase is sensed by the current sensor; the shaft speed is also sensed. the simulation time selected is 2 s."
"case-3: in this case ( figure 37 ) the load is fulfilled through individual ultracapacitor, battery and ultracapacitor together, and individual battery for 0.5ms each. for mode-3, the battery current is 0.04 a, for mode-2 it is 1.6 a and for mode-1 it is 2.8 a. the average battery current is 1.48 a."
"as everyone is aware, fuel sources are dwindling day by day and demand is increasing at a rapid pace, resulting in an exponential rise in prices. in today's scenario, due to the increase in cost and pollution, there is an urgent requirement for attractive and reliable solutions to replace conventional vehicles, which can cater to the needs of society as a whole. keeping this in mind, the manufacturing of electric vehicles is taking rapid strides."
"to validate the dynamic response of the ultracapacitor, the simulation is carried out in a matlab environment wherein the battery is directly connected to the dc link and the ultracapacitors connected to the dc link via a dc/dc converter. to understand the basic analogy of hybridization, a simulation for design 1.2 (a dc/dc converter based on uc/battery design) was chosen. in this design the resistive load is increased gradually over time [cit] . the resistive load comes into the circuit at time steps of 300 s, 400 s, and 700 s. during the dynamic change of the load, the peak load is shared by the ultracapacitor (12 farad) and the battery. the simulation results reveal that the battery discharging starts at 320 s with a small reduction in the battery terminal voltage (v b ) due to the internal resistance of battery; the current obtained is 2 a."
"although, the video abstraction has recently received many attention as an emerging research field, its practical implementation still suffers from many problems due to complexity in its methods [cit] . these problems arose from the following three challenges: first, difficulty in successful analyzing of video content; second, inability to deal with various changes in environmental condition and camera movement; third, failure to accurately handling real-time application. moreover, selecting the proper technique to generate video abstract considering the video domain, especially surveillance video, and its requirement makes the issue more complicated."
"hardware is implemented using six igbts, which constitute the three-phase inverter, supplying battery power to the ac link and an ultracapacitor bank connected to ac link through six igbts. thus, a total 12 igbts are used in the implementation. hardware was implemented (figure 30 ) based on three modes, as stated previously. using express pcb software, the pcb was designed. a single-side pcb was etched and prepared with fixing components (figure 31 ). the microchip processor 18f4520 was used with a control strategy based on the lookup table, with signal generation using the svpwm analogy. digital control signals were developed within six sectors using dqo vectors ( figure 32 and table 5 ). hardware was implemented (figure 30 ) based on three modes, as stated previously. using express pcb software, the pcb was designed. a single-side pcb was etched and prepared with fixing components (figure 31 ). the microchip processor 18f4520 was used with a control strategy based on the lookup table, with signal generation using the svpwm analogy. digital control signals were developed within six sectors using dqo vectors (figure 32 and table 5 ). to reduce the computation time of the controller, a lookup table with 864 states is included; thus, we obtained a two-level inverter output. buffer (74hc244) is used to select the modes."
"the battery (v b )is not used and switches s l1, s l2, s l3, and s u11, s u12, s u13 enable the ultracapacitor (v u ) to supply the motor (rl load) ( figure 26 ). the hardware consists of the buffer ic (74hc244) to control modes with an adjustment of the duty cycle using the enable signal. en1 is used to control upper switches su1, su2, su3. en2 controls sl1, sl2, sl3, whereas en1* and en2* are used to control su11, su12, su13, and su21, su22, su23, respectively. a low state (digital zero) of latch allows input at latch output ( figure 27 ). the simulation is carried out using the proteus design suite environment [cit] . the hardware consists of the buffer ic (74hc244) to control modes with an adjustment of the duty cycle using the enable signal. en1 is used to control upper switches s u1, s u2, s u3 . en2 controls s l1, s l2, s l3, whereas en1* and en2* are used to control s u11, s u12, s u13, and s u21, s u22, s u23, respectively. a low state (digital zero) of latch allows input at latch output ( figure 27 ). the simulation is carried out using the proteus design suite environment [cit] . (figure 27 ). the simulation is carried out using the proteus design suite environment [cit] . mosfet switches are controlled through latch, explained as table 4 . figure 28 . the operation of the circuit is explained with a flowchart (figure 29 ) as follows. the operation of the circuit is explained with a flowchart (figure 29 ) as follows. the operation of the circuit is explained with a flowchart (figure 29 ) as follows."
"basically, there are three types of electric vehicles: battery electric vehicles, hybrid electric vehicles, and plug-in hybrid electric vehicles. out of these, battery electric vehicles are totally powered by electrical energy. on the other hand, hybrid electric vehicles operate on multiple sources, such as an internal combustion engine (ice), fuel cell, or renewable energy source [cit] ."
"in mode-3, fast dynamics of the ultracapacitor is observed. we can use the ultracapacitor as a source with respect to motor connection [cit] . sharing of power during operation is observed with these characteristics (figure 20) . at 0.3 s the load reference is increased, which initializes the power sharing of ultracapacitor and the battery."
"the the output with input for case-1 (figure 35 ), where the battery supplies power to the load, is as follows. readings of battery current are noted by a variation in the three phase load with continuous intermittent switching of 10 s."
"case-2: by using latch enable signals, the mode can be changed. this is the proposed mode (figure 36 ) where findings and conclusions are analyzed and interpreted by applying a combination of different sources simultaneously (mode-2, mode-3)."
"the ragone plot gives an idea about the comparative study of the power and energy density performance of various energy devices. from this plot, it is evident that the batteries have a relatively high energy density but lower power density. on the contrary, the uc has a much lower energy density and a sufficiently higher power density. in addition, the life cycle of the uc is much higher than that of batteries. furthermore, ucs have a better low-temperature performance than batteries. figure 1 shows the ragone plot. hence, the combination of an li-ion battery and a uc was used to get better results. from the ragone chart, we can conclude that the power density of uc is high (6800 w/kg); therefore, the uc fulfills the peak load demand. on the other hand, the energy density of a battery is high (up to 100 to 265wh/kg); therefore, the combination offers better performance as compared to the use of either alone [cit] 13] ."
"as a case study, a comparative analysis that clarifies the effectiveness of the hybridization of sources is carried out. an energy management system is designed based on the peak power requirements of an electric vehicle. a conventional energy management system consists of batteries that deliver both peak power and energy to the vehicle. the ragone chart gives an idea of the power and energy densities of various energy and power sources. some researchers [cit] carried out a comparative analysis by using a battery alone, an ultracapacitor alone and combining the two sources, i.e., the battery and ultracapacitor. on the basis of different driving cycles, peak power requirements are considered 20% in the entire driving cycle. consider a vehicle as having 150 kw as the maximum power requirement, from which 30 kw is the peak power requirement and 120 kw is the average power requirement. conventional battery-based energy management systems are table 1 represents a comparison of various energy storage elements. according to the table, the battery is more suited to provide high specific energy (hse), whereas the ultracapacitor is more suited to provide high specific power (hsp). hence a combination of ultracapacitor and battery gives excellent performance. 1.1. comparative analysis of energy management system (cost, weight)"
"3) density-based method, the two previous methods cluster the object according to their distances. however, they are only able to obtain spherical-shaped cluster and fails to find the clusters of other shapes. the density-based method overcomes this problem using the density (number of frames) notion. this method aims to grow a given cluster while the density of its neighbors is greater than threshold. therefore, this method can successfully discover any arbitrary-shaped cluster and filter noise [cit] . the alternative way to discover the arbitrary-shaped (nonspherical-shaped) clusters is to model the clusters like dense region that are distinct by sparse regions. the three typical algorithms of density-based clustering methods are: density-based spatial clustering of applications with noise (dbscan), denclue, and optics."
"the remainder of the paper is divided into the following four sections. section ii presents a hierarchical classification of different techniques applicable in surveillance video abstraction and subsequently, overview of those techniques is discussed in several subsections. section iii introduces the proposed density-based surveillance video abstraction (dbsva) approach. section iv gives the conclusion as well as the possible future work."
"finally, we merged all extracted keyframes in their temporal order to generate a static video abstract from the given surveillance video. this video abstract can be explored or used for further indexing and retrieval process."
"density-based clustering, we apply an adapted density-based clustering (denclue) algorithm to the extracted features in order to cluster similar frames. denclue is based on kernel density estimation (kde) and it assigns all data objects (frames) with the same local maximum of the estimated density function into the same cluster [cit] . the advantages of denclue over other clustering algorithm are: (i) it can cluster the high-dimensional feature vectors with large amounts of noise, (ii) it is faster than existing clustering algorithm, (iii) it works efficiently on large scale datasets, (iv) its time complexity is o (n log n)."
"basic topologies are implemented using multisource connected to load through a dc/dc converter. in the proposed topology, an inverter-fed ac link is used to connect the battery and the ultracapacitor via the space vector pulse width modulation technique ( figure 11 ). the design reduces the size and weight of the system. hardware for the uc/battery design simulated and implemented to validate the design."
"by adjusting the duty cycle of source utilization, the prototype is tested for different combinations of sources. the duty cycle adjustment provision is incorporated in the hardware using a potentiometer. the duty cycle is set for three different combinations and the average current of the battery is calculated for a constant load. a battery alone and the battery ultra-capacitor combination are used for 0.5 ms throughout the cycle. the battery current for mode-2 is 1.60 a; for mode-3, it is 0.04 a. the average battery current obtained for 1 ms with the two modes concerned was found to be 0.82 a. in this case, the total energy requirement is fulfilled by the battery alone. the current drawn from the battery in this case is 4 a for 5 s and 2 a for next 5 s interval, which gives an average battery current of 3 a."
"similarly, another work used the moving object trajectories to generate a static video abstract [cit] . in this work, each video segment was summarized in one image including background and detected moving object followed by their related trajectories. to generate this image, firstly, they selected the keyframe by taking the last frame of each video segment, and then detected the moving objects and extracted their trajectories (which indicates the essential part of segment). this work fails to successfully generate video abstract from densely crowded scene video. moreover, the final video abstract was generated from a set of images such that each of them represented one unrelated segment."
"most video abstraction approaches have been developed as browsing tools to facilitate viewing and exploring large video. nevertheless, video abstraction can be utilized as a preprocessing stage in video indexing and retrieval applications by reducing the number of video frames which need to be processed [cit] . therefore, we proposed a density-based surveillance video abstraction (dbsva) approach considering this prospect. fig. 2 illustrates the main steps of dbsva approach for generating a static video abstract and the brief descriptions for each step are given bellow."
"in this mode, vb drives the motor/rl load and vu is not used. as the above results show that the magnitude of power is in the positive direction, active and reactive power are injected towards the motor (figure 16 )."
"another simple test we can perform is to see how mi in high dimensional situations handles redundant and noisy variables. specifically, we will look at how the mi changes as we dial up the noise present in redundant variables by randomizing their values with respect to the other variables."
"choosing the same density for fixed k in the joint space causes the k values in the marginal spaces to vary, and hence we arrive at the expression,"
"in investigating the efficacy of such a method, we discovered that it's not very robust. this is mainly due to the fact that like the original method in (3.7), (3.8) is not coordinate invariant in general, and while the volume supplied by redundant variables can in principle be canceled in the denominator of (3.8), the volumes themselves will be computed with respect to spaces of different dimension, and will therefore not exactly cancel. the effect is to still increase (mi) under the influence of redundant variables, which is undesirable since vanilla (ksg) is most successful in this domain. thus while (lnc) fixes one aspect of (ksg), it reduces its efficacy in another aspect."
"since (ksg) uses the l ∞ norm to define the region of uniform probability for the estimate of pi, this automatically presents a problem with coordinate invariance. (ksg) will not even be invariant under linear scalings of the data, let alone arbitrary coordinate transformations 5 . essentially, if one variable, say z, is scaled by a large order of magnitude with respect to the other variables, then the side lengths of the l ∞ box will get chosen to be the length of the kth nearest neighbor in the direction of z. while this will not necessarily cause a problem with the values of ψ(xz + 1), it will cause the counts for the other variables to be much larger than they necessarily should be. as an example, consider the following bivariate normal case, as one can see from the figure, scaling one variable of the figure 1 : comparison of mutual information estimates (ksg) for a bivariate normal distribution before (npeet 1) and after (npeet 2) a linear transformation of one variable by a factor of 10 5 . the second plot shows the difference between (npeet 1) and (npeet 2)."
"where d is the dimension of the space and c d is the volume of the unit d-ball 4 . putting (3.3) into the unbiased estimator one arrives at,"
comparison of mutual information estimates (ksg) for a multivariate normal distribution with equal correlation coefficients ρ ij before (npeet 1) and after (npeet 2) a linear transformation of one variable by a factor of 10. the second plot compares the (ksg) estimators after four variables are multiplied by a factor of 10.
"while this fact is somewhat trivial on its own, when combined with other types of transformations it can be quite powerful. in this paper we will study three main types of transformations, the first being coordinate transformations. the second kind of transformation of interest is marginalization, and the third is products. marginalization is simply the the projecting out of some variables, which according to the design criteria of (mi) [cit], can only ever decrease the correlations present. on the other hand, products of spaces can increase correlations when the new variables provide new information. the most trivial type of product is an embedding, which is discussed in the next section."
"we will access the robustness of the (ksg) estimator and its variants with respect to the three types of transformations outlined in section two (coordinate transformations, redundancy and noise). most tests in this section will use a multivariate normal distribution,"
"where ψ(k) is the digamma function. from here one can determine an approximation for the logarithm of the true distribution by assuming something about the local behavior of p(xi) with respect to the probability mass pi. in the (kl) approximation (and as well in the ksg approximation), it is assumed that the probability within the region defined by pi is uniform with respect to the true distribution at the point xi,"
"as an attempted correction to (ksg)'s problem with using the l ∞ box, s. gao et. al. proposed the local non-uniform correction technique. this technique adjusts the unbiased estimator for (mi) by replacing the l ∞ volume in the joint space with a volume computed from a pca analysis. the basic idea is the following. consider a point xi whose kth-neighbor is x k . with the collection of k+1 points including xi, x k and all points closer than x k, construct the correlation matrix cij and find its eigenvectors. by then projecting each point along the maximal eigenvectors, we can find a pca bounding box, which is rotated and skewed with respect to the l ∞ box. the assumption in this case is that the rotated pca box is a much better representation of the region of uniform probability around xi. once each volume is found, the (mi) is given by,"
"much like in eq. (2.4), (mi) is also invariant under the addition of noise, which are defined as variables, z, that are uncorrelated to both x and y,"
"where s i is the overall score for adaptation problem i, s i, j, k, and w i, j, k are the score and weight assigned for criterion j of problem i by expert k 1 . a higher s i means the higher applicability and potential value of decision analysis. equation (1) is, in essence, an additive multicriteria value function. attention needs to be paid to ensuring that weights indeed reflect priorities of the participants. a more informative analysis would explore how different perspectives (weights from different people) would change the problem ranks, because the implications of different perspectives might actually be more interesting to managers than some hypothetical average. this is an acknowledged principle of good multicriteria decision making [cit] ."
"in addition, high measurable performance means that there exist potential collaborators such as researchers, local governments, or non-governmental organizations. partnerships enlarge the group of experts available for obtaining the data necessary to do a decision analysis."
"however, when applying any of the methods we have described, users should recognize that the quality of the analysis depends on the willingness to consider a wide range of possible actions and scenarios, and by the quality of inputs such as expert advice. therefore, it is essential to draw upon the expertise of individuals from diverse backgrounds and perspectives. when outside experts are available, such as local universities, environmental groups, or consultancies, practitioners should try to involve them."
"adaptation problems passing the six-step screening procedure are likely to benefit from a comprehensive decision analysis involving uncertainties and multiple decision stages. outlining the problem characteristics that increase the value of decision analysis can guide the design of benefit-cost analysis of near-term commitments even when the formal six-step procedure cannot be implemented. we identify three characteristics-fitness, importance, and measurable performance-that can make a comprehensive analysis particularly useful for climate adaptation. each is broken down further, yielding a total of nine specific criteria, which we use below to rank problems. the detailed definitions and rating schemes for the nine criteria are shown in table s1 ."
"in conclusion, decision analysis can provide significant value and new insights for climate adaptation problems. but such analyses take effort that must be justified by the benefits gained. the two complementary procedures we have proposed can assess whether a thorough uncertainty-based decision analysis is needed for a specific adaptation decision and can identify which adaptation decisions can be significantly improved by such an analysis. the two procedures can improve the effectiveness and efficiency of analyses by avoiding wasting time and personnel on problems that are unlikely to benefit from a thorough study. future research should develop guidance concerning which specific methodology of decision analysis-e.g., traditional decision trees [cit], roa [cit], dapp [cit], or rdm [cit] -is most appropriate for particular adaptation decision problems."
"in step 6, we consider adaptive decision making as represented by multiple decision stages and solve it using the decision tree method (fig. 4) . the results indicate that waiting for more information and postponing the decision is the best near-term strategy (\"strategy 4\"). if high damage is observed in the near term ($3200k), then a floodwall should be built in the second stage because high damage early on is an indication that high damage later is more likely. however, if near-term damage turns out to be low, the substations' owner should just do nothing in year 20. waiting, in this situation, is worthwhile even if the substations are unprotected in the near term, as the increased confidence in what will happen in the longterm together with the reduced construction cost will decrease the overall expected cost. with this screening procedure, multi-scenario analysis with adaptive options (type iii) proves to be the most appropriate type of analysis for this substation hardening decision."
"however, planners often face several adaptation problems simultaneously. it is neither practical and necessary to apply the six-step screening procedure to each problem, nor can this procedure readily rank adaptation problems in order of net benefit. with limited analytical resources (budget, personnel, time, etc.), a quicker way to set priorities among problems would be useful. therefore, we now devise a framework to compare multiple adaptation problems in terms of the necessity and value of a thorough decision analysis. we quantify the nine specific criteria outlined in section 3 on a 0-5 scale using expert judgment (table s1) . these ratings can then be used to rank the relative value of comprehensive decision analyses for different adaptation problems; in section 4.2, we illustrate this framework by comparing 12 problems in the chesapeake bay region. this comparison was part of a research planning exercise by the mid-atlantic regional integrated sciences and assessments (marisa) program (www. midatlanticrisa.org)."
"this six-step screening procedure can work well if a manager is focusing on one particular adaptation problem. however, managers have limited resources and time, and so might not be able to apply the procedure to several problems at the same time. for example, emergency preparedness agencies may have responsibility for both coastal and inland flooding hazards at several locations. hence, a practical question is which of several climate adaptation problems might benefit most from a comprehensive analysis. this question can be answered by understanding the characteristics of adaptation problems that make decision analysis useful, even without going through all the above six steps."
"the above discussion explicitly defines the meaning of \"fitness\", \"importance\", and \"measurable performance\" and reasons why each contributes to making decision analysis particularly insightful and useful for climate adaptation problems. in the next section, we present a framework that employs the nine specific criteria to quantitatively compare different adaptation problems in terms of the applicability of decision analysis. we then use the framework to rank twelve adaptation problems in the chesapeake bay region. 4 procedure 2: determining which problems might benefit from comprehensive decision analysis"
"step 2: rank general adaptation problems. when one wants to rank alternatives (here, problems) that differ in many dimensions that are difficult to compare yet all matter, it is widely recognized that, first, multicriteria analysis is a practical and insightful way to compare alternatives and, second, additive value functions are a transparent and relatively simple to apply mca method. practice and the literature [cit] indicate that additive functions are widely used and can effectively deal with multiple objectives that are valued differently by different people. here, users score the problems in terms of section 3's nine criteria and also weigh criteria in terms of relative importance. guidelines for defining scoring metrics should be provided for rating the problems. table s1 shows an example, but users should design their own metrics based on their preferences. involving a range of experts, such as academics, local planners, environmentalists, and engineers, in these assessments can ensure that different perspectives are taken into consideration [cit] . then each of the problems is scored as follows:"
"thus, high sensitivity to climate uncertainty and the possibility of enhancing system flexibility are features that make a climate adaptation problem more likely to benefit from a multi-stage, scenario-based decision framework."
"the intended practitioners are people and organizations who are responsible for city or regional climate adaptation. examples include municipal sustainability offices and regional water management organizations. these practitioners often have significant expertise concerning climate change and adaptation. the initial screening/ranking process can be implemented by making judgments regarding probabilities of climate scenarios and costs and benefits associated with each adaptation alternative under each scenario. such assessments might be approximate, but nonetheless useful for assessing whether comprehensive analyses are likely to be insightful and useful."
"we present a tutorial example to illustrate how the six-step screening procedure can be used to identify the most appropriate type of decision analysis for a specific adaptation decision problem. we create this example for the purpose of demonstration, progressing through all six steps of the process. costs and probabilities are meant to be broadly illustrative of what might be encountered. the example is as follows:"
"in our case, we defined a representative set of general adaptation problems in the chesapeake region that encompass the range of issues and decisions faced by resource managers and policy makers. after extensive interviews with managers, researchers, and stakeholders, we identified a large set of specific problems, which we then grouped into coastal and inland flooding, water pollution, and heat impacts. in contrast, for an agency with a specific domain of responsibility, the set of problems might instead include several location-specific instances of one problem type, such as candidate locations for storm surge protection."
"the approach used to solve the multi-stage, multi-scenario problem here is more similar to rule-based roa applications [cit] than applications of classic financial roa [cit] . the decision structures of our approach and rule-based roa are the same, including immediate commitments, uncertainties unfolding over time, and later wait-and-see options. the methods differ in two aspects. first, our folding-back approach identifies the optimal solution based on discretization of decision variables and uncertain outcomes followed by backwards dynamic programming, while rule-based roa applies genetic algorithms [cit] to find optimal thresholds and subsequent choices that define a rule-based strategy. the rules involve thresholds for an uncertain variable such that one alternative is implemented if the variable exceeds the threshold, and another is implemented otherwise. second, our approach applies bayes law to update probabilities to represent learning through time."
"a utility owns a set of coastal electric substations that could sustain damage in the event of flooding, resulting in an extended electrical outage for nearby customers. the utility needs to decide whether to build a floodwall to protect the substation from future storm events whose severity and frequency will possibly intensify with climate change. [cit] . the substation manager's objective is to minimize the expected present worth of floodwall plus flood damage over the next 60 years."
"a second feature is that it is possible to add flexibility in the decision system. flexibility often includes delaying or modifying investments or rules when better climate information is gained [cit] ). in our example, delaying first and deciding later when better information becomes available (step 6 multi-scenario analysis with adaptive options) is the best near-term decision. near-term risks might increase, but there may compensate savings in investment costs as well as better long-run outcomes. two mechanisms can explain the value of flexibility [cit] ). the first is it avoids irreversible investments at the initial stage but reserves the option to expand later if it is necessary. the second is interest savings from delaying investments."
"as that figure shows, the problem \"coastal protection infrastructure\" receives the highest scores for the general characteristics of \"fitness\" and \"importance.\" it also scores second highest for \"measurable performance,\" slightly lower than \"coastal land acquisition.\" by contrast, \"heat resistant pavement\" has relatively low scores in all three dimensions, especially in \"importance.\" viewed together, these rankings also indicate that trade-offs will need to be made. for instance, \"substation hardening\" ranks higher in \"fitness\" than \"green infrastructure investment,\" but ranks lower in \"importance\" and \"measurable performance.\" the overall score eq. (1) is used to rank our 12 problems for relative value of decision analysis. the average weights of the three characteristics are 0.34, 0.35, and 0.31, respectively. it is clear that experts value each characteristic but place slightly more value on \"importance.\""
"for adaptation problems satisfying these assumptions, the screening procedure is as follows: fig. 1 flowchart of the six-step screening procedure (1). base-case analysis with benchmark scenario"
"(1) fitness. by \"fitness,\" we mean that an adaptation problem has features that a comprehensive decision analysis can usefully address. first, the optimal decision is sensitive to future climate where the rankings of alternatives vary depending on which climate scenario occurs. such adaptation problems often consider large and irreversible near-term commitments, span a long planning horizon, and involve alternatives whose long-run performance is unclear due to fundamental climate uncertainties [cit] ). in section 2.2's example, severity of future flooding depends on future climate states, which are difficult to predict when making initial decisions. the optimal decision changes when we consider different climate scenarios (step 1 base-case analysis with benchmark scenario and step 2 analysis with extreme scenario). if only expected climate change is considered, the \"flaw of averages\" could result in poorer expected performance than an optimal strategy that considers risk (step 4 multi-scenario analysis without adaptive options)."
"the 40 cases were grouped into 12 general adaptation decision problems (table 2) for which climate risks are relevant, which were further aggregated into four broad categories of cbw management concerns: natural resources management, infrastructure management, land use protection, and public health and safety. classifications of the 40 case studies and 12 general problems by risk type and broad decision concern are displayed in table s2 and s3, respectively."
"in the final step, users assess whether considering the ability to adapt the plan at a later stage could significantly enhance its performance [cit] . for example, they might compare the consequences of acting right now versus delaying a decision until better information is available, or they might consider investing in an initial modest level of an adaptation strategy and then augmenting it later if risks increase. if the option value of waiting or building in flexibility is potentially significant, then a multi-scenario analysis with adaptive options is preferred for this problem. such an analysis could use either decision trees or real-options analysis, as discussed in \""
"in section 2, we looked at one adaptation problem and provided a quantitative approach to help select the type of decision analysis that could be most useful. this procedure can help planners who are focused on a specific adaptation problem."
"(3) measurable performance. by \"measurable performance,\" we mean that how well adaptation strategies perform under various climate scenarios can be meaningfully quantified. in our example, construction costs are relatively easy to estimate, and we might rely on existing studies of the extent of storm surge under different climate scenarios. availability of such studies or expertise increases the insights and trustworthiness of comparisons of alternatives."
"when rankings of near-term commitments would not be altered by a more sophisticated multi-scenario analysis (type ii or iii), adaptation managers can address just one or a few climate scenarios, and not consider the complexities resulting from including later adaptation possibilities. however, if rankings depend on which climate scenario is modeled or whether the flexibility to change course later is considered, then a more complex decision analysis is justifiable. managers should then consult with climate change experts and officials about the relative likelihoods of various climate scenarios and how they might affect the performance (cost, risk, or environmental impact) of alternatives. although precise probabilities are not required to make informed decisions about adaptation [cit], some sense of the plausibility and relative likelihood of scenarios is useful for obtaining more precise conclusions about which alternatives may have the highest net benefits. an advantage of undertaking a type iii analysis of multistage adaptation options under risk is that it will focus a manager's attention on how flexibility can be built into a plan so that it can be modified in response to climate and social developments, as information improves. for instance, options can be preserved by modular infrastructure designs or delayed decisions. the possibility of acquiring more information can then be valued. this is the basic philosophy of adaptive environmental management [cit] ."
"we create a hypothetical dataset for this example (table 1), [cit] . we use decision trees [cit] to illustrate the calculations made in each of the six steps. in a decision tree, decision nodes are denoted by squares, and uncertainty nodes as circles, with time proceeding from left to right. the performance of a particular sequence of decisions and uncertain outcomes is shown as the cost value at the end of a branch."
"this screening analysis only requires approximate judgments by the users, which might be highly preliminary but yet still reflective of the relative magnitudes of strategies' performance (cost, benefits) and the relative likelihood of climate scenarios. this is much less effort than needed in a full analysis that would have to be documented and subjected to public review. of course, the recommendations of a thorough analysis might differ in some ways from those resulting from approximate six-step screening analysis, but the latter can still provide insights regarding whether the thorough analysis might be worth doing. in a full analysis, the best available information concerning possible climate scenarios and their consequences should be used, based on modeling, historical patterns, or expert judgment. relative likelihoods of different scenarios can be provided by experts informed, for instance, by regional climate impact analyses [cit] and should be subjected to sensitivity analysis."
"we do not introduce new decision analysis tools in this paper, but rather new approaches for choosing an appropriately sophisticated tool. to our knowledge, this is the first paper that addresses trade-offs between the costs (e.g., personnel and time) and benefits (e.g., usefulness of insights) of simple deterministic adaptation planning versus more comprehensive analyses incorporating climate scenarios and future options."
"we calculate the total score for each problem, and show the overall rankings in table 2 . consistent with fig. 5, two coastal protection problems (involving construction or land acquisition) have the strongest likelihood to benefit from a comprehensive decision analysis. this type of problem is a good fit for decision analysis because it has a long-term planning horizon, and the performance of near-term alternatives is sensitive to climate change. the region has long, densely populated coastlines, so protecting coastal areas from flooding and inundation is highly important. moreover, numerous organizations such as the maryland dnr are working on coastal protection projects in the cbw, providing opportunities for collaboration, which addresses the characteristic of measurable performance. similarly, \"green infrastructure investment\" possesses those three characteristics as well, but instead involves investments to reduce urban stormwater runoff and nonpoint pollution, both of which could be affected by increased storm severity. the six experts assigned different weights to the nine characteristics, and so their specific rankings of problems also differ, as indicated in table s4 . however, there is broad agreement on the most and least suitable problems. for instance, four experts identified \"coastal protection infrastructure\" as the most suitable problem and the other two identified it as second most suitable. in order to further test the sensitivity of rankings to the weights, we randomly changed the weights that each expert assigned. in particular, for each weight, we independently selected a weight from a uniform distribution [0.5x, 2x], where the original weight is x. we repeated the sampling 100 times. we then checked if ranks fluctuated drastically by calculating the standard deviation of each decision's rank over the 100 repetitions (table s4) . overall, sensitivities are fairly low, with the standard deviations less than 1.5 in nearly all cases and with only one above 2.0. this suggests that different weights do not drastically change general conclusions about which problems are most suitable."
"first, we propose a six-step screening procedure in section 2 to identify which of three types of decision analyses is best suited for a particular adaptation problem. a predict-then-act analysis (type i) will be recommended when uncertainty can be disregarded without deterioration in expected performance. meanwhile, we suggest a more thorough analysis (types ii or iii) for situations in which explicitly considering uncertainty and adaptation options is essential for comparing near-term strategies. the aim of the procedures is to avoid investing too much effort on analyzing problems whose decisions would be unlikely to significantly change after more in-depth study. we provide a tutorial example to illustrate how this screening procedure can be applied to a real-world adaptation problem. then in section 3, we introduce three characteristics-\"fitness\", \"importance\", and \"measurable performance\"-that contribute to making comprehensive decision analysis valuable for adaptation. based on those characteristics, we propose a framework in section 4 for evaluating and ranking multiple adaptation problems in terms of which would benefit most from a comprehensive decision analysis. as a practical application, climate adaptation problems in the chesapeake bay area are ranked. the logic of our paper is to first help readers understand the principles of analyzing uncertainty in single and multistage decisions with a concrete example and then to introduce a more abstract procedure based upon those concepts. however, in an actual decisionmaking process, practitioners might first use the ranking framework to select problems that are most likely to benefit from a comprehensive analysis, and then apply the six-step screening procedure to find the most appropriate analytic method for each of those problems."
"step 1: identify general adaptation problems in the cbw. from the existing literature and interviews with 24 adaptation managers and experts 2, we identified approximately 40 specific adaptation cases in the cbw. the main climate-related risks are coastal flooding, inland stormwater, and extreme heat. many studies concern flood risks for infrastructure and development since several states have long coastlines; adaptation to increased risk of coastal flooding can involve large investments in repair and upgrades."
"should an organization responsible for managing a particular adaptation problem invest in a sophisticated multi-scenario analysis (e.g., type ii or iii)? if significantly more cost-effective decisions could be achieved by doing so, the answer can be \"yes\". however, if no or slight improvements in decisions would result, a predict-then-act analysis (such as type i) might suffice. for guidance in selecting an appropriate type of analysis, we propose a six-step screening procedure, shown in fig. 1 ."
"(2) importance. by \"importance,\" we mean that an adaptation problem has short-term urgency and its alternatives involve objectives of high concern to stakeholders. in our example, flooding will potentially cause large damages if no adaptation is made, especially in the longterm if severe climate change occurs. recent hurricanes have raised the public visibility of this threat. importance can be gauged by the potential magnitude of regret (step 3 regret check). for example, if the owners of the substation decide to take no action but severe damage takes place, the total costs could be $1000k more than the cost of the optimal strategy (build a floodwall). a thorough decision analysis can identify ways to significantly reduce the potential regret."
"in this procedure, steps 1-3 serve as an initial check on whether the decision is sensitive to the climate conditions and whether the relative performance of the alternatives significantly differs between the base and extreme scenarios. steps 4-6 are more in-depth, and their purpose is to determine whether consideration of multiple scenarios and adaptation options may lead to a different (and better) decision. to demonstrate how to use this screening procedure, we provide a tutorial adaptation problem here. users could apply this same general approach for their own problems. with this screening procedure, users can make a quick decision on which type of analysis would likely provide useful insights. if the potential benefit of a sophisticated decision analysis outweighs its cost, we recommend that it be undertaken. otherwise, a type i analysis may suffice."
"step 1: characterize adaptation problems. the first step is to describe candidate adaptation problems, based on the literature and interviews with experts, managers, and stakeholders. the following information is needed for each problem: (1) what concerns and objectives do managers and stakeholders have?; (2) what types of local hazards or other impacts arise from climate change (e.g., sea level rise, changing precipitation, rising temperature)?; (3) what nearterm adaptation investment or regulatory commitments might be feasible, and what longerterm options exist to modify near-term plans?; and (4) how might uncertainties concerning climate impacts affect the estimated long-term performance of near-term commitments?"
"climate adaptation planning is becoming increasingly urgent. uncertainty is perceived as a major obstacle to assessing and ranking different adaptation strategies, especially when they involve long-lived investments, regulatory reforms, and other difficult to reverse commitments. considering multiple scenarios can reduce the risk of making-and, later, regretting-a suboptimal decision chosen based on just one single scenario. further, recognizing future options can improve plans by quantifying the benefit of flexibility to modify plans later after resolving uncertainty."
"in this example, although we only consider three climate scenarios and two decision stages, different near-term strategies were selected in the six steps. an analysis that only considers a single future state (e.g., expected sea level rise) could lead to a poor decision because of the \"flaw of averages\". a multi-scenario analysis with future options can better inform decisions by considering more information and flexible strategies."
"multiple objectives considering significant social, environmental, and other co-benefits can increase public concern, and therefore may make a decision even more important. in our example, in addition to repair costs, there may be large social impacts if extensive power outages occur. the aesthetic impacts of floodwalls may also matter to the local community. in addition, some adaptation alternatives may yield significant co-benefits. for example, floodwalls might enhance security against sabotage threats."
& from problem to analysis: a screening procedure that selects the most appropriate type of decision analysis for a specific climate adaptation problem; and & from analysis to problem: a procedure that identifies which of several adaptation decisions are most likely to benefit from careful decision analyses.
"decision analysis can help analyze climate uncertainties and future adaptation options, yet the expense of doing such a thorough analysis must be justified. with the six-step screening procedure proposed in section 2, a manager can quickly determine which is the most appropriate type of decision analysis for a particular adaptation problem: (1) predict-then-act analysis (type i), (2) multi-scenario analysis without adaptive options (type ii), or (3) multiscenario analysis with adaptive options (type iii). the screening procedure will help reduce the risk of investing resources in a more elaborate analysis when simpler ones will do. however, adaptation managers might also be responsible for multiple adaptation problems at various locations. for that situation, we propose a two-step prioritization procedure in section 4 to identify adaptation decisions that are most likely to benefit from a comprehensive uncertaintybased decision analysis. with these two complementary procedures, the effectiveness of adaptation management plans can be increased by quantitatively considering whether uncertainties and flexibility can affect decisions concerning near-term commitments to reduce vulnerability."
"adaptation practitioners may, at first, be unsure about which of the three types of analysis is most suited for their problem, or how their decisions could be affected. when facing a specific adaptation problem and deciding how to analyze it, planners need to trade off the complexity and cost of an analysis versus the usefulness of its insights. the expense of thorough analyses (type ii or iii) needs to be justified by the benefits in the form of improved decisions. some adaptation situations might not significantly benefit from these sophisticated analyses, and a simple predict-then-act analysis (type i) might give sufficient insight to justify a near-term decision. sophisticated analyses are not worthwhile if they are unlikely to change near-term decisions. on the other hand, when managers confront multiple adaptation problems (e.g., cities that are concerned with both flooding and heat waves), managers might want to know which of those problems can benefit most from thorough analysis so that they can deploy their limited analytical budget, personnel, and time most effectively. thus, this paper addresses two interrelated questions. first, how can adaptation managers identify the most appropriate type of analysis for a particular problem? second, how can managers screen a large set of adaptation decisions to identify those that are most likely to be improved by comprehensive analysis?"
"the chesapeake bay watershed (cbw) covers 64,000 mi 2 across six states and washington, dc and is home to diverse natural communities and~17 m people. this region confronts many climate-driven risks whose magnitudes and implications are not fully understood. there are numerous public and private sector decisions in which commitments are being considered today whose net benefits could be dramatically affected by climate change. typically, such decisions involve large investments or regulatory commitments that will affect system function well into the future. examples include: investment in infrastructure such as sewerage upgrades in response to cbw nutrient and stormwater mandates; renovation of the conowingo dam for ecosystem restoration or sediment management; utility and transport infrastructure investments in flood-prone areas, such as the anacostia area in washington, dc; and proposals for gray or green coastal infrastructure for reducing shoreline erosion and flooding in areas threatened by sea level rise and increased storm severity. given time and staffing limits, the marisa team needs to focus on adaptation decisions that are most likely to benefit from better climate information and careful risk analysis. we therefore applied our nine-criterion scoring framework to compare twelve general cbw adaptation problems."
"step 2: ranking adaptation problems. we surveyed six experts from the marisa team working in academia (penn state university) or in think tank (rand corporation) with relevant experience on climate adaptation and decision analysis. they rated the problems with scoring metrics we provided (table s1 ). figure 5 shows the ratings for the three general characteristics of fitness, importance, and measurable performance (averaged across experts and constituent criteria)."
"to define a benchmark scenario, users select a single nominal (e.g., expected) set of values for the uncertain climate variables. the most favorable strategy (call it strategy 1) under this scenario is then identified based on the users' choice of performance metrics. (2). analysis with extreme scenario in this step, users define an extreme climate scenario (e.g., worst case), and then assess whether there is some other strategy (say, strategy 2) that is likely to be preferred to strategy 1 if the worst climate scenario occurs. if the answer is \"no\", then the decision is probably insensitive to climate uncertainties and it is unlikely that a more in-depth analysis would significantly affect the decision. the cost of such an analysis might not be justified, and a predict-then-act analysis (type i) is recommended. but if the answer is \"yes\", users would proceed to the next step. (3). regret check \"regret\" is defined here as the difference between the base case strategy's performance and that of the optimal strategy in a given scenario [cit] . here, users need to calculate the regret of strategy 1 in the extreme climate scenario defined in step 2 (i.e., the difference between strategy 1's performance and strategy 2's in that case). if users view such regret as minor, the cost of a comprehensive analysis might not be justified even if it identifies a different strategy. otherwise, users should go to the next step. (4). multi-scenario analysis without adaptive options considering a small but representative set of climate scenarios (often include the most extreme scenarios (worst-and best-case) and/or a situation in between), users need to undertake a simple probabilistic analysis to calculate the expected performance of all strategies. since this is a screening process, users can provide rough initial guesses for the probability of each scenario. if the strategy with the best expected performance (strategy 3) is close to or identical to strategy 1 under a range of probability assumptions, a multi-scenario analysis is likely to generate the same result as a type i analysis. hence, there is no need to invest in a multi-scenario analysis (type ii or iii). otherwise, multiscenario analysis will provide additional insights that are possibly worth the cost. in that case, users also proceed to the next step to decide whether flexibility exists for some alternatives. (5). identification of flexibility users should further assess if flexibility, in the form of future options to modify system design or operations, are available for some or all of the initial possible alternatives considered in the above steps. flexibility can, for instance, enable planners to delay, abandon, expand, or otherwise modify the original plan. if such options exist, users should then undertake step 6 to check whether considering flexibility can further improve the performance of the recommended strategy. if improvements are insignificant, users can then apply type ii multi-scenario analysis without adaptive options. (6). multi-scenario analysis with adaptive options"
"in this section we discuss several details related to the implementation of the codes. the first thing to consider is language interoperability. the developed codes are written in c++, whereas petsc is programmed in c and scalapack and most of the rest of dense libraries are encoded in fortran. combining the three programming languages in a portable way requires a careful design of the interfaces. petsc offers interfaces for c++ and fortran programmers, as well as portable wrappers to lapack subroutines. we have extended this to also wrap subroutines from scalapack and the other libraries."
"to couple the fem with the sbfem, the entries of the matrices in (1) have to be split into the near-and far-fields, which results in the fem equation:"
"the scamat encapsulates all information necessary to handle a scalapack matrix, including the process grid context, the array descriptor, various sizes such as local and global dimensions, block sizes, and leading dimension, as well as the local array to store the elements of the 2-d block cyclic distributed matrix. additionally, two boolean flags are included, one indicating whether the matrix is stored in symmetric format, and a second one to distinguish between storage in the upper or lower triangular parts. table 1 linear algebra operations implemented in the scamat interface."
"the integration of scalapack in a petsc application amounts to providing a mechanism for accessing scalapack matrices in a petsc style. instead of the extensibility mechanism mentioned above, we have adopted a simpler scheme in which a \"quasi-object\" scamat is defined that mimics the behaviour of a mat, but implements only the functionality that is strictly necessary for our application."
so that equation (6) can be rewritten in discrete form as this equation is then transformed using hht-α (with γ parameter of hht-α scheme) into
"in this expression, blocks with subscript \"nn\" contain nodes at the near-field while blocks with subscript \"f f \" comprise nodes at the far-field. the coupling of near-field nodes and far-field nodes is reflected in those blocks marked with the subscripts \"nf \" and \"f n\". vector p b, which acts only at the boundary γ (see figure 1 ), denotes the far-field influence on the near-field, so that the behaviour of the infinite half space can be applied to the fem subdomain as a load. the second part of this sub-structuring approach is the far-field represented by the sbfem. the forces p b at the near-field/far-field interface are given by the convolution integral"
"for the sparse computations we have used petsc [cit], a parallel framework for the numerical solution of problems arising in applications modeled by partial differential equations. it follows an object-oriented design to encapsulate data structures (such as parallel vectors an matrices) and solution algorithms (such as iterative linear solvers and preconditioners)."
"the storage requirements of the algorithm are high, because the number of matrices is considerable. as mentioned before, the input matrices c 1, c 2, c 3 and m are sparse. in [1.1]-[1.4] we shift to dense storage. these operations correspond to equations (23)-(26) (the expression forc 3 has been simplified). sparse storage is recovered only in [1.13] . allm ∞ n need to be kept for later computations. in contrast, matrices in and jn can be computed in-place (overwriting the same memory locations) since they depend only on the previous time step."
"the paper is structured as follows. in section 2 the fem/sbfem coupling is introduced briefly and a more detailed description of the sbfem part follows in section 2.1. the algorithm to solve the sbfem part and its implementation for time domain simulations is discussed in detail in section 3. this section also includes a summary of the libraries we use to implement the software. numerical tests to validate the code and to illustrate its parallel performance are presented in sections 4 and 5, respectively. we wrap up with a brief discussion in section 6."
"in this section we report the parallel performance of the code. the tests have been carried out on a cluster of 28 nodes, each with 2 opteron-240 processors at 1.4 ghz table 4 measured computational time (in seconds) for different number of processors (p) corresponding to the overall computation, the time domain analysis (algorithm 1), the riccati solver (algorithm 3), and the n-th time step (algorithm 5). and 3 [cit] interconnect. we have used one mpi rank per processor, fully populating one node before adding a new one (thus, for example, experiments with 2 processes employ the two cpus in a single node). the parallel performance is analyzed in terms of both strong and weak scaling. for strong scaling, time measurements are done considering a constant problem size and increasing number of processors. for weak scaling, the problem size grows proportionally to the number of processors. table 4 shows the execution times for different number of processors in the solution of problem csm3, described in the previous section. the number of unknowns for this problem in the sbfem analysis is 1,227. to evaluate the parallel performance the analysis has been limited to 100 time steps (only 100 m ∞ n matrices need to be stored). for this small example we cannot expect very good parallel performance, because the local problem size gets too small with increasing number of processes. this eventually leads to a negligible computation time while the overhead of communication is ever increasing."
"roberts [cit] was the first to use the matrix sign function for solving lyapunov equations with stable coefficient matrix a. in particular, roberts shows that, when applied to the newton matrix sign function, this yields the procedure in algorithm 4 for the solution of the lyapunov equation."
"isotropic, homogeneous and fully linear elastic material can be described by three material parameters only: young's modulus e, poisson ratio ν, and density ρ. a semianalytical solution is known for this problem [cit] . this solution is valid for constant loads in time and space, so that the determined displacement is constant and time independent, which yields to a static solution. to make sure that the implemented software is valid for different materials, two sets of parameters are defined."
"here, e denotes the elasticity matrix, and b 1, b 2 are operator matrices that contain the evaluated shape functions and their derivations. introducing equations (12), (13) and (14) in (11), performing the integration by parts, and introducing the element coefficient matrices"
"where m ∞ (t) is the acceleration unit-impulse response matrix, also known as the influence matrix. in order to solve the convolution integral equation (6) in time domain, a piecewise constant acceleration unit impulse response matrix is assumed, i.e.,"
"we will next describe the libraries used for the implementation of the time domain analysis. figure 4 illustrates the main building blocks, with abstraction level increasing from bottom up. as mentioned before, the resulting code is hybrid, integrating sparse and dense computations. the upper dashed line separates libraries whose design philosophy is oriented to sparse computations (petsc) from the rest. all parallel libraries follow the message-pasing programming paradigm, and are built on top of mpi [cit] . some of the software components are really sequential libraries (those below the lower dashed line) that are employed to perform local computations in the parallel algorithms. the arrows in the diagram indicate dependencies. the heart of the hierarchy of libraries is scalapack, since most of the computation involves dense matrices. the interaction between the sparse and dense parts of the code is represented by the dashed arrow between scalapack and petsc. this interface will be described in section 3.6."
"the proposed algorithm exhibits a good numerical behaviour as the numerical test cases evaluated in section 4 feature convergence to the semi-analytical solution, as expected. this demonstrates that, in practice, when run on a moderately-sized cluster, the sbfem method can yield the solution not only to academic benchmarks, but also to real problems."
"in time domain analysis the acceleration unit impulse response matrix m ∞ (t) is required and equation (20) has to be transformed into this domain. the fourier transformation directly relates m ∞ (t) and s ∞ (ω), which is given as"
"by using a substructuring method (see figure 1 ), the soil-structure system is divided into two subdomains. the structure and its foundation as well as parts of the surrounding soil represent the near-field, denoted by ω. the unbounded soil around the near-field can be represented by a far-field for which the radiation condition has to be fulfilled. both fields, near and far, are coupled at the interface γ . the near-field can be easily represented by finite elements, and the far-field by boundary element method (bem) or a scaled boundary finite element method (sbfem) [cit], which yields to a direct coupling of structure and the surrounding unbounded domain. the sbfem is used in the following because it combines the advantages of both fem and bem [cit] . like the fem, the sbfem does not require a fundamental solution and the coefficient matrices are symmetric and can be added to the fem matrices without changing their size. on the other hand, at the boundary the spatial dimension is reduced by one and the radiation conditions are satisfied exactly, as they are in the bem [cit] . another major advantage is that the coefficient matrices can be reused, once they are computed. this allows the user to modify the setup of the near-field, e.g., buildings, infrastructure and loadings, without computing the coefficient matrices again. the only limitation is that the material parameters at the interface γ must not change. the sbfem is also applicable to non-linear analysis. in this case, the coefficient matrices have to be computed during the coupled fem-sbfem computation."
"for better evaluation of the parallel performance of each part of the computation, fig. 10 displays the parallel speedup, which is defined as the ratio between time with one and p processes, representing the gain factor when the code is run with p processors. the plot shows a steady increasing trend of the speedup, although far from the theoretical maximum (line \"theory\" in fig. 10) . here, the maximum achieved speedup for the time domain analysis (algorithm 1) is about 20 with 56 processes."
the coupling of fem and sbfem is done at the common interface of both subdomains with the sbfem part (9) being simply added to the sorted fem part (5):
"compute coefficient matrices in eqn. (15), (16), (17) and (18) compute coefficient matrices in eqn. (23), (24), (25) the consequence is that we need to develop a hybrid sparse-dense code that operates in parallel. this adds a lot of complexity to programming since it requires the integration of numerical libraries of different nature, and the use of the appropriate data structures in each case."
"the numerical model is implemented for time domain analysis so that the displacements are time-dependent. as the coupled fem/sbfem approach fulfills the radiation condition, the nodal displacements become constant after a certain time, whenever constant loads are applied. in those cases, the numerical results approximate the static semi-analytical solution. to show the accuracy of the method we use two different meshing strategies, which are discussed in detail in the following sections."
"the flowchart in figure 3 summarizes the operations required to compute the m ∞ n matrices. the first consideration when attempting to implement this algorithm is whether the sparsity of matrices can be exploited. the short answer is no. the matrices resulting from the sbfem, (15)- (18), are sparse and must be assembled and stored in some sparse format. however, sparsity is lost when the coefficient matrices of the time domain formulation, (23)- (26), are computed. one could think of maintaining matrices such asc 2 in implicit form, with a sparse cholesky factor l. however, even if this was feasible, it is not possible to compute low-rank approximations to the solution of the matrix equations (riccati and lyapunov). consequently, most of the algorithm must be addressed with techniques pertaining to dense linear algebra. fortunately, the resulting matrices m ∞ n are again sparse, enabling their conversion to sparse format. this is crucial for reducing the complexity of subsequent computations related to the coupling of near-field and far-field, as described in section 2."
"from the times corresponding to 1 processor, we can observe that more than 99.5% of the time is spent in the time domain analysis (shaded part in the flowchart of fig. 3, which corresponds to algorithm 1). the computation of the first time step is dominated by the riccati solver (algorithm 3), and the time required for subsequent time steps (algorithm 5) represents the major part of time consumption. this latter time accounts mainly for the convolution integral and the lyapunov solver. the remaining time belonging to the time domain analysis (not shown in table 4 ) is mainly due to the computation of the schur factorization and the coefficient matrices."
"in this section, we survey practical aspects related to the methods and software tools required to implement them. specifically, we concentrate on the algorithm to compute the acceleration unit impulse response matrices m ∞ n and the solvers that are employed for the riccati and lyapunov matrix equations. we assume that the size of relevant problems ranges from moderate to large, thus calling for parallel computing capabilities."
"in terms of dense computations, we can derive a quasi-triangular form of the lyapunov equation (33) by using the (real) schur form. in this particular case, this transformation represents a huge computational saving. (note that the same does not hold for the lyapunov equation that has to be solved at each step of newton iteration for the riccati equation as, there, the coefficient matrix a k changes at every iteration.) we can write equation (33)"
"although the sbfem is used in time domain, its derivation initiates in frequency domain. therefore, in the following we will briefly describe the latter and postpone the presentation in time domain to subsection 2.1.2. for full details on the computational approach to the three dimensional wave propagation, see, e.g., [cit] ."
"the code is structured as a petsc application, in which the dense computations take place at some point, and therefore some calls have to be done to scalapack and related libraries. we now discuss how this integration has been implemented."
"domains or infinite half-spaces, require careful analysis and call for efficient methods in order to model wave-propagation to infinity. this work focuses on the analysis of ssi. whenever vibrations or impulses are emitted to soil, they induce waves traveling through the ground that can provoke structures to vibrate and even to fail. traffic, blasting operations and earthquakes are some of the reasons that generate those kinds of emissions. basically there are two major motivations: in active seismic areas reliable earthquake resistant structures are required, and in our urban society a very important and challenging task is to encapsulate a building from the surrounding emissions to increase its comfort. in both cases it is essential not only to analyze the structure itself but also to take the surrounding soil into account [cit] . analyzing ssi questions is complex since two very different mechanical problems have to be solved: the structure itself and the infinite half-space, which surrounds the domain of interest. usually the engineers have to deal with complex geometries or materials as well. for those complex problem types no analytical or even semianalytical solution is available, which asks for the use of computational models. the structure can be discretized by using standard methods like finite differences or finite elements. in contrast, the infinite half-space cannot be analyzed with those standard methods as they do not fulfill the radiation condition. because the standard methods are neither able to discretize infinite domains nor to satisfy the radiation condition, it seems natural to divide the problem into two separate subproblems."
"in the following, we shortly introduce the fem and sbfem used to model the nearfield and far-field, respectively. the displacement-based finite element method at an arbitrary time step can be written as"
"publications concerning the application of sbfem to large scale or larger scaled problems in time-domain, where the far-field is discretized by only one sub-domain, are not known to the authors. ssi analysis using substructured far-fields have been discussed in the literature [cit] . this work presents a way to realize a coupled fem-sbfem approach for large scale problems without sub-structuring the interface γ, so that no artificial boundaries have to be introduced which would cause invalid physical behavior."
"we have implemented several basic object management functions for scamat, for creation, destruction, duplication and copy, visualization of information and matrix entries, and for naming. also, we have functions to set the symmetry flag and to transform between symmetric and full (general) storage. the linear algebra operations that we employed are listed in table 1 . additionally, we have implemented operations for inserting a petsc mat into a scamat (scamatinsertmat), and to convert a sparse scamat into a regular petsc matrix based on a tolerance for nonzero elements (scamatsparsify)."
the discretization is now fully described. the following section is concerned with the formulation of the sbfem in time domain and the computation of the acceleration unit-impulse response matrices m ∞ (t) which is needed in equation (10).
"both quantities are depicted in figure 11 . for programs with good scaling characteristics the parallel efficiency is constant. in our case, the graphs display close-to-horizontal lines for 3 to 56 processors, which indicates that the developed code will scale well for large-scale applications. the parallel efficiency is much higher when using only 1 or 2 nodes. this is likely due to the non existent or more effective communication while running on one compute node."
first time step. solving the convolution integral of equation (22) leads to a quadratic equation in the unknown matrixm ∞ at the first time step:
"newton's method for the riccati equation is composed of highly-parallel matrixmatrix products, as in the computation of r (x k ) in [3.3] and the coefficient matrix a k in [3.4] . the major computation, however, is the solution of the lyapunov equation in [3.5] . fortunately, a k is stable and, therefore, this equation can be efficiently solved via the matrix-sign function on a parallel architecture, as explained in the next subsection."
"the methodology and results described in this paper show that, in practice, parallel computing can provide an answer to the high computational cost resulting from the application of sbfem to very large scaled engineering problems. the parallel method that we designed to deal with these problems makes extensive use of external numerical libraries for linear algebra computations: due to the nature of the computation, a hybrid sparse-dense program is required, which adds a significant programming complexity as it requires the integration of numerical libraries of different nature, and the use of the appropriate data structures in each case. the proposed solution mixes petsc and scalapack functionality, resulting in an efficient and reasonably scalable code."
"in order to verify the implemented sbfem code, we chose a simple settlement problem, consisting in the settlement of a flexible foundation under constant load. using an infinite half space, the flexible foundation and its surrounding soil can be easily modeled. as already discussed in section 1, the problem has to be split into two separate domains. the near-field is represented by linear finite elements and the far-field by linear scaled boundary finite elements. the fem/sbfem coupling is implemented as illustrated in section 2."
"as mentioned before, the petsc programming paradigm enforces encapsulation of data structures inside opaque objects. this is the case of mat, the class intended for representing matrices in petsc. a mat object may have one of several internal representations, the most common being a parallel compressed sparse row format. the user may replace the internal data structure at run time, since application code uses an abstract interface that is independent from the actual data structure. furthermore, petsc provides simple extensibility mechanisms that allow, e.g., the creation of a new mat format."
"we now detail the main steps of the computation in algorithmic form. the complete time domain analysis is shown in algorithm 1. from the computational viewpoint, the most expensive operations are the computation of the schur decomposition, in [1.9], and the solution of the matrix equations associated with the computation of the first acceleration unit impulse response matrixm ∞ 0, in [1.5], and subsequent time steps, in [1.11] . the matrix equations solvers will be described in detail in subsections 3.2, 3.3 and 3.4. before doing so, we discuss some other important implementation issues."
"other methods that consider the surrounding unbounded domain by a transmitting boundary, which approximates the radiation condition, have been developed during the last years. the viscous boundary condition is one of the simplest transmitting boundary conditions, acting like a dashpot [cit] . other local, arbitrary order absorbing boundary conditions [cit] and several other types of transmitting boundaries (e.g., infinite elements [cit] ) have been proposed, but none of them is able to fulfill the radiation condition exactly."
"then, newton's method for the riccati equation can be formulated as shown in algorithm 3. the basic idea is to improve the current approximate solution x k by a correctionx obtained by solving the lyapunov equation"
"when a basic ssi problem consisting of a geometrically simple foundation and the surrounding space is discretized, the number of degrees of freedom is small. such problems can generally be analyzed using a standard desktop pc. as the size of the foundation, the structure itself, and their complexity grow, the necessary number of degrees of freedom increases as well to yield the same accuracy in the results. handling a large number of degrees of freedom on a standard pc results in over-proportionally increased computation time. more often, this is even impossible due to the lack of sufficient memory. hence, the options for analyzing large ssi problems are either to simplify the model, at the expense of loss in accuracy, or to keep the detailed model and use parallel computing techniques."
"parallel computing combines several computational cores, which can potentially lead to a significant reduction of computation time, enabling the analysis of complex geometries. as the costs for so-called pc clusters have dropped considerably in the past years while computing capabilities have been improved constantly, parallel computation has become more popular and is turning into a state-of-the-art tool to handle large-scale problems. developing efficient parallel codes for matrix computations can be difficult, but much of this programming burden is removed thanks to existing numerical libraries."
"this is a non-linear first order differential equation, which represents the scaled boundary finite element equation in frequency domain for three dimensional elastodynamics. s ∞ (ω) is the unknown dynamic stiffness matrix for the unbounded medium, with the independent frequency ω [cit] ."
"we must note that we are considering a continuous solution to the node capacity adjustment. in practice, the physical resources allow a discrete set of working frequencies, with corresponding processing capacities. this would also ensure that the processing speed does not decrease below a lower threshold, avoiding excessive delay in the case of low load. the unconstrained problem is an idealized variant that would make sense only when the load on the node is not too small."
"on mobile platforms, such as smartphone, it was evident that the hand-shaking problem should be resolved ( figure 10 compares the mean errors of \"m1 and p2\" and \"m2 and p2\"). however, in the previous study [cit], where only the front camera was used, it was required that the head be tracked at a distance, which made accuracy and robustness less than not addressing the hand-shaking problem. at this point, we consider that the previous study [cit] first raised the necessity of the hand-shaking handling but failed to provide an effective solution. in contrast, the proposed hand-shaking handling method allows the head to be tracked at close range and, to accurately measure the hand-shaking noise, attempts to track background features at higher resolution than the previous study [cit] . for this purpose, using both the front and rear cameras was advantageous and necessary."
"the reliability performance for diverse traffic types of thmac is examined for different traffic loads as shown in figure 16 . having reliability constraints, both em and rc traffic achieves 100% pdr in all traffic loads for thmac. the emergency data management as well as contention free polling period for rc traffic causes such better reliability performance. the 10% big data is also successfully delivered to the bc in the gts of the cfp period. due to employing map for rc traffic 802.15.6 also depicts 100% pdr in all the traffic loads. however, the pdr of em traffic in 802.15.6 slightly decreases at high traffic load due to the increased contention with dc traffic during eap. the non-reliability constrained traffic, dc and nr shows a bit poorer reliability performance as the traffic load increases for both thmac and 802.15.6. figure 17 portrays the average energy consumption for both thmac and 802.15.6 in different traffic loads. in all the traffic loads, thmac outruns 802.15.6 in terms of energy consumption. as the higher traffic load results in lower duty cycle of the nodes in thmac, the average energy consumption does not increase with the increase in data rate. moreover, the separate transmission period for diverse traffic types causes lower number of packet retransmissions which also influences in achieving lower energy consumption."
"focusing on solving these challenges will help improve reproducibility and traceability in cell measurements. whilst some areas have advanced with traceability established to the si-e.g., medical physics and genomics operating at organ and genetic levels-traceable measurements at the cellular level are lacking. these are yet important to address healthcare challenges as well as implement traceability requirements in medicine in light of new regulations (e.g., [cit] /746). here key for success is to define appropriate measurands that allow the accurate characterization of cellular behavior and the biological function it underpins (figure 1) . in principle, cell size, shape, and morphology can be made traceable to the meter. the amount of substance can be expressed as cell count for which the coherent si unit is the number 1. the expression of function can also be expressed as the number of specific antigens that might be determined, e.g., by counting fluorescence markers [cit] . cell viability provides another important dimension for measurements at cellular levels, though the viability of an individual cell is an ordinal quantity that cannot be traceable to the si. nonetheless, this does not exclude traceability in a more generic sense, e.g., by using reference materials or defining viability as a limit of dye permeability."
"here, the slot refers to a pcsmaslotlength. figure 6 illustrates the data uploading operation during cap. during cap, a dc traffic source may originate small data or request for slot allocation of big data. as the figure shows, node n1 originates small dc data and node n2 generates a request for big data to the bc. on the other hand, n3 and n4 sources nr traffic. due to smaller ifs and contention window parameters, both n1 and n2 preempts their transmission over n3 and n4. nodes having either dc or nr traffic continue their operation until the cap ends. however, during cap, if any node has traffic other than dc or nr, keeps its radio turned off to save energy as well as to prevent temperature increase. in figure 6, both node n6 and n7 are in sleep state as they possess rc traffic. figure 7 depicts the rc traffic transmission for small data. we employ polling period for rc traffic to ensure the reliability as bc performs coordinated transmission with the sensor nodes. after ifs period, bc starts polling to the sensor nodes. if a node has small rc data it immediately responds with the data that serves as an acknowledgement of the poll packet. bc continues transmitting poll packet to the other nodes. the poll packet also acts as an acknowledgement of the earlier data packets. a node having big rc data also requests for slot allocation to transmit during cfp period. all the other nodes that do not have rc traffic remain in the sleep state. as shown in figure 7, node n5 transmits a small rc data while node n6 appeals for slot allocation for the transmission in the cfp period. node n1 and n3 are in sleep state due to having dc and nr traffic respectively. therefore, after polling to n1, bc waits for a timeout period and then polls to n3 after the timeout occurs, and continues accordingly."
"finally, figure 10 shows the product of the power consumption and the delay for each studied case. as described in the previous sections, our criterion aims at minimizing this product. as can be easily seen, the saving resulting from the application of our policy is always above 10%."
"a good number of studies are found in the literature that introduced mac protocols for wbans. ieee 802.15.6 [cit], with specifications provided for phy and mac layers [cit] . the standard exploited only star topology, where nodes deployed on/in the human body are coordinated by a hub. ieee 802.15.6 adopted a hybrid approach combining both contention based csma/ca as well as contention free tdma like scheme. the superframe is divided into a number of periods including exclusive access phases (eap1 and eap2), random access phases (rap1 and rap2), a managed access phase (map) and a contention access phase (cap). the eaps are utilized for higher priority or emergency traffic, the raps are used for transmitted non recurring traffic. the map usually uses tdma to support contention free transmission such as polling and scheduled allocation. the standard defined a number of user priorities (ups) for diverse traffic types ranging from 0 to 7. the priority value 0 is regarded as the lowest priority while the priority value 7 is the highest priority. for differentiated service provisioning, different contention values are specified for diverse traffic priorities. the standard stimulated the rapid development of wban and there has been an upsurge of research on wban in recent years."
"the growing trend toward implementing networking functionalities by means of software [cit] on general-purpose machines and of making more aggressive use of virtualization-as represented by the paradigms of software defined networking (sdn) [cit] and network functions virtualization (nfv) [cit] -would also not be sufficient in itself to reduce power consumption, unless accompanied by \"green\" optimization and consolidation strategies acting as energy-aware traffic engineering policies at the network level [cit] . at the same time, processing devices inside the network need to be capable of adapting their performance to the changing traffic conditions, by trading off power and quality of service (qos) requirements. among the various techniques that can be adopted to this purpose to implement control policies (cps) in network processing devices, dynamic adaptation ones consist of adapting the processing rate (ar) or of exploiting low power consumption states in idle periods (lpi) [cit] ."
"thmac adopts the model for thermal rise similar to the model as presented in reference [cit] . this model assumes the wban or the part of it is inside the cross section of a tissue, and the cross section is further divided into grids. an in-vivo node is assumed to be located in a grid of fixed length and width. each node in a grid is assumed to have an initial temperature which gradually varies due to the sensor communication."
"in a wban, emergency traffic is treated as the most critical one that needs to be reported urgently. em traffic usually occurs when a node senses some unusual variations in physiological parameters that might happen any time during the superframe. therefore, thmac does not bound the em traffic transmission into a certain period rather than allowing it in any period (i.e., cap, polling, cfp and even sleep period)."
"the biomedical and life sciences industry develop cell therapeutic products (stem cells), materials that can stimulate tissue restoration (scaffolds, implants) and materials able to prevent or reduce infection and biofilm formation (antimicrobials and their carriers). barriers to commercialization for innovative technologies include high costs associated with managing tissue treatments (tissue grafts, cell transplants). measurement needs that derive from these applications include cell assessment as well as interactions of cells with each other through and with their micro-environments or niches provided by scaffolds, culture media and environmental forces [cit] ."
"note that we forced the front and rear cameras to run at the same resolution and at maximum frame rates in this study. therefore, in future, we intend to develop an effective calibration method to rescale and synchronize the signals extracted from two different cameras. the proposed system assumes that users attempt to restrict hand and head movements as much as possible. such movements are a common weakness of head-motion-based systems [cit] . to improve the usability of the proposed system, it would be more effective to allow users to move their hands and head to some extent. however, this requires a method to discriminate between voluntary and involuntary movements (such as a previously proposed method used in a desktop system [cit] ), and this will also be a focus of future work."
"one consideration for establishing a roster of measurands relevant to the elucidation of structure-function relationships in cell behavior is to identify measurements that elucidate the role of subcellular components (e.g., dna, proteins, sugars, lipids) critical to manifesting quality attributes of the cell [cit] ) . indeed, cells utilize compartmentalization, spatial organization, and dynamic geometric and chemical environments, complex signaling pathways, which all define the measurable attributes of the cell. as such cells express properties of emergent behavior-novel properties that arise from a collection of individual constituents that do not themselves exhibit these properties in isolation-an important consideration when extending cellular analysis beyond purely compositional measurements to functional parameters of intact cells."
"the hand-shaking noise can be suppressed by m3; however, the performance was unacceptable, which was the motivation of this study, and will be evaluated in the heart-rate computation results. in (b,c), the hand-shaking-related frequencies of (a) mislead the position of the highest frequency. in (d), by suppressing the hand-shaking noise, the highest frequency is much closer to the true heart rate."
"in this regard, the cawg coordinates the work of nmis and designated institutes (dis) aiming at the development of reference materials based on both eukaryotic and prokaryotic cells. examples include eukaryotic cell reference materials that are designed to support the quantification of blood cells (erythrocytes, leukocytes, thrombocytes) in a blood matrix, e.g., cd4+ [cit] and prokaryotic cell reference materials that target potential impact in the determination of surface material biodegradation for a community of microorganisms, water and food safety."
"considering the effect of thermal rise in a body, a temperature aware probabilistic sleep cycle management scheme has been proposed for wbans [cit] . this study examined three random sleep durations including lognormal, poisson and binomial distributions, and observed the effect of overall temperature rise of the nodes and the achieved throughput. however, the study did not reflect the effect of actually measured temperature on the sleep duration of the nodes that might cause some highly heated nodes to be participated in communication activities. moreover, the study only focused on observing the thermal effect through random sleep duration rather devising a full-fledged mac scheme."
"overall, algorithm 1 shows slightly aggressive behavior in raising the communication period while the temperature increases, and in contrast it adopts conservative approach for the decrease of communication period when the node cools down. this is due to considering the thermal increase as the most crucial factor because of its severe consequences to the tissue damage. however, a long communication period also might disrupt the qos provisioning. therefore, the constant parameters used in this algorithm need to be chosen carefully that could balance between the qos provisioning and thermal raise which has been discussed in section 5.3.1."
"only recently, due to the rise in energy price, the continuous growth of customer population, the increase in broadband access demand, and the expanding number of services being offered by telecoms and providers, has energy efficiency become a high-priority objective also for wired networks and service infrastructures (after having started to be addressed for datacenters and wireless networks)."
"for each vnfs-nodes setting, 30 random input configurations are generated to produce the final average values. in figures 2-10, the labels ee and nee refer to the energyefficient case (quadratic cost) and to the non-energy-efficient one (see (13) and (14)), respectively. instead, the label \"saving (%)\" refers to the saving (in percentage) of the ee case compared to the nee one. when it is negative, the ee case presents higher values than the nee one. finally, the label \"user\" refers to the vnf."
"virtualization represents an efficient and cost-effective strategy to exploit and share physical network resources. in this context, the network embedding problem (nep) has been considered in several recent works [cit] . in particular, the virtual network embedding problem (vnep) consists of finding the mapping between a set of requests for virtual network resources and the available underlying physical infrastructure (the so-called substrate), ensuring that some given performance requirements (on nodes and links) are guaranteed. typical node requirements are computational resources (i.e., cpu) or storage space, whereas links have a limited bandwidth and introduce a delay. it has been shown that this problem is np-hard (it includes as subproblem the multiway separator problem). for this reason, heuristic approaches have been devised [cit] ."
"where n is the number of data samples (the product of data sampling rate and video length). finally, the periodicity of signal s i is computed by the bivariate correlation with its lagged version s i as follows:"
"to construct algorithms with provable convergence to equilibrium points, many approaches consider network models that can be mapped to specially constructed games. among this type of games, potential games use a real-valued function that represents the entire player set to optimize some performance metric [cit] . we mention [cit], who provide an extensive survey on networking games. the models and papers discussed in this reference mostly deal with noncooperative gt, the only exception being a short section focused on bargaining games. finally, [cit] presented an approach based on two-stage noncooperative games for bandwidth allocation, which aims at reducing the complexity of network management and avoiding bandwidth performance problems in a virtualized network environment. the first stage of the game is the bandwidth negotiation, where the sp requests bandwidth from multiple infrastructure providers (inps). each inp decides whether to accept or deny the request when the sp would cause link congestion. the second stage is the bandwidth provisioning game, where different sps compete for the bandwidth capacity of a physical link managed by a single inp."
"chemical shift perturbation is very useful as a simple tool to elucidate macromolecular interactions in smalland medium-sized complexes, including proteinprotein, protein-drugs, and protein-dna/rna interactions [cit], either in solution, or in solid state nmr [cit] . the csp method works best when recording heteronuclear nmr spectra on samples in which the biomolecule, e.g. the protein, is isotopically labelled as to allow for selective detection. fortunately, protein overexpression and isotope labelling, e.g. by 15 n or 13 c, has now become routine in escherichia coli and pichia pastoris, with new and promising developments for expression in higher eukaryotic systems, which guarantees the presence of a more complex folding machinery and post-translational modifications [cit] . together with new developments in nmr technology, e.g. direct 15 n-observation [cit], the csp method will find even more widespread application."
"to evaluate the performance of thmac, we conduct experiments through simulations. this section discusses the thmac performance evaluation including simulation environment, simulation metrics, and the results obtained."
"to address the aforementioned challenges, we propose a thermal-aware duty cycle mac protocol (thmac) for iot healthcare. we intend to present a full-fledged mac scheme, which on the one hand controls the duty cycle of the sensor nodes aiming to reduce thermal rise of the body sensor nodes, and on the other hand attempts to deliver the diverse physiological parameters meeting their respective qos demands. overall, our contributions are as follows:"
"from equation (3), we can find the temperature of a node at grid point (x,y) at time t which is a function of the temperature at (x,y) at time t − 1, and the function of the temperature of surrounding nodes at grid points ((x+1,y), (x,y+1), (x−1,y), and (x,y−1)). the assumption for fixed node positions is valid since the in-vivo nodes are surgically implanted. if we know the tissue properties, the properties of blood flow, and the heat absorbed by the tissue, we can easily estimate the temperature at a given time. figure 2 illustrates the basic superframe structure of thmac. thmac adopts the beacon enabled mode operation due to its flexibility to manage and regulate the superframe structure. the superframe starts with a beacon period where bc broadcasts a beacon frame containing management information which includes initial synchronization with the body sensors such as clock information, superframe length, active period length, duty-cycle information etc. the beacon frame is generated with relatively long interval, denoted as t b, also known as superframe period. following the beacon period, active period starts which is further divided into contention access period (cap), polling period, download period (dl) and contention free period (cfp) as depicted in figure 3 . during cap, nodes employ random access protocol, for example, csma/ca and transmits sensed data to the bc. the polling period exercises the controlled access where the bc deliberately polls certain body sensor nodes to upload their data which also ensures reliable transmission. the dl period is used for transmitting data in the form of broadcast or unicast from the bc to the nodes. cfp, followed by dl contains some guaranteed time slots (gts) to ensure contention free reliable transmission for certain nodes. the slot assignment of the nodes will be notified through a broadcast packet during dl period. being a duty cycle mac protocol, thmac employs a long sleep period where the nodes enter to the deep sleep condition or low power listening mode for energy conservation. usually no transmission is allowed during sleep period but we grant an exception for emergency data transmission as to be discussed later."
"the dl period is used for download data transmission from the bc to the sensor nodes. bc exploits both broadcast and unicast transmission in this period as depicted in figure 8 . we divide the dl period into a number of slots. each slot starts with an ifs period followed by download data from bc. in the whole period, all the nodes are in active state. upon receiving the slot allocation request during cap or polling period, bc notifies the respective nodes regarding the slot allocation as unicast packet. as the figure shows, node n2 and n6 having dc and rc traffic respectively, receive a slot notification packet from bc in response to their request sent during cap and polling period."
"the heart rate has been estimated using electrical or electronic sensors, such as electrocardiogram sensors [cit], oximeters [cit], ultrasound sensors [cit], thermal imaging sensors [cit], and micro-impulse radar [cit] . in addition, commercially available wearable devices, such as watches [cit], rings, gloves [cit], smart chairs and car seats [cit], and chest and ankle belts [cit], have also been used to estimate the heart rate. however, such equipment is not commonly available or not always accessible in daily life. in addition, some equipment can harm to the body [cit], or can cause rashes or discomfort when used for long periods [cit] ."
"in the last few years, power consumption has shown a growing and alarming trend in all industrial sectors, particularly in information and communication technology (ict). public organizations, internet service providers (isps), [cit] s [cit] . the global e-sustainability initiative (gesi) estimated a growth of ict greenhouse gas emissions (in gtco 2 e, gt of co 2 equivalent gases) to 2.3% of global emissions (from 1.3% [cit] ) [cit], if no green network technologies (gnts) would be adopted [cit] . on the other hand, the abatement potential of ict in other industrial sectors is seven times the size of the ict sector's own carbon footprint."
"more specifically, we consider a bank of physical network nodes (in this paper, we also use the terms node and resource to refer to the physical network node) performing tasks on requests submitted by players' population. hence, in this game, the role of the players is represented by vnfs that compete for the processing capacity pool, each by seeking the minimization of an individual cost function. the nodes can dynamically adjust their processing capacity according to the incoming workload (the processing power required by incoming vnf requests) by means of an ar strategy that aims at minimizing the product of energy consumption and processing delay. on the basis of the result of the nodes' ar strategy, the vnfs' resource sharing costs assume a polynomial form in the workloads, which admits a unique ne."
"here, t s is the data sampling period,s i ands i are the sample means. this approach implies that a periodic signal will tend to lag relative to the length of its maximum period."
"some earlier efforts [cit] are also found exploiting the previous ieee 802. 15.4 standard [cit], which was basically designed for the typical wsns. these protocols modified the ieee 802.15.4 structure to achieve mainly energy efficiency and qos provisioning to some extent."
"we have examined the possible effect of introducing simple energy optimization strategies in physical network nodes performing services for a set of vnfs that request their computational power within an nfv environment. the vnfs' strategy profiles for resource sharing have been obtained from the solution of a noncooperative game, where the players are aware of the adaptive behavior of the resources, and aim at minimizing costs that reflect this behavior. we have compared the energy consumption and processing delay with those stemming from a game played with non-energy-aware nodes and vnfs. the results show that the effect on delay is almost negligible, whereas significant power saving can be obtained. in particular, the energy saving is always higher than 18% in every case, with a delay increase always lower than 4%."
analysisassign can be freely downloaded for non-commercial academic usage from http://www.ccpn.ac.uk/ v3-software/downloads. the csp module and tutorials will be included in its upcoming version 3.0 release.
"in this paper, we proposed thmac-a thermal aware duty cycle mac protocol for iot healthcare with the aim of maintaining a bearable temperature level in a body as well as achieving qos provisioning for diverse traffic types. thmac introduces a thermal-aware duty cycle scheme and presents a superframe structure with detailed mac operations to meet the respective qos demand of diverse traffic types. we compared the performance of thmac with that of the ieee 802.15.6, a representative standard for wbans. simulation results show that thmac clearly outperforms ieee 802.15.6 in achieving lower temperature rise as well as keeping the temperature surrounding a tissue below some hotspot level. thmac also achieves to meet the required qos demands of diverse traffic types in terms of latency and reliability. moreover, thmac has better energy efficiency compared to ieee 802.15.6, and achieve fairness in qos provisioning for both small and big data."
"the mission of establishing a strong infrastructure supporting reproducibility and traceability in cell measurements is driven by impact for the end user. a lack of measurement assurance infrastructure-including traceability to the iuis associated with the issues of reproducibility which are of immediate importance to cell metrology. the state of irreproducible results in biological measurements and their impact on industry is well documented. [cit] amgen reported that only six out of 53 preclinical studies could be repeated [cit] . in the subsequent year, global biological standards institute (gbsi) released the \"case for standards\" [cit] ). [cit] action plan that outlines priorities in the development of standards for biological reagents, cell culture, sera validation, cell assays as well as laboratory protocols, many of which are underpinned with the fundamentals of cell metrology ."
"it is the ultimate aim of the molecular biologist to understand cellular functioning in its molecular context. as such, it is imperative to know at which time and place specific biomolecules are active to exert their function. at the root of our understanding, however, is the realisation that the interactions between individual molecules that together form active complexes of sometimes intricate complexity, constitute the underpinning basis of all the biological processes. structural biology is the field of science which aims to describe such interactions between biologically relevant molecules at an atomic level. it is based on the notion that the interactions are facilitated by the specific molecular shapes and, as has nowadays become evident, also their dynamical changes. together these are crucial in determining the affinities that drive the assembly of the macromolecular complexes [cit] ."
"dynamic adaptation techniques in physical network nodes reduce the energy usage, by exploiting the fact that systems do not need to run at peak performance all the time."
"as one of the home or personal healthcare technologies, daily monitoring of vital signs, such as blood pressure and heart rate, is important to prevent and manage various conditions, and patients who have their vital signs monitored are more likely to take control of their health [cit] . imbalance or variation in heart rate may be a symptom of heart-related diseases, which are the primary cause of mortality worldwide [cit] . thus, developing effective daily heart-rate monitoring systems that allow at risk and elderly populations to recognize potential heart-related diseases in early or curable stages is critical."
"in this paper, we propose a touchless, bcg-based heart-rate monitoring system that reliably runs on a smartphone. to improve heart-rate measurement accuracy (in this paper, the system accuracy is represented with errors in measurement), the proposed system uses two cameras (i.e., the front and rear cameras on dual-camera smartphones) and addresses the problems inherent in the previous system [cit] . compared to the previous method, the proposed method realizes the following."
"to address the hand-shaking problem that occurs with smartphones, we propose using both the front and rear cameras available in most smartphones to track background and facial features simultaneously ( figure 4) . by using the rear camera to track only the background features, more features can be tracked robustly. this differs from the previous method [cit] that used restricted rois. in addition, by using the entire frame of the front camera to track only facial features in portrait mode, features can be tracked more robustly at closer range (lower than 20 cm from the camera). note that three issues must be addressed to use both the front and rear cameras. first, to ensure that pixel sizes are approximately equal, both cameras should be set to equal resolution; otherwise, the magnitude of motions in both cameras should be rescaled with calibration. second, both cameras must be synchronized. for soft synchronization, both cameras are set to run at the maximum frame rate (approximately 30 hz), and the data sampling rate is set to a lower frequency (20 hz) than the frame rate of both cameras. thus, the data extracted from both cameras will have the same timestamp. third, using two cameras can reduce operating speed; however, most android devices support multi-threading, which enables several frames to be processed simultaneously. in addition, the proposed system is implemented with a queue-structured frame flow such that processed frames can be released from memory as soon as the calculations are complete. thus, the proposed system's operating speed is maintained and it can run in real time."
"the above deployment scenario forms a star topology as shown in figure 1b, where the body sensors are directly connected to he bc. we denote a body sensor node as n i, and assume each node n i communicates with the bc using fixed transmission power. we further assume the communication links between a n i and bc are symmetric."
"the process flow of the proposed system is shown in figure 3 . except for two processes, i.e., hand-shaking handling using dual cameras and the correlation-based signal periodicity computation for signal selection (which will be explained in detail in the subsequent sections), the process flow is the same as that in previous studies [cit] . the process flow is summarized as follows. first, two camera images are obtained and facial/background features are tracked. second, the hand-shaking-free head motion signal is extracted from the tracked features. third, the signal is enhanced by applying a bandpass filter. fourth, the five most principal components are extracted from the enhanced signal. fifth, the most periodic component signal is selected. finally, the heart rate is computed from the selected signal."
"researchers working in the life sciences sector strive to provide solutions to societal challenges ranging from tissue restoration and generic disorders to cancer and microbial diagnostics. the sector is burgeoning, while the recent progress acknowledges the lack of standards that are necessary to guide diverse stakeholders and foster an environment for adherence [cit] . the national measurement institutes (nmis) invest a concerted effort to create mesurement assurance methods and underpin measurement systems with traceabile standards. this is critical to helping to translate an extensive body of research knowledge into commercial products in this and other industry sectors. improving reproducibility and traceability in measurement results will conform to the competence requirements of bio-measurement laboratories and assure confidence in research [cit] . however, \"irreproducibility\" is symptomatic of far broader challenges in biological measurements that cannot be addressed by an individual method, technique or material [cit] . reference materials, methods, protocols and appropriate documentary standards are necessary [cit] . each of these can address a specific measurand, but to describe a cellular process completely measurements must be validated in a continuum. cross validation between different measurands can mitigate the problem of confounding variables in \"noisy\" cellular environments, which contribute to the notorious complexity of cellular systems [cit] . currently, si units do not cover it fully: based on existing capabilities and regulatory requirements, many measurements are made in arbitrary units that do not necessarily allow for comparison across studies. in addition, biological measurements are often performed to determine nominal properties such as color obtained by gram staining. however, even if one focuses on measurements that could in principle be si traceable, gaps remain in the continuum of characterized cellular properties across length and time scales. filling these gaps, while revealing different sources of uncertainty, is expected to support the provision of a complete cell metrology framework that can ultimately increase confidence in research. furthermore, cell metrology can demonstrate value in emerging areas of global importance including regenerative medicine, infectious disease, eukaryotic and prokaryotic cell therapeutics as well as gene therapies to accelerate the translation of advanced medicinal products."
"in order to evaluate our criterion, we present numerical results deriving from the application of the simplest control policy (cp1), which implies the solution of a completely quadratic problem (corresponding to costs as in (9) for the nep). we compare the resulting allocations, power consumption, and average delays with those stemming from the application (on non-energy-aware nodes) of the algorithm for the minimization of the pure delay function that was developed in [35, proposition 1] . this algorithm is considered a valid reference point in order to provide good validation of our criterion. the corresponding cost of vnf is"
"in this experiment, we evaluate the impact of big data on the latency and pdr performance of small data for both thmac and 802.15.6. as the occurrence of big data is not common in nature for wban, however, its existence might affect the qos performance of small data due to its dominance in the limited active period. figure 18a illustrates the impact of big data on the latency performance of small data for both the protocols. here, we only consider em and dc traffic as they are delay constrained. since thmac allocates a distinct contention free period for delivering big data, increasing its proportion does not disrupt the latency performance for small data. however, with the increasing percentage of big data the latency of small data considerably increases for both em and dc traffic in 802.15.6. this is due to the fact that the big data occupies most of the time in eap period. thus, in spite of having lower contention window value, em traffic does not get the channel access for an already on going big data transmission. for dc traffic, the situation is worse than that of em traffic. the impact of big data on the pdr of small data has been depicted in figure 18b . here, we show the pdr performance of em and rc traffic due to their reliability constraints. the separate cfp for big data in thmac also does not change the pdr performance of small data. however, the proportional increase of big data reduces the pdr of em traffic for 802.15.6. despite exploiting map, the pdr for rc traffic also decreases while the big data percentage is higher. the main reason is the longer occupation time of big data which increases node congestion and small data loss due to buffer overflow."
we examine the effect of different (unconstrained and constrained) forms of the nodes' optimization problem on the equilibrium and compare the power consumption and delay achieved with energy-aware and non-energy-aware strategy profiles.
"molecular therapies constitute an exemplar domain where quantitative intracellular measurements are urgently needed and may have far reaching benefits [cit] . an ability to deliver gene and macromolecular drugs holds promise for the imminent therapeutic control of major diseases including cardiovascular and genetic disorders as well as cancers. therapeutics are available. however, their acceptance and application in clinical settings are hampered by uncertainties in intracellular delivery and the structural inconsistency of delivery vectors. initiatives in industry (e.g., genzyme corp.-erickson's case, novartisled global clinical trials of gene treatments for glioblastoma multiforme) build upon the need for more efficient and quantitative intracellular delivery as well as the scalability of vector production [cit] . quantitative and correlative measurements of intracellular delivery are anticipated to support the systemic assessment of the safety and efficacy of delivery technologies [cit] . such measurements rely on parameterised measurands that are relevant to the delivery vectors themselves [cit] . one can adapt the concept of viral titer as a quantitative measure of the vector in a given volume (number of particles per ml). however, viral load testing is performed using gene amplification techniques that are restricted to the total amount of virus often given in rna or dna copies per ml. these measurements do not reveal a ratio of functional loaded and empty particles. in comparison, non-viral systems are not as monodisperse as viruses and their size is not strictly tailored to the size of genetic cargo they carry. although this allows artificial systems to accommodate different genetic cargo, the particles they form are categorized into loaded, over-loaded and empty particles, the ratio of which is also undefined. with no available measurements that can provide an explicit answer with regards to the ratio of loaded vs. empty particles, vector developers accept broad variations of loading. the efficacy of viral and non-viral vectors is linked to their ability to cross cellular membranes and release the cargo without preventing subsequent functions of gene silencing or expression [cit] . each of these steps has different measurement challenges, whereas an overarching challenge in intracellular delivery is to quantitatively relate the number of gene-delivery particles before and after transfection. current macromolecular drugs that modulate genetic reactions overcome the problems of stability, excretion and uptake by phagocytes, but the lack of concordant results in gene uptake by the target cells as a function of functional and structural inconsistency of delivery vectors remains unsolved 1 . this is why gene therapy technologies have reached a point where quantitative control over macromolecular transfer is necessary for further progress. to better understand factors improving intracellular delivery, measurements for the quantitative assessment of delivery vectors, their uptake to target cells and specificity of targeting are all important but remain largely untapped by the current development in cell metrology."
"in general, the energy-efficient optimization tends to save energy at the expense of a relatively small increase in delay. indeed, as shown in the figures, the saving is positive for the energy consumption and negative for delay. the energy saving is always higher than 18% in every case, while the delay increase is always lower than 4%."
"an exploitable answer to quantitative gene and drug delivery, at least in vitro, can be provided by a reference measure that would incorporate contributions from different measurands. a relatively straightforward way to express it is by combining two key events relevant to any vector candidate-transfection efficacy and genetic reaction (knockdown or expression efficacy of its cargo)-and by normalizing these against the total counts of viable cells at different ratios of cargo to vector (molar or charge). the resulting relative or reference fitness is expressed in arbitrary units, since each event is characterized by a different measurand using a different method, e.g., genetic reactions by pcr against reference genes, transfection efficacy by flow cytometry, microscopy and mass-spectrometry, individually or combined, and cell viability using cell proliferation and viability assays providing quantitative chemical and enzymatic redox indicators of metabolically active cells. all these methodologies have their own limitations that need addressing before they can be combined in continua of correlated measurements [cit] . for example, intracellular staining remains an important requirement for cytometry and microscopy, an apparent difficulty for which is to quantify the fixation-permeabilization step, while a similar problem for pcr methods consists in the quantitative extraction of target nucleic acids [cit] . nevertheless, established and emerging techniques and methodologies are already showing encouraging progress in different application areas, providing reference materials and user guidelines, which create a necessary basis for the reference measure of gene delivery. such a measure can be traceable to a cell count, but as in the case of extracellular measurements does require supporting reference materials, which in this case are gene delivery vectors. the criteria for a meaningful candidate are also relatively well understood. one has to ensure that the reference fitness is reproducible, i.e., the candidate has to be able to transfect cells without cytotoxicity in a wide range of concentrations and ratios with its cargo and has no affinity to the cargo following cytoplasmic release [cit] . additional requirements are structural monodispersity, avoiding aggregation or agglomeration effects, neutral or close to neutral surface charge, and accessible surface chemistry for potential modifications as determined by requirements for specialist reference materials on its basis. in conjunction with that viruses remain the most effective transfection reagents, these criteria are best met by a biopolymer shell (polypeptide or polysaccharide) that is assembled by following the principles of the virus architecture. research in this area, both in academia and industry, is significant and provides an improved set of structure-activity principles that can guide the engineering of a reference material to support the advancement of gene therapy [cit] . there is also a number of suitable candidates under consideration by some metrology institutes, which are ripe for inter-laboratory comparability studies [cit] . what remains to be achieved is a consensus concerning the reference measure, what it incorporates and what measurement capabilities are most appropriate to characterize a proposed material. the choice of an appropriate cell line is a fundamental variability in cell analysis [cit] . this is unlikely to be solved in any foreseeable future, but can be pre-defined and endorsed by stakeholder communities whose products are typically developed against specific cell lines. reference materials are indispensable components for establishing traceability and such a joint approach will accelerate the establishment of a reference system that can deployable to the needs of gene therapy and manufacturing."
"heart rates were measured using different hand-shaking handling and signal periodicity computation methods. tables 1-3 show the results for a subject at rest and after light and strenuous exercise. tables 4 and 5 show the results (mean of those obtained under different illumination conditions) of three subjects at rest and after strenuous exercise, respectively. figure 10 shows the mean of the results for all subjects, activities and illumination conditions. as can be seen, the proposed hand-shaking handling method (m1) demonstrates the most consistent and accurate results across different illumination conditions. note that this trend was observed among different subjects and activities. furthermore, the proposed periodicity computation method (p2) proved to better select the signal component relative to the true heart rate than the previously proposed method [cit] . thus, using the proposed periodicity computation method, the mean error was reduced by approximately 5-7 bpm in the proposed system and approximately 3-6 bpm in the existing systems [cit] . the proposed system (m1 and p2) could measure the heart rate with a mean error less than 1.5 bpm and was sufficiently accurate for use in daily heart-rate monitoring. in contrast, the other systems that used either or neither of the m1 and p2 methods demonstrated much higher errors (5-10 bpm on average). for all systems, the influence of the illumination condition was not recognizable because there was no light intensity variation over the short heart-rate measurement period (12.5 s) and similar feature tracking performance could be maintained. table 1 . mean and errors of heart rates in beats per minute (bpm) estimated by different methods when the subject was at rest. \"m1 and p2\" represents the results of the proposed system. the values to the right of the/represent errors. table 2 . mean and errors of heart rates in bpm estimated by different methods when the subject performed warm-up exercise (moving up and down stairs). \"m1 and p2\" represents the results of the proposed system. the values to the right of the/represent errors. table 3 . mean and errors of heart rates in bpm estimated by different methods when the subject performed strenuous exercise (pushups). \"m1 and p2\" represents the results of the proposed system. the values to the right of the/represent errors. figure 10 . relationship between mean errors in bpm of different heart-rate estimation methods and heart-rate magnitude. \"m1 and p2\" represents the results of the proposed system."
"in our preliminary experiments, we observed that the previous method [cit] to estimate the rigid motion between consecutive frames did not always yield the well-scaled motion parameters required to correct the hand-shaking problem effectively. this appears to be due to ambiguity between rotation and translation [cit], which implies that other state-of-the-art methods [cit] are ineffective. in addition, computationally-intensive methods on mobile platforms are ineffective. we also noted that, in response to hand-shaking, the average point (mass center in figure 4 ) of all background features was stably located; thus, in the proposed system, rather than computing the locations of facial feature points relative to the frame origin, the proposed method computes the locations of facial feature points relative to the average point of the background features. the relative motion between facial features and the average point can isolate the head motion from the hand-shaking movements without ambiguity. note that the proposed method only measures the vertical displacement of the head to compute heart rates. therefore, we are only interested in hand-shaking that affects the displacement value; thus, we assume that hand-shaking involves only vertical translational movements. figure 5 shows the average magnitude spectrum of 20 different signals that recorded the vertical movement of the average point for 12.8 s. to remove noise, the proposed method employs a bandpass filter [cit] . as shown in figure 5, hand-shaking demonstrates many frequencies within the passband [0.75, 5] hz. thus, hand-shaking noise can bypass the filtering process and must be removed explicitly by subtracting the movement."
"the simple control structure that we envisage actually represents a situation that may be of interest in a number of different operational settings. we only mention two different cases of current high relevance: (i) a multitenant environment, where a number of isps are offered services by a datacenter operator (or by a telecom operator in the access network) on a number of virtual machines, and (ii) a collection of virtual environments created by a sp on behalf of its customers, where services are activated to represent customers' functionalities on their own private virtual lans. it is worth noting that in both cases the nodes' customers may be unwilling to disclose their loads to one another, which justifies the decentralized game optimization."
"in practice, the analysis of one-dimensional biomolecular spectra is prohibitive because of spectral when a protein is titrated with a ligand, e.g. with a drug or another biomolecule, the chemical shift of the nitrogen and proton nuclei of the residues that are in close proximity to the binding site will be most affected. thus, the binding of the ligand results in changes in the chemical shifts of these nuclei, causing the resulting peaks to alter their position in the nmr spectrum. by recording a series of nmr experiments at varying stoichiometries of protein and ligand, the resulting series of spectra conveys information regarding the affinity of the ligand, as well as identifying the important residues involved in the interaction."
"author contributions: conceptualization, j.-p.l. and h.p.; funding acquisition, h.p.; methodology, j.-p.l.; software, j.-p.l.; supervision, h.p.; validation, j.-p.l. and h.p.; writing-original draft, j.-p.l.; writing-review and editing, h.p."
"a wban can thus be equipped with implanted or wearable sensors capable of sensing and transmitting body parameters from diverse body locations [cit] . however, the tiny sensors are required to transmit their data to a more capable device, known as body coordinator (bc) for furtherprocessing and communicating with the users. the continuous sensing as well as communication of the sensed parameters; however, produce heat resulting in severe thermal damage to the human tissue if it prolongs for significant period of time [cit] which also might eventually appear as a threat to human life. therefore, thermal-rise is regarded as one of the crucial factors while designing communication protocols for wbans."
human health is a very important factor for the development and progress of society. home or personal healthcare technologies can resolve the inconvenience of visiting a medical office and help users to handle their health conditions well [cit] .
"since both the dc and nr traffic contend in this period, differentiated csma/ca parameters are used to prioritize dc traffic. table 1 lists the contention parameters used for both these traffic types."
"to briefly explain the theoretical aspect of this phenomenon, consider for simplicity a peak for a nucleus i in a protein p at 7.0 d ppm in a one-dimensional (1d) nmr spectrum ( (1), where the protein is in its unbound state. upon addition of a high-affinity ligand, under the condition that the k off is small, the equilibrium lies fully towards the bound state. consequently, addition of the ligand causes the i peak in the unbound state to decrease in intensity, whereas a new peak at a different position appears; this new peak represents the bound state of the same protein nucleus i. in nmr, the above situation when the k off is much smaller than δd i is called the 'slow exchange' regime. in contrast, in a 'fast exchange' regime, when k off is much greater than δd i, in each of the spectra recorded at different [p] : [l] stochiometries, the peak position for nucleus i represents the population weighted average of its free and bound positions. consequently, the peak appears to be 'moving' from its original position ( fig. 1d 1 : 0 eq) towards a its bound position as the ligand concentration increases (fig. 1d, 1 : 0.5 eq to 1 : 2 eq). in cases where k off $ δd i, the peak typically disappears due to line broadening effects and this situation is called 'intermediate-exchange' (not shown)."
"in this paper, we introduce a game-theory-based solution for energy-aware allocation of virtualized network functions (vnfs) within nfv environments. in more detail, in nfv networks, a collection of service chains must be allocated on physical network nodes. a service chain is a set of one or more vnfs grouped together to provide specific service functionality and can be represented by an oriented graph, where each node corresponds to a particular vnf and each edge describes the operational flow exchanged between a pair of vnfs."
"rate adaptation is obtained by tuning the clock frequency and/or voltage of processors (dvfs, dynamic voltage and frequency scaling) or by throttling the cpu clock (i.e., the clock signal is disabled for some number of cycles at regular intervals). decreasing the operating frequency and the voltage of the node, or throttling its clock, obviously allows the reduction of power consumption and of heat dissipation, at the price of slower performance."
"the real power of the csp method lies in the identification of protein residues most affected by the ligand, i.e. those residues with nuclei that display large δd i values. in the case of multi-dimensional spectra, such as the 15 n-hsqc spectrum ( fig. 2a), the total chemical shift change involving all dimensions is usually taken (vide infra, eqn 2) and used as a proxy for the importance of the specific residue in the interaction. furthermore, for the fast exchange regime k d can be determined from the observed δd i values as a function of ligand concentration [cit] ."
"despite managing to suppress the hand-shaking noise, the head motion signal was still noisier than that acquired from a static camera, as in the previous study [cit] . however, this was overlooked, and the conventional periodicity computation method was used for signal selection in the previous study [cit] . in our experiments, the conventional periodicity computation method was highly vulnerable to noisy signals (see the mean errors of \"m1 and p1\" in figure 10 ). in contrast, the proposed correlation-based method can robustly compute the periodicity of noisy signals (see the mean errors of \"m1 and p2\" in figure 10 ), worked well for signals where hand-shaking noise was not suppressed, and enabled heart-rate computation with quite high accuracy (the mean errors of \"m2 and p2\" were lower than 5 bpm (figure 10) )."
"the evolution of fifth generation (5g) wireless communication technologies has reached an unprecedented height with the aim of providing connectivity for any device type and any application. all of these potential applications exploiting 5g come under the name of internet of things (iot). thanks to the moore's law, the continuous miniaturization of electronic devices [cit] whilst growing their capacity eventually led to the development of tiny and portable sensors, capable of communicating wirelessly which form the basis of iot. among many other applications, iot healthcare is one of the prominent ones and gained tremendous interest to the research community [cit] ."
"wireless body area networks (wbans) is a network paradigm designed to provide connectivity among diverse miniature body sensors that support different iot healthcare applications. wireless body area networks emerge as a promising solution to threat the ever-increasing healthcare expenses [cit] . although wireless sensor networks (wsns) continue to develop for a wide range of applications, they cannot precisely address the challenges of human body monitoring. human body monitoring can be achieved through attaching sensors to the body surface in conjunction with embedding them inside the tissues to collect more factual physiological data for the healthcare professionals. this will considerably reduce the overall health care cost, yielding better utilization of healthcare resources. this also brings about other opportunities including infant monitoring as well as independent living of aged people."
"in this paper, we focus on the second type of service deployment. we refer to this paradigm as pure nfv. as already outlined above, the sp processes the service request by means of vnfs. in particular, we developed an energyaware solution to the problem of vnfs' allocation on physical network nodes. this solution is based on the concept of game theory (gt). gt is used to model interactions among selfinterested players and predict their choice of strategies to optimize cost or utility functions, until a nash equilibrium (ne) is reached, where no player can further increase its corresponding utility through individual action (see, e.g., [cit] for specific applications in networking)."
"different industries are beginning to define the need for specialist measurements and standards for extracellular systems relevant to regenerative medicine, biofilm prevention and microbiome environments. the restoration of damaged tissues and the prevention of infections are among the challenges of the highest priority for healthcare. indeed, the cost to the national health service in the uk for managing a chronic wound alone is conservatively estimated at £5 bn per year [cit], raising over the last 10 years up to 5% of the total outturn expenditure on healthcare [cit] . in addition, biofilm formation is one of the main contributing factors to chronic wounds (e.g., ulcers) [cit] . biofilms account for up to 90% of chronic wounds. although only 6% of these are considered as acute, these are associated with the increasing incidence of diabetes and obesity, which compound the burden of chronic wounds on national healthcare systems [cit] ."
"it is noticeable that the results of m2 were better than those of m3, which indicates that high-resolution feature tracking without hand-shaking handling with the camera placed close to the face can be better than feature tracking at a lower resolution with hand-shaking handling. in other words, obtaining high-resolution face images may be more important than addressing the hand-shaking problem. this was also observable in the previous study [cit] and appears to be the primary reason why the previous system suffered from low accuracy and low robustness despite addressing the hand-shaking problem to some extent. in this context, it was evident that the proposed dual camera system which realizes high-resolution tracking of both the facial and background features, was advantageous for head-motion-based heart-rate estimation in mobile scenarios and was required for hand-shaking handling."
"as mentioned earlier, the main reason for thermal increase around the tissue is the radiation from the node antenna, which depends on the degree of communication activities by the sensor nodes. in particular, the continuous operation of a node's transceiver raises the temperature of the node surrounding a tissue and eventually reaches to a certain level causing tissue damage. we define that threshold level of the temperature as the hotspot threshold, denoted as th h . in contrast, the lack of communication operation gradually causes a node to be cooled down. therefore, by controlling the communication activity the temperature around a node can be regulated."
packet delivery ratio (pdr). it is the ratio of the total number of unique packets received by bc to the total number of packets generated by the nodes.
"where n is the number of nodes, p l, p t, p r, p s denote the listening power, transmission power, reception power and sleep power, and t i l, t i t, t i r and t i s refer to the time spent in listening, transmitting, receiving and sleeping state respectively."
"we now discuss the most relevant works that deal with the resource allocation of vnfs within nfv environments. we decided not only to describe solutions based on gt, but also to provide an overview of the current state of the art in this area. as introduced in the previous section, the key enabling paradigms that will considerably affect the dynamics of ict networks are sdn and nfv, which are discussed in recent surveys [cit] . indeed, sps and network operators (nos) are facing increasing problems to design and implement novel network functionalities, following rapid changes that characterize the current isps and telecom operators (tos) [cit] ."
"various engineering problems, where the action of one component has some impacts on the other components, have been modeled by gt. indeed, gt, which has been applied at the beginning in economics and related domains, is gaining much interest today as a powerful tool to analyze and design communication networks [cit] . therefore, the problems can be formulated in the gt framework, and a stable solution for the components is obtained using the concept of equilibrium [cit] . in this regard, gt has been used extensively to develop understanding of stable operating points for autonomous networks. the nodes are considered as the players. payoff functions are often defined according to achieved connection bandwidth or similar technical metrics."
"the cfp period is used only for big data transmission from the nodes to the bc. being notified by bc regarding the assigned slot number, a node wakes up in that particular slot, transmits its big data, and then goes to sleep again after receiving the acknowledgement. as the figure 9 shows, node n2 and n6 transmits their big data at their assigned slot no 3 and 13 respectively. based on the request, n2 has been assigned 6 gts while n6 is assigned 5 gts. all other nodes remain in the sleep state."
"the increasing network energy consumption essentially depends on new services offered, which follow moore's law, by doubling every two years, and on the need to sustain an evergrowing population of users and user devices. in order to support new generation network infrastructures and related services, telecoms and isps need a larger equipment base, with sophisticated architecture able to perform more and more complex operations in a scalable way. notwithstanding these efforts, it is well known that most networks and networking equipment are currently still provisioned for busy or rush hour load, which typically exceeds their average utilization by a wide margin. while this margin is generally reached in rare and short time periods, the overall power consumption in today's networks remains more or less constant with respect to different traffic utilization levels."
"creating an international metrology network providing a sustainable access to measurement and standards that support regulation and the availability of measurement services may accelerate all these developments and their implementation for the end-user needs. to this effect, working groups in ccqm provide vehicles to advance measurement assurance principles and establish a means to metrological traceability. given that cell metrology is at early days to be able to fully address the complexity of cellular measurements, there is an increasing tendency to place a stronger emphasis on property related efforts thereby setting priorities on particular cellular processes or cell-based products. this will also influence a reciprocal end-user view in addressing fundamental questions that evolve around the lack of and need for traceability in the life sciences sector [cit] . coupled with regulations that establish more stringent criteria for traceability through the supply chain and are driven by requirements to better protect public health and patient safety, the metrology of cell-based systems is well-positioned for positive impact [cit] . concomitantly, consensus standards strive to provide clarity for regulatory expectations for pre-market submissions via a declaration of conformity or for general use (e.g., fda guidance on voluntary consensus standards). by focusing on reference measurement systems, the cawg brings up the need for comparability of measurement results in a continuum of pertinent properties and is striving to understand the value of reference materials and other measurement services for method harmonization where appropriate. this can also mitigate the growing risks of the replication drive that threatens to compromise promising research results due to the lack of reference systems that can fully address the complexity of cellular measurements [cit] ."
"taking into account these two causes for thermal rise, the temperature of a node at a grid point (x,y) at time t, denoted as t t (x, y), can be estimated utilizing fdtd [cit] which is an electromagnetic modeling technique that discretizes the differential form of space and time [cit] ."
"we evaluate the thermal performance of thmac and ieee 802.15.6 in different data generation rate. figure 14a shows the average temperature rise for both the protocols in different traffic load. as the traffic load increases, the average temperature also increases for both the protocols. however, due to temperature aware duty cycling mechanism thmac depicts significantly better performance compared to 802.15.6. at high traffic load, due to temperature increase, nodes multiplicatively increase the communication period that eventually lowers the average temperature. contrarily, the lack of any thermal-aware duty cycling mechanism causes a sharp rise in average temperature for 802.15.6."
"as mentioned before, the dl period is for downward data transmission from bc to sensor nodes. however, since em traffic transmission is in upward direction to bc, thmac employs the ifs period for emergency traffic transmission. we adopt the mechanism as described in reference [cit] . at the beginning of every slot bc waits for an ifs period before downloading any data. if a node has emergency traffic, it chooses an ifs period shorter than the ifs chosen by bc, and transmits its emergency data. thus, the shorter ifs preempts the download data from bc. however, since the preemption causes transmission failure, bc transmits the data in the consecutive slot as depicted in figure 10 ."
"for practical use, it would be desirable to compute the heart rate without saving/gathering frames in advance by tracking the facial and background features in real time. thus, we analyzed the feature tracking time of each system and found a noticeable difference because m1 and m3 allow additional background feature tracking for hand-shaking handling and m1 uses two cameras rather than one. in our experiments, m1, m2, and m3 took 50.636, 19.175, and 23.560 ms per frame, respectively. therefore, m2 and m3 can effectively work in real time because the feature tracking time was much shorter than the data sampling interval (50 ms). in contrast, the feature tracking time of m1 was not short and was similar to the data sampling interval. therefore, in m1, incoming frames were saved in a small queue, while features were tracked in the saved frames via multi-threading. with this scheme, features could be tracked in real time using m1. consequently, the computational overhead caused by using two cameras was negligible."
"one area in the metrology of tissue engineered materials focuses on enabling parameterised interdependencies between cell interactions and forces that cells are exposed to. combinatory measurements able to monitor cell behavior in response to extracellular environments, both stimulating and detrimental, may provide these interdependencies and improve the understanding of physicochemical properties of extracellular guidance on cell and bioactivity development. these would support controlled cell differentiation in \"intelligent\" 3d cultures, the assessment of cellular responses in bacteriachallenged environments and would enable regenerative medicine, which unlike conventional and often palliative medicines, offers more comprehensive solutions for tissue restoration. the substantial potential of these medicines in regenerating diseased organs is a major reason for the intensive r&d activities in the field [cit] . the growing markets of tissue treatments are influenced by increasing aging population and patient numbers, while the uptake of promising technologies is bound to more clinical trials (e.g., diabetes, viacyte, and spinal cord injury, geron, usa) thereby impacting on current regulation policies. if regulatory routes can be shortened with relatively small and inexpensive clinical trials new technologies might be commercialized faster. however, shortening regulatory approval times requires justification and reference points against which new technologies can be assessed at early stages, while the availability of reference protocols and materials remains low [cit] ."
we used the following metrics to evaluate thmac. maximum temperature rise. maximum temperature rise denotes the maximal estimated temperature by any node throughout the simulation period. the metric manifests the protocol performance for hotspot formation and the associated tissue damage.
"the rest of the paper is organized as follows: section 2 summarizes the related works. section 3 presents some preliminaries and assumptions behind the protocol. section 4 describes the protocol operations in detail. section 5 demonstrates the protocol performance using simulation. finally, section 6 presents the concluding remarks."
"one of the root causes for thermal increase is the radiation from the node antenna. to estimate the radiation level absorbed by the tissue, specific absorption rate (sar) is exploited. the space surrounding the antenna is split into near and far field. sar in the near and far field can be estimated as follows [cit] :"
"in comparing the various programmes for csp data analysis, we find that the csp module of analysisassign version-3 provides for a simple and interactive graphical tool that allows users to significantly reduce the time required for a csp analysis."
"all in all, a stronger response to end-user needs at an early stage of cell metrology developments through focused efforts of individual metrology institutes and engaging in consensus standards activities, while taking into account the complexity of cellular measurements can provide a significant step change in cell measurements with farreaching benefits to stakeholders. in particular, this is the case for small and medium size enterprises that operate at early, high-risk stages, where there exist severe gaps in measurement infrastructure support making it difficult for them to survive. on a more fundamental scale where cell metrology may demonstrate its share of value, this strategy will allow metrology institutes to impact on reverting the current prevalence of irreproducible preclinical research, which is estimated to exceed 50%, with associated annual costs being in the range of $28 bn in the united states alone ."
"differing from figure 7d, where both signal selection methods (using p1 and p2, respectively) successfully selected the correct principal component signal, the p1 method often failed to select the correct principal component signal despite suppressing the hand-shaking noise. figure 9 shows an example. figure 9a -e represent the magnitude spectrum of the first five principal component signals extracted from a head motion signal. the p1 method selected the signal of figure 9c because its periodicity index was highest (9.3%), while the p2 method selected that of figure 9d where the correlation-based periodicity index was the highest (26.71%). in the two selected principal component signals, the frequency with the maximum magnitude differed significantly, and the heart rate computed from figure 9d was much closer to the ground truth. consequently, only the p2 method successfully selected the correct principal component signal. as can be seen in figure 9, the periodicity indices computed by p1 have low variance (i.e., the highest periodicity index does not differ significantly from the others). we consider that the low variance caused p1 to be susceptible to noise and frequently fail to select the correct principal component signal. on the other hand, it is evident that the periodicity indices computed by p2 are much more discriminative. the frequency with the greatest magnitude and its first harmonic is shown with blue arrows and frequency values. these signals were brought from a case where the ground-truth heart rate was 74 beats per minute (bpm) (≈1.23 hz). with both methods, a greater percentage indicates greater signal periodicity. the periodicity indices computed by p1 have lower variance (1-2%) than those computed by p2, which indicates that those computed by p2 are more discriminative and more robust to noise."
"the mac protocol for a wban mainly coordinates the channel access to avoid collisions and maximize the throughput. however, in a highly constrained wban with a number of conflicting objectives, particularly maintaining lower thermal rise and energy consumption as well as meeting the respective qos demands of diverse traffic types is a profoundly challenging task. a good number of studies attempted to devise mac protocol for wban [cit] . the majority of them focused mainly on energy efficiency and qos provisioning. no significant research exists addressing the thermal-rise issue of wban except in reference [cit] . here, the authors proposed temperature aware probabilistic sleep cycle management for wbans. however, the study mainly investigates the effect of different probabilistic sleep duration on thermal rise and attained throughput rather than devising a mac protocol."
"we now consider two specific control problems, whose resulting resource allocations are interconnected. specifically, we assume the presence of control policies (cps) acting in the network node, whose aim is to dynamically adjust the processing rate, in order to minimize cost functions of the form of (4). on the other hand, the players representing the vnfs, knowing the policy adopted by the cps and the resulting form of their optimal costs, implement their own strategies to distribute their requests among the different resources."
"the main goal of the proposed thmac is to reduce thermal increase due to wireless communication of the in-vivo sensor nodes as well as to avoid the creation of a hotspot. furthermore, we opt to meet the diverse qos requirements of different traffic types in terms of delay and reliability. considering the energy constraints of in-vivo sensor nodes, thmac aims to reduce the energy depletion of the nodes to prolong the network lifetime. finally, thmac also addresses emergency traffic handling and tries to deliver the em traffic to the least possible delay with high reliability. this section discusses the design details of thmac."
"this section introduces the mac operations of thmac during a superframe. as the superframe consists of different periods, we discuss how the frames are exchanged between nodes and the bc for different traffic types in a corresponding period."
"accurate tracking of background features in portrait mode at higher resolution -more robust computation of signal periodicity in the spatial domain -isolation of head motion from hand-shaking movements without rotation-translation ambiguity note that a dual camera system has been reported [cit] . however, that system differs from the proposed system in that only the rear camera was used to measure heart rate, and the front camera was used to measure the breathing rate. in addition, touch-based and ppg-based methods were used to measure the heart rate."
"in this study, to address the limitations of previous heart-rate monitoring systems, we have developed a practical head-motion-based (i.e., bcg-based) heart-rate monitoring system that can run on smartphones. the proposed method employs a dual camera system, where the front and rear cameras track facial features and background features, respectively. here, the head and hand-shaking movements can be measured more accurately by tracking a greater number of facial and background features at higher resolution. we have also proposed a method to negate hand-shaking movement that involves computing the relative motion between facial features and the average point of the background features. here, we assume that hand-shaking involves only vertical translational movements. finally, an autocorrelation-based signal periodicity computation method has been proposed to enable hidden heart-rate-related signals to be separated from noisy head-motion signals accurately. compared to previous systems, the proposed system demonstrated more accurate (i.e., lower mean errors in heart-rate measurement) and consistent results, and the accuracy was sufficient for practical heart-rate monitoring. to the best of our knowledge, the proposed system is the only bcg-based heart-rate monitoring system that runs reliably on a smartphone."
"through the experiments, we separately evaluated the performance of the proposed hand-shaking handling method (m1) and periodicity computation method (p2). both methods improved accuracy compared to the previous methods. however, only the proposed system using both methods together was reliable and demonstrated accuracy sufficient for daily heart-rate monitoring (figure 10 ). this was consistent (or the difference in performance was not recognizable) across different illumination conditions, subjects, and activities. this is the primary contribution of this study."
"we consider a deployment scenario, in which heterogeneous in-vivo nodes are implanted in a human body forming a wban. a central data sink, also known as body coordinator (bc) is attached to the body surface as shown in figure 1a . the bc is not power hungry and could be equipped with external power source, however, the in-vivo nodes are usually energy constrained. in-vivo sensor nodes mainly perform sensing and communication activities while the bc manages some major operations of wban such as data aggregation, synchronization, context recognition and exchange of control and management packets. the bc accumulates the data from the in-vivo nodes, process it and then transmits it to a remote server through access networks using wifi, gsm or wcdma as depicted in figure 1a, and this communication paradigm is out of the scope of this paper."
"has recently established a cell analysis working group (cawg) whose mission is to identify, establish and underpin global comparability of cell measurement capabilities through reference measurement systems of the highest possible metrological order with traceability to the si or to other internationally agreed units. the group benchmarks claimed competences of the national measurement institutes for measurement services in the quantification of intact cells and cell properties. this strategy is driven toward transforming trial-and-error approaches used in translational research into more predictive solutions. the mission however faces two main challenges:"
"to transmit em traffic during cfp, some slots are allocated at the beginning of cfp, known as the emergency transmission slot (ets). thus, the remaining slots during cfp are used as gts for transmitting big data. however, since the em traffic is usually small data containing biological parameter, we assume no big data will be transmitted as em traffic. thus the restriction for transmitting big data during cfp is not applied for em traffic."
"currently freely available nmr software suites, such as sparky, allow retrieving csp values from their tables, but users are required to manually export to third party software to carry on the analysis or are limited to single and static plotting, like in the case of ccpnmr analysis version-2. the possibility to graphically and interactively inspect the nmr data that identify the residues involved in protein-ligand interactions, makes the new analysisassign csp module extremely useful for non-experts and drastically reduces the time required for a csp analysis. the csp module is also not limited to 1 h-15 n as it can accommodate any combination of nuclei, e.g. 1 h- 13 c in case of methyl residues. analysisassign is implemented in a flexible fashion that will facilitate easy adaptation to insert specific calculation modes for δd i values, automatic pre-and user-defined k d fittings as well as direct links to external auto-docking software such as had-dock [cit] . for more advanced users, it is also possible to use the analysisassign libraries to create specific, but simple macros to extrapolate further information from the dataset (table 1) . for example, a macro can be used to plot the minimal shift changes resulting from the mutation of a specific residue in a protein, including in this calculation only residues in which the csp is above the defined threshold value. several settings and data exporters based on the nmr-exchange format (nef, [cit] easily export information to other programs for further analysis if so required. a series of other programmes have also been developed to address specific tasks using peaks in nmr spectra and nmr assignments, such as the programme farseer, which performs analyses on large and multivariable datasets, including csp [cit] . other programmes include auto-face, which facilitates the identification of binding mechanisms from csp data [cit] and titan, which uses peak perturbations trajectories to help the identification of interaction mechanisms [cit] ."
"here, ∆ t is the discretized time step, ∆ is the discretized space step, b is the blood pressure perfusion constant, c p is the specific heat of the tissue, t b is the fixed blood temperature, k is the thermal conductivity of the tissue."
"exemplar measurements include identification and quantification of cell number or cell components (e.g., cell surface receptors, in situ genes or proteins) and measures of biological response (e.g., cell morphology, gene expression rate). measurands can thus be conducive to increasing complexity starting with more generic values (e.g., cell count), which is feasible and pursued at the moment (figure 1 ) [cit] ) . such a bottom-up approach can support metrological traceability with relevance to end-user applications and consequently to measurement services that to date range from a complete blood count and biomarker expression by flow cytometry to cell viability. metrology institutes already possess capabilities to characterize cell density and confluency fraction of cells and cell shape in given environments, while data on stem cells and dose delivered by specific therapeutic products are on the horizon. measurements of cell authenticity, viability, and toxicology are already provided as routine contracted services. there are capabilities used to detect rare cells in blood products and characterizing nanoparticles interacting with cells and permeabilizing cells, and capabilities that are technology dependent (e.g., defined by a technology or technique like flow cytometry) as well as technology agnostic measurement services (e.g., quantification of a specific cell type in a given matrix). involving end-users early in the selection of a particular service proves essential and mutually beneficial for progress toward standardization. for example, cell quantitation remains a paradigm objective of cell metrology. it is necessary to better understand structure-activity, biochemical and physical properties of the cell, and establish quantifiable relationships across length and time scales. to enable predictability for product design and ultimately support translational research, such relationships must be free of a-priori constraints imposed by the limitations of a particular technique. therefore, metrology community also coordinates their activities with those of standardization organizations. this helps better align measurement capabilities with real end-user needs, which are also better informed by technology developers. as an exemplar, the launch of novel therapies, including gene and cell therapies, is estimated to reach prescription sales at $1.2 [cit] . on the one hand, the emergence of new technologies is rapid, which requires more animal tests and clinical trials. on the other hand, existing and emerging regulatory policies emphasize the lack of suitable standards that limit the use of advanced therapies (ec/1394/2007; ec/2001/83). these factors expose persistent gaps in the availability of higher order reference measurement procedures and reference materials that are necessary to facilitate the translation of innovation into cost-effective products [cit] . different organizations have stood up efforts to begin to fill these gaps (e.g., [cit] 1-1:2018, astm f2739), while the life sciences community is placing a stronger focus on cell and gene therapies. in terms of functional measurements this emphasis can be broadly grouped into extra-and intracellular measurements."
"fifteen healthy participants (22 to 35 years) with different skin tones and genders took part in the experiment. their heart rates in beats per minute (bpm) were measured several times in a sitting position after performing various activities, such as resting and light/hard exercising with a long interval between measurements. the participants were required to relax but refrain from moving their heads and hands if possible during each measurement. the experiment was conducted under three different illumination conditions, i.e., indoor daytime (artificial lights inside the room and sunlight), outside daytime (sunlight), and indoor nighttime (artificial lights). see figure 6 for some examples. in the following tables and figures, m1, m2 and m3 represent a system with the proposed hand-shaking handling method, a system without hand-shaking handling that focuses on tracking facial features at close distance (high resolution) [cit], and a system with the previously proposed hand-shaking handling method [cit], respectively. p1 is the previously proposed signal periodicity computation method [cit] based on the percentage of total spectral power accounted for by the frequency with maximum power and its first harmonic, and p2 is the proposed correlation-based method. therefore, \"m2 and p1\" and \"m3 and p1\" indicate the very previous systems, [cit], respectively. note that m1 and m2 work in portrait camera mode, and m3 works in landscape mode. therefore, in m1 and m2, the face fully occupies the camera frame and can be tracked at a higher resolution than in m3."
thmac exploits cap for uploading small data to the bc. nodes generating dc or nr traffic only contend during this period. nodes employ the usual csma/ca mechanism for data transmission.
"the symptomatic irreproducibility of data in biomedicine and biotechnology prompts the need for higher order measurements of cells in their native and near-native environments. such measurements may support the adoption of new technologies as well as the development of research programs across different sectors including healthcare and clinic, environmental control and national security. with an increasing demand for reliable cell-based products and services, cellular metrology is poised to help address current and emerging measurement challenges faced by end-users. however, metrological foundations in cell analysis remain sparse and significant advances are necessary to keep pace with the needs of modern medicine and industry. herein we discuss a role of metrology in cell and cell-related r&d activities to underpin growing international measurement capabilities. relevant measurands are outlined and the lack of reference methods and materials, particularly those based on functional cell responses in native environments, is highlighted. the status quo and current challenges in cellular measurements are discussed in the light of metrological traceability in cell analysis and applications (e.g., a functional cell count). an emphasis is made on the consistency of measurement results independent of the analytical platform used, high confidence in data quality vs. quantity, scale of measurements and issues of building infrastructure for end-users."
"in an implanted wban, the traffic generated by the in-vivo body sensors are inherently diverse in regard to their quality of service (qos) demands. considering the diverse qos requirements, we categorize the traffic as follows:"
"due to the high potential of a wban having in-vivo sensors, it can be utilized to measure a wide range of body parameters including temperature, blood pressure, blood oxygen saturation, pulse, electrocardiogram (ecg) and so forth [cit] . the diverse sensing parameters also possess distinct quality of service (qos) requirements in terms of delay and reliability. for instance, electroencephalogram (eeg), electrocardiogram (ecg) and electromyography (emg) and so forth require timely delivery while respiration monitoring, ph level monitoring has reliability constraint. furthermore, due to potentially life threatening situations, some parameters require emergency transmission with reliability. the in-vivo sensors are also typically powered by battery having limited energy capacity. hence, low energy consumption is another vital requirement for wban communication protocols to prolong the network lifetime."
"according to the aforementioned research background, it is obvious that the existing efforts for developing mac for wbans mainly focused on energy efficiency and qos provisioning. to the best of our knowledge, no solution exists that devised a comprehensive mac scheme taking into account the thermal effect of in-vivo sensor nodes in addition to achieve qos provisioning and emergency data handling. these motivate us to devise thmac."
end-to-end latency. end-to-end latency of a packet is estimated as the difference between the packet generation time and the time the packet is received by the bc. latency experienced by distinct data packets are averaged over the total number of distinct packets received by bc.
"nmr is a spectroscopic technique that employs an inherent property of many nuclei called 'spin' to yield spectra of various nuclei of biological interest, e.g. 1 h, 13 c, 15 (fig. 1b) . the spread of each of the three distributions reflects the different conformations, i.e. the chemical environments, and dynamics, i.e. the change in these environments, of each of the nuclei in the various alanine residues in their respective proteins."
"one of the striking design goals of thmac is qos provisioning of diverse body sensor types. considering the respective qos demands, we exploit different periods of an active period for the transmission of diverse traffic. cap is designated for dc and nr traffic transmission. however, due to the delay constraint, dc exploits prioritized transmission as to be discussed in section 4.4.1. the polling period mainly handles rc traffic being reliability constrained. both cap and polling period are utilized for small data transmission. however, nodes transmit the big data frame during cfp period. figure 4 portrays the frame structure of thmac. in a wban, most of the traffic are of periodic nature, and small payload length of 7 bytes can accommodate for most commands and data, and we term this frame as small data frame. the payload length exceeding 7 bytes is regarded as big data frame. in a superframe, if a node generates big data of either dc or rc traffic type, it requests for the desired number of gts slots during cap or polling period. upon receiving the request, bc allocates the required gts slots for the big data and notify them during dl period. the main benefit for the allocation of diverse periods for diverse traffic types lies in reducing the contention that could result in lower latency and higher packet delivery ratio. moreover, since the occurrence of big data is not common in nature, designating a separate cfp period ensures the fairness with the frequent small data in terms of latency, reliability as well as energy consumption."
"a service request can be allocated on dedicated hardware or by using resources deployed by the service provider (sp) that processes the request through virtualized instances. because of this, two types of service deployments are possible in an nfv network: (i) on physical nodes and (ii) on virtualized instances."
"nmr is one of the three major techniques that provides structural, dynamical and also interaction data [cit] . in this minireview, we will illustrate how a simple yet powerful experimental nmr technique, the socalled chemical shift perturbation (csp) analysis, can be used to investigate interactions between biomolecules or biomolecules and small drug-like compounds. since it was first proposed, the csp analysis has become well-established, as illustrated by the increasing number of papers referring to the technique (fig. 1a), with currently~80 references annually. in this paper, we also discuss how current nmr software packages can facilitate the csp data analysis. particular focus will be given to the ccpnmr analysisassign version-3, which provides several user-friendly tools for retrieving the relevant data thus, providing for invaluable biological information."
