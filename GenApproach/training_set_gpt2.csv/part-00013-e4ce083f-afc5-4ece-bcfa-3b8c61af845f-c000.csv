text
"we use the matlab discrete wavelet transform (dwt) toolbox to analyze the dwt performance after principal component analysis (pca). note that we have extended the duration of the extracted activity to include stationary piece for further analysis. the time-frequency component diagram is demonstrated in fig. 9 . each layer represents a frequency range, the frequency increases in an orderly manner from the first layer to the ninth layer, and brightness indicates feature amount in frequency components. the darker it is in fig. 9, the more features there are in this frequency components. it is shown in fig. 9a that the frequency components of \"walk\" are no more than a b fig. 8 performance of the proposed detection algorithm. a, b the values of fpr, fnr, and f1-measure in outdoor and indoor, respectively seventh layer. it is shown in both fig. 9a and b that the frequency components of \"run\" with eight layers is no more layer than that of \"walk\". it is shown in both fig. 9c and d that features of \"squat\" and \"sit down\" are mostly concentrated in the lower layers. it is shown in fig. 9e and b that similar to 'run', there are more high-frequency components of \"fall down\" due to the sudden increase of the speed in the moment of the falling. the difference between \"squat\" and \"sit down\" is mostly reflected by the fourth and fifth layers and the difference between \"run\" and \"fall down\" mostly exist in the intermediate frequency section. the difference between \"squat\" and \"sit down\" is mainly reflected by the fourth and fifth layers and the differences between \"run\" and \"fall down\" mostly exist in the intermediate frequency section. it can also be demonstrated from fig. 9 that the time-frequency components are concentrated in the first layer in the stationary phase. in addition, the high-frequency components increased obviously in the action phase. the above results demonstrate that each activity can be well characterized by the time-frequency component. the offline activity database is trained in both indoor and outdoor environment. figure 10 shows the confusion matrix of the recognition results. it can be found from fig. 10 that the recognition accuracy of the outdoor achieves more than 90%, and the misjudgment rate of the indoor significantly increases. as the result of the influence of multi-path, the fluctuation of the signal is more random, which may cause the extracted features to deviate from a certain frequency component, thereby affecting the recognition result."
"the wideband siw cavity-backed slot antenna topology depicted in fig. 2 is adopted, making use of the bandwidth enhancement method [cit] . both . a copper-plated taffeta electrotextile with a sheet resistance of 0.2 ω/sq is applied for the conducting top patch and bottom groundplane. these electrotextile patches are attached to the cork substrate using thermally activated adhesive sheets. the siw cavity is split into half-cavities a and b by a non-resonant slot in the top patch, much longer than half a wavelength, leading to the simultaneous excitation of two hybrid cavity modes [cit] . correct dimensioning of the half-cavities and slot yields resonance frequencies of the two excited hybrid modes that are close together in the frequency band of interest, which leads to an enlarged impedance bandwidth. the lower resonance frequency hybrid mode consists of a weak te 110 and a strong te 120 resonance. the fields in halfcavities a and b are out of phase and the dominant fields are in half-cavity a. the higher resonance frequency hybrid mode consists of a weak te 120 with a strong te 110 resonance. the fields in half-cavities a and b are in phase and the dominant fields are in half-cavity b. the field distributions of both hybrid modes lead to consistent radiation by the slot. this enables wideband far-field properties, which is a key requirement for uwb antennas. the resulting antenna is linearly polarized and the yz-plane is referred to as the e-plane, and the xz-plane as the h-plane. the structure is fed with a coaxial probe, positioned to magnetically couple efficiently to the two hybrid modes and to achieve good impedance matching."
"† internet based communication networks and services, department of information technology, ghent university/iminds, 9000 ghent, belgium. backed antenna topologies [cit] exhibit many desirable properties for uwb applications. they provide high radiation efficiency, good front-to-back ratio (ftbr), and high isolation from their surroundings, making them suitable for low power applications. various bandwidth enhancement techniques [cit] can be applied to achieve an ultra-wide bandwidth. finally, their low-profile allows for integration into floor and wall materials."
"in this paper, an adaptive detection method is first proposed for extracting activity durations. it is adaptive and instantaneous in an indoor and outdoor wireless environment and works in various environments or adding other activities. secondly, a recognition method based on frequency domain features is proposed to design a subspace selection scheme and a weight assignment scheme, which can overcome the poor robustness of existing sole classifier. from our experimental result, even in the various experimental site, the extraction accuracy rate of cdhar can reach 99.80% in outdoor environment and 99.60% in indoor environment, respectively. besides, the rate of recognition accuracy can achieve 91.2% in outdoor environment and 90.2% in indoor environment, respectively."
"an uwb siw cavity-backed slot antenna covering the lower part of the 3.1-10.6 ghz block allocated to uwb transmission systems (3.1-3.6 ghz) was designed, constructed and validated. owing to the planar topology, low profile and the use of cork substrate material, the proposed antenna may be integrated unobtrusively in any cork surface. prior to the antenna design, the cork substrate material was characterized. the design was conducted based on the average properties, while maintaining some impedance bandwidth margins to allow for varying cork material properties. measurements performed on a prototype in free-space conditions confirm the high performance observed in simulation and prove that the far-field properties only vary slightly over the considered frequency band, which is the most important requirement to uwb antennas."
"in this paper, we apply three evaluation indexes to analyze the performance of the proposed detection algorithm, namely false-positive rate (fpr), false-positive rate (fnr), and f1-measure. due to fpr and fnr are indicators of the error ratio, the lower the values of fnr and fpr, the better the system performance. f1-measure is a comprehensive evaluation index used to evaluate the performance of the detection algorithm and it represents the average of the precision and the recall [cit] . therefore, the f1-measure is higher and the system performance is better. we can find from fig. 7 that in both outdoor and indoor scenarios, the fpr and the fnr are lower than 10% as well as the f1-measure which can reach more than 90% and the proposed detection algorithm meets the requirements of activity extraction. compared to outdoor environment, the indoor fpr is higher and the fnr is lower, as a result of the indoor multi-path environment."
"the link between commercial wifi devices are used in our system to detect human activities and it is shown in fig. 2 . it is considered that the case that the transmitter is sending wifi frames to the receiver continuously. when an action emerges, the signal reflected by the human body will influence the signals traveled through the line-of-sight (los) path as shown in fig. 2 . receivers can measure small signal changes caused by human movements and apply these changes to recognize human activities by monitoring the wireless channel state. in order to extract the activity durations in online phase, adaptive detection threshold is estimated by cdhar in the offline phase. let h f j, t denote the amplitude of csi, f j and t represent the frequency at the jth subcarrier and the time moment, respectively. to analyze the fluctuation feature of the received signal, we calculate the mean and the variance of h f j, t at moment t in a sliding window with length l."
"where m j,t and v j,t represent the mean and variance of h f j, t in a sliding window, respectively. cdhar employs v j,t as the feature of the signal fluctuation."
"section 2 discusses the characterization of the cork substrate and the design requirements, while section 3 outlines the topology and the computeraided design of the siw cavity-backed slot antenna. measurements performed on a prototype in freespace conditions are presented in section 4."
"different from the previous systems which struggle to set the detection threshold manually and utilizes a simple classifier with time domain feature to reach har, cdhar obtains the adaptive threshold for the extraction of activity durations and complete recognition by the proposed ensemble method. the main contributions of this paper are summarized as follows."
"human activity recognition (har) is the vital technology nowadays, and it enables to use for realizing applications such as intelligent sensory games, smart homes, and human body posture monitoring and some other individual applications [cit] . channel statement information (csi) is a kind of fine-grained physical layer information with high resolution [cit] . therefore, csi-based har is popular with researchers and has been extensively studied [cit] . traditional har systems distinguish the difference of csi between action phase (the case that someone is interfering with the link) and stationary phase (the case that no one is interfering with the link) [10, [cit] and artificially set the threshold to detect the start and the end of the activity. however, when the environment varies the threshold needs to be reset or new activities need to be identified, which limits the adaptability and instantaneity of this system. because the har system is sensitive to unexpected errors, using this unique method will result in incorrect or incomplete activity duration extraction. in addition, the existing har system ignores the frequency domain feature, which is the key parameter of recognition. [cit] 2020: 36 page 2 of 10 sole classifier with poor robustness and low recognition rate, such as k-nearest neighbor (knn) classification algorithm [cit] and support vector machine (svm) [cit] . when identifying similar activities, the recognition accuracy of this sole classifier is not satisfactory. because ensemble learning is a way to train and combine multiple classifiers [cit], a recent work cited an integrated method in bagging-svm to identify cite r11. however, bagging-svm is replacing samples, so some samples may appear multiple times in the same training set, while others may be ignored, which will reduce the recognition accuracy."
"since kernel density estimation (kde) [cit] constructs the distribution model of the data according to the data itself instead of depending on the assumption of the distribution in advance, cdhar statistically estimates the v j,t extracted above and establishes the distribution model by kde."
"we design a succession of experiments to proof the relationship between the csi amplitude and target's moving speed. volunteers push a smooth plate respectively across the link at a slow and fast speed. they collect 5 groups of data in each case and extract the amplitude of csi. at the same time, the instantaneous phase of csi can be obtained through hilbert transformation. from fig. 8a, the phase of csi does not vary when the plate is stationary, while a b fig. 7 instantaneous phase-time graph of csi amplitude after descrambling. a outdoor. b indoor the phase quickly changes when the plate is moving. comparing fig. 8a from fig. 8b, we can find when the plate moves faster, the phase changes quicker. due to the phase is calculated by the csi amplitude, it can verify the csi amplitude reflects the target speed to some extent."
"the cavity width w c is fixed to 70.0 mm, which allows for sufficient impedance bandwidth to meet the specifications. the length of the non-resonant slot l slot is chosen 62.0 mm, much longer than half a wavelength. the other parameters, being halfcavity lengths (d sl and d su ), coaxial feed position (y coax ) and slot width (h slot ), are optimized using the frequency solver of cst microwave studio to meet the specified impedance bandwidth. the values of the optimized antenna dimensions are added to the caption of fig. 2 . the corresponding simulated reflection coefficient is shown in fig. 3 . the two hybrid modes resonate at 3.21 ghz and 3.55 ghz. simulation of the far-field properties indicates that the broadside gain varies no more than 2.0 db within the frequency band of interest."
"the rest of the paper is organized as follows. section 2 introduces the process of the proposed adaptive detection threshold and the detail steps of the proposed random subspace ensemble method. the extensive experiments and evaluation are shown in section 3. finally, we conclude the paper in section 4."
"we observe a significant difference in the distribution of csi amplitude variance between the action phase and the stationary phase as demonstrated in fig. 6 . a red solid dot marks the intersection of blue and orange line, and the adaptive detection threshold is represented by the abscissa value of it. from the ordinate range in fig. 6a and b, it can find that the value of the variance of csi amplitude in outdoor environment is smaller than that in indoor as a result of the muti-path. it also can know from it that as the existence of muti-path the threshold in indoor environment is higher than that in outdoor."
"a prototype has been constructed and characterized inside an anechoic chamber, to reproduce freespace conditions, using an agilent n5242a pna-x microwave network analyzer and an orbit/fr dbdr antenna positioning system. far-field gain patterns are measured with the gain comparison method using a standard gain horn model mi-12-2.6. fig. 3 compares the simulated and measured input reflection coefficients. a good agreement is observed. the prototype exhibits an impedance bandwidth of 700 mhz (20.9%), which is slightly higher than simulated. this is primarily a consequence of fabrication inaccuracies, and to a lesser extent of variation of the dielectric properties of cork material. the frequency band of interest, being 3.1-3.6 ghz, is fully covered. fig. 4 shows the simulated and measured farfield gain patterns at the lower (3.10 ghz), center (3.35 ghz) and upper frequency (3.60 ghz), both in the e-plane and h-plane. the most important performance measurements, derived from the radiation patterns, are summarized in table 1 . these include the radiation efficiency η rad, ftbr (defined as the ratio of the gain in the +z direction to the gain in the −z direction), maximum gain g max and half-power beam width (hpbw) in the e-plane and h-plane. again, a good agreement between mea-2.6 2. surement and simulation is observed. the measurements indicate that the maximum gain, gain pattern shape, e-plane and h-plane hpbw, and the other far-field properties only vary slightly over the considered frequency band, which is a key requirement to uwb antennas. the variation of the maximum gain is smaller than predicted by simulation."
"a composite cork agglomerate by amorim cork composites s.a., made of cork granules bound by polyurethane, with a density of 0.145 g/cm 3 and a thickness of 3 mm, is applied as antenna substrate."
"end if 11: end for 12 : end for offline activity databases have been built in the two scenarios. the structure of the database is the same in indoor and outdoor environments. for outdoor environment, the database includes the above 5 activities, each of which is repeated 30 groups (30 times). to obtain the detection threshold, we collected data for 10 min without taking any action. volunteers are invited to perform these 5 activities indoors and outdoors, and each activity will collect 100 groups."
"we show the framework of cdhar in fig. 1 . cdhar is composed of two parts: offline phase and online phase. in the offline phase, it extracts activity durations by using the proposed detection algorithm. then, a random subspace set classification method based on support vector machine is designed. in the online phase, it uses the same method as the classifier trained in the offline phase to extract features and classify activities."
"most existing har systems apply unique classifiers (such as knn, svm, and bagging-svm) to accomplish har. by using the same data, the proposed algorithm is compared with these common recognition algorithms to analyze the recognition performance of cdhar. as can be seen from fig. 11, the algorithm has good performance whether in an outdoor or indoor environment. in addition, the recognition accuracy can reach 90%. the performance of knn is dependent on the selection of neighboring points k, and it has insufficient robustness. what is more, the recognition rate will be further reduced if the feature sizes of various samples is uneven. due to the use of signals, svm is not stable and in the multi-path in indoor environment, the misjudgement rate increases significantly. because of the limitation of samples and sampling methods on bagging-svm, the training samples are uneven and lead to the insufficient training of a certain type of activity. from the results of our experiments, cdhar has good detection performance and can ensure the requirements for activity extraction. moreover, the recognition rate of proposed algorithm perform slightly well than intensely used conventional recognition method for existing har system."
"cdhar proposed a random subspace classifier integration method for classification. first of all, it divides the feature space into subspaces through the proposed selection method, which takes into account the balance of each feature. then, the svm is used to classify the subspace to generate a classifier. then, cdhar assigns weights to each classifier through the proposed weight allocation method. finally, the recognition result is obtained through the weighted sum of the results of each classifier."
"radio-frequency identification (rfid) systems based on ultra-wideband (uwb) technology, and more in particular impulse radio uwb technology (ir-uwb), offer many advantages over rfid technologies operating in the ultra-high frequency (uhf) band. these benefits include a better range resolution, more robustness to interference and multipath, and higher security. furthermore, the transmitter can operate at baseband, allowing a low power implementation [cit] . uwb signals are defined by the federal communications commission (fcc) as signals with an instantaneous spectral occupancy of 500 mhz or more, or a fractional bandwidth of 20% or more. the fcc puts the frequency spectrum ranging from 3.1 ghz to 10.6 ghz at the disposal of uwb transmission systems [cit] ."
"where k(·) represents the kernel function, h j and n denote the length and the number of sliding window, respectively. the type of k(·) does not affect the estimation result and cdhar chooses gaussian as the kernel function for the universality of gaussian function. then, cdhar calculates the optimal bandwidth by eq. (4) whose robustness and practicality have been tested [cit] ."
"in this paper, we propose a csi-based device-free har (cdhar) system that integrates wifi-sensing radar on a uav for har to overcome the shortcomings of existing har systems. first of all, cdhar estimates the probability density distribution of the csi of each subcarrier in the action phase and the rest phase according to the fluctuation of the signal, so as to obtain the adaptive detection threshold, and then, use this threshold to complete the extraction of the activity duration. in order to make full use of the frequency domain features, the discrete wavelet transform (dwt) is used to extract the time-frequency component features of each activity [cit] . in addition, a sampling criterion is proposed to choose subsets of components from the input feature matrix by cdhar. finally, svm is conducted on the subsets in order to generate a set of classifiers and giving each of them a weight by the weight assignment method, and the classification results are combined according to the obtained weight vector to get the final recognition result."
"from the user's perspective, they can find three major improvements of ds1.5 regarding the performance. first of all, ds1.5 is much faster than ds1.0. the overall optimization and the contextindependent pore model have sped up for a typical run 50 times with little simulation quality compromise, which allows the users to do large-scale read simulations. secondly, with the help of the lowpass filter, the simulated signals from the enhanced signal simulator can mimic the real-world signals much better than those from ds1.0. detailed results and comparisons can be found in supplementary section s3. thirdly, because of the multiple updates in ds1.5, the profile of the simulated reads from ds1.5 can keep up with that of the real reads generated from the newest nanopore chemistry."
"bandwidth normalized throughput of the systems. in fig. 4 we compare the performances achieved by the three algorithms, which include the dma of section iv-b, the ca described in section iii-b and another benchmark, namely random algorithm (ra). in the context of the ra, each pu makes an offer α l,k to a cu, which is randomly selected from its preference list. the cu chooses the specific matching pair that provides an increased rate to itself and discards the one having a lower rate. more specifically, the α l,k is chosen in order to maximize the optimization weight. in fig. 4, we have considered the scenario for case 1 of eq. (9) as introduced in section iii. in our simulations, we used the optimal power allocation factor for the ra according to different objective functions as shown in eq. (9), eq. (10) and eq. (11)."
"in this paper, we have applied matching theory to solve the user pairing and power allocation problem in the cr-noma systems. in our proposed distributed matching algorithm, the pus trade the available power with the cus by negotiating the power allocation coefficients, which guarantees that the rate requirements of both the pus and cus are satisfied. we have shown that the proposed dma results in a stable matching and implement a low complexity. our numerical analysis has revealed that the dma achieved a better performance than the ra benchmark scheme, and it is close to the optimum ca. importantly, we have also shown that the cr-noma system can achieve a significant performance advantage over the oma system. moreover, we have investigated a practical attcm for the cr-noma system, and the system's throughput can be further improved according to our simulation results."
"based on definition 1, 2, 3 and 4, we now proceed to prove that our dma as shown in section iv-b constitutes a stable matching."
"1 this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. [cit] ), bi-directional long short-term memory (bi-lstm), which can capture both local and context information of the input sequences, to model the relation between the input sequences and the corresponding raw signals. such designs can incorporate the error profile into the simulated signals and reads implicitly, which has been proved to benefit the simulation performance greatly [cit] ."
"similarly, each cu also has its preferred pu list. when a cu transmits on the spectrum band occupied by the preferred pu, its achievable transmission rate should be higher than its minimum rate requirement r cu k,req . thus, the preference list for cu k is given by:"
"wei liang received the m.sc. and ph.d. degrees in wireless communication from the university of southampton, southampton, u.k., [cit], respectively. she is currently a research fellow with lancaster university. her research interests include adaptive coded modulation, network coding, matching theory, game theory, cooperative communication, cognitive radio network, and nonorthogonal multiple access scheme."
"the received snr at cu k can be expressed as: where the term α l,k is the power allocation coefficient of pu l, p s is the transmit power emit from the bs and the pathloss is detailed in section ii. the quasi-static rayleigh fading channels between the bs and pu l is denoted as h b, p l, while this between the bs and cu k is represented by h b,c k . the channel gains are independent of each other."
"although the first version of deepsimulator (ds1.0) has been recognized and used by a number of users (https://github.com/ lykaust15/deepsimulator) [cit], there is still a large room for improvement. for example, though the final simulated reads have almost the same error distribution as the real reads, for some sequences, the divergence between the simulated raw signals and the real signals can be large, which can be inconvenient for the users who care about the signal outputs. in addition, the nanopore technology has evolved greatly since ds1.0 was released. it is thus necessary to update ds significantly to accommodate those changes, such as the extended reads' length. here, we present a substantially updated version of ds, deepsimulator1.5 (ds1.5), which is more powerful, quicker and lighter than ds1.0. in this new version, we have updated all the three modules substantially. regarding the sequence generator, we updated the sample read length distribution to reflect the newest real reads' features. in terms of the signal generator, which is the core of ds, we added one more pore model, the context-independent pore model, which is much faster than the previous context-dependent pore model. furthermore, to make the generated signals more similar to the real ones and to make the simulator flexible enough to simulate signals with variant qualities reflecting the real-world complex situations, we added a low-pass filter to post-process the pore model signals. as for the basecaller, we added the support for the newest state-of-the-art basecaller, guppy. unlike albacore, guppy can support both gpu and cpu. in addition, multiple optimizations, related to multiprocessing control, memory and storage management, have been implemented to make ds1.5 a much more amenable and lighter simulator than the original ds1.0."
"the channel h is defined in eq. (1) and eq. (2). the channel estimation errorñ is assumed to be independent identical gaussian distributed obeying the standard gaussian distribution of zero mean. the degree of csi estimation errors is governed by the channel estimation factor (cef), namely w (db) defined by:"
"as shown in fig. 5, for both two optimization objective functions, maximizing the average throughput of matched pus and cus, and maximize the throughput of matched cus, the ca achieves the best performance. we observe from fig. 5 that, the performance of our dma is close to that of the ca, which implies the dma provides a promising suboptimal solution to the problem in eq. (8) . however, the ra achieves the worst performance among the three algorithms in the cr-noma system. once again, fig. 5 illustrates our cr-noma system can outperform the oma system under all cases considered."
"the key contributions of the paper are summarize as follows: 2) we propose a distributed algorithm for the user pairing and power allocation in the cr-noma system based on the 1 in the cr inspired noma networks, the weak user can be viewed as the primary user (pu) and the strong user can be seen as the cognitive user (cu) [cit] . the rest of the paper is organized as follows. the system model of the cr-noma network is outlined in section ii. the general problem of the user pairing and power allocation is discussed in section iii. the proposed distributed approach for solving the user pairing problem is introduced in section iv. a practical adaptive coded modulation scheme is described in section vi. the theoretical performance of the proposed cr-noma scheme is evaluated in section v and the practical performance of the attcm aided cr-noma scheme is investigated in section vi-b. finally, our conclusions are presented in section vii."
"we set the rate of the pu l using conventional oma-cr as the minimum rate requirement, namely r pu l,req, and it can be expressed as:"
we use the notation γ pu r to refer to the instantaneous received snr of the link between the bs and the pu l and γ cu r represents the instantaneous received snr of the link between the bs and the cu k . the received snr at pu l is given by
"in this section we investigate the practical design of the cr-noma system advocated using attcm. employing ttcm has the advantage that the system's effective throughput can be increased upon increasing the code rate, when the channel-quality improves. additionally, the bit error ratio (ber) performance of the system may be improved when ttcm is used [cit] . the ttcm encoder comprises two fig. 9 . the ber versus snr r performance of ttcm using a frame length of 120,000 symbols, when communicating over rayleigh fading channels. four ttcm iterations were invoked."
"specifically, the pu is willing to perform noma with the cu only if it can achieve a higher rate in comparison with the conventional orthogonal multiple access (oma) transmission [cit] ."
"in addition to the aforementioned core updates, which are mainly made to improve the simulation quality, we have performed the fig. 1 . the overall workflow of deepsimulator as well as the differences between ds1.5 and ds1.0. in brief, the deepsimulator framework contains three modules: sequence generator, signal generator and basecaller. as shown in the last two rows, ds1.5 is significantly improved from ds1.0, with a greatly enhanced sequence generator and signal simulator, multiple new components as well as numerous optimizations. more discussions about the improvement of ds1.5 over ds1.0 can be referred to section 2, supplementary sections s4 and s5 following updates to improve the user experience. firstly, we simplified the installation process: with only one command and no more configurations, the entire installation can be done. secondly, we added threading management so that the users can control the resources allocated to the simulator. thirdly, memory and storage management are optimized. the execution of ds1.5 is much lighter than that of ds1.0. on the other hand, all the intermediate results can still be outputted with optional parameters specified, if the users are interested in investigating them. fourthly, we have refined the user interface as well as the overall code structure to make the code more readable so that the users can extend the tool or develop customized tools based on it. in addition, to help the users get used to ds with minimum efforts, we have provided multiple case studies with code in supplementary section s6 and the code repository of ds1.5 on github."
"different to the ca as described in section iii, we motivate to use the matching theory to find a suboptimal solution of eq. (8) . in the context of the ca, the bs needs to know all the csis of all the pus and cus. additionally, in our proposed dma, there is no central controller and does not require the bs to know the full csi of users, which reduces the system complexity. instead, the bs only needs to have the rank information of channels, and is able to broadcast this information to pus and cus for operating distributed user paring and power allocation. then, pus and cus can construct their preference list without any csi. the key idea of this distributed algorithm in case 1 scenario is that each pu make an offer to its most preferred cu from its preference list. then each cu has right to accept or reject these offers. when all pus make their offer once, one round of proposals is preformed. in order to construct the user pairs among the pus and cus in the considered cr inspired noma networks, the pu having a poorer channel condition matches with the cu having a better channel condition. then we distinguish the matched pus and cus in the power domain, aiming to improve the total sum-rate of the pus and cus. additionally, each cu and its paired pu transmit simultaneously in appropriate different power levels by exchanging their information. the bs allocates the initialized power allocations to the pu and cu, which the pu would get more power than the cu. after that, the pus and cus negotiate with each other for a specific value of power, i.e. α l,k, as introduced in section iii. employing the dma for efficiently representing the interaction among the pus, where each pu chooses its allocation independently of the others."
"the pair (pu l, cu k ) is referred to as a blocking pair for the matching μ, if both pu l and cu k would prefer to be matched with each other, but they are not matched under the current matching μ [14, p. 21] . both pu l and cu k achieve a higher rate if they are matched in comparison with their current matching partners under μ at a given powerallocation factor α l,k . this mathematically implies that:"
"in this work, we reported a new version of the previously published work on simulating the nanopore sequencing, deepsimulator1.5. in this updated version, we have updated all the three modules of deepsimulator significantly with several crucial overall optimizations, resulting in a more powerful, quicker and lighter simulator. this major update can remarkably broaden its applications in largescale sequencing simulations as well as studies focusing on the nanopore signals. in the future, we will further equip deepsimulator with the capability to simulate rna sequencing and dna modifications [cit] ."
"by considering the three cases of the optimization problems as described in eq. (9), eq. (10) and eq. (11), the setting of α cen l,k in eq. (12) are derived as:"
"then cu k discards its current matching in favor of the new matching. this is because a larger value of α l,k leads to a lower achievable rate of the cu, which is refer to eq. (4). moreover, the rejected pu cur will update the value of the power allocated factor by setting it to"
"additionally, fig. 5 shows the average throughput of matched pus and cus as well as the sum-rate of matched cus versus the number of pus in the cr-noma system. in fig. 5, we consider the performances for the two optimization objective functions, which are the case 2 of eq. (9) and case 3 of eq. (11) as described in section iii. the average sum-rate of matched cus is formulated as: then the average sum-rate of matched pus and cus is given by:"
"the conditions (a) and (b) ensure that the minimum rate requirements of the pus and cus can be achieved. the condition (c) specifies the range of the power allocation factor of the pu. therefore, conditions (d) and (e) ensure that each pu (or cu) will only be matched to one cu (or pu). the condition (f) states that the corresponding matching matrix entry is either 1 or 0. conditions (g) defines the total number of available frequency bands n f is equal to the number of pus. without loss of generality, we assume that each pu occupies a single spectrum band. moreover, the choice of the weight w (α l,k ) of eq. (8) depends on the objective function to be optimized. we consider three cases to set up the relative optimization functions, which are described below:"
"after obtaining the signals produced by the signal generator, the next step is to translate the signals into the final reads, which correspond to the final sequence outputs in the real experiment. although the users can feed a customized basecaller to ds, based on our experience, the users tend to use the default basecaller. previously, the default basecaller of ds1.0 is albacore. [cit] (lc19), the nanopore tech has officially released a more powerful basecaller, guppy. to cope with this evolution, we added both the gpu and cpu versions of guppy into ds1.5 and made the gpu one the default basecaller (https://github.com/lykaust15/deepsimulator/ issues/20). at the same time, we preserved the option to use albacore, in case the users need to do so."
"in order to find the optimal solution, we use the centralized algorithm (ca). we consider all possible matching pairs of the pus and cus, and then select that particular matched pair, which has the maximum sum-rate according to our intended objective functions as shown in eq. (9), eq. (10) and eq. (11) . by referring to eq. (18), the centralized solution requires an exhaustive search over all possible matching pairs and the power allocation combinations. specifically, the ca relies on an exhaustive search method that imposes the highest number of operations for the sake of finding the optimum solution. therefore, the ca has the highest complexity. additionally, the amount of complexity required for this ca increases with the number of users, which may become quite high, rendering it impractical. therefore, we will propose a distributed approach in association with the matching algorithm as described in section iv."
"note that, the objective function of eq. (8) is a function of continuous variables α l,k and binary variables m l,k . the objective function in eq. (8) can be decoupled with respect to two entries, which are the optimal power allocation matrix p and the optimal matching matrix m [cit] . hence we solve this optimization problem in two steps in section iii-a."
"and then it will reconstruct its preference list based on α * l cur,k and repeat the matching process. this algorithm aims to find the specific power allocation factors, which can be accepted by both the pu l as well as the cu k, and the algorithm is terminated when each pu has found its appropriate matched partner, provided that both their rate requirements can be satisfied. similar to the dma of case 1 as shown in table i, when we consider the case 2 scenario, the cus become the proposer and the pus are the selectors. the details are described in table ii . additionally, when we consider the case 3 scenario, the pus and cus would take turn to be the proposers depending on their current avenue as shown in table iii . note that, we have some particular situations that two players may compete with each other for appealing the same choice. during the competition, each player reduces its own power allocation factor, and this competition completes when one of the user's achievable rate is inferior to its minimum rate requirement. then we allow the losing user to select its surplus best choice from its current preference list."
") times for each of the (k − 1) cus in its preference list. then, the pu finds a final matched cu when the pu makes the last offer to the cu in the bottom of its list. for this extreme case, the number of operations required should be"
"the complexity required by the ca and the conventional dma will be discussed in this section in terms of the number of operations [cit] . the optimal ca requires the exhaustive search. all possible matching pairs over pus and cus are searched, and then the most profitable matched pair is selected according to the objective function as discussed in eq. (8) of section iii. the complexity of the ca can be expressed as:"
"definition 1: we define a matching μ as a one-to-one matching of the set l ∪ k . we refer to μ(l) and μ(k) as the partner of l and k, respectively. then we have"
"ds was designed to simulate the entire nanopore sequencing procedure, including sequence generator, raw signal generator and basecaller. given the target genome sequence, the sequence generator samples sequences from the genome, which correspond to the dna segments that pass through the molecular pore in the real experiments. although this module is conceptually simple, we have included the following updates into ds1.5 to meet the needs of different users. previously, by default, this module can only sample the linear genome. now, we equipped it with the power to sample the circular genome or generate the reads without sampling. furthermore, based on the feedback of the users (https://github.com/ lykaust15/deepsimulator/issues/13), we have made it easier to control the number of sampled reads and the coverage. [cit] have covered most of the circumstances in nanopore sequencing, the overall average read length has become longer since the release of ds1.0. to accommodate this (https://github.com/ lykaust15/deepsimulator/issues/21), we have added an option for the users to specify the desired mean read length with the distribution still fitting the real case. more detailed discussion about this module can be referred to supplementary section s3.1."
"it can be seen from eq. (27) and eq. (28) that, the ca has a significantly higher complexity compared with the dma, since the complexity of ca increases exponentially by the number of pus or cus."
"our problem can be modeled as a two-side one-to-one matching problem [cit], in which a set of pus will be matched with a set of cus. we consider a canonical matching problem [cit], where the preferences of one player set depends on the other player set. specifically, before the information exchange (proposals), each player is required to merely collect the information from the opposite set users they are interested, and performs a ranking according to its preferences. in the considered cr-noma system, the bs broadcasts the ranking information of the channels to all the pus and cus. according to the objective functions presented in eq.(9), eq.(10) and eq.(11), we treat one sets of users as the proposers and the opposite sets of users as selectors. the users who to be the proposers or/and selectors depend on the consideration of the objective functions. in case 1 of eq.(9), pus are act as the proposers and cus are the selectors, then before any offer is made to the cus, each of the pus construct a preference list of cus, which can satisfy the pu's rate requirement. thus, the preference list for pu l is given by:"
"moreover, we have a minimum rate requirement of the cu k, namely r cu k,req, in order to make sure cu k can benefit from accessing the pu's spectrum band. furthermore, the minimum rate requirement of each cu is dependent on its channel conditions."
"in basic discrete event simulation, which we also call entity-relationship (er) simulation, we deal with two basic categories of entities: objects and events. a simulation model defines a number of object types and event types, each of them with one or more properties and zero or more functions (to be used for various kinds of computations). there are two different kinds of event types: those that define exogenous events (typically with some random periodicity) and those that define caused events that follow from the occurrence of other events. the state of the environment (i.e. the system state) is given by the combination of the states of all objects. environment rules define how the state of objects is changed by (and which caused events result from) the occurrence of an event."
sensor -provides information on up to 16 objects/avatars found within a specified range; events of this type have to be triggered by invoking the active perception procedure llsensor no_sensor -provides the information that there are no objects/avatars within the specified range touch -provides the information that the object has been clicked by a user/avatar at_target -occurs when a target position (set before with lltarget) is reached collision -when a collision with another object occurs land_collision -when a collision with land occurs timer -occurs after some time span set before with lltimer alle these events can be handled in an object's script by providing suitable code to be executed in response to the occurrence of an event of such a type (such a section of code is often called an event handler).
"for creating the objects and agents of an aor simulation scenario in an opensim world, a special opensim module has to be developed. this module would read an aorsl file and create corresponding sl:objects, with appropriate lsl scripts (defining their behavior), and add them to the inventory of a \"master object\" representing the simulation scenario. this master object is put in-world by the module, wagner where it listens to specific commands from an avatar for starting, stopping or resetting the simulation. on simulation start, the master object creates (\"rezzes\") the required sl:objects from its inventory."
"the copulas have been increasingly popular for modeling statistical dependence in multivariate data and have been applied to many areas including finance [cit], medical research [cit], econometrics [cit], environmental science [cit], actuarial science [cit], just to name a few. a key feature of copulas is that they provide flexible representations of the multivariate distribution by allowing for the dependence structure of the variables of interest to be modeled separately from the marginal structure."
"in each simulation step, all those rules are fired whose triggering event types are matched by one of the current events. the firing of rules may lead to updates of the states of certain objects and it may create new future events to be added to the future events list. after this, the simulation time is incremented to the occurrence time of the next future event (if no continuous changes have been defined for the given model), and the evaluation and application of rules starts over."
"like aorsl, sl also makes a distinction between objects and agents. however, in sl an agent is not a special object that interacts with its environment, but rather, together with its associated avatar, it represents a human user. for avoiding terminological confusion, we will identify the sl term \"agent\" with \"avatar\". so, the basic entities in sl are avatars and objects, both of which are positioned on a region (\"sim\") that consists of land parcels, as depicted in figure 4 . there are different kinds of sl objects: active objects, as opposed to passive objects, have behaviors defined by scripts written in the linden scripting language (lsl). physical objects are active objects subject to the laws of physical kinematics and dynamics (rendered with the help of a physics engine)."
the important properties of the copula c of x are that the copula c represents the dependence structure of x on a quantile scale and it is invariant under strictly increasing transformations of the marginals.
"in the form of agent-based discrete event simulation, which we call agent-object-relationship (aor) simulation, we deal with three basic categories of entities: objects, agents and events. when we introduce agents, we have to make further distinctions between different types of events, as depicted in figure 3 . in particular, we need to consider perception events and action events in order to account for the perceptionaction cycle defining the foundation of agent behavior. an agent type is defined by means of: (1) a set of (objective) properties; (2) a set of (subjective) selfbelief properties as well as an optional set of (subjective) belief entity types; and (3) a set of reaction rules, which define the agent's reactive behavior in response to perception events (and internal time events"
"in the copula literature there are three commonly used estimation methods, the maximum likelihood (ml), the inference functions for margins (ifm) [cit], and the maximum pseudo-likelihood (mpl) [cit] . the ml and ifm methods require the specification of parametric models for the marginals. on the other hand, the mpl method uses the rank-based estimators for the marginals and is robust against misspecification of the marginal models [cit] . in this paper we employ the mpl method for estimating the proposed class of skew normal copulas."
"most of the commonly used copulas for applied research (for example, archimediean copulas and all metaelliptical copulas) assume that the dependence structure between the variables of interest is exchangeable."
"given a bivariate random vector (x, y), kendall's tau, which is one of the commonly-used concordance measures, is defined as the probability of concordance minus the probability of discordance [cit], i.e.,"
"avatars and active objects can communicate with each other via a mechanism for broadcasting simple string messages, called \"chat\". the distance that a broadcast message can be heard depends on the type of chat used (whisper, say, shout, regionsay). there is no support for point-to-point communication and for typed messages. however, using specific channel numbers (and possibly further filtering techniques), a kind of point-to-point communication can be achieved. for receiving chat messages, one or more lllisten actions, setting one or more filters, have to be performed first. after that listen events may occur, providing the chat messages received according to the current filter setting."
"the upper level ontological categories of aor simulation are objects (including agents, physical objects and physical agents), messages and events, as depicted in figure 2 . according to this upper-level ontology of aor simulation, agents are special objects; for simplicity it is common, though, to say just 'object' instead of using the unambiguous but clumsy term 'non-agentive object'. notice that only objects, but neither events nor messages, have a state that may change over time."
"t be the multivariate normal distributed random vector with standard normal marginals, independent of z ∼ n(0, 1). the joint distribution of z and z is given by"
"both the behavior of the environment (its causality laws) and the behavior of agents are modeled with the help of rules, which support high-level declarative simulation modeling."
"in future work, we plan to consider two more use cases. the second use case are single-user participatory simulations where a user may interact with a simulation run via user interface events. in this case, the user interface events (such as clicking the mouse or pressing a key on the keyboard) have to be mapped to the action repertoire of an sl/opensim avatar. the third use case are multi-user participatory simulations where many users may visit our sl/opensim region and interact with a running multi-agent simulation scenario consisting of a number of passive objects and artificial agents. here, the goal is to support the authoring of multi-agent simulation scenarios where users can freely choose the agent over which they want to take control. this requires a modification of the association between a user and her avatar, which is currently a frozen one-to-one association in sl/opensim. based on our initial experiments, we have identified the following issues that need further investigation:"
our first experiments with using the sl/opensim platform for implementing and running discrete event simulation (des) scenarios have shown that this platform allows to deploy and publish des models such that their runs can be visualized with 3d graphics and observed by any visitor of the region on which they have been deployed. the added value of using sl/opensim for publishing a des model results from:
"transformations of random variables, while the pearson's correlation is unaffected only by the linear transformation. thus, the kendall's tau is useful in comparing association/dependence structure in the data computed under different copulas. note that in this paper we numerically compute kendall's tau for the skew normal copulas using the following formula and the numerical integration function 'adaptintegrate' [cit] in r [cit],"
"most of the commonly used copulas are exchangeable, which means that the value of the copula is invariant under permutations of its arguments. for some practical situations where one component of the variables influences the other one more than the other way around, exchangeability assumption on copula is not suitable. this is because the dependence based on exchangeable copulas cannot distinguish between components of the variables."
the exchangeability assumption on copulas is too restrictive for some applied situations where one component of the variables influences the other one more than the other way around. in the bivariate case exchangeability means that the conditional probability distribution of the first variable given the second variable equals the conditional probability distribution of the second variable given the first variable.
"the non-exchangeable copulas are the fundamental tools to analyze the non-exchangeable/asymmetric dependence structure between multiple variables. we proposed a flexible class of multivariate skew normal copulas to model the high-dimensional data with non-exchangeable dependence patterns and developed the efficient block coordinate ascent algorithm for the parameters estimation. as the future work, we will apply the class of skew normal copulas to analysis of directional dependence using the regression models. the non-exchangeable copula based regression enables us to study the directional dependence stemming from not only marginal behavior of variables, but also the joint behavior of them."
"in this paper we propose a flexible class of multivariate skew normal copulas to model high-dimensional nonexchangeable dependence patterns. the proposed skew normal copula derived from the multivariate skew normal distribution [cit] has the two sets of parameters capturing non-exchangeable dependence between the variables of interest, one for correlation between the variables and the other for skewness of the variables. depending on the restrictions on these parameters, the proposed multivariate skew normal copula produces six parsimonious (nonexchangeable/exchangeable) copulas. we also propose using the block coordinate ascent algorithm to efficiently estimate the parameters in the proposed class of multivariate skew normal copulas. instead of estimating all parameters simultaneously, the introduced algorithm partitions the parameters into two disjoint blocks, one for the correlation matrix and the other for skewness parameters, and update block by block. this paper is organized as follows. in section 2, we briefly review the concept of the multivariate copula and its non-exchangeable property. section 3 proposes a class of the multivariate skew normal copulas that can capture various (non-exchangeable/exchangeable) dependence structures. in section 4 we introduce the block coordinate algorithm for estimating the proposed copulas and discuss its convergence property. section 5 illustrates the proposed class of skew normal copulas with a real data example. we end this article with a discussion in section 6."
"we have shown how aor simulation scenarios can be rewritten as sl/opensim scenarios. our approach facilitates transferring basic and agent-based discrete event simulations to the sl/opensim platform. the most basic use case is the one where such simulations are provided within sl/opensim such that they can be started and observed by any user visiting the world under consideration. this could be useful for teaching. more advanced use cases involve realizing participatory simulation scenarios with sl/opensim, which is a topic for future work."
where φ(·) and φ(·) are the pdf and the cdf of a standard normal variable. [cit] introduced the transformation method and the conditioning method for constructing the multivariate skew normal distribution. the transformation method provides the multivariate extension of eq. (2) utilizing the following lemma.
an entity type is defined by means of a set of properties and a set of functions. there are two kinds of properties: attributes and reference properties. attributes are properties whose range is a data type; reference properties are properties whose range is another entity type.
"we here briefly review the multivariate non-exchangeable copula. for the general copula theory, see [cit] states that if the marginals of x are continuous, then there exist a unique copula c such that"
"the main goal of this article is to discuss possible mappings from an aor simulation scenario to suitable sl/opensim code, such that the mapped scenario can be run with sl/opensim. in this approach, the aor simulation scenario represents a platform-independent model that can be transformed into various platform-specific models, including an sl/opensim model. for gathering some first experiences, we have re-implemented two aorsl scenarios as sl/opensim scenarios. the first scenario represents a simple car traffic model with a one-dimensional circular space model. the second scenario is about bugs moving around in a grid space. this investigation is just a first step that has resulted in the preliminary mapping shown in table 1 . further analysis has to be done before a first complete-enough transformation can be defined. whenever we want to indicate the vocabulary from which a term comes, we use the xml namespace prefix syntax. for instance, \"sl:agent\" and \"aors:agent\" denote two terms having the same local name (\"agent\") but being distinct due to the fact that they are defined in two different namespaces: the second life vocabulary and the aor simulation vocabulary."
"kersting proposed a method to estimate voltage drops by assuming a constant load density in three typical geometric configurations, without considering three-phase imbalance [cit] . these geometric configurations can be converted into the corresponding load current distributions along the feeder."
"this paper proposes a novel scalable methodology for voltage-driven reinforcement cost (vrc) estimations where the level of information required for an accurate three-phase power flow study is not available. the methodology consists of a novel general vrc model and five novel specific vrc models. the five models incorporate five typical load current distributions (i.e. the uniform, head-dominated triangular, tail-dominated triangular, trapezoid, and triangular-rectangular distributions) into an invariant equivalent impedance matrix for a straightforward non-iterative estimation of the vrc. the following conclusions are drawn from the case study:"
"if we premultiply and postmultiply r(n, m) with unitary matrices directly, r(n, m) cannot be transformed into real-valued, because the matrix d(n, m) is complex. therefore, we need to construct a new matrix associated with r(n, m) before unitary transformation to guarantee this property. let us define:"
"to avoid peak searching, to retain the 2-d doa estimation real-valued and reduce the computational complexity, we develop a simple implement of the 2-d unitary esprit based on the 1-d solution [cit] ."
"it has been proven that r(n, m) is full-rank, provided the values of p and q are selected properly. in this case, the rank of r(n, m) equals the number of incoming signals. therefore, lots of high resolution methods for 2-d doa estimation of the uncorrelated or partly correlated signals can be used."
"the obgsm tool works under the assumption that the semantic layer is used to enhance gsm, but not to govern it. in fact, the construction of the rts is handled internally by gsmc, it is not possible (at least for the time being) to prune it so as to remove inconsistent states. hence, obgsm must assume that all the states in the rts are consistent with the constraints of the semantic layer. this can be trivially achieved by, e.g., avoiding to use negative inclusion assertions in the tbox, which are the only source of inconsistency for owl 2 ql. if inconsistent states can be generated by the gsm specification, the strategy of delegating the verification to gsmc as a black box cannot be followed directly. one possible solution to this is to minimally change the gsmc implementation by introducing a test to detect states that should not be added to the rts during the construction, then implementing it as a satisfiability check wrt the ontology. the other possible solution is to consider fragments of the verification logic, and investigate whether the check for consistency can be embedded in the formula to verify, so as to avoid any impact on gsmc. these scenarios provide us with interesting problems for future investigation."
the method has the prospect to be scaled up to a utility level due to: 1) its non-iterative nature as opposed to powerflow-based methods; 2) the ability to account for a variety of load current distributions; and 3) the suitability where the level of information is insufficient to support feeder-byfeeder power flow studies. the next stage of the research will be applying the method to a utility scale with millions of feeders.
"step 4. conduct evd of the complex-valued matrix, υ u + jυ v . extract u k and v k from the eigenvalues by equation (31). at last, estimate θ k and φ k using equation (32)."
"2) when i a (0) is greater than the currents of the other two phases (the percentage of deviation is positive), an increasing i a (0) causes a relatively fast decrease of the terminal voltage for phase a, causing a relatively fast increase of the vrc."
"in description logics (dls) [cit], the domain of interest is modeled by means of concepts, representing classes of objects, and roles, representing binary relations between objects. 2 in dl-lite r, concepts c and roles u obey to the following syntax:"
"given a sas s, we are interested in studying verification of semantic dynamic/temporal properties specified over the semantic layer, i.e., to be checked against the sts υ s s . as verification formalism, we consider a variant of first-order µ-calculus [cit], called µl eql a [cit] . we observe that µ-calculus is one of the most powerful temporal logics: it subsumes ltl, psl, and ctl* [cit] . the logic µl eql a supports querying the states of the sts through the first-order epistemic queries introduced in section 2. in µl eql a, first-order quantification is restricted to objects present in the current abox, and can be used to relate objects across states. the syntax of µl eql a is as follows:"
"step 3. compute e sϕ as the k dominant eigenvectors of ϕ and calculate t 1, t 2 through equations (25) and (29). conduct the svd of t 1, t 2 to obtain the right singular vector matrices, w u, w v, and get υ u, υ v from equations (28) and (30), respectively."
"sections iv. c -d prove that the balanced condition corresponds to the minimum vrc; if any single variable deviates from the balanced condition whilst other input variables remain unchanged, the vrc will increase."
"where q is a ucq. the answer to q over (t, a) is the set ans(q, t, a) of tuples of constants in adom(a) defined by composing the certain answers ans (q, t, a) of ucqs q through first-order constructs, and interpreting existentials as ranging over adom(a)."
"suppose that the demand currents of the three phases follow a triangular-rectangular distribution with the same β factor, where uniform and tail-dominated triangular distributions are the special cases. the vrcs are computed under a range of β factors, as depicted in fig. 8 . fig. 8 shows that the vrc increases with β, because an increasing β corresponds to an increasing skewness of the demand current distributions towards the feeder terminal, thus increasing the voltage drops of the three phases and the vrc."
"in dls, the domain knowledge is split into an intensional part (tbox), and an extensional part (abox). specifically, a dl-lite r ontology is a pair (t, a), where the tbox t is a finite set of (concept and role) inclusion assertions of the forms b c and u 1 u 2, and of disjointness assertions of the forms disjoint(b 1, b 2 ) and disjoint(u 1, u 2 ). the abox a is a finite set of facts (membership assertions) of the forms n (c 1 ) and p (c 1, c 2 ), where n and p occur in t, and c 1 and c 2 are constants."
"in this case, the demand current of each phase is decreasing linearly with the increase of the distance from the substation, as depicted in fig. 4 . the demand currents are given as the functions of the distance l from the substation:"
"three-phase imbalance causes inefficient uses of low voltage (lv) network assets. an imbalanced allocation of singlephase loads in a three-phase lv network causes voltage and current imbalance [cit] and uneven voltage drops along the feeder [cit] . at the same time, a long-term load growth causes a long-term decrease of the terminal voltages in lv networks [cit], the majority of which are neither monitored nor controlled [cit] . when long-term load growth is coupled with three-phase imbalance in passive lv networks, there will be a phase whose voltage drops to the statutory lower limit earlier than the other two phases, thus prompting the distribution network operator (dno) to take actionsa common practice is network reinforcements [cit], which bring a voltage-driven reinforcement cost (vrc). there are two major challenges associated with the vrc estimation: 1) the lack of visibility along lv feeders, as is the general case over the uk dno's service areas; and 2) vrc estimations at a utility level in future would require an efficient and scalable method, applicable from an individual feeder to millions of feeders. three-phase power flow studies are capable of computing the vrc for individual feeders with full smart metering data, customers' phase allocation data, and locationdependent impedance. but they are not applicable when these data are absent; and applying them to millions of feeders means a prohibitively high computational burden due to i) the extensiveness of lv networks with varying characteristics across different regions; and ii) the iterative nature of power flow analyses. to address the challenges, this paper focuses on the development of a scalable methodology for vrc estimations under severe data deficiency."
", satisfies equation (8), the matrix b u is real. combined with the real-valued f + f *, we can easily deduced that φ is real."
"(ii) a semantic layer, which contains an owl 2 ql ontology that conceptually accounts for the domain under study. (iii) a set of mapping assertions describing how to virtually project data concretely maintained at the relational layer into concepts and relations modeled in the semantic layer, thus providing a link between the artifact information models and the ontology."
"b(v k ) are both conjugate centro-symmetric, the same as the steering vector given by equation (6) . substituting equations (14) and (15) into equation (13), r y can be rewritten as:"
"it should be noted that network reinforcements are a common practice for dnos to address the voltage issues caused by the demand growth in imbalanced three-phase lv networks [cit] . such a practice is not necessarily the least-cost solution. in future, with increasing knowledge of customers' phase connectivity, it is possible to adopt alternative solutions to address the voltage issues and defer network reinforcements, e.g. phase balancing [cit], demand side managements, and energy storage [cit], and the use of static var compensator [cit] and other power electronic devices [cit], etc. the vrc of the conventional network reinforcement solution serves as a benchmark, with which the costs of alternative solutions can be compared."
the following real multiplications involved in the υ u and υ v achievement is listed in table 1 . denote the total number involved in it as c 1 . the computational order of svd is obtained by the chan svd [cit] . table 1 . real multiplications involved in the computations of υ u and υ v .
"other references on lv network investments [cit], lv network expansion planning [cit], and smart network planning strategies [cit] all assumed balanced three phases. reference [cit] estimated the voltage drops under balanced three phases, considering three typical load current distributions."
"in this case, the demand current of each phase is distributed evenly along the feeder, as depicted in fig. 2 . the demand currents are presented as the functions of the distance l from the substation:"
"it is worth mentioning that our proposed algorithm is fit for dealing with the estimation of highly correlated signals. in the situation where all the signals are uncorrelated or partly correlated, the method given in this paper will suffer degradation to some extent."
"the execution semantics of a sas s is provided by means of transition systems. while the temporal structure of such systems is fully determined by the transition relation of s, the content of each state in the system depends on whether the dynamics of sass is understood directly at the relational layer, or through the conceptual lens of the semantic layer ontology. in the former case, each state is associated to a database instance that represents the current snapshot of the artifact information models, whereas in the latter case each state is associated to an abox that represents the current state of the system, as understood by the semantic layer."
"it should be noted that the following derivation will be performed under the assumption that there is no noise existing in the received data, which can be seen from equation (11) . further study on the complex situation with spatially white noise will be carried out in section 5 through several simulations."
"an application of unitary transformation to 2-d doa estimation of coherent sources has been proposed in this paper. the decorrelation is performed based on the existing 2-d esprit-like method. while the computational load is significantly reduced by transforming the complex matrices into real-valued ones and conducting the evd with 1-d matrices, the 2-d uesprit-like method can also provide better performance in doa estimation by preprocessing the block hankel matrix using the forward-backward averaging-like method. computational analysis and simulation results have shown the significant reduction of computations and the dramatic low rmse in doa estimations. a less restrictive requirement for the array geometry is also provided to generalize the application of this method."
"this paper makes a substantial enhancement by incorporating five demand current distributions into an equivalent impedance matrix for the vrc estimation under three-phase imbalance. the five distributions are the uniform, tail-dominated triangular, head-dominated triangular, trapezoid, and triangular-rectangular distributions."
the proposed methodology applies to demand-dominated lv networks with a low penetration of renewable energy. future work will extend the methodology to consider a high penetration of renewable energy.
"it should be noted that the vrc models, whether general or specific, are applicable to demand-dominated lv feeders with a low penetration of renewable generation."
the words 'fast' and 'slow' are relative between the above two cases. the results demonstrate that the vrc is more sensitive to voltage imbalance than to current imbalance.
"owl 2 ql is a profile of the web ontology language owl 2 standardized by the w3c. owl 2 ql is specifically designed for building an ontology layer to wrap possibly very large data sources. technically, owl 2 ql is based on the description logic dllite r, which is a member of the dl-lite family [cit], designed specifically for effective ontology-based data access, and which we adopt in the following."
"since the temporal formalism supported by gsmc is a variant of the first-order branching time logic ctl [cit] with a restricted form of quantification across states, the µl eql a has been restricted accordingly in the tool 4 . this, in turn, required to suitably accommodate the mapping language so as to ensure that temporal formulae over the semantic layer correspond, once rewritten and unfolded, to properties that can be processed by gsmc. furthermore, in accordance to -ontop-, both the temporal properties and the mappings rely on the sparql query language to query the semantic layer."
"existing papers and reports on lv network reinforcement costs are mainly based on balanced three phases. voltage constraints are a key driver for network reinforcements (especially for rural networks), of which the costs were quantified on a utility scale under balanced three phases [cit] ."
substitute the characteristic impedance vector [z c∅ ] defined in (14) into (19) . the three non-zero elements of the equivalent impedance matrix are given by
"theoretically, the demand current distribution can be infinitely accurate; but in reality, the exact location-dependent impedance matrix z (x) and the distribution of demand currentsi̇ d∅ (x) are unavailable due to the high cost to obtain them and the excessive computational burden of accounting for them on a utility scale. even for an individual feeder, these data are not always available. this prompts the need to develop specific vrc models, which are derived by applying simplifications to the general vrc model. therefore, the general vrc model introduced in this section serves as the basis for the specific vrc models introduced in the next section."
"substitute (12) and the average impedance matrix per unit length z ave into (6). the three non-zero elements of the equivalent impedance matrix form an equivalent impedance vector, given by"
"define the equivalent impedance matrix as the one that would lead to the same voltage drops at the feeder end, given the same substation-side currents. the equivalent impedance matrix is defined in the form of"
the vrc models demonstrate suitability for cases with severe data deficiency and the prospect to be scaled up to a utility level with millions of feeders.
"where u p and u q use the definition of equation (7) or (9) . premultiplying and postmultiplying r y by u p,q, yields:"
"in the following, we will first derive an estimate of the order of real multiplications involved in each step. then, we will compare the computational order of our new method, namely the 2-d uesprit-like method, against that of the 2-d esprit-like method."
"in the last decade, the marriage between processes and data has been increasingly advocated as a key objective towards a comprehensive modeling and management of complex enterprises [cit] . this requires to go beyond classical (business) process specification languages, which largely leave the connection between the process dimension and the data dimension underspecified, and to consider data and processes as \"two sides of the same coin\" [cit] . in this respect, artifact-centric systems [cit] have lately emerged as an effective framework to model business-relevant entities, by combining in a holistic way their static and dynamic aspects. artifacts are characterized by an \"information model\", which maintains the artifact data, and by a lifecycle that specifies the allowed ways to progress the information model. among the different proposals for artifact-centric process modelling, the guard-stage-milestone (gsm) approach has been recently proposed to model artifacts and their lifecycle in a declarative, flexible way [cit] . gsm is equipped with a formal execution semantics [cit], which unambiguously characterizes the artifact progression in response to external events. several key constructs of the omg standard on case management and model notation 1 have been borrowed from gsm."
"where ic and d denote the future reinforcement cost and the discount rate, respectively. the general vrc model considers a range of factors: 1) the existence of different branch sections that have different impedance per unit length;"
"therefore, (β k, γ k ) can be expressed as (e ju k, e jv k ) . rewriting equation (1) in vector notation, we get:"
denote x as the distance between the point concerned and the substation. suppose the location-dependent impedance matrix is z(x). the distribution of demand currents along phase ∅ is denoted asi̇ d∅ (x).
"we compose ucqs using ecqs, i.e., queries of the query language eqllite(ucq) [cit], which is the fol query language whose atoms are ucqs. an ecq over t and a is a possibly open formula of the form"
"given a lack of smart metering data, equation (30) can be simplified by assuming that the k factors are the same for the three phases:"
"the invariability of the equivalent impedance matrix used in the vrc models is proven in this section: after n years' demand growth, the demand current and phase current are given byi̇ therefore, the equivalent impedance matrix is proved to be invariant under demand growth, i.e. it is not a function of the annual demand growth rate r. this is a very important feature as it allows the vrc to be calculated in a straightforward non-iterative way."
"since e sr y and b ′ both span the signal subspace of r y, as equation (16) has shown, there exists a unique, nonsingular matrix,"
"by reconstructing the matrix, [e s1, e s2 ], and performing the unitary transformation, the tls problem can be solved by computing the svd of the real matrix:"
"to overcome the challenges of data deficiency and scalability, approximations have to be made to the distributions of demand currents, thus leading to specific vrc models."
"it should be noted that z(x) on the right hand side of (5) considers both the transposed and non-transposed conditions of feeders. rather than being a physical impedance, each equivalent impedance on the left hand side of (5) represents the relationship between the substation-side current of the phase in question and the terminal voltage drop of that phase. such a relationship does not change when the load currents of the three phases grow by the same percentage each year, regardless of whether the three phases are transposed or not -this is proven in the appendix. the invariability of this relationship enables the use of the equivalent impedance matrix."
"the formal framework of sass has led to the development of the obgsm tool. obgsm assumes that the rts is obtained from artifacts, specified using the guard-stagemilestone (gsm) approach. the main task accomplished by the tool is the reformulation of temporal properties expressed over the ontology in terms of the underlying gsm information model. in particular, obgsm adopts: (i) the state of the art obda system -ontop- 3 to efficiently rewrite and unfold the epistemic queries embedded in the temporal property to verify. (ii) the recently developed gsmc model checker for gsm [cit] to accomplish the actual verification phase. gsmc is currently the only model checker able to verify temporal formulae over artifact systems."
the rest of this paper is organized as follows: section ii presents a general vrc model based on equivalent impedance matrix; section iii presents five specific vrc models where each model considers a specific load current distribution; section iv presents a case study; and section v concludes the paper.
the number of years it takes for the feeder-end voltage of phase ∅ to drop to the statutory lower limit is calculated by substituting (15) into (9) . the time horizon of the feeder is given by (10) . the vrc is computed by (11).
"lv networks experience non-trivial three-phase imbalance [cit] . numerous publications focused on power losses resulting from three-phase voltage and current imbalance [cit] . furthermore, a number of publications focused on the impact of voltage imbalance on the customer side, e.g. customers' induction motors [cit] . while power losses and potential damages to customers' appliances are both key issues from three-phase imbalance, the increased reinforcement costs are no less important. however, there is very limited work on lv network reinforcement costs considering three-phase imbalance. references [cit] mentioned the impact of three-phase imbalance on network reinforcements qualitatively. reference [cit] quantified additional reinforcement costs from three-phase imbalance, considering thermal constraints only. our recent paper on the quantification of the vrc for a typical lv circuit [cit], however, has the following limitations: 1) it was limited to a typical lv circuit as a combination of a transformer and a feeder where the neutral line current is zero for this specific combination; and 2) it assumed a uniform distribution of load currents."
"given a lack of smart metering data, equation (35) can be simplified by assuming that the beta factors are the same for the three phases:"
"the proposed method is developed in the 2-d scenario, which has been introduced in section 2.1. in order to resolve the rank deficiency problem caused by signal coherency, we first construct the following hankel matrix from equation (5)"
"each diagonal element of z eq∅ is a function ofi̇ a (0),i̇ b (0), andi̇ c (0), implicitly incorporating the mutual inductance and the impact of the neutral current, as explained later in this section."
"although we can apply the eigenstructure techniques to estimate 2-d doa based on the full-rank r(n, m), the computational burden is much heavier, because of the complex computations involved in it. in this note, we develop a 2-d unitary transformation method to reduce the complex computations to real ones."
"in this section, we present simulation results that compare the proposed method with several other 2-d doa estimations in the presence of a zero mean gaussian white noise. except the developed scheme, doa extractions and pairings in other methods are all performed using mmemp [cit] ."
"compared with equation (4), (16) can be viewed as a new array response matrix and f + f * as the equivalent covariance matrix of the incoming signals. notice that the achievement of r y is consistent with the forward-backward average processing [cit] . compared with r(n, m), r y decorrelates the coherent signals more thoroughly and, therefore, can provide higher estimation accuracy. since b(u k ) and b(v k ) in b ′ have the similar form as equation (6), we can make use of the 1-d real-valued processing-like equation (8) to perform the 2-d unitary transformation. define the 2-d unitary matrix as:"
"this framework has led to the development of a tool called obgsm, which relies on gsm as the artifact model, and on state of the art technologies for dealing with the ontology and the mappings, and for performing verification. we refer to an extended version of this paper [cit] for proofs and the application of obgsm on a real case study."
"the proposed methodology allows dnos to estimate vrcs under severe data deficiency -only substation-side voltages and currents are required. this is particularly useful in the uk where there is a lack of visibility along lv feeders [cit] . furthermore, it works not only for an individual feeder but is also scalable to a utility level because of its non-iterative analytical nature."
"where r denotes the annual growth rate of the demands. according to (8), the use of the equivalent impedance matrix is the key, as it allows the voltage drops to be expressed as the functions of phase currents. furthermore, the equivalent impedance matrix is invariant under demand growth, i.e. it is not a function of the annual demand growth rate r, as proven in the appendix. this enables a straightforward non-iterative calculation of the vrc."
"now this paper makes a fundamental upgrade to the methodology: we propose a novel scalable methodology for vrc estimations where the level of information required for an accurate three-phase power flow study is not available. the methodology effectively addresses the limitations of the previous work by incorporating the impact of the neutral current into an impedance matrix and considering more general distribution patterns of load currents. it consists of a novel general vrc model and five novel specific vrc models: five typical load current distributions (uniform, headdominated triangular, tail-dominated triangular, trapezoid, and triangular-rectangular distributions) are incorporated into an equivalent impedance matrix which is invariant under demand growth, thus allowing for a non-iterative estimation of the vrc."
"a general vrc model is proposed in this section, accounting for the location-dependent impedance of different feeder sections and a generic distribution of demand currents along each phase of the feeder. the model considers uneven voltage drops along the three phases, based on substation-side voltages and currents under peak demand. unless stated otherwise, all voltages and currents data used in this paper are monitored under the peak demand."
"since the computations of k 1, k 2 require a large amount of multiplies, we intend to simplify their expressions with lower dimensions. let j"
"in this case, the demand current of each phase increases linearly with the distance from the substation, as depicted in fig. 3 . the demand currents are given as the functions of the distance l from the substation:"
"the methodology is applied to an individual feeder, based on the substation-side three-phase currents and voltages data. the input data for the base case are presented in table 1 . suppose that the demand currents of the three phases follow a trapezoid distribution with the same k factor (uniform, tail-dominated triangular, and head-dominated triangular distributions are the special cases of the trapezoid distribution). the vrcs are computed under a range of k factors, as depicted in fig. 7 . fig. 7 shows that the vrc decreases with the increase of the k factor, because an increasing k factor causes the demand current distributions to be skewed towards the substation side, thus reducing the voltage drops of the three phases and leading to a decrease of the vrc."
"suppose that the demand currents monitored at the substation grow by a percentage r each year, reflecting a long-term demand growth. by the end of the nth year, the feeder-end voltage drops are given by"
a time horizon is defined as the number of years it takes for the feeder-end voltage to drop to the statutory lower limit. the time horizon of the feeder is the time horizon of the phase of which the terminal voltage first drops to the lower limit.
"with u 2k defined by equation (9) . note the eigenvectors are associated with the k largest eigenvalues of matrix, ϕ, as e sϕ . from equation (17), we have:"
"in this case, the triangular-rectangular (tr) distribution of demand currents on phase ∅ is depicted in fig. 6 . the demand currents on phase ∅ are given bẏ"
"in the absence of any intervention, the system stabilizes with high values of insurgency as would be expected from the aforementioned dynamics. to illustrate the framework's ability in handling 'what-if' scenarios, three possible counterinsurgent interventions were modelled: direct support for economic development (e.g., by restoring the infrastructure and building local economic capacity), direct support for security (e.g., by holding areas and training governmental security forces), and a combination of the two. in each case, the intervention was modelled by adding a concept in the fuzzy cognitive map via the concept map editor, and linking that concept to the ones that are directly impacted. consequently, the economic intervention increases the level of economic development (with weight 0.5) whereas the security interventions decreases the ability of insurgents to control the population (with weight 0.5), and the combination does both. the impact of these three possible interventions is summarized in table 1 by showing the improvements in terms of increase in economic development and decrease in rebelliousness, unemployment, and insurgent control. results highlight that, in isolation, economic or security interventions have little impact on rebelliousness. however, combining them can achieve a reduction of almost 10%. this is in line with recent military strategies such as the \"us national strategy for victory in iraq\", which pointed out the importance of intervening simultaneously on political, economical, and security aspects [cit] ."
"long-standing research in geometric modeling focused on the function representation (frep) 1 due to its feature set and natural 3d printing suitability. we also decided that existing accessible user interface systems were unsuitable for the project's needs, so we developed a new solution."
"to evaluate the system's real guidance performance, field tests with four visually impaired people are conducted. the characteristics of the four test subjects are listed in table 11 . all of the subjects use white canes as their usual mobility aids; the purpose of this field test is to evaluate whether the use of the proposed system will reduce the time required for the user to negotiate an unfamiliar pedestrian pavement. age vision level usual aid 1 male 26 low cane 2 male 31 none cane 3 female 28 none cane 4 female 30 low cane table 11 . participants' characteristics for field test"
2. a node describing a subtraction operation is added to the tree. the branches of this node are the cylinder's transform node from step 1 and the top node for the model. 3. the top node for the model becomes the subtraction node added in the previous step.
"fuzzy logic theory is a valuable mathematical technique to deal with the conflicting and uncertain evidence found in the wealth of testimonies, media reports, and other forms of 'thick description' . as described by li, fuzzy logic theory [cit] resembles human reasoning under approximate information and inaccurate data to generate decisions under uncertain environments. it is designed to mathematically represent uncertainty and vagueness, and to provide formalized tools for dealing with imprecision in real-world problems."
the core of the shiva project is interactive applications that allow users to manipulate the virtual objects in order to create shapes in a 3d environment. all the implemented software applications incorporate two main components: an accessible gui and an interactive solid modeling system that allows the manipulation of geometric shapes.
"our framework supports the integration of data from different sources so that analysts can understand conflicts and run 'what-if' scenarios for counterinsurgency scenarios. it also provides methodological support for scholars of insurgency in two ways. first, it allows for military theories, individually or synthetically, to be tested for consistency by exploring the implications of their suppositions. second, it allows for intriguing empirical phenomena to be encountered and explored as the disjuncture between the actual world and the world contained within the model. we expect that the use of our framework for these different endeavours will further drive its evolution, both through changes in software and refinement of its mathematical structure."
"as an example, for the drill operation (see figure 2b) the following steps are performed on the existing tree in order to modify the object based on the user's input:"
"many frameworks have been proposed to model diffusion in networks [cit], and they account for uncertainty in different ways. in the weighted generalized annotated program (wgap) framework, changes are expressed by rules whose probability reflects the certainty [cit] . for example, the rule supportinsurgents(a) 0.75 ← supportinsurgents(b) ∧ leader(b, a) states that if the village leader b for inhabitant a supports insurgents, then a will support the insurgents with 75% certainty. similarly, the linear threshold model (ltm) and the independent cascade model (icm) have rules for changes in the nodes' attitudes, and random thresholds are associated with these rules to model uncertainty [cit] . the multi-attribute networks and cascades (mancalog) framework provides more flexibility, the properties of nodes and edges have a weight whose uncertainty is represented by an interval that can be open or closed [cit] . however, a key distinction is that these frameworks are designed to operate once the values for the uncertainty have been specified: they are not made to take in (possibly contradictory) qualitative assessments and turn them into values based on the uncertainty."
"some of the more able students were invited to try the software during the development phase. this was introduced to them in a careful, controlled manner because their reactions to potentially unstable prototype software were unknown. in reality, the students were extremely enthusiastic and took particular delight in discovering software bugs."
"the whole algorithm is implemented using c++ on a windows platform. to test the performance of the algorithm, we attached a camera to a belt and fixed it to the user's waist, angled slightly downwards towards the road ahead of the user. the camera captures images of the road, which are then processed by the system software, which runs on a laptop computer carried in the user's backpack. the generated messages are turned into a synthetic voice and delivered to the user via a loudspeaker. the prototype system is shown in figure 19, and configuration of experimental platform is listed in table 5 ."
"on the urban test path, some low piles built to prevent illegal parking represented a very high threat for blind pedestrians using only the white cane. these situations are shown in figure 24 . in the second run on the urban path, the subjects equipped only with the white cane did spot the danger presented by these low piles. however, in the first run with the guidance system, these low piles could be detected much further ahead of the user, and verbal feedback given to help keep them away from those potential collision threats. after the field test, the test subjects all agreed that the system was capable of detecting and identifying obstacles effectively within a medium range, providing intuitive verbal feedback at appropriate times that was easy to interpret and act upon. a few limitations of the proposed system were also observed. the first limitation is the assumption of a flat road plane. the second is that the camera is required to be fixed on the user's body at a certain downward viewing angle, and camera parameters are required for top-view mapping."
"the message generator works with the fuzzy state estimator discussed in the previous section. it determines the message sets that are most suitable to be delivered to the user in the current state, and filters out other less necessary messages. the filtering rules are defined as shown in table 2 . in the \"danger\" state, a safe walking direction message must be acquired instantly, while in the other states it is more necessary to report obstacle positions in the surrounding environment in order for users to be able to maintain a safe walking direction by themselves. a message set example is shown in table 3 . the words in brackets are template words which can be changed according to the detection result. object types are classified into vertical types (including poles and blocks) and planar types (including curbs and piles), as discussed in section 2.4. rather than using metres to report distance, the number of average steps is used to enable more intuitive cognition. in the user motion set, the message \"large departure attention\" is given when the user deviates too far from the safe direction. if user speed is too fast in danger mode, \"please slow down\" will be prompted. on the other hand, if the user moves too slowly in safe mode, the system can also suggest that the user walks faster. if there are too many obstacles ahead, and insufficient safe space can be detected, the \"stop\" message may be delivered. as figure 18 shows, clock-face directions are used to give direction messages. clock-face directions are broadly accepted as a common way to indicate directions for blind people. the estimated free path is mapped from the topview domain (figure 16a ) to the original-view domain (figure 16b ), which is divided by projecting the top-half clock-face area (10 o'clock ~ 2 o'clock) onto the centre horizontal line in the original image space. the centre of the mapped free path on the centre line is defined as the safe direction indicator. the clock-face section into which it falls determines the safe direction to be suggested to the user. detected obstacle directions are also delivered in this way after mapping to the original image."
"top-view mapping is an inhomogeneous re-sampling process that has been widely used in applications like lane detection, mainly for the purpose of road geometry recovery. some researchers have also attempted obstacle detection on top-view images. the basic idea is to generate a difference image by associating two top-view images either spatially [cit] with a stereo camera or temporally via a single camera [cit] . on this difference image, planar patterns like road textures are removed, while high objects like vehicles are retained in the form of large clusters of non-zero pixels with a specific shape. while this approach is effective to detect vertical obstacles like vehicles on a highway, problems emerge when it comes to blind navigation in an urban environment. first, ground-level obstacles are removed on the difference image, which could be dangerous for the blind pedestrian. second, due to the low-speed, forward-rolling motion of pedestrians, an obstacle's blob patterns may not be prominent enough to identify them against noise on a temporal correlated difference image. in this paper, rather than using a difference image, the effects of top-view re-sampling and mapping on obstacle edges are studied, and several useful properties are modelled for the identification of obstacle edges in background clutters. in this section, this re-sampling process is re-formulated in horizontal and vertical directions, and its effect on the scale and connectivity of edges is discussed. the model of vertical direction re-sampling is illustrated in figure 2a . in figure 2a, cr is the real camera centre with sr as its image plane, while cv is the virtual top-view camera centre with sv as the virtual top-view plane. to figure out the re-sampling relationship between the sr and sv planes, the only parameters that are required are φ and θ. according to the geometrical description in figure 2a, for each point pv on the virtual top-view plane sv, the corresponding sampling point pr on the real image plane sr can be calculated based on the common projection point pg on the ground plane. as (1) shows, for each point i on the top-view plane, the corresponding sampling point h on the real image plane can be obtained:"
"another project objective was to allow the users to perform operations on geometric shapes in a virtual environment (sculpt virtual objects). to make the modeling core of the system extensible, the layout that works with representation and manipulations of the geometry should be universal. in our system we represent geometry in the implicit form by using the frep, 1 which represents geometric objects using continuous real functions over a set of coordinates. this representation lets us describe a vast number of geometric primitives and perform operations in the shape modeling system in a more simple and efficient way compared with other traditional representations, such as polygonal meshes. easy formulation allows us to work with traditional geometric primitives such as a sphere, box, and cylinder. beyond this, more complex geometric primitives such as polygonal meshes can be represented efficiently in the form of signed distance fields, 3 which are a natural subset of frep. traditionally, the main disadvantage of such a representation is that the object's geometry cannot be rendered using the common software for standard graphics hardware. instead, in our system we use direct rendering of the object in the form of real-time ray-casting through acceleration with graphics hardware. 4 the objects and operations are represented in the form of a tree structure that generates the defining function for the model. in the leaves of such a tree, we have the geometric primitives, while in the nodes we have operations over other nodes and leaves. 5 this allows us to perform operations in the modeling system by modifying the structure of the tree itself by adding and removing nodes. existing models are fully parameterized by modifying the values for the parameters of primitives and operations."
"the primary technical goal of the shiva project was to allow users with disabilities to create virtual and then 3d-printed sculptural artifacts. using the system, students have been able to successfully produce a range of objects, thereby validating the software and the process. figure 4 shows some results."
"within the shiva project, we developed two exercise applications, metamorphosis and totem pole, with different levels of complexity for both the user interface and geometric modeling sides."
"the user's physical needs were gradually identified and discussed, often using simple test applications, in a way that focused on the direct requirements for the interface and identified the required features and their ranges. this informed the development of a generic gui system that could be used to map all sculpting features to onscreen buttons. this provided a lowest common denominator from which all interface devices and modalities could be successfully employed. for single button access, the interface used switch scanning, 2 where each gui button is highlighted in turn until the user presses a switch to select the current element. the use of switch scanning allows an interactive system to work with a range of physical user interface technologies ranging from gaze/blink systems to sip-and-puff tubes; finger, foot, or head motions; and others that are adaptable for users across a broad range of disabilities."
"the shiva software has been successfully used to help teach students about spatial relationships between objects and general spatial awareness. it has been used to teach and reinforce spatial concepts such as up, down, behind, and rotate. this has also helped the teachers to understand how young people with severe physical difficulties perceive certain concepts."
"given these motivations, our primary research objectives were to ■ consider and establish the ranges of interface requirements needed by young people with disabilities to enable them to use software of varying complexity;"
"in a direct approach, a purposeful sample of experts is assembled and guided through a three step process. first, experts are given a question that will prompt them to iteratively identify concepts (e.g., writing them on sticky notes), arrange them (e.g., creating clusters or hierarchies by moving the notes), link them and re-arrange them to facilitate the display of links [cit] . this step results in the map's structure. while that step can be carried on straightforwardly for simple problems when experts are all available in one place, it can be challenging for complex problems where a panel of international experts is needed in order to account for each part of the problem [cit] . thus, the map's structure may be set based on a subcommittee of experts. the second step is to ask all experts about the strength of each relationship, and the final step combines their knowledge using fuzzy logic theory as in the previous section."
"a rtistic activities are an important educational subject in their own right, but they can also provide strong links with other core subjects and life skills, including spatial awareness, object recognition, and aspects such as self-expression and building self-con dence. however, using clay or any other sculpting material is nontrivial for disabled individuals with little or no limb control."
"these guidance systems can be categorized according to how the information is gathered from the environment and delivered to the blind user [cit] . in general, information can be gathered with ultrasonic sensors [cit], laser scanners [cit], or cameras, and users can be informed via auditory [cit] or tactile sense [cit] . in recent years, camera-based systems have won much attention due to advantages like large sensing area, rich sensing data and low cost. most existing vision-based guidance systems use stereo-vision methods. in these systems, stereo cameras are used to create a depth map of the surrounding environment, and then this depth map is transformed into stereo sound or tactile vibration. for instance, mora [cit] developed a navigation device to transform a depth map into a stereo sound space. meanwhile, the tvs [cit] and tyflos [cit] navigator systems convert a depth map into vibration sensing on a 2-d vibration array attached to the user's abdomen. the envs system [cit] transforms a depth map into electrical pulses that stimulate the nerves in the hand's skin."
"the shiva project was designed to use computerbased technologies to extend access to artistic tools for particularly vulnerable population groups: people in rehabilitation (through french partners at the lille 1 university and through the hopale foundation) and young people with various types of disabilities (through bournemouth university and victoria education centre in the uk). to achieve this, we built a generic, accessible gui and a suitable geometric modeling system and used these to produce two prototype modeling exercises. these tools were deployed in a school for students with complex disabilities and are now being used for a variety of educational and developmental purposes. speci cally, our goal was to enable such young people to learn about manipulating objects by providing a few basic virtual sculpting tools."
"virtual sculpting is a computer-aided technology that allows for the creation of sculptural artifacts. it can be performed in various ways: using 2d/3d input, an interactive modeling technique that employs pressure-sensitive or haptic interactions, or alternatively, a set of vr interface tools such as cybergloves or digital clay."
"the current state of the art in accessible technology and virtual sculpting (see the sidebar) includes a range of impressive tools. however, we found that the available virtual sculpting software would not allow us to achieve the goals of the shiva project. our team had to develop new solutions for the virtual sculpting based on the group's t wo aspects were essential to the shiva project: accessible user interface technologies and virtual sculpting. the brief summaries provided here review the state of the art in both areas."
"impact of neighbouring influences mediated by local context. all locations (circles) have the same concepts and mechanisms (large black arrows), but neighbouring influences (blue arrows) will impact some concepts (thin black arrows). in this situation, the hostility of the terrain follows a north-south gradient, while the insurgency has a stronghold in the north-east."
"another important factor that affects guidance performance is the timing of guidance instructions. here, guidance instructions are divided into \"hard-timing\" and \"soft-timing\" instructions, as shown in table 4 . hard-timing instructions have high priority over softtiming instructions, and must be delivered instantly whenever the safe direction changes. a soft-timer is defined as: t0 +τ·s, where t0 is an average interval between two delivered message sets. t0 is usually set to 5 seconds in the experiment.τis a weight concerning guidance states. safe state will be assigned a large weight, while danger state has a small weight. normal state will have a medium weight. s is user's walking speed. the termτ·s defines a flexible interval between delivered message sets."
1. the parameters for the drill are obtained from the camera position and direction and from the predefined (or set by the user from the gui) diameter. these parameters define a cylinder and a parent affine transformation node that gives it the correct orientation.
"the shiva software prototypes provided a relatively small subset of modeling features, which limits the range of objects that can be created with the system. a more flexible system would be desired for more advanced users (children and adults), although it is unclear which modeling paradigms would be the most suitable for disabled access."
"taking into account the broader context in which individuals make decisions can be very challenging due to the complexity of this context and the difficulty of collecting data about it. focusing on the counterinsurgency environment, upshur and colleagues wrote that \"researchers are not impartial but rather armed actors in a conflict; thus there is a 'combatant observer effect'. the interviewer's obvious association with a combatant organization affects the openness and honesty of respondents, as does the power disparity between a member of an occupying military force and an unarmed local population\" [cit] . consequently, data corruption results in uncertainty and bias. furthermore, disagreements on the mechanisms are not only found between the reports of direct observers but also in the analyses of scholars. for example, fearon and laitin reported that \"the effect of primary commodity exports is considerable: [...] a country with no natural resource export only has a probability of warstart of 1%\" compared to 22% when exporting\" [cit], but ross considered that \"the claim that primary commodity exports are linked to civil war appears fragile and should be treated with caution\" [cit] . therefore, there is a need for computational models that can use the qualitative data provided by process-tracking and ethnographical studies, and have specific mathematical ways to address the uncertainty and conflicts found in the data."
"the opportunity to rebel is strongly linked to the strength of the insurgent organisation, which affects whether recruitment mechanisms, arms [cit], information [cit], and logistical capacity make it possible to rebel in an organised or meaningful way. the strength of an insurgent organisation increases in the presence of weak government institutions, limited in their ability to identify and neutralise insurgent agents [cit] . demographic precursors for insurgency establish conditions of instability that can be exploited by insurgent groups to provide political justifications and normative space for rebellion. an existing territorial dispute can evolve into an enduring ethnic or sectarian rivalry, providing the fault-lines for civil conflict and increased fractionalisation [cit] . the presence of foreign military forces staging an intervention can inhibit the power of any party to achieve significant political progress through force of arms, paradoxically often making conflict longer-lasting and more intractable [cit] . a recent previous civil war can ensure that the population has both lurking hostilities and access to weapons. the balkan wars are one example of the facilitating effect that weapons-saturation has upon making participation in civil wars feasible [cit] . a supportive foreign diaspora can make funding insurgent activities easier, while the social exclusion of certain groups makes conflict increasingly easier to justify and prosecute as the excluded group grows in size."
"young people with disabilities may have a very different experience of the physical world than those without. this experience may be in uenced by their range of movement, limited gross or ne motor control, or having spent their lives in wheelchairs. because of these physical dif culties, they may not have had the opportunity to explore the physical properties of different objects and materials in a conventional sense. new technologies are helping to provide ways for young people with disabilities to have these experiences in a virtual sense."
"the final shiva gui system features include switch-scanning support with adjustable timing parameters; direct progression with multiple switches; mouse or touchscreen control; button debouncing options; key-mapping options with activation on trailing or leading edges; basic eyegaze support with adjustable dwell time and configurable rest zones; fully configurable gui layouts that can be saved and loaded from user profile; visual styling in themes for use across multiple profiles; visual adjustment in themes and profiles; and configurable graphics for buttons, symbols, and text, including sophisticated color replacement in graphics."
"in order to develop accurate models of conflicts that can support military analysis, computational techniques need to utilize vasts amounts of data to the best of its potential. however, uncertainty and conflicts abound in data collected by observers or synthesized by experts, making it challenging to effectively incorporate it into quantitative models. furthermore, adequately capturing the spatial and social dynamics of insurgency tends to require different computational techniques. our previous work proposed a novel approach to create models of insurgency from imperfect data while accounting for both spatial and social dynamics [cit] . while this early framework addressed some of the needs for modelling insurgency, it also came with three limitations. first, the space had to be divided into a set of square cells, which could lead to either an over-simplification of key spatial features (e.g., when cells are too large and cover very distinct neighbourhoods) or a computational burden (e.g., when small cells unnecessarily partition a homogeneous space). second, the process of model building was centred on the nuanced reading of scholarly articles, which could not be straightforwardly applied to gathering first-hand observations. finally, the initial development of software highlighted the need for a more intuitive approach to model design, such that modellers could focus on key aspects while the modelling process would be transparent for stakeholders. this paper extends our previous work and addresses all three shortcomings aforementioned. the space is now represented using complex networks, and generators are also provided to create space when detailed maps are not available. we detail how models can be built directly from participants' experience, and provide a proof-of-concept that synthesizes the expertise of five scholars in insurgency. finally, our focus on usability during software development has resulted in a set of tools that can effectively guide modellers and stakeholders through the process of building a computational model of insurgency."
"for example, the shiva totem pole software was used by two eye-gaze users, and it was discovered that they both had similar difficulties with the concept of a stack. teachers used a physical stack of primitive objects and asked the users to reproduce the stack using the software. in the shiva totem pole software, objects are added in a stack that starts from the bottom, adding one object on top of the other. however, both eye-gaze users would consistently try to start the stack from the topmost shape first. the teachers believe the reason for this is that these users have simply never had the experience of physically placing one object upon another, so the concept is new to them. identifying this has then helped the teachers in their understanding of which fundamental spatial concepts need to be introduced to such students and in assessing their comprehension."
"initially of interest primarily to soldier-scholars seeking to systematise their experiences as counter-insurgents in wars of decolonisation [cit], insurgency (and its cognates) increasingly became the subject of several landmark studies by political scientists [cit], historians [cit], and economists [cit] . together with this considerable renaissance in the study of insurgency, computational research into the dynamics of terrorism and rebellion has grown in prominence [cit] . one of the drivers of this growth is the ability of computational approaches to account for the many complex interactions between the factors that shape a conflict, which makes such approaches a valuable complement to the associational analyses stemming from economics and seeking to identify 'root causes'. computational models have indeed been able to systematise and articulate many of the key processes which define insurgency as a particular type of conflict. for example, a recent model used 19 factors and over 80 parameters, accounting for processes such as the consequences of intelligence gathering on the insurgent's organization or the impact of correspondence: giabba@sfu.ca interdisciplinary research in the mathematical and computational sciences (irmacs) centre, simon fraser university, 8888 university drive, v5a 1s6 burnaby, canada outside support on the insurgents' actions [cit] . the task of populating the model's parameters with real-world data is often left to the analysts [cit] but this task can be particularly challenging as models require specific numerical values despite sources mostly providing qualitative data from empirical evidence. while the challenge of creating quantitative models of insurgency when given qualitative data has already been thoroughly addressed in the political methodology from a statistical perspective [cit], this challenge remains relatively unexplored from a computational perspective."
"the myui project included an adaptive interface but was targeted mostly at elderly people. 1 the ui presents a set of predefined applications including email access, tv, games, and similar consumables. the system automatically adjusts the user profile, which covers vision, hearing, language, information processing, memory, computer skills, speech, dexterity, and muscle strength, while the user is working with it. however, a limited number of sensors are used to detect the user's abilities and only simple data is stored, so it is unclear if this data can be used in any practical sense."
"the rebelliousness of a community indicates the level of its participation in insurgent activities. it is thus the most closely monitored factor for this scenario. it is determined by two factors: motive and opportunity. the motive is determined by the socio-economic advantage to insurgency, which collectively incorporates the social, political, and economic reasons for which an individual or community would want to rebel. opportunities consist of the mechanisms and material conditions that make rebellious acts possible on an incidental basis. this model also accounts for the self-reinforcing effect of rebellion, in which existing rebelliousness facilitates further rebelliousness though mechanisms such as insurgent recruitment networks and the solidification of ascribed political loyalties [cit] ."
"the message generation module works as a kind of human-machine interface between the guidance system and the blind user. the task of this module is to transform the information obtained from the image domain to the language domain, and deliver the right messages to the user at the right time. for the user feedback scheme, stereo sound and tactile arrays are also widely used. however, extensive training is required to enable the user to perceive the sound and vibration pattern. verbal message feedback can provide semantic information in a more user-friendly way. here, a message generation scheme using a fuzzy logic approach is proposed. as shown in figure 15c, the key idea of this message generation scheme is guidance states estimation. figure 15b illustrates the definition of these four state variables."
"to test the verbal message generation scheme, a user walking trajectory is generated using the estimated safe direction and user's walking speed. this walking trajectory is then mapped to a top-view occupancy map generated using the obstacle detection algorithm. a segment of this synthesized map is shown in figure 22, which is obtained from walking on an urban pedestrian pavement. the map is divided into 16 time slots: each slot corresponds to 5 seconds, which is the average time interval between delivered message sets. user's walking speed at each time slot is shown above the synthesized map, with estimated guidance state gs shown in the middle. the circles on the user's trajectory indicate the points where guidance messages are delivered. these points are indexed as 1 to 12 from left to right, and their corresponding message sets are listed in table 9 . it can be observed that hard-timing messages like safe directions are properly delivered at each transition point on the user trajectory. the fuzzy state estimator keeps track of the guidance state through each time slot. when the user enters a danger state with a high speed of 1.1 m/s, the system prompts \"please slow down\" at point 1. when the user leaves the danger state and enters a normal state with a low speed of 0.8 m/s, the system prompts \"you may walk faster\" at point 4. these user motion messages are shown to be effective in adapting the user's walking speed according to different states. soft-timing messages like those reporting obstacle positions follow the soft-timer, which is defined as: t0 +τ·s. it can be observed that the message points are not evenly distributed between each five-second time slot. in a danger state when user speed is low, the message points are prompted densely, while in a safe state when user speed is high, the message points are prompted sparsely."
"in this paper, we develop a novel computational method that can be used to model how an array of interacting factors come together to determine loyalty or rebelliousness. this method involves two computational techniques whose synergies are essential to address the specific needs of modelling in insurgency. the artificial intelligence technique of fuzzy cognitive maps (fcms) has a proven track record in allowing for the development of models when supporting data is vague or conflicting. using this technique will allow us to address one of the main shortcomings of current models as aforementioned. while fcms have been used to model complex political phenomena, such as crises in the republic of macedonia [cit] or cyprus [cit], this technique is not designed to http://www.security-informatics.com/content/3/1/2 capture the spatial dynamics that play a very important role in conflicts (c.f., [cit] for the role that urban geography plays in terrorism). for example, not adequately capturing the diffusion of civil wars over space [cit] or the localized nature of attacks [cit] would significantly lower the relevance of models to analysts. we previously demonstrated that fcms could be used to model social processes but that they would have to be combined with another technique for local dynamics [cit] . our early work combined fcms with cellular automaton [cit], which have a long history of being used to model spatial dynamics [cit] but suffer from having to rigidly divide the space into equal square cells. therefore, we propose to represent the space as a complex network (cn), which offers several advantages over the early version of this framework: the space can be arbitrarily divided, standard generators can be used to test how a strategy would unfold in different types of space, and the analyses tools developed for complex networks can be used to explore the relationship between the structure of that space and the dynamics of insurgency."
"a) vertical sampling rate b) horizontal sampling rate figure 4 shows a comparison between the vertical edge map on an original image and a top-view image. on the original image edge map, it is very difficult to discriminate the obstacle edges because of the pavement edges. however, on the top-view edge map, the obstacle edges are enhanced by oversampling while the pavement edges are suppressed by sub-sampling."
"thus, adding objects to the stack are union operations with affine transform nodes giving each object an offset position. orientation operations on each object then adjust the parameters of this transform node, and blending is achieved with a smooth-blended union instead. as we discussed earlier, the easy definition of the geometric objects and of the operations lets us create a simple definition of the objects and therefore use these as input for a direct rendering system in the form of real-time ray-casting for interactive visualization of the intermediate stages of the modeling process."
"this article presents the shiva project's motivations, approach, and implementation details together with initial results, including examples of the 3d printed objects designed by young people with disabilities."
"under the experimental platform configuration shown in table 5, the average runtime performance values of the major functions are listed in table 10 . if the system runs in full function mode, it can achieve an average frame rate of 12 fps on our experimental platform. in our experiment, a blind pedestrian walks at a speed of around 0.5 m/s~1.8 m/s on average, a little bit slower than a normal pedestrian. at this walking speed, three to five seconds would be an appropriate time interval for message delivery, while a 2 fps image processing speed would be enough to meet the runtime requirement. therefore, the proposed algorithm can fully satisfy the real-time requirements for a general outdoor guidance task."
"from the geometric point of view, in the application we perform metamorphosis operations over two frep objects. given the nature of the application, we used polygonal meshes of existing real-world objects converted to scalar fields as sources for the input shapes. the metamorphosis operation in its simplest form can be seen as a linear interpolation between the values of the scalar field for the initial object and the target object. we have also tried more complex metamorphosis operations 6 such that the user can influence the process in order to obtain more artist-friendly intermediate shapes."
"by applying the above monocular vision algorithms to the top-view image, three types of necessary information for guidance can be obtained: safe walking direction, obstacle positions, and user's walking motion. the next important step is to transform the information obtained from the image domain to the language domain, and deliver the verbal messages to the user in an appropriate manner."
"virtual sculpting techniques employ a variety of sculptural metaphors, 3 such as the construction of 3d shapes using constructive solid geometry (csg) techniques or global and local deformations that are suitable for disabled users. interactive local modifications can be done using different sculpting metaphors. the concept of virtual clay is perhaps the most natural metaphor for virtual sculpting. 4 it is supported by a number of commercial and research products such as geomagic freeform and claytools, cubify sculpt, pixologic's zbrush, and sculptris."
"when trying to evaluate the strength (and thus the existence of ) a mechanism, straightforwardly assigning a score to each source and simply computing the average or the mode would address neither vagueness nor conflicts. instead, fuzzy logic theory allows us to summarize opinions via linguistic terms (e.g., a mechanism may have a \"weak\" or \"medium\" effect) and consider that each term is associated to a range of values. a membership function represents the range of each term, and ranges can overlap to account for the possibility that an expert judges a relationship to be \"strong\" while actually thinking the same as the expert who calls the relationship \"very strong\". the choice of a membership function depends on the problem. triangular membership functions ( figure 1 ) are often used [cit], and a plethora of alternatives exists (e.g., z-shape, gaussian, and s-shaped membership functions [cit] )."
"the prototype software implementations were installed at the victoria education centre (vec) and, thus far, have been used by 11 students with disabilities including two eye-gaze users (see figure 3) . [cit] we were able to collect feedback from actual users, which was vital for refining the gui system and software prototypes. since then, the software has been used by teachers and speech therapists with a variety of students, leading to actual 3d printed results. in each case, the student was under the close supervision and guidance of an assistive technologist in addition to relevant educators and speech therapists."
"the shiva gui represented a state-of-the-art prototype system, but there were a number of issues and further needs resulting from the project. for example, the system had a heavy requirement on technical support staff for profile creation, which was stored in an xml format. also, automatic user adaptation was identified as important because it would help adjust to the user's needs. in the future, we plan to include support for additional input modalities, such as brain computer interface, gesture, multitouch (including gesture interpretation and for collaborative input), tablet devices, and more general and complete eye-gaze support."
"to overcome modern operating system limitations, several specialist accessible interface toolkits have been produced. the grid 2 is a commercial system commonly used in special schools that provides disabled access primarily for communication as well as for other features (see http://sensorysoftware.com/grid-software-for-aac/). it has a deep level of support for communication, such as offering phonetics for synthesized vocal output, but these features require a specialist speech or language therapist for use and setup. the software provides access through switch, touch screen, eye gaze, head pointer, and mouse and keyboard. this covers a wide range of inputs and features but is not open or flexible enough for a heavyweight application such as interactive 3d modeling."
"lap-top computer intel core duo t7100 the algorithm is tested on several outdoor pedestrian path scenes, with various obstacles and cluttered road surface. to evaluate obstacle detection performance, the test scenes are divided into three sets, as is shown in figure 20 and table 6 . in each test set, 1000 frames are randomly sampled, with all the critical obstacle positions and types labelled manually as ground truth data. a true positive (tp) detection is defined to be such that the detection corresponds with an actual obstacle, and the deviation should not exceed 20% of the obstacle's size, otherwise it is considered as a false positive (fp), obstacle that is not detected is false negative (fn). table 7 shows the detection results on three test sets."
"for this user modeling, a functional approach was determined the most suitable because it directly maps the user's interface needs with the software settings. the choice of input mode and the specific settings would be stored in a user profile, which was to be created for each individual student to meet their needs."
"the guide project has a user-centered approach to creating basic uis with a variety of input and accessibility options. 2 the project, which is mainly aimed at and tested with elderly people, simplifies access to entertainment (tv) and communication (video calls). the system incorporates user profile setup by guiding users through a simple test that determines their cognitive abilities as well as some disabilities such as color blindness. the interface's input devices include tablets, speech, gestures, and eye gaze. for our purposes, the main limitation of the system is that it is oriented toward information consumption rather than information creation."
"user interfaces provide both the hardware and the software interface layers between the user and the software application. most computers currently support user input from a keyboard, mouse, touch screen, and to some extent, voice command, and most output modalities rely on informationrich visual displays, which can be difficult for the visually impaired. although some of the students in our project could use a mouse or touch screen, others struggle to press a single large button or have absolutely no limb control and can only access software through an eye-gaze system."
"our framework currently represents the space using an unweighted planar graph, such that influences either totally flow between two adjacent locations or do not. in practice, adjacency is dynamic: for example, a wall built for security purposes around the district of adamiyah would virtually cut it off from neighbouring districts (figure 2 ) once completed. adjacency is also a social construct, as members of one ethnic group may rarely move to places populated by another ethnic group during a conflict. our framework could be augmented to take these aspects into consideration, by having a dynamic network and weighting its edges depending on social factors. furthermore, the assumption of a planar graph can be challenged due to the spread of violence in non-contiguous areas via tribal ties. this effect is particularly salient in iraq, which has an estimated 150 tribes and 2,000 clans. while the ba'athist ideology under saddam hussein emphasized the state over ethnic/sectarian divisions, tribal loyalties were nonetheless essential to maintain military support and continue to play a key role in iraq [cit] . consequently, models often focus on tribal relationships and road network accessibility to link locations [cit] . capturing such relationships can be achieved in two stages. first, the requirement for planarity could be waived, as generating non-planar graphs is straightforward due to the availability of numerous graph generators [cit] . second, the requirement for a single edge between two nodes can also be waived. frameworks such as wgap already use multiple labelled edges between nodes [cit], which would allow to connect places based on multiple criteria such as geographical and ethnic proximity. such http://www.security-informatics.com/content/3/1/2 additions are virtually endless in a modelling endeavour, which highlights the need for a trade-off between the accuracy of the models and the additional complexity brought into the modelling process. therefore, applications of our framework will prove instrumental in gradually establishing the guidelines for computational methods of insurgency and continuing to meet them via innovative frameworks."
"where f and q are the corresponding feature locations in adjacent frames, and e is estimation error term. to find the optimized values for r and t, the weighted sum of squared error term in (11) should be minimized. here the weight wi is set using the largest eigenvalue of the inverse hessian matrix for each selected feature point. the final solution for r and t is used as an approximation for the user's walking motion, based on which the user's walking speed and direction can be estimated."
"each location contains an fcm that expresses the social dynamics within that location. the fcms all have the same structure, as the concepts and mechanisms contributing to insurgency are selected for the entire event (e.g., war in baghdad, insurgency in syria) rather than for a specific neighbourhood. however, the values of these concepts depend on the location. for example, we might consider that a lack of political representation contributes to both the weakness of a government and the exclusion of an ethnic group, which in turn favour insurgency. however, the level of political representation might differ across neighbourhood; in the case of iraq and baghdad (figure 2), the sunni neighbourhoods of azamiyah and doura would have a different level of political representation compared to the shia neighbourhoods of new baghdad and kazimiyah. some of the concepts are influenced by the values of concepts in surrounding locations, as defined by the network in the previous section. figure 3 illustrates this influence: all locations (circles) have the same concepts linked in the same way, but one concept is influenced by neighbours. focusing on the dotted orange circle, the ability of insurgents to control population movements at that location is influenced by the control that is exerted in the surrounding four locations. network influences are applied first, and then the fcms are updated to reflect how such influences would turn out based on the local context. it is possible for such influences to be cancelled out because of opposing forces (e.g., inhabitants in the target neighbourhood are politically well-represented and have a high level of trust in institutions), or they could be reinforced due to the presence of factors fuelling insurgency (e.g., household poverty and number of unemployed young men). this coupling allows to accurately represent local dynamics through fcms, and asssess the spread of influences over larger areas using a network."
"software was built to support the development of models of insurgency via our framework. while our framework is first and foremost mathematical, it can be mostly used via a graphical user interface that aims at making the modelling process as intuitive as possible so that modellers can focus on the few equations that require a fine tuning. the modelling process is divided into three steps (figure 6 ), and each step has a dedicated software component that supports both design and analysis."
"once these hypotheses have been implemented using the concept map editor, the dynamics of the fuzzy cognitive map can be simulated to see how events would unfold independently of geographical influences between http://www.security-informatics.com/content/3/1/2 regions ( figure 9 ). the simulation of this toy model of insurgency confirms the expectations of experts, as insurgency gradually arises (figure 9a ) and impairs economic development (figure 9b ), thereby providing further ground for insurgency. given that the values of some concepts are drawn from probability distributions, the outcome of several runs of the simulation can be different. to explore the range of possible outcomes, figures 9 (c-d) provide histograms of the final values. two equally probable outcomes appear for insurgency, either by sustaining it at an intermediate level or by having a high level of conflict. geographical enablers are high in most cases."
"the second software prototype was a totem pole exercise, which provides a more complex sculpting environment. here, the user stacks a small number of objects together and then performs simple modeling operations such as affine transformations on individual objects within the stack, or operations such as blending and drilling on the entire stack. the accessible ui is more advanced for this application (see figure 2), compared with the metamorphosis application, because it supports more operations, and more input is required from the user. the general approach remains the same, however. in this application, we allow the user to choose from a number of simple geometric shapes as input. in the current implementation, these shapes include a sphere, box, cylinder, and cone. the set of operations over these objects includes set-theoretic operations (union and subtraction), smooth union blending between objects, and affine transformations. from the implementation point of view, the operations are achieved by modifying the tree representing the object."
"this paper has presented a mono-vision-based guidance system for blind people in an outdoor environment. its first contribution is in presenting an effective way to discriminate obstacles from a cluttered background by means of inhomogeneous top-view re-sampling. it has also presented the directional ellipse model and dro feature in the top-view domain for obstacle type classification. for guidance, polar histogram tracking can make safe-area estimation more reliable; meanwhile, a fuzzy state estimator can provide valuable state information for message delivery. our real field tests show that the described techniques allow the system to be usefully applied in real-time obstacle detection and guidance on complex-scene pedestrian pathways."
"as an intermediate format for the geometry, we use the volumetric object format developed by the norwegian company uformia. this supports most of the operations and primitives existing in the current state of the art in modeling with geometry represented in the implicit form. this format allows the interchange of models between applications, using them as the source for direct fabrication and also converting these models to other formats for further operations in other modeling systems and applications. for example, most 3d printing hardware takes only polygonal meshes as an input, so we must convert the model from implicit form to a polygonal mesh by using polygonization methods."
"the results are shown in table 8 . the confusion matrix shows that the major problem is how to distinguish bulky obstacles from thin ones. for example, \"blocks\" can be wrongly identified as \"poles\" (10.6%), and \"piles\" are incorrectly identified as \"curbs\" (8.3%). this is because, in urban scenes, one bulky obstacle may contain several isolated edges, resulting in several independent edge-blobs so that the bulky obstacle is split into several thin obstacles. the situation is similar when identifying thin obstacles from bulky ones. for example, when several pedestrians are very close to each other, their edge-blobs tend to merge into a single bulky one, which may result in an incorrect \"block\" identification. despite the splitting and merging problems on edge-blobs, the distinguishing of vertical and planar types based on dro features is more stable. for instance, \"poles\" are wrongly identified as \"curbs\" in only 2.2% of the cases, which shows the effect of the proposed dro features in the top-view domain."
"the physical benefits of the software are also being investigated. therapists are starting to use it as a tool from the point of view of an aid in improving manual dexterity or, using the touch-screen interface, to gradually encourage students to increase their range of movement by asking them to reach for more distant buttons and input controls."
"the final step is to generate the spatial network. during the early phases of model development, an exact mapping might not be available to modellers. consequently, the model might have to operate on assumptions regarding the broad characteristics of the space, and only if the model proves useful then partnerships can support the acquisition of accurate data. our software provides extensive support for the early phase by allowing modellers to choose network generators with a desired set of properties (figure 8b-1), such as creating planar powerlaw networks (figure 8b ) which represent a densely connected urban centre linked to increasingly isolated settlements. since such generators require a fine tuning, analysis tools are provided both for visual inspection (figure 8b-2) as well as for the quantification of key metrics (e.g., clustering coefficient in figure 8b -3 or degree distribution in figure 8b-4) . once the right network has been generated, the simulation can be performed and analyzed to see how factors of the fcm changed over time (figure 8b-5 )."
"this paper proposes a directional ellipse model for discrimination of vertical and planar type obstacles, and the properties of the edge-blob feature on the top-view domain are further explored in this section. vertical obstacles are defined here as obstacles that rise significantly above the road plane, like trees, poles, and other pedestrians. these vertical obstacles usually have vertical edges in the original-view domain. planar obstacles are those lower obstacles that are close to the road plane, like road-side curbs and stairs; these obstacles usually have significant edges along the road direction in the original view. in the top-view domain, obstacles can also be characterized by their distinct edge orientations, although the edge orientation feature is different to that in the original view domain. this is illustrated in figure 6 : cr is the real camera's optical centre and sr is the real camera's image plane, while cv is the top-view virtual camera's optical centre and sv is the top-view virtual plane. in figure 6a, a vertical obstacle is mapped to the sr plane through central projection, with the vertical edges still appearing vertical. during the top-view mapping process, the obstacle's image on the sr plane is mapped to the sv plane, which is parallel to the ground plane sg. as a result, on the sv plane, the horizontal edges of the obstacle still appear horizontal, but the vertical edges are stretched toward point pr, which is the perpendicular projection of cr on the ground. it can be observed that, through topview mapping, vertical lines in the original image are mapped to lines passing through the same point in the top-view domain. this vertical line distortion can be partly explained by the inhomogeneous re-sampling process discussed in section 2.1; it can also be derived from an ipm (inverse perspective mapping) formula [cit] ."
"as we have seen, many blind guidance systems, using stereo or monocular vision systems, have been successfully tested in indoor environments [cit] . however, very few have been reported to be equally highly effective in outdoor scenarios with complex backgrounds. the main contribution of this paper is a monocular edgefeature-based approach for obstacle detection and avoidance in complex outdoor environments. an overview of the system is illustrated in figure 1 . in the proposed system, a camera is attached to the blind user's waist and angled slightly downward towards the road in front. as shown in figure 1a, the white cane still acts as a reliable tool covering a close range of up to 2 metres in front of the user, with the downward-looking camera acting as a complementary sensor covering a medium range of about 10 metres. with this configuration, the white cane can be used to detect ground-level obstacles and holes in the near field, while the camera looking a further distance ahead can provide useful information like safe walking direction and obstacle locations and numbers."
"the field test areas are the same as the three test scenes shown in figure 20 and table 6 . for each test scene, a test path 200 metres long is selected. the field test is carried out on the same day in the morning. the four test subjects are not familiar with the test paths selected. before the real test starts, 30 minutes training is given to show the subjects how to use the system together with the white cane, and to explain the rules of the field test. in the field test, each subject is required to do two test runs on each path. for the first run, test subjects use both the guidance system and the white cane; for the second, they use only the white cane. in each test run, the time they take to pass along the 200-metre path is recorded. the data are presented in figure 23 . as shown in figure 23a, on the open-space path, the average time for the first run is 154 seconds, and for second run 185 seconds. the guidance system therefore improves the user's travelling speed by 17%. on the urban path, with narrower space and more obstacles, the use of the guidance system in the first run brings an even bigger improvement of 28.5% in the user's average travelling speed. the results show that our system leads to a reduction of almost 30% in the time taken to negotiate obstacles after only a few minutes training with the system."
"the output of the application is a solid object representing the intermediate stage of the metamorphosis between two objects. because the object is solid, it can be further used as an input object for the application or used as an input for 3d printing."
"based on the detected obstacles, a polar edge-blob histogram is constructed on the top-view image for the estimation of the safe walking area. as shown in figure 11c, on the edge-blob image, from the right boundary to the left boundary, radial directions (marked red dashes) are sampled with respect to the convergence point c. for each sampled radial direction, the number of edge-blob pixels that lie along this direction is counted. by accumulating all the sampled radial directions, a polar edge-blob histogram can be constructed as shown in figure 11d . in the polar edgeblob histogram, the horizontal axis represents sampled radial directions in angles, and the vertical axis is the number of edge-blob pixels that lie along each sampled direction angle. the bins with high values indicate the directions where obstacles appear, while bins with zero values correspond to the directions where no obstacles exist. therefore, the safe area should be estimated by the bins with zero values."
"we start by providing the technical background to our computational framework, and we formally specify it. then, we detail the first and key step in building a model based on this framework: how to design a conceptual map articulating the interactions between several factors contributing to insurgency. we further highlight the functioning of the framework through our software implementation, with a focus on how it enables experts to interactively and efficiently set up computer models of insurgency tailored for the context they are interested in. finally, we discuss the strengths and limitations of this framework."
"to illustrate these steps, we asked international experts about the contributors to rebelliousness. due to the complexity of the problem and the wide range of expertise required to achieve a comprehensive understanding of insurgency, we structured the map as a set of 24 concepts and 44 relationships based on the feedback obtained about figure 4 . experts were then asked to evaluate the strength of each relationship, by categorizing it as 'nonexistent', 'very low', 'low', 'medium', 'high', or 'very high'; experts were also given the possibility of choosing 'unsure' in order to skip evaluating relationships about which they did not feel confident. finally, expert opinions were combined using fuzzy logic theory for each relationship using the standard procedure aforementioned (i.e., membership functions specified by equations 1 to 5, mamdani http://www.security-informatics.com/content/3/1/2 algorithm, sum method of aggregation, and centroid for defuzzification). the result is shown in figure 5 ."
"in our earlier augmented sculpture project, 3 we created a specific interactive environment with embedded sculptural means. users experience an immersion into a virtual space where they can generate new shapes using either metamorphosis between several predefined sculpture models or the virtual carving tool with such operations as subtraction, offsetting, and blending. finally, we 3d printed the resulting sculpting artifacts to produce new physical sculptures. the project had both artistic and educational merits 5 and the tools and lessons learned fed directly into the shiva project."
"the shiva software has started to be incorporated into regular scheduled art lessons at vec. figure 5 shows examples from a shoe-box landscape project inspired by the work of british sculptor andy goldsworthy. this project involves students creating sculptural arches as viewing windows, similar to those in goldsworthy's famous works. this was well received by students and led to increased engagement with the project. figure 5 shows installations created by students."
"one of the key aspects of access for disabled users is that each individual has different and specific interface requirements, these requirements can vary according to their specific physical or cognitive abilities, and these abilities are dynamic and liable to change, often throughout each day. the user interface must therefore store some information about its users in order to be configured to their requirements. this information is called a user model and can generally be stored in two ways: a medical approach stores information about the user's physical capabilities and limitations, and a functional approach stores the user's interface requirements."
"the simple model used in this section does not aim to make accurate recommendations regarding counterinsurgency strategies. rather, the experiments focus on demonstrating the ability of our framework to easily represent complex dynamics and handle 'what-if' scenarios. we use the model represented in figure 7, which articulates how geographical (e.g., inhospitable terrain), economical (e.g., economic development and unemployment of young men), and political factors come together in shaping rebelliousness. the values were obtained by aggregating expert knowledge using fuzzy logic, as in figure 5 . in this sample scenario, an insurgency has begun in a resource-based economy. consequently, the following assumptions were made on the initial values of concepts:"
"speech therapists at vec have successfully used the software with students in their regular activities to help with speaking and listening as well as cognitive development aspects. they work with students on the concepts of sequencing, following instructions, communicating ideas, and collaborative work. early observations suggest this approach will lead to good results for the students, especially due to the high levels of engagement with the software."
"to achieve the project's research objectives and overcome the limitations of the current state of the art (see the sidebar), we needed to develop new interface tools to allow each disabled user to interact with the software. an interface solution must therefore provide a vast range of flexibility and the ability to store settings for each user."
"since the camera is mounted on the waist of user, the camera's motion can be used as an approximation for the user's walking motion. in top-view image sequences, the movement of ground pixels can be regarded as an approximation of the user's walking motion projected on the top-view plane. as the ground pavement structure is reconstructed by top-view mapping, it would be very convenient to calculate the movement of ground pixels in the top-view domain. to calculate ground pixel movements, a klt (kanade-lucas-tomasi) tracker is used to track ground pixels through top-view image frames. as is shown in figure 14, after obstacle edgeblobs are extracted, their corresponding directionalellipse region can be cropped from the top-view image domain, so that only ground areas remains. the klt tracker is then applied to the ground area to select ground feature points and track them through image frames. the user's walking motion projected on the top-view plane can be decoupled into translational motion and rotational motion. define r as the rotation matrix and t as the translation matrix; these can be calculated by (10):"
"in contrast to sainarayanan's method, which uses pixelwise features, edge-based features are explored to discriminate obstacles from a complex road pavement background. by re-sampling the original image inhomogeneously and mapping it onto a top-view virtual plane, pavement edges in the near field are sub-sampled, while obstacle edges in the far field are over-sampled. morphology filters are then used to enhance this inhomogeneous re-sampling effect on connectivity and scale of edges, so that enhanced obstacle edge-blobs can be distinguished. to further classify obstacles, a directional ellipse model is built for edge-blobs on the top-view plane. finally, information regarding obstacles, safe area and user motion is converged at the message generation engine, where a fuzzy state estimator is designed to determine what types of messages should be generated and when to deliver them to the user."
"■ examine how a generic user interface system can be designed to meet these requirements to give access to users with a broad range of physical input requirements; and ■ identify and develop virtual sculpting methods that would be appropriate for the target audience (young people with physical and/or cognitive disabilities) and accessible to them given their input requirements, while allowing 3d printing for real-world output."
"the first prototype software using the accessible gui was a metamorphosis exercise, specifically for younger or less cognitively able students. here, the user chooses two objects and can produce an intermediate shape that is a blend of the two objects. primary interaction for this is through a slider, and the blended shape is displayed to the user and updated interactively. the user can then rotate their object and apply a color to it. figure 1 shows the interface of the metamorphosis application. the interface is implemented in a simplistic way that allows even users with severe disabilities to interactively perform the transition operation between two shapes."
"the software is frequently used to help students understand how the shape of an object may be constructed from a set of simple primitive shapes and help them understand the differences between representations of 2d and 3d shapes. a teacher will often draw an idea on a sheet of paper and then ask the student to reconstruct it using the shiva software, giving instructions and answering questions during the process. the teaching approach focuses on the students enjoying the experience, rather than on them producing a \"correct\" end result. however, students have already shown progress. one student who started by creating essentially random objects is now able to create identifiable models of simple objects such as a cat or a teddy bear."
"once the evidence is summarized via a linguistic term, a set of if-then rules is produced. for example, we asked experts to evaluate the extent to which an inhospitable terrain would contribute to the government's institutional weakness. two experts judged the effect to be 'medium' while two saw it as 'high' and one called it 'very high' . the if-then rules associate a crisp antecedent (e.g., whether the terrain is inhospitable) to a fuzzy consequent (e.g., a 'medium' or 'high' impact on the government's institutional weakness) and a confidence factor (e.g., number of experts who produced that rule). in this example, we obtain: these rules are combined to formulate a fuzzy inference systems that yields one quantitative value. fuzzy set theory is repeatedly applied to obtain one crisp value from the evidence supporting each relationship in the model. these relationships are connected: for example, an inhospitable terrain can impact the ability of insurgents to control the population which in turns affects the socio-economic advantage to insurgency. therefore, these connections can be viewed as a network named a fuzzy cognitive map (fcm). the nodes of the network represent fuzzy domain concepts (e.g., trustworthiness of institutions, baseline tension). the edges stand for causal connections, and their values are obtained via fuzzy set theory. edges are either positive or negative to indicate that the target concept respectively increases or decreases with the source concept. [cit] and its initial application as an artificial intelligence tool to public policy [cit], fcms have been used successfully in critical situations where accuracy has to be obtained despite vagueness [cit], such as evaluating the vulnerability of facilities to terrorism [cit] ."
"autonomous mobility is of extreme importance for visually impaired people, and white canes are their primary tools when travelling independently. however, white canes are very limited in sensing the environment. therefore, considerable efforts have been made over the last 20 years to complement the white cane with various types of electronic guidance systems able to detect obstacles at a greater range."
"as shown in figure 5c, many small edge-blobs from the pavement are eliminated. finally, as shown in figure 5d, only two major edge-blobs are selected, which correspond to possible obstacle regions. as mentioned in section 2.1, since top-view re-sampling sub-samples the original image in the horizontal direction, obstacle width will shrink in the top-view domain. this property makes it easier for edge-blobs to fill up the whole obstacle region in the top-view domain. therefore, these edge-blobs can be used as a kind of obstacle representation on the topview plane."
"the detection evaluation index values for detection of the converter faults are shown in table 31 . it can be seen from table 31 that the detection accuracy (94.87%) and precision (100%) by the convolutional neural network are higher than the svm method and the svr model. the false alarm rate is also lower than the other methods. the recall and negative detection values are also higher than the other two methods. given the results shown in table 31, it proves that the convolutional neural network is superior to the other two methods for detection of the converter system fault."
"this rule is enforced continuously, against every flow in the system, unlike traditional access control where there is no subsequent control of disclosure after a principal has been authorised to access data."
the following formatting rules must be followed strictly. this (.doc) document may be used as a template for papers prepared using microsoft word. papers not conforming to these requirements may not be published in the conference proceedings.
"according to the radar image generated by scada data collected on site, the model is trained by resnet50 network, and the model obtained by training is used to predict the operation of the future system state under fault operation."
"this paper introduces the radar chart method into the wind turbine system fault detection methodology. the faults related to three frequently occurred system failures such as generator, converter and pitch system failures can be detected by the convolutional neural network, the svm method and the svr model developed in this paper. the convolutional neural network selects the resnet50 structure as the backbone network. the procedure in detail for development of the convolutional neural network method is given. through comparison of the detection evaluation indices for the fault detection by these three methods, it is clearly proved that the convolutional neural network method is superior to the svm and the svr method based on the preprocessing data."
"we assume that the rest of the kernel can be trusted and does not interfere with the enforcement mechanism. a further guarantee of this particular point could be leveraged from hardware inverse sandbox [cit] . lsm system hooks have been statically and dynamically verified [cit], and our implementation inherits from lsm the formal assurance of ifc's correct placement on the path to any controlled kernel object. this is sufficient to guarantee that we control flow and record audit on any operation on a controlled kernel object."
social bookmarking services show one characteristic compared to the other tools and platforms. it is the only service whose figures for news dissemination are higher than production and consumption. this could be explained with the fact that these services were originally designed to bookmark interesting and worthy content in order to make them public. therefore the main focus is on the news dissemination. thus this result should not be a surprising.
"additional research can be done to reveal whether device (pc, laptop, tablet, smartphone) influence the news seeking behaviour. do smartphone users tend to use social media while pc users prefer traditional media? another interesting aspect of the social media is the community effect; whether an event perceived more relevant and credible when published by a friend."
"in our implementation [cit], we enforce difc within devices or cloud vm through use of a linux security module framework [cit] . our module (camflow-lsm) can be deployed on linux systems (desktop, cloud servers, android devices or be embedded). we enforce difc between machines through a specific messaging middleware (camflow-mw 1 ) operates on *nix based systems (ios, linux, android etc.), and thus can operate to manage the interactions between 'things' and cloud services. fig. 1 gives a general overview of our system architecture."
the figures reveal that users access to social web predominantly for their news consumption. the possibility to be an active member is used by few users. also the sharing and forwarding of daily news is merely a purpose when accessing social web. this result could be caused because of the following facts:
"it can be clearly seen from tables 30-32 that the convolutional neural network can give the highest detection accuracy than the svm method and the svr model for detecting the generator, converter and pitch system faults. the convolutional neural network method is superior to the other two methods for detection of the faults related to the three subsystem failures."
in question research 3 we analyzed the usage of social software in general. in the previous section the purpose was focused. now we want to combine both questions to find out which software is used in which context/ for which purpose.
"social networking sites rank at the first position (24,4 %) in the news production. this result could be explained with different characteristics of these platforms: most social networking sites give their users the possibility to publish a status message. this function is originally for publishing what the user is currently doing. however this function could also be used to post news information. in facebook for example the user status message will be shared and shown on every friend's site. therefore the production and dissemination is more or less done in one move."
"the flow constraints have implications for the end-to-end properties of data throughout a chai of services. secrecy and integrity tags are not only associated with some original data, but with all data derived from it whether by computation or combination. in an iot scenario, not only is the integrity of an actuation command ensured within the last hop to the device, but the flow rule for integrity ensures that any prior components in a chain leading to this command have at least the same integrity tags."
"therefore, this paper adopts the residual network structure as shown in figure 3 to improve the performance and accuracy of the network by increasing the network depth. the residual structure can be applied to solving the problem of gradient disappearance caused by the increase of network depth. common residual network models include resnet18, resnet50, and resnet101. as the number of layers increases, the amount of computation of the network increases accordingly. by considering the operation speed and accuracy, in this paper, the resnet50 structure is selected as the backbone network [cit] ."
"personal publishing services are both used in a passive way. microblogging services are only accessed by 6,6% to publish news information or participate in discussions. also weblogs are used by only few (7,8%) users for this purpose. likewise are the figures within the news dissemination. only 4,4% (microblogging services) and 2,2% (weblogs) share or forward content within these services."
"by making comparison analysis of the above radar charts, it is found that each indicator's values are relatively stable and the distribution is regular under the normal generator operation whereas the indicator's values are fluctuating and the chart distribution is relatively cluttered under faulty operation. the traditional analysis methods are mostly based on the training analysis of the data. in this paper, the convolutional neural network and support vector machine (svm) methods are used to train models to identify the image characteristics during normal and fault operation. the fault detection accuracy is evaluated by analyzing the fault detection indices, which proves that the convolutional neural network is more suitable for detection of frequently occurred faults in wind turbine operation."
"as we already stated news production always takes additional work. this extra effort applies especially to the production of videos. therefore this result seemed not to be surprising. however with the rise of smartphones and the mobile web videoshare platforms should not be neglected. dave sifry, the founder of the blog search engine technorati applies the videoshare platforms a special role in the news process: \"just wait till people understand the power of the mobile phoneit's got a camera, a microphone, an ear piece and a display -then you have a broadcast device in your hand\" [cit] ."
"our work considers end-to-end difc enforcement and audit throughout the iot service chain. several assumptions underpin our work, which represent general operational concerns: trusted platform module (tpm): we assume that tpm [cit] or vtpm [cit] technology are available across the cloud components and their integrity can be remotely monitored [cit] . such technology is also becoming available on commodity hardware and mobile devices [cit] . hardware integrity: we assume that the cloud provider has taken sufficient technical and non-technical measures to ensure that the hardware has not been tampered with (e.g. see cisco vs nsa). the hardware integrity of edge devices (e.g. mobile phones, home automation devices etc.) is harder to guarantee. protecting the hardware is likely to fall under the responsibility of the device/data owner. this may present challenges in situations where these are different parties. low-level software stack: we assume that the integrity of the low-level software stack is recorded and monitored. the lowlevel software stack here includes: bios, boot loader code and configuration, options rom, host platform configuration, etc. we assume that this integrity measurement is kept safe and cannot be tampered with. tamper-proof hardware storage may help with keeping integrity measurements safe. cryptographic security: we assume cryptographic functions to be secure and data exchange across machines to be encrypted. we assume that message integrity on exchanges between machines can be verified. furthermore, data may be encrypted on disc to provide a further guarantee. physical security: we assume that best practice is in place to restrict physical access to the hardware managed by cloud providers, or third parties managing the underlying infrastructure on their behalf. physical access to edge devices (e.g. things, mobile devices, etc.) is harder to monitor and protect."
"it seems that user access social web in a passive habit since they use the social web predominantly for their news consumption. however, publishing and dissemination news should not be neglected as it stands for the interactivity of the new web."
"schmidt describes platforms as applications that enable users an infrastructure for communication and interaction. the very feature element of platforms is that the user has to be a registered member of the platform in order to produce and share content. schmidt proposes to divide the platforms in two groups: the multimedia platforms and the social networking platforms. in the group of the multimedia platforms we can state video, photo or audioshare platforms [cit] . youtube is the most successful videoshare platform worldwide and is one of the biggest websites for three years [cit] . youtube have been ranked in third position in alexa global website ranking [cit] . numerous online phenomenons have proved that youtube is a news source especially when eye witnesses upload videos relating to news events, in this cases traditional media needed to refer to youtube. [cit] during political unrest in iran youtube and twitter were the only news source and even traditional media adapted their reporting to youtube [cit] ."
"however, the challenges introduced by connected devices, potentially monitoring every aspect of the environment and daily life, require security beyond the current standard authentication, access control and secure channels [cit] . there is a clear need for an end-to-end security mechanism that underlies every data exchange, and ensures that data is used for the purposes specified by those who own and/or are responsible for it [cit] ."
"in the training process of the convolutional neural network model, the characteristics of the images in the training data set are learned; as the number of network iterations increases, the network parameters are adjusted accordingly; thus, more representative image features are extracted, and finally the system model is determined for identification of fault condition."
"in summary, there are a number of approaches developed for detecting and predicting wind turbine system faults. usually the fault diagnosis can be done by mathematical modeling. however, it is difficult to establish an accurate fault model as a wind turbine fault may be caused by interactions among multiple components and coupling relationships exist between them. the operation of wind turbines involves a complicated control process and it is hard to establish an accurate model [cit] . therefore, this paper adopts a data-driven approach for wind turbine fault detection. the research focuses on generator, converter and pitch system failures. the scada system data collected from 1.5 mw wind turbines installed in a wind farm in hebei province of china was utilized in analysis and modelling. the fault data was collected from a total of 24 wind turbines in the wind farm. based on analysis, the main faults occurred in generator, converter and pitch system and the associated fault indicators are shown in tables 2-7 . table 2 . main faults of generator."
"in difc, entities-including processes and data-holding objects e.g. files, key-value store entries, messages-are associated with secrecy (s) and integrity (i) labels, encapsulating the security policy. a label is a set of a tags, each tag representing a particular security concern. the current state of these labels is the entity's security context."
"with the application of new technologies, the operation and maintenance (o & m) costs of wind turbines are continuously going down and hence causing a reduction of the cost of wind energy [cit] . the operation and maintenance costs account for approximately 10-15% of the overall energy generation cost for onshore wind farms and 20-25% for offshore ones [cit] . when a wind turbine is frequently shut down due to failure, the system reliability will be affected, causing the operation and maintenance costs to increase. therefore, it is important to conduct condition monitoring and fault diagnostic analysis at an early stage of the fault in order to predict it earlier so as to avoid its occurrence [cit] . condition monitoring systems (cmss) have been implemented in wind farms in recent years to help improve wind turbine operational efficiency and reduce o&m costs. [cit] . the purposefully designed cmss are utilized to record high-frequency signals (e.g., 2-20 khz, or even higher) to help improve the fault diagnosis accuracy. however, one concern for deployment of cms is the cost. the majority of old wind turbines have not been equipped with cms due to the extra added cost. it is, therefore, important to conduct wind turbine component condition assessment by using supervisory control and data acquisition (scada) system data. as scada system data is recorded regularly during wind turbine operation, the wind turbine component condition status may be revealed using scada system data. therefore, the research focus of this paper is to provide wind turbine fault detection based on scada system data."
"the core of the convolutional neural network is to train and learn from a large amount of sample data, and to extract the deep feature expression of the sample data through multiple"
"personal publishing tools provide users the possibility to publish content online [cit] . articles in these tools can directly be published. therefore there is no such a filter function in these tools [cit] . the main tools of this category are weblogs and microblogging services. the word weblog comes from the words \"web\" and \"logbook\" [cit] . merriam webster describes weblog as \"a website that contains an online personal journal with reflections, comments, and often hyperlinks provided by the writer;\" [cit] . there are different types of blogs such as private blogs, corporate blogs and media blogs [cit] . in the context of daily news there are blogs which are critical to the traditional media for example the spiegelkritik.de . another genre which is especially spread in the usa is warblogs. this kind of blogs has emerged with wars in afghanistan and iraq [cit] . warblogs are good illustrations of social software in use of a source of daily news."
"the pew research center's project for excellence in journalism discussed americans' reliance on social media in their daily news seeking. accordingly 9% of interviewees stated that they receive news through facebook or twitter very often. 36% of interviewees visit news websites or apps regularly. excluding people who do not receive news online at all, the percentages raise from 9% to 52% and from 36% to 92%. accordingly 92% of online users still visit news websites. social media users keep visiting the news websites/applications for the daily news. therefore, pew research center concludes that the traditional media still dominate, followed by facebook and then by twitter. authors state that social media are additional paths to news, not replacement for more traditional ones [cit] ."
"the grid side converter is ready to be closed and the wiring is faulty this paper proposes to map the above three types of fault indicators data onto radar charts, and use convolutional neural networks and svm methods to extract the image information of the above three groups of radar charts under the normal and faulty operation of the wind turbines for fault detection. at the same time, in order to prove the effectiveness of the proposed method, it uses svr method to directly process the data and train the model, to realize the fault detection. using the above methods, the operation status of the generator, converter and pitch system in the next shorter period of time (e.g., in next 15 days) can be estimated, i.e., normal status or operation with faults. by comparing the fault detection accuracy, it is verified that the convolutional neural network is more suitable for detection of the frequently occurred faults in generator, converter and pitch system during wind power production. therefore, the contribution of this study is to develop and apply convolutional neural networks and svm method to wind turbine fault detection based on the radar charts generated using the fault indicators data extracted from the scada system. the developed methods are applied to 10-min data and the data with higher sampling frequency such as one sample per 10 s. it is found that this is a new attempt to use convolutional neural networks to deal with wind turbine scada system data for fault detection based on a quick literature survey in google scholar and web of science."
"in the above formula, q is the desired output, p is the actual output, and h(p, q) is the cross entropy loss. when the actual output p is closer to the expected output q, the value of the loss function is smaller, and conversely, the value is larger."
"the svm algorithm is used fault detection based on radar charts generated corresponding to different operating states of wind turbine generator, converter and pitch system by following the procedure described in section 3.2."
"the forecast indices, tp, fn, fp, and tn for generator fault detection are shown in table 24 . using the svr method, the fault detection accuracy for the generator and pitch system can reach more than 74% but the fault detection accuracy for the converter is only above 63%."
"although microblogging services belong to the newer generation of social software, 31 % of the users quoted that they acquire daily news through such services. also this result might be explained with the success of a sole provider: twitter ranked on 10th among top 500 site rankings [cit] ."
"the task is to determine a cross entropy as the loss function. the cross entropy characterizes the distance between the actual output (probability) and the expected output (probability), as shown in equation (2):"
"the general process of using svm to process image classification problem is as follows: (1) use appropriate algorithm to extract feature data from image data and establish data samples; (2) select training data set, test set and kernel function, and use the training data set to train classifier model;"
"the web is an emerging source of current news. [cit], 48% of the web users stated that they use the internet often/occasionally to access in daily news. [cit] this increased to 61 % [cit] . currently internet is an essential source of daily news. however in the web there are numerous instruments enabling users to access in daily news."
whereas the second category social software focuses on the user and collectively produced content. therefore the community aspect is the main point of this category.
funding: this work was partially supported by the national natural science foundation of china (61703135; 61773151; 51577008); hebei natural science foundation [cit] 202231); youth fund of hebei education department [cit] 122); the excellent going abroad experts training program in hebei province.
"wikis (39%), weblogs (36%) and microblogging services rank similarly and are used from roughly more than one third of the users. the fact that wikis rank on the third place may be expected with news sites like wikinews and wikipedia. knowing that these are relatively mature news platform these are surprisingly ranked in lower rank in our survey."
"with increasing number of deep learning network layers, the features of different layers can be extracted. the more abstract the feature expression, the richer the semantic information. however, a simple increase in the number of original network layers in deep learning may result in gradient disappearance or gradient explosion. the traditional solutions generally use reasonable weight initialization and regularization methods to solve the gradient problem [cit], but bring new problems of network performance degradation [cit] . resnet is a residual learning framework that can improve the network performance under the premise of increasing depth. resnet's residual unit structure diagram is shown in figure 3 [cit] ."
"(3) use the test set to test the obtained model for fault detection; and (4) finally, the classification result and the classifier effect evaluation are obtained."
"overall we can state that the users' behavior towards social media news is still more passive since the user access social web for its news consumption. however, the possibility to publish and disseminate news is perceived by few users, we assume that there is a high potential in this area, as the user will more and more adapt to his new role and develop a more active social media news habit."
"therefore, this paper adopts the residual network structure as shown in figure 3 to improve the performance and accuracy of the network by increasing the network depth. the residual structure can be applied to solving the problem of gradient disappearance caused by the increase of network depth."
"these security mechanisms can be used, or adapted to support iot [cit] . however, they tend to operate at specific policy enforcement points, concerning a particular principal taking a particular action. while this protects the specific interaction, it does not provide continuous control over the data. the approaches do not allow a data producer to control, or trace, their data after it has been transferred to a third-party."
"as shown in the figures 7 and 8 below, the mean and variance of the extracted features are significantly different under two different operation states of the system with and without faults. since there is only some part of the overlap of the two curves shown in figures 7a and 8b, it proves that the svm method is effective to detect the system operation status with and without faults."
"one such approach is to leverage trusted platform modules (tpm) [cit], as used for remote attestation [cit] . tpm is used to generate an unforgeable hash representing the state of the hardware and software of a given platform, that can be remotely verified. therefore, a company could audit the implementation of a difc enforcement mechanism and ensure that the kernel security module, messaging middleware and the configuration they provide are indeed running on the platform. any difference between the expected state of the software stack and the platform would be detected and might represent a breach of trust. tpm, with remote attestation, is reaching maturity for cloud computing [cit], with ibm rolling out an open source, scalable trusted platform based on virtual tpm [cit] describe a mechanism allowing tpm and remote attestation to be provided for virtual machine and containerbased solutions, covering the whole range of contemporary cloud offerings. furthermore, the approach not only allows the state of the software stack to be verified at boot time, but also during execution, and can thus prevent run-time modification of the system configuration. similar mechanisms exist for mobile phones [cit] and embedded systems [cit] ."
"for the nonlinear case, the svm first maps the input vector x to the high-dimensional feature space through the selected nonlinear mapping, and then constructs the hyperplane in this space for optimal linear classification."
"the indicators data is processed prior to model training. as described in section 2, 200 sets of data are selected representing operation of generator, converter and pitch system with and without faults, respectively. the training data set includes 18,100 rows of the seven indicators data, and the test data set includes 9050 rows of the data."
in the last section we analyzed the usage of social software. a further step which will be discussed in this section is in which way or for which purpose social software is accessed in the context of news. with this approach we can also find out if social web usage in this context more active or passive. therefore we differentiate between three following purposes:
"in addition, social networking sites have the function to found groups. within these groups user can participate in discussions. 13,3% of the respondents said that they take part in discussion in these sites. also this figure is the highest within the item participation in discussion compared to the other applications. despite the relative low figures for news production and dissemination we assume that social networking sites are used in a passive but also in active manner. [cit] or the arab spring has shown that facebook was a way to keep the public updated by actively publishing content."
"the detection evaluation index values by the convolutional neural network model, the svm and the svr method for detection of faults relating to the generator, converter and pitch system failure are summarized in tables 30-32. in these tables, false alarm rate is given by (1 − accuracy). it is a rate at which the wind turbine system health status is diagnosed wrongly."
an online survey was conducted on the news seeking behaviour in the context of social web to collect data for our analysis. the survey consists of 22 questions. this paper focuses below three questions:
"further, it has been demonstrated [cit] that applications running on top of an ifc-enforcing os need not be trusted. this indicates a security mechanism with an extremely thin tcb compared to those discussed in §ii."
to conclude web media users like the social media for features that traditional media does not provide thus social media is perceived as an additional path to the traditional media.
"before extracting the characteristics of the radar chart, the graphics of radar charts are first grayscaled and binarized, as shown in figure 6 . the grayscale values are set from 0 to 255. 0 means black while 255 means white. then it is to classify the image pixels into black or white such that the pixels with grayscale values of less than 128 is classified as black and the others are deemed as white."
"we now briefly consider the extent to which the established security mechanisms, assumed to be available for deployment when and where appropriate, can be used to meet the challenges brought by the emerging iot. we then highlight major remaining issues."
"social bookmarking services (13%), newsrooms (11 %) and photoshare platforms (11 %) are adopted roughly by 10 % of users. the result of social bookmarking services and newsrooms might be explained with their quiet new existing. the low user figures for photo-platforms may be related to the fact that most news articles include already pictures. therefore the search in these platforms costs additional work and time. however in some cases like natural disasters the extra work might be worth to get greater insights about the catastrophe."
"further studies on these questions will allow us to have a better understanding of the changing media landscape, the new role of the user in the news process as well as the power and dynamics of networks."
"the scada system data of one type of 1.5 mw wind turbines was collected. there are a total of 24 wind turbines in the wind farm. the wind farm is equipped with relatively new scada system that can afford data recording with sampling frequencies up to 1 hz. the data used in this study was extracted from the scada system with sampling frequency of once per 10 s and once per 10 min (i.e., 10-min data). because the wind turbine downtime caused by generator, converter and pitch system failure takes high proportion in operation, the overall detection is for the whole system of generator, converter and pitch system, respectively, not for a specific fault."
"by making comparison analysis of the above radar charts, it is found that each indicator's values are relatively stable and the distribution is regular under the normal generator operation whereas the indicator's values are fluctuating and the chart distribution is relatively cluttered under faulty operation. the traditional analysis methods are mostly based on the training analysis of the data. in this paper, the convolutional neural network and support vector machine (svm) methods are used to train models to identify the image characteristics during normal and fault operation. the fault detection accuracy is evaluated by analyzing the fault detection indices, which proves that the convolutional neural network is more suitable for detection of frequently occurred faults in wind turbine operation."
"difc protection is only guaranteed above the technical layer in which the difc mechanism operates. safe exchange of data in a difc-context relies on trust placed in this mechanism. in a cloud context, the enforcement mechanism is provided and guaranteed by the cloud provider. if trust can be established with the cloud provider, no other party need to be trusted to guarantee secrecy and integrity of data [cit] . this trust relationship can be established as the cloud provider is bound by contract, regulation and economic interest. this trust is demonstrated every day by companies adopting the cloud. however, when moving towards a potentially self-managed 'things' infrastructure, this trust relationship becomes more complex to establish. there is a need to demonstrate, reliably, that a particular machine has the appropriate untampered difc enforcement mechanisms in place."
"we described how our approach operates to complement existing security techniques, by enabling data management policy to be enforced continuously, system-wide. difc offers compelling advantages, but only if the difc implementation is itself trustworthy. our key contribution here, is in considering the role of trusted hardware and remote attestation, which when combined with a difc implementation, can raise the levels of trust and assurance in the supporting infrastructure."
"when analysed user group data, 67% of social web users appreciate the variety of information that is not offered by the traditional media thus integrates social web in the news seeking process. users prefer to choose news channels among a larger source and to by-pass traditional media that act such 'gatekeeper'. users of social media like to decide the importance and the relevance of an event by their own and dislike that traditional media filter the news for them. similarly 67% of the users like to be exposed to the different perspectives that social web offers."
"in this section, the svm method is used to predict the above three types of system failures. the establishment of the data sets is the same as section 2. svm is a machine learning method which improves the generalization ability of machine learning through structural risk minimization [cit] ."
"in conclusion it can be stated that user tend to integrate videoshare platforms, social networking sites and wikis for the news consumption. whereas for the news production and dissemination the use of social networking sites seems to be more common. however these results should be treated carefully and questioned in further studies with more participants."
"based on the data extraction and processing as described above, two groups of radar charts are generated representing normal and faulty operation condition as shown in figure 1 . the wind speed is selected from 3 m/s to 20 m/s (cut-in and cut-off speed). some of the data when wind speed is out of the range is removed. the collected data are plotted into the radar charts through the seven indicators selected and shown in tables 3, 5 and 7. in figure 1, the numbers 1-7 represent the indicators, respectively, in tables 3, 5 and 7. the radar charts are distinguished by wind speed. for example, when the wind speed is 3 m/s, all indicators data under the condition is collected to plot the radar chart. in the graph, if the data is relatively stable, there are fewer curve lines to be observed as most of the frames are overlapping so that the pattern is regular and, otherwise, it is vice versa."
"before extracting the characteristics of the radar chart, the graphics of radar charts are first grayscaled and binarized, as shown in figure 6 . the grayscale values are set from 0 to 255. 0 means black while 255 means white. then it is to classify the image pixels into black or white such that the pixels with grayscale values of less than 128 is classified as black and the others are deemed as white."
"such policies must account for secrecy concerns, such as privacy and confidentiality aspects, as well as integrity concerns, including data quality and authority. the policy must be consistently enforced, throughout the iot supply chain, backed with strong evidence of policy adherence."
"question 1: do users integrate social software in their daily news seeking? question 2: what are the reasons and motivations that underlie users' preferences relating to use of the social web in the context of daily news? the first research question revealed that among 180 participants who check online sources for daily news, 90 stated that they integrate social web in their routine of news seeking whereas the other 90 participants negated this question. to get insights on motivations underlying users' preferences, participants have been split into two equal groups of users and non-users of social media. the group of non-users replied 7 questions in a five-level likert scale while the group of social media users answered to 5 different statements. participants are allowed to replies as agreed, disagreed or no answer."
"a confusion matrix shows visualization effect of the performance of an algorithm through a specific matrix, which is a situation analysis table that summarizes the detection results of the classification model in machine learning. in the evaluation, the terms of tp, tn, fp, and fn are utilized. tp (true positive) means that the true value is true and the predicted value is true; fn (false negative) means that the true value is true and the predicted value is false; fp (false positive) means that the true value is false and the predicted value is true; and tn (true negative) means that the true value is false and the predicted value is false. the diagnosis performance of a svm classifier is evaluated by the following five indices [cit]"
"videoshare platforms are accessed from 36,7 % of the users in order to get news. about one-third (33,3 %) of the respondents state that they use social networking sites for their daily news consumption. also wikis are used from 33,3% for this purpose. weblogs are taken into account from 25,6%. one in five (20%) get news from microblogging services like twitter. the percentages for the other tools are under 10%: newsrooms (7,8%), photoshare platforms (4,4%), social bookmarking services (2,2%). the figures for the news consumption are relatively high compared to the figures for news production and news dissemination. especially in the case of videoshare platforms we find high differences: whilst it is the most used platform in the consumption routine, only 2,2% produce or publish videos in these platforms."
"the comparison of the detection evaluation indices for generator faults is given in table 30 below. it can be seen from the above table that the convolutional neural network and the svm method are better than svr model for detection of generator faults. the accuracy and the precision of the convolutional neural network are the highest among the three methods. the detection accuracy is 97.12 and the precision is 100. by comparing to the other two methods, the detection accuracy is obviously improved using the convolutional neural network method for detection of generator faults."
data reveals that the main reason for not using the social web as a source for current news is that 'anybody can write an article and publish it'. 82.2% of the non-users do not intend to use social media as the articles are not written by qualified journalists.
"we propose using decentralised information flow control (difc) [cit], that extends traditional ifc [cit], to address the security and privacy concerns inherent in iot. this is by allowing the continuous management of data as it flows throughout iot systems, including within and between cloud services. ifc technology has has been used for embedded software in bmw cars [cit], has been proposed to control data usage of third party applications in a social network environment [cit], and has been demonstrated to be applicable to more general use cases [cit] . our own implementation, camflow [cit], extends ifc with provenance-like features [cit], by providing a directed graph audit mechanism, to allow demonstration of compliance with policy (regulations and contracts)."
"with increasing number of deep learning network layers, the features of different layers can be extracted. the more abstract the feature expression, the richer the semantic information. however, a simple increase in the number of original network layers in deep learning may result in gradient disappearance or gradient explosion. the traditional solutions generally use reasonable weight initialization and regularization methods to solve the gradient problem [cit], but bring new problems of network performance degradation [cit] . resnet is a residual learning framework that can improve the network performance under the premise of increasing depth. resnet's residual unit structure diagram is shown in figure 3 [cit] ."
for the purpose of this paper the second category will be the guiding aspect of social software where users play an active role in the production.
this results in a great deal of trust being placed throughout the entire iot supply chain-which includes cloud service providers and other collaborative entities-that data will (only) be used and transferred in accordance with the purposes for which it was originally produced and/or shared [cit] .
another survey reveals that only 7% of the users seek for news on a daily basis in communities. whereas 62% stated that they never get their daily information in such platforms [cit] .
"the svm algorithm is used fault detection based on radar charts generated corresponding to different operating states of wind turbine generator, converter and pitch system by following the procedure described in section 3.2."
"for the faults related to the three types of wind turbine system failures, the detection accuracy by the convolutional neural network method is the highest, which is more than 94% using 10-s resolution data or 89% based on 10-min data; while it is close to 81% or 74% for the svm method, and 63% or higher for the svr model based on the preprocessing data. from the results obtained in this paper, it can be concluded that the fault detection method based on the convolutional neural network is more suitable for wind turbine system fault detection. in the future, the convolutional neural network and the svm method will be applied to and tested for detection of other wind turbine faults based on the scada system data."
"another argument that disfavour social media content that it tends to be subjective and even personal. additionally 66.7% of non-users distrust social media content because it is not reviewed and approved by an authority. non-users are sceptical of the truthfulness of news coming from social media. to sum up non-users of social media intentionally avoid the content generated by the users, which is the central element of social media."
"generator fault detection by analyzing the above indices, the detection accuracy of svm method for fault detection is higher than 80%, which proves the feasibility of this method in application."
in summary social networking sites and video sharing platforms i.e. facebook and youtube have widely adopted in the context of daily news. however weblogs and specially microblogging services such as twitter should not be neglected in this context.
"the core of the convolutional neural network is to train and learn from a large amount of sample data, and to extract the deep feature expression of the sample data through multiple iterations, and finally provide diagnosis according to different tasks and sample data [cit] . this paper proposes a radar chart classification model based on the restnet50 convolutional neural network structure to detect fault. the method includes preprocessing of the collected data, training of the target radar chart, and model performance testing. the training flow chart of the convolutional neural network model is shown in figure 2 . iterations, and finally provide diagnosis according to different tasks and sample data [cit] . this paper proposes a radar chart classification model based on the restnet50 convolutional neural network structure to detect fault. the method includes preprocessing of the collected data, training of the target radar chart, and model performance testing. the training flow chart of the convolutional neural network model is shown in figure 2 . the procedure for data preprocessing and data set construction has been described in section 2. in the training process of the convolutional neural network model, the characteristics of the images in the training data set are learned; as the number of network iterations increases, the network parameters are adjusted accordingly; thus, more representative image features are extracted, and finally the system model is determined for identification of fault condition."
"the detection evaluation index values for detection of the pitch system faults are shown in table 32 . it can be seen from table 32 that the detection accuracy for the pitch system faults by the convolutional neural network is higher than the svm method and the svr model. the false alarm rate is obviously reduced. overall, it is better than the svm method and the svr model for detection of the pitch system faults."
"for example, a patient alice may be discharged from hospital to home monitoring. home monitoring data tagged as medical in its secrecy label s can only flow to a processing entity that contains a secrecy tag medical in its s. this ensures that data produced by alice's home monitoring equipment is only processed by a dedicated cloud application. further, selected medical data may be released for research. such data should be secrecy tagged with the research purpose, e.g. diabetes-research, to ensure proper data usage. further, entities running in the research environment may want to specify an integrity tag consent-to-research as their requirement for receiving and taking responsibility for the data [cit] ."
"since the generator failure frequency is relatively higher with the selected type of wind turbines, we use the generator failure as example to explain the data collection and processing process. first, the generator fault indicators data is extracted. these fault indicators are those given in table 3 above. use the data collected by the scada system from 15 days ahead of the time when a failure occurs as the generator faulty data. the faulty data were collected from among the 24 wind turbines. in addition, when there is no fault occurrence, the above indicators data is extracted from the scada system as the normal system operation data. secondly, 100 sets of faulty operation data and 100 sets of normal operation data are selected. each data set contains seven columns of the indicators and 181 rows of the data collected. by combining the 100 data sets representing the generator in faulty operation, a matrix q 1 is constructed which contains 18,100 rows and seven columns. similarly, a matrix q 2 is constructed including 18,100 rows and seven columns of normal operation data. therefore, the whole data set including the both faulty and normal operation data is given by another matrix, m. m includes q 1 and q 2 with 36,200 rows and seven columns. the extraction of the indicators data for converter and pitch system fault detection is the same as above. it should be noted that the fault indicators used for fault detection of generator, converter and pitch system are different. therefore, we present data tables 8-10 to illustrate the collected data. table 9 shows the data collected for the relevant indicators when the converter system fails. it can be seen from the table that when the fault occurs, the rotor speed and the converter speed are reduced, and the converter input power cannot reach the rated value. it can be seen from table 10 above that although the wind speed is within the power generation range, the pitch system failure causes the wind turbine to stop operation, the rotor speed is significantly reduced, and the system output power is 0 wm at some times. under the faulty condition, the rated power cannot be achieved."
"the svm method is used to detect the faults in generator, converter and pitch system. the results are shown in tables 18-23. a. generator fault detection"
"author contributions: c.x. was responsible for theoretical development and developed algorithms; z.l. contributed to the methodology and verified the modelling; x.z. helped to collect data as required and develop algorithms together with c.x.; c.x. and t.z. drafted the manuscript; t.z. verified the algorithms and the results, finalized the paper and was responsible for paper revision and submission. each author has contribution to the research approach development. all authors have read and agreed to the published version of the manuscript."
the information management tools have their focus more on supporting the information management than on communication or producing content [cit] . examples for these tools are social bookmarking services as digg and social newsrooms such as yigg.
"on the one hand there are online formats of tv and radio channels as well as of newspapers and magazines. on the other hand there are newspapers and magazines that are exclusively published online. in the context of this paper all these formats are categorised as traditional media. in contrast to traditional media, a new trend has emerged where ordinary web users publish information. tools and platforms such as weblogs and social networking sites give users the chance to share, comment, publish, produce any kind of information online; where the sole requirement is to have internet access. compared to the traditional media here, users adopt an active role in news production and may act such as journalists or editors. this paper investigates news seeking behavior in the web 2.0 context with the help of an online survey. motivations underlying users' preferences have been examined."
"in wind turbine operation, electrical and control systems, such as the pitch control system are found to be the most prone to failure. table 1 below provides statistics of wind turbine shutdown time caused by each subsystem in operation. the statistics are based on the data collected from two wind farms in china. it can be seen that the downtime caused by the generator, converter and pitch system accounts for 68% of the total downtime [cit] . therefore, the study of this paper focuses on the wind turbine failures caused by these three subsystems. table 1 . percentage of downtime per wind turbine subsystem [cit] ."
"for each type of generator, converter and pitch system failure, 17,001 normal radar charts and 17,001 fault radar charts were generated. since the selected fault indicators are all 7 and the operation process is consistent, we use the indicators data representing the generator's normal and faulty operation conditions as an example to illustrate the analysis process. for the generator case, a total of 11,900 images of radar charts were selected for training the models and another 5101 images of radar charts were used for testing."
other factors such time and accessibility demotivates users to adopt social media as well. data reveals that 29% of the participants think that getting news from social web is time consuming. and 43% of them think that searching for news in web platforms is very difficult.
on the other hand users of social media like to decide the importance and the relevance of an event on their own and dislike that traditional media filter the news flow for them.
"in this context an ifc-labelled active entity may be a cloud application instance, a mobile application, a process on a desktop os etc. an entity's local flow of data (through files, pipes, shared memory) is locally constrained. the lsm provides security hooks on interactions between kernel objects, which is where difc policies are enforced. external flows to and from components running under difc constraints are strictly restricted to the messaging mw which ensures security across machines: including secure channels and component authentication/authorisation. software integrity verification is also possible through hardware features (see §vi)."
it is interesting to notice that the fault detection accuracy for pitch system is higher than generator and generator is better than converter by using each of the three methods. the pitch system faults are composed of both mechanical and electrical component faults while the generator failure is mainly caused by the electrical system faults and the converter system failure is mainly due to the failures of microelectronic components. a mechanical system fault typically presents a clearer degradation process (wear-out process).
"moreover, it could be argued that the user still have to adapt his new role in the news process which includes a more active and participating user behavior. this is the main reason why the changing media landscape is often characterized as follows: \"it focuses on the process more than the product\" [cit] ."
"data management remains a challenge for cloud computing, and these issues will be exacerbated by the emerging internet of things. in this paper, we presented our ongoing work on difc, as a mechanism for managing data within and throughout cloud-supported iot infrastructure."
"internet of things (iot) solutions tend to rely heavily on cloud computing. in a recent survey [cit], 33 out of 38 of the iot infrastructures surveyed involved the use of cloud services. traditionally, iot applications were built in silos where sensors and actuators, cloud services and end-user applications were tightly coupled and inseparable. current research and standardisation efforts aim towards breaking those silos, by allowing fully customisable service chains [cit] ."
the convolutional layer extracts the features of the image by using convolution operation of the convolution kernel and the input image. the features extracted by different convolution kernels are different. the calculation formula of the convolution is:
"for each type of generator, converter and pitch system failure, 17,001 normal radar charts and 17,001 fault radar charts were generated. since the selected fault indicators are all 7 and the operation process is consistent, we use the indicators data representing the generator's normal and faulty operation conditions as an example to illustrate the analysis process. for the generator case, a total of 11,900 images of radar charts were selected for training the models and another 5101 images of radar charts were used for testing."
"news consumption is the reading of articles or watching videos/pictures which is a more passive behavior. whereas the news production is a more active habit as it includes writing articles and participating in discussions. the answer item \"participating in a discussion\" was pointed out separately as it is a key feature of social media and stands for its interactivity. also news dissemination is an active behavior since it implies sharing and forwarding news articles, links or videos/pictures."
finally web platforms offer interactivity between users and the content. in social web users participate in discussions and shares their opinions with others. dan gilmore compares this new form of journalism with conversation or seminar [cit] . users that have already accustomed to interactive feature of social web applied it in the context of daily news as well.
"as already stated in question research 3 newsrooms take a minor role in the users' news behavior within the context of social media. this fact can be confirmed with the figures for news production (3,3%) and dissemination (0%). similar to newsrooms are the figures for photoshare platforms. only 4,4% access them to publish news information. the percentage for the dissemination is even lower with only 2,2%."
"on the other hand a subcategory, microblogs, let users to publish content in a limited way [cit] . twitter is most well known microblogging service. [cit] when a passenger took the pictures an emergency landing and posted in real time in twitter o'reillys to celebrated twitter as \"the world's real-time newspaper\" [cit] . this example supports the thesis that microblogging services could play an essential role on the news seeking attitude."
"conducted online survey reveals two districts group of user and non-user of social media. nonusers intentionally avoid content generated by the users, which is the central element of social media. non-users are sceptical of the truthfulness of news coming from social media."
"we argue that there is a clear need to complement the existing security approaches with mechanisms that operate beyond the point-based interaction, applying continuously throughout the iot infrastructure. simple low-level, data-centric security primitives must be provided to ensure the enforcement of the data owner's requirements regarding how and where its data is used. furthermore, this security mechanism should be agnostic to and separate from applications."
"besides social networking sites, video sharing platforms functions as new platform as well. 55% of the respondents stated that they acquire news through video sharing platforms. this figure might be explained with the remarkable success of youtube which is ranked third on the top 500 sites rankings [cit] ."
"another widely spread platform are social networking platforms or social networking sites. user has to register and develop their personal profile in order to interact with other users [cit] . among the social networking sites facebook is the most successful and ranked in first place at the 500 top sites worldwide [cit] . mark zuckerberg, the founder of facebook once said, that in the last 100 years journalists decided for the people which news are worth to be reported about. now with platforms like facebook people decide for themselves. he added that he trust an article or information more when it is recommended or written by a friend than by a random journalist that you even do not know [cit] ."
"the data reveals that social networking sites play a major role on news seeking process. 78% of social web users seek news via these platforms. as described in 2 social networking sites, especially facebook, gained a lot of success in the last years by attracting extensive public interest."
the wiki technology allows registered users to edit existing articles. in this form of social software the community takes over the role of an editor and assures somehow a certain level of quality [cit] . the technology gained his first success with the raise of wikipedia [cit] .
almost half of the users marked that they access to social web platforms in order to get information directly from eye witnesses. especially in the raising of smartphones this argument is getting more relevant.
"especially with the rise of smartphones, development of their video cameras, and the mobile web reports of eye witnesses will get more and more relevance as they can upload their videos of an event or accident in real time in one of their social web accounts."
"difc considers two types of security guarantees. secrecy policy broadly covers the usage/confidentiality of the data, restricting its propagation to components trusted to handle such types of data. for example, medical data may be restricted to approved devices and services, or data may be restricted to a certain geographical area [cit] . integrity broadly covers the assessment of some properties (e.g. quality, state, origin) associated with the data. for example, a certain accuracy may be required or a storage service may refuse the responsibility of handling confidential data 'in-clear' and may first require its encryption [cit] ."
"the convolutional neural network resnet50 is used to diagnose the operating states of the wind turbine generator. the values of the forecast indices, tp, fn, fp and tn are shown in table 12 below. based on the data obtained above, the detection accuracy, the detection is true accuracy, the true is true accuracy, the true false accuracy, and the detection is false accuracy, are shown in table 13 . table 13 . detection evaluation indices. the convolutional neural network resnet50 is used to identify the operating states of the wind turbine converter, and the indices, tp, fn, fp, and tn are given in table 14 below. the detection evaluation indices are shown in table 15 . by analyzing the data shown in tables 12-17, it is found that the method of using the convolutional neural network for model training can give a fault detection accuracy of over 94.8%."
publishing articles or taking part in a discussion is always more time consuming and takes also extra work. therefore it seemed that not all users are willing to accept this additional work. since sharing or forwarding takes also extra time and work this argument could also be applied to the result of the news dissemination.
to conclude web media users like social media platforms for features such as interactivity and diversity that traditional media does not provide thus social media is perceived as an additional path to the traditional media than being its alternative.
"difc policies are simple to express and enforce. this is particularly important in an iot context as low powered edge devices need to be able to enforce the policy. further, trusted components are able to change the security context of entities; e.g. declassifying data after anonymisation or endorsing data after sanitisation. we have demonstrated [cit] that, through the simple flow constraints, and trusted components declassifying and endorsing data, it is possible to build complex policies."
"by using trusted hardware, remote attestation and a provenance-like audit log generated during difc enforcement, we are able to produce audit data that can be used to demonstrate compliance with regulatory or contractual obligations."
"when using the svr method for fault detection based on the indicators data, the first step is to perform data preprocessing and remove the useless data. then, use the preprocessed data to train the selected model."
"since the defendant's computer had a complete copy of the movie, the tracker server labeled it as a \"seeder computer.\" the defendant maintained the connection between the tracker server and his computer so that other peers could download the movie from his computer."
the probability values for the network nodes are presented in figure 3 . figure 3 (a) presents the initial probability values in the network without any observed evidence. figure 3(b) shows the updated probabilities assuming that evidence e 1 is observed to be yes while e 2 and e 3 are still unobservable.
"the input weather dataset contains the values of temperature, time, place etc, the dataset file on the fig.2 . is split into data blocks. a method known as an inputsplit is used to control the block size. inputsplit is located within a fileinputformat class. the quantity of the splits is subjective to the size of the block, hadoop distributed file system, and one mapper job is made for every data block split. on the datanode is the mapper process which receives each split block of data, called the \"record\" and process it. in this proposed framework, key value pairs are created by the map process, where the value represents the (temp) and key represents the word (place). then, the key value pairs are sorted by the key type into lists. the sorted outputs then become inputs to reduce tasks, and which eventually reduce the values of the dataset volume. the reduce outputs are simply a list of key value averages. the mapreduce framework controls every single other task, such as resources and scheduling."
"it is the first study to propose a method to quantify the uncertainty of the prediction for mapping using a neural network model. on average, the prediction uncertainty was slightly underestimated (table 4) . this is an expected result reported in previous studies [cit] and 15%, respectively. in the present study were obtained 90.51% for clay and 92.88% for sand, which are values very close to the expected value of probability interval. the uncertainty was therefore much better quantified than this existing work using the same dataset in similar conditions."
"a cnn model is composed of several layers [cit], among which an input layer supplying the images x to the network. the input layer is 85 connected to a hidden layer which in turn is connected to another hidden layer or to an output layer. each layer contains neurons which are independent within the layer but connected to each neurons from the previous and to the next layer. hidden layers can be classified in three main categories, called convolutional, pooling and fully connected layers."
"which should be distributed as χ 2 with 1 degree of freedom so that its mean δ should be close to 1. [cit] showed that its medianδ is more useful because it is less sensitive to small or large values,δ should be close to 0.455."
"to illustrate the bayesian network methodology, we focus on hypothesis h 1 : the pirated file was copied from the seized optical disk (found at the crime scene) to the seized computer."
"note that other evidence exists in the bittorrent case. this includes email exchanges, detailed comparisons of the torrent file metadata with computer trails, and timeline analysis. however, as the focus of this paper is to demonstrate the utility of bayesian networks in digital forensic investigations, only the most important pieces of digital evidence were considered in the discussion."
"equation (3) is the celebrated bayes' theorem. from a statistical point of view, it denotes the conditional probability of e caused by h. this is also referred as the likelihood ratio of h given e. it denotes the degree of belief that e will occur given a situation where h is true."
"to enhance the reliability and accuracy of the probability assignments for posterior evidence, we attempted to use objective probability assignments obtained by aggregating the responses of experienced law enforcement agents and analysts. a questionnaire (available at www.cs.hku.hk/kylai/qr.pdf) was created to obtain the required information from personnel with the technical crime bureau of the hong kong police and the computer forensic laboratory of hong kong customs. the questionnaire solicited the following information from the respondents: (i) digital forensics training and experience, (ii) degree of belief in digital evidence resulting from general computer operations, and (iii) degree of belief in the digital evidence related to the operation of the bittorrent protocol."
"furthermore, since h 1, h 2, h 3, h 4 and h 5 are in a diverging connection with the parent hypothesis h, changes to the probabilities of h 2, h 3 and h 4 influence the probabilities of h 1 and h 5 . table 5(b) shows the probability values obtained after the states of e 8 and e 14 are changed from yes to no. the propagated probability for h from the available evidence is 92.27%. in other words, based on the observed evidence, there is a probability of 92.27% that the seized computer was used as the initial seeder to distribute the pirated movie on a bittorrent network. this is the most that a digital forensic analyst can provide. it is up to the court to decide whether or not this probability value is sufficient to support the hypothesis h."
"h : the seized computer was used as the initial seeder to share the pirated file on a bittorrent network. next, we express the possible states of the hypothesis (yes, no and uncertain) and assign probability values to these states. the values are also called the prior probabilities of the hypothesis."
"hadoop is a platform that offers an efficient and effective method for storing and processing massive amounts of data unlike traditional offerings, hadoop was designed and built from the ground up to address the requirements and challenges of big data. hadoop is powerful in its ability to allow businesses to stop worrying about building big data capable infrastructure and to focus on what really matters: extracting business value from the data [cit] . apache hadoop use cases are many, and show up in many industries, including: risk, fraud and portfolio analysis in financial services; behavior analysis and personalization in retail; social network, relationship and sentiment analysis for marketing; drug interaction modelling and genome data processing in healthcare and life sciences and so on to name a few. in addition, many companies provide hadoop commercial execution and/or support, including cloudera, ibm, mapr, emc, and oracle. [cit] ."
"mapreduce is a programming paradigm associated with the hadoop and have two separate and distinct tasks. first is the map job, which takes a set of data and converts it into another set of data, where individual elements are broken down into (key/value) pairs. the reduce job takes the output of map as input and shuffle/sort it and reduce the (key/value) [cit] . as the sequence of the name mapreduce the reduce job is always performed after the map job. putting the map and reduce functions to work efficiently requires an algorithm too. the standard steps for a mapreduce work-flow. mapreduce using to processing the data that storing in datanode. hadoop distributed file system (hdfs) implementation of a distributed filesystem [cit], t is designed to hold a huge amount of data, and provide access to this data to many clients distributed across a network. the big data collected by ncd0c (national climatic data center) is collected across more than 116 weather stations and more than 1000 observations centers. the data generated by them is unstructured, which becomes a challenging task to analyze it. the sample picture of the raw data is shown below in fig.2 . these huge amount of data is the loaded onto a hadoop distributed file system. this file system consists of number of clusters. once the data is loaded onto the hdfs file system the data is balanced across the clusters. these hdfs system are fault tolerant, as there exists replication of data among the clusters and if any one of the node fails the whole setup doesn't crash, hdfs obtains the replicated file system. thus one the semi structured data is uploaded on to hdfs file system the data can be used by tools for analysis."
"aitken and taroni [cit] state that likelihood is an exercise in hypothetical reasoning. it denotes the degree of belief in the truth of a hypothesis. in the scientific community, belief is often expressed in terms of probability. probability theories provide mechanisms for deducing the likelihood of hypotheses from assumptions. although probabilistic methods may be useful for proving or refuting the hypotheses involved in a criminal investigation, jones and co-workers [cit] argue that obtaining all the probability distributions for the entailing evidence is impractical. given the large volume of evidence involved, it is not feasible to obtain the joint probability distributions for all possible evidential variables. moreover, simple probabilistic methods do not capture the complex dependencies that exist between items of evidence; therefore, the methods have limited value from an analytical point of view [cit] . indeed, many researchers [cit] emphasize that comprehensive probabilistic models should accurately model the conditional dependencies existing between items of evidence."
"veracity: due to increase in velocity, data cannot be cleaned before using it. hence for decision making and business there must be a mechanism to deal with imprecise data. big data is the combination of precise, imprecise, accurate, inaccurate form of data [cit] ."
"to distribute the movie, the defendant sent the torrent file to several newsgroups. he then activated the torrent file on his computer, which caused his computer to connect to the tracker server. the tracker server queried the defendant's computer about the metadata of the torrent file. the tracker server then returned a list with the ip addresses of peer machines on the network and the percentages of the target file that existed on the peer machines."
"it is worth mentioning that the \"torrent file node\" (e 8 ) is a common node for h 2, h 3 and h 4 . in other words, there is a converging connection to e 8 from these three hypotheses. according to the rules of probability propagation for a converging connection, when the state of e 8 is known, the probabilities of h 2, h 3 and h 4 will influence each other. therefore, a change in the state of e 8 changes the probabilities of these three hypotheses."
"a bayesian network uses probability theory and graph theory to construct probabilistic inference and reasoning models. it is defined as a directed acyclic graph with nodes and arcs. nodes represent variables, events or evidence. an arc between two nodes represents a conditional dependency between the nodes. arcs are unidirectional and feedback loops are not permitted. because of this feature, it is easy to identify the parent-child relationship or the probability dependency between two nodes."
velocity: refers to data arriving at high speed. big data analytics are near real-time hence high performance and high computing facility is needed [cit] .
"to estimate the variance of the prediction, i use a two-step method called bootstrap plus variance estimate [cit] . recall from eq. 1 that errors are assumed to be statistically independent and identically distributed. using a predicted mean of the soil property of interest by regression model 135f (x,θ), shortly denotedf hereafter, one can rewrite eq. 1 as:"
sandy soils such as podzols are found in the landes or sologne while large areas in the massif central and brittany are covered by dystric cambisols originating from moderate weathering of different types of parent materials [cit] .
"in addition, the prediction intervals coverage probability (picp) is computed. the picp is the percentage of observations covered by a defined prediction 195 interval [cit], in this case corresponding to a 90% probability of occurrence. the picp is calculated as follows:"
"items of digital evidence correspond to past digital events (or posterior evidence) that can be used to support or refute the five sub-hypotheses, which, in turn, support or refute h. one of the main challenges in applying a bayesian network to evaluate evidence is assigning probability values to posterior evidence. this is because the assignments are usually based on subjective personal beliefs. although the personal beliefs (regarding a case) of a digital forensic analyst are assumed to arise from professional knowledge and experience, there is no means to determine whether they truly represent the accepted views of the digital forensic discipline, let alone whether or not the probability values assigned to posterior evidence are, in fact, accurate."
"like other forensic disciplines, digital forensics involves the formulation of hypotheses based on the available evidence and facts, and the assessment of the likelihood that they support or refute the hypotheses. although substantial research has focused on principles and tools for retrieving digital evidence [cit], little, if any, work has examined the accuracy of hypotheses based on the evidence."
"the objectives of this study were to use a single cnn model for multivariate soil mapping and to quantify the uncertainty of the predictions. the methodology is tested in a potential application scenario, mapping topsoil clay, silt, sand, organic carbon, total nitrogen and ph in caci 2 solution over france. the 70 predicted soil maps were validated and the uncertainty was estimated of each soil property."
"the partial bayesian network for hypothesis h 1 is presented in figure 2 . the arguments describing events or evidence that would be caused by copying a file from an optical disk to a local hard disk are : (i) e 1 : modification time of the destination file equals that of the source file (states: yes, no, uncertain), (ii) e 2 : creation time of the destination file is after its own modification time (states: yes, no, uncertain) and (iii) e 3 : hash value of the destination file matches that of the source file (states: yes, no, uncertain). the next task is to assign conditional probability values to the events or evidence. table 4 lists the conditional probabilities of e 1, e 2 and e 3, given the state of h 1 . next, the probability of h 1 based on the observed probabilities of e 1, e 2 and e 3 is calculated. the msbnx bayesian network editor and tool kit [cit] was used to calculate this probability and to propagate probability values within the bayesian network."
"the connection between the seized computer and the tracker server was maintained. since the sub-hypotheses are dependent on h, they are assigned conditional probability values. table 2 presents the conditional probability values of hypothesis h 1 given the state of h. initial or prior probability values are assigned to the possible states of h 1 for different states of h. for example, an initial value of 0.6 is assigned for the situation when h and h 1 are both yes. this means that when the seized computer has been used as an initial seeder, the probability that the pirated file found on the computer had been copied from the optical disk seized at the crime scene is 0.6. however, it is also possible that, although the seized computer was the initial seeder, the pirated file was downloaded from the internet or copied from another computer in a local network; a probability value of 0.35 is assigned to these scenarios."
"hypothesis h 1 is in a diverging connection with e 1, e 2 and e 3 . therefore, if the state of h 1 is unobserved, any change in the probability of e 1 will change the probability of h 1 . when h 1 changes, the likelihood ratios of e 2 and e 3 also change. similarly, since h, h 1 and e 1 are in a serial connection, a change in the probability of e 1 will propagate to h if h 1 remains unobservable."
"media reports about the bittorrent trial mentioned that there was no indication that the torrent file was present on the seized computer. also, there was no mention of cookies that are required to publish the torrent file in newsgroups. therefore, the corresponding observations about the existence of the created torrent file (node e 8 in figure 5 ) and cookies of newsgroups (node e 14 ) should be amended from yes to no in order to reveal their impact on the hypotheses."
six soil properties were predicted using a single multi-task cnn model. the cnn shared a common architecture for all soil properties. this reduces both the risk of overfitting and computational resources that would be needed to fit each model separately. this is important for future dsm studies because the number of available geodata is constantly increasing .
"the original dataset was randomly split between calibration (80%) and test (20%) sets (fig. 1) . all six soil properties (clay, silt, sand, oc, ph and n) were simultaneously selected for either test of calibration. each soil property was normalized between 0 and 1 while the covariates were standardized by subtract- next, a sequential multi-task cnn model is built for predicting jointly the six soil properties. the cnn model has a common architecture for the convolu-260 tional and pooling layers followed by separated fully connected layers which output either one (model for bootstrap) or two (model for variance estimate) values per soil property. the model specifications are reported in table 2 and a graphical representation is given in fig. 2 . zero padding is always applied to the convolutional layers to preserve the original size of the input image and 265 conserve information at an early stage of the network."
"picp shows however that the 90% prediction interval covers satisfactorily the observed values of clay, silt and sand but is too wide for ph and n and slightly too narrow for oc. this is confirmed by the accuracy plot in fig. 7 : prediction intervals as obtained for the soil properties are too narrow, leading to an overall underestimation of 375 the uncertainty. this is more severe for ph where deviation from the 1:1 line is entirely due to underestimation of the uncertainty. uncertainty for oc and n is underestimated for small values of nominal q while it is overestimated for nominal q values in the range 0.6 − 1."
"value: value refers to hidden information from big data. the challenge here is to identify, extract, transform and analyses this information to find the hidden value from it. a number of studies have been reported on efforts made by researchers at improving the analytics with big data and their impact on value and when we talk about weather forecasting with big data, numerous research focus on this matters. these research is still in progress since they found a hole between weather forecasting and big data analysis. analyzing patient, characteristics with combination results of medications [cit] ."
"weather forecasting is always a big challenge for the meteorologists to predict the state of the atmosphere at some future time and the weather conditions that may be expected. it is obvious that knowing the future of the weather can be important for individuals and organizations. accurate weather forecasts can tell a farmer the best time to plant, an airport control tower what information to send to planes that are landing and taking off, and residents of a coastal region when a hurricane might strike [cit] . the process of weather forecasting is developing as the effect of advancement in technology right from the realization of increasing size of data, weather forecasting was found to be based on big data [cit] . different sensors employed in the smart city can be used to measure weather parameters. weather forecast department has begun collect and analysis massive amount of data. they use different sensor values like temperature, humidity to predict the rainfall etc. when the number of sensors increases, the data becomes high volume and the sensors data have high-velocity data. there is a need of a scalable analytics tool to process the massive amount of data. as the name suggests, big data refers to a great volume of data, but this is not enough to describe the meaning of the concept. pc mag (famous magazine in light of most recent innovation news), \"enormous data alludes to the huge measures of information that gathers after some time that are hard to break down and handle utilizing basic database administration instruments\". john weathington has characterized big data as an aggressive key parameter in various measurements, for example, clients, suppliers, new participants and substitutes. as indicated by him, enormous information makes items which are important and extraordinary, and prelude different items from fulfilling the same need. he additionally portrayed, \"enormous data is customarily described as a hurrying waterway: a lot of information streaming at a fast pace\" [cit] . meanwhile, the traditional approach to process the data is very slow. process the sensor data with mapreduce in hadoop framework which removes the scalability bottleneck. hadoop is an open framework used for big data analytics. hadoop's main processing engine is mapreduce, which is currently one of the most popular big data processing frameworks available. mapreduce is a framework for executing highly parallelizable and distributable algorithms across huge datasets using a large number of commodity computers. using mapreduce with hadoop, the weather temperature can be analyzed without scalability issues. the speed of processing data can increase rapidly when across multi-cluster distributed network [cit] . thus, there is a need for a flexible platform for the maintenance of this big data and help weather forecasting using that big data. thus, apache open source hadoop is solution for it, that provides high speed clustered processing for the analysis of a large set of data smoothly and efficiently [cit] . in this paper, we proposed big data prediction framework for weather temperature based on mapreduce algorithm to producing results without scalability bottleneck."
"variety: refers to data coming from heterogeneous resources. data can either be in dbms table format as structured form or in a semi-structured form or in the unstructured form such as email attachments, images, audios, videos, or medical records such as mris, ecgs, x-rays etc [cit] ."
"a criminal investigation is an abductive diagnosis problem [cit] . however, it is difficult to design a model that can deterministically describe all the assumptions involved in an investigation. poole [cit] has attempted to address this issue by proposing a model that describes crime scenarios non-deterministically using symbolic logic and probabilistic bayesian methods. unfortunately, poole's model is too abstract to be applied in real scenarios."
whereẑ b is the prediction of the bth bootstrap model for the soil property p. the model error variance term can be estimated using the prediction 155 from b bootstrap models by:
"soil fraction clay, silt and sand are reported in percent, which must sum to 100. to handle this compositional constrain, the prediction from the output layer of the range (0, 1) and the vector adds up to one. the output values of clay, silt and sand are then multiplied by 100. the softmax function is very similar to the additive log-ratio transform generally applied to compositional variables in soil mapping studies."
"the method described in this study for uncertainty quantification assumes independent and normally distributed residuals. the latter assumption was tested by visual inspection of the residuals and computation of a quantile-quantile (q-530 q) plot (not shown). it was found that the gaussian assumption of the residuals was satisfied. in cases where the gaussian assumption is too restrictive, one could use a transformation of the data prior to modelling (e.g. logarithm, square-root or box-cox transform). remaining spatial autocorrelation in the residuals was further tested to ensure that the assumption of independence was 535 satisfied. spatial autocorrelation in the residuals indicates that the prediction might be biased, and that the quantified uncertainty is possibly underestimated. the moran's i (mi) [cit] ) was used to test for spatial autocorrelation of the residuals of the maps presented in fig. 5 and 6 . the values of the mi range from -1 to 1, with 0 indicating no autocorrelation. ten nearest 540 neighbours were used to compute the mi on the original values and residuals of the soil properties. results indicate significant spatial autocorrelation in the original soil properties, with a mi value larger than 0.27 for all soil properties (e.g. mi value for ph is 0.41), while being smaller than 0.1 on the residuals (mi value for residuals of ph is 0.05). thus, the cnn model was efficient to 545 eliminate spatial autocorrelation. in case there would remain autocorrelation in the residuals, predictions and prediction uncertainty quantification would be improved by kriging the residuals, and by summing the kriged mean and variance maps to the predicted mean and variance maps made by the cnn model. [cit] ."
"cnn predictions are compared to those made by rf (table 3) as a baseline. cnn accuracy measures are on average equivalent or better than those of rf. clay, silt, sand and ph are better predicted by rf than cnn, as shown by the r 2 and rmse. cnn does better for predicting oc and n with a significantly larger r 2 for oc (r 2 for cnn is 0.15 and r 2 for rf is 0.12). this is confirmed 325 by the ccc values which are greater for oc and n predicted by cnn. while clay, silt, sand and ph predicted by rf have a larger correlation coefficient with the measured values than those predicted by cnn, the ccc shows that cnn predictions are either better or equal to those of rf. since the ccc assesses the deviation of the predictions with respect to the 1:1 line it is a useful measure 330 to serve model comparison [cit] . the me values show that prediction are relatively unbiased, with the exception oc predictions which are either slightly positively of moderately negatively biased for rf and cnn, respectively. this bias is certainly due to the measured oc values greater than 60 g kg −1 that are systematically underestimated. this is visible in fig. 4 ."
"the proposed mapreduce application is shown in fig.4 . the mapper function will find the average temperature and associated with the key as place. the all values are combined ie reduce to form the final result. the average temperature, maximum temperature, minimum temperature etc can be calculated in the reduce phase."
"finally, there is the possibility that, even though the seized computer was the initial seeder, the evidence may not be able to confirm a yes or no state for h 1 . therefore, there is a chance that the seized computer was the initial seeder, but the source from where the pirated movie was copied is uncertain. following the assignment of conditional probabilities to the five subhypotheses, we proceed to develop the entailing casual events or evidence for the sub-hypotheses. this is because a bayesian network propagates probabilities for linked hypotheses based on the states of events or evidence."
"before we discuss bayesian networks, it is important to emphasize that digital evidence deals with \"past\" events that were caused by some other hypothetical events that have to be verified. for example, if a suspect had child pornography on his computer, he may have downloaded it from a pornographic web site, which could be verified by the presence of the url in the history file of his browser."
"the defendant in the case was alleged to have used his computer to distribute a pirated movie on the internet using bittorrent [cit] . the defendant had the optical disk of the movie in his possession. he copied the movie from the optical disk to his computer and then used bittorrent to create a \"torrent file\" from the movie file. the torrent file contained metadata of the source file (movie file) and the url of the bittorrent tracker server."
a real case involving the distribution of a pirated movie via the bittorrent peer-to-peer network is used to demonstrate the utility of the bayesian network model. the digital evidence discussed in this paper was presented in court during the criminal trial.
"for the vector z p at n − n test locations where n is the number of calibration location and n is the total number of sampling locations, the quality of the 180 prediction is quantified by the mean prediction error (me), root mean squared error (rmse), amount of variance explained by the model (r 2 ) and concordance correlation coefficient (ccc). the latter is derived as follows [cit] :"
"since the likelihood ratio is proportional to the posterior probability, a larger posterior probability denotes a higher likelihood ratio. in the evidentiary context, it also means that the greater the evidence supporting the hypothesis, the more likely that the hypothesis is true."
the apache hadoop project consists of the hdfs and hadoop map reduce in addition to other modules. the software is modeled to harvest upon the processing power of clustered computing while managing failures at node level [cit] .
"the optimization process runs for a number of epochs. an epoch describes the number of times the network sees the entire input dataset. during each epoch, the entire dataset is shown to the network in small subsets shuffled at random, called batches."
"hypothesis h and the five sub-hypotheses have a diverging connection. the nodes in a diverging connection influence each other when the state of their parent node is still unknown. therefore, the five subhypotheses are related to each other in a probabilistic manner. also, their probabilities are affected by all the child events or evidence under them."
"a bayesian network has three elementary connections between its nodes that represent three different types of probability distributions ( figure 1 ). for a serial connection, if b's evidential state is unknown, then a and c are dependent on each other. in other words, there is an evidential influence between a and c if the evidential state of b is unknown. however, if b's state is known, then a and c are independent of each other; this means that a and c are conditionally independent of each other given b. in a diverging connection, the same conditional independence is observed for a and c, i.e., if b's state is known, then a and c are independent. in a converging connection, if b's state is unknown, then a and c are independent. in other words, unless the state of b is known, a and c can influence each other."
"deviation from the 1:1 line is due to overestimation or underestimation of the modelled uncertainty, depending whether the points lie above or below the line, respectively."
"it is important to observe that digital events are discrete computer events that are deterministic in nature and have a temporal causal sequence. therefore, it is common practice for digital forensic analysts to establish their abductive reasoning based on the existence or validity of the causal events that entail their hypotheses. however, it is difficult to have consistent models that determine the supporting events for hypotheses. different analysts may attach different events to the same hypothesis. even if they agree on the same set of events, they usually assign different (subjective) probabilities to the events."
"without reliable and scientific models, the conclusions made by digital forensic analysts can be challenged on the grounds that they are mere speculation. the problem is acerbated by the fact that forensic conclusions derived from the same digital evidence can vary from analyst to analyst. this can severely impact the reliability of digital forensic findings as well as the credibility of analysts. speculation and subjective views offered by forensic analysts under the guise of expert opinion have little (if any) value in legal proceedings [cit] . this paper presents a formal model for reasoning about digital evidence. the model, which is based on probability distributions of hypotheses in a bayesian network, quantifies the evidential strengths of the hypotheses and, thereby, enhances the reliability and traceability of the analytical results produced by digital forensic investigations. the validity of the model is investigated using a real court case involving the illegal dissemination of a movie using the bittorrent peer-to-peer network."
"according to the multiplication law of probability, which expresses commutativity, if h is relevant for e, then e must also be relevant for h. the corresponding joint probability expression is:"
"the methodology is tested on the metropolitan territory of france which is about 543,965 km 2 excluding corsica and other islands. france has a very diverse landscape and climate. the altitude ranges from 0 to more than 4,500 m in the alps. the climate is mediterranean in the south and temperate in the north, influenced by a west-east gradient of decreasing precipitation and tem-220 perature due to the increasing distance from the atlantic ocean. france is mostly covered by soils from calcareous rocks such as the rendzic, calcaric leptosol and calcisols on the champagne region or in argonne. soils from clayey sediments such a haplic and vertisols are found in patches north of massif central and in the southern alps mountains with fertile loess soils (luvisols) in the north."
"several methods exist for simultaneous prediction of soil properties, such as co-25 kriging [cit], regression co-kriging [cit] and structural equations modelling (sem) [cit] . these linear methods model the interrelation between properties explicitly but are computationally demanding when the size of the observation dataset is large or the number of properties to predict increases. in addition, they rely heavily on 30 rigid statistical assumptions about the distribution of the soil properties. as an alternative, non-linear techniques such as machine learning have garnered wide interest during the past decade. [cit] have adapted support vector machine for predicting many dependent variables 35 simultaneously. despite those examples, the use of machine learning techniques for multivariate soil mapping have been largely unexplored."
"a bayesian network is a useful formalism for quantifying and propagating the strengths of investigative hypotheses and supporting evidence. the internet piracy trial provides an excellent case study for validating the approach. the hypotheses in the case and their supporting events and evidence are clearly specified, along with their causal relationships and probability values. thus, the bayesian network model is not only an analytical tool for evaluating evidence, but also a tracking tool that enables digital forensic practitioners to review and analyze the original findings."
"volume: refers to large amount of data available in an organization. at present if the data is present in terabytes and it is supposed to increase to exabyte in near future. therefore, it is difficult to handle by traditional data processing applications [cit] ."
"forensics is the process of analyzing and interpreting evidence to determine the likelihood that a crime occurred. many researchers (see, e.g., [cit] ) argue that that this process should cover the formulation of hypotheses from evidence and the evaluation of the likelihood of the hypotheses for the purpose of legal proceedings."
"a bayesian network operates on conditional probability. for example, if the occurrence of some evidence e is dependent on a hypothesis h, the probability that both h and e occurred, p (h, e), is given by:"
"the construction of a bayesian network model begins with the main hypothesis that the analyst intends to determine. in order to prove the illegal act in the bittorrent case, we use the following hypothesis:"
"analysts also must reason about hypotheses in the face of missing and/or uncertain information about events. the events for which evidence is available may not prove the complete truth of the hypotheses; however, they can be used very effectively to compute degrees of likelihood for the hypotheses. consequently, probabilistic approaches are well suited to developing formal models for reasoning about digital evidence in criminal investigations."
"to evaluate the system performance we use the hpc2n workload trace from parallel workloads archive [cit], containing information regarding applications submitted to a linux cluster from sweden. the cluster has 120 nodes with two 2 [cit] + processors each. we assigned to each node 2 gb of memory. as the memory information was not specified for all the applications missing memory requirements were completed by assigning a random amount of memory, between 10% and 50% of the node's memory capacity. we ran each experiment by considering the first 1000 jobs, which were submitted over a time period of 18 days. we scale the inter-arrival time with a factor between 0.1 and 1 and we obtain 10 traces with different contention levels. a factor of 0.1 gives a highly contended system while a factor of 1 gives a lightly loaded system."
"t grace -the time interval in which the application can be started or restarted without any risk of not meeting the deadline; when starting the application t grace is computed as t deadline − t ideal, as seen in figure 5 (a). after the application started executing, t grace is computed as t deadline − t re f ideal · (n steps − n current ) as noted in figure 5 (b)."
". to show how an application controller can adapt to changes in application resource allocation using the vertical scaling policy due to price variation, we ran a micro-benchmark using an mpi application, zephyr [cit] . zephyr is a fluid dynamics simulation which runs for a tenant-defined number of iterations. each iteration performs some computation, and if the application is started with multiple processes, also data is exchanged among them. zephyr receives as input a configuration file and it simulates a volume filled by fluid for a specified simulated time, zephyr periodically writes in a log file the following information: the cpu time for each iteration and the current number of iterations. for an slo-driven zephyr application, the application controller reads periodically the cpu time for each iteration and it compares it with a reference cpu time computed such that the application finishes its iterations at the tenant's given deadline."
"(i) the first style varies edge thickness to convey the depth information. a larger thickness is used in regions with a low depth value since they are closer to the camera, this guides the users attention to close-by objects."
"we consider that all applications have a deadline and a re-chargeable budget. as we couldn't find any information regarding application deadlines, we assigned synthetic deadlines to applications, between 1.5 and 10 times the application execution time. we assume that the budget amount the tenant wants to pay depends on the application's deadline: a tenant with a less urgent application wants to pay less. [cit] credits per time period."
"we deployed both merkat and maui on a cloud of 10 compute nodes on the edel cluster of the grenoble site from grid'5000. when using maui, we deploy the vms and add them to maui as physical nodes at the beginning of the experiment. each node has 2 intel xeon e5420 qc processors (4 cores) and 24gb of memory. the vm scheduler of merkat assigns for each node an available capacity of 700 cpu units and 23 gb ram (one core is reserved for the hypervisor/node's operating system) and it distributes it among the vms running on the node. all nodes are connected through a gigabit ethernet and an infiniband link. we use the gigabit ethernet link for vm communication and the infiniband link for vm migration. the vm images are stored on a nfs server. to speed-up the vm deployment we use copy-on-write vm images. merkat's vm scheduler uses a scheduling period of 60 seconds and the application controllers read the application's performance metrics at every 80 seconds. we wanted a scheduling period that is small enough to allow applications to adapt fast their bids, and large enough to allow the vm migrations to finish."
"to illustrate the use of a horizontal policy, we consider a tenant that wants to execute bag-of-tasks applications on the infrastructure. the slo for this application type is to minimize the workload processing time."
meryn allocates them per framework; (ii) and it focuses on managing contention on a private infrastructure by implementing a virtual economy while meryn uses cloud bursting to offload its workload in public clouds while optimizing the paas provider's profit.
"for clarity, we first run the zephyr application with a best-effort controller instead of a slo-driven one. figure 6 shows the progress the application makes over time, i.e., its iteration execution time: (i) when it runs alone on the node (the ideal iteration time); (ii) and after the other applications are submitted (the best-effort iteration time). the performance difference in this case is highly noticeable: after all applications started executing, the iteration execution time increased almost 10 times. this degradation is not only due to its reduced resource allocation but also due to the other applications."
"auctions establish the resource price based on how much tenants are willing to pay for resources. auctions clear the market faster than the price computation algorithms from the commodity markets, allowing tenants with the most urgent demand to get their resources with minimum delays. multiple attempts were made to use auctions to schedule static mpi applications, on clusters [cit] or to run bag-of-tasks applications on grids [cit] . in this case, tenants bid for an application execution and the scheduler decides which application gets to run through the auction. nevertheless, these proposed systems address the case of applications with known execution times and static resource requirements."
"torque cpu alloca7on figure 12 : the variation of cpu allocation for both frameworks, as a percentage of the total available cpu capacity of the cloud. we omitted the variation of memory allocation as it is similar."
". let us illustrate the use of such a policy in a scenario in which a cluster needs to be shared between two commonly used frameworks: condor [cit] and torque [cit] . tenants submit applications to each framework. these frameworks are usually used by scientific organizations to manage their hpc clusters. each framework has its own scheduler which puts applications in a queue and runs them on nodes when resources become available for them. we implemented a scaling policy for each framework that uses algorithm 4 to provision vms. for torque applications, the policy minimizes the wait time in queue, while for the condor applications the policy maximizes the throughput. in both scaling policies, the framework copes with fluctuations in price in two ways. first, it avoids requesting a large number of vms per time period, as provisioning them is wasteful if the price increases in the next period. second it avoids starting new vms if some vms were released due to a price increase in the previous period. we deployed the condor framework to process parameter sweep applications and the torque framework to process mpi applications; both application types are commonly used at edf. then, we have studied how merkat adapts the resource demand of each framework based on its workload."
"opposed to these systems, merkat uses a market to distribute resources, leading to a more fine-grain differentiation between application priorities. moreover, merkat is designed to be generic enough to support more than one slo and application type. nevertheless, merkat can leverage in its application controllers many of the algorithms proposed in such papers, regarding application performance profiling and avoidance of performance interference between different application types."
"unless otherwise specified, all vms have a maximum capacity of 100 cpu units and 900mb of ram. each vm is configured with debian squeeze 6.0.1 os and kvm [cit] as a hypervisor on each node."
"to manage the tenants and the vms, merkat interacts with an iaas cloud manager [cit] . the iaas cloud manager provides interfaces to start, delete and migrate vms, manage their storage, network, and moreover, to keep information about the infrastructure's tenants."
"the resource allocation computation is performed in two steps: (i) to ensure maximum utilization, the vm scheduler computes the vm allocation considering the entire infrastructure as a single physical node; (ii) to cope with the resource fragmentation, i.e., the infrastructure capacity is divided among nodes, the vm scheduler corrects the allocations to fit to the node capacity."
"in our case, we simulate the datacenter, the vm scheduler and multiple applications, created dynamically during the simulation. applications are created according to their submission times, taken from a workload trace, and are destroyed when they finish their execution. in our simulator there is no distinction between an application and its virtual platform. to model the applications, we consider the batch application presented in section 3.1. we simulate these applications as sets of tasks, with each task requiring one cpu core and a specified amount of memory. the application applies the vertical scaling policy and interacts with the datacenter to change the bids for its vms. as cloudsim does not model the cost of vm operations, we have also implemented a model for several vm-related performance overheads: memory over-commit, vm boot/resume and vm migration [cit] ."
"we propose a morphological filter that works in the same flow-based filtering framework. the thickness of the lines can be controlled by the maximum distance to the location of the maximum response perpendicular to the flow, represented by green dots. using a distance of zero yields only the maximum itself, depicted as blue dashed line. in theory, more than just one local maxima is possible in gradient direction. we simply use the closest maxima to determine the distance. the depth information is used to determine two parameters, line thickness and opacity, to derive two line drawing styles described in the next section. results of this filter with both rendering styles in comparison to the unchanged filter version are shown in figure 6 ."
"this resource allocation must fulfill the following requirements. first, the resource allocation has to respect the infrastructure's capacity limitations. having enough capacity to meet all tenant requests in the highest demand periods is rarely the case, as expanding the resource pool is expensive. to differentiate between tenant requests in these periods, most of the state of the art systems [cit] rely on priority classes, i.e., tenants are given a priority class for their applications, or quotas, i.e., tenants have limited resource amounts to provision. however, in this case the tenants might abuse their rights and run less urgent applications, taking resources from tenants who really need them."
"there is a large number of iterative applications (e.g., zephyr [cit] ) that follow this model. these applications have a relatively stable execution time per iteration. the execution time of an iteration can be tuned by modifying the resource allocation that each task receives. for example, if each task receives one full core, one iteration can take 1 second. nevertheless, if the resource allocation drops at half, the same iteration can take 2 seconds."
"where ω(x) denotes the neighbourhood of a 2d position x with radius r and k is the vector normalization term. the magnitude weight function wm gives higher weight to gradients with a large magnitude, preserving strong edges. the directional weight function w d gives higher weight to gradients with similar directions, creating a more uniform alignment of the vectors and avoiding swirls in the vector field. for more details about these two weight functions please consider the paper of the original authors."
"the n iter threshold is used to ensure the algorithm finishes: if there is no improvement in the last n iter iterations, the algorithm stops."
"the emergence of new workload types, like data analytics, raised multiple issues regarding how resources of a cluster should be managed. currently, clusters need to be shared between multiple tenants, each requiring different frameworks, and having various application performance constraints, e.g., high availability, throughput, latency. supporting this variety of requirements can lead to a reduced resource utilization of the cluster and inefficient energy consumption. in this context, there are substantial efforts in the state of the art towards designing resource management systems to cope with these issues."
"finally, in both our policies, the controllers use vms with a predefined maximum size. this size can be set up by the tenant, if for example she knows that her application will never use more than that, or it can be configured by the cloud infrastructure, e.g., as the capacity of the node. nevertheless, the vm resource allocation changes dynamically, as the vertical scaling policy can steer it during the application runtime."
"i/o resources. the current resource allocation algorithms could be extended to consider network and storage resources. such resources can become a bottleneck when network or data intensive applications are executed. thus, it is normal to make them available at a price that reflects the total resource demand. regarding the network resource, software tools can be used to limit the bandwidth available to each virtual machine and ensure a proportional share."
"in merkat, the implementation of the proportional-share policy follows four steps: (i) vm bid submission; (ii) vm resource allocation; (iii) vm load balancing; (iv) price computation."
"the value with which the bid changes is given by t (line 2), and it represents the \"gap\" between the current performance value and the performance reference value. a large gap allows the application to reach its reference performance fast. to avoid too many bid oscillations the algorithm uses the value of the past bid change, i.e., last bid change in its bid computation process. for example, if the bid was previously increased and at the current time period the bid needs to be decreased with a similar value, the bid oscillates indefinitely. thus, the algorithm decreases the current bid with half of its value (lines 10, 18)."
"then, tenants might have different slos for their applications. many state-of-the art systems allocate resources to meet tenant slos by using offline and online profiling and specialized policies designed only for some application types, mostly batch data analytics [cit] or web applications [cit] . these systems focus on specific slos and provide poor tenant differentiation policies, by separating the workloads in classes, e.g., silver, gold [cit] or best-effort and latency-sensitive [cit] ."
"especially, in the case of hpc applications, care must be taken in placing communicating application processes as close as possible to each other. currently, merkat does not consider the application's or the cluster's topology."
"when vm requests are received the vm scheduler places them initially on the nodes with the lowest resource utilization. to minimize the vm allocation error, the vm scheduler might migrate vms between nodes. the process of migrating vms among nodes is called load balancing. as having a high number of migrations leads to a performance degradation for the applications running in the vms, the load balancing process tries to make a trade-off between the number of performed migrations and the vm allocation error. for example, it won't make sense to migrate a vm when its allocation error is 1%. to select the vms to be migrated at each scheduling period, the vm scheduler relies on an algorithm based on a tabu-search heuristic [cit] . tabu-search is a local-search method for finding the good solutions of a problem by starting from a potential solution and applying incremental changes to it."
"given that the system is not highly dynamic, and the application controller has time to adapt, the application can run with a smaller resource allocation and optimize its budget. the application controller optimizes the application execution cost while allowing other applications with less budget to use resources. however, our previously-presented policies do not cover all the possible cases. for example, the tenant's deadline might be too strict for the current price."
"we submitted 61 zephyr applications to torque with execution parameters taken from a trace generated using a lublin model [cit] : the number of processors was generated between 1 and 8 and the execution time had an average of 2479 seconds with a standard deviation of 1243.5. we submitted 8 parameter sweep applications composed of 1000 jobs of one processor each to condor with an inter-arrival time of one hour. as we did not have access to a real parameter sweep application, we used the stress benchmark, which ran a cpu intensive worker for different execution time intervals generated with a gaussian distribution. the average task execution time was 478 seconds, with a standard deviation of 363. both frameworks were deployed on 32 nodes managed by merkat and receive an equal budget. condor cpu alloca7on"
other resource management systems focus on minimizing the energy consumption of the infrastructure [cit] . the common approach is to consolidate applications on as few nodes as possible and shut down the idle resources. in this paper we focus on maximizing the resource utilization of the infrastructure while making users to take educated decisions regarding their used resources (through the use of currency).
"finally, a user study was conducted showing that our method improves depth perception of the line drawings, that our line renderings are preferred over others to depict depth information, and that our method can be used to focus the users attention within the image."
"then we ran the application with three different deadlines: 12000 seconds, 9000 seconds and 6000 seconds. we repeated each experiment three times and computed the average of the obtained values. figure 7 shows the bid of the slo-driven controller for cpu resource for the different application deadlines."
"scalability. as the current scale on which merkat was tested is quite limited, also due to limitations in the underlying third party software stack, we could not identify the upper scalability limit of merkat. we believe that this upper limit is determined by two factors: (i) the performance of the vm scheduler in allocating resources when it has to cope with adaptation requests from a large number of applications; (ii) and the performance of the iaas manager. while the performance of the iaas manager is outside the scope of this work, the performance of the vm scheduler can be further improved by optimizing the vm allocation and placement algorithm."
"we evaluated merkat in simulation and on the grid'5000 testbed. the obtained results show that: (i) merkat is flexible enough to support different applications and slos; (ii) merkat can adapt the application resource demand to the infrastructure load and application's slo, maximizing resource utilization while leading to a better tenant satisfaction; (iii) the resource control decentralization has a reasonable performance impact compared to centralized resource management systems."
"(ii) the second style varies edge thickness and also the opacity of lines. while the first style highlights close regions with thicker edges, a problem arises if object details are discarded due to the thick edges. in contrast to this, the second style uses thin, opaque lines for low depth values and thick, faded lines located at a further distance. this is analogue to most painterly rendering techniques that process the rendering from back to front, with larger brushes in the back and smaller brushes in the front [cit] ]."
"terminology. before explaining how an application controller uses the previously defined policy we give some definitions of the terms used. when submitting her application, the tenant needs to provide the ideal execution time of the application, t ideal, the ideal execution time per iteration step, t re f ideal and the total number of iterations the application needs to perform, n steps ."
"we used simulation to test merkat's algorithms with a large workload. running a large workload gives a better insight in the total satisfaction that the system can provide. due to time and resource limitations, running a large workload on merkat in a real environment, which lasts for days, or possibly weeks, and with a large number of nodes, would have been unpractical. we discuss next the implementation of merkat. then we describe the results we obtained from measuring the total tenant satisfaction provided by merkat in simulation and on a real-world testbed."
"merkat's controller is capable to take advantage of the under-utilization periods of the infrastructure and provision up to as many vms as allowed by the infrastructure's capacity for the condor framework. in the second and fourth phase, both frameworks need more resources than their fair share to process their workload, thus each of them provisions an equal number of vms. for clarity, figure 12 shows the total cpu allocation for each framework, as a percentage from the total available cloud capacity. we have omitted the memory allocation, as it exhibits a similar trend. in contention periods, each framework receives half of the available cpu while in under-utilization periods, the condor framework scales its cpu allocation according to its workload demand."
"high availability. to provide tenants with a production system, merkat services require self-healing capabilities to make the platform highly available. state-of-the art solutions [cit] can be used to achieve these goals."
"the main problem with this filtering framework is that it does not consider different scales of features that are described within the edge tangent field. when choosing a large radius, small but visually important details may disappear. on the contrary, if the chosen filter size is too small, noise may still be present in larger structures. thus, we propose an adaptive filter size, which is determined by the local average angular deviation. before applying eq. 1, the user-defined radius r is used to determine a locally-scaled version r (x) along the flow field ω centred at x:"
"merkat has been designed to manage the clusters owned by an organization that needs to run hpc workloads but cannot use public cloud resources due to security constraints, e.g., the data that needs to be processed is too sensitive. this is often the case of organizations carrying out research activities in computational sciences. an example of such organization is electricite de france (edf), which relies on hpc simulations to optimize the day-today electricity production or to choose the safest and most effective configurations for nuclear refueling. the clusters are shared among a multitude of tenants (e.g., scientific researchers) which might come from different departments (e.g., production, development) and might need to use specific frameworks or libraries to run their applications. these tenants not only have different slos for their applications, e.g., getting computation results until a specific deadline, or executing the application as fast as possible, but they might also want to assign different importance degrees to their requests. merkat meets the tenant demands, while also allowing the organization to get the best out of its infrastructure, by maximizing its resource utilization. figure 1 gives an overview of merkat. merkat relies on: (i) a virtual machine market to allocate resources and internal currency to manage the priority of tenants' demands and (ii) a set of virtual platforms that manage and elastically scale applications to meet tenant slos."
"this scheduler receives application submission requests, keeps them in a queue and dispatches them to nodes when resources become available. at any time, the tenant can retrieve information about the number of running and queued tasks."
"in this paper we present the design and evaluation of merkat, a hpc cloud platform. the novelty of our proposed system comes from the decentralized resource allocation model, in which applications provision resources autonomously to manage their tenants' slos by controlling the amount they pay for resources; the resources are allocated to them through a proportional-share market that provides fine-grained resource allocation and a dynamic pricing model which varies with the total infrastructure demand. the contributions of merkat are the following:"
"when a tenant wants to run an application, she assigns it a replenishable amount of credits, called application budget, and sends a request to merkat to start a virtual platform for it. this application budget reflects the maximum cost the tenant is willing to support, or the true priority, for running an application. these budgets are replenished automatically at a system-wide interval defined by the administrator. the replenishment is defined as transferring a tenant-defined amount from her account to the application's budget. merkat ensures that the amount with which the budget is replenished never leads to a total budget that exceeds the initial budget amount. replenishable budgets are used to minimize the risk of depleting the application's budget in the middle of the application's execution."
"the vertical scaling policy controls the amount of cpu and memory resources allocated to a vm as well the amount that is payed for them. this policy computes periodically the resource bids for each vm based on the following information: (i) current, minimum (alloc min ) and maximum (alloc max ) vm resource allocation; (ii) current"
"these systems do not provide support for different slos, as, more specifically, they do not monitor and adapt the application resource demand on the market. in contrast to these systems, merkat controllers can adapt applications in two different ways, vertical and horizontal, to meet tenant slos."
"the virtual platform template contains information regarding the application controller, adaptation policy parameters, including desired tenant slo, and the configuration of the vms, e.g., vm disk image. we define the slo as the performance objective the tenant wants for her application, e.g., a specific execution time or throughput. to start the virtual platform, the virtual platform manager checks any initial deployment conditions that a tenant has specified."
"as t grace is used now for resuming the application, we are interested in knowing in how much of the remaining time to deadline the remaining computation can be finished."
"to be able to run applications on the infrastructure, tenants are assigned budgets by an administrator in the form of a virtual currency. merkat disposes of a total amount of currency, which is distributed among uses based on defined weights. we call the currency unit a credit. a tenant will receive a budget of credits proportional to its weight in the system. it is the task of the administrator to add/remove tenants to/from the system and set up and adjust the total amount of currency and the tenants' weights. virtual currency is desirable when managing a private infrastructure; because no external currency is introduced, price inflation is bounded."
"the policies are run periodically and use two performance thresholds, upper and lower, as a trigger: when the application performance metric crosses the thresholds, the policy takes an action that changes the virtual platform resource demand."
"the maximum limit at which the bid can be increased is given by bid max, by distributing all the application's budget over the remaining time to deadline. we apply this distribution to avoid cases in which the application will run out of credits in the middle of its execution. as we assumed that the application receives a budget renewed with r credits every t renew, the total budget the application receives until its deadline is: (b + r · (t deadline /t renew )."
"the value of the total credit amount influences the performance of the system. if the total credit amount is too large, the resource prices become too inflated and lead to poor tenant satisfaction. if the credit amount is small and new tenants arrive, current tenants might be funded at a rate too small to allow them to run highly urgent applications."
"as part of this adaptation policy, the application controller checks 3 conditions: (i) if it needs to stop the application execution; (ii) if it needs to suspend the application; (iii) if it can restart the application execution when the application is suspended. these conditions are checked as workload variation on the infrastructure leads to high prices. in this situations the application cannot meet its deadline and it is useless to spend more budget for its execution. the first condition becomes true when the application misses its deadline. the second condition happens due to high price periods. in a high price period the application controller cannot afford to keep the reference execution time per step, t re f, and thus it suspends the virtual platform. finally, the third condition is true when the price drops at an affordable value. if the application cannot resume in t grace time then the controller stops its execution."
"we started an slo-driven application and four best-effort applications (zephyr applications running with a controller that does not change the bid during the application runtime) on a node, each in a vm with 4 cores. the slo-driven application has a budget of 60000 credits, from which it can spend as much as it wants, while the other best-effort applications start with a bid of 100 cpu credits and 900 memory credits which remains unchanged during their execution. for this setup we used a node from our cloud. the slo-driven application controller was started at the beginning of the experiment, while the other four application controllers were started during its execution. the last best-effort application was submitted after 20 minutes from the experiment start. the slo-driven application has an ideal execution time of 77.5 minutes."
"a feedback controller tunes the cpu allocation of the vm such that the application running inside makes enough progress in its computation to meet its deadline. in this way, more applications can run on the infrastructure. a global admission control is used to avoid overloading the system."
"many paas systems, commercial [cit], research [cit], and open source [cit] provide runtime support for applications hiding from tenants the complexities of managing resources. these systems, however, provide typically closed environments, forcing tenants to run only specific application types (e.g., web, mapreduce). if new programming frameworks appear, the paas provider needs to first develop the necessary support on the infrastructure, and then offer to the tenants the possibility to use it. in contrast to these systems, merkat allows tenants to run new application types while distributing the resources among them based on their value."
we demonstrate the case of virtual platforms in merkat by proposing and applying two policies to adapt the resource demand of an application: vertical and horizontal scaling. these policies use the dynamic resource price as a feedback signal regarding the resource contention and respond by adapting the application resource demand given the tenant's slos.
"in merkat setting this amount can be done by the infrastructure administrator based on the number of tenants in the system, the price history and the capacity of the infrastructure. if the administrator observes that the price increased considerably in the last period, she can reduce the total credit amount. if new tenants arrive, she can increase this amount. designing an automatic system to manage this amount remains an open issue. such a system needs to adapt the amount based on past price fluctuations, infrastructure size and feedback from the tenants regarding their resource usage versus assigned budget."
"the remaining of this paper is organized as follows. section 2 describes our approach. it introduces the principles behind it and the resource management process. section 3 describes the vertical and horizontal policies implemented in merkat. we illustrate the use of the policies with examples validated on the grid'5000 testbed. section 4 presents evaluation results, with simulation, showing how merkat's algorithms behave on large-scale system, and how the merkat prototype behaves on a small real testbed. section 5 discusses limitations and future directions of improvement while section 6 describes the related work. finally, section 7 concludes the paper."
"in this case, the application controller might need to estimate the future infrastructure load and send feedback to the tenant regarding the minimum deadline it can meet. starting from the simple mechanisms presented here, to improve the slo support more complex policies can be developed, based on application profiling and price prediction."
"the performance gap between our mechanism and edf can be explained as follows. when the contention is low, our system provides higher satisfaction than edf, due to its fine-grained allocation policies. when the contention is high, more applications arrive at the same time and, edf is capable to take better scheduling decisions: thus more applications with smaller deadlines, get to run on time. because the deadline urgency is reflected in the application's value, edf also leads to higher tenant satisfaction. in the case of our system, applications do not take the best allocation decision, as they adapt independently with only limited information. this decentralization leads to a loss in performance, compared to edf. the performance degradation is the \"price\" payed by the nature of our system, which allows applications to behave selfishly. we also measured the total tenant satisfaction when tenants have different models for the satisfaction they get from their application execution. figure 14 describes the proportional-share market performance for each of the different previously discussed tenant models, in terms of total tenant satisfaction and, for clarity, number of successfully finished applications (i.e., their deadlines were met). figure 14 (b) describes the total satisfaction that our system provides to tenants when applications use the each of the three different policies. the best satisfaction is provided by the partial deadline policy, as tenants still gain a positive value from getting the results at their deadline, despite of not having all of them. the worse satisfaction is provided by the full performance policy, as tenants are also more demanding: they get dissatisfied really fast and they perceive a negative value if their jobs don't finish at their deadline. as the application adaptation algorithms lead to vm operations, we measure their cost. we keep the same settings from the previous experiments and we record the number of vm operations during the experiment run. figure 15 describes the average number of vm operations per hour performed by the vm scheduler for different tenant utility functions. we notice that the full-deadline policy is the most effective as it has the least number of vm operations. this is due to the fact that with this policy the applications are less aggressive in acquiring resources. we notice that in the most highly loaded periods the number of migrations decreases. this is explained by the fact that when there is a high load, many applications don't resume and even don't start their execution as their deadline cannot be met. the partial-deadline policy has the most migrations, number which increases with the contention, as even in highly contended periods applications continue running with smaller resource amounts. moreover, applications start executing whenever they can get a small resource amount. this leads to more bid adaptations, and thus resource allocation changes and more migrations. the number of suspend/resume operations follows the same trend. the reason behind this is that applications resume at a small allocation, as the policy considers that any progress the application makes is useful. the full-performance policy follows a similar trend as the partial-deadline policy but the number of migrations becomes higher even when there is not much contention. this is explained by the aggressiveness of the application adaptation policy that always increases the bid to get as much resource as possible."
"in addition to varying the thickness and the contrast of the lines depending on the depth, we also adjust the level of details of the lines themselves. we increase noise suppression and smoothing in the dog calculation for regions further away and vice versa. this results in a detailed foreground, but only larger structures remaining in the background."
"a first class of resource management systems focuses on the flexibility of sharing the cluster between applications requiring different frameworks, with the goal of maximizing the cluster utilization. mesos [cit] allocates resources to frameworks based on the concept of \"resource offer\", i.e., a list of available resources on nodes. mesos gives resource offers to frameworks while frameworks can filter the offers and decide what resources to accept. omega [cit] presents frameworks with a \"replicated\" shared view of the cluster, called a \"cell state\". each framework keeps its own cell state, which is synchronized periodically to reflect allocation changes, and selects from it what resources to use. in"
"using additional depth information, we propose two artistic styles aiming at creating a better depth perception of line drawings. in the conducted user study we showed their usefulness when determining the spatial arrangement of scenes."
"we think that one cause for this variation is the sharing of physical cores between more vm processes. the slodriven application receives more cpu than the best-effort applications, and thus less of the vms in which these best-effort applications run get scheduled on the same physical cores as its own."
"our results show that: (i) merkat is flexible, allowing the co-habitation of different applications and policies on the infrastructure; (ii) merkat increases the infrastructure resource utilization, through vertical and horizontal scaling of applications; (ii) merkat has low performance degradation compared to a centralized system that supports a fixed slo type; this degradation is caused by the decentralized nature and the application selfish behavior."
"to see how the performance is influenced by the contention level, we measure the obtained satisfaction for each of the 10 obtained workload traces and for full deadline tenants. we repeated each experiment four times and the results is not high, despite reaching almost the same number of finished applications as edf, our system still outperforms it in terms of tenant satisfaction; (iii) however, when the system is highly loaded, its performance degradation increases."
we classify the related work in three categories: (i) resource management systems for clusters; (ii) platform as a service systems for clouds; (iii) and resource management systems that apply a market.
". merkat's application controller can also react to a tenant imposed condition. to show how it does so, we ran a slodriven zephyr application and changed the tenant-specified deadline during the application execution. we submitted a deadline-driven application and four best-effort applications to merkat. all the applications are started on the same node, each in a vm with 4 cores. the best-effort applications are started in the first few minutes after the start of the deadline-driven application. the slo-driven application has a budget of 60000 credits, from which it can spend as much as it wants, while the other best-effort applications use a fixed bid of 100 cpu credits and 900 memory credits per vm."
"tenant model. to measure the total tenant satisfaction, we model the tenant satisfaction as a utility function of the budget assigned by the tenant to its application and the application execution time. there are different functions which can be used to model this satisfaction, and they depend on the behavior of the tenant. in this paper we define several functions, derived from discussions with scientists at electricité de france:"
"we conducted a user study with 40 undergraduate and graduate students to evaluate the effectiveness of our method. in the first task, we asked the subjects to order similar objects of different scales from back to front. these objects were rendered with diffuse shading, diffuse shading and shadows, uniform lines, and lines of different width, depending on the depth. we used the kendall tau rank correlation coefficient to measure the difference between the orderings. while diffuse shading without shadows and uniform lines performed worst in our test (∼ 0.55), non-uniform lines performed almost as well (∼ 0.88) as using diffuse shading and shadows (∼ 0.95). in the second task, the subjects were given an input image and two line renderings: one with our style and one with a randomly generated style based upon line thickness and opacity. when given the task to select the line rendering which created the most similar depth perception, 70% of the subjects chose one of our styles. in the last task, we showed that our method can be used to focus the users attention in a similar fashion to using depth-of-field. when given an image created by using depth-of-field, all subjects were able to determine the focus within the image. for a different image created with our method focussing on a certain depth range, 76% of the subjects were able to determine the focal area. this confirms the improved depth perception of our algorithm, as well as its potential usage to focus the users attention on certain areas of the line drawing."
"algorithm 1 details the load balancing process. the algorithm receives the list of current nodes, nodes, the list of running vms, vms, the list of vms to be started at the current scheduling period, newvms, and three thresholds: (i) maximum number of iterations performed by the algorithm to obtain a better placement than the current one, n iter ;"
"merkat is implemented in python and depends on the twisted [cit], paramiko [cit] and zeromq [cit] libraries. we used the twisted framework to develop the xml-rpc services. paramiko is used for the vm connection: the application management system needs to test and configure the vms in which the application runs and it does so through ssh. zeromq is used for the internal communication between various components of merkat (e.g., the applications manager and application controllers, or the application controllers and virtual cluster monitors). merkat's services store their state in a database storage for which we have used a mysql server. the connections to the vms are done through ssh in parallel, by using a thread pool with a size defined in the configuration file. as an iaas cloud manager we use opennebula [cit] 4.6. merkat's services communicate with opennebula through its xml-rpc api. we also shut down opennebula's default scheduler, and replaced it with our vm scheduler."
"for example, the tenant might want to start running an application only if the resource price is below a threshold. if these conditions are not met, the deployment of the virtual platform is either postponed or canceled. for example, the deployment is canceled if the application needs to start its execution before a given deadline (the tenant's defined slo) and the price is too high to allow it. if these conditions are met, the virtual platform manager creates the 5 virtual platform and registers it with the virtual currency manager. the virtual platform manager is also in charge of resuming any virtual platforms that might suspend their vms to avoid executing applications in high price periods."
"the first method will lead to a less reactive system, while the second method requires sophisticated algorithms for price and application performance prediction. designing such prediction algorithms remains an open research question."
"second, the resource allocation has to consider the application characteristics and tenant service level objectives (slos). some applications might have a resource demand that varies frequently due to the variability in the number of requests they have to process, e.g., web applications, while others can simply scale and use all currently available resources, e.g., bags of tasks or mapreduce applications. at the same time, while some applications can achieve better performance by scaling horizontally, i.e., scaling the number of nodes, others might also benefit from scaling their resource demand vertically, i.e., scaling the resource amount per node [cit] . statically allocating resources is inefficient as it leads to under-or over-provisioning, thus either poor application performance or poor utilization."
"however, it can be further extended to allow tenants to express and pay for running their application processes as close as possible to each other."
"merkat is composed of three main services, the vm scheduler, the virtual currency manager and the virtual platform manager. figure 2 gives an overview of them. the virtual currency manager applies virtual currency distribution policies and manages tenant and application budgets. the vm scheduler is in charge of allocating resources to running vms and computing their node placement. the used algorithms are described in section 2.2. the virtual platform manager acts as an entry point for tenants to run their applications on our system. to start an application on the infrastructure, a tenant submits a request containing a virtual platform template to the virtual platform manager."
"we believe this overall difference in the number of operations comes from the lack of accurate application modeling in cloudsim. in our experiment with merkat we use zephyr as an example application, which uses mpi for process communication. cloudsim relies on much simpler application model, in which the application performance is not influenced by network sharing and performance interference is not reliable modeled. thus, when running applications on merkat, merkat's algorithms have to cope with higher variability in the application performance metrics, which leads to higher variability in bids and resource allocations and thus a higher number of migrations. this number can be reduced by tolerating higher allocation errors in the vm placement algorithm."
"this paper introduced a platform for application and resource management in private clouds, called merkat. the goal of merkat is to maximize the resource utilization of the managed infrastructure while providing support for different application resource demand models and tenant slos. to meet this goal merkat transforms the organization's infrastructure to a private cloud and relies on: (i) a proportional-share market to allocate fine-grained cpu and memory amounts to vms and to make tenants aware of the cost of using resources; (ii) a set of autonomous application controllers that can scale the application's resource demand vertically and horizontally to meet the tenant's slo under current price and tenant budget constraints. merkat supports different applications and tenant slos by decentralizing the resource control and treats contention periods that might appear on the private infrastructure by using market mechanisms."
"the algorithm starts from the current vm placement and tries to minimize the maximum vm allocation error, noted with e max in algorithm 1 while keeping the number of vm migrations within the given limit m max . if new vms need to be created, they are placed on the least loaded nodes (lines 6-8) before the vm placement is improved. then, the vm placement is incrementally improved by placing the vm with the highest allocation error among the cpu and memory resources (the allocation error for the cpu/memory resource of the vm i is noted with e ic and, respectively, e im ) to the node that minimizes it (lines [cit] . note that, as each vm has two allocated resources, the maximum allocation error is the maximum among the computed error for each resource (line 15). the vm allocation error computation is performed by a method called computeerrors. to avoid being stuck in a suboptimal solution, the algorithm uses a list that memorizes the last changes (line 21)."
"system stability. poor adaptation policies, currency management policies or a too small scheduling interval might lead to system thrashing, i.e., the system would spend too much time adapting, wasting resources and leading to poor application performance. the system's stability can be improved in two ways: (i) increasing the vm scheduler's scheduling period, together with the application's controller's adaptation period; (ii) improving the adaptation policies."
"the vm scheduler periodically computes resource allocations for the vms for which a bid was submitted, by considering the value of their bid and a resource utilization cap, a max, i.e., the maximum resource utilization of a vm during its lifetime. the vm scheduler uses the a max values to distribute free resources to other vms needing them. we consider that the tenant can estimate a max for her vms. for example, on a cluster with 8-core nodes the tenant knows that her vm cannot use more than 8 cores."
self-tuning virtual machines [cit] run deadline-driven scientific applications in a fully decentralised system where applications are altruistic and adapt with no knowledge one from another. applications run in single vms on a node.
"a virtual platform runs the tenant's application by acquiring vms from the virtual machine market. the design of the virtual platform is specific to the application type the tenant wants to run. vms are acquired by submitting payments for their resources, also called bids. bids can be scaled up or down during the vm runtime. this mechanism has several advantages. first, it provides flexibility for designing a variety of policies to adjust the resource allocation of the application in a selfish way, with regard only to the tenant's slo and application budget. this selfish behavior is natural, as tenants care only about the performance of their applications. in this context, currency management allows some control over this selfish behavior and gives tenants incentives to use better the infrastructure, while also allowing flexibility in meeting their slos. second, allowing a bid per resource per vm leads to efficient resource utilization for the organization, as there is a strong incentive for tenants to design policies that avoid oversubscription by requesting just as much as the application uses from the capacity of a vm. lastly, communicating resource demands through the use of bids leads to a generic system, capable of supporting any kind of application and slo."
"merkat also leads to a higher number of suspended/resumed applications (11 more applications in merkat), as well as number of vm suspend/resume operations (37 more vm suspend/resume operations in merkat) together with a significant number of migrations (310 compared to 62)."
"with similar local directions, this factor tends towards 1, leaving the filter size unchanged. having a higher angular deviation, the factor becomes smaller, which decreases the filter size proportionally to the average angular deviation. we demonstrate the importance of this adaptive filter size in figure 2, where we compare our adaptive filter to the constant version on an image with circles of increasing scale in front of a noisy background. while the constant filter has problems with features smaller than a certain size, the adaptive filter works well up to the point where features cannot be differentiated from noise any longer."
"application model. we consider the following application model. the application is composed of a fixed number of tasks. each task requires one cpu core and a specified amount of memory. we don't model the communication between tasks. to finish their execution the applications need to perform a certain computation amount (e.g., 1000 iterations)."
"when running the application on the merkat's virtual machine market, it is not sufficient for the virtual platform logic just to acquire a number of vms and compute their bids at the beginning of the application execution. as the resource price and allocation are dynamic, this logic will not guarantee that the application receives the right amount of resources to efficiently meet its slo. if the price increases, the current tenant budget might not be enough to cover the cost of the needed resources. thus, the application will run with less resources and not only will the slo not be met, but also the budget will be wasted. if the price decreases, more resources could be provisioned, or the tenant could spend less for the already acquired resources. these issues are solved by the virtual platform, which monitors the application performance and adapts autonomously to the resource price and tenant requirements. figure 4 illustrates the architecture of a virtual platform. a virtual platform is composed of one or more virtual clusters, and an application controller, that manages them on behalf of the application. the application controller receives as input a virtual platform template, containing the application description and adaptation policies. an application can have one or more components, requiring different software configurations. thus, for each application component, the application controller deploys a virtual cluster and starts the application component in it. we define a virtual cluster as a group of vms that have the same resource configuration and use the same base vm image. as the application's components might have different performance metrics, a different virtual cluster monitor running tenant-specific monitoring policies can be started in each virtual cluster."
"to measure the tenant satisfaction that merkat provides on a real testbed, we set up a cloud managed first by merkat and then by a batch scheduler, e.g., maui [cit] . maui is often used by hpc organizations to manage their clusters. to run applications, maui uses algorithms based on fcfs, with backfilling."
"this experiment shows that the horizontal scaling policy allows applications to expand and shrink their resource demand according to the resource availability of the infrastructure, thus improving the resource utilization."
"we measured the performance of our system in terms of total tenant satisfaction in different contention scenarios, of the art, but a large majority is specific to one application type, e.g., bag of tasks, workflows, environment, e.g., public clouds, or are offline solutions. moreover, it is difficult to choose among these different algorithms the most representative one. thus, we chose the edf policy as it is well-known in the state of the art and is often used as a comparison baseline. it is important to note that cloud managers cannot practically apply edf or similar algorithms without limiting their support to a predefined set of application goals (e.g., meeting deadlines). nevertheless, we wanted to compare our system with a centralized system that targets a fixed type slo."
"originally, this policy was used by the operating system schedulers to allocate cpu time to tasks proportionally to a given weight, and inversely proportionally to the sum of all the other concurrent task weights [cit] . merkat uses a modified version of this policy, in which each provisioned vm has an associated bid for its resources, i.e., cpu and memory, and it receives an amount of resource proportional to the bid and inversely proportional to the sum of all concurrent bids [cit] . the value of the bid of a vm can be changed during the vm runtime."
"hadoop [cit], to allocate map/reduce slots to applications using proportional-share. tenants are assigned a budget and they spend it to run applications on the cluster, by specifying a spending rate, i.e. the tenant's willingness to pay for a slot and for a time period. the budget and the spending rate allow the tenant to control how many slots get assigned to her applications over time, and thus, to control the application execution time."
"using a market to manage the resources of a distributed infrastructure is a well-studied problem in the context of clusters and grids [cit] . the reason why markets became so popular in the distributed systems community is the notion of cost, which makes tenants more aware of how many resources to acquire for their own use. based on the pricing model, there are two commonly used market models: commodity markets and auctions."
"algorithm. now, let us describe the application controller behavior. the application controller starts the application when the resource price is low enough to afford a given allocation for each application vm. during the application execution, to keep the application execution time below a given deadline, the controller adapts the vm bid with the vertical scaling policy. the bid is adapted at each application monitoring period, based on the value of t step . if the t step is faster than the reference 0.75 · t re f, the bid is decreased. otherwise, if t step is larger than the reference 0.95 · t re f the bid is increased. we use these thresholds to avoid too many bid oscillations, and thus, resource re-allocations."
"the bid stabilizes after all the applications were submitted. the application with the smallest deadline demands a maximum allocation and thus, the submitted bid is also much higher than in the other two cases. figure 8(a) shows the variation in the resource allocation (also called share) and respectively utilization for the application when it is run with a deadline of 9000 seconds. as the behavior is similar for the other two tested deadlines, we omit depicting them. remember that the vm resource share is the proportional-share the vm gets according to its bid. the resource utilization is how much the application inside the vm consumes. the left axis shows the cpu resource, i.e., percentage of total cpu time, while the right axis shows the memory resource, i.e., in mbs. the besteffort application arrivals can be noticed by looking at the changes in the slo-driven vm resource share. in this case, the memory share reflects the best these arrivals. when the application started alone on the node the vm's share is the entire node capacity. after each application arrival, this share decreases until it equalizes the vm utilization. after all the best-effort applications started running, the slo-driven controller keeps a reduced cpu share as the application can meet its deadline. figure 8(b) shows the variation in the slo-driven application iteration execution time. to understand the behavior of the vertical scaling algorithm, we also give the lower and upper scaling thresholds, v low and v high . we notice that after all the best-effort applications started running, the application controller manages fairly well to keep the iteration execution time between these two thresholds. however, there are cases in which the iteration execution time oscillates."
"before discussing the signification of utility functions, we define the following terms: t exec is the application execution time; t deadline is the time from the submission to deadline; t ideal is the ideal execution time, i.e., when the application runs on a dedicated infrastructure; work done represents the number of iterations the application managed to execute until it was stopped; work total represents the total number of iterations; b is the applications budget per budget renewal period and per task. b is assigned by the tenant and reflects the applications importance. table 1 summarizes the used utility functions. the full deadline tenant values the application execution at her full budget if the application finishes before deadline. otherwise, we express her dissatisfaction as a \"penalty\", which represents the negative value of her budget. the partial deadline tenant is satisfied with the amount of work done until the deadline. thus the value of her satisfaction is proportional to this amount. the full performance tenant becomes dissatisfied proportionally to her application execution slowdown. we bound the value of her dissatisfaction at the negative value of her budget."
"in daily life and also in photorealistic rendering the shading of objects give necessary cues about spatial arrangements in a scene. for line drawings, similar effects can be created by varying the line thickness, as depicted in figure 4 ."
"provisioned over time. if each framework is assigned an equal share of the infrastructure, it would obtain a maximum of 112 vms (green line from figure 11 ). we divided our experiment in five phases. in the first, third and fifth phase"
"finally, other cluster resource management systems focus on providing slo support. these systems use controllers which allocate resources to applications based on obtained, offline and/or online, performance models. applications are usually allocated enough resources to meet their slos; thus more applications run on the infrastructure and the resource utilization is improved. earlier systems focused on web applications [cit] . in this case, resources are allocated to vms in a fine-grained fashion as applications like web servers can have varying cpu and memory utilization. vm migrations are employed to move the vms among nodes either reactively [cit], or proactively [cit] ."
"in this section, we analyze the performance of merkat in terms of total tenant satisfaction when applications adapt their resource demands to track a given slo, and we show its flexibility in supporting different tenant types. tenant satisfaction is an important metric regarding the performance of a resource management system. this satisfaction depends whether that the tenant's slo is violated or not and on how much the tenant actually valued the execution of her application."
"the slo-driven application is started with an initial deadline of 12000 seconds. after 64 minutes from the application start the deadline is changed to 9000 seconds. the application controller's bid adaptation and the allocation changes can be noticed in figure 9 . figure 10 shows the application controller behavior and the variations in the estimated application execution time due to allocation and bid fluctuations. the additional submitted best-effort applications at the beginning of the experiment leads to a decrease in application allocation and thus an increase in its execution time. however, the application controller doesn't react aggressively as its allocation, in this case 266 cpu units, is enough to meet the application deadline. when we decrease the application's deadline (noticed in figure 10 from the change in the reference time), the application controller also adjusts the bid aggressively, leading to an increased resource allocation, in this case close to 400 cpu units, which is the maximum vm allocation. this increase allows the application to keep its iteration execution time close to the reference, thus meeting its deadline."
"the parameters alloc max and alloc min can be determined by the tenant or the system administrator. for example, the tenant could know that a vm will never use more than 8 cores, thus alloc max can be set at 800 cpu units."
"the time period over which the tenant budgets are renewed influences the tenant behavior. if the time period is small, tenants will not have incentives to save budget, and thus, to judiciously use resources. if the time period is too large, tenants might starve while resources are left idle. we consider that such a period should be in terms of weeks or several months. however, deciding properly this time period remains an open issue as it requires running the system on a real-world infrastructure and getting feedback from tenants."
"in our approach we compute the resource price as the sum of all bids divided by the total infrastructure capacity. if this price is smaller than a predefined price, i.e., reserve price, then the reserve price is used. tenants are charged for their running applications at each scheduling interval with a credit amount equal to the product between the resource price and the allocated resource amounts for all running vms."
"to give an example of how the vertical scaling policy can be used by an application controller, let us consider a tenant that wants to execute a static mpi application with nvms vms and a budget b, renewed at a time interval t renew with r credits. the slo requested by the tenant is to finish the application execution before a deadline. the reasoning behind this policy, is that as long as its deadline can be met, the application can reduce its allocation to minimize its execution cost."
"this policy is advantageous as it allows provisioning vms with arbitrary resource allocations at a small time complexity (o(m), where m is the number of vms in the system), aspect which becomes important with the increasing scale of the infrastructures. moreover, this policy is easy to understand and use as it avoids starvation and involuntary vm shutdown or preemption. because each vm receives a resource amount, even in high price periods, policies can be designed to allow applications to decide whether to adapt to these small allocations or voluntary shutdown some of their components."
"consider a system with only slow reactions. the critical reactions (group (iv)) are labeled as 1,···,q and the coarse-grainable reactions (group (ii)) are labeled as q+1,···, m. define a λ (x) as the sum of rates of all the critical reactions,"
"7. invoke subroutine d if exit conditions are met, otherwise collect information: . \"∆t and ∆t new \" is the time length of the previous and next monitor window, respectively. \"ω and ω new \" is the fast reactions set in the previous and next monitor window, respectively. under different conditions, we will take different \"actions\", with the method of choosing the new window length ∆t, the boosting coefficient κ and the fast reaction table ω for the next monitor window. some rational of doing this is given in the appendix."
"in this paper we present a new general algorithm, called boosted hybrid method, that can efficiently simulate complex chemical reaction systems with time-scale separation and disparity in species population. the algorithm combines the idea of boosting and hybrid simulations. it collects the system state information on-the-fly and automatically partition the reactions into fast and slow groups if certain criteria is satisfied. then the original system is approximated by a boosted system, in which the fast reactions are \"slowed-down\" by a change of time-unit. the new system, which poses less scaleseparation in time, is simulated using a hybrid method."
"the nested ssa and boosted ssa share similar theoretical basis: both need to partition the reactions, and share the same quasi-equilibration time h. the major difference is in the implementation: in the former, the fast and slow reactions are simulated separately and only the slow reactions get updated, but in the latter, the fast reactions are first slowed down, then all the reactions are updated together. boosting is easier to implement, especially for multiscale systems with a hierarchy of time scales. in such a situation, if using nested-ssa, we need to identify and order the hierarchy of time scales and implement successively a hierarchy of fast solvers for the different scales involved with the averaged rates for one level coming from the quasi-equilibrium of the fast reaction at the previous level. in the boosting framework, one does not need any additional subroutine to handle the time scale separations, we just boost the reaction rates of the fast dynamics, and use some single scale hybrid solver in hand to perform the time integration."
"the omp approach aims at identifying which time-delayed copy of the reference signal lies in the surveillance signal through an iterative process on the time delays, removing from the strongest to the weakest copy of the reference signal in the surveillance signal. thus, omp assumes that the whole surveillance and reference datasets have been collected and stored in memory. such an approach is poorly suited to fpga implementation which is best designed for pipelined processing of new samples as they are collected. furthermore, the iterative matrix inversion algorithm depicted earlier, despite removing the need for matrix inversion, still requires computing the scalar inverse 1/k, a challenging operating on an fpga."
"in the full paper [cit], we also examine rate performance versus training length t and for various combinations of transmit and receive antennas (m, n)."
"recently, multiple-input multiple-output (mimo) relaying has been proposed as a means of increasing spectral efficiency (e.g., [cit] ). by mimo relaying, we mean that the source, relay, and destination each use multiple antennas for both reception and transmission. mimo relaying brings the possibility of full-duplex operation through spatial self-interference suppres sion (e.g., [cit] . still, the following fundamental questions about full-duplex mimo relaying in the presence of self interference remain: 1) what is the maximum achievable end to-end throughput under a transmit power constraint? 2) how can the system be designed to achieve this throughput?"
"5 by a differential measurement in which multiple echoes induced by the acoustic transducer and returned by the sensor are sampled, subtracting delays between multiple echoes gets rid of the bistatic range delay dependence and provides means for remote sensing physical quantities using passive, cooperative targets probed by a passive radar receiver. the design of the saw transducer aims at delaying echoes beyond clutter and environmental reflections: considering the receiver sensitivity, delaying echoes by 1 to 2 µs (150 to 300 m monostatic range) guarantees that the sensor response is well beyond any non-cooperative target reflection (fig. 2) ."
"the the numerical results are shown in fig. 7 . during the simulation, six components of κ are less than 1, but they are not very small (see fig. 7 (a) ). this means that the effect of boosting is limited. moreover, even after boosting, fig. 7 (b) shows that the rates a j still occupy quite a large scope, which means the system is still very stiff. since here the scale separation is not large enough we can not boost the system further. for this system, the boosted hybrid method is even slower than ssa because of the widely and \"continuously\" distributed reaction rates."
"we found two situations in which the boosted hybrid method is not very efficient. first, when the reaction rates are spread over a large interval without obvious scale separation, boosting strategy is difficult to be applied. in this situation we do not know of any other method capable of handling efficiently this kind of system. we note that thanks to the adaptivity in the boosted hybrid method, the algorithm remains robust (i.e. gives a reasonable approximation of the original system) even though not it is not efficient when compared to ssa. the second situation, illustrated in section 5.5, is concerned with a system where an efficient simulation requires to change the original model. this remains a challenging and interesting task for future work. refinement and further applications of the boosted hybrid method is also currently under investigation."
"if we simulate this system with ssa, fast reactions will fire frequently and involve a high computational cost. but it is only the slow dynamics that we want to capture. if we restrict our attention to slow reactions, the waiting time for the next slow reaction in the system, τ, has the probability density function"
"we proposed a new numerical method to simulate chemical reaction systems with timescale separation and disparity in population species. the method relies on boosting, which is a strategy to decrease the stiffness of a system in presence of time-scale separation. after stiffness reduction, the system may still exhibit disparity in species and we suggest the use of a hybrid method for its simulation in which certain reactions are coarse-grained. we showed that both stiffness reduction and coarse-graining approximation can be adapted in an automatic way in time. this adaptivity is suitable in practice. we notice that works remain to be done to optimize some parameters of the algorithm. even so, by combining boosting and hybrid methods it is possible to save substantial computational cost for many complex systems, when compared to ssa. this has been illustrated numerically for various chemical systems. we showed how the main source of error in the algorithm, the error from boosting and the error from coarse-graining, can be controlled by tuning the scale separation threshold at which the boosting is turned on or by tuning the parameter threshold to set the number of reactions chosen for coarsegraining."
"consider a chemical reaction system with only critical reactions. the slow reactions in group (iii) are labeled as 1,2,···,q, and the fast reactions in group (iv) are labeled as q+1,q+2,···, m. scale separation between fast and slow reactions requires min(a q+1,···,a m ) ≫ max(a 1,···,a q )."
"use ω new and ∆t new 3. set the window length ∆t, the boosting coefficient κ and the fast reaction set ω according to the rules given by table 1 . invoke subroutine b."
in this section we start by describing how to adaptively partition the reactions in a complex chemical reaction systems. a hybrid method is then introduced which allows to use different numerical strategies for chemical species with different population size.
"we will use ns and nr to denote the number of transmit antennas at the source and relay, respectively, and mr and lvld to denote the number of receive antennas at the relay and destination, respectively. here and in the sequel, we use the 978-1-4673-5051-8/12/$3l.00 ©20 12 ieeesubscripts s for source, r for relay, and d for destination, and we omit subscripts when referring to common quantities."
"we use the hybrid method to simulate the chemical system for a time of length ∆t, which is called a monitoring window. information such as reaction rates and system state is collected during the monitoring window, with which the monitor can identify timescale separation between fast and slow reactions, and if so, check if quasi-equilibrium is reached for the fast reactions. based on these judgments, the method is able to maintain a reasonable approximation of the original system."
"thus, such an approach does not address the practical measurement issues that 1/ the time delays and weights of the reference signal copies in the surveillance signal hardly vary from one measurement to another considering the fast (radiofrequency) sampling rate and 2/ data from the surveillance and reference channels are continuously streamed from the two analog-to-digital converter channels. hence, as shown in fig. 4, rather than addressing the dsi suppression issue along the time-delay (matrix column) direction, we consider the challenge of identifying time-delayed copies of the reference signal in the surveillance signal along the acquisition (matrix lines) dimension."
"boosting can also be used in coarse-grainable reactions, so to get the boosted tau-leaping. because in tau-leaping the step-size τ is deterministic, the conventional procedure of boosting that is used in solving stiff odes and sdes can be applied here. the following algorithm can be obtained by following [cit] ."
"passive radar provides an attractive solution to multiple remote sensing issues including not relying on a dedicated emitter to illuminate targets bathed in electromagnetic smog, 1 and being able to detect signals returned by a cooperative target whose signature is representative on a physical quantity (identification, temperature, strain ...). 2 the basic signal processing principle of passive radar is to collect on the one hand a reference signal of the pseudo-random signal -most passive radar systems use existing commercial communication or navigation signals with appropriate random properties -illuminating the target, and on the other hand the surveillance signal reflected by the target, possibly frequency shifted by a doppler offset frequency if the target is moving 3 (fig. 1) . unlike active radar, the emitted continuous signal is not known so that must be observed by a reference channel (or reconstructed when a single receiver is used 4 ) rather than locally memorized. the emitted signal might leak on the surveillance channel, inducing a strong background correlation signal that might be broad enough to overwhelm the signals reflected by the targets. redpitaya platform: zynq7010 figure 1 . left: data acquisition and processing principle: a redpitaya board based zynq system on chip collects data from 125 ms/s analog to digital converters (adc) following a radiofrequency to baseband frequency transposition (hardware mixer and filters). the sampled data are frequency transposed and filtered for adjacent wifi channel data collection by the programmable logic (pl) part of the zynq. at the moment, the resulting data are transferred to the processing system (ps) to be sent to the host computer for further processing, limiting the measurement refresh rate. the present work aims (red) at including the dsi suppression and correlation in the pl. right: experimental setup using an 8-patch antenna array as receiver for direction of arrival computation, making the dsi suppression load even heavier when probing multiple sensors. the antenna array (background) to sensor (foreground) distance is 1 m. the inset on the left is a screenshot of the computer display on the right exhibiting the azimuth-range map with the sensor echo delays (representative of the physical quantity) along the verical axis and the azimuth for spatially separating the sensor response along the horizontal axis."
", which is consistent with the leaping condition (2.4). the hybrid method is given in algorithm 1. note that we need to do a new partition (step 3 below) each time after we update the system state because the critical reaction set may have changed."
"thus, the iterative matrix inversion implementation of omp is functional as an algorithm for extracting sensor echoes from the correlation between the dsi-cleaned surveillance signal and the reference signal. the implementation remains computationally intensive and requires keeping a memory of the whole reference and surveillance signals, using most resources on the low-end fpgas (e.g. zynq-7010 fitted on the redpitaya board) considered for assembling an embedded reader with real time sensor echo detection capability."
"recalling that the data communication period is partitioned into two periods, '?data [cit] and '?data [cit], and that-within each the transmitted signals are wide-sense stationary, the relay's (instantaneous, distorted) signal at any time t e '?data [l] is + (y'rkhrr -djhrr)(xr(t) + cr(t)) + er(t), (7) as implied by fig. 1 and (5) . defining the aggregate noise term !:::."
"using omp and the iterative matrix inversion implementation, the dsi is indeed suppressed as shown in fig. 6 . in this figure, the result of dsi suppression using the pseudo-inverse (top) is favorably compared with the omp result (middle) as emphasized by the difference between the two curves (bottom)."
"we consider the problem of communicating from source to destination nodes through a relay node. traditional relay systems operate in a half-duplex, whereby the time-frequency signal-space used for the source-to-relay link is kept orthog onal to that used for the relay-to-destination link, such as with non-overlapping time periods or frequency bands. half duplex operation is used to avoid the high levels of relay self interference that are faced with full-duplex operation, where the source and relay share a common time-frequency signal space. for example, it is not unusual for the ratio between the relay's self-interference power and desired incoming signal power to exceed that of the relay's front-end hardware, making it impossible to recover the desired signal. the importance of limited dynamic-range (dr) cannot be overstressed; notice that, even if the self-interference signal was perfectly known, limited-dr renders perfect cancellation impossible."
"we also modified other parameters in the algorithm, such as ǫ that controls the coarsegrained time-step. we change ǫ from 0.1 to 0.05. so now critical reaction is more frequent and the leaping step-size is smaller. the algorithm will be much slower but no appreciable improvement in accuracy is observed for this system (results are not shown)."
adaptive selection of fast and slow reactions. we further partition all the reactions as being either fast or slow reactions. define the characteristic rate for reaction j as
"since the new matrix inverse results from a previous iteration with the addition of one more vector from the reference signal to be subtracted from the surveillance, the problem of finding this new inverse can be expressed with the following shaped matrices:"
"since the classical estimator for detecting echoes in a radar signal is the correlation, passive radar target identification heavily relies on correlating an unknown but measured reference signal from a noncooperative illuminator with the surveillance signal collected by an antenna (or antenna array in our case) directed towards the sensor. however, any structure in the illuminating signal as often found in communication signals will depart from the random structure whose correlation would be a dirac function and will introduce some long term memory effects with strong correlation side lobes. thus, even if clutter has faded out by the time the sensor echoes are detected, the illuminating signal source autocorrelation might still overwhelm the sensor signal as will be demonstrated in the case of wifi. this direct signal interference (dsi) removal is the core requirement to extract the target (here cooperative with the sensor designed on purpose) signature from the correlation between the reference signal and the cleaned surveillance signal."
"the next section will compare both the least square estimate of the weights of the time-delayed copies of the reference signal in the surveillance signal using the pseudo-inverse (appendix listing 1) and the result of iterative inverse matrix computation (appendix listing 2) in which no explicit matrix inversion is needed but only matrix products. the data have been collected from an experimental wifi passive radar setup with an 8-path antenna array receiver so that eight independent measurements of the the surveillance signals are analyzed. 2 the reference signal will either be used to synthesize known surveillance signals for algorithm assessment, or will be compared with experimental measurements of surveillance signals for sensor characteristics extraction."
"the sgd method 15 aims at solving the least-squares problem using an iterative optimization scheme which avoids matrix inversions and can proceed incrementally, yielding a highly scalable algorithm. in our setting, identifying the weights of the time delayed copies of the reference signal in the surveillance signal can be rephrased as a least squares problem"
"remark 2.2. note that after several steps of the numerical method for the coarsegrainable reactions, the classification of the critical and non-critical reactions may no longer be valid. in this case, we have to partition the system again."
"on the other hand, reactions involving species with large population size fire very frequently which also make the ssa computationally inefficient. to overcome this difficulty, haseltine and rawlings proposed a hybrid method for solving chemical reaction systems with disparity in species population [cit] . the main idea is to apply a coarse-graining approximation (based on stochastic or ordinary differential equations) for species with large population size and ssa for species with small population size. many variants of hybrid method have been proposed [6, [cit] . based on the system size, the τ-leaping method [cit], chemical langevin equations or reaction rate equations [cit] are often used as the coarse solver."
this system describes the heat shock response of the e. coli bacteria [cit] . it consists of 28 species and 61 reactions as given in the appendix b.
"in chemical reaction systems, it is very common to have large time-scale separations. a widely used strategy is quasi-equilibrium approximation. there exist a number of ways of implementing this idea in chemical reaction systems, such as slow-scale ssa, nested ssa. here we consider another approach called boosting, first introduced by vandeneijnden [cit] . this approach, as we will see, has the advantage of allowing for an easy implementation. the idea of boosting has been originally proposed in dynamical systems with scale-separation in time, such as stiff odes and sdes. we will see here that it is also a good strategy for stochastic systems arising in chemical reactions."
"moreover, the rate it (q) bits-per-channel-use (bpcu) can be achieved via independent gaussian codebooks at the transmit ters and maximum-likelihood detection at the receivers [cit] ."
"this approach was used in 10 to demonstrate passive radar measurement of passive cooperative targets illuminated by a wifi signal. however, the measurement update rate is limited by dsi suppression: the data acquisition, frequency transposition and filtering are processed in a pipeline running in the field programmable gate array (fpga) of the zynq system on chip (fig. 1) . due to the computational burden of dsi suppression, the current implementation transfers the resulting reference and surveillance vectors to the host computer in charge of running the dsi suppression and cross-correlation. since cross-correlation is readily achieved as a pipelined algorithm on an fpga, running the full processing algorithm on fpga for increased data acquisition rate requires solving the dsi suppression following an algorithm compatible with a pipelined approach applicable to a gate array."
"one fundamental method in simulating chemical reaction systems is gillespie's stochastic simulating algorithm (ssa) [cit] . it can generate statistically exact trajectories of the system state by randomly sampling each reaction event. in principle, ssa applies to any chemical reaction system, but the method become computationally costly when reaction events occur very frequently in the system. this often happens because of the co-existence of fast and slow dynamics in a system, or reactions involving species with very large populations, or both."
"we have addressed the issue of real time direct signal interference removal in a passive radar setup designed to measure passive sensor responses. since radiofrequency datastreams are collected using a field programmable gate array, algorithms compatible with such architectures are considered, namely with minimal memory consumption and lack of matrix inversion. we have demonstrated that an iterative matrix inversion applied to orthogonal matching pursuit will remove the need for matrix inversion in the fpga and restrict computations to vector multiplications. however, a scalar value inverse remains to be computed, a potentially challenging task in the fpga hardly supporting division. as an alternative, we have explored the stochastic gradient descent algorithm with hard thresholding which demonstrates comparable performance with an algorithm suitable for processing streams of data and adapting continuously the weights of the direct signal interference for subtraction and correlating the cleaned surveillance signal with the reference signal."
"advances in experimental and computational methods over the last decades have made a quantitative, systematic understanding of cellular processes in molecular level possible [cit] . for micro-scale biochemical systems, such as one single living cell, consider-able evidence indicates that stochasticity plays an important role, especially when lowmolecular-number reactant species are being considered [cit] . the usefulness of the traditional deterministic approach, based on reaction rate equations, is limited in such situation. in turn, many stochastic biochemical reaction networks have been built to take into account the randomness in biological processes. because of the disparity of time scales and species population, the simulation of such systems is often challenging and the development of new numerical techniques for multiscale chemical reactions has become an active research field [cit] ."
"11 based on compressive sensing (cs) theory, 9 the dsi can be estimated by omp algorithm with under-sampled measurement, which can help to reduce the computational complexity. indeed, omp implementation on fpga has been demonstrated already [cit] with reasonable computational power requirement with respect to those provided by the zynq 7010 system on chip fitted on the redpitaya platform we are using on our experiment."
"reset to the original system. with this statistical information, it will decide if quasi-equilibrium is reached. if so, a boosted system will be proposed to replace the original system to be simulated by the hybrid method."
"in this algorithm, the contribution of each time delayed copy of x in the under-sampled surveillance signal sur is computed as (x ) h · sur (fig. 5) . the time delay with highest contribution is selected at index p 1 and its weight is computed as pinv(x (p 1 )) · sur, where x (p 1 ) is the p 1 th column of x (appendix, first listing). notice that in this first iteration, p 1 is a scalar so that x (p 1 ) is a vector and"
"in this section we test the efficiency and the versatility of our method on five numerical examples. we will see that for the first three examples, our method is efficient and can handle numerically the chemical systems in robust and adaptive way. for the last two examples, the method shows limited applicability. the comparison of our method with other algorithms will be discussed in a future work."
"the above equation is exact, but x(t) contains the information of the fast reactions which we do not want to resolve. if the fast reactions quickly drive x(t) into quasi-equilibrium, then we can approximate the effective rate of slow reaction j b j as"
the paper is organized as follows. in section 2 we discuss the criteria for partitioning the reactions and present a improved hybrid method. the boosting strategy is introduced in section 3. its combination with the boosting method is presented in section 4. various numerical experiments are given in section 5 illustrating the versatility and efficiency of the method. finally in section 6 we summarize our findings and discuss unsolved issues and future works.
"the computational burden of inverting the matrix remains high since it requires collecting the whole dataset before performing the dsi suppression. here we consider the iterative subtraction process of subtracting sequentially the heaviest contributions of the reference signal in the surveillance channel, using the omp algorithm."
"adaptive selection of critical and non-critical reactions. given the system state x, we define the bottleneck specie number z j for each reaction j as,"
our new approach to the problem combines the stochastic gradient scheme (6) with the iterative hard thresholding scheme of 17 by setting to 0 the weights below a given threshold defined as a fraction of the maximum of the weight values to accelerate the convergence of the algorithm.
"2. select the step-size δt for the fast reactions and ∆t for the slow reactions. choose a parameter k, which is the estimated number of steps that the fast reactions require to reach quasi-equilibrium. because of the time-scale separation between fast and slow reactions, we have kδt ≪ τ. repeat the following procedure k times:"
"in this paper, we aim to answer these two fundamental questions while paying special attention to the effects of both limited-dr and imperfect channel-state information (csi). limited-dr is a natural consequence of non-ideal amplifiers, oscillators, analog-to-digital converters (adcs), and digital to-analog converters (dacs). to model the effects of limited receiver-dr, we inject, at each receive antenna, an additive white gaussian \"receiver distortion\" with variance j3 times the energy impinging on that receive antenna (where j3 « 1)."
"the classical approach to suppress this dsi signal is to use a least square weighing approach where time shifted copies of the reference signal are sought in the surveillance signal. 6 assuming the echoes generated by targets -whether non-cooperative or cooperative sensor targets for which the echoes are designed to lie beyond clutter -are delayed by more than a minimum time offset t, all copies of the reference signal delayed between 0 and t in the surveillance signal with estimated weights are subtracted before cross-correlating the reference and the surveillance signal to identify target. practically, this operation is achieved by creating a matrix x with time delayed copies of the reference signal ref and identifying the weight w of the time-delayed copies of ref in the surveillance signal sur as"
"another limitation of the new method (and most of the current simulation method) is illustrated by the following simple example, a is a transcription factor, e is dna, ea is the protein-dna binding complex, and b is protein. usually the species a and b have large populations and specie e is only one in molecular number. assume the rate constant of reactions 2 and 3 are much faster than reactions 1 and 4. the overall effect is that a is constantly converted into b. in this system, reactions 1 and 4 are suitable for coarse-grained approximation while reactions 2 and 3 are critical. moreover, the number of e will quickly reach to a stationary distribution. however, we can not apply boosting here because no partial equilibrium holds for the fast reactions 2 and 3 as there is a non-zero flux from a to b."
"is the state-change vector corresponding to this reaction. given the initial state x(0) and the aforementioned evolutionary law, we can use gillespie's ssa [cit] to simulate trajectories of x(t) and study its statistical properties. but for systems with multiple well separated time-scales and disparity in species population, this procedure often becomes too costly. the aim of this work is to provide a more efficient simulation strategy for such systems. the first step is to partition the system. we divide the reactions into different groups according to their property (see fig. 2) . each group will then be treated with different algorithms. as shown in fig. 2, we partition all the reactions into four groups based on the species population numbers and the reaction rates. reactions with fast dynamics are simply called fast reactions, the remaining reactions are called slow reactions. we also distinguish reactions involving species with large population size, called coarse-grainable reactions, from reactions involving species with small population size, called critical reactions. we give below the implementation details for realizing such a partition."
"in this section we introduce the boosted hybrid method as an adaptive solver for multiscale chemical reaction system. as fig. 1 shows, it has two major components. the monitor will maintain an approximating system. it will monitor the system state during the simulation to see if there exists quasi-equilibrium for fast reactions and decide whether it is possible to boost the system to reduce stiffness. the approximating system is simulated using the hybrid method."
"while the above scheme works for tau-leaping, we do not know how to generalize it into ssa because of the randomness of the step-size. the boosted tau-leaping algorithm is as follows: algorithm 4. boosted tau-leaping with modified rates. in fact, algorithms 3 and 4 are theoretically equivalent, but the latter is more flexible, and more importantly, it works for both ssa and tau-leaping."
"the length of the monitoring window should be large enough to let the fast reactions reach equilibrium, but not too large, otherwise both the efficiency and the accuracy of the system may be compromised. we choose ∆t to be 100 times the minimum characteristic rate among all the fast reactions:"
"where r j is the total number that reaction j fired, and record x min i and x max i, the minimum and the maximum state reached during the monitoring window, respectively. we then compute the so-called redundancy coefficient defined by,"
"this work is supported by the oscillatorimp anr (french agence nationale de la recherche) equipex grant as well as the fast-lab labcom anr grant. j.-mf is grateful to tohoku university (sendai, japan) [cit] which prompted this investigation."
"the new algorithm has many advantages. first, for chemical systems with multiple time scales, it can hierarchically slow down (boost) the system which can then be numerically integrated by a single-scale solver. it avoids the use of multiscale solver such as the slow-scale ssa or nested ssa solver which makes its coding easier for complex systems. second, it does not need an a priori knowledge of the features of the chemical system (such as reaction rates and population size). third, it allows for a systematic control of the main sources of the numerical errors coming from the boosting in time and the coarse graining procedure for species with large population size. numerical experiments show good performance for the long-time simulation of systems with multiple scales in time and population size."
"when 'rjr « pr, the 'rjr-dependent terms in (21) can be ignored, after which it is straightforward to show that, under the constraint (15), the optimal covariances are the \"full duplex\""
"to apply quasi-equilibrium approximations, time-scale separation between the averaged characteristic rates is required. then we can partition all the reactions into four non-overlapping groups as shown in fig. 2."
the context of this study focuses on probing one particular case of targets designed as passive wireless sensors with echoes delay by 1 to 3 µs. a cooperative target illuminated by a non-cooperative emitter returns echoes whose delays are dependent not only on the bistatic range but also on some physical quantities of the sensor environment. such a result is achieved by converting the incoming radiofrequency signal into an acoustic wave by patterning electrodes on a piezoelectric substrate in the classical surface acoustic wave (saw) transducer operation.
"the above equation is exactly (3.2). with the effective rates, an outer-ssa subroutine is called to simulate one single event among the slow reactions and update the system state and the time."
"the hybrid method allows efficient simulation of chemical reaction systems with disparity in species population. it is also a major building block of the boosted hybrid method relying on the use of boosting to handle the time-scale separation and a hybrid treatment of the resulting reactions groups (ii) and (iv). the idea is to use ssa for critical reactions and at the same time use tau-leaping method for coarse-grainable reaction. the difficulty is the coupling of time-steps between those used by tau-leaping and ssa. there are a lot of works in this direction [6, [cit] . the coupling strategy that we use here relies on [cit] . we however propose some improvements of the aforementioned work related to an adaptive partitions of the reactions as explained below."
"to solve the multi-objective problem, memetic algorithm (ma) is utilized as a form of population-based evolutionary algorithms hybridized with individual learning procedure that are capable of performing local refinements. without loss of generality, the framework of multi-objective memetic algorithms (momas) can be summarized by figure 2 ."
de castro and von zuben developed the clonal selection algorithm on the basis of clonal selection theory of the immune system [cit] . it was proved that can perform pattern recognition and adapt to solve multi-modal optimization tasks. the clonalg algorithm can be described as follows:
"the proposed framework was tested and compared with a widely used existing method (bfe algorithm). synthetic data sets simulating trajectories generated by large number of moving objects were used to test the scalability of the framework. real data sets from different application domains and with different characteristics were used to assess the performance and analyse the discovered patterns. compared with the existing method (bfe), the proposed framework shows better performance in high-dimensional data sets."
"where d denote the sum of ratings and the sum of 1's (flags) of item i j respectively. therefore, the average rating of item i j is calculated as follows."
"where d is the number of sub-pixels, c is the number of land cover classes, and funciton irandom(1, d) returns a random integer value with the range [1, d] . after initialization, the simulation of the clonal selection process begins."
"depending on different scenarios, the types of recommendations for users may vary. in our model, we consider two types of recommendations, by which the solutions represent determining recommendation scores for all items and based on these scores target user is able to choose the most suitable item for him."
"once these ciphertexts are received by the server, it computes new ciphertexts of similarity between item i 1 and i 2 homomorphically as to server by"
"times; hence it is necessary to check this requirement, in addition to the minimum duration (δ), before reporting it as a valid flock. as in the bfe algorithm, the overlapping issue requires the pruning of duplicates and checking for redundant patterns. using a tree structure, flock patterns with the same members (and the same start and end timestamps) will be easily detected and excluded. redundant patterns occur when two patterns share exactly the same members but the time duration of one of them is contained by the longest pattern. using the same data structure, this kind of pattern can also be detected, keeping just the longest one. once the postprocessing stage finishes, the final flock patterns are reported."
"unlike the existing work, in this article, we focus on a spatiotemporal analysis of cdrs for optimizing and categorizing network traffic. the categorization of network traces will be helpful in efficiently monitoring and optimizing network traffic."
"step 3: select n of the best highest affinity elements of m and generate copies of these individuals proportionally to their affinity with the antigen. the higher the affinity, the higher the number of copies, and vice-versa;"
"information about time and location for each point of the trajectories was encoded into unique ids for the disks. once the lcm algorithm retrieves the set of frequent patterns, it is necessary to decode this information and check the quality and validity of the flocks. it is possible that the members of a valid frequent pattern belong to disks in non-consecutive table 1 . transactional version of the data set from figure 1 ."
"this paper presents a privacy-preserving recommender system based on item-item similarity, which can protect the user profile and rating history from any third parties or even from other users. moreover, the server is able to compute the desired computations for recommendations without compromising the true rating information. the experimental results of our proposed model demonstrate high accuracy in-terms of the computation and communication costs as well as assuring the privacy, while the existing works suffer by failing to maintain the balance between them. furthermore, the proposed method outperforms other cryptographic-based techniques in-terms of computational cost to generate recommendations. our future work includes developing more efficient and secure recommender system using user-based similarity."
"as after the data preprocessing, we summarize data in three columns. the first column has information about grid id (grid id represents the location of the activity in milan city). the second column contains the time of the particular activity and third activity level (we sum up the values of received sms, sent sms out, incoming calls and outgoing calls and named it as activity level). we may better understand dynamic nature of cdr data by analyzing it in both spatial and temporal context. this static analysis signifies the relationship of cdr activity in both spatial and time domains. based on the spatio-temporal charecteristics, we develop a framework for mobile traffic classification."
"there is always a possibility of stagnation in msmf. to move away from the point of stagnation, a feasible operation is a neighborhood or local search, which can be applied to a solution to find a more feasible solution in the local neighborhood. in this paper, the values in the individual indexed by i and j are then swapped. the objective function of the new individual is calculated, and only if there is an improvement is the new solution accepted. in addition, the pixel-swapping algorithm is also utilized in the local search."
"the useful insights obtained from spatiotemporal analytics of cdrs are helpful in subscribers' behavior profiling, resources consumption profiling, and mobility profiling. these useful insights will perform a key role in policy making and the provision of services for the implementation of smart cities and urban computing."
"unlike the cbf, the cf-based algorithm generates recommendations based on item ratings. cf also works in two steps: first calculating similarity among the items based on their ratings and then predicting new ratings for the items. given similarities among the items, the item-based cf computes the prediction of user u i for item i k by"
"after cloning and mutation, the new offspring combines c* with the parent population to form a temporary population. the new population ab is obtained by the non-dominated sorting and selection (see also 4.3)."
the proposed method does not have any accuracy loss during recommendation generation. we carried out the experiments twice with and without the security protocol to check if there is any effect in recommendation results. we found that there is no loss of recommendation accuracy while the proposed security protocol is added into recommendation process.
"lastly, we present a generic data-driven resource allocation approach for cellular networks. the approach utilizes unsupervised clustering and a trained neural network to classify cells in a cluster according to the respective activity level."
"as elgamal cryptosystem can not handle fraction number, the server multiplies every plaintexts by 100. the average ratings of all items are shown in table 3 ."
", where the value of each bit in the string t ab represents the class of each sub-pixel. because the number of sub-pixels for each class i nc has been calculated by fraction images in adavance, the algorithm should satisfy the limition of equation (3). the initial process is as follows: for each class i, selecting i nc sub-pixels using the following equation until all sub-pixels have been assigned to the land cover class"
"we utilize machine learning's unsupervised clustering algorithm for categorizing the mobile traffic patterns in different groups/classes. then using the clustering results, we train a neural network for classification of network traffic."
"with the help of expression presented in equation (1), the proposed framework can work for partially available information. for example, wherever required, we have taken an average of previous and next value to get the current missing value."
we assume our privacy-preserving recommender system protocol is based on a semi-trusted recommender server and multiple users participated in the recommendation system. the proof that our proposed solutions really fulfil the privacy requirements consist of three main observations:
"understanding of network traces produces numerous benefits for social network analysis [cit] as well as for cellular network analysis [cit] . the network traces are divided into two categories (i) traces obtained from the mobile devices (ii) traces obtained from the mobile operators. in the first category, network information such as network performance, geographic information, service quality, throughput, and latency are monitored automatically by the applications (apps) running on the mobile devices [cit] . the limitation of this approach is that the network information/traces are obtained only for a limited number of users who have the app installed on their devices. so the insights obtained from such network traces are not applicable for monitoring the large-scale mobile network. on the other hand, the traces obtained from network operator provide a bundle of information such as users mobility, service quality, network performance etc for an entire cellular network [cit] . the insights obtained from these traces are very useful for monitoring the network as well as the behavior of the users [cit] . the dataset utilized in our work is for a densely populated area, and obtained from the network operator."
"the popularity of tracking devices continues to contribute to increasing volumes of spatio-temporal data about moving objects. current approaches in analysing these data are unable to capture collective behaviour and correlations among moving objects. an example of these types of patterns is moving flocks. this article develops an improved algorithm for mining such patterns following a frequent pattern discovery approach, a well-known task in traditional data mining. it uses transaction-based data representation of trajectories to generate a database that facilitates the application of scalable and efficient frequent pattern mining algorithms. results were compared with an existing method (basic flock evaluation or bfe) and are demonstrated for both synthetic and real data sets with a large number of trajectories. the results illustrate a significant performance increase. furthermore, the improved algorithm has been embedded into a visual environment that allows manipulation of input parameters and interactive recomputation of the resulting flocks. to illustrate the visual environment a data set containing 30 years of tropical cyclone tracks with 6 hourly observations is used. the example illustrates how the visual environment facilitates exploration and verification of flocks by changing the input parameters and instantly showing the spatio-temporal distribution of the resulting flocks in the space-time cube and interactively selecting, querying and saving the resulting flocks for further analysis and verification."
"the fimi [cit] ) is an initiative that collects the most representative state-ofthe-art algorithms, their implementations, their open source code and the sample data sets. it also analyses the performance of different techniques with varying parameters and in relation to different data sets [cit], goethals and ."
"to build the proposed framework, it was decided to keep the first part of the bfe algorithm, but to address the combinatorial problem using a fpm approach. the first stage in both the approaches involved generation of a final set of disks of trajectory clusters in each timestamp. furthermore, in order to use fpm to address the combinatorial problem of the bfe, the following three steps are needed:"
"open access this article is distributed under the terms of the creative commons attribution 4.0 international license (http://crea tivecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the creative commons license, and indicate if changes were made."
"the proposed framework is modular, and different techniques can be applied to improve the performance depending on the particular application domain. although a specific frequent pattern and spatial clustering algorithm was implemented in this framework, alternative methodologies can be used."
"the high performance allows for interactive recomputation of the flocks, until desired/ optimal values are found. the search for optimal flock patterns is a subjective process that depends on the application domain and the definition of flock in given data sets; there is no known computational way for directly finding the input values of the algorithm. for instance, the flocking patterns that are visualised in figure 5 were determined interactively, and they were found to be representative for the data set and consistent with the findings that indicate that the average size of tropical cyclones varies between 330 and 660 km [cit] ."
"the proposed clustering based artificial neural network model (c-ann) is applied for observing the future cdr activity class. with c-ann, the cdrs activity classification is converted from unsupervised to supervised multi-class classification problem. from the categorical distribution of cdr activities obtained from clustering analysis, targets for c-ann model are set. c-ann model classifies cdr activities according to activity levels and spatiotemporal characteristics. the inputs of the c-ann model are cdr activity level and spatial feature. a spatial feature is obtained from the spatial analysis described in the prior section. the outputs of c-ann model are multiple classes named c0, c1, c2, and c3 (where c0 represents very high activity cluster/class, similarly c1, c2, c3 represents the cluster/class with medium, low and very low activity)."
"in our c-ann model for activity classification, loss and accuracy are adopted as performance metrics. as in the proposed model, our aim is multi-class classification, so categorical cross entropy is used as a loss function. simulation results show that training losses and testing losses are decreasing as the number of epochs is increasing. it is shown in figure 15 that the training loss is decreasing rapidly in the first three epochs and after that reach to a stable state. the testing loss also follows the training loss. this phenomenon shows that the model is fast converging and time efficient. similarly, training and testing accuracies are increasing as the number of epochs is increasing and reach a stable state after some epochs. figures 15 and 16 represent training and testing losses and accuracies curves for c-ann model of activity classification. further, c-ann based classification model can be used in many real-world network scenarios such as spectrum distribution or small cells deployment. for example, the proposed model can be used to identify low and high activity regions. thus, additional small cells and spectrum can then be allocated to the high activity region. in this way, qos would be improved and energy consumption would be reduced. for such case, the accuracy of the model is very important, if the proposed model is not able to identify the region accurately then the quality of service will greatly be affected. in next-generation networks, there will be ultra densification of small cells and billions of mobile devices/user equipment, so such type of model is heavily needed for efficient deployment of small cells and improvement in qos."
"although the patterns are considered as valid output from fpm algorithms, they will require additional checking before they can be reported as valid flocks (see section 3.4)."
"temporal correlation is observed for measuring the temporal dependencies in the cdrs. autocorrelation function (acf) is widely used for measuring temporal variation of time series data [cit] . for observing the temporal dependencies in time series, acf calculates the correlation of time series with the previous observations of the same time series called lags (this is also termed as serial correlation). equation (4) shows the mathematical representation of temporal correlation."
"as we have also observed from the temporal analysis of network traces, that the user follows a specific temporal pattern for a day and week. so the resources can be distributed according to peak hours and off-peak hours. in this way, a large number of network resources can be saved from wasting otherwise, due to low activity."
where i j and i k represent two individual items. r i;j and r i;k represent the ratings provided by the user u i on those two items and n represents the total number of users.
"the first step of the framework, common to both the approaches, is to identify a final set of clusters in each timestamp. at this stage, both the bfe algorithm and the proposed framework use a fixed disk shape, a circumference with a predefined diameter and an euclidean distance metric. the main objective is the generation of a final set of disks which cluster the number of trajectories in groups according to proximity criteria. this step still uses the parameter \", to define the diameter of the disks, and μ, [cit] ."
"for instance, we show the process of private cbf-based recommendation generation for user u 2 on item i 1 user u 2 locally decrypts the results using his own private key x 2 as follows."
"there are two basic entities that drive a recommender system: users who use the recommender system to provide opinions as well as receive recommendations and items that are rated by users. the inputs to a recommender system are usually arithmetic rating values, which express the users' opinions of items and follow a specified numerical scale (example: 1: bad to 5: excellent). the outputs of a recommender system can be either predictions or recommendations."
"the most common way to visualise the clustering results is by colouring the display elements representing the clustered items according to their cluster membership and involving graphical summarisation of clusters, such as generation of convex hulls [cit] . we have used both the approaches for the task of flock verification; flocks are shown as cylinders where the size is proportional to the value of \" parameter. selection of flocks on screen changes the colouring of the flocks and trajectories to indicate membership. thus, users have options of selecting flocks, enquiring flock id and ids of trajectories that the flocks consist of ( figure 6 ) and saving the resulting flocks in a column or in a separate table if needed for further analysis and verification (figure 4) . the stc and the newly developed fpm flock detection functionality have been delivered as a plugin for ilwis, the gis software that is developed by itc and 52north. readers are encouraged to try this functionality by downloading this free and open source software (http://52north.org/communities/ilwis/about)."
"recommender systems [cit] provide meaningful and useful recommendations to users by making use of explicit and implicit information about user preferences. recommendations are also often based on the degree of similarity between the active user and all other users, or one particular item that the user has rated and all other items. the items can be of any type: books, movies, web pages, restaurants, sightseeing places, online news, and even lifestyles. by collecting information about users' preferences for different items, a recommender system creates their profiles. these preferences can help the recommender system to predict other items that might also be of interest to the user in the future. content-based filtering (cbf) and collaborative filtering (cf) are the most commonly used techniques that generate recommendations for users based on their preferences. cbf predicts a user's rating on a particular item based on the previous ratings and item features, while cf generates recommendations based on the previous ratings only. in order to run the process of recommendations, users' profiles must be available to the recommender server (or service providers). therefore, there are risks that such information is leaked to malicious parties which can lead to severe damage to the user's privacy (e.g. exposure or generating false recommendations) [cit] . figure 1 shows the general architecture of a conventional recommender system and possible ways in which privacy breaches can occur. it is thus crucial to adequately protect privacy of information managed by recommender systems. existing approaches can be categorized as follows."
"thus, the similarities among other items are calculated, and the resultant matrix is stored by the server. the range of the similarity between two items is à1 to 1, where they denote most dissimilarity and most similarity, respectively. the similarities among all items are shown in table 4, where each similarity is multiplied by 100 to cope with homomorphic properties of elgamal encryption."
"provide each ab in the clone set c with the opportunity to produce mutated offspring c * . the higher the affinity, the smaller the mutation rate. the mutation process utilizes the non-uniform mutation operator, and exchanges the position of the sub-pixels. to improve the intelligence of msmf, the mutation rate of each cloned antibody, pm, is adaptively determined according to its affinity."
"due to the increasing availability of movement data sets, the interest in querying patterns which describe collective behaviour has also increased. [cit] identified three groups of 'collective' patterns in moving object databases: clustering for moving objects, convoy queries and flock patterns. moving clusters [cit] ) and convoy queries [cit] ) differ from flock patterns mainly because they do not necessarily contain the same objects during the lifetime of a cluster or convoy. in addition, as these methods are based on spatial clustering, they do not assume any shape restriction."
"early approaches to discovering patterns from spatio-temporal data sets include ad hoc queries aimed at answering single predicate range or nearest neighbour queries. as a result, it is difficult to capture collective behaviour and correlations among the entities involved using this type of approach. recently, a new demand for querying patterns capturing 'group' or 'common' behaviour among moving entities has emerged. of particular interest has been the development of approaches that can identify groups of moving objects whose members share a strong relationship by being present within a defined spatial region during a given time duration. some examples of these kinds of approaches are moving cluster analyses [cit], convoy queries [cit] ) and flock patterns [cit] . [cit] define moving flock patterns as groups of at least μ entities moving in the same direction while being close to one another during a given time interval δ (figure 1 ). they consider group of trajectories to be close together if there exists a disk with a given radius \" that encloses all of them. the current approach to discover moving flock patterns consists of finding a set of such disks in each time instance and then merging the results from one time instance to another. as consequence, the performance and the number of final patterns depend on both the number of disks and how they are combined."
"at the faculty of geo-information science and earth observation (itc), university of twente, the netherlands, a software implementation of torsten hägerstrand's stc was developed, in order to support research in visualisation and analysis of spatio-temporal and multi-attribute data sets [cit] . the objective was to take full advantage of the capabilities of 3d graphics hardware, that is available on all modern computers, in order to resemble an interactive stc. over the past years, the stc's capabilities were extended with a large quantity of options for visualisation of and interaction with the data, in order to support several usecases. among others, the current version of the cube can visualise spatio-temporal data as space-time paths, stations, taxels, with attribute-dependent size and colour, inline graphs, text and multimedia annotations. the stc has proven to be useful for the visualisation of movement data [cit] and could be extended to show spatiotemporal distribution of the resulting flocks, to verify the flock patterns and to represent the actual trajectories that formed those flocks."
"in the temporal approach, we have fixed the spatial variation and observed the cdr activities for a day, a week and a month. to fix the spatial variation, we observed cdr activities for a specific grid. the simulations results for temporal analysis show that cdr activities are very high in the evening and very low at midnight. the time-series curves for cdr activity are shown in figures 8 and 9 . in figure 8 the temporal variation of cdr activities aggregated on ten minutes interval over a period of one day is shown, whereas the varaition in cdr activities aggregated on one hour interval over a period of one week is shown in figure 9 . we have sum up the values of incoming calls and outgoing calls to quantify the activity level, as represented in equation (3): it is shown in figure 8 that cdr activity is increasing from midnight to morning, then decreasing from noon to afternoon and increasing from evening to mid-night. similar pattern of cdr activities are observed for a day, a week and a month. the cdr activity over a week period is represented in figure 9 ."
"the main purpose of the article was to detect and display flocks instantly by optimising the algorithm and provide users an experience of real-time flock exploration and verification thorough the visual environment. the focus was less on the visualisation functionality that might benefit from such an optimisation. a future direction might include flock visualisation that would involve study of flock composition, recognition of leaders and followers in a particular flock and keeping identity of the same flocks over time."
"figures 4 and 5 reveal that in the first half of the year (december to may), flocks occur mostly south of the equator, in the indian and in the pacific ocean. majority of flocks however occur in the second half of the year, north of the equator in eastern and western pacific and in the western atlantic. especially in the western pacific tropical cyclones tend to follow the same path over the season and form dense flocks ( figure 5) ."
"after the spatiotemporal analysis of preprocessed data, the later step is to classify cdrs according to their activity levels and spatiotemporal characteristics. for cdr activity classification, firstly, clustering analysis is applied for observing the density of clusters. after that artificial neural network is trained based on the clustering results. the overall classification framework is shown in the figure 11 ."
"the method uses fraction values of neighboring pixels to find the sub-pixel location of the fractions inside the pixel under consideration. the neighboring pixels are considered to have an influence attracting the sub-pixels of the same class in neighboring pixels. so, ij sdi can be calculated as follows:"
this article used the fpm approach for discovering moving flock patterns in large spatiotemporal data sets. an extended framework which integrates techniques to identify groups of moving entities and the longest duration flocks patterns has been implemented and tested with synthetic and real data sets.
"using discrete logarithm user u 2 locally retrieve the exponent of decryption results as finally, the prediction for user u 1 on item i 1 is calculated by"
"machine learning's clustering algorithm is adopted for grouping the cdr activities according to their activity level and spatiotemporal characteristics. to identify patterns of variations in cdr activities over the whole city of milan, cdr activities of entire region are passed through clustering scheme. we applied gmm based clustering scheme on our dataset. one motivation for using gmm is the use of a probabilistic approach for cluster assignment in gmm, which is otherwise not found in k-means clustering and thus, k-means fails to perform well in certain scenarios. in gmm, the cluster is formed on the basis of the probability of each point belonging to an associated cluster. gmm is loosely considered as an extension of k-means with enhanced accuracy. the gmm clustering algorithm is summarized in algorithm 1."
"different strategies could be used to limit the size of a trajectory. for example, long periods without change of a position or abrupt jumps in time or location can be used to split long trajectories into shorter segments, without significant loss of spatial information. similarly, depending on the context, it might be acceptable to interpolate trajectories to longer time intervals. for example, it is possible to obtain a suitable iceberg data set with samples taken every week instead of daily intervals."
"in experiment 2, a part of hyperspectral digital imagery collection experiment (hydice) airborne hyperspectral data over the washington, d. c. mall with 192 bands was used, which it comprised 300 lines and 240 columns as shown in figure 5 (a). figure 5 ( as shown in figure 5 and table 2, msmf obtains smoother results with most classes' structural information preserved and has the highest overall accuracy, kappa coefficient, adjusted overall accuracy, and adjusted kappa coefficient, which are 75.22%, 0.6824, 68.50%, and 0.6037, respectively. compared to conventional eas, for example, genetic algorithm, there are two possible key issues for the success of msmf. one is an appropriate balance between global and local search, and other is a cost effective coordination of local search. based on the above results, msmf can obtain better sub-pixel mapping results for the complex hyperspectral remote sensing image with more classes and higher spatial resolution."
"as the dataset is temporarily aggregated on 10 min time interval, but in dataset values at some timestamps are missing. as the cdr activity is happening before and after missing value's time interval. so it is obvious that the cdr activity will be also happening at missing value's time interval. so for handling missing values in the dataset, missing values are replaced with the average values of the previous and next timestamp values. the expression for finding the missing cdrs is shown in equation (1)"
"if the generation, g, does not meet the maximum generation number, max g, go to step 4.3. otherwise, output the best individuals as the sub-pixels of the pixel ij x . the above process is repeated from step 4.1 until the sub-pixels of all pixels in the fraction image are located. finally, the proposed algorithm outputs the sub-pixel mapping image."
"with the mammoth development in mobile and the internet of things (iot) technologies, it can be anticipated that in next generation-networks (ngn) everything will be connected. the number of mobile devices such as smart phones, tablets, and wearable devices are on the rise exponentially. so, the next generation-networks will be considered as worldwide wireless networks (wwwn). according to a report provided by ericsson, in future there will be billions of connected devices [cit] . such a huge connectivity of wireless devices will increase the data traffic volume largely. during the last decade, 4000 fold increase occurred in the data volume and it will increase further in next-generation networks (ngn) [cit] . according to cisco report, wireless traffic is generating 24 exabyte (eb) data per month and it is continuously growing [cit] ."
"we perform the spatiotemporal analysis of cdr data. for a spatiotemporal analysis of cdr data, we also investigate the spatial and temporal correlation for understanding and extracting mobile traffic patterns."
"based on memetic algorithm, this paper proposed an adaptive multi-objective sub-pixel mapping framework, namely msmf, for hyperspectral remote sensing images. the proposed msmf defines two objective function to describe the spatial dependence between pixels and sub-pixels. the sub-pixel mapping problem is transformed to a multi-objective problem. msmf utilizes the memetic algorithm to solve this problem by combining the global search and local search. the experimental result shows that msmf algorithm consistently outperforms the previous sub-pixel mapping algorithms, and hence provides an effective method for remote sensing image sub-pixel mapping."
"according to fig. 2, our proposed scheme is mainly divided into two phases. firstly, we provide privacy-preserving item average computation and privacy-preserving similarity computation as shown in fig. 2a . secondly, we represent privacy-preserving recommendations generation as shown in fig. 2b . in the first phase, users encrypt their ratings and send the ciphertexts to the server. server computes the averages as well as similarity among the items using homomorphic properties and all users collaborate to decrypt these ciphertexts. afterwards, the similarity and averages are stored in server's database. according to fig. 2b, only one user participates to get recommendations ('target user') and sends the ciphertexts of his item preferences to the server. the server computes recommendation scores homomorphically and sends the resultant ciphertexts to the user. the target user finally decrypts the ciphertexts using own private key. two types of secure recommendation scores generation are shown: (1) cbf-based recommendations and (2) cf-based recommendations. table 1 shows the mathematical notations and symbols used in later sections."
"we assume that the target user has requested for recommendations over all items and sent his encrypted ratings to the server (user multiplies his rating with 100 before sending the ciphertexts of ratings). in both types of recommendations, the server generates two different ciphertexts separately [numerator and denominator of eqs. (2) and (3)] and repeats the process for all items. finally, the target user decrypts the results and chooses the item with highest recommendation score. the solutions to generate recommendations securely are described numerically in below."
"besides the synthetic data sets tested above, the proposed framework was also evaluated with trajectories collected from the real world. after some preliminary evaluation, two real data sets from different application domains were selected to test the proposed framework. [cit] . the national ice centre (nic) and brigham young university (byu) microwave earth remote sensing laboratory have used various satellite sensors to manually track large antarctic icebergs and collect their positions. [cit] presented a long-term analysis of the antarctic iceberg activity based on scatterometer and radiometer data. [cit] since they presented the largest amount of records. the second real data set collects movement information from a group of people around the metropolitan area of beijing, china. the data set was collected during the geolife project [cit] by 165 anonymous users who used gps devices over a period of 2 [cit] . locations were recorded by different gps loggers or smartphones, and most of them have a high sampling rate."
"to generate recommendations, one of the key steps is to calculate similarities/correlations among the item pairs. cosine similarity is one of the commonly adopted similarity measures to determine the nearest neighbour in recommendation generation. recalling the notations mentioned previously, the similarity between two items is defined as"
"overall, the fpm approach has made a tremendous progress in the past decade, and it is thought that this can contribute adequately to solve the drawbacks of finding moving flock patterns in large trajectory data sets. therefore the main aim of the remainder of this article is to (i) explore a methodology which allows the identification of moving flock patterns using fpm approach, (ii) compare the performance of the original bfe and the one with the fpm implementation and (iii) embed the improved algorithm into a visual environment and provide an interactive interface for changing the algorithm's parameters."
"to calculate averages ratings of each item, users encrypt their ratings including flags: 1 if there is any ratings, or 0 otherwise using the common public key y. thus, the ratings including which items have been actually rated are secure. all users jointly decrypt the ciphertexts of total ratings and flags [shown in eqs. (15) and (16)] without revealing and individual's ratings. since the server is semi-trusted, it does not collude to reveal user ratings. 2. security in similarity calculation users first locally compute pairwise products and square of items' ratings. then, they encrypt these results using common public key y and send to server. therefore, user ratings are secured. once the server receives the ciphertexts, it homomorphically computes the similarities and allows all users to jointly decrypt the results [eqs. (26) and (27)]. being semi-trusted, the server does not pose any threat to user ratings."
"the performance analysis of our proposed model is conducted in two parts. we first analyse our method in-terms of computation and communication costs which infer the efficiency in privacy and secondly we analyse the method in-terms of recommendation accuracy. to conduct the experiment, we use java 2 se 8 platform with os windows 7, 64 bit and 3.6 ghz-core i7, 8gb cpu unit. java cryptographic-based libraries are also used for our experiment. the proposed method is evaluated using publicly available data provided by grouplens [cit] which consists of 100,000 [cit] items provided by 943 users on a scale of 1-5. in our experiments, we choose 200 items and assign 943 users who have rated on those items. therefore, the performance analysis of our model consists of 943 users and 200 items. once the items' averages and similarities are calculated, one user (''target user'') is randomly assigned for recommendations generation."
"in the initial step, each individual t ab in msmf represents the possible sub-pixel configuration of the pixel and is directly described by a string consisting of integer numbers, which the length of each antibody string is equal to the number of subpixels."
"the use of static values of \" will inevitably generate groups which 'share' trajectories due to the overlapping of disks. this has a negative impact introducing duplicate patterns. formal definitions of flocks set \" as a fixed parameter [cit] . however, natural behaviour of moving objects show that groups increase and decrease their members, as well as the extent of the space which they occupy, over time. for example, vehicles move in constrained space with variable speed, which affects the size and shape of the resulting flocks. it seems reasonable to think that flexible shapes and values for \" will model the interaction among moving entities in a more intelligent way."
"the server broadcasts the message m ð2þ containing a portion of the ciphertexts for all items to all users by finally, the server decrypts the ciphertexts of ratings and flags as"
"based on the above, lcm [cit] ) was chosen since it demonstrated a remarkable efficiency using extremely low values of support in sparse data sets. lcm is a backtracking (or depth-first) algorithm based on recursive calls. [cit] . lcm has two variants of the program available; lcm_max and lcm_closed will retrieve the maximal or closed set of frequent patterns, respectively. in our case, the output m is a plain text file, where each line is a maximal pattern that contains a set of disk ids separated by spaces."
"similar to the cbf-based recommendation process, the proposed private cf-based method 3 also works in three main steps, which include sending target user's encrypted ratings to server, performing homomorphic operations as well as encryption by the server and finally, determining the final recommendations by the target user. the detailed steps are as follows."
"from last few years, the network traces are largely exploited for understanding and modeling the network dynamics. [cit] described that log normal distribution efficiently approximated the network traffic density. [cit] also performed the spatiotemporal analysis of cellular traces and showed that the cellular traces follow trimodal distribution. in addition, [cit] analyzed network traces for anomaly detection and traffic prediction for cellular networks."
"to generate cbf-based recommendations, the protocol is divided into three main steps. firstly, the target user encrypts his personal rating vector and sends the ciphertexts to server. secondly, the server performs homomorphic operations and encryption to generate the ciphertexts of numerator and denominator of eq. (2) (cbf-based method), respectively. finally, the user receives these ciphertexts from server and decrypts them to determine recommendation scores. the detailed steps are described below."
"the cdrs of five days is used for training and testing of c-ann model. previously, data (cdrs) is temporally aggregated on one hour time interval, for ease of processing and reducing data overhead. cdr data is also temporally aggregated for three hours time interval for the whole city of milan. for training of c-ann, we have used the cdr data temporally aggregated on three hours interval. in this way, there are total eight-time slots for one day. as milan city is spatially divided into 10,000 grids, so there are 80,000 samples for one day and for five days activities, there is a total of 0.4 million samples. from these samples, 80% samples are used for training and 20% samples for testing of the c-ann model. though, it is the simplest approach but outperforms for cdrs activity classification."
"we analyze the cdr data of large scale cellular network. the dataset contains the cdr activity for milan city. such a vast dataset helps us to understand and model the network traffic for urban, suburban and rural areas."
"the elgamal encryption scheme is a homomorphic and probabilistic public key encryption based on diffie-hellman key exchange. the elgamal encryption scheme can be defined over any cyclic group g. its security depends upon the difficulty of a certain problem in g related to computing discrete logarithms. the elgamal encryption scheme consists of three algorithms: the key generation, the encryption algorithm, and the decryption algorithm."
"due to the increasing availability of spatial-temporal databases, many different methodologies have been explored in the search for meaningful information hidden in this kind of data. understanding of how diverse entities move in a spatial context has demonstrated their usefulness in topics as diverse as sports [cit], socio-economic geography [cit], animal migration [cit] ) and security and surveillance [cit] ."
"the remainder of this paper is organized as follows. section 2 provides the mathematical formulation of the sub-pixel mapping algorithm, and section 3 deals with the multi-objective memetic algorithm. section 4 presents the proposed adaptive multi-objective sub-pixel mapping framework based on memetic algorithm for remote sensing imagery, namely msmf. in section 5, the experimental results are provided. finally, the conclusion is given in section 6."
"the rest of the paper is organized as follows: in section 2, we present the related work. section 3 presents the system model and description of the dataset. section 4 introduces the reader to the underlying dynamic nature and spatio-temporal characteristics of the cdr data at hand, which then forms the basis for network activity classification and prediction using these spatio-temporal characteristics. in section 5, we describe the proposed clustering based classification model. a generic data-driven resource allocation/management approach for cellular network is presented in section 6. finally, we conclude our work in section 7."
the analysis of large spatio-temporal data sets presents significant challenges for data mining in general. fpm techniques have been shown to have important contributions to make in this area. combining these with the visual interface of the stc within ilwis allows users to interactively explore large spatio-temporal data sets in search for flocks. this functionality represents a contribution to the field of geovisual analytics that is presently unavailable elsewhere.
"modern data acquisition techniques such as global positioning system (gps), radiofrequency identification (rfid) and mobile phones have resulted in the collection of massive amounts of movement data in recent years. increasing popularity of these technologies and the ubiquity of mobile devices imply that this volume of spatio-temporal data being collected will increase at accelerated rates in the future."
"in the spatial approach, we have fixed the temporal variation. cdrs activities at a specific time period are observed over the entire city of milan. for this purpose, the data of a specific day is taken. after data preparation step, the data is further aggregated on the three-hour interval. hence, we have data at eight different time slots for an overall period of 24 h. in this way, cdr activity of the entire milan city is observed for a day."
"in parallel with developments in the fields of spatial databases and computational geometry, some areas of traditional data mining have also focused on discovering frequent patterns in general attribute data. association rule learning and frequent pattern mining (fpm) [cit] are popular and well-researched methods for discovering interesting relations between variables in large databases. these approaches were initially developed to solve a specific task in the commerce sector. frequent patterns are itemsets, subsequences or substructures that can be said to be present in a data set if their frequency exceeds a userspecified threshold [cit] . it is thought that approaches to find frequent patterns might also contribute to find moving flock patterns, for example, through the efficient handling of candidates and combinations [cit] . another issue of finding flock patterns involves verification and interpretation of those patterns. it is a difficult process and depends on both the characteristics of the object under study and the parameters of the algorithms. the objects might be animals, pedestrians, vehicles or natural phenomena such as hurricanes. the significance of how the objects interact with one another and how they move together will depend on the established knowledge about the objects or phenomena, and accurate interpretation often requires expert knowledge [cit] . but even within the same application domain, flock patterns are context dependent, thus requiring interactive exploration of the input parameters to the algorithm in real time."
"in this example, we show how to compute average of user ratings on item i 1, denoted as r 1 . firstly, users u 1, u 2 and u 3 send their ratings and the flags for item i 1 as follows"
"once the server receives these messages of ratings and flags, it starts to compute products of the ciphertexts (sum of the ratings in plaintexts) by following."
"as shown in figure 2, one major difference between mas and conventional eas is that the local search operator is added in addition to the evolutionary operators. hence, the success of mas is largely due to the appropriate adoption of local search operators. unlike the evolutionary operators, which are usually very general and applicable to various problems, momas provide the general framework, which the global search and the local search operators are usually expected to incorporate some domain specific heuristics, so that mas can balance well between generality and problem specificity. in this paper, artificial immune algorithm is selected as the global search algorithm."
"the aforementioned techniques have been applied successfully to diverse scenarios such as bioinformatics [cit], gis [cit] [cit] . [cit] for a comprehensive survey of the fpm approach. additionally, the frequent itemset mining implementation (fimi) repository [cit] ) presents a collection of open source implementations for the most efficient and scalable frequent/closed/maximal pattern mining algorithms."
"1. an efficient privacy-preserving item-based recommender system to protect user privacy during recommendation process. 2. privacy-preserving item average and similarity computation protocols to calculate averages and similarities among the items without compromising user ratings. moreover, which items have been rated are also hidden during these processes. 3. two different types of solutions to generate recommendations securely: cbf and cf-based recommendations."
"step 4: mutate all these copies with a rate proportional to their affinity with the input pattern: the higher the affinity, the smaller the mutation rate;"
"let us suppose the linear pixel unmixing model yields the fraction images for c land cover classes and the coarse resolution pixels are to be divided into d sub-pixels. to construct the mathematical model, the attribute of each subpixel can be defined by ij x as follows"
"the resulting ciphertext is the encryption of m 1 m 2 which is a multiplication of two plaintexts. it has been shown that elgamal is semantically secure, i.e. it is computationally infeasible to distinguish between the encryptions of any two given messages, if the decisional diffie-hellman problem is intractable [cit] ."
"remark 1 since all users jointly decrypt the sum of ratings for each item as well as the number of users who have actually rated on that item, the server or any users pose no threats to any user's private key, personal ratings and flags."
"where t is the total number of time steps in the time series andŷ is the average value of time series data. figure 10 represents the autocorrelation function for 168 lags. 168 lags represent the temporal dependencies of cdrs for a week period with a step size of one hour. from figure 10, it is observed that there is exist an hourly pattern in cdrs because autocorrelation function plot have peaks after every 24 lags. so, from the temporal analysis, the temporal feature of cdr activities can be extracted. temporally, cdr activities can be divided into different domains such as cdr activities during off-peak hours, on-peak hours and normal peak hours."
step 5: add these mutated individuals to the population m and reselect m of these maturated individuals to be kept as memories of the systems;
"mobile network traces provide extensive information about the network and its subscribers' behavior. however, the analysis and modeling of network traces is a non-trivial task [cit] . the existing solutions for understanding and modeling of network traces and its pattern provide limited information. as the key factors affecting the network traffic variations are still needed further research. such limitations, increase the operation cost of the cellular network and degrade the quality of service especially in densely populated areas."
"it is observed that the spatial correlation is high between the targeted cell and the cells near to target cell and spatial correlation is low between the target cell and the cells far away from the target cells. hence, the spatial correlation is dependent on the distances between cells. on the other hand, another interesting fact is observed from figure 7, as the cell (2,3) and (4, 3) are at the same distance from the target cell (3,3) but spatial correlation differs a lot. this phenomenon shows that spatial correlation is also location dependent. hence, from the spatial analysis of cdr activities, the spatial feature can be extracted. the cdr activities of entire milan region are divided into different domains such as cdr activities from a high-interest area, medium interest area, and low-interest area."
"the mixed pixel is a common phenomenon in remote sensing image because the sensor's instantaneous field-of-view (ifov) often includes more than one land cover class on the ground [cit] . the land-cover classification accuracy may be severely compromised by the presence of mixed pixels. although spectral unmixing technique [cit] and fuzzy classifiers can obtain the fraction image indicating the percentage of each class in the mixed pixel, they do not provide any indication about the sub-pixel spatial distribution of the classes within the coarse pixel. sub-pixel mapping techniques can be used to specify the sub-pixel spatial distribution of each class, based on the fraction image, by dividing pixels into smaller sub-pixels based on the spatial dependence phenomenon [cit] in which observations close together are more alike than those further apart. the key problem of sub-pixel mapping is determining the most likely locations of the fractions of each land cover class within the pixel [cit] . different potential techniques have been proposed for sub-pixel mapping based on the assumption of the spatial dependence, such as sub-pixel mapping algorithms based on direct neighboring algorithm (dnsm), spatial attraction model (sasm) [cit], bp neural network (bpsm) [cit], linear optimization techniques [cit], genetic algorithm (gasm) [cit] . these current sub-pixel mapping algorithms may obtain the classification result with a finer resolution, but they only consider the spatial dependence at the pixel level by utilizing the neighbour pixels of the mixed pixel. these sub-pixel mapping algorithms with calculation at pixel level ignore the spatial dependence of each sub-pixel and they can be further improved by using the spatial information at sub-pixel level."
"to illustrate the functionality of the visual environment we have chosen the real-world data set of tropical cyclones. the data set contains 30 [cit], containing 2759 storm tracks with six hourly observations (national climatic data center (ncdc), [cit] ). a typical scenario when analysing this data set would be to find the similarity between the tropical cyclones paths. since tropical cyclones form over the warmer oceans, these flocks would be useful for answering the question if similarly moving cyclones also share similarity in the environmental factors that influence them?"
"although it was easy to convert the improved flocking algorithm the stc plugin of ilwis, the resulting visualisation might not be the best that is suited for tasks of verifying flocks. more rigorous evaluation is required, preferably in several knowledge domains in order to claim the usefulness. nevertheless, a combination of the ability to handle large data sets with the possibility to change the input parameters and visually inspect the resulting flocks in real time, at the best of our knowledge, has not been currently implemented or researched."
"in this example, we show how to compute similarity between item i 1 and i 2 securely. firstly, all users compute product of pairwise ratings and square of each individual rating locally, thereby encrypt the results. for instance, all users send the message m ð4þ i containing the ciphertexts for item i 1 and i 2 to the server as follows:"
"in this section, we discuss the framework of the proposed system model and describe the description of the observed dataset. in proposed architecture, the area of the city (milan) is divided into four subregions namely, high activity area, medium activity area, low activity area and very low activity area. this division is based on the cdr activity level of the mobile phone's subscribers. the high activity area mainly consists of the city center where many shopping malls, buildings, hospitals, and other busy centers are located. the areas in the surroundings of the city center are considered to be located in the medium activity area. similarly, the areas far away from the city center are low activity and very low activity areas. figure 1 represents the overall framework. the overall system model presented in our work comprises of three stages: (i) data preparation and analysis step, (ii) clustering analysis step and (iii) cdrs classification step. figure 2 represents the layered structure of the proposed model. in the proposed model, firstly cdr data is collected and stored. the available data is spatiotemporal, so spatial and temporal analysis are performed for observing the spatial and temporal dependencies. then, from clustering analysis, the cdr activities are categorized into different groups according to spatiotemporal variation and the entire region is divided into four different sub-regions. after the clustering analysis, an artificial neural network (ann) is trained based on the clustering results for classification of future network traffic (cdrs). the clustering analysis is performed prior to the training of ann for transforming the problem from unsupervised to a supervised multi-class classification problem. the proposed model is named as clustering based ann (c-ann) model. the overall system model is represented in figure 3 ."
"from the useful insights of spatiotemporal analysis of network traces (cdrs) and traffic classification framework, we have proposed an approach for optimizing network resources. in the proposed approach, cluster set g is divided into k clusters, each cluster is then allocated network resources according to cdr activity level. when cells in the cluster set are assigned to different clusters (according to the respective cdr activity level), a new grid set of optimized cells c t will arise. as in our proposed approach optimize a network grid after every h hours (3 h in our case) through an offline process that collects cdr data and calculates the spatial and temporal feature sets f of cdr activity data for last h hours. hence, resources are allocated to grids according to their cluster membership. the proposed approach is summarized in in algorithm 2. allocate network resources to each cell in g according to cluster membership and assign it to optimized set c t 5 end 6 return c t figure 17 further demonstrates the utilization of overall framework. figure 17 shows that first of all cdrs are collected. as the obtained cdrs contain a large amount of data, so data preprocessing is performed prior to analyzing these records. after cdr preprocessing step, the data is in ready-to-use format and is sent to analysis phase. from cdr analysis step, the spatial and temporal insights within the data are extracted. after spatiotemporal analysis, the cdrs are sent to traffic optimization phase where the records are classified according to the activity level and spatiotemporal characteristics. in the proposed optimization framework, cdr collection and traffic optimization are online processes that directly interact with the network infrastructure continuously. while cdr preprocessing and analysis are offline processes that start a certain time period (a parameter decided based on density and activity levels in the cluster), apply proposed technique on cdr data from the inactivity period, predict network activity for the next quantum and suggest fresh network parameters to the traffic optimizer. thus, the proposed method uses unsupervised machine learning to extract spatiotemporal features to prepare training data set for the supervised learning algorithm that actually tags the activity levels based on spatial and temporal features."
"the proposed framework is now able to handle large data sets and efficiently find flock patterns. in order to verify those flocks, the visual interface should also be able to deal with the visualisation of these large data sets. [cit] offer comprehensive review of the most common ways to visualise the trajectory data and spatio-temporal events. the established visualisation techniques are animated map and space-time cube (stc) [cit] ."
"the obtained dataset was in raw form, so the dataset has the different type of impurities and irregularities. such irregularities include missing values, unrecognized numbers, and misleading patterns. so data must be prepared before performing the analytical step. data preparation is helpful in improving output quality and reducing processing overhead."
"the useful insights obtained from the network traffic analysis are highly valuable for network operators as well as network subscribers mainly in the case of ultra-dense networks [cit] . if the cellular network traffic patterns are accurately predicted and modeled, then the network operators"
"as shown in figure 4 table 1 using the overall accuracy (oa), kappa coefficient (kappa). in addition, an adjusted confusion matrix was utilized, which it is calculated only for mixed pixels. based on the adjusted confusion matrix, we adjust oa (oa*), kappa coefficient (kappa*), the average of the producer's accuracy (apa), and the average of the user's accuracy (aua). as with the visual results, the sub-pixel mapping accuracy of dnsm and bpsm are lower than that of the other algorithms. for instance, one can see from"
"according to the initial antibody population, the affinity of all m ab's in the antibody population ab are calculated using the criterion function 1 ( ) f ab and 2 ( ) f ab using equation (3) and (5)."
"based on the multi-objective sub-pixel mapping problem, the adaptive multi-objective sub-pixel mapping framework based on memetic algorithm, namely msmf, is proposed. in contrast to the previous single-objective optimization for sub-pixel mapping, such as genetic sub-pixel mapping algorithm (gasm) [cit], msmf has no single global solution, and it is often necessary to determine a set of points that all fit a predetermined definition for an optimum using the concept of pareto optimality [cit] . as shown in figure 3, msmf provides the framework, and the evolutionary algorithms and local search may be selected using different algorithms according to the real problem. in this paper, clonal selection algorithm and random swapping algorithm are selected. the msmf is completed according to the following steps."
"in this paper, to consider the spatial dependence between pixels and sub-pixels together, a new search strategy inspired by multi-objective memetic algorithm [cit], namely multi-objective memetic sub-pixel mapping framework (msmf), is proposed. in msmf, the sub-pixel mapping problem becomes a multi-objective optimization problem, which one of objective function using spatial dependence index (sdi) describes the spatial dependence between pixels [cit], and another function with moran's i considers the spatial dependence between sub-pixels [cit] . memetic algorithm [cit], a union of a population-based global search and local improvements, provides a power and effective algorithm for multi-objective optimization problem. msmf utilizes memetic algorithm to construct a multi-objective sub-pixel mapping optimization framework with global and local search. in this opening framework, global search may be carried out using different evolutionary algorithms, such as artificial immune systems [cit], differential evolution [cit] . local search for sub-pixel mapping may be designed by random algorithm or pixel swapping algorithm. in this paper, msmf utilizes artificial immune systems for global search, and random algorithm for local search to perform the task of the sub-pixel mapping for hyperspectral remote sensing imagery as follows. (1) in msmf, each candidate individual represents the possible sub-pixel configuration of the pixel, and pareto dominated individuals based upon multi-objective function evaluations are removed by pareto ranking. (2) evolutionary operations are performed to generate new individuals by the adaptive evolution operators, such as clonal, selection, mutation. these operators can draw the evolutionary process closer to the goal, i.e. the optimal sub-pixel distribution and these parameters can be adaptively calculated without userdefined. for example, the mutation rate is determined according to the value of objective function, which better individual will have smaller mutation rate. (3) after evolution, pareto dominated and infeasible individuals are removed and good individuals are retained for the next generation by using ranking and elitism selection. (4) in addition, to avoid stagnation in global search, msmf explores the local neighbourhood regions in objective space to find a more feasible solution. the proposed method was tested using the synthetic and degraded real imagery, and experimental results demonstrate that the proposed approach has better results."
"spatial clustering algorithms are an option then. these work by discovering set of clusters (instead of disks) of arbitrary shape and size. dbscan, swarm intelligence techniques [cit] and grid-based methods [cit] ) are alternatives to be considered in order to avoid the use of disks with a predefined radius."
"in cbf, the recommendations are generated based on the items' features. the process is to check for similarity among the items which is calculated using item features first, and then, based on those similarity, the cbf generates recommendations for the target user. the equation for predicting the recommendation using cbf is:"
"in order to be able to work on large data sets and alter the input parameters of the algorithm in real time, the flocking algorithm should be efficient and scalable. so far, results imply that sample size was either too small to test the scalability of the algorithm or relatively high computational complexity and long response times. therefore, the aim of this article is twofold: (i) to improve the algorithm for mining flock patterns by using a frequent pattern discovery approach and (ii) to provide visual functionality and real-time manipulation of the input parameters of the improved algorithm for verification and interpretation by a domain expert."
"the server finally determine the plaintexts by computing discrete logarithm as, after extracting the plaintexts from above equations, the server computes the similarity between two items i j and i k using eq. (1) as follows."
"the knowledge extracted from spatiotemporal analysis help in predicting the resource requirements at different geographical locations at a specific time perioid. in addition, the mobility pattern of the user is also predcited. for example, in events with big gatherings (say, a soccer world cup match), huge crowd is typically attracted to the venue, which results in network congestion at the particular location for a specific period of time. hence, with this predicted insights obtained from spatiotemporal analytics, network operators deploy extra resources at the region of interest, which help in avoiding network congestion."
"the target user may send the indices of items for which he has not provided any ratings. in this case, the server does not consider those items while computing the ciphertexts of recommendations."
"in statistics, moran's i is a measure of spatial autocorrelation developed by moran [cit] . spatial autocorrelation is characterized by a correlation in a signal among nearby locations in space. so, in this paper, moran's i is used to estimate the spatial dependence between sub-pixels as follows. so, sub-pixel mapping can be formulated a multi-objective optimal problem, while maximizing the spatial dependence index (sdi) and moran's i, simultaneity using the following equations from equation (1)."
"the maximal (or 'closed') sets of frequent patterns avoids the need to set a parameter δ to limit the duration of the flock patterns. in the proposed framework, the parameter δ is used only to set the minimum duration allowed, but flocks with the longest duration will be reported. in contrast, bfe used δ to report flocks with that specific time duration in order to minimise the number of intermediate flocks to be combined. as a result, the final number of flocks reported by the proposed framework is significantly smaller than the number of flocks reported by bfe."
"in conventional scenarios, resources deployment were not optimized due to lack of advancements in data analytics tool. with the useful insights obtained from the spatiotemporal analysis of network traces, cellular network operators are able to make intelligent decisions which aid in network optimization. the extracted information is also helpful in understanding and predicting future trends of the network traces [cit] ."
"initial tests with synthetic data sets showed a high performance of the proposed approach with respect to the traditional method. however under tests with the icebergs data set that difference disappeared. [cit] that not only the length of the involved transactions but also the length of the resulting patterns have a direct impact in the performance of frequent pattern techniques. the results from the experiments reveal that the shorter the trajectory size, the better the performance of the proposed framework."
"the full data set collects more than 1.2 million of point locations and 23,800 trajectories. however, this data set tracks relatively few moving entities (165 users) in a long time window (more than 2 years) with little temporal overlap. therefore it was decided to create a modified version by having all of them start at the same time. again, due to the memory constrains, the trajectories shorter than 10 minutes and longer than 3 hours were pruned. the alternative data set stores 760,814 point locations and 18,216 real trajectories happening together. table 3 summarises the details for the iceberg data set and for both the original and alternative beijing data sets."
"the framework contends that a moving flock pattern can be generalised as a typical frequent pattern. it works by converting a trajectory data set into a transactional database, based on the locations visited by each trajectory. once a transactional version of the data set is available, fpm algorithms can be applied."
"it is observed, from the spatiotemporal analysis performed in section iv, that network traces have time-varying as well as space varying characteristics. these spatiotemporal characteristics are beneficial for deploying as well as optimizing the cellular network."
calculate the affinity * 1 ( ) f ab and * 2 ( ) f ab using equation (9) and (5) of the matured clones c * .
"the main disadvantages of perturbation and differential privacy-based methods are as follows. firstly, they suffer from poor quality in selecting the neighbours (similar users or items) due to inducing large noise. secondly, they are not highly guaranteed in-terms of providing rigorous security. they also suffer from a trade-off between privacy and recommendation accuracy. additionally, the existing homomorphic-based approaches experience high computational costs [cit] ."
"users have options of manipulating the stc, sliding the temporal plane, switching between 2d (map) and 3d (stc) representations and changing the input parameters for finding flocks. it is implemented in such a way that altering the μ, δ values almost instantly changes the number of resulting flocks on the display ( figure 5 ). this is because the most computationally intensive part of the algorithm does not have to be repeated when changing those values. delays in visualisation of the resulting flocks can be observed when increasing the \" value (e.g., changing \" from 300 to 600 km in these data sets introduces delay of about 1-5 seconds depending on the system configuration since the algorithm has to start from the beginning)."
"the fpm approach showed to be useful to deal with the problems found in the bfe algorithm for data sets with a large numbers of trajectories. the proposed framework is shown to handle the disk combination problem efficiently by using scalable frequent pattern algorithms. additionally, the flocks which are discovered are 'cleaner' since they disregard spurious flocks, while maximal pattern mining techniques are able to detect the longest duration flocks."
"(a) cbf-based recommendations to generate recommendations in cbf, target user encrypts item preferences using own public key y i and sends them to the server. the server homomorphically generates ciphertexts of recommendations leveraging the item-item similarity, which is already available to it, thereby sends the ciphertexts to target user. while generating recommendations, the similarities among the items are encrypted using target user's public key y i . the ciphertexts are decrypted by the user's own secret key x i . therefore, during this process target user's personal ratings and recommendations results are not revealed and thus secure. (b) cf-based recommendations similar to the cbf-based, cf-based process generates recommendations using ciphertexts of user's ratings and items' similarities except one additional operation: subtracting item's average from corresponding item's rating. in this case, the item's rating is already encrypted by the user and average is stored in server in plaintexts format. to overcome this situation, server encrypts the average rating using target user's public key y i and performs this subtraction homomorphically."
"where k w is the weight of each neighboring pixel, n is the number of neighboring pixels, k fraction is the fraction of the kth neighboring pixel for the i-th land cover class. thus, the sub-pixel mapping problem of d sub-pixels with c land cover classes can be suitably formulated as an optimization problem to find the global maximum of the criterion function z (see also equation (3))."
"after receiving antibody individuals closer to the solution, the next generation should mainly be derived from the betterfitting individuals. thus, the n selected ab's are cloned based on their antigenic affinities, generating the clone set c. the total number of clones-generated n c is defined as follows:"
"this article elaborates the potential benefits of cdr data analytics in mobile networks. our work provides an effective spatiotemporal analysis approach to understand and monitor the mobile traffic patterns of large scale city. experimental results show that the useful insights extracted from cdr data would be very helpful in optimizing mobile network resources. these findings help to understand the spatial and temporal dynamics of the traffic pattern in a comprehensive way. the framework for optimizing network traffic is built on the basis of insights obtained from cdr data analytics. thus, we believe our classification framework for mobile traffic partitioning is enhanced network performance and qos requirements."
"it is not necessary to have a complete set of all frequent patterns. only the set of maximal frequent patterns will suffice, since only the longest flock patterns are reported."
"it is observed that the cdrs activities are not distributed uniformly over the city of milan because of different dynamics of milan sub-regions such as commercial, residential, rural etc. a popular and widely adapted parameter, pearson correlation coefficient r is used for measuring the variation in spatial correlation [cit] . the variation in spatial correlation among the targeted grids, for example, the grids in the center of the city, and grids in the surrounding of the targeted grids is calculated with pearson correlation coefficient. equation (2) represents the mathematical expression of pearson correlation coefficient."
where r is an arbitrary random number chosen by the encrypter. c 1 and c 2 are represented as ciphertexts. decryption the ciphertexts are decrypted by computing with the private key x as follows.
"our contributions in this paper, we propose a new privacy-preserving recommender system, which allows the computations required for recommendations in a distributed manner and preserves user privacy without compromising recommendation accuracy and efficiency. we introduce the privacy protocol by elgamal encryption which is based on public key cryptosystem. the main advantage of this cryptosystem is that it is semantically secure and allows certain types of computations on the ciphertexts. we assume a semi-trusted server named ''recommender server'' whose task is to perform the computations for recommendation on encrypted data. we propose different privacy protocols for item average and similarity computations as well as recommendations generations by which the privacy of users is preserved. specifically, our main contributions are:"
"initialisation: gmm based clustering analysis on activity data results in four different clusters categorized in different activity-types, as very high activity, high activity, medium activity, and very low activity cluster. figure 12 represents the gmm based clustering approach at a specific time instant, different clusters are shown with different colors. from figure 12, it is observed that the cluster form in the center of the region, consists of very high cdr activities. so we named it as very high activity cluster. similarly, the cluster form near the center of the region also consists of high cdr activities but lower than the central cluster activities. on the other hand clusters further away from central region consists of low cdr activities. hence, based on the results of clustering analysis, the entire is divided into different sub-regions/groups. for understanding the spatiotemporal as well as the dynamic nature of the source (mobile users), the clustering results at different time instants of a day are shown in figure 13 ."
"second, we acquired the interaction profile of each mirna d p according to the semantic similarity between d p and its k nearest known diseases as follows:"
"overgeneralization (lines 30-32): finally, the noisy channel considers the possibility of producing overgeneralized word forms (like \"maked\" and \"childs\") in place of their correct irregular forms. the overgen function produces the incorrect overgeneralized form. we draw from a distribution which chooses between this form and the correct original word. our model maintains separate distributions for nouns (overgeneralized plurals) and verbs (overgeneralized past tense)."
"the availability of child language is also key to the design of computational models of language learning [cit], which can support the plausibility of proposed human strategies for tasks like semantic role labeling [cit] or word learning [cit] . to our knowledge this paper is the first work on error correction in the first language learning domain. previous work has employed a classifier-based approach to identify speech errors indicative of language disorders in children [cit] ."
"the model allows us to chart aspects of language development over time, without the need for additional human annotation. our experiments show that children share common developmental stages in language learning, while pointing to child-specific subtleties in preposition use."
"all distributions receive an asymmetric dirichlet prior which favors retention of the adult word. with the sole exception of word insertions, the distributions are parameterized and learned during training. our model consists of 217 multinomial distributions, with 6,718 free parameters."
"however, choosing the closest node from the message hosting node as the next routing hop may not necessarily be the best choice. we will use an example to show it in section iii."
"a. gdv utility function 1) definition of gdv-the utility function is designed to compute a utility value for each routing entry in the peernodelist by taking into account of the link latency and the geo-distance from the next hop to the destination. the utility value refers to the qualification of entry node in the peernodelist to be a forwarding node for a message with source node s and destination node d. we define the gdv utility function by introducing a tunable influence parameter fj, ranging from 0 to 1, to adjust the importance of geo-distance factor, denoted by fdis, and latency factor, denoted by fla."
"different from predictive quantization, scalar quantization performance for correlated processes can also be improved by our previous work [cit] which leaves the encoder unchanged and employs a predictor only at the decoder side. in this approach, the signal is predicted depending on the previously reconstructed samples in the decoder. at each time instant, according to the fixed encoder decision levels and an instantaneously shifted decoder-sided prediction error probability density function (pdf), a time-variant quantization codebook can be computed based on the centroid condition. this approach leads to significant improvements especially for low rate quantization and highly correlated processes. moreover, it can advantageously be applied both in error-free and error-prone transmission conditions, either using hard decisions or soft decisions [cit] ."
the language model provides a prior distribution over adult form sentences. we build a a trigram language model fst with kneser-ney smoothing using opengrm [cit] . the language model is trained on all parent speech in the childes studies from which our errorful sentences are drawn.
"tree maintains the information about its parent node and children nodes periodically, the node exchanges heartbeat messages with those nodes and updates the information about their state. to reduce the overhead introduced by multicast maintenance, the nodes' update information is piggybacked in heartbeat messages used by shortcut maintenance or in data messages transmitted among nodes."
"therefore, two diseases would likely have greater similarities if they share a larger part of their dags, and we can calculate semantic similarities between disease d i and d j as follows:"
"pns selects routing table entries for each node from the closest nodes in the underlying network that meet the needs of the overlay routing. when message arrives, the host node routes the message to the node that is numerically closest to the"
"the deletion case requires no decision after action selection. in the insertion case, the class of the inserted word, c, is selected conditioned on c prev, the class of the previous adult word. the precise identity of the inserted word is then drawn from a uniform distribution over words in class c . it is important to note that in the insertion case, the input word at a given iteration will be re-processed at the next iteration (lines 33-35)."
"besides smoothing our estimated distributions, the pseudo-counts given by our asymmetric dirichlet priors favor multinomials that retain the adult word form (swaps, identical lemmas, and identical inflections). concretely, we use pseudo-counts of .5 for these favored outcomes, and pseudo-counts of .01 for all others. 6 in practice, 109 of the child sentences in our data set cannot be translated into a corresponding adult version using our model. this is due to a range of rare phenomena like rephrasing, lexical word swaps and word-order errors. in these cases, the composed fst has no valid paths from start to finish and the sentence is removed from training. we run em for 100 iterations, at which time the log likelihood of all sentences generally converges to within .01."
"to provide the grammatically correct counterpart to child data, our errorful sentences were corrected by workers on amazon's mechanical turk web service. given a child utterance and its surrounding conversational context, annotators were instructed to translate the child utterance into adult-like english. we limited eligible workers to native english speakers residing in the us. we also required annotators to follow a brief tutorial in which they practice correcting sample utterances according to our guidelines. these guidelines instructed workers to minimally alter sentences to be grammatically consistent with a conversation or written letter, without altering underlying meaning. annotators were evaluated on a worker-by-worker basis and rejected in the rare case that they ignored our guidelines. accepted workers were paid 7 cents for correcting each set of 5 sentences. to achieve a consistent judgment, we posted each set of sentences for correction by 7 different annotators. once multiple reference translations were obtained we selected a single best correction by plurality, arbitrating ties as necessary. there were several cases in which corrections obtained by plurality decision did not perfectly follow instructions. these were manually corrected. both the raw translations provided by individual annotators as well as the curated final adult forms are provided online as part of our data set. 3 resulting pairs of errorful child sentences and their adult-like corrections were split into 73% training, 7% development and 20% test data, which we use to build, tune and evaluate our grammar correction system. in the final test phase, development data is included in the training set."
"this example also shows that neither the routing path with the shortest routing length in hop counts nor the routing path with each hop optimized by the shortest link latency is the best route in terms of routing efficiency. thus, it is a necessity of having an efficient way to combine the shortcut and latency together for the purpose of optimizing the routing performance. to this end, our utility-driven routing (udr) protocol is proposed."
"in recent years, as one of the common methods of recommendation systems, nonnegative matrix factorization (nmf) has been widely used as an effective prediction algorithm in the field of bioinformatics [cit] . two non-negative matrices w and h, which are optimal approximations to the original matrix y, can be found by nmf, where w and h satisfy equation (20) ."
"our model is based on a noisy-channel assumption, allowing for the deletion and corruption of individual words, and is trained using fst techniques. despite the debatable cognitive plausibility of our setup, our results demonstrate that our model captures many standard divergences and reduces the average error of child sentences by approximately 20%, with high performance on specific frequently occurring error types."
"inspired by this, we define the parameter fl by using the following equation, aiming to provide an efficient way for nodes to adjust the setting of fl based on their local knowledge about the overlay network."
"through a comparison, the shortcut with the lowest probability is selected to represent its cluster and the others with higher probability are discarded. for two nodes with same probability, the node that has higher capacity is elected as the respective of the cluster."
two optimization techniques are discussed in section 5 to further enhance the performance of our routing scheme. we evaluate the effectiveness of the proposed approach through simulation based experiments in section 6 and summarize the contributions of the paper in section 7.
"then the notification messages are sent out from split node to notify of the arrival of new node. in such a way, the new node can be included in peernodelist of other nodes. readers may refer to our technical report [cit] for detailed construction process and examples of node arrival and departure."
"once a node p wants to route a message to the node with the given destination coordinates, it first checks if the coordinates are contained by the region it owns. if not, it looks up the routing nodes in its peernodelist and chooses the node with the shortest distance to the destination as its next hop to routes the message. this routing process repeats until the message reaches its destination."
"we now compare the efficiency of three multicast construc tion schemes: geocast with nb, geocast with sgd routing and geocast based on udr with random setting."
is calculated upon each speech file considering all samples: the final result is obtained by the mean of the 96 (linear) gp values and thereafter transformed into the db domain.
"in this section, we describe the design of our utility-driven routing (udr) protocol. we first give a design overview of the utility function, then discuss how to utilize this utility function to set up the best message delivery path in terms of short path length and short path latency. for reference convenience, we refer to the version of geocast powered by utility driven routing as the enhanced geocast in the rest of the paper."
"action selection (lines 3-7): on reading an input word, an action category a is selected from a probability distribution conditioned on the input word's class. our model allows up to two function word insertions or deletions in a row before a swap is required. lexical content words may not be deleted or inserted, only swapped."
"however, this setting of fl may not be appropriate when source node s resides closely to the destination node d as shown in fig. 3(b) . now setting fl to large is not good since the link latency from s to node 12 is the worst (50 ms). to prevent the message delivery from long transmission delay, we set fl to small (say 0.2) and route the message to node 6 through the link with lowest delay. in contrast, such setting is no longer suitable when the source node is located in a densely populated area as shown in fig. 3(c) and fig. 3(d), a partial enlargement of fig. 3(c) . now a larger value of fl, such as 0.8, is preferred in order to reduce the number of nodes involved in the message delivery and at the same time minimize the end to-end latency. in fig. 3(d), node 21 is selected as the next forwarding node with such setting and the routing procedure repeats until the message reaches destination node d."
"the pesq mos results of the standard adpcm and our proposed decoding approach are shown in table 1, with the explicit results of british english, chinese, german, french and spanish, and with the average value of all 20 languages. mos gains in the range of 0.12 to 0.19 can be observed by using our proposed decoder. the average mos of the 20 languages has been improved by 0.15. moreover, as shown in table 2, more than 20 db prediction gain can be achieved, which implies the 10th-order nlms-based predictor works reasonably well in our system. these high values are particularly remarkable, since in adpcm already an arma predictor is employed. therefore, we can conclude that our approach significantly and consistently outperforms the standard adpcm decoding (even) in error-free transmission conditions."
"we train all models on 80% of our child-adult sentence pairs and test on the remaining 20%. for illustration, selected output from our model is shown in table 2 ."
"one could argue that our noisy channel model mirrors the cognitive process of child language production by appealing to the hypothesis that children rapidly learn adult-like grammar but produce errors due to performance factors [cit] . that being said, our primary goal in this paper is not cognitive plausibility, but rather the creation of a practical tool to aid in the empirical study of language acquisition. by automatically inferring adult-like forms of child sentences, our model can highlight and compare developmental trends of children over time using large quantities of data, while minimizing the need for human annotation."
"in contrast, the esl baseline suffers because its generative model is too restricted for the domain of transcribed child language. as shown above in table 4, child deletions of function words are the most frequent error types in our data. since the esl model does not capture word deletions, and has a more restricted notion of word swaps, 88% of child sentences in our training corpus cannot be translated to their reference adult versions. the result is that the esl model tends to rely too heavily on the language model. for example, on the sentence \"i com-ing to you,\" the esl model improves n-gram probability by producing \"i came to you\" instead of the correct \"i am coming to you\". this increases error over the child sentence itself."
"in this paper, we argue that the geo-distance based routing protocols used in existing overlay networks are inefficient in terms of both resource use and environmental accommodation for multicast applications. we devise a utility driven routing scheme to improve the routing efficiency with three unique features. first, our utility function is defined based on a careful combination of hop counts and routing path latency. second, we use can-like routing as an example and extend it by utilizing shortcuts to reduce the routing path length and by introducing a utility function to combine path latency with geo-distance based metric in determining the near-optimal route for each routing request. third and most importantly, our utility function is designed by using a tunable influence parameter to allow nodes to adaptively make the most promising routing decision according to their specific network state and circumstances, such as overlay connectivity and next hop latency. our experimental evaluation shows that the utility-driven routing scheme is highly scalable and efficient compared to existing geo-distance based routing proto cols and demonstrates that by combining shortcuts, path latency with geo-distance can effectively enhance the multicast delivery efficiency for large scale group communication applications. include the dht based multicast systems, such as scribe [cit], peercast [cit], splitstream [cit], bullet [cit] and nice [cit], as well as unstructured overlay multicast systems that use gossip based routing algorithms, such as coolstream [cit], esm and chainsaw [cit] . by carefully examining these diverse research efforts, we observe some interesting and important facts:"
"contains noun subject, accusative noun subject: the first boolean feature indicates whether the dependency parse of candidate translation t i contains a \"nsubj\" relation. the second indicates if a \"nsubj\" relation exists where the dependent is an accusative pronoun (e.g. \"him ate the cookie\"). these features and the one following have previously been used in classifier based error detection [cit] ."
"delay penalty the impact of routing schemes on the application performance is first investigated based on three metrics: path length, processing latency and routing latency."
"these fsts provide the basis for our translation process. we represent sentences by building a simple linear chain fst, progressing from node to node with each arc accepting and yielding one word in the sentence. all arcs are weighted with probability one. 4 auxiliary lemmas include have, do, go, will, and get."
"predictions are evaluated with bleu score [cit] and word error rate (wer), defined as the minimum string edit distance (in words) between reference and predicted translations, divided by length of the reference. as a control, we compare all results against scores for the uncorrected child sentences themselves. as reported in table 3, our model achieves the best scores for both metrics. bleu score increases from 50 for child sentences to 62, while wer is reduced from .271 to .224. interestingly, moses achieves a bleu score of 58 -still four points below our model -but actually increases wer to .449. for both metrics, the esl system increases error. this is not surprising given that its intended application is in an entirely different domain."
"a fair amount of research has been carried out for improving dht routing efficiency [cit] [lo] [ii], most of which incorporate proximity into the dht routing protocols. generally, they can be classified into three categories: proximity neighbor selec tion scheme(pns), proximity route selection scheme(prs), and proximity identifier selection scheme(pis)."
"first, it is conunonly recognized that the properties of the underlying overlay networks, such as the colmnunication efficiency and system scalability, tend to dominate the per formance of the overlay system and the multicast applications built on top of it. concretely, the efficiency of any overlay net work heavily depends on the efficiency of its routing protocol."
"in the first prototype of geocast, we introduce a system defined parameter m for shortcut clustering. the setting of m allows us to limit the size of each peernodelist maintained in the system. as the network grows, the shortcuts are being grouped into m clusters, each of which selects one represen tative shortcut and keeps it in the peernodelist."
"number of nodes in fig. 11(a), we observe that even with a small value of shortcut availability, the schemes of udr can deliver the messages to the destination nodes at a far more speed."
"adaptive differential pulse code modulation (adpcm) [cit], including adaptive prediction backwards (apb) and adaptive quantization backwards (aqb) methods, offers speech quality at different levels and is widely used in digital enhanced cordless telephony (dect) [cit], new generation dect [cit], and voice over ip (voip). when the input process is nonstationary (e.g., speech), the variance changes over time; then adaptive quantization can provide a better performance, especially for low bit rate quantization [cit] . in adaptive quantization with backward estimation (aqb), the step size adaptation is performed on the basis of the reconstructed/quantized signal. moreover, the nonstationarity of speech also requires adapting the predictor coefficients over time leading to a higher prediction gain than using fixed predictor coefficients [cit] . in adpcm, the adaptation information is recursively computed according to previous quantized signals (apb) [cit] . for speech with a sampling rate of 8 khz, the predictor order of 10 is normally adequate. however, in g.726 and g.722 adpcm [cit], only a second-order all-pole predictor and a sixth-order all-zero predictor are employed (autoregressive moving-average (arma) predictor)."
"the second optimization is focused on reducing the impact of network dynamics, especially the oscillation of link latency on the stability of multicast trees in geocast."
"even in the larger system, the difference between udr and sgd is less than 1 hop. it is important to note that both nb and rnb perform quite poorly in all cases. this is because in those schemes, only neighbors are taken into consideration when selecting the next hop for message delivery, which may result in routing paths that are longer than that of the others. both sgd and udr outperform the others due to their shorter forwarding route in terms of hop counts. we can see that the differences become pronounced when the system size is larger. this is because the routing path is getting longer as the system size increases as shown in table ii . specifically, in the system of 8,000 nodes, rnb needs to take about 5000 ms to transmit messages to the destination on average, which is aout 8 times as many as that of routing schemes with shortcut (sgd or udr). petitors. with the growth of system sizes, the shortcut based routing scheme keeps a relatively steady performance and take no more than 15 ms for message processing during the entire routing procedure. in contrast, nb and rnb schemes need to take much more time for message processing and this situation gets worse as the system size increases."
"it is meaningful and significant to predict disease-related mirnas in studying the intrinsic aetiological factors of human diseases. a new model named grl 2, 1 -nmf was developed in this work for potential mirna-disease association prediction. first, we integrated experimentally validated connections between mirnas and disease as well as mirna functional similarities along with two kinds of disease semantic similarities, and then we calculated the gip kernel similarities of micrornas and diseases. moreover, we used wknkn to convert the value of matrix y into a decimal between 0 and 1 and decrease the sparsity of matrix y. furthermore, the tikhonov (l 2 ), graph laplacian regularization terms and the l 2, 1norm were added into the traditional nmf model for predicting mirna-disease connections. in addition, the tikhonov regularization was utilized to penalize the non-smoothness of w and h, and the graph laplacian regularization was primarily intended to guarantee localbased representation by leveraging the geometric structure of the data. the l 2, 1 -norm was added to increase the disease matrix sparsity and eliminate unattached disease pairs. our method performs well in global loocv, 5-cv and case studies in heterogeneous omics data. the experimental results indicate that grl 2, 1 -nmf can effectively and powerfully infer disease-related mirnas, even if there are no known mirna-disease associations. however, this method still has limitations that need further research. first, our similarity measurement for grl 2, 1 -nmf might not be perfect, and other mirna information still needs to be taken into account. moreover, there is still room for improvement in the predictive performance of our method."
"it is also interesting to observe that for any message, the number of nodes that are suitable to be selected as routing nodes decreases when destination d is approaching."
we use transit-stub graph model from the gt-itm topol ogy generator to generate network topologies for our simula- the metrics used to in our experimental evaluation are sulmnarized in table i .
"we use 10 word classes to parameterize our model: pronouns, negators, wh-words, conjunctions, prepositions, determiners, modal verbs, \"be\" verbs, other auxiliary verbs, and lexical content words. the list of words in each class is provided as part of our data set. for each input adult word w, the model generates output word w as a hierarchical series of draws from multinomial distributions, conditioned on the original word w and its class c."
"nodes and the latency of the selected links is dynamically changed in the different ranges associated to the link type. have similar tendency to react the changes of network. this is because only a small part of branches are being rebuilt at runtime while the majority of branches remained in the multicast trees do not have any change. we also find that after preaches 90, increasing the value of parameter further does not achieve dramatic improvement in terms of end-to-end latency."
"our approach to address this problem is to employ the concept of latency levels. the motivation is to determine the adequate threshold value for capturing the significant changes in the advertised link latency. in terms of latency distribution, the links among nodes in the system is divided into l levels."
"even though the geo-distance based routing with shortcut in basic geocast is more efficient than can like geo-distance routing in terms of routing path length (hop counts), we argue that the shortest geo-distance based routing path may be a long forwarding path in terms of network latency, which leads to inefficient routing performance. concretely, as shown in fig. 2(b), by using the shortest geo-distance routing path, the message reaches the destination in 240 ms, denoted by the solid line in fig. 2(c) . in contrast, the dashed line gives a faster routing path by incorporating network latency in routing selection algorithm, only 120 ms. this is because the link transmission latency in the first two hops of the shortest geo-distance routing path are relatively high with up to 100 ms delay, which can be caused by either bad ip traffic in those regions or low capacities of the forwarding nodes along the routing path. we observe from this example that combining the shortcut and latency enables us to enjoy the benefit from one hop and yet fast routing jump to the region close to the destination. thus we conjecture that the best routing path should be the one that have both short geo-distance (minimizing path length or hop counts) and short network latency (minimizing path latency)."
"for the global loocv, every known mirnadisease connection was selected in turn for testing, and others that had also been experimentally verified were considered as training sets for the purpose of model training. in addition, all mirna-disease associations without evidence were regarded as candidate samples. next, we calculated the prediction score of all associations by implementing grl 2, 1 -nmf and thus obtained the ranking of each test sample compared with that of the candidate samples. we hold our model as efficient if the ranking of each test sample was higher than a certain threshold. we obtained the corresponding true positive rate (tpr, sensitivity) and false positive rate (fpr, 1-specificity) by setting various thresholds. sensitivity is the proportion of the testing samples whose ranking was higher than the threshold, while 1-specificity calculates the percentage of the testing samples whose ranking was lower than the threshold. thus, the receiver operating characteristic (roc) curve can be plotted in line with tprs and fprs obtained by different thresholds. finally, to evaluate the performance and compare it with that of the other models, the areas under the roc curve (aucs) were computed. the auc value is between 0 and 1, and a model whose auc value is higher has a better performance. the results showed that grl 2, 1 -nmf, icfmda, sacmda and imcmda achieved auc values of 0.9280, 0.9072, 0.8777 and 0.8384, respectively (see fig. 1 ). clearly, grl 2, 1 -nmf obtained the best performance among the four explored methods."
"first, we acquired the interaction profile of each mirna m q according to the functional similarity between m q and its k nearest known mirnas as follows:"
"grl 2, 1 -nmf here, a new nonnegative matrix factorization method was presented to identify underlying mirna-disease connections. the flow chart of grl 2, 1 -nmf is shown in fig. 3 . we incorporated tikhonov (l 2 ), graph laplacian regularization terms and the l 2, 1 -norm into the traditional nmf model for predicting mirna-disease connections. the tikhonov regularization is utilized to penalize the non-smoothness of w and h [cit], and the graph laplacian regularization is primarily intended to ensure local-based representation by fig. 3 flow chart of grl2,1-nmf leveraging the geometric structure of the data [cit] . the l 2, 1 -norm was added to increase the disease matrix sparsity and eliminate unattached disease pairs [cit] . the optimization problem of grl 2, 1 -nmf can be formularized as follows:"
"in this paper we propose a new scheme named utility driven routing (udr) to improve the efficiency of geo distance based routing protocols and enhance the performance of applications. our utility-driven method has three unique features. first, we define the utility function based on a careful combination of hop counts, routing path latency, and geographical locality of nodes. second, given the nature of can-like dht that is considered more reliable than chord like dht due to its multi-dimensionality characteristics, we use can-like routing as an example and extend the can like geo-distance based routing by utilizing shortcuts to reduce the routing path length and by introducing a utility function to combine path latency with geo-distance based metric in determining the most promising route for each routing request."
"(4) with the shifted prediction error pdf being a function of dln. furthermore, the centroid condition from (1) given a known predicted signal becomes a conditional one:"
"effect of parameters on the performance of grl 2, 1 -nmf in this work, we measured two disease semantic similarities, mirna functional similarity and gip similarities for mirnas and diseases. these two disease semantic similarities were integrated as eq. (1), and the final disease similarity and mirna similarity were measured as eq. (2) and eq. (3), respectively. we defined six parameters, namely, α 1, α 2, γ 1, γ 2, θ 1 and θ 2, to balance the items in eq. (1), eq. (2) and eq. (3). the values of α 1 and α 2 ranged from 0.1, 0.2, 0.3, ... to 0.9. γ 1, γ 2, θ 1 and θ 2 ranged from 0,0.1,0.2, ... 0.9, to 1. we conducted a series of experiments on the above parameters to acquire the effects of these parameters. the experimental results are shown in table 1 and table 2 ."
"however, these approaches based on matrix factorization ignored the sparsity of the mirna-disease association matrix y, so we utilized a pre-processing step named weighted k nearest known neighbours (wknkn) [cit] to convert the value of the mirnadisease associations matrix y into a decimal between 0 and 1. in addition, unlike the traditional nonnegative matrix factorization (nmf) methods, we added l 2, 1norm as well as gip (gaussian interaction profile) kernels into the nmf model. the l 2, 1 -norm was added to increase the disease matrix sparsity and eliminate unattached disease pairs [cit] . moreover, tikhonov regularization was added to penalize the nonsmoothness of w and h [cit], and the graph regularization was primarily intended to assure localbased representation by leveraging the geometry of the data [cit] ."
"in general, hypothesizing dropped words burdens the noise model by adding additional draws from multinomial distributions to the derivation. to pre- dict a deletion, either the language model or the reranker must strongly prefer including the omitted word. a syntax-based noise model may achieve better performance in detecting and correcting child word drops. while our model parameterization and performance rely on the largely constrained nature of child language errors, we observe some instances in which it is overly restrictive. for 10% of utterances in our corpus, it is impossible to recover the exact gold-standard adult sentence. these sentences feature errors like reordering or lexical lemma swapsfor example \"i talk mexican\" for \"i speak spanish.\" while our model may correct other errors in these sentences, a perfect correction is unattainable. sometimes, our model produces appropriate forms which by happenstance do not conform to the annotators' decision. for example, in the second row of table 2, the model corrects \"this one have water?\" to \"this one has water?\", instead of the more verbose correction chosen by the annotators (\"does this one have water?\"). similarly, our model sometimes produces corrections which seem appropriate in isolation, but do not preserve the meaning implied by the larger conversational context. for example, in row three of table 2, the sentence \"want to read the book.\" is recognized both by our human annotators and the system as requiring a pronoun subject. unlike the annotators, however, the model has no knowledge of conversational context, so it chooses the highest probability pronoun -in this case \"you\" -instead of the contextually correct \"i.\" learning curves in figure 2, we see that the learning curves for our model initially rise sharply, then remain relatively flat. using only 10% of our training data (80 sentences), we increase bleu from 44 (using just the language model) to almost 61. we only reach our reported bleu score of 62 when adding the final 20% of training data. this result emphasizes the specificity of our parameterization. because our model is so tailored to the childlanguage scenario, only a few examples of each error type are needed to find good parameter values. we suspect that more annotated data would lead to a continued but slow increase in performance."
"where the numerator of equation (9) represents the common ancestor nodes of diseases d i and d j, and the denominator denotes the entire ancestor nodes of diseases d i and d j ."
"the source of a mul ticast service uses the corresponding multicast tree for deliv ering the multicast data to all the subscribers. it injects the data at the root of the multicast tree, which gets disseminated through the tree and reaches all the subscribers."
"we have presented a utility driven routing scheme, which is unique in two aspects. first, it improves existing can like geo-distance routing and existing proximity based routing protocols by carefully combining shortcut, geo-distance met ric, with link latency metric in the message forwarding path selection process. second, a utility function is designed by using a tunable influence parameter to provide an adaptive way for nodes to make the near-optimal routing decision with respect to their specific circumstances and network scenarios."
"as detailed in section 4, our noise model consists of a series of multinomial distributions which govern in an actual decoding fst many more transduction arcs exist, including those translating \"that\" and \"him\" to any determiner and pronoun, respectively, and affording opportunities for many more deletions and insertions. input and output strings given by fst paths correspond to possible adult-to-child translations."
"in this paper we introduce a corpus of divergent child sentences with corresponding adult forms, enabling the systematic computational modeling of child language by relating it to adult grammar. we propose a child-to-adult translation task as a means to investigate child language development, and provide an initial model for this task."
"c. the impact of adaptive setting in fig.13, we observe that the end-to-end latency can be minimized in all cases when j.l is set to 0.6. additionally, table ii), which makes it more attractive than existing approaches. given geo distance routing (sgd) fails to address the network latency issue in the routing algorithm, it achieves a poor performance."
"memoryless quantizers [cit] are designed in the same way for correlated as for uncorrelated processes. utilizing source correlation asks for vector quantization (vq) [cit] or scalar quantization (sq) with memory [cit], the latter having predictive quantization [cit] and transform coding [cit] as two representative approaches. in predictive quantization, predictors are required both at the encoder and decoder side; the difference between the original signal and its predicted signal is quantized [6, 7, [cit] ."
"in future work, we intend to dynamically model child language ability as it grows and shifts in response to internal processes and external stimuli. we also plan to develop and train models specializing in the detection of specific error categories. by explicitly shifting our model's objective from childadult translation to the detection of some particular error, we hope to improve our analysis of child divergences over time."
"our utility-driven routing scheme presented in this paper, to the best of our knowledge, is the first one that provides a combination of techniques to allow nodes to adaptively make the best routing decision, by utilizing shortcuts to reduce the routing path length, introducing utility function and a tunable influence parameter to integrate path latency and shortcut with geo-distance based routing."
"maintenance cost now, we simulate multicast session to investigate the impact of shortcut clustering on the routing performance of udr in the system with 4,000 nodes. in each simulation, there are 10 trees consisting of 20 subscribers on average. during runtime, root node issues 50 m meta-data to their groups and we measure the messages generated for data transmission and topology maintenance. cut availability respectively. we notice that the more short-"
"to represent all possible derivations of each child sentence s from its adult translation t, we compose the sentence fsts with the noise model, obtaining:"
"geocast is a geographical overlay system built on top of geogrid [cit] for providing group communication services. it is composed of two-tier substrates: overlay network manage ment and end system node multicast management. in geocast, we use it to denote the available bandwidth of the node. ei'r.w and ei.r.h refer to the width and height of region r owned by ei, respectively."
"baselines we compare our system's performance with two pre-existing baselines. the first is a standard phrase-based machine translation system using moses [cit] with giza++ [cit] word alignments. we hold out 9% of the training data for tuning using the mert algorithm with bleu objective [cit] . like our system, this baseline trains fst noise models using em in the v-expectation semiring. our noise model is crafted specifically for the child language domain, and so differs from park and levy's in several ways: first, we capture a wider range of word-swaps, with richer parameterization allowing many more translation options. as a result, our model has 6,718 parameters, many more than the esl model's 187. these parameters correspond to learned probability distributions, whereas in the esl model many of the distributions are fixed as uniform. we also capture a larger class of errors, including deletions, change of auxiliary lemma, and inflectional overgeneralizations. finally, we use a discriminative reranking step to model long-range syntactic dependencies. although the esl model is originally geared towards fully unsupervised training, we train this baseline in the same supervised framework as our model."
"third, the performance enhancement to the underlying dht networks is typically independent of and complementary to the existing multicast algorithms developed for decentralized overlay networks, such as splitstream [cit], coolstream [cit], to name a few. surprisingly, existing research efforts have been mostly dedicated to efficient message delivery techniques, such as reducing the delivery path length (hop counts) or optimizing routing path by utilizing network locality. we argue that the geo-distance based routing protocols used in existing overlay networks are inefficient for supporting multicast applications due to two reasons. first, most of the overlay routing protocols fail to make a careful integration of path length, path latency, and network locality into the geo-distance based routing algorithms. in their systems, either nodes or links might be imposed on heavy load, which results in poor system performance. second, few overlay routing schemes to date are capable of adapting their routing decisions for each message to the network dynamics. in such a case, some data messages might have long transmission delay or be lost before reaching the destination node due to ignoring the network state. it thus is a necessity to have an efficient routing protocol with consideration of each message's specific situation."
"in the language model fst, the input and output words of each arc are identical. arcs are weighted with the probability of the n-gram beginning with some prefix associated with the source node, and ending with the arc's input/output word. in this setup, the probability of a string is the total weight of the path accepting and emitting that string."
"in this section, we report our experimental evaluation of the utility driven routing (udr) scheme with respect to effec tiveness, scalability and robustness by conducting companions with can like geo-distance based routing (nb) [cit], rtt weighed neighbor-based routing (rnb) [cit], and geo distance based routing with shortcut (sgd)."
"in the scheme of udr with random setting, nodes set the table ii, we can see that with random setting, udr exhibits a better performance than both nb and rnb in terms of path length."
"if w is not in either of the above two categories, it is a lexical word, and our model only allows changes in conjugation or declension. if the source word is a noun it may swap to singular or plural form conditioned on the source form. if the word is a verb, it may swap to any conjugated or non-finite form, again conditioned on the source form. lexical words that are not marked by celex [cit] as nouns or verbs may only swap to the exact same word."
". pns is appropriate since it can achieve both low delay routes and low bandwidth use. however, it comes at the expense of high overhead. in prsnodes are dedicated to minimizing the hop routing latency under the constraint that each hop should be closer to the destination."
"second, most of the overlay network topologies and routing protocols do not match well with the packet routing structure in the underlying network. it is common that one hop dis tance in an overlay network may incur ip traffics across two continents and lead to a long link latency in the underlying network. thus, routing efficiency should take into account of reducing the routing path length and path latency, as well as optimizing routing path by utilizing the network locality."
"it is noted that the optimization/training of the prediction error pdf is carried out with the american english database, but the optimal values can also be applied to other languages. we can state that the optimization is independent of language."
"after training our noise model, we apply the system to translate divergent child language to adultlike speech. as in training, the noise fst is composed with the fst for each child sentence s. in place of the adult sentence, the language model fst is used, yielding:"
"statistical machine translation (smt) has been applied in diverse contexts including grammar correction as well as paraphrasing [cit], question answering [cit] and prediction of twitter responses [cit] . in the realm of error correction, smt has been applied to identify and correct spelling errors in internet search queries [cit] took an unsupervised smt approach to esl error correction using weighted finite state transducers (fsts). the work described in this paper is inspired by that of park and levy, and in section 6 we detail differences between our approaches. we also include their model as a baseline."
"abe we do not observe the same pattern. 7 this points to a degree of variance across children, and suggests the use of our model as a tool for further empirical refinement of language development hypotheses."
"to simulate test conditions, we train the weight vector on n-best lists from 8-fold cross-validation over training data, using the averaged perceptron reranking algorithm [cit] . since the n-best list might not include the exact gold-standard correction, a target correction which maximizes our evaluation metric is chosen from the list. the n-best list is non-linearly separable, so perceptron training iterates for 1000 rounds, when it is terminated without converging. our feature function f (s, t i ) yields nine boolean and real-valued features derived from (i) the fst that generates child sentence s from candidate adultform t i, and (ii) the pos sequence and dependency parse of candidate t i obtained with the stanford parser [cit] . features were selected based on their performance in reranking heldout development data from the training set. reranking features are given below:"
"it is important to note that the goal of minimizing the routing path length (hop counts) can sometimes be in conflict with the goal of minimizing the routing path latency. surprisingly, none of the existing routing schemes has shown how to make the best routing decision by a careful tradeoff between minimizing routing path length and minimizing routing path latency."
"our experiments show that the utility driven routing scheme is scalable and latency efficient for large scale multicast applications compared to existing routing protocols. in our next work, further studies on the evaluation of the utility driven routing (udr) scheme in the wide area network will be conducted."
"to train and evaluate our translation system, we first collected a corpus of 1,000 errorful child-language utterances from the american english portion of the childes database. to encourage diversity in the grammatical divergences captured by our corpus, our data is drawn from a large pool of studies (see bibliography for the full list of citations). in the annotation process, candidate child sentences were randomly selected from the pool and classified by hand as either grammatically correct, divergent or unclassifiable (when it was not possible to tell what a child is trying to say). we continued this process until 1,000 divergent sentences were found. along the way we also encountered 5,197 grammatically correct utterances and 909 that were unclassifiable. 2 because childes includes speech samples from children of diverse age, background and language ability, our corpus does not capture any specific stage of language development. instead, the corpus represents a general snapshot of a learner who has not yet mastered english as their first language."
this example shows that the constant setting of the influence parameter fl is impractical in a highly dynamic environment where any two nodes may communicate with one another at any time and the system may have unpredictable churn rate.
"in enhanced geocast, nodes use the utility based routing algorithm to forward the message, until it reaches destination node specified by message. along the routing path, each node keeps computing the geo-distance based filter for itself by examining the entries in its peernodelist, and uses it to generate a gdv candidate list. this candidate list contains all nodes that are closer to the destination node in terms of geo-distance than the current node. by calculating the gdv value for every node in the candidate list produced above, the message is routed to the node with the smallest gdv value."
"in this section, we introduce two optimization techniques implemented in geocast to enhance the performance of the udr algorithm. the first optimization is to use shortcut clustering to enable the different applications to control the amount of shortcuts to be created and maintained at each node by balancing the storage and maintenance cost of shortcuts."
"discussion our error correction system is designed to be more constrained than a full-scale mt system, focusing parameter learning on errors that are known to be common to child language learners. reorderings are prohibited, lexical word swaps are limited to inflectional changes, and deletions are restricted to function word categories. by highly restricting our hypothesis space, we provide an inductive bias for our model that matches the child language domain. this is particularly important since the size of our training set is much smaller than that usually used in mt. indeed, as figure 2 shows, very little data is needed to achieve good performance."
"besides this, our model's predictive success itself has theoretical implications. by aggregating training and testing data across children, our model instantiates the brown hypothesis of a shared developmental path. even when adequate per-child training data exists, using data only from other children leads to no degradation in performance, suggesting that the learned parameters capture general child language phenomena and not just individual habits. besides aggregating across children, our model coarsely lumps together all stages of development, providing a frozen snapshot of child grammar. this establishes a baseline for more cognitively plausible and temporally dynamic models."
"this boolean feature is true if the pos tags of t i include a finite verb. this feature differentiates structures like \"i am going\" from \"i going.\" question template features: we define templates for wh-and yes-no questions. a sentence fits the wh-question template if it begins with a whword, followed by an auxiliary or copula verb (e.g. \"who did...\"). a sentence fits the yes-no template when it begins with an auxiliary or copula verb, then a noun subject followed by a verb or adjective (e.g. \"are you going...\"). we include one boolean feature for each of these templates indicating when a template match is inappropriate, when the original child utterance terminates in a period instead of a question mark. in addition to the two features for inappropriate template matches, we have a single feature that signals appropriate matches of either question template -when the original child utterance terminates in a question mark. table 2 : randomly selected test output generated by our complete error correction model, along with corresponding child utterances and human corrections."
"we simulated the overlay networks consisting of 1,000 to 8000 nodes. table ii shows the results for four routing schemes: nb, rnb, sgd and udr with random setting."
"interestingly, after shortcut availability reaches 0.4, increasing the shortcut availability further does not achieve dramatic improvement in end-to-end latency. from fig. 11(b), we also argue that with such setting, the maintenance cost can be constrained within an acceptable level, which relates to the requirement of the applications."
"in this section, we describe steps necessary to build, train and test our error correction model. weighted finite state transducers (fsts) used in our model are constructed with openfst [cit] ."
"the paper is structured as follows: section 2 briefly revisits a standard adpcm encoder and decoder for later reference and notations. section 3 presents our improved adpcm decoder. simulation results are discussed in section 4. finally, conclusions are drawn in section 5."
"in this paper, we present an improved adaptive differential pulse code modulation (adpcm) [cit] 23rd european signal processing conference (eusipco) approach, which adopts an unmodified encoder and employs an additional predictor on the basis of the normalized least-mean-squares (nlms) algorithm in the decoder. according to the standard decision levels and using the centroid condition, a new adaptive quantization codebook can be generated to be used only in the decoder. we apply this approach to the adpcm decoder operating at 16 kbit/s and show that the mean opinion score (mos) can be increased by about 0.15 points using our proposed approach in error-free transmission conditions for low bit rate. the proposed approach is applicable in a standard-compatible fashion and can straightforwardly be employed to improve new generation dect devices."
"obviously, the choice of parameter m has significant influ ence on the routing performance of udr. the higher m value means more shortcut nodes are maintained in the peernodelist, the more accurate the gdv values are, and the higher proba bility the udr can find the best forwarding path in terms of routing efficiency. however, by setting m to be a reasonable value with respect to the size of the network, our experimental results show that the udr routing algorithm optimized with shortcut clustering can offer comparable performance of udr without shortcut clustering."
"automatic correction of second language (l2) writing is a common objective in computer assisted language learning (call). these tasks generally target high-frequency error categories including article, word-form, and preposition choice. previous work in call error correction includes identifying word choice errors in toefl essays based on context [cit], correcting errors with a generative lattice and pcfg reranking [cit], and identifying a broad range of errors in esl essays by examining linguistic features of words in sequence [cit] shared esl correction task [cit], the best performing system [cit] corrected preposition, article, punctuation and spelling errors by building classifiers for each category. this line of work is grounded in the practical application of automatic error correction as a learning tool for esl students."
"p ublishing the multicast service multicast sources can join geocast as nodes or select nodes in geocast to be their delegates for the purpose of information dissemination. each multicast service is associated with two identifiers: service identifier and group identifier. the service identifier will be used to advertise and publish meta-information about the service, whereas the group identifier is used by other peer nodes to subscribe or unsubscribe the multicast service."
"the second component in the formula is ij2q-r-\\ which is introduced for two purposes. on the one hand, we observe that nodes locating in the sparsely populated area tend to have fewer nodes contained in their peernodelists even in a large network and consequently they might have less knowledge about the network. the factor of ij2q-r-1 takes a larger value when both q and t are small. in such case, the node with shorter geo-distance to the destination has higher priority to be selected as the forwarding node. on the other hand, the factor of ij2q-r-1 prevents j-l from stumbling in a small value. given the scenario 3 in fig. 3, it is desirable for j-l to be set to a larger value even when the node ei is not far away from the destination node d. note that the first component of the j-l formula is designed to make j-l converge to a smaller value when the destination is nearby and set j-l larger when the destination is relatively far away from the current node. thus, the second component is employed to alleviate the influence of the first component when the distance factor should play a more important role in routing node selection even though the destination is nearby in terms of geo-distance (t is small and q is small)."
"to compare our routing scheme with existing can-like geo-distance based routing approaches, we develop geocast, a can-like decentralized geographical overlay for end-to end multicast services. we support geo-distance based routing in basic geocast system and implement the geographical proximity aware udr routing in enhanced geocast system. the rest of the paper is organized as follows. we first discuss related work in section 2 and then describe the structure of geocast system and the motivation of the design of our utility-driven routing scheme in section 3. we describe the design of our utility-driven routing scheme and analyze the setting of the tunable influence parameter in section 4."
"it potentially ensures the links between nodes locating in the vicinity are with low latency. however, this method has a main drawback in load balancing, where nodes in pis may be imposed on heavy load when their contents are highly desired by users. in such a case, the messages in pis may be delayed, which leads to a significant degradation of system performance."
"the itu-t recommendations g.726 narrowband codec and g.722 wideband codec are mandatory for new generation dect devices [cit] . in g.722 subband-adpcm [cit], speech signals with 16 khz sampling rate are split into a higher band and a lower band, with the signals in each subband (sampled at 8 khz) being adpcmencoded. the higher subband can produce 16 kbit/s signals using a 2 bit quantizer, while the lower subband is operating at 48 kbit/s with a 6 bit quantizer. due to the similarity of g.726 adpcm [cit] and the subband structure in g.722 adpcm, we focus on the description and improvement of (g.726) adpcm in this paper."
"this is the higher layer for multicast service publica tion, subscription management, multicast payload delivery and group membership management. it is built on top of the overlay network management substrate and uses its api to carry out management functions."
"we constructed a simulation experiment to further demonstrate the effectiveness of grl 2, 1 -nmf for inferring likely disease-connected mirnas. here, all manually validated mirna-disease connections were utilized for prediction, and other associations that did not have evidence were regarded as candidate connections for validation. for every disease, the candidate mirnas were ranked based on the prediction scores. we used two mirna-disease databases, namely, hmdd [cit] and dbdemc [cit], to verify the inferred possible micrornas for the investigated disease, including prostate neoplasms, breast neoplasms and lung neoplasms. finally, the top 50 disease-related mirnas predicted via grl 2, 1 -nmf are demonstrated in table 3, table 4 and table 5 . there are 48,45 and 45 of 50 inferred mirnas confirmed to have associations with prostate neoplasms, breast neoplasms and lung neoplasms, respectively, by the dbdemc database and hmdd v3.0 database."
"in geocast, each node has an average of o ( 2d ) neighbors and o(logn) shortcuts maintained in its peernodelist, where d is the dimensions of coordinate space and n is the number of nodes currently in the system. unlike can like geo-distance based routing, the scheme of geo-distance based routing with shortcut ensures that any node in the system can be reached in less than o(logn) hops, achieving similar performance to chord [cit] and expressway [cit] . fig.2 illustrates the difference of the routing schemes using an example. given source s and destination d in a system of 24 nodes. the scheme of geo distance routing with shortcut only needs 3 hops to reach node d (see solid line in fig. 2(b) ) whereas can like geo-distance based routing needs two times as many hops as that of geo distance based routing with shortcut, as shown in fig. 2(a) ."
"in this study, we present a computational algorithm based on graph regularized l 2, 1 -nonnegative matrix factorization (grl 2, 1 -nmf) to infer the possible connections between micrornas and diseases in heterogeneous omics data. first, we integrated manually validated microrna-disease connection information, mirna functional similarity information and two kinds of disease semantic similarity information, and then we calculated the gip kernel similarities for the diseases and mirnas. then, we utilized wknkn to decrease the sparsity of matrix y. furthermore, we added tikhonov (l 2 ), graph laplacian regularization terms and the l 2, 1 -norm to the standard nmf model for predicting disease-associated mirnas. finally, five-fold cross validation and global leave-one-out cross validation were implemented to evaluate the effectiveness of our model, and we obtained aucs of 0.9276 and 0.9280, respectively. furthermore, we performed case studies on three high-risk human diseases (prostate neoplasms, lung neoplasms and breast neoplasms). as a result, 48, 45 and 45 out of the top 50 likely connected mirnas of prostate neoplasms, lung neoplasms and breast neoplasms, respectively, were confirmed by hmdd [cit] and dbdemc [cit] . based on the experimental results, we can clearly see that grl 2, 1 -nmf is a valuable approach for inferring possible mirna-disease connections."
"comparing to existing link latency-aware approaches, our approach to tune the influence parameter j-l in the gdv utility function is unique and highly effective because it allows a message to be routed at different nodes with different j-l depending on multiple factors, including link latency, distance to destination, node density nearby the destination. thus our utility driven routing protocol is highly adaptive to both the real time network dynamics and the diversity of the locations of the source and destination."
"while child error correction is a novel task, computational methods are frequently used to study first language acquisition. the computational study of speech is facilitated by talkbank [cit], a large database of transcribed dialogues including childes [cit], a subsection composed entirely of child conversation data. computational tools have been developed specifically for the large-scale analysis of childes. these tools enable further computational study such as the automatic calculation of the language development metrics ipsyn [cit] and d-level [cit], or the automatic formulation of novel language development metrics themselves [cit] ."
"in addition to the domain-specific generative model, our approach has the advantage of longrange syntactic information encoded by reranking features. although the perceptron algorithm places high weight on the generative model probability, it alters the predictions in 17 out of 201 test sentences, in all cases an improvement. three of these reranking changes add a noun subject, five enforce question structure, and nine add a main verb."
"if w is an auxiliary verb, the swap procedure consists of two parallel steps. a lemma is selected from possible auxiliary lemmas, conditioned on the lemma of the source word. 4 in the second step, an output inflection type is selected from a distribution conditioned on the source word's inflection. the precise output word is fully specified by the choice of lemma and conjugation."
"error analysis we measured the performance of our model over the six most common categories of child divergence, including deletions of various function words and overgeneralizations of past tense forms (e.g. \"maked\" for \"made\"). we first identified model parameters associated with each category, and then counted the number of correct and incorrect parameter firings on the test sentences. as table 4 indicates, our model performs reasonably well on \"be\" verb deletions, preposition deletions, and overgeneralizations, but has difficulty correcting pronoun and auxiliary deletions."
"the idea behind the shortcut clustering optimization is to maintain \"enough\" shortcuts for each node instead of all shortcuts in addition to the neighbor nodes. it offers end system nodes with ability to keep those shortcut nodes that has higher capacity and thus can handle more routing workload and also to reduce the shortcut nodes that are representative for the same enclosing zone."
"there is a hypothesis that if two mirnas are similar functionally, they are more likely to have connections with diseases that have high similarity, and vice versa [cit] shared their investigation results, and researchers can download mirna functional similarity information at http://www.cuilab.cn/files/images/cuilab/misim.zip. here, we established a matrix s m that was denoted as the microrna functional similarities. the item s m (m i, m j ) denotes the functional similarities among micrornas m i and m j ."
"third and most importantly, we design our utility function with a tunable influence parameter to allow nodes to adaptively make the near-optimal routing decision based on their specific network state and circumstances, such as overlay network connectivity, next hop latency. thus, our utility based routing scheme can dynamically determine the best routing path for each message in terms of hop counts and routing latency."
"if two diseases are similar, they are likely to have associations with micrornas that are functionally approximate, and vice versa [cit] . gaussian interaction profile (gip) kernel similarities have been adopted to quantify disease similarities and mirna similarities [cit] . we also calculated gip kernel similarities for diseases and mirnas in this work. first, based on whether disease d i (m j ) has a known connection with each mirna (disease) of the adjacency matrix y, the interaction profiles ip(d i ) and ip(m j ) were constructed for disease d i and mirna m j, respectively. then, the gip kernel similarity between a disease pair and a mirna pair is computed as equation (10) and equation (11), respectively."
but it may lead to a long routing path in terms of hop counts and end-to-end latency of the messages could be much higher than that of other proximity based routing schemes. a detailed comparative study of pns and prs over a variety of overlay networks [cit] shows that pns is much more efficient than prs in improving the routing performance in terms of latency.
"the precise form and parameterization of our model were handcrafted for performance on the development data, using trial and error. we also considered more fine-grained model forms (i.e. one parameter for each non-lexical input-output word pair), as well as coarser parameterizations (i.e. a single shared parameter denoting any inflection change). the model we describe here seemed to achieve the best balance of specificity and generalization."
"in table 1 . our model does not allow reorderings, and can thus be described in terms of word-by-word stochastic transformations to the adult sentence."
"here, we performed a pre-processing procedure named weighted k nearest known neighbours (wknkn) [cit] for mirnas and diseases without any known associations to resolve the abovementioned sparse problem and thus improve the prediction accuracy. after executing wknkn, the entry y ij was replaced with a continuous value ranging from 0 to 1, and the specific steps are as follows."
"we compare our correction system against two baselines, a phrase-based machine translation (mt) system, and a model designed for english second language (esl) error correction. relative to the best performing baseline, our approach achieves a 30% decrease in word error-rate and a four point increase in bleu score. we analyze the performance of our system on various child error categories, highlighting our model's strengths (correcting be drops and morphological overgeneralizations) as well as its weaknesses (correcting pronoun and auxiliary drops). we also assess the learning rate of our model, showing that very little annotation is needed to achieve high performance. finally, to showcase a potential application, we use our model to chart one aspect of four children's grammar acquisition over time. while generally vindicating the brown thesis of a common developmental path, the results point to subtleties in variation across individuals that merit further investigation."
"training and testing across children we use our system to investigate the hypothesis that language acquisition follows a similar path across children [cit] data. these results are contrasted with performance of 8-fold cross validation training and testing solely on adam's utterances. performance statistics are given in table 5 . we first note that models trained in both scenarios lead to large error reductions over the child sentences. this provides evidence that our model captures general, and not child-specific, error patterns. although training exclusively on adam does lead to increased bleu score (72.58 vs 69.83), wer is minimized when using the larger volume of training data from other children (.186 vs .226). taken as a whole, these results suggest that training and testing on separate children does not degrade performance. this finding supports the general hypothesis of shared developmental paths."
"the terminology shown in table 2 lists properties that can be used to describe an amplifier. most of the defined properties are selfexplanatory. a special feature is seen with the \"switchingfrequency\" and the \"dutycycle\" properties. they define a \"dependency\" and a \"dependencyvalue,\" indicating that they are only meaningful if the specified dependency property (here: \"operationmode\") assumes a specific value (here: \"discontinuous\"). tools using the metadata can make use of the dependency information for consistency checks or to adapt a gui to offer appropriate values and properties."
"channeled imaging polarimeters create a spatial modulation of stokes parameters. a key aspect of this technique is to accurately reconstruct the spatially resolved stokes parameters from the modulated measurement. note that we will examine the linear stokes parameters, but the analysis in this work can be extended to include s 3 . the state-of-the-art algorithm for reconstruction uses the fourier transform to recover the stokes parameters by separating them into channels based on their carrier frequencies."
"1 for convenience, we refer to this algorithm as fourier reconstruction (fr). while this approach is straightforward, it suffers from noise in the measurement and from channel cross-talk. common experimental sources of noise include environmental vibrations, thermal fluctuations, and imperfect sampling. 7, 8 in addition, fourier reconstruction imposes bandwidth limitations from windowing the fourier transform in order to filter out channels, thus cutting off high frequency details. 9 to overcome these drawbacks, we propose a reconstruction method called compressed channeled linear imaging polarimetry (cclip). in our proposed framework, reconstruction in channeled linear imaging polarimetry is an pg pg, lp (0°) obj underdetermined problem, where we take n measurements and solve for 3n unknown stokes parameters. we formulate an optimization problem by creating a mathematical model of the channeled linear imaging polarimeter with inspiration from compressed sensing. [cit] it reduces the need for windowing used in fourier reconstruction to extract channels. more generally, our analysis applies to all channeled polarimeters, including those that are temporally or spatially channeled, by solving for the stokes parameters from a system of underdetermined equations. our framework for compressed channeled polarimetry enables future research to reconstruct stokes parameters with less than n measurements while maintaining the same resolution, potentially allowing sensors to be smaller in size, lighter weight, and lower power."
"figure 3.3 compares ground truth with fourier reconstruction and cclip. ground truth denotes the known input stokes parameters used create the simulated measurement in fig. 3.1 . in the s 0 (x, y) image for fourier reconstruction, we highlight one area of interest with a white rectangle. note that cclip resolves the stripes on top of the zebra's snout, whereas fourier reconstruction blurs these lines."
"ideally, a metadata terminology should, as much as possible, be in agreement with existing terminologies and ontologies [cit] b; [cit] . however, these standardized sets will often be insufficient to fully describe the metadata for a given experiment. in particular, terminologies for the field of neurophysiology are currently still at a developmental stage [cit] . to encourage usage of standardized terminologies from the very start of data and metadata collection, we started to set up several can infer from the type that the described entity is a hardware item. this does not imply that specialized section types inherit all properties of the parent type. the section name could be any identifying name that is used to uniquely refer to the specific item."
"the likelihood term l( s 0, s 1, s 2 ) minimizes the error with measured data. the regularizer term r( s 0, s 1, s 2 ) promotes sparsity in the basis coefficients s 0, s 1, s 2 . when the regularizer weight β is increased, the solution favors more sparse solutions. as noise increases, increasing β helps to improve robustness to noise."
"to the same dataset. in this case the file property would simply have several values. the value definition can then be used to specify what the files contain. again, we do not aim at providing a description of the format the data is saved in. this is a challenge on its own (see, e.g., [cit] ) and should rather be part of the data file itself."
"the odml metadata model is so general that arbitrary content can be exchanged or stored. in this form it could be used locally in a single lab or by an individual user. however, to achieve interoperability and allow sharing of metadata, standards defining propertynames and section types are needed. the odml-terminologies are meant as a starting point for such standards."
"the rather unconstrained approach proposed here could be valuable in refining and adapting these standards through automatized matching with the terminologies used in the community. all terminologies described here are available on the odml homepage (see text footnote 6). the terminologies will never be complete and many more properties and sections can and should be specified as needed. extension of the terminologies is necessary and has to be driven by the scientific community. the version of a terminology is part of the url and is provided in the terminology rootsection. within one version properties and sections may be added, but definitions will not be altered."
"to overcome the drawbacks of fourier reconstruction, we propose a reconstruction method called compressed channeled linear imaging polarimetry. first, we analyze signals that vary in one spatial dimension. then we describe how to perform reconstruction in two dimensions. finally we will see how this algorithm is similar to related work in channeled spectropolarimetry and can apply to channeled polarimeters in general."
"where w, b represents the parameters of the hyperplane. data sets are not always linearly separable. in case of nonlinear dataset, we can use kernel function to project dataset into a higher dimensional space in which data are linearly separable. but in case of pedestrian detection problems, typical linear svm is sufficient to get the high detection rate. using kernel function, we can get higher detection rate and decrease false positive but it take more computational resources and processing time."
"1 these instruments are part of a broader class of channeled polarimeters that encode the stokes parameters onto some domain. for example, rotating polarizers and channeled spectropolarimeters multiplex polarization onto the temporal and spectral domains, respectively."
"the traditional markowitz mvo approach is based on a single-period setting that limits its applicability to long-term investors with liabilities and goals at different times in the future. single-period optimization does not use any data and decisions beyond the rebalancing time horizon with the result that its policies are myopic in nature. more importantly, singleperiod optimization does not account for the fact that a decision made in a rebalancing today affects decisions made in future rebalancings. this is a drawback even if the rebalancing is done periodically in light of new information. a multi-period model has several stages with the first stage representing the current rebalancing and stages two and beyond representing future rebalancings. the model allows the simultaneous rebalancing of a single portfolio across different periods (extending beyond the rebalancing time horizon) in a single optimization problem. each stage in the model has its own data forecasts, objectives, and policy decisions and the stages in successive periods are coupled together. in most cases, we are only interested in the first-stage solution since the solution to the remaining stages can be updated as new information becomes available. the first-stage solution to the multiperiod model incorporates a wait-and-see feature, since it is aware of future decisions and data forecasts through the other stages in the model."
"2. the short-term and long-term alphas are down-weighted by their autocorrelations in the second period. more specifically, the short-term signal weight in the second period is 0.1 times its weight in the first period. the long-term signal weight in the second period is 0.9 times its weight in the first period."
"in our experiment, we use two well-known pedestrian datasets: mit cbcl pedestrian dataset and inria person dataset. mit cbcl pedestrian has total 924 positive images with frontal and back views only. mit dataset doesn't separate into testing and training. in our experiment, we used 700 images for training and 224 images for testing. our system recognized near-perfectly in this database with 99.55% correct detection rate, only one case is false detection. we also make an experiment with another much more challenge dataset -inria person dataset. in inria, all positive images is cropped to 64x128 pixels image which human is in center of positive images. sizes of all negative images are variance. these negative images are taken from natural scene without including people. 'inria' contains 2416 positive images and 1218 negative images for training set. for testing set, inria include 1126 positive images and 453 negative images. in experiment, each negative image will generate randomly ten 64x128 negative window images for training and testing. it means that we used 12180 negative images for training and 4530 negative images for testing. because of using 64x123 pixel images for training, our system only can detect pedestrians in the center of windows whose size is equal or bigger that window size. without pca steps, our system recognize with accuracy is 98.39% and false positive is 8. figure 6 for more detail)."
"the preceding analysis describes how to reconstruct stokes parameters s 0, s 1, and s 2 that vary in one spatial dimension. this algorithm would operate over one row of the modulated output from the channeled linear imaging polarimeter. for each row of the modulated image, we solve the optimization problem from eq. (2.3.21) to recover the stokes parameters. once all the rows are processed, we obtain the stokes parameters over the entire image. extract row i of i(x, y) and set it as i measured ."
"in order to obtain initial object location hypotheses, we use the siding window technique, where the detector window is sided at various scales and all locations over the image ( figure 5 ). at one scale and location, we will extract hog from this window image, and then project into principal component space using formula (1) to obtain the final feature vector. we term this feature is pca-hog. pca-hog feature will be classified by linear svm to decide pedestrian or non-pedestrian."
"a variety of instruments can measure the stokes parameters, 2 which describe incoherent, partially polarized radiation. 6 we will focus on channeled linear imaging polarimeters, which measure the two dimensional spatial distribution of the stokes parameters."
"frontiers in neuroinformatics www.frontiersin.org defined by the option flag, includes external files, resolves links, and converts the tree, if mapping information is provided. mappings are either specified in the metadata itself or in the used terminologies. line 9 and following: the dataset sections defined in the metadata are found. these are retrieved by looking for the \"dataset\" section type. one could also look for them by name, if known. the returned set (a java vector) contains references to the respective sections in the metadata tree which can be passed to the analysis function. such a function is sketched in listing 2. in this example loading data is handled by a further matlab function (that is not part of the odml library line 13). the analysis function returns, besides the results, the analysis metadata (line 14) which are appended to the dataset metadata (line 18). finally, the new metadata is written to a file using an instance of the writer class (lines 22, 23)."
"we also used a scaled version of the short-term alpha as an estimate for the roll-forward returns. we found that the performance of the multi-period approach was (a) similar to the zero roll-forward estimate case that we reported in the paper, and (b) was not very sensitive to the choice of these roll-forward returns. we also performed an additional experiment, where we use the true forward looking returns in the roll-forward estimate, but we did not use those true returns in the alpha estimates. in this case, we found that the performance of the multi-period approach drastically improved. obviously, this is not of practical use but we do think that coming up with good roll-forward estimates that improve the performance of the multi-period algorithm is a worthy topic for future investigation."
"in the context of this paper we understand metadata as that information that describes the conditions under which a certain dataset has been recorded. this includes descriptions of environmental parameters like temperature, humidity, date, and time, etc., descriptions of the stimulus and recording protocols, settings of the used hardware and software, information about the experimental subject, and much more. storing metadata in as much detail as possible allows replication of an experiment or the reconstruction of an analysis and thus enables reproducing results. it eases the re-use of once-acquired data and thus can increase the outcome of scientific efforts. our goal is to provide the means to conveniently and automatically capture as many as possible of what we call the hard metadata, i.e., those parameters that can be directly measured (temperature, recording date, and time, etc.) or are known in advance (e.g., stimulus parameters). the more descriptive, soft metadata (e.g., the experimental rationale, context information, etc.) provide important background information but are much harder to capture automatically and, even if present, can hardly replace a discussion with the experimenter in person. in the interest of data sharing and reproducibility, datasets should be annotated with as much of the hard metadata as possible."
"figures 3.1 and 3.2 show the simulated measurement and its fourier transform, respectfully. the interferometric fringes from the measurement result in peaks in the fourier domain. the center peak corresponds to s 0 (x, y), while the sidebands contain information about s 1 (x, y) and s 2 (x, y) as described in section 2.2. in fourier reconstruction, circular filters extract channels c 0 (u, v) and c 1 (u, v), as depicted by the two white circles and described in the flowchart in fig. 2.2(a) . we describe these filters as this approach suffers from channel cross-talk, which occurs when spectral content overlaps between channels. the circular filters further degrade the reconstruction by cutting off high frequencies."
"if other optical elements are used, we can modify the output equation following a similar jones or mueller matrix analysis. 1 the carrier frequency f c is"
"(f ) denotes the dct coefficient corresponding to frequency f for the stokes parameter s i . this problem is very similar to the optimization for channeled linear imaging polarimetry. the models of both optical systems take the same form, where the output is a modulation of the stokes parameters. both problems are convex; we can apply a similar analysis of convexity from section 5 to this problem. the main difference between the models is that variables are either a function of wavenumbers for channeled spectropolarimeters, or of space for channeled imaging polarimeters. however, since the underlying models are the same, we are essentially solving equivalent problems. hence we can apply algorithm 2.1. more generally, we can formulate reconstruction for channeled polarimeters as solving an underdetermined problem where the unknown stokes parameters outnumber the measurements. this work opens the avenue for future research into improving reconstruction for channeled polarimeters."
"the encapsulating rootsection contains elements to provide information about the whole document: these are author, date, version, and repository. the repository given in the rootsection defines a default repository valid for the whole document unless locally redefined."
"4. alpha-decay: this is the application that we consider in this paper. in a typical portfolio application, the composite alpha signal is a composition of signals with different strengths and decay rates. usually, the signals that carry more information decay quickly. in an influential paper, garleanu & pedersen [cit] consider an unconstrained infinite-dimensional multi-period model that trades off risk, return, and quadratic market-impact. this model provides an opportunity to trade-off the strength of the fast decaying signals and the persistence (memory) of the slow decaying signals."
"2. for each seed, we run the multi-period and single-period backtests with 10 assets covering 10000 periods where one solves the following model in each period."
"the basic idea of histogram of oriented gradients (hog) is that local object appearance and shape can often be characterized rather well by the distribution of local intensity gradients or edge directions, even without precise knowledge of the corresponding gradient or edge positions [cit] . input image is divided into small spatial regions (cells), for each cell accumulating a local 1-d histogram of gradient directions or edge orientations over the pixels of the cell. in order to obtain a complete descriptor of an image, we have computed local histograms of gradient according to the following steps: firstly, we compute gradients of the image; secondly, we build histogram of oriented gradient for each cell; thirdly, normalizing histograms within each block of cells; finally, all histograms are concatenated to build the final hog feature ( figure 2 and 3) ."
"in this section, we describe the optical system of a channeled linear imaging polarimeter. then we review the state-of-the-art algorithm to recover the stokes parameters. finally we present a new reconstruction method called compressed channeled linear imaging polarimetry."
"there are situations in which a strict application of the rules about the hierarchical organization (see above) would lead to complicated and redundant structures. to avoid this odml sections define link, include, and reference elements which can be used to introduce relations that exist outside the hierarchical organization."
"the second example ( figure 5b) shows a case where datasets originating from two different cells recorded in the same subject have to be described. again, each of the dataset sections shown are related to the same project, experiment, recording, and subject information, according to rule 2. their parent \"cell\" sections are different, reflecting the two different cells from which the datasets were obtained."
"one further advantage of the generic model lies in the interchange of (meta)data between databases. instead of writing and maintaining converters between all the different data models, it would suffice if each tool or platform could import and export metadata from the generic model. odml is very different form metamodel approaches like xmi 12 (which are designed for data transfer between different data models). xmi, however, requires the existence of detailed data models which do not yet exist for neurophysiological metadata."
"for these choice of parameters the single-period backtester is equivalent to the multiperiod algorithm. as we show in the technical appendix, the single-period backtester has to increase the weight on the long-term signal in order to recover the multi-period solution."
"the stimulus that was used to evoke the recorded neuronal response is a highly important piece of information for which, so far, there is no standard way of description. this paragraph briefly illustrates how the often quite complex information about the stimulus can be stored in an odml metadata file. stimuli are described by a \"stimulus\"-type section and its subsections. figure 4 shows how such a description might look like. this visual stimulus (trace on top of figure 4 ) is a combination of three components. a dc shift, a sine wave, and a white noise component are combined and delivered using the same \"outputchannel\" (\"led1\"). the timing of the stimulus components is defined by the \"temporaloffset\" and \"duration\" properties. a \"temporaloffset\" of zero represents the beginning of the stimulus and all other times are relative to this. if the different components were multimodal (e.g., a visual stimulus combined with a current injection) the respective components must define their own \"modality\" and \"outputchannel\" properties."
"the first example ( figure 5a ) illustrates a very simple case in which the hierarchy can be kept flat, since all datasets (two are shown in this example) depend on all other sections (rule 2) and there is no need for more complex structures in the actual metadata file (rules 1 and 5). other structures to arrange these items are possible but not encouraged."
"to summarize, multi-period optimization allows the investor to naturally trade-off signal strength and decay in one consolidated optimization problem. the signal weights in the first stage of the model can be chosen entirely on signal strength. these weights are appropriately down-weighted in the later stages of the model based on the signal decay; the short-term signal weight in each of these stages gets down-weighted more than the corresponding longterm signal weight. an optimizer is necessary to solve the multi-period model with realistic draft multi-period portfolio optimization with alpha decay constraints and our computational experience indicates that the solution time taken by the rolling-horizon two stage multi-period algorithm is comparable to a single-period backtester in practice."
"different communities, laboratories, or individuals in the neurosciences do not always agree on how to refer to the same entities. the odml approach respects this by allowing to avoid the standard without breaking it."
"that trades off the present value of all the future excess expected return (alpha), risk, return, and transaction costs. 1 the expectation is taken over the random α t . the risk is represented by the variance of the portfolio returns, where σ is the asset covariance matrix. the transaction costs are modeled by a quadratic function with the costs proportional to risk. this assumption enables us to derive a simple closed form solution for the model. the positive scalars γ, δ represent the risk and tc aversion parameters, respectively. using the law of total expectation, we can write the objective function in (4) as"
"the model can incorporate modifications of the optical system. for example, the optics may exhibit a spatially dependent attenuation, which would represent a multiplicative factor in eq. (2.3.7)."
"given that one can replicate the multi-period setting in a single-period framework, why should one be interested in a multi-period portfolio model? the answer is that the equivalence between the multi-period and single-period setups holds only under limited settings. some settings may require changing more single-period data parameters and there are several cases where the equivalence breaks down."
"and while this approach is straightforward, it suffers from noise in the measurement and from channel cross-talk. it also requires the choice of a window function to extract the channels c 0 (d) and c 1 (d) as described in eqs. (2.2.1)-(2.2.2). the window imposes bandwidth limitations, which cuts off high frequency details."
"the upper exhibit of figure 4 presents a scatter plot of the maximum single-period and multi-period sharpe ratios for the 30 seeds. about 53% of the dots lie above the 45 degree reference line showing that the multi-period backtester performs slightly better than the single-period backtester over these 30 seeds. the lower exhibit of figure 4 presents a scatter plot of the mean sharpe ratios for the 30 seeds. about 96% of the dots are now above the 45 degree line. this shows that the multi-period backtester performs better much better than the single-period backtester when the portfolio manager is \"average\" at picking the signal weights."
"1. we only implement the first-stage decision of the two period model. we are not interested in the second-stage decision as it can be refined as more information becomes available in the future. as we show below, the second-stage in the multi-period model induces a wait-and-see feature in the first-stage decision."
"currently, none of the data sharing portals for neurophysiology [cit] offers the possibility to routinely enter the metadata into a database by providing them in a machine-readable format. at the carmen platform, users can define metadata templates to ease the manual metadata entry for similar data sets. at the (see text footnote 1) database, data, and metadata upload is possible"
"where l is the number of legendre polynomials. in addition to the haar wavelet and legendre polynomials, other bases are possible to model the signal or scene of interest. for example, the discrete cosine transform can help to model the stokes parameters in channeled spectropolarmetry. 13 future research can investigate other wavelets and bases which may better represent natural scenes."
"instead propose a \"bottom-up\" approach aiming primarily at the scientific work in the laboratories. our goal is the convenient, semi, or fully automated handling of metadata that can be embedded into the laboratory workflow and is thus of direct use for the scientist. at the same time we want to foster interoperability by providing a possibility to apply \"standards\" enabling data exchange between tools or via data sharing platforms. thus, we aim at (i) a format in which arbitrary information can be stored, and (ii) mechanisms to apply conventions regarding the content. the scope of these conventions may vary from local, laboratory needs, to a global community scope. accordingly, our approach has two parts. the first is the rather simple open metadata markup language (odml) format or (meta)data model: in odml so called properties are grouped in sections resulting in a highly flexible hierarchical tree structure. this structure is to some extent similar to the way data annotations can be handled in the hdf5 5 data format. hdf5 also contains a hierarchical structure of nodes which can contain data or attributes for data annotation. nodes and attributes are similar to our sections and properties, but there are distinct differences between these formats which are discussed below (see section 5). the second component of our approach is to provide terminologies for definitions of properties and sections. the terminologies can be used to guarantee interoperability between tools. at the same time it is possible to immediately provide additional definitions for new properties and sections. thus, odml offers the required flexibility and freedom to store any information that is necessary to describe a given dataset while supporting interoperability by using it in combination with specific terminologies. given its acceptance, this \"bottom-up\" approach can lead to a community driven definition of global terminologies and general models of metadata in neurosciences."
"the section defines two further elements, link and include. these are related and are introduced to allow inheritance and to distribute information across multiple files, respectively (see \"relations outside the hierarchy\" for more information)."
"when describing hardware items we distinguish the properties of the item from its actual settings. properties describe the fixed characteristics of a hardware component (e.g., the \"model,\" \"manufacturer,\" or the maximum possible sampling rate of an analog input \"aimaxsamplerate,\" etc.), whereas settings are the actual settings of adjustable features of the device (e.g., the currently used sampling rate \"aisamplerate\"). accordingly there are two terminologies containing hardwareproperties or hardwaresettings typed sections. these do not define any properties of their own but are meant as containers for respective hardware sections. this separation allows the same name (e.g., \"lowpasscutoff \") to be used as a hardware property for an amplifier with a fixed filter as well as a setting for one with an adjustable filter. using the same section names in both contexts (properties and settings) relates them to the same entity (figure 3) . the separation is not required but we consider it helpful when describing setups and hardware."
"a link is used to refer to sections within the document and contains a path in the tree. paths defining the position in the tree are given by section names, separated by slashes (\"/\"). paths are absolute, i.e., they begin at the root of the tree. to illustrate the use of links, we consider a case in which a number of datasets have been recorded using a rather complex stimulus. the stimulus is repeated for each dataset, but each time a single stimulus parameter, e.g., the intensity, has been changed. it would be valid to provide the full stimulus description for each dataset individually but, obviously, this would be cumbersome and inefficient. instead, one defines the stimulus once within the document and then links to it for each dataset. the referring stimulus sections contain only the changes (i.e., the \"intensity\" properties). links can only exist between sections of the same type and include all subsections and properties. local information, given in the linking section, overrides items inherited from the linked section."
"(2) the tree structure defines three basic relations between nodes (i.e., the sections in odml trees) are defined: (i) parent, (ii) child, and (iii) sibling relationships. in odml these relations have different precedence. the child relation (subsections, and their subsections) is strongest. the strength of the relation between nodes descends through siblings (sections that have the same parent) to parents and their siblings. more distant relations are not considered. for example, if a dataset contains data recorded from a single cell this dataset is related that cell. in the odml tree this relation can be expressed by having the cell section being child of the dataset section. if there several datasets originating from the same cell, the dataset sections could be children of the same cell section. siblings are treated equally and without order. (3) two instances of the same section type that logically belong to the same super-section (e.g., two recorded cells that contributed to one dataset) have to be siblings on the same branch of the tree. (4) if some metadatum is described by a combination of subsections these compositions must be \"pure\" in the sense that all subsections must be of, or derived from, the same type. for example a stimulus description can be a composition of stimulus-derived subsections (see figure 4) . (5) the hierarchy should be kept as flat as possible. (6) there are some restrictions on the style in which property and section names as well as section types are given. all must begin with a letter and are then treated case insensitive. in case of section names and types, slashes (\"/\") are reserved to separate paths in the tree, respectively type components (e.g., \"hardware/ amplifier\"). by convention the properties and sections are given in \"upper camelcase\" while section types are all lower case."
"in this paper, we demonstrate a completely pedestrian detection system. we proposed a method can detect pedestrians which are fast and accurate at the same time by using principal components analysis to reduce dimensional of hog therefore speeding up the classification time and training time as well. our system speed up pedestrian detection system about 25 times compare to original hog/linear svm system. we also find out that using 100 principal components can give a best trade off option between detection rate and processing time. our method can execute require less resources than original hog feature; it will have advantages when apply to low resource devices such as mobile phone. in figure 8, we show some typical false positive detection of our system, the most errors occur in window images which have strong vertical structure. we also show some our true positive samples in figure 9 with different situations such as different views, different detected scaled windows or crowed image. for future works, we are going to apply cascade approach and boosting for feature selection to further speeding up our system."
"using odml should leave all options to the user. the only real restrictions imposed are almost trivial: any section must be of a specified type and should be named. any property must have a name and a value. optional fields offer further information: for numerical values a unit might be necessary and, if appropriate, a value definition that points to a resource that defines the value (for an example see table a4 in appendix). for neurophysiological datasets we strongly encourage using the terms as defined in the terminologies (see text footnote 6). however, the approach should be pragmatic:"
"note that we can express the model as a single matrix-vector product by concatenating the basis coefficients into a single vector. 13 the likelihood, regularizer, and cost functions are defined as before:"
". the g-node data management system 11 supports odml as a format for metadata import and export. furthermore, it is planned to integrate odml support into the vision egg [cit], a free tool to generate and present visual stimuli."
"also have subsections. thus, a tree-like structure can be built up by nesting sections. the rootsection, finally, constitutes the root of the tree that embraces a set of sections. this kind of tree-like data model is rather generic and thus offers the required flexibility but also has some disadvantages (see section 5)."
"the hardware terminologies provide section and property definitions for describing hardware that was part of the experimental setup. there is a range of hardware related terminologies to describe specific hardware items. all defined hardware terminologies are specialized versions of the \"hardware\" type. the type of a section to describe an amplifier is then \"hardware/amplifier.\" with this construct a reading tool that may have no concept of an amplifier"
"the main design goal of odml is its immediate extensibility. any metadata item can be included into a valid odml file, no matter whether it is already defined in a standard terminology. at the same time no information is mandatory. with odml, the only restriction is the metadata model, not its content. a big advantage of this approach may be the future possibility of machine-aided construction, extension, and refinement of ontologies. analyzing the structure and terminology of the metadata provided by a large number of scientists from a given neuroscience subfield may enable a \"bottom-up\" development of ontologies for this subfield, which may be an efficient complement to the \"top-down\" approach of gathering experience and contributions from selected experts. in order to have the necessary metadata in machine-readable formats in the future, it is time to start collecting them in the laboratories now."
the dependencyvalue further specifies the dependencies of this property. it can restrict the dependency to the case in which the property referred by the dependency field assumes the very value given with this field (see table 2 for an example). can be given only once -
"the include element can be used to establish relations to sections (same type) that are located in an external file. include entries can be either an url or a path in the file system (relative or absolute). the url is followed by a hash symbol (#) and the absolute path of the target section. for example, a stimulus section could contain an include element like \"stimulus-metadata. xml#mystimulus.\" this indicates that the stimulus information is provided in a stimulus-type section of the name \"mystimulus\" located in the \"stimulus-metadata.xml\" file in the same folder in the file system. of course care has to be taken if the data is shared when local files are referenced. as for the link element the locally provided information overrides the one given in the included section."
"the \"powerspectrum\" function sketched in listing 2 takes the data, the respective metadata section and a constant. for the performed tasks it needs to extract some information from the metadata. lines 6 and 7: for this, the library is asked for a related section of the specified type. for example, the rate with which the data is sampled should be found in a property called \"samplerate\" that is part of a hardware/daq (data acquisition) type section. in odml there are no strict rules where this section is located in the metadata tree. the library tries to return the section with the strongest relation according to the rules defined above. if this operation fails the code will raise an exception which is caught by the surrounding try-catch clause. line 14: here, a section is created that will contain the analysis metadata. line 15: setting the repository url indicates that the terminology defining a section of this type can be found at the specified location. line 17: this line uses the matlab function \"mfilename\" to find out the actual function name which is inserted into the method-property. line 18: an instance of the java.util. date class, which contains the current date, is passed to the date-property."
"size of cells is 8x8 pixels. for each cell, we compute the histogram of gradient by accumulating votes into bins for each orientation. votes could be weighted by the magnitude of a gradient, so that histogram takes into account the importance of gradient at a given point. a gradient orientation around an edge in se should be mor uniform region gradient strengths of im"
"to document several datasets recorded in different recording sessions, it would be possible to have a number of recording sections in the same metadata file. in this case all dataset sections would need to be children of the respective recording sections (rule 1)."
"we consider a simple variant of the alpha-decay use case in this paper. the investor has an alpha that is composed of two signals: (a) a short-term signal that better predicts the returns at the end of the rebalancing time horizon but decays quickly, i.e., the signal in the next rebalancing period is very different to the one in the current period, and (b) a long-term signal that has more memory, i.e., the signal in the next rebalancing period is similar to the one in the current period but is poorer at predicting the realized returns at the end of the rebalancing period than the short-term signal. a short-term investor who only chases the short-term signal would incur high turnover as he rebalances his portfolio over time. this will eat into his returns. the long-term investor who only chases the long-term signal is giving up the valuable alpha in the short-term signal. so the dilemma faced by the investor is this: how do i trade-off the strength of the short-term signal and the slow-decay of the long-term signal while satisfying all my other mandates? we consider a simple two-stage multi-period model that is embedded in a rolling-horizon backtester to address this question. the backtester solves a two-stage multi-period model in each period and returns the first-stage portfolio as the final holdings. these holdings are then rolled-forward using the actual realized returns and a new two-stage multi-period model is solved in the next period and so on. to avoid any confusion, we will use periods to represent the different time periods (iterations) in the backtest and stages to represent the different rebalancings in a point-in-time multi-period model. our model is inspired by the work in garleanu & pedersen [cit] but we also consider realistic portfolio constraints in each stage of the model. we must emphasize that one needs an optimizer to solve a multi-period model with realistic constraints. the model no longer admits a closed form solution and also specialized approaches such as dynamic programming no longer apply. there has been a lot of other work in the alpha-decay case. israelov and katz [cit] develop an interesting informed trading heuristic that uses uses the short-term alpha signal to time the portfolio's trades; specifically, they continue with a trade only if both short-term and the long-term components of the composite alpha signal agree on the direction of the trade and otherwise cancel it. they test their heuristic on a realistic example and a simulated example with promising results. grinold [cit], and sneddon [cit] each solve a static auxiliary problem that trades-off signal strength and time decay to determine the signal weights. these weights are then used in a single-period model to generate the portfolio holdings."
"4. comparing (17) and (18), we show in the technical appendix that one can recover the first-stage solution of the multi-period algorithm in a single-period backtesting framework if we choose the single-period tc aversion and short-term and long-term weights asδ"
"note that we run a frontier using the single-period and multi-period approaches by varying the relative weights on the short-term and the long-term signals. the upper exhibit of figure 2 presents a scatter plot of the maximum sharpe ratios obtained using the two approaches with the mi objective (case 1) for the 100 seeds. note that for each seed, the maximum sharpe ratio is the best sharpe ratio obtained over the signal weight frontier, where we vary the weight on the long-term signal from 10 − 100%. we have also included the 45 degree line as a reference in this exhibit. all dots above the 45 degree line represent scenarios where the multi-period setup returned better maximum sharpe ratios than the single-period setup. by comparing the best sharpe ratio of both approaches across the frontier, we want to see whether we can recover the multi-period solution in a single-period setting. we see that most of the dots are clustered across the 45 degree line indicating that both frontier backtests produce similar maximum sharpe ratios. this is not surprising given that the model in case 1 is very close to the unconstrained models that we considered in section 4. there we made the case that it is possible to recover the multi-period solution in a draft multi-period portfolio optimization with alpha decay axiomadraft single-period framework by appropriately changing some of the parameters in the singleperiod model -notably the weight on the short-term and the long-term signals. loosely speaking we can interpret the result in the upper exhibit of figure 2 as follows: in a simple strategy, if a portfolio manager is good at picking the alpha weights, then they can recover the performance of multi-period backtest in a single-period setting. the lower exhibit of figure 2 presents a scatter plot of the average sharpe ratios obtained using the single-period and multi-period frontier backtests with the mi objective (case 1) for the 100 seeds. this plot tells a different story; most of the dots are above and some are well above the 45 degree reference line indicating that the multi-period backtester is able to deliver better sharpe ratios on average. loosely speaking we can interpret these results as follows: if a portfolio manager is average at picking the alpha weights, then the multi-period setting with its inbuilt alpha-decay feature is more likely to deliver a better realized performance. the upper exhibit of figure 3 presents a scatter plot of the maximum sharpe ratios obtained using the single-period and multi-period frontier backtests with the turnover constraint and enforcing leverage (case 2) for the 100 seeds. about 70% of the dots are above the 45 degree line indicating the multi-period backtester has a better realized performance when more realistic and complicated combinatorial constraints are present in the model. in this case, it is more difficult to achieve the multi-period performance in a single-period setting by just varying the weights on the alpha signals. the lower exhibit of figure 3 presents a scatter plot of the average sharpe ratios obtained using the two approaches for case 2 over the 100 seeds. almost all the dots are above the 45 degree line. moreover, a greater proportion of these dots are above the 45 degree line than in the lower exhibit of figure 2 indicating that on average the multi-period backtester is more likely to deliver a better sharpe ratio with a more complicated strategy."
"we propose to use a generic data model for the metadata. the nested tree-like organization, in our opinion, is easily comprehensible and flexible. it further has a very limited number of structural elements. an alternative would have been a fully defined data model with clearly defined terms and constructs, as used in most annotation approaches [cit] . such a design has clear advantages, for example, a guaranteed structure with detailed validation options. however, in our view it also has severe disadvantages: (i) it is hard to foresee what entities any one scientist might need to describe and (ii) using a completely specified model means that the user has to internalize all of its elements and dependencies and to accept the designated terms defined in the model. the odml data model itself does not impose standards. the terminologies introduce options for standardization and validation in a \"soft\" fashion which may be overridden by the user. this bears the risk of inconsistencies and leaves responsibility on validity to the user. however, this loose standardization is the key for flexibility and immediate extensibility, which we consider essential for practical application in the laboratory. restricting as little as possible may be the key to convince researchers of annotating their data to allow reproducibility and support data sharing."
at the beginning of period t. we use (14) to derive the expression of the expected second stage alpha in the multi-period model. we hold the first-stage holding w 1 throughout period t.
"the key aspect of our approach to metadata handling is a common format for both the actual metadata and terminologies. this allows for flexible storage of any metadata, since new keys (property-names) can be immediately added, without the need to extend a terminology or schema beforehand. terminologies guarantee interoperability and are built-in a bottom-up way by the scientist that work with the data. with odml we provide a format and tools for automated metadata handling, so that the threshold for collecting metadata is considerably lowered. we hope that the flexibility of odml will convince scientists to embed metadata handling into their recording, data analysis, and management tools, thereby laying the foundation for large-scale collaborations, in particular in the neurosciences."
"this section describes the realistic backtest that we used to compare the multi-period and single-period algorithms. the short-term and the long-term alphas are obtained from two independent exponentially weighted moving averages (with a half-life of 0.3 months for the short-term signal and 6 months for the long-term signal) of the 24 month forward-looking s&p 500 realized returns plus noise. they are then standardized to z-scores. we generate 30 different series of the short-term and long-term signals by varying the noise. table 2 the short-term signal better predicts the immediate returns than the long-term signal but its predictive power decays rapidly as we increase the number of lags. in particular, the short-term alpha loses its predictive power within two months. the autocorrelations for the short-term and long-term signals are 0.1 and 0.9, respectively."
"the basic idea of the odml approach is to combine a rather general data model with domain specific terminologies. independence of format and content offers a maximum of flexibility. the terminologies introduce the basis for standardization that, however, can be ignored or extended when necessary. for example, if the terminology does not define a term that is needed for annotation, it should be possible to instantaneously add new terms with their respective definitions. the odml model is designed for both use cases: (i) exchanging metadata and (ii) definition of terminologies. hence, the format contains more elements than are needed for either case alone. using the same format for both the actual content and definitions (the terminologies) may seem confusing at first, but in our view is the key for granting immediate extensibility required to satisfy the ever-changing scientific needs."
"2. the short-term and long-term alphas are generated from two independent exponentially weighted moving averages (ewma) (with appropriate half-lives) of the realized returns plus noise. the ewma construction proceeds backwards in time so that the alphas can explain future returns. the short-term and long-term alphas describe a first-order autoregressive ar(1) process, respectively."
"we will represent the stokes parameter s i in terms of coefficients from the haar wavelet and legendre polynomials. the haar wavelet helps to capture sharp transitions, though it does not compactly represent low order polynomials. the legendre polynomials are an orthogonal basis that help to model signals such as linear, quadratric, and cubic polynomials. let p n be the nth polynomial basis vector:"
"1. consider a two period model where one maximizes the two-term alpha subject to constraints that impose upper bounds on risk and tc in the two stages. in this case, one can use the duality theory of convex optimization (see the technical appendix for details) to rewrite this problem as (15) where γ and δ are the optimal duals to the risk and tc constraints. these duals will vary over time and so an equivalent single-period framework would have to dynamically change the tc aversion and more importantly the short-term and long-term signal weights over time."
an image gradient is a directional change in the intensity or color in an image. image gradient are less susceptible to lighting changes. the gradient of an image has been simply obtained by filtering it with two 1-d filters:
"in our laboratory, odml is used to transfer metadata from the recording program that automatically writes metadata to disk (relacs by jan benda) 9 and our data management tool (the lablog by jan grewe)"
"we consider a portfolio construction model with a composite alpha signal that is composed of a short-term and a long-term alpha signal. this is a classical problem in investing and alpha signals that can be classified as short-term or long-term are already part of the arsenal of most quantitative researchers. we develop a simple two-stage multi-period model that incorporates such an alpha model and generates a more informed first-stage decision with the available information. the first-stage decision also incorporates a wait-and-see feature since this decision is aware of data and decisions beyond the rebalancing time horizon through the second stage in the multi-period model. we embed the two-stage multi-period model in a rolling-horizon backtester and compare this algorithm with the traditional single-period backtester on a simulated example from israelov & katz [cit] and also a sizable, more realistic strategy. we show that the multi-period algorithm generates portfolios with a better realized performance in both sets of experiments. it is clear that the multi-period algorithm implicitly favors the long-term signal in the optimization and one can attempt to reproduce this in the single-period backtester by assigning more weight to the long-term signal. this is why we generated frontiers for the single-period and multi-period approaches by varying the relative weights on the short-term and the long-term signals. by comparing the best (over the frontier) sharpe ratio of each approach, one can see how close one can get to the multi-period portfolio performance by using the single-period approach. we observe that with the more complex, more constrained strategies, even the best sharpe ratio with the single-period model is generally not that close to the best sharpe ratio obtained via the multi-period model. this indicates that one cannot recover the multi-period portfolios by choosing a fixed adjustment (through time) to the signal weights in the single-period approach. one can still hope for a time varying adjustment of the single-period signal weights that would allow a single-period model achieve the benefits of the multi-period approach. however, the complexity of finding such weight adjustments in realistic strategies makes this impractical."
"an odml-terminology is specified by an odml file that provides definitions of sections and properties. usually, there are separate terminologies for each defined section type and provide the names, definitions, units, and data types of properties and values. all of this information can be overridden by the user, if necessary. the existence of a possibly large number of terminologies and contained terms does not imply that all these terms must be specified in an actual metadata file. the odml approach never requires any information to be provided by the user. thus, all terms are suggestions that should be used when appropriate, but are not mandatory (see \"using odml\" below)."
"note that f is the focal length of the objective lens, λ is the period of the polarization grating, m is the diffraction order (equal to ±1), and t is the separation between polarization gratings. a key aspect of this technique is to accurately reconstruct the stokes parameters from the modulated image. we will discuss reconstruction algorithms to accomplish this task."
(b) signal strength component -this component is the sum of the markowitz portfolios for the short-term signal (trading-off risk and short-term return) over the two stages. it emphasizes the better predictive power of the short-term signal.
"due to th the images, it histograms are the neighbore among a group tried with mu sizes in the o horizontal (1 including both that 2x2 block a norma and all histogr to this normal been performe single feature possible for a block. the no schemes:"
"where i(u, v) is the inverse fourier transform of i(x, y), denotes convolution, and u and v are fourier transform variables corresponding to x and y, respectively."
(in the form of an api) that bring data and metadata again closer together while ideally being independent of the actual format in which data and metadata are stored.
"for convenience, we refer to this approach as fourier reconstruction (fr). the first step is to take the inverse fourier transform of the measurement to obtain an interferogram:"
"many researchers tried to reduce computational cost of pedestrian detection system. [cit] integrate the cascade of rejectors approach with hog features to speed up detection system without losing performance. adaboost is used for feature selection. they reduce processing time in classification time by adapt hog with a faster classification algorithm. however, in our method, we follow another approach by optimizing hog feature its self using pca. applying pca to hog feature in entire image has additional advantages. first, pca will help reduce significantly the dimension of hog. hog is a high dimensional vector (3780 dimension), and hence it causes long processing time for extracting and classification. after pca steps, time consumption for extracting and classification will be reduced. second, because our training images are taken from natural scene images, there is variety of backgrounds; it will lead the noise and redundant information in final hog feature. these noises and redundant information will be gotten rid through pca steps and keep only the most principal information of human shape."
"the likelihood term l( s 0, s 1, s 2 ) minimizes the error with measured data. the regularizer term r( s 0, s 1, s 2 ) promotes sparsity in the basis coefficients s 0, s 1, s 2 . when the regularizer weight β is increased, the solution favors more sparse solutions. as noise increases, increasing β helps to improve robustness to noise, and it is possible to perform simulations of reconstruction accuracy versus noise to tune β."
"the notation is similar to reconstruction in one dimension as described in section 2.3.1, except the variables are defined along wavenumber. let s 0, s 1, and s 2 be the stokes parameters along one spatial dimension:"
(c) time-decay component -this component is the sum of the markowitz portfolios for the long-term signal (trading-off risk and long-term return) over the two stages. it emphasizes the slow decay of the long-term signal.
"save the stokes parameters as the ith rows of s 0 (x, y), s 1 (x, y), and s 2 (x, y). 11: end for 12: output: stokes parameters s 0 (x, y), s 1 (x, y), and s 2 (x, y) algorithm 2.2 describes the proposed algorithm for compressed channeled linear imaging polarimetry. the input to problem (2.3.21) is a row from the modulated measurement i measured . we form the matrices m cos and m sin using an estimate of the phase of the optical system as described in eq. (2.2.7). note that we estimate the phase from a reference measurement, and we can pull rows from the phase estimate as we recover the goal of these algorithms is to reconstruct the stokes parameters from the modulated measurement of the channeled linear imaging polarimeter. cclip takes the same inputs as fourier reconstruction and produces the same output."
"the odml-terminologies are no replacement for ontologies but can support ontology development. for example, currently there are no ontologies for setup, hardware, or stimulus descriptions. our proposed method for specifying metadata could in turn provide an efficient method to support these developments by analyzing the metadata that scientists actually use in certain contexts."
"market-impact term in the objective, this market-impact term is applied to both stages of the multi-period model -case 2 has a round-trip turnover constraint that is applied to both stages of the multi-period model. moreover, in case 2, we also fix the leverage of the dollar-neutral portfolio to the reference size, i.e., the long and the short sides of the portfolio sum up to the reference size of the portfolio."
"the elements of the odml metadata model are derived from the requirements: we need a metadata model that offers a flexible structure, can take various kinds of metadata, offers the means to ensure interoperability, and can be used for carrying metadata and for defining terminologies, while respecting conventions of the various scientific communities through customization. figure 2 shows the data model as an entity-relation diagram. a tabular description can be found in the appendix. we will start the description of the model with a coarse description of the structure and then go into more detail of the defined elements."
"algorithm 2.1 describes the proposed algorithm for compressed channeled polarimetry in one dimension. more generally, we refer to the optimization problem from eq. (2.3.21) as compressed channeled polarimetry because it can be adapted to many types of channeled polarimeters. for example, the general algorithm recovers the spectrally resolved stokes parameters for channeled spectropolarimeters. 13 in this case, the phase is a one dimensional function of wavenumber, estimated in a similar way to eq. (2.2.7). the matrix m support differs slightly in terms of which bases are used, though we solve for the basis coefficients from a similar optimization problem as eq. (2.3.21), as we will discuss in section 2.3.3. in this work, we use it to reconstruct the stokes parameters for each row of the output from a channeled linear imaging polarimeter. our innovation is to formulate reconstruction for channeled polarimeters as solving an underdetermined problem where the unknown stokes parameters outnumber the measurements."
"1 other designs are possible. figure 2 .1 shows the optical setup. this instrument produces a modulated image at the output. to simplify our analysis, we assume that the intensity modulation varies in the x direction. this analysis can be generalized for modulations occuring in arbitrary directions. the output is described by"
"pedestrian detection has many application in our life, the applications include robotics, entertainment, surveillance and advanced driver assistance systems. detecting people in images is the difficult task in computer vision because there are many varied situations such as changes of appearances (difference clothes and color, changing size), wide variety articulated human poses, the unconstraint illumination and viewpoint, etc. due to these important and challenge, pedestrian detection has attracted an extensive amount of interest from the computer vision community over the past few years."
our pedestrian detection system has two main phases: training phase and detection phase. the overall of system is showed in figure 1 . detailed steps of two phases will be described in the next paragraphs.
"frontiers in neuroinformatics www.frontiersin.org there, one can use the carmen_mini terminology http://portal.gnode.org/odml/terminologies/v1.0/carmenmini/carmen_mini.xml which defines the mapping between the mini and the standard odml-terminologies. thus, transferring data between the carmen database and an odml compliant tool is directly possible. figure 6 shows how the mapping works. the odml tree on the left side is created with the terms defined in the carmen_mini terminology. the sections and properties carry mapping information (the urls) to respective sections and properties in the standard odml-terminologies. the odml-libraries (see below) can use these mappings to convert the tree to one that complies the odml-terminologies (right part of figure 6 ). in cases in which a property does not provide mapping information (\"speciesidentifier\" property in the \"subject\" section) it kept as is and is mapped into the its parent's counterpart. the same principle applies for sections that do not provide mapping information. generally mapping information is provided in the terminologies. thus, it is not necessary to provide them with the actual metadata files. in case of a conflict between the provided mapping and the one given in the terminology, the one in the actual metadata file overrides the terminology mapping."
"the short-term alpha is designed to forecast returns over a short-time horizon; over this short horizon it forecasts returns better than the long-term alpha. however, the short-alpha signal decays quickly; the daily decay rate is 1 3 for the 10 assets. the long-term alpha signal is designed to forecast returns over a longer time horizon with a decay rate of 1 261 . figure 1 plots the autocorrelations (250 time lags) of the fast and the slow moving alphas for asset 1. this plot shows the fast (slow) decay of the short-term (long-term) alpha signals. the autocorrelation plots for the remaining nine assets are similar."
"the dataset terminology is a single section of the \"dataset\"-type. it defines properties that can be used to provide general information about a recorded dataset ( table 1 ). the name of the section identifies this particular dataset. here a dataset is understood as a set of files that belong together, i.e., have been recorded in the same recording session. the \"file\" property is of special interest. it occurs twice: (i) with the data type \"url\" it can provide the location of a file associated with this dataset and (ii) with the data type \"binary\" the file content itself can be included in the metadata. even though odml is meant to carry data about data, it is nevertheless possible to include binary content directly into an odml file. in case binary content is included, the filename element can provide a file name that should be used when extracting the data from the odml file. generally, we recommend not to include the content of a file but to use the corresponding url property instead. there can be several files related mation should be provided with the reference element of values and sections. besides numerical or textual metadata, including binary content like the picture of a recorded cell could be necessary. binary data can be included in two different ways: (i) in form of an url of the file location or, (ii) directly by including the binary content. in the latter case one can use the value's filename element to note the file name to be used when writing the binary content to disk. when binary content is included use the value's encoder element to state the encoder used. the checksum element takes the checksum information to verify a file's integrity. a checksum should be given in the format: \"algorithm$checksum\" (e.g., \"crc32$b84892a2\" for a checksum calculated with a \"crc\" algorithm applying a 32-bit polynom). the property further defines the dependency and dependencyvalue elements to allow content validation or the adaptation of tools. with the dependency it can be stated that this property is only meaningful if also the dependency is present. dependencyvalue further restricts this in a way that the required property should assume a certain value."
"in odml the hierarchical organization of the metadata tree plays a central role in defining which metadata belong together, e.g., which sections are related to the same dataset. regarding the organization of the metadata some simple rules should be obeyed:"
"many data formats like the exif format for image metadata 14 or id3 tags for music annotation 15 use pre-defined key-value pairs for data annotation. this approach is especially useful for cases in which the type of data that requires description and thus the required metadata terms are known in advance. the situation is more difficult when describing scientific data which is variable in many respects. the odml approach of storing metadata in the hierarchical way is to some extent similar to the way data annotations can be handled in other formats. hdf5 specifies the tree structure of nodes. these can contain attributes which are essentially key-value pairs. nodes and attributes are thus similar to sections and properties but lack the opportunity of standardization. the \"scientific data set\" extension of hdf5"
"3) and (2.2.5). finally, we extract s 1 (x, y)/s 0 (x, y) and s 2 (x, y)/s 0 (x, y) from c 1 (x, y):"
"the following paragraphs exemplify how to use odml and how to organize the metadata according to these rules when describing electrophysiological data. figure 5c ). these situations are briefly described in the following paragraphs. as metadata is data about data, the linchpin of odml files discussed here are the datasets, that is the dataset-type sections."
"in general, the state-of-the-art reconstruction for channeled polarimeters uses fourier reconstruction, which performs a fourier transform to separate the modulated data into channels and recover the stokes parameters. in contrast, our algorithm solves an optimization problem to recover the stokes parameters, and in our framework, reconstruction in channeled polarimetry is an underdetermined problem. to provide insight into how this algorithm can be applied to other forms of channeled polarimetry, we provide a short summary of how we applied this approach to channeled spectropolarimetry."
"one issue with multi-period models is that they require forecast information beyond the rebalancing time horizon that is subject to greater uncertainty. a second issue is that the resulting model is complex and requires greater solution time. despite these issues, multiperiod models have generated a lot of interest in the financial community since they attempt to model the future dynamics of the portfolio taking into account data and decisions beyond the rebalancing time horizon. this leads to better solutions implemented today than would be possible with a myopic single-period model."
"for databases and data sharing platforms in the neurosciences to be useful in the long run, machine-readable data annotation should not require additional manual effort when data are uploaded to a database. instead, data annotation would be ideally integrated within the data acquisition and analysis workflow in the laboratory. this would have the further benefit of facilitated data management and data analysis for the individual scientist, independent of whether the data are uploaded to a data sharing platform or not. moreover, scientists may be much more willing to contribute to data sharing initiatives if the upload is just a single command or button click because the metadata already exist in a machinereadable form and do not have to be entered manually. odml provides a format that enables this computer-based metadata management and exchange from the local laboratory to global neuroscience databases."
"in this paper, we aim to describe an effective pedestrian detection system. firstly, we extract hog from input image, then using principal components analysis to reduce the dimensional of hog, it can help we reduce computational time and resources consumption, it can help speed up the training phase, especially the classification phase. next, linear svm is used as training and classification tool with input is the output of pca step. using linear svm for simplicity and speed, the experiment still is able to get very excellent results. non-linear svm can give a slightly better performance, but we have to trace off with the computational resources and processing time. our system can archive the same performance with original hog feature of navneel dalat and bill triggs with both mit [cit] and inria pedestrian dataset ( [cit] ) while the processing time is reduced. http://dx.doi.org/10.5392/ijoc.2013.9.3.001"
"after shifting detector window at all scale and all locations of image, we will obtain many detected window candidates which include human (figure 4) . in order to obtain the final detected windows, we have to evaluate all detected window pairs. without loss of generality, we can denote two detected windows as and ., 0.5 (2) if two detected windows satisfy condition (2) we will eliminate smaller window and keep the bigger one for the next evaluating iterations. in many cases, we will have some false detection at one or some few scales. in order to eliminate false detection, we map all top-left points of detected windows into a 2d space. then a mean-shift algorithm is used to group detected points into clusters. all small clusters, where the number of windows is less than a threshold, are considered as noises. we remove all these noisy windows from the detected window list."
"in the following paragraphs selected terminologies will be introduced to point out some conceptual aspects. a list with all currently defined section types can be found in the appendix (table a6 in appendix). generally, electrophysiological data are recorded from a certain preparation of a subject by an experimenter in a recording session, using an experimental setup consisting of various hardware components with specific settings, and presenting defined stimuli. accordingly, respective terminologies to describe these experimental conditions are needed. the description provided by these terminologies is much more detailed than in most cases needed for data sharing. however, odml is intended to be also of use for metadata management in the laboratory, where all this information should be kept available. the following descriptions start with the \"dataset\" all other metadata are referring to. it is then briefly illustrated how the used hardware items and their respective settings as well as stimuli can be described using odml."
"we compare simple unconstrained two-stage multi-period (mpo) and single-period (spo) models that trade off risk, return, and transaction costs. the alpha signal for the two models are constructed from short-term and long-term alpha signals."
"annotating data may seem a costly process that requires the scientist to manually record a large number of values. however, most hard metadata are directly available and could thus be automatically recorded during data acquisition, with minimal manual intervention. further information is typically derived during subsequent processing steps, for example analyses, etc. ideally, all components of the data analysis tool-chain, from data acquisition, data analysis, and data management to data sharing, should be able to work hand in hand and exchange data and metadata in an automated fashion. the goal of odml is to provide the basic components for this automation. the basis of this \"food chain, \" on top, is the laboratory in which the data is originally recorded, stored, managed and analyzed. here metadata are important in many respects. data management uses them to categorize and organize the data, during data analysis stimulus information is required and further, derived, data characteristics are added which again may be useful for querying data, etc. data may further be shared with collaborators for discussion and re-evaluation. eventually, data may be made available via public databases like the g-node [cit] . on all levels data exchange between people as well as computer programs requires a detailed annotation of the raw data with metadata."
"we represent the stokes parameters in terms of bases from legendre polynomials and the discrete cosine transform (dct), which are combined in the support matrix m support :"
"is the target portfolio that trades off risk and return. comparing (8) and (9), we see that the optimal multi-period portfolio uses the target portfolio instead of the markowitz portfolio at stage j. moreover, the target portfolio is basically the markowitz portfolio with each factor scaled down by its decay rate."
"the interesting question is what happens to the first stage solution when one adds infinitely many stages to the model in (15). when the number of stages is very large, we can use the garleanu-pedersen result to show that the first stage solution at time t converges to"
"compared to other disciplines in biology, such as genomics and proteomics (e.g., [cit] via an xml interface. this means, however, that only the set of pre-specified metadata elements can be provided and additional information, which may be essential to meaningfully analyze the data, cannot be entered easily."
"(1) the hierarchical organization reflects the relations between sections. for example, if there is a \"cell\" section that is child of a \"subject\" section, then this cell is from that subject. the dual recording in figure 5c again yields a simple flat tree structure. regarding rules 1 and 2 the two cells could have been children of the subject section but this would \"violate\" the rule 5 of a flat tree structure."
"xml has some disadvantages as well. for example the format is not the most efficient regarding file-size or readability. there are other quite successful languages like yaml (www.yaml.org) or json (www.json.org) that can be more efficient and offer some other useful features, like a built-in support for lists, which is not supported by xml directly. our format resembles to some extent definitions made in the rdf-format (www.w3.org/tr/rdf-schema) but is much more focused on the specific uses described here. odml could be implemented in any of these languages likewise."
"3. we compare our multi-period backtester with a single-period backtester. the singleperiod backtester follows all the steps of the rolling multi-period algorithm except that it uses different weights on the short-term and long-term signals, a different tc aversion parameter, and solves the following single-period model"
"this is a common problem that is handled by large sell-side firms and investment banks. a trade scheduling problem aims to get from an initial portfolio to a target portfolio in stages while trading off the risk and market-impact across all these stages. the risk term in the objective forces the trading to happen quickly (move draft multi-period portfolio optimization with alpha decay slowly and the market will move you!). the market-impact term in the objective slows down the trading to avoid the market prices moving in an unfavorable fashion (move quickly and you will move the market!). one must consider implementation shortfall, which is defined as the difference between the prevailing portfolio price and the effective execution price for the proposed trade schedule. in a seminal paper, almgren & chriss [cit] minimize the first two moments of implementation shortfall in a markowitz meanvariance framework to determine the optimal trade schedule. they show that the first moment of the implementation shortfall models the market-impact and the second moment of implementation shortfall models risk. a similar model is also considered in grinold & kahn [cit] . multi-period models that only trade-off market-impact and time varying alpha are considered in bertsimas & lo [cit] . the market-impact is usually a non-linear power function (quadratic, three-halves, five-thirds) of the transactions (see [cit] for more details). almgren & chriss consider a quadratic market-impact model and do not include realistic constraints (such as long-only) for the different stages in their multi-period model. consequently, their model resembles the classical lqr problem from optimal control and they are able to derive a closedform solution for their model by solving an algebraic riccati equation."
2. transition management and dynamic benchmark tracking: this is a passive management framework where the aim is to track a benchmark or target portfolio closely over time while minimizing the tracking error and market-impact across several stages. some of the stages can also include potential cash flows and portfolio injections. this is a common problem faced by large etf providers who receive cash inflows and portfolios from investors over time and need to track a dynamic index closely. [cit] provide a good overview of multi-period optimization models in this area.
"we would like to thank andrey sobolev, christine seitz, christian kellner, and christian tatarau for programming and discussions, adrian stoewer, alvaro tejero-cantero, colin ingram, fritz sommer, gwen jacobs, marianne martone, piotr durka, and raphael ritz for fruitful discussions. raphael ritz and zbigniew jędrzejewsky-szmek for comments on an earlier version of the manuscript. supported by bmbf grants 01gq0802 and 01gq0801."
"the state-of-the-art algorithm for reconstruction uses the fourier transform to recover the stokes parameters by separating them into channels based on the carrier frequencies of the modulated output measurement i(x, y)."
"we have presented a reconstruction method called compressed channeled linear imaging polarimetry (cclip). in our proposed framework, reconstruction in channeled linear imaging polarimetry is an underdetermined problem, where we take n measurements and solve for 3n unknown stokes parameters. we have formulated an optimization problem by creating a mathematical model of the channeled linear imaging polarimeter with inspiration from compressed sensing. our simulations show that cclip can produce more accurate reconstructions. in particular, cclip mitigates artifacts seen in fourier reconstruction, including image blurring and degradation and ringing artifacts caused by windowing and channel cross-talk. by demonstrating more accurate reconstructions, we push performance to the native resolution of the sensor, allowing more information to be recovered from a single measurement of a channeled linear imaging polarimeter."
we briefly describe the garleanu-pedersen multi-period portfolio model [cit] that incorporates alpha-decay in this section. consider a composite alpha signal over n assets that is composed of k factors with varying decay rates. the composite alpha in stage j of the multi-period model is given by
this paper is organized as follows: section 2 introduces multi-period portfolio models. section 3 gives a brief overview of the garleanu-pedersen model that is the inspiration for our work. section 4 presents our rolling-horizon two-stage multi-period algorithm for the two alpha problem. section 5 compares the performance of our multi-period algorithm with a single-period backtester on a simulated example that is taken from israelov & katz [cit] . section 6 presents our computational experiences with the multi-period algorithm on a large strategy with realistic constraints. section 7 presents our conclusions. the technical appendix gives the mathematical details of some of our main results.
"that trades off risk, return, and transaction costs across two stages. the variables w 1 and w 2 represent the final holdings of the portfolio and ∆w 1 and ∆w 2 represent the trades made in the first and second stages of the model, respectively. the initial holdings w 0 are known. the first set of equations describe the final holdings of the portfolio in the second stage as the rolled-forward holdings from the first stage plus the trades made in the second stage. note that these equations actually couple stages one and two. the third and the fourth set of equations represent the constraints that the portfolio must satisfy in the first and second stages of the model. we will hereafter refer to these set of constraints as independent constraints since they only involve the holding and the trade variables of a stage."
this paragraph briefly describes an example scenario when using odml in the lab. for this example it is assumed that a set of datasets has been recorded and that the recording tool has written the metadata into an odml file. the following listings describe how one could use the metadata during data analysis in matlab. the underlying odml library is written in java and can be easily used in matlab.
"we will consider a channeled linear imaging polarimeter that consists of two polarization gratings, an objective lens, a linear polarizer, and a focal plane array."
"finally, sections and values can be descriptions of entities that are entries in a data management system. these entries usually have an id, or primary key, for unique identification. the reference element of section and value is meant to keep this reference information. for example, a dataset that is already stored in a database is exported and used for further analysis. if it is intended to import the respective results back into the data management system a correct linking can be easily done if the primary key is included in the reference element."
"16 offers pre-defined attributes, in addition to the \"free\" attributes. this gives control over the terms used in the attributes but requires that the user commits himself to the pre-defined terminology. in our view these solutions are not sufficient for flexible and extensible data annotations. for this we decided to keep the metadata separated from the actual data and to use a format that does not restrict the user but allows to apply the terminologies for standardization. for future development we aim at solutions"
"frontiers in neuroinformatics www.frontiersin.org table a5 ) of the value. this can be used by tools to adjust the appearance and handling of, e.g., \"integer\" or \"text\" entries."
"all content is encapsulated into a \"root section\" which contains some document-related information (table a1), and a set of sections but no properties. the elements defined in a section are shown in table table a2 . sections contain subsections and properties (table a3 ) which in turn contain values (table a4 ). table a5 lists the data types to be used in odml files. table a6 lists all so far defined odml-terminologies together with a short definition. this list, is not fixed and will grow. all terminologies can be found on the project web pages http://www.g-node. org/odml. i.e., this very property may only be meaningful if a certain other property is also specified in the same section (see table 2 for an example). the odml library or graphical user interfaces (guis) can use this information to validate the content or to adjust the gui. can be given only once -"
"on the odml website at www.g-node.org/odml we offer libraries and tools to manipulate odml metadata. so far, there is a java implementation which can be easily used with matlab. we currently work on implementations of these odml-libraries in c/c++ and python. by means of these libraries custom software can be easily extended to automatically write or read odml files. we also offer an editor to create, view, and manipulate odml files. all this software is freely available and open-source."
"tash allows finer auditory cortex segmentation than is currently possible using existing automated software, with the option of selecting specific auditory cortex subregions and combinations thereof. furthermore, given the automated nature of our method, it is particularly well suited for examining possible structural plasticity effects longitudinally in the same participants, because tash's fine and reproducible feature selection should allow for greater sensitivity to possible experience-dependent structural change over time. perspectives for new developments of tash include the implementation of regular updates and improvements to this toolbox in order to adapt and optimize its performance in the context of the changing and variable demands that will likely arise in relation to the anatomical measures required to better explain brain structural phenotypes. for example, next steps include extending tash for the automated extraction and quantification of the shape of the transverse temporal gyrus/gyri (i.e. including the number and type of gyri), in order to allow the automated exploration of gyrification differences in health and disease. moreover, given evidence for higher myelination within gray matter regions of the pac 36, 37, 99, future developments of tash can include extensions allowing the integration of data from different imaging modalities (e.g. myelin mapping 36, functional tonotopic mapping 4, 36 and diffusion-weighted mri), together with priors derived from cytoarchitectonic atlases 35, 78, with the goal of localizing pac rather than landmark-based features corresponding to hg."
"automated approaches for the segmentation of specific, small brain structures based on structural mri data (i.e. t1-weighted images) have been developed in the last years. these have mainly aimed to segment subcortical brain regions such as the caudate nucleus 41, the thalamus 42, the putamen 43 and the hippocampus 44 . these approaches have included methods based on bayesian models of shape and appearance 45, probabilistic models 46, fuzzy templates 47, large deformation diffeomorphic metric mapping 48 and decision fusion 49 . these automated methods have shown good performance for the segmentation of subcortical structures. this is likely facilitated by the fact that subcortical nuclei are constituted of closed surfaces, and thus, grey-white matter intensity contrast values can largely assist in improving segmentation performance. the automatic segmentation of structures such as hg, which lie on 'open' surfaces and are thus contiguous with adjacent structures, however, poses a much bigger challenge since the boundaries of such structures are more arbitrary and less easily identifiable using t1-weighted image properties such as grey/white matter contrast."
"quantification of results. scripts adapted from freesurfer are then used to extract measures including the volume, surface area, mean thickness and its standard deviation, mean curvature, gaussian curvature, curvature index and folding index of the hg segmentations. for extracting the above measures, the 'mri_anatomical_stats' command is used, which allows to extract grey (and not white) matter volumes. these results can then be analyzed across subjects using standard parametric statistics, non-parametric statistics, machine learning, or other approaches, depending on the goals and questions of the study. note that the above measures are in native space, and that when performing group statistics, measures such as mean hemispheric (or cortical volume, surface area, thickness, etc) need to be controlled for statistically."
"here, we present a novel method for the automated segmentation of hg. our auditory cortex segmentation toolbox, called tash (toolbox for the automatic segmentation of hg), makes use of the output of the freesurfer structural segmentation pipeline, and then implements further steps to provide a finer segmentation of hg. by default, our pipeline segments the most anterior transverse temporal gyrus, including csds when present (i.e. regardless of the length of the sulcus intermedius), but excluding fpds and further (i.e. 3 rd or 4 th ) transverse temporal gyri. we henceforth refer to this delineation as 'hg' . moreover, this pipeline is currently being adapted for alternative feature selection approaches (see discussion). we first validate the method by testing for relationships between hg volumes obtained using manual labelling and using tash, and by examining the quality of feature selection by tash, in three independent smri datasets (the 'musicianship' data) obtained on different mri scanners and at different magnetic field strengths. the purpose of validating our method across different datasets was not only to show that our validation replicates across different data, but also to show that tash performs robust feature selection independently of scanner and field strength. following the validation, we also provide two applications of our method to (a) this same, 'musicianship' data and also to (b) two additional smri datasets (the 'phonetic learning' data, also used for the toolbox development), to test for whether tash replicates previously observed relationships between hg volume and (a) language skill, and (b) musicianship. indeed, previous studies have shown, using manual labelling of hg, that people who are faster at learning to hear foreign speech sounds have a larger left hg volume compared to slower phonetic learners 9, and also that both amateur and professional musicians have larger grey matter volumes of hg bilaterally compared to non-musicians 16, 56 . we also tested for group differences in hg surface area and cortical thickness. we predicted that tash, a fully automated method, would successfully replicate these previous manual labelling findings of (a) a larger left hg in faster compared to slower phonetic learners 9, and of (b) bilaterally larger hg in professional and amateur musicians compared to non-musicians 16, 56, and that in the latter context tash would extend these findings to new data."
"the reliable and reproducible localization and delineation of hg is necessary not only in studies on the anatomy of this brain region per se, but also in order to help in the localization of data obtained from other imaging modalities such as functional mri (fmri), positron emission tomography (pet) and diffusion tensor imaging (dti) 26 . the anatomy of hg is highly variable across individuals and hemispheres, both in terms of size and morphology (i.e. of gyrification patterns) 1, 27, 28 . on average and in healthy populations, hg is larger in the left compared to the right hemisphere 27, 29, 30 . furthermore, the most common gyrification patterns include single hg, common stem duplications (csd, partial split of the gyrus by a sulcus intermedius) and full posterior duplications (fpd, two fully separated transverse temporal gyri, also known as complete posterior duplications) 6, 31, 32, but in some cases a third or even additional gyri can be present 17 . early cytoarchitectonic studies showed that the granular core comprising the pac is located in the medial portion of hg 2, 33 . however, later such work performed on larger samples of postmortem brains showed that although the medial aspect of hg consistently contains pac, the extent to which pac extends to adjacent transverse temporal gyri varies between hemispheres and brains 34, and that the mapping between cytoarchitectonic borders and gyral/sulcal boundaries is poor 35 . in other words, hg is not synonymous with the pac, but rather includes pac. 2, 33 . more recent, in-vivo myelin mapping studies have shown higher myelination values within the medial two thirds of hg, and higher myelination is thought to be one of the in-vivo markers of primary auditory cortex 36, 37 . although there is not a clear relationship between these gross anatomical features and cytoarchitecture, hg is macroscopically defined as comprising the most anterior transverse temporal gyrus, including common stem duplications when they are present, in particular when the intermediate sulcus spans less than half the length of hg 27 . full posterior duplications, when present, are ascribed to the planum temporale (pt), which includes secondary auditory cortex 38 . thus, using gross anatomical landmarks, the posterior border of hg (this being the anterior boundary of the pt) is typically defined as corresponding either to: a) the first complete sulcus posteriorly adjacent to hg (also known as heschl's sulcus (hs)), in the case of single hg, in the case of full posterior duplications or in the case of csds when the sulcus intermedius is short, or b) it is defined as corresponding to the sulcus intermedius in the case of common stem duplications where the sulcus intermedius is long 27, 32 ."
"in this paper, we present a novel, automated toolbox serving to segment hg, here defined as the first transverse temporal gyrus together with existing csds but excluding fpds, based on gross anatomical landmarks obtained from t1-weighted structural mri data only. the fully automated nature of this toolbox allows to perform reproducible macroscopic feature selection in large datasets in which information from other imaging modalities (e.g. myelin mapping, tonotopy) is not available. due to the large inter-hemispheric and inter-individual variability in the morphology of hg, the gold standard for the delineation of hg in structural mri data is manual (i.e. visually guided) labelling 9, 13, 16, 39 . this approach, however, is not fully reproducible and is thus error prone 40, and is also very time consuming."
"normative studies (i.e., in healthy controls) have revealed a hemispheric asymmetry in the size of hg, with larger volumes in the left than in the right hemisphere 27, 78, 79, especially in men 80 . we have replicated this finding using our automated tash toolbox in a large dataset, further validating our method. tash can be extended to a wide range of applications. for example, initiatives exist to study the development of the auditory cortex and of other brain structures more generally, from infancy to adulthood 23, 69, 81, 82, and to study the brains of bilingual adults 13 and children 83 . molecular genetic studies have also been done in search of genes which affect auditory cortex structural features 25, 84, and heritability studies have explored the possible contribution of auditory cortex morphology to dyslexia 85 . our method will allow to perform further such studies in much larger sample sizes using a comparable approach across datasets, allowing for more robust and generalizable findings."
"the human auditory cortex translates continuous, complex acoustic stimuli into signals allowing communication during speech perception, into musical experiences, and into cues allowing sound localization, to name a few. heschl's gyrus (hg), or the first transverse temporal gyrus, first described by richard heschl 1, is an oblique convolution located on the inferior surface of the lateral fissure (also known as the sylvian fissure), which runs transversely (mediolaterally) from near the insula towards the lateral part of the superior temporal gyrus 2 . hg includes the primary auditory cortex (pac, or 'auditory core'), this being the first cortical relay station of auditory information in the brain. although the vast majority of non-invasive brain imaging studies on the auditory cortex have focused on auditory cortex functional recruitment [cit], the last 15 years have shown a growing number of structural magnetic resonance imaging (smri) studies on the anatomy (e.g. size, shape, thickness etc) of human auditory cortex, and of hg more specifically 6, 7 . these studies have shown relationships between individual differences in hg size and morphology (i.e., gyrification) with individual differences in language skill, learning and expertise [cit], professional musicianship [cit] and reading disorders such as dyslexia [cit], to name a few. these auditory cortex structural differences could be due to individual differences in experience-dependent structural plasticity 24, and/or to differences in pre-existing, possibly innate factors 25 . more generally, such macroscopic differences (i.e. ones visible with smri) are likely to be related to differences in underlying cellular and molecular differences 24, which in turn likely partly account for the aforementioned relationships between gross anatomical differences and individual differences in behavioral skill, expertise and disease."
"delineation of full extent of auditory cortex gyri. the gyral 'crowns' identified as described in section 2.3.3 are then 'grown back', or expanded within the final expansion mask, in order to recover the full extent of each gyrus. this is done by selecting all adjacent vertices having mean curvature values below 0 within the limits of the expansion mask (panel j and green arrows in fig. 1 )."
"note that there are two differences in the way that tash extracts hg volumes compared to the way this is done during manual labeling. first, when manual labelling is performed, the gyri are selected from their upper convolution down to the bottom of the sulci that adjacently delimit them, whereas when gyri are selected within freesurfer and tash, the selection does not go all the way down to the sulcus but stops approximately mid-way between the bottom of the sulcus and the upper-most point of the gyrus. this is because of the way that freesurfer (and thus tash) defines gyri; gyri are defined as corresponding to vertices having negative curvature values. second, during manual labelling, as mentioned above, the medial two-thirds of hg was selected, while in freesurfer and tash, the entire gyrus was selected. as described above, however, despite this, due to the more liberal definition of the anterior border of the full extent of hg during manual labeling, in many cases the antero-lateral border corresponding to the medial two-thirds during manual labeling was actually similar to the lateral border of hg as considered within freesurfer, and thus also within tash (see supplementary fig. 1 ). due to the above differences between these approaches, the correlations between manually labelled hg volumes and those obtained using tash and freesurfer thus somewhat underestimate the strength of existing relationships, whereas the strength of the relationship between hg volumes obtained using freesurfer and tash should not be affected. note also that in all three methods, the grey matter volumes (surface areas, etc.) generated are in native space, making it unnecessary to control for cortical volumes when evaluating differences among these 3 labeling methods."
"applications. following the validation of tash, we applied this toolbox to the 'phonetic learning' and to the 'musicianship' data that were used to develop and to validate tash, respectively, with the goal of testing for whether hg volumes obtained automatically using tash would replicate and extend previously published demonstrations of larger hg volumes in faster phonetic learners and in musicians, respectively."
"the computational code developed, and the data generated and/or analyzed during the current study are available from the corresponding author upon reasonable request. the tash code is available at github (https://github. com/ [cit] /tash)."
"one of the most widely used software packages allowing the fully automated segmentation of different cortical and subcortical brain regions is freesurfer 54 . this software performs parcellation of cortical and subcortical regions using probabilistic classification based on two different manually labeled atlases, these being the destrieux 51 and the desikan atlases 55 . the former atlas, in particular, provides a more detailed parcellation of the auditory cortex than the latter, although it has not been tailored for the fine delineation of this brain region specifically. indeed, it labels the transverse temporal gyrus (i.e. hg), the transverse temporal sulcus and the pt among a total of 74 cortical regions that are segmented in each hemisphere, to provide a complete labeling of sulci and gyri of the human cortex. due to the high anatomical variability of the transverse temporal gyrus, and to the fact that freesurfer has not been explicitly designed to segment this region specifically, the labelling of this brain region is prone to errors. for example, the most medial portion of hg is often erroneously excluded from the transverse temporal gyrus region of interest. moreover, for the case of csds, parts or all of the second gyrus are sometimes assigned to the pt regardless of the length of the sulcus intermedius, even though according to conventional definitions of hg, the second gyrus ought to be assigned to hg in cases where the sulcus intermedius is short 27 . conversely, again for the case of csds, parts of the second gyrus are sometimes erroneously assigned to the transverse temporal sulcus label. freesurfer normally correctly assigns full posterior duplications (fpds, i.e. second and third or other transverse temporal gyri) to the pt label."
"only a few studies have attempted to develop methods to automatically segment hg specifically, using only gross landmarks from t1-weighted smri data [cit] . for example, automatic delineation of the transverse temporal gyrus has been tackled by using sulci as landmarks. the approach, developed by engel and colleagues 50, segments hg using a deformable model of folding patterns that adapts to the curvature of the cortical surface. this approach uses anatomical gyral and sulcal landmarks to parcellate the auditory cortex, and then uses pattern recognition algorithms to label the auditory cortex. however, the high inter-subject variability of sulco-gyral landmarks poses problems for such pattern recognition algorithms, and also, this approach is not fully automated 50 . another technique for the automated segmentation of hg has been proposed based on the construction of deformation-based probabilistic maps, but this approach is based on the use of a template, which biases results towards the anatomy of the selected template 53 . yet another method relies on a partially automated algorithm for the anatomical parcellation of the auditory cortex, making use of a contextual pattern recognition method that relies on rendering a deformable prototype-based recognition method, and adding basic atlas information 52 . this approach, however, relies on training a machine-learning algorithm based on manually delineated surfaces, and as such, is not fully automated and still requires arbitrary decisions to be made regarding where to place regional boundaries. therefore, there remains a need for fully automated algorithms for the segmentation and parcellation of the transverse temporal gyrus, especially in the context of studies where large sample sizes are involved."
"manual labeling studies on this topic, it had been shown that professional and amateur musicians have larger hg volumes in the medial two-thirds of the first transverse temporal gyrus (with the border of the full extent of the transverse temporal gyrus being relatively liberally defined) 16, however, in a later such study these volume differences were shown for both the medial and also for the lateral hg 56 . a larger volume of a gyrus (or of gyri, in the case of common stem duplications) is associated with a relatively larger surface area, as also shown in our results, and thus also with a relatively greater overall presence of superficial cortical layers compared to what can be expected to exist in smaller gyri. interestingly, it has recently been shown that within the primary auditory cortex (i.e. in regions likely to be comprised by hg), there is greater processing complexity in superficial cortical layers 75 . thus, a speculative explanation for larger bilateral hg in musicians and of a larger left hg in faster phonetic learners is that there may be a relatively greater presence of superficial layers in the hg of these populations, and this might underlie a capacity for better processing of complex or subtle musical and linguistic sounds these groups. it remains to be established how differences in the relative proportion of different cortical layers in gyri versus sulci 76 might relate to overall gyrification patterns and to macroscopic differences in volume or surface area observed across people with different degrees of musical or linguistic skill. these questions can be explored in future, laminar resolution functional and structural imaging studies not only of cases with single transverse temporal gyri or with common stem duplications but also of more complex gyrification patterns. there were a few differences in terms of how hg was defined during manual labeling versus within tash. as described above (see methods), in the current study the manual labeling considered the medial two-thirds of hg (i.e. 'medial hg'), whereas tash considers the full extent of hg. despite this apparent difference across methods, the way that the lateral border of medial hg was defined during manual labeling actually corresponded relatively well to the lateral border of hg as determined by tash (this was due to differences in how the full extent of hg is defined across methods, see the methods section). given, however, that during manual labeling the lateral border of the medial two-thirds of hg was defined using a fixed distance spanning laterally along the direction of hg, we expect that the correlations that we report between hg volumes obtained using tash compared to manual labelling actually underestimate the strength of the true relationship between the two (i.e. the manually obtained volumes are likely more variable than they would have been had a ratio of the distance rather than an absolute distance been used for determining the medial two-thirds of hg). also, there was a difference between the way in which the current analyses were implemented for examining the relationship between hg volumes and language/musical skill compared to those reported in the original studies: in the original studies, brain structural scans were normalized before hg were manually labelled, whereas in the current study, given that tash generates grey matter volumes in native space, hemispheric cortical volumes were controlled for statistically (note however that in order to allow better comparability with tash and freesurfer, the manual labeling performed for the validation (and applications) in the current study was done in native space)."
"we find that tash and freesurfer converge less in terms of how they segment hg in the right compared to the left hemisphere. this might be due to several reasons, one being that the most anterior hg tends to be shallower in the right compared to the left hemisphere 65, 72, 73, possibly making it more difficult for a segmentation tool that is not fine-tuned for such a small and variable structure as hg to accurately segment the region. another reason for the relatively weaker convergence between right hg segmentations with tash vs with freesurfer may be related to the fact that there tends to be greater morphological variability in the stg in the right compared to the left hemisphere in healthy populations 27, 74 . our validation dataset included data from healthy controls and also from musicians; it has been shown that also in musicians, the gyrification index tends to be higher in the right compared to the left hg, even when only considering csd morphotypes 17 . thus, again, it's possible that tash performs finer feature selection in the right hg than does freesurfer by virtue of tash being explicitly designed to perform fine hg segmentation that is robust to variations in the anatomy of this region."
"identification of hg. the location of the volumetric center of each gyrus delineated in the above step (panel j) is then determined. an automatic check for additional remaining clusters is then performed, and when found, smaller clusters than 100 vertices are eliminated. next, to identify the gyrus that is deemed to correspond to hg (i.e. the most anterior transverse temporal gyrus, including csds), we select the most anterior of the identified gyri, based on their volumetric center (panel k). this is the final step for the delineation of hg, which is saved in freesurfer's 'label' file format."
"for the development of this method we made use of two existing smri datasets 9, 59, henceforth called the 'phonetic learning' data. these included the fastest and slowest phonetic learners from two previous studies 9,59 having examined the relationship between brain structure and phonetic learning skill, totaling 41 participants (71% females; mean age: 25.9 years). one of the t1 image datasets (scanning parameters reported in 9 ) was acquired on a 1.5 t signa horizon echospeed mri scanner (general electric medical systems, milwaukee, wi), and the second t1 image dataset (scanning parameters reported in 59 ) was obtained on a 1.5 t philips gyroscan scanner. once developed, tash was then validated in three independent smri datasets (i.e. not the same as those used for developing the method), henceforth called the 'musicianship' data. the details of the musicianship data are provided in section 2.4, entitled 'validation and applications of the method', and in table 1 . after the validation of tash, we provide two applications of this toolbox to data in the contexts of (a) phonetic learning, using the data on which the method was developed 9, 59, and (b) musicianship, using the data on which tash was validated (see section 2.4). all of the data used for the development of tash, for its validation and for the applications were acquired using procedures that were approved by the respective local research ethics committees. these included the ethics committee of the medical faculty of heidelberg university, the research ethics committee of the montreal neurological institute and hospital and the 'comité de protection des personnes régional du kremlin bicêtre' . all subjects gave their informed consent for participation in the experiments."
"in the methods section, we first provide an overview of tash, followed by a description of the methods used for the validation and subsequently for the applications of this novel toolbox. data acquisition. individual subject t1 weighted structural mri data are required as input to the tash pipeline, with a recommended spatial resolution of 1.0 cubic millimeter. for good segmentation by freesurfer, it is recommended that voxels do not exceed 1.5 mm along any direction (see freesurfer user guidelines). good gray/white matter contrast of the t1 images is crucial for accurate results. initial processing by freesurfer. depending on the quality of the t1 images, if noise is identified, denoising is a recommended step that can improve segmentation and labelling by freesurfer. for denoising, we recommend use of the spatially adaptive non-local means (sanlm) filter 60 (university of jena, germany), available as a matlab based toolbox in vbm 8 (within spm 8) and in cat 12 (within spm 12) 61 . this filter removes both equally distributed and/or locally specific noise while preserving edges, which is the key information required for good grey/white matter segmentation and for subsequent cortical surface reconstruction. sanlm can also be combined with the 'patch and pixel similarity' approach for denoising 62 . following denoising (if required), the native space t1 data are processed by freesurfer's brain structural pipeline 63 to achieve grey/white matter segmentation, cortical reconstruction and parcellation of different brain regions. tash pipeline. selection and merging of auditory cortex rois. tash makes use of specific labels, or regions of interest (rois), in and around the auditory cortex arising from freesurfer's destrieux atlas parcellation 51 . the initial selection of a broader region around hg allows for the subsequent refinement of auditory cortex feature selection while accounting for the large variability in the anatomy of this region across individuals. the following cortical regions are extracted from each subject's freesurfer's segmentation using tcsh shell scripts (see panels a and b in fig. 1 ): (1) hg, called the 'transverse temporal gyrus' in freesurfer's destrieux atlas, (2) hs, called the 'transverse temporal sulcus', (3) the pt and (4) the posterior sylvian fissure, called the 'posterior segment of the lateral sulcus' . the first 3 rois are then merged to form a first 'raw auditory complex' (panel c in fig. 1 ), and all 4 above rois are merged to form the 'raw expansion mask' (panel d in fig. 1 ). as described below, the 'auditory complex' is subsequently used as a search region for locating the crown of hg (see the blue arrow stream in fig. 1 ), while the 'expansion mask' is used to delineate (i.e. to outline) the full extent of the identified gyrus/gyri from those (see red arrow stream in fig. 1 ). refinement of the raw auditory complex (blue arrow stream in fig. 1 ). in order to refine the raw auditory complex (i.e. the merged hg, hs & pt region), first, we identify all of the gyri within this region by applying a threshold to the curvature values determined by freesurfer on the gray-white matter boundary surface, thus selecting only the corresponding vertices with negative curvature values. indeed, within freesurfer, vertices with negative curvature values are locally convex and thus correspond to points on gyri, and conversely, ones with positive curvature values are locally concave, and thus correspond to points on sulci. we thus first apply a threshold of 0 to the mean curvature values in order to retain only vertices having negative curvature values (panel e in fig. 1 ). once the vertices corresponding to gyri are selected, these are submitted to an opening procedure (i.e. erosion followed by dilation) 64 (panel f). this serves to eliminate regions that are erroneously included within the freesurfer rois which contribute to this mask, and which can result in the erroneous selection of gyri which are not hg but which lie further posteriorly along the pt, including thin formations that run along the superior temporal gyrus but that do not belong to hg. the opening procedure uses a disk having a radius equal to 2 times the edge length on the vertex space. it serves to first remove three layers of vertices at the edge of the selected patch (i.e. the three external-most layers) within the 2-d graph representation, thus discarding thin formations having a width of up to six vertices, and subsequently to restore the eroded outer-most layers of hg, but this time no longer including the thin formations. this step yields the 'final auditory complex' (panel f in fig. 1 ), and includes all transverse temporal gyri that are located within the hg/pt region. fig. 1 ). within the 'final auditory complex', the crowns (i.e., parts of highest curvature) of the existing gyri are identified by applying a threshold of −0.1 to the mean curvature values of the corresponding points on the gray-white matter boundary surface, thus retaining only the vertices with a mean curvature lower than −0.1 (panel g in fig. 1 ). vertices that survive this thresholding procedure correspond to the crowns of the gyri within the hg/pt complex, on which an expansion procedure is subsequently applied (see section 2.3.5) in order to recover the full extent of the gyri within a larger expansion mask (see next section). fig. 1 ). this step serves to refine the mask within which the gyri identified in the above step are allowed to 'grow back' . as described in section 2.3.1, the raw expansion mask includes hg, hs and pt but also the posterior segment of the lateral fissure, located medially to the auditory complex (panel d). inclusion of this latter region in the expansion mask serves to ensure that tash correctly selects the medial-most portion of hg, which is often erroneously excluded in the freesurfer transverse temporal gyrus label (see introduction). the raw expansion mask is refined (see red arrow stream in fig. 1 ) following similar procedures as were used for refining the raw auditory complex. first, in order to restrict the final expansion mask to gyri, we apply a threshold of 0 to the mean curvature values in order to retain only vertices having negative curvature values (panel h). this gyrus-specific mask is then further refined using an opening procedure, again using a disk having a radius equal to 2 times the edge length on the vertex space. this provides the 'final expansion mask' (panel i)."
"other applications of our method include further exploration of relationships between auditory cortex structure and basic auditory processing 5, 12, [cit] as well as with higher-level linguistic or musical processing and expertise 8, 12, 13, 39, 89, 90 . clinical applications include extension and replication of studies having examined the auditory cortex structure in dyslexia 21, 22, 91, in deafness 72, 92, 93, in autism 82 and in schizophrenia 30, 94, 95, the latter likely in relation to auditory hallucinations 96 . our approach can also be adapted for auditory cortex feature selection in the non-human primate brain 97, 98 ."
"our replications of previous findings of larger hg volumes in relation to language learning and to musicianship using our fully automated approach further validate our method in showing that it can be used to uncover brain structure-behaviour relationships in a fully automated and replicable manner. this opens doors to numerous applications for studies where fine assessment of hg volumes (or surface area, thickness, etc) is needed, and in particular in studies where large sample sizes are involved. further, tash is a surface-based approach which makes use of t1-weighted structural mri data. as such, it relies only on macroscopic structural landmarks for auditory cortex feature selection. thus, it is complementary to functional imaging studies in terms of helping to localize functional responses in and around hg 26, and also to dti studies, where it can for example be used to determine seeds for tracking of auditory cortex white matter fibers 77 ."
"visualization of output. the hg segmentation (i.e. in the 'label' file format), can be viewed using freesurfer's visualization tools (e.g. freeview, tksurfer), where it can be overlaid onto the participant's brain structural scan. in order to also be able to open the images on other platforms, the hg segmentations are additionally converted from the 'label' to the 'tiff ' file format."
"assuming that the user queries the airfare of day (t+n) on day t, the reference prices selected at similar time is: the actual minimum price of day (t+n-7), the quoted price of day (t+n) on day t and the quoted price of day (t+n+7) on day t."
"the multipath transmission algorithm based on friend circles is shown in algorithm 1, its process is explained as follows: at a certain point, the node v c has a data packet p to send to the node v d, and it meets the node v i ."
"now, equations (1) and (2) are integrated to transfer the longitudinal and lateral direction velocities of the tracked combine harvester to the local coordinates. in order to run this tracked combine harvester in real time, the velocities in the longitudinal and lateral direction in the harvester coordinates are expressed in a global reference frame by the equation (7):"
"under the second situation, the differences between tickets in the adjacent dates are less than rmb300 with an average of rmb106.88, and the difference between the maximum and the minimum is rmb510 for the whole time period. at this period, the minimum of uif's rmse is 125.59 and the maximum is 167.30. the accuracy of uif is 41.21% higher than wma's, 54.90% than es's and 59.19% than that in last year'."
"single-path transmission algorithm in order to reduce the network overhead and ensure the security of data, we propose a single-path transmission algorithm based on friend circles. in this algorithm, only one piece of data is allowed to exist in the whole transmission process, and the data are transferred along a path and finally arrive at the destination node. in order to improve the forwarding effect, the relay nodes need to be selectively identified. therefore, we designed a transmission algorithm based on the greedy strategy. if the encounter node is a member in the friend circles of the current node, and it has more opportunities to access the destination node, it can be selected as the relay node. furthermore, if the encounter node is in the friend circles of the destination node, it is considered to have more opportunities to access the destination node. in addition, if the encounter node has more friends than the current node, it is considered to have more opportunities to access the destination node. in a word, the selection strategy of relay node is as follows: the encounter node is a member in the friend circles of the current the algorithm can only forward the data packet to an optimal encountered node which is within the friend circles of the current node and considered to have more opportunities to access the destination node. it is similar to the experiment of chain letter, that is, it can verify the six-degree separation theory to a certain extent. therefore, this algorithm has the minimum number of data copies and good transmission performance. it is suitable for the application scenario with dense distribution of nodes and high-security requirements. for example, the patients' physiological data are uploaded to the hospital in mhealth."
"(1) social features in oppnets, nodes are the smart mobile devices used or carried by people, so they have the social characteristics of people, including natural features (such as gender, age, and body mass index) and social features (such as classmate, friend, and colleague). these features can be used to describe the relationship between nodes in society, and affect node movement and data forwarding. it is proved that the smaller the feature distance between nodes is, the more their connection is [cit] . however, there are many social features of nodes. two real trace datasets [cit] and mit reality [cit] ) provide more than 10 social features, such as affiliation, city, neighborhood, research group. in these features, only a small fraction has a significant impact on the relationship between nodes, and can be picked out by their shannon entropy as follows: where x j is a possible value of the social feature f i . p(x j ) is the probability of x j . clearly, the larger the shannon entropy e(f i ) is, the bigger the impact of f i is."
"only a subset of the values from the feature pool is used to train or test the neural network modules in the pnncb. the number of these values used (or the number of features) is selected such that the ccr mean is maximized, at the same time the ccr variance is minimized [cit] as shown in fig. 6(a) and fig. 6(b) . the mean and variance are calculated for the ccr values obtained from using different spread parameters for the neural network modules. this is illustrated in fig. 5(c) . maximizing the ccr mean enhances the classifier performance and minimizing the variance guarantees that the variation within the feature set for each class is reduced."
"as you go down the list from number one to number three, the subscriber gains more control over what they can do within the space of the cloud. the cloud provider has less control in an iaas system than with saas agreement. available. 6) traditional enterprise software dealers are moving to the cloud (e.g. oracle). 7) interoperability between saas is possible using web standards (possibly hosted in cloud). 8) u.s. government has created a saas apps.gov web site. 9) paas tools make it easier to create scalable, customized saas. predictions! there will be an explosive growth in saas applications due to low cost of entry for vendors and users, ability to rapidly deploy and update capabilities, elasticity and scalability of billing. the key issues will be: 1) choosing appropriate applications for saas deployment. 2) security, reliability, privacy, integration."
"the social circles of nodes are shown in fig. 3 . here, a social feature corresponds to a certain kind of social circle, and a value of the social feature corresponds to a specific social circle."
"after the inverse deconvolution, the overlapping emitters are well separated (figs. s1 (g1-g2)), which can then be identified by finding local maxima, and their approximate positions can be estimated by calculating the center of mass. to identify individual emitters, we used a threshold to avoid artifacts caused by the dim emitters. the threshold is determined as a quarter of the average photon counts of the emitters. this thresholding process is also built into our windstorm software, but the user just needs to set the average emitter intensity of each dataset. we suggest that for those who cannot determine this parameter, they can analyze a single frame with single-emitter fitting algorithms (e.g. thunderstorm) to get a rough estimation of the emitter intensity."
"the mhealth framework faces some challenges such as the vastness of medical data, sensitivity of patient information and ubiquity of patient physiological information collection, whereas oppnets has the characteristics of node mobile, self-organization, and ubiquitous, which makes it has a good application prospect in various normal and nonnormal scenarios. in this paper, we use oppnets to transmit medical data in mhealth, which is a good solution, and can effectively divert and offload the data traffic of mobile internet. in addition, the structure of friend circles is beneficial to safely and efficiently transmit the patient's physiological parameters and medical health information, and the data compression based on the integer wavelet transform can effectively reduce the amount and size of data and is beneficial to the faster transmission of medical data. the experimental results show that the proposed scheme is effective and has good transmission performance while ensuring the safety and reliability of media data."
automatic software updates: all the software's need update and the great thing with cloud computing is that you do not have to worry for any updates and also your organization will not have any additional expenses when a new upgrade or update is necessary.
increased data safety: there is no point to worry for disk failures or a disaster at your office. all the data is stored in the cloud.
"the ticket pricing is affected by revenue management and has a lot to do with time. when the reference date is closer to the target date, the historical fares on the reference day are more associated with the fare of target day. according to this basic idea, cwts makes prediction according to the trend of airfares in the continuous nearby past. we gave the first definition of cwts here."
"in oppnets, a flooding-based routing algorithm, such as epidemic [cit], is proposed. it has the highest delivery ratio and minimum transmission delay by sending a large number of data copies along multiple paths. based on the above idea, we propose a multipath transmission algorithm based on the structure of friend circles. its transmission strategy is designed as follows: the source node forwards a copy of data to each of its friend circles, and each recipient does the same until the destination node receive the data. forwarding via the friends of the node, the data are transmitted along the multiple circles of friends, which can improve the chance of meeting the destination node as much as possible, and obtain a larger delivery ratio and a smaller transmission delay. this algorithm is suitable for the application scenarios where the data need to be transmitted quickly and widely. for example, the medical and health information that patients need is transmitted in mhealth."
"as discussed in section 1, cepstral features are used to extract the salient content from the time domain signals, and these cepstral feature vectors are then used for developing the neural classifier to discriminate between a plinian volcano eruption and maws. the following steps are taken to extract the cepstral features [cit] : every filtered signal is normalized by the absolute value of its largest sample. it is then divided by its standard deviation and the signal mean is subtracted. for a signal 'y' normalized by the maximum amplitude sample and standard deviation, we compute:"
"unlike wheeled vehicles, tracked vehicles are widely popular due to their non-linear contact characteristics between the tracks and the ground, which allows them to be operated under adverse field conditions for agricultural production, and allows turning at high speed with a small turning radius or higher steering command. under autonomous conditions, the tracked vehicle has a global positioning system and inertial sensor for providing the vehicle state and direction, but when this tracked vehicle is turning, the inertial sensor reading has measurement uncertainties. since the vehicle direction is more important for autonomous guidance and other navigation purposes, it is necessary to compensate this uncertainty of the inertial sensor measurements."
"during the moderate to high speed turns of the tracked combine harvester, the yaw rate measurement from the imu sensor will give a bias error that creates a heading drift error, which requires compensation for estimating the absolute heading. for this reason, the model given in equation (9) was used to estimate the heading of the tracked combine harvester by using a non-linear extended kalman filter. according to the given equation (9), the state vector of tracked combine harvester was defined as the equation (10):"
plinian [cit] volcanic eruptions can inject a substantial column of gas and volcanic ash into the stratosphere. the resulting ash cloud presents a severe hazard to air traffic [cit] .
"therefore, mhealth needs a secure and efficient data transmission technology. the opportunistic networks (oppnets) do not need a complete connecting path between nodes. it uses the encounter opportunity formed by node movement to realize communication in the scheme of \"store-carrying-forward, \" which has many advantages such as flexible networking, rapid expansion, and distributed control [cit] . in particular, with the development of communication technology in recent years, mobile intelligent terminals have been rapidly popularized. using these devices to network, oppnets can realize conveniently, quickly, and efficiently the sharing of content, resources, and services. the emergence of oppnets promotes the process of free communication in medical data sharing environment, expands the use range of network, and is an important part of ubiquitous communication in the future [cit] . in this paper, the oppnets is used to transmit medical data in mhealth applications. to improve the performance of transmission, and avoid the privacy issue and the selfish behavior of nodes, we propose a novel trust transmission scheme based on friend circles in oppnets for mhealth. this scheme utilizes the historical contacts and social character of nodes to construct the friend circles in order to create a collaborative and secure transmission environment, and selects a node as the relay only if it satisfies the following conditions: it is within the friend circles of the source node, and has more opportunity to access the destination node. by forwarding each packet via friends, this mechanism can prevent the strangers from participating in the transmission, and avoid significantly privacy issues and the selfish behavior. at the same time, it has high transmission performance because of the structure of friend circles."
"the fitting error for the estimated background is within ± 1.5 photons for the range. note that, the cubic polynomial fitting function is not the only choice, and other functions such as spline may also be used. the decay factor v r is calculated for each frame (fth frame), defined as the average intensity of the non-blinking area (v f ) divided by the expected temporal minima ( min e ) over the 100-frame sub-stack. to determine the non-blinking area v f, given the slowlyvarying non-blinking area and fast-changing emitters, the standard deviation of non-blinking area should be smaller than that of emitters. for poisson-distributed signals, the mean is equal to the variance. thus, we defined those pixels with standard deviation (over the 100-frame sub-stack) smaller than two times the square root of ( 1) r   (approximated as the mean uniform background) as approximation for the nonblinking area. this estimated background has shown robust performance for a wide range of image characteristics, such as various emitter density, emitter intensity, emitter size, and structured background (17)."
"experiments show that uif can get the most accurate results when cwts uses the cubic under the first situation, the differences between tickets in the adjacent dates are more than rmb106 with an average of rmb338.56, and the difference between maximum and minimum is rmb1830 for the whole time period. at this period, the minimum of uif's rmse is 127.95 and the maximum is 171.49. the accuracy of uif is 42.25% higher than wma's, 55.94% than es and 63.26% than that in last year."
"better performance: due to the fact that no programs or files are loaded on the local pc, users will not experience delays when switching on/off their computers and also the internal network will be much faster since no internal traffic will occur."
"3) infrastructure as a service -an iaas agreement, as the name states, deals primarily with computational infrastructure. in an iaas agreement, the subscriber completely outsources the storage and resources, such as hardware and software, which they need.examples of providers using iaas applications are terremark, go grid etc. (fig. 7) fig. 7. examples of providers using iaas applications,i.e, terremark, go grid etc."
"as discussed in section 1, plinian volcanic eruptions must be clearly distinguished from the noi events for the purpose of possibly using this information as early warning for commercial aircraft. a list of plinian volcano and noi events is provided in table 1 . in this initial study, to obtain an understanding of the neuralclassifier's ability to distinguish between the volcanic events and other natural events, the noi events are considered to be made up of only the maw infrasound signals. table 2 ."
"in the diagnosis and treatment of patients, we produce a large number of medical data, such as patient information, medical record, examination data, doctor's advice, etc. among them, the examination data are especially large. to get an accurate understanding of the patient's condition, various examinations are often required. in the laboratory inspection, it includes routine, biochemical, immunologic, bacteriological, and other tests; each examination contains a number of subitem checks; and each check contains medical data of a hundred fields. table 1 shows a routine biochemical test report which contains 26 inspection items, and more items are examined in some special cases. in the imaging examination, it includes x-ray, ct, b-ultrasound, nmr, and these examinations will produce a lot of images. in the case of pet-ct, each patient produces an average of 400 images, of about 2 gb. according to the report [cit], the global medical big data were 153 [cit], and is expected to reach 2314 [cit], estimated at an annual growth rate of 48%, as shown in fig. 1b . faced with such a large amount of data, the communication technology is not enough to deal with them. therefore, a compression technique is needed to reduce the amount of medical data in the network."
"the discrete-time process model is developed from the continuous-time process model, where the state space equation is obtained by integrating the continuous-time equations over the interval from t k to t k+1 . the discrete-time model is an approximation of the continuous-time model. now, the discretization of the equation (8) can be written by the following equation (9):"
"here, volcanic infrasound signals are used to develop near, real-time neural-classifiers that are capable of discriminating between plinian volcano eruptions and the other natural phenomena with overlapping spectral content. this work is an extension of the initial study that focused on discriminating between three different volcanoes that illustrated the feasibility of using neural-classifiers to classify different types of eruptive activity from their cepstral-based feature vectors [cit] . fig. 1 shows the infrasound frequency spectrum with some events that can produce infrasound."
"lower-cost computers for users: this point is one of the financial advantages of cloud computing. there is no need to purchase powerful and expensive equipment to use cloud computing since all the processing is not at your local computer but in the cloud. since the application runs in the cloud, not on the desktop pc, that desktop pc does not need the processing power or hard disk space demanded by traditional desktop software [cit] lower software costs: using cloud computing there is no need to purchase software packages for each computer in the organization, only those employees actually using an application need access to that application in the cloud."
"circles of friend definition 1 for any two nodes in the network, if their relationship metric is greater than a certain threshold, namely, they have a close relationship, then they can be called friend. it is described as follows:"
"in the section, to meet the needs of different application scenarios of mhealth, we provide two trust transmission algorithms based on the structure of friend circles."
where w ij is the relationship metric defined in formula (2) . τ is the threshold which is used to adjust the degree of intimacy relationship between nodes. f i is the friend set of node v i .
"there are three types of cloud providers that you can subscribe to: software as a service (saas), platform as a service (paas), and infrastructure as a service (iaas) [cit] there are three types which differ in the amount of control that one have over your information, and conversely, how much you can expect your provider to do for you. briefly, here is what you can expect from each type. 1) software as a service -a saas provider gives subscribers access to both resources and applications. saas makes it unnecessary for you to have a physical copy of software to install on your devices. saas also makes it easier to have the same software on all of your devices at once by accessing it on the cloud. in a saas agreement, you have the least control over the cloud. examples of providers using saas applications are 2) platform as a service -a paas system goes a level above the software as a service setup. a paas provider gives subscribers access to the components that they require to develop and operate applications over the internet. examples of providers using paas applications are microsoft azure, google app engine etc. (fig. 6 ) fig. 6 . examples of providers using paas applications,i.e, microsoft azure, google app engine etc."
"in the existing mhealth research, wireless access network and mobile terminal are mainly used to transmit medical data and provide telemedicine services. at the same time, privacy protection and secure transmission are discussed in terms of security technology and means. however, the current mobile internet has been overburdened, and the data traffic needs to be diverted and offloaded. therefore, exploring the data transmission technology of oppnets and applying it to mobile medical service is of great significance to promote the application of mhealth."
"cloud computing is a general term used to describe a new class of network-based computing that takes place over the internet, basically a step on from utility computing. 1) a collection or a group of integrated and networked hardware, software and internet infrastructure (called a platform). 2) using the internet for communication and transport provides hardware, software and networking services to clients, to general public, enterprises, corporations and businesses markets. 3) these platforms hide the complexity and details of the underlying infrastructure from users and applications by providing very simple graphical interface or api (applications programming interface). 4) in addition, the platform provides on demand services that are always on, anywhere, anytime and anyplace."
"after the icelandic volcanic crisis, the international civil aviation organization (icao) began working more closely with european aviation officials to enhance the forecasting abilities of volcanic ash trajectory and dispersion. a mobile radar unit will soon be stationed in iceland to improve the accuracy and speed of predicting the height of any future ash cloud. according to rodrigues and cusick [cit] under the \"single european sky\" initiative [cit] ."
limitless storage capacity: the cloud offers virtually unlimited storage capacity.also at any time you can expand your storage capacity with a small additional charge on your monthly fee.
"4. 1st order energy minimization and harvesting model will be assumed for transmission and reception of sensor nodes as it is applicable for terrestrial in figure 9 it is clear that end-to-end delay in eh-dbr protocol is minimum as compared to dbr. figure 10 shows that energy consumption is minimized in eh-dbr. moreover, figure11 shows an increase in packet delivery ratio pdr due to the fact that more number of alive nodes available as continues supply of harvested energy. consequently, fewer holes created and the probability of packet drop decrease which is the major cause of improving pdr."
"an autonomous tracked combine harvester developed by zhang [cit] was used to cut wheat and paddy rice in real time by using an appropriate harvesting map which is created based on a real time global positioning system (rtk-gps) and inertial measurement units (imus); before harvesting, the outside crop near to headland is cut twice or thrice by the tracked combine harvester, and during this time the sensor measurements are logged for making a navigation map. generally, the tracked combine harvester makes turns at moderate to high speed with a small turning radius, which is very popular with farmers, and this turning position is represented by a circle marked in figure 1 . during this turn, the measured heading of a tracked combine harvester contains drift errors, which result from the imu gyro measurement bias. to get an absolute heading, this bias needs to be compensated by using a tracked combine harvester model and sensor fusion method. in this research, the main contributions are the estimated absolute heading of tracked combine harvester during non-linear conditions (especially high speed turns with small turning radii) by using a tracked combine harvester dynamic model which was developed by us based on a real time global positioning system (rtk-gps) and inertial measurement units (imu). for more details readers should see [cit] . in practice, this estimated heading can further be used to obtain the exact crop periphery for calculating the harvesting map of a robot combine harvester (for more details see [cit] )."
"in depth based routing (dbr), protocol of uwsn full dimensional localization is not required. therefore, we can achieve high packet delivery ratio (pdr) on the basis of depth information i-e node near the sink will forward the packet. but to achieve this high ratio more energy will be consumed and underwater it is challenging to replace or recharge the batteries. so its life time will be effected i.e. if the energy of an individual node or a set of the nodes is depleted, it cannot enable the participation of these nodes to fulfill the connectivity requirement and hence disconnected from the remaining network which can cause the entire network blackout. even if the network does not collapse by energy failure of few or more nodes, still it will degrade the efficiency and performance of the network. keeping in mind these challenges, we proposed eh-dbr protocol. the basic idea of eh-dbr is when a node receives a packet; it forwards the packet if its depth is smaller than that embedded in the packet. otherwise, it discards the packet. obviously, if there are multiple energy harvesting data sinks deployed at the water surface, as in dbr [cit], eh-dbr can naturally take advantage of them. packets reach any of the sinks are treated as successfully delivered to the final destination since these water-surface sinks can communicate with each other efficiently through radio channels, which have much higher bandwidths and much lower propagation delays. we introduce the concept of energy harvesting in which sensors are powered by an"
"internet connection is required: it is impossible to work if your internet connection is down, since you are using internet to connect to your \"cloud pc\". if there is no internet connection, then no access."
"your data is 100% in the cloud: all the data that you had until now on your local pc, it is stored in the cloud. theoretically, data stored in the cloud is safe since a cloud hosting company uses several ways of backup in order ensure that, on any case the data will not be lost. however, if your data is missing (even one in a million), you have no physical or local backup of your data."
"author heartily thanks his father, sanjay barde, mother anjali barde and aunt vrushali tickle, whose encouragement, guidance and support, from the initial to the final level enabled him to develop an understanding of the subject. lastly, he offers his regards and sincere thanks to all those who have supported him in any respect during the completion of the project."
"infrasound signals travel long distances in the earth's atmosphere making it possible to detect and record them using sensors located at large distances from the source. the dataset used in this work is obtained from various sensor arrays deployed in the global infrasound monitoring network [cit] . these sensors can monitor and record infrasound activities such as volcanic eruptions, mountain associated waves, avalanches, tsunamis, bolides (meteors), and many man-made events."
"possibly has more resources and expertise than the average user to secure their computers and networks. whatever may be the security problems, but the fact is the world is going crazy for cloud computing usage (fig. 4) ."
"the jacobian matrices of a k and h k in the prediction step are the following, in which a k was obtained from the partial derivatives of each state vector by using the equation (9), and h k was obtained from the partial derivatives of each measurement vector."
"less it infrastructure costs: the it department of large organizations could experience decreasing on the expenses in regards with infrastructure with the adoption of the cloud computing technology. instead of investing in larger numbers of more powerful servers, the it staff can use the computing power of the cloud to supplement or replace internal computing resources."
"large plinian volcanic eruptions radiate infrasonic signals that can be detected by the global infrasound array network [cit] . in an effort to reduce potential hazards for commercial aviation from volcanic ash, these infrasound sensor arrays have been used to detect infrasonic signals released by sustained volcanic eruptions that may inject ash into the stratosphere at the aircraft's cruising altitudes, typically in the order of 10km [cit] . so systems capable of near, real-time eruption detection which can report low latency notifications are necessary to provide ash monitoring for aviation. these systems should be capable of detecting hazardous eruptions and also discriminating the eruption intensity based on the volcano's infrasound signature."
"as the society progresses and the economy grows, the airplane is now becoming one of the most important forms of transportation [cit] . while the airplane makes our life convenient, its fare always changes violently. affected by holidays, low and peak seasons, supply and demand, etc., seats of the same class in the same flight usually have different prices, which makes passengers hard to make decision when to buy; therefore, it is meaningful to analyze and predict the trend for the lowest airfare in the future. up to now, many domestic websites provide airfare query function but cannot give a prediction. some of foreign websites provide prediction services, such as farecast [cit], farecompare [cit] and yapta [cit], however, the prediction time is too long. moreover, the difference between domestic and foreign air freight management models leads to the research blank in this field."
"to improve the compression performance, the data need to be rescheduled and shuffled before entropy coding to prioritize the same bytes as possible. in the compression process, the algorithm of entropy code is the deflate algorithm which is a general lossless compression algorithm. it is a combination of the lz77 dictionary coding and the huffman coding [cit] . in addition, ts wavelet filter is chosen to carry out integer wavelet transform, and its advantage is that the compression and decompression time overhead is much less and the compression ratio is slightly less than the binary arithmetic coding."
"if you are considering using the cloud, be certain that you identify what information you will be putting out in the cloud? who will have access to that information? what you will need to make sure it is protected? & what is the reputation and responsibilities of the providers, consider everything before you sign up! however, with increased ease also come drawbacks. you have less control over, who has access to your information and little to no knowledge of where it is stored. you also must be aware of the security risks of having data stored on the cloud. the cloud is a big target for mischievous individuals and may have disadvantages because it can be accessed through an unsecured internet connection. 1) cloud computing is outperforming the it industry as real business value can be realized by customers. 2) proper planning and migration services are needed to ensure a successful implementation. 3) cloud solutions are simple to obtain, don't require long term contracts and are easier to scale up and down as needed. 4) public and private clouds can be set up together to leverage the best of both. 5) third party monitoring services ensures, customers are getting the most out of their cloud environment. 6) security abidance and monitoring is achievable with careful planning and analysis."
"generally speaking, the initial value of the weights is 0.5:0.5. uif adjusts them according to specific needs. for example, w 1 increases in the off-season, while w 2 decreases. the case is opposite on holidays."
"assuming that t is the date of target day, we have known the lowest prices of the target day and the last 7 days (i.e. day (t-7) to day t) and the quoted prices to the next two weeks. calculate the lowest forecast prices in the next 7 days (i.e. day (t + 1) to day (t + 7)). the key to this problem is to find a balance between stability and flexibility in the existing prediction algorithms and make our new algorithm prediction with the accuracy of 85% to 90%."
"the historical encounters can generally be obtained from the records of software attached to the mobile intelligent terminal. they can reflect the relationship between nodes and reveal the characteristics of node mobility over a period of time by some statistical measurements, including the number, average time, and average time intervals of encounters, and so on. in general, the more the number of encounters between nodes is, the longer their encounter time is, and the higher their encounter frequency is, the closer their relationship is. on the contrary, the less their encounter number is, the shorter their encounter time is, and the lower their encounter frequency is, the more distant their relationship is.to exactly measure the relationship between nodes, we introduce a metric as follows:"
"in image processing, the input data are expressed as integers, so we use the integer wavelet transform to compress medical image data. integer wavelet transform can remove the correlation between data to a certain extent, eliminate redundant information, and reduce the entropy of data; thus, it can provide a theoretical basis for lossless data compression [cit] . for the original signal s i, it is decomposed into the low-frequency signal s i−1 and the high-frequency detail signal d i−1 by integer wavelet transform, and the transformation process contains the following three steps."
"assumptions of the network 1. instead of conducting practical experimental research, a simulation environment will be created which will assume same underwater channel impairments as is the case in real-world scenario."
"the performance of the classifier is measured with respect to the correct classification rate (ccr) [cit] . that is: another performance measure that can be used is the accuracy (acc), that is:"
"the information housed on the cloud is often seen as valuable to individuals with malicious intent. there is a lot of personal information and potentially secured data that people store on their computers, and this information is now being transferred to the cloud. this makes it critical for anyone, to understand the security of the data and it is equally important to take personal precautions to secure the data. the first thing one must look into, is the security measures that your cloud provider already has in place.these vary from provider to provider and amongst the various types of clouds. what methods of protection do they have in place for the actual hardware that your data will be stored on? will they have backups of your data? do they have firewalls set up? if you have a community cloud, what are the barriers which keep your information separate from other companies? many cloud providers have standard terms, a negotiation room in their cloud contract.a small business user may have slightly more room to discuss the terms of their contract with the provider and will be able to ask these questions during that time.there are many questions that you can ask, but it is important to choose a cloud provider that considers the security of your data as a major concern. no matter how careful you are with your personal data, by subscribing to the cloud, you will be giving up some control to an external source. this distance between you and the physical location of your data creates a barrier. it may also create more space for a third party to access your information. however, to take advantage of the benefits of the cloud, you will have to knowingly give up direct control of your data [cit] . on the"
"cwts and sta have the conditions of use. cwts is a prediction method based on a continuous time, which is accurate when the fare is stable. in contrast, sta is based on the discontinuous similar time points and has high accuracy when it encounters holidays and emergencies. in order to balance these two different situations, we present uif as follows."
"in previous studies, wireless access networks such as 3g/4g and wi-fi were mainly used to transmit medical data in mhealth. however, with the vigorous development of the mobile internet, its data traffic is growing exponentially, which brings serious challenges to divert and unload these traffic and has become a common concern of academia and industry. the introducing of oppnets can not only lighten the load of access networks, but also reduce the cost pressure of users. therefore, it is a significant attempt to introduce oppnets into mhealth. in our study, the transmission algorithm based on friend circles has a low time complexity and only o(n). its disadvantage lies in the use of the social features and historical encounters of nodes. the former can be obtained by filling in a questionnaire when the user is registered, and the latter is a kind of dynamic data which need to be updated and maintained every now and then. in addition, the data compression algorithm based on integer wavelet transform is a mature and developed technology, which has the advantages of consuming less computation and less compression times."
"alleviating the difficulty of getting medical treatment, reducing medical costs, improving the level of diagnosis and treatment, and promoting health and disease prevention, mhealth has become a hot topic in academia and industry recently [cit] . in china, from the perspective of market size, there was about 2.95 [cit] and increased by 44.7% to 4.27 [cit], as shown in fig. 1a . from the aspect of user size, they reached 72 [cit] and 138 [cit] mhealth platforms."
"of note, the cutoff spatial frequency determined by the diffraction-limited resolution is still below the truncated frequency, so that such frequency truncation does not reduce the image resolution determined by the optical system."
"the first recorded impact of volcanic activity on aviation was on march 22, 1944 when mount vesuvius produced more damage to an airfield than that created by opposing forces during world war ii. it was 36 years later that the next event occurred when a civil lockheed c-130 hercules inadvertently penetrated an ash cloud from mount st. helens following its second eruption."
"anywhere access to your documents: when you are in the cloud, there is no need to take your documents with you. instead, you can access your actual pc from anywhere,where"
"each provider serves a specific function, giving users more or less control over their cloud depending on the type. when you choose a provider, compare your needs to the cloud services available. your cloud needs will vary depending on how you intend to use the space and resources associated with the cloud. if it will be for personal home use, you will need a different cloud type and provider than, if you will be using the cloud for business. the cloud provider will be pay-as-you-go,it means that if your technological needs change at any point, you can purchase more storage space (or less) from your cloud provider."
……………………. (1) where d is the depth difference of the current node and the previous one. r is the maximal transmission range of a sensor node.
"the plinian volcano and maw signals exhibit strong spectral characteristics in the 0.01 hz -0.1 hz range, as mentioned in section 1. the signals presented to the classifier are restricted to this frequency range by a 4 th order butterworth bandpass filter with cutoff frequencies: 0.01 hz and 0.1 hz. filtering in this spectral region also ensures that the signal is devoid of any microbarom [cit] noise. figure 4 shows a noisy raw infrasound signal associated with a plinian volcano eruption and its filtered version."
"cloud computing is receiving a great deal of attention, both in publications and among users, from individuals at home to all the governments. yet, it is not always clearly defined. cloud computing is a subscription-based service where you can obtain networked storage space and computer resources. one way to think of cloud computing is to consider your experience with email. your computing is to consider your experience with email. your email client, if it is yahoo!, gmail, hotmail, and so on, takes care of housing all of the hardware and software necessary to support your personal email account. when you want to access your email, you open your web browser, go to the email client, and log in. the most important part of the process is having internet access. your email is not housed on your physical computer. you access it through an internet connection and you can access it anywhere. if you are on a trip, at work, or down the street getting coffee, you can check your email as long as you have access to the internet. your email is different than software installed on your computer, such as a word processing program. when you create a document using word processing software, that document stays on the device you used to make it unless you physically move it. an email client is similar to how cloud computing works. except instead of accessing just your email, you can choose what information you have access to within the cloud [cit] . 2) \"all words ever spoken by human beings\" ~ 5 eb 3) cern's lhc (large hydrogen collider,i.e, world's highest enery particle accelerator) will generate 15 pb a 5) pay as-you-use. it's elastic (fig. 1 ). 6) scale up and down in capacity and functionalities [cit] ."
"the pnncb described in section 1 forms the basis of the neural-classifier. the architecture for the plinian volcano and maw classifier is shown in fig. 7 . fig. 7 . pnncb architecture for plinian volcano and maw classifier each neural module in the pnncb produces an output which is compared to the threshold for the corresponding class. if the output is found to be greater than or equal to the threshold, the signal belongs to that class and vice-versa. the optimal threshold for a class is selected using a 3-d receiver operating characteristic (roc) curve [cit] . along with the standard axes for the true positive (tp) rate and the false positive (fp) rate, the 3-d roc curve incorporates a third axis representing the misclassifications between neural modules in the pnncb. the ideal point on this curve is where the false positive rate and the misclassification are \"0\" while the true positive rate is \"1\". the point on the 3-d roc curve that is closest to the ideal point (0, 1, 0) is chosen to obtain the threshold. fig. 8 shows the 3-d roc curve for the plinian volcano and maw classes. the associated threshold values selected are provided in table 3 ."
"a recent hazardous volcanic ash eruption was reported on april 14, 2010 when london's aviation authority issued an alert that an ash plume was moving from an eruption in iceland towards northwestern europe. this eruption resulted in the immediate closure of large areas of european airspace, around 10 million passengers were affected, and total economic damage reached almost 5 billion dollars [cit] . according to reports, this was the largest air-traffic shut-down since world war ii. over 95,000 flights had been cancelled all across europe during the six-day travel ban [cit], with later figures suggesting 107,000 flights cancelled during an eight day period, accounting for 48% of total air traffic and roughly 10 million passengers [cit] . the danger of aircraft damage or flameout disrupted air traffic for over two weeks."
"stored data might not be secure: data is stored \"in the cloud\". however, where exactly is the cloud and is it really secure? these are questions arising for users that have confidential data."
"latest version availability: one more thing in relation with documents is that when you edit one document at the office and then you go somewhere else and open it, the latest version will be displayed since all the work is done centrally in the cloud."
"underwater acoustic sensor network (uasn) is an application of wireless sensor networks (wsns) that is specially used for examining oceanic environment. the acoustic sensors collect the data and then using a routing scheme to relay it towards sink. the applications of uasns are pollution examining, ocean current detection, submarine discovery, habitat monitoring, oil discovery, underwater surveys and management of seabed [cit] . the acoustic wireless sensors with one or more sinks, on sea surface or dropped under water, organize the basic body of uasn. sink has normally less power constraint, but the acoustic sensors have limited battery life [cit] . the role of a sink on a sea surface is to gather information (useful data) and forward it through radio link to the fusion centers for further processing. sinks are equipped with sound and radio modems along with gps module. every sensor node, in sensor equipped aquatic (sea) swarm architecture, examines the nearby underwater activities and routes the appropriate information to sink using multi hop routing. the general architecture of uwsn is given in figure 1."
"in the transmission algorithm, the parameter τ is mainly used to control the degree of intimacy between nodes. the larger the value of τ is, the closer the relationship between friends is, and the fewer the number of nodes involved in medical data transmission is, the higher the data security is, but the greater the transmission delay of medical data is. on the contrary, the smaller the value of τ is, the more the number of nodes participated in transmission is, the lower the medical data security is, and the shorter the transmission delay of medical data is."
"in the simulation, the effect of threshold τ is evaluated. figure 5 shows that τ has important influences on the multipath scheme, while the effect of the single-path scheme is relatively small. in the multipath scheme, with the threshold τ increases, the lists of nodes friend get smaller, and nodes which participate in the forwarding get smaller. hence, the number of forwarding gets smaller, the hop count gets smaller, and the delivery delay rises. to our surprise, the delivery ratio increases maximum and then decreases. the reason is as follows: the threshold decreases to a certain value, the copies of the message is so enough that the cache is full and some packages are discarded. hence, the delivery ratio reduces."
"weighted moving average method (wma) [cit] gives the data different weights according to the distance of time and then calculates the average. however, wma can't keep up with the change when predicts with obvious trend of rising or falling. in addition, it will reduce wma's sensitivity to fluctuations to increase the number of iterations. because of these weakness, wma only fits the situation with little change in a short-term trend."
"the whole flow of uif is shown as figure 1, which consists of two parts, cwts and sta. a. cwts calculates the weighted average cwts(t) from the quoted prices in the past week according to formula 2.1."
"the classifier presented here is capable of distinguishing between plinian volcano and maw infrasound signals with a ccr of 91.46 %. the bandpass filter cut-offs are set to the frequency range of interest, that is, 0.01 hz to 0.1 hz. the pre-processing parameters are optimized in order to select the best features for training and testing the neural network modules. the neural module threshold values for each class are optimized by use of a 3-d roc curve."
"linear deconvolution has the potential to achieve fast processing of raw images with high-density emitters due to its non-iterative nature and simplicity. but the presence of non-uniform background often introduces significant artifacts, limiting its use to identify and localize the true overlapping emitters. we made two important modifications to improve its performance in super-resolution localization microscopy."
"from the error analysis of these headings, the rms errors between the measured or estimated figure 6 indicates the measured and estimated heading of circular trajectories (left figures) as well as the difference of these headings from the reference gps heading (right figures) which was obtained by the linear regression of the noisy trend gps heading. the heading from gps measurements is indicated by a blue line, where the measured and estimated headings are marked by the black and red lines. in general, a full circle rotation is counted to 360°, and when the tracked combine harvester was run in the field with a constant steering command, therefore, a full trajectory of circle will be 360°. that why, the left figure 6a,b indicate that the output headings were bounded by 360°. according to these figures, the red line tried to follow the blue one, which means the estimated heading based on the extended kalman filter was consistent with the gps heading. in addition, the heading difference analysis is shown in right figure 6a,b, which indicates the difference between the measured and estimated headings from the reference gps heading. since, the gps heading provides the exact direction of the vehicle that is computed from the exact position of the tracked combine harvester, the linear regression of the gps heading was used as a reference gps heading for the evaluation of estimated headings. the result indicated out that the estimated heading (red line) matches the reference gps heading for all circular trajectories as shown in figure 6a,b (right figures) rather than the measured heading."
"as an extension to this work, the noi class will be expanded to include the other natural phenomena such as tsunamis, bolides (meteors) and avalanches which have infrasonic characteristics in the same frequency range as the plinian volcano signatures. with the development of a robust classifier capable of discriminating plinian volcano events from a more comprehensive set of noi events, a near, real-time warning system may be developed for use by the commercial aviation industry."
"this research deals with the tracked combine harvester dynamic model, sensor measurements and the extended kalman filter, which are used to estimate the absolute heading of a tracked combine harvester under non-linear condition by compensating the imu yaw rate gyro measurement bias which influences the heading drift errors. different sets of experiments such as circular, sinusoidal and concave polygon trajectories were executed to evaluate this estimation method. the results for different trajectories revealed that the imu yaw rate gyro measurement bias was compensated based on the extended kalman filter and the tracked combine harvester dynamic model, and the absolute heading was simultaneously determined, which is better than the measured heading. the rms errors of the estimated heading are lower than those of the measured heading for all trajectories. therefore, this estimation method can be used to estimate the absolute heading when the tracked combine harvester will makes turn at high speed and with high order steering commands in order to cut wheat and paddy rice near headlands to calculate the exact crop periphery for the development of a harvesting map, for more details see [cit] ."
"step 2. the friend circles are constructed by removing strangers from the social circles of the node using the formulas (3) and (4). thus, we have where fc i is a friend circle of node v i ."
"next, we perform a linear deconvolution (f) based on inverse filtering on the background-corrected image (i c ) to recover the overlapping emitters (deconvolved image d):"
"as the airfare is closely linked to time, the traditional method selects a time-series algorithm and adjusts its coefficients on the idea of time series. however, as described above in this paper, wma is prone to \"lag\" for predictions of increasing or decreasing trend and es is too sensitive to make prediction. in addition, these methods have very limited scope of application. the factors affecting the airfare are numerous and changeable but the traditional algorithms cannot be so thoughtful, which to a large extent affected the practicality of the prediction."
"(1) to lessen the cost pressure on users and also lighten the load on wireless access networks, we introduce oppnets into mhealth systems. (2) to achieve higher transmission performance and deal with the selfishness and privacy issues, we propose a collaborative and secure medical data transmission scheme based on friend circles. (3) to reduce the amount of medical data in the transmission, we propose a lossless compression scheme with less computation and higher compression ratio."
the job of communication unit is to exchange the information among the nodes. it is consists of transducer which transmit and receive information and the control sensing and processing unit perform sensing and doing data processing.
"the steps involved in the development of the event classifier presented here are described in sections 2 to 4. the dataset to train the neural-classifier is selected to include a wide variety of signals, at the same time allocating sufficient data for testing and validation. the raw data is filtered appropriately and cepstral-based features [cit] are extracted from the signals for classification. as compared to the time-domain signals (see fig. 3 ), the cepstral feature vectors provide better uniformity among the infrasonic characteristics of a particular class, and distinctiveness between sets of feature vectors for different classes. this serves to enhance the overall performance of the classifier."
"the holding time for a forwarding node is calculated based on d, where d is the depth difference between previous hope and current hope of the packet. nodes with different depth will have different holding time. dbr selects the node with minimum depth from the water surface to forward the packet and suppress all other nodes in the transmission range in order to prevent the duplicate packets forwarding."
"in order to make up for the defects, this paper proposes uif. firstly, we adjust the coefficient function of the wma and modify its structure to enlarge the scale of the reference data. we propose the idea of similarity time based on the thoughts of k-nearest neighbor [cit] . the data selected from similarity times and a special coefficient will make es predict more accurat and reduce its sensitivity to smoothing coefficients. finally, the two algorithms are combined to balance the advantages and disadvantages. as a result, uif can be accurately predicted in both cases of smooth and fluctuating trend."
"the experiment uses root mean square error(rmse) [cit] to measure the error between actual price and forecast result of uif. the smaller the rmse is, the less error will be."
supplementary materials and methods fig. s1 . the workflow of inverse deconvolution and frequency truncation for log-normal intensity distribution with 5000 [cit] (mean and sd) and 350 and 70.
"for any nodes in the network, if they have the same value of a social feature (that is, they have the same hobbies and characteristics) and they are friends with each other, they form a circle of friends."
"the transmission mechanism in mhealth is described as follows: (1) transmission of the patient physiological data. to ensure the security, these data use a single-path transmission scheme. that is, they are forwarded in turn by the people who are within the friend circles of the previous one and have more opportunity to access the destination. (2) transmission of the health information accessed from other people. to improve the transmission efficiency, this information uses a multipath transmission scheme. in other words, they are copied into all friend circles of the people, and the process ends when the people obtain this information."
"in the simulations, the proposed transmission algorithms are assessed by comparing to server existing algorithms. from fig. 6, it is clear that the multipath scheme has a larger delivery ratio and a shorter delay. it can achieve 62% of delivery ratio, while single-path, simbet, s-w, st-e, and f-r, could only deliver 58%, 59%, 56%, 61.2%, and 60.6% respectively. in addition, the single-path scheme has the least hop counts and number of forwarding. compared to multipath scheme, simbet, s-w, st-e, and f-r, the single-path scheme decreases the number of forwarding by about 50.9%, 20.3%, 62.1%, 48.6%, and 46.3% respectively. in the mit reality mining datasets, the simulations result is shown in table 3 . clearly, our algorithm is better than other. compared to the single-path, simbet, s-w, st-e, and f-r, the multipath scheme increases the delivery rate by about 7.3%, 5.1%, 10.2%, 2.8% and 4.0%, and reduces the latency by about 5.5%, 3.6%, 7.2%, 2.0% and 2.4% respectively. compared to the multipath, simbet, s-w, st-e, and f-r, the single-path scheme decreases number of forwarding by about 55.8%, 29.7%, 65.4%, 54.4%, and 54.3% respectively. the simulation results are consistent with the above results."
"in this study, we mainly use the friend circles to achieve the safe and efficient medical data transmission. among them, the definition of friends only takes into account historical encounter data, not other factors, such as recommendations of others, and it can be carried out to more accurately measure credibility between nodes in the future. in addition, we only consider simply the data compression problem, and we should construct a complete data compression scheme in the future from the perspectives of time and space."
2. it will be assumed that the sensor nodes are static at their locations even with the movements of the water currents. however the sink linearly moves at different depth in underwater layers.
"to summarize, the cloud computing provides many options for everyday users as well as for large and small businesses. it opens up the world of computing to a broader range of uses and increases the ease of use by giving access through any internet connection."
"control sensing and processing (csp), unit performs sensing the neighbor ack, neighbor request, and data packets when arrive at the node and doing necessary processing before transmitting."
"the first step is to subtract the background-based on the conceptual framework of our recently developed extreme value-based emitter recovery method (ever) (17)-from the raw image before linear deconvolution, which is described below."
"in the section, we first analyze the relationship between nodes reflected by their historical encounters, and discuss the importance of different social features of nodes, then construct the friend circles of nodes according to this information."
"the event classifier developed here uses a parallel neural network classifier bank (pnncb) architecture. it consists of individual classifier modules constructed using radial basis function neural networks (rbf nn) [cit] . the modules work in parallel to identify the signals belonging to a particular class. the number of modules in the pnncb corresponds to the number of classes of interest. the general pnncb architecture is presented in fig 2 . as shown in fig. 2, each module of the pnncb is trained independently to recognize or reject the signals of a particular class. a negative reinforcement approach is used to train the pnncb modules. this entails each block is trained not only to identify a signal of a particular class, but also to reject signals belonging to the other classes."
"as shown in figure 2 the eh-dbr node architecture consists of four fundamental units harvesting unit, power unit, communication unit, control sensing and processing unit"
windstorm is a high-speed high-density localization method for high-throughput nanoscopy. it is based on two mathematically simple non-iterative steps: (1) overlapping emitter decomposition via inverse deconvolution with windowed (truncated) frequency; and (2) emitter localization for precise localization via surrounding emitter deduction and gradient fitting.
there are four performance metrics used to assess each algorithm. (1) delivery ratio: the ratio of the number of medical data delivered successfully to send out during a given interval. (2) delivery delay: the time taken for the data to be successfully delivered. (3) hop count: the number of nodes through which the data was successfully sent. (4) number of forwarding: the number of data being forwarded during transmission.
"however, mhealth faces the following problems in the application. first, it is regarding how to process the massive medical data. the diagnosis and treatment of patients will generate a lot of information, including their personal information, past medical history, examination items, hospitalization records, and discharge records. according to the statistics of the second xiangya hospital of the central south university, each patient undergoing pet-ct examination will produce about 300 images, a total of about 1 gb of data. unfortunately, massive images and videos generated by mobile intelligent terminals have overwhelmed the current mobile internet, and their rapid growth rate has far exceeded the speed of the expansion of mobile internet bandwidth [cit] . the yearbook report of cisco also shows that video data account for more than 85% [cit] . second, it expresses concerns about the patient privacy and data security. mobile healthcare is highly dependent on network and information technology, and it is difficult to ensure the security of patients' personal information and medical information. in the process of data transmission, data anomalies and leakage problems will be caused by the external malicious interference. in the interview, 25% interviewees expressed concerns about patient privacy and data security. it can be seen that the privacy and security of data are the focus and difficulty for both patients and medical workers."
"in the oppnets, we assume that each node has different social relations and behaviors which can be described by his/her social features and history encounters. these social features can be obtained by a certain means (for example, questionnaire survey) before the deployment of the network, and the history encounters can also be collected with the wireless terminal devices after a period of network running."
"(1) multipath transmission algorithm based on friend circles (tafc-m): each node copies the message to all encounter nodes if they are the members of the friend circles of the current node. (2) single-path transmission algorithm based on friend circles (tafc-s): a node is forwarded only if it is within the friend circles of the current node and has more opportunity to access the destination. (3) spray-and-wait (s-w) [cit] : there are 10 data copies at the beginning. first, each node with more than one copy sends half to the encounter node, and then none of the nodes forwards any data copy until the destination is encountered. (4) simbet [cit] : it forwards data based on simbet utility. (5) stepidemic (st-e) [cit] : an effective transmission algorithm based on social trust. (6) f-r [cit] : an effective transmission algorithm based on friendship."
"in order to solve the accuracy problem of the prediction algorithm for the lowest airfare in the future, this paper presents an united intelligent forecasting algorithm(uif) consisting of two sub-algorithms, the composite weighted time series method(cwts) and the similarity time average method (sta) based on the idea of time series. cwts calculates the sub-price on the target day from the quoted prices on the same day and a period of time in the past. sta calculates the sub-price on the target day from the prices on the similar period of time. experiments on the real datasets show that uif outperforms the traditional prediction algorithm and provides enhanced accuracy for airfare prediction. this airfare forecasting model based on time series can effectively solve the predictive conflict between sequences with smooth and fluctuating trends and thus a class of predictive analysis problems for the lowest airfare of air tickets at all kinds of time points are solved."
"in this article, we have proposed a routing protocol capable of utilizing and scheduling the harvesting energy from the underwater energy harvester. it is in turn improves the lifetime and end-to-end delay. keeping in mind the energy scarcity, we designed routing protocol which is able to address the energy constraint challenge to their optimum levels. it will prove to be energy efficient with better throughput and enhancing the stability period of the network. be computationally simple and implementable. harvesters can be the part of the sensor node or it may be the separate entity which can provide energy wirelessly in the form of acoustic signal. the deployment of sensor nodes along with harvesters proves to be successful in term of maximizing the network lifetime. moreover, the depletion of lesser depth node in dbr protocol is overcome due to continue harvesting energy supply from the harvesting system. in future, we will extend our work to design the movement model of the energy harvester that could be able to provide the required power to all the nodes at different location of the entire network. that will effectively eliminate the void hole or energy hole problem specially in randomly deployed uwsn."
"the heading estimation method based on the tracked combine harvester dynamic model and the extended kalman filter can also be evaluated for any polygon-shaped field, because this polygon field with crops will be harvested by the robot combine harvester, which is our main research target. here, only a concave polygon field as shown in figure 8 was used for this estimation. the circle marked on the concave polygon field (top figure) indicates the turning position of the tracked combine harvester. in these circle positions, the tracked combine harvester cuts the surrounding crop to calculate the exact crop periphery for obtaining the working path of the robot combine harvester by forward and backward movement, and sometimes turns at high speed with low turning radius are considered, which creates a yaw rate gyro measurement bias. this bias creates the measured heading drift error that is important to compensate. figure 8 (bottom figure) shows a heading of a concave polygon field where the black and red lines are marked for the measured and estimated headings, which was analyzed with respect to the reference gps heading (blue color line). this reference gps heading was obtained from the moving average of the noisy trend gps heading. the estimated heading is better matched with the reference gps heading than the measured heading in figure 8 (bottom figure) . in addition, the heading difference between the measured heading or estimated heading and the reference gps heading was calculated, as shown in figure 9 . the rms error was calculated from the measured or estimated heading and the reference gps heading, and from the error analysis, the rms errors for measured and estimating heading were 14.93° and 5.17°, respectively. since, the rms error of estimated heading is lower than the measured heading, the estimated heading shows more consistency with the reference gps heading. the results also describe that the measured heading drift error was compensated by the extended kalman filter and the tracked combine harvester dynamic model."
"in the second deployment scenario, the source is assumed to be a circular piston type projector for which directivity index is related to the vertex angle of acoustic transmission (θ) by"
"in our work, the model of mhealth system is designed as shown in fig. 2 . based on the transmission of oppnets, the patient physiological data collected by various sensors are sent to the hospital in order to realize the personal monitoring of human health, and patients can access whenever and wherever a variety of medical health information from doctors or other peoples to establish a health concept oriented to promoting disease prevention and strengthening disease prevention."
"where f i is the ith feature value of the node. f(v j ) is a function for finding the feature values of node v j . c i is the ith social circle of the node and is composed of nodes with f i . it is worth mentioning that there are many social features for each node, and each feature has many values. in practical application, important social features and eigenvalues should be screened according to the formula (2) to avoid interfering with the process of data transmission by the irrelevant social features."
"the exponential smoothing method (es) [cit] calculates a series of exponential smoothing values to eliminate irregular changes and reflects the trend of time series. however, the result of es is easily affected by the smoothing coefficient. that is, the sensitivity is high."
"less maintenance costs: maintenance costs also will be reduced using cloud computing since both hardware and software maintenance for organizations of all sizes will be much less. for example, fewer servers are necessary in the organization which means that maintenance costs are immediately lowered. as to software maintenance, there is no software on the organization's computers for the it staff to maintain."
"these methods above have their own shortcomings. they can only predict accurately in particular situations. there isn't a way to balance their advantages and disadvantages to reduce the sensitivity of the change of the fare. in this paper, we propose a united prediction algorithm based on wma and es to improve the accuracy of results by adjusting the proportional coefficient functions."
"increased power of computing: when using cloud computing, you can use the cloud computing power since you are no longer limited to what a single desktop computer can do."
"based on the existing methods, we propose an united intelligent forecasting algorithm (uif), which consists of two sub-algorithms. our main contributes include: a. uif is flexible and adaptive to both cases of smooth and fluctuating trend. the first subalgorithm composite weighted time series method (cwts) improves the stability of the prediction by adjusting the proportional coefficient function. the second sub-algorithm similarity time average method (sta) added similarity time selection and special influence coefficient to improve the sensitivity of fluctuation points."
"use your computer from anywhere: this is one of the biggest advantages of cloud computing. basically, when you use this technology, you are not limited to work on a single pc. you just use your \"cloud pc\" from anywhere and any pc and your existing applications and documents follow you through the cloud to you. move to a portable device, and your applications and documents are still available."
"as shown in fig. s1, after inverse filtering (figs. s1e1-e2), the high spatial frequency noise dominates. therefore, we perform truncation on the spatial frequency to remove those high-frequency noise in the deconvolved image in the spatial frequency domain, described by"
b. sta calculates the sum sta(t) of weighted average from similar times and holiday adjustment factor according to formula 2.2. c. forget the results of two sub-algorithms. uif calculates the final prediction result uif(t) according to formula 2.3.
"the time series is proposed as a solution to rank the airfare in a chronological sequence. it is an important concept that can reflect the developing process and direction usefully in prediction. currently, the following time-series prediction algorithms [cit] are in common use."
"low-speed connections are not at all recommended: this is not a very important disadvantage since everybody today has at least 1 mbps connection at work and at home. however, it is important to mention that cloud computing cannot work with slow internet connections such as dial-up since web-based applications often require a lot of bandwidth to download, as do large documents sometimes is slow: also, with fast connections, sometimes you might experience delays since web-based applications can sometimes be slower than accessing a similar software program on your desktop pc. the reasons for that are because of the demanding upload and download bandwidth that web applications need."
"assuming that the price of the next 7 days is predicted on day t, the user queries the price on day t, the composite time series used is the quoted prices for the next 7 days and the past week. as shown in the table 1 ."
"the event classifier developed here to distinguish between plinian volcanic eruptions and mountain associated waves has a ccr of 91.46 % with a 95 % confidence interval of (0.8990, 0.9303). the accuracy for this classifier is 92.01 % with a 95 % confidence interval of (0.9058, 0.9360). the confusion matrix is shown in table 4 . fig. 9 . fig. 9 . scatter plot for neural-classifier outputs"
"motivation towards eh-dbr. section 5 shows working methodology of dbr. flow chart is shown in section 6. in section 7, eh-dbr protocol and its equations are highlighted. we present the simulation results and its discussion in section 8 followed by our conclusions and future works in section 9. new innovation in routing schemes is needed and therefore we will propose the mathematical models inspired by the modeling by research community in literature review conducted which will help to cover those areas as well which have not been yet addressed."
"there are different types of clouds that you can subscribe to, depending on your needs. as a home user or small business owner, you will most likely use public cloud services."
"in this section, we compare struct to a stateof-the-art nlg system, crisp, 1 and evaluate three hypotheses: (i) struct is comparable in speed and generation quality to crisp as it generates increasingly large referring expressions, (ii) struct is comparable in speed and generation quality to crisp as the size of the grammar which they use increases, and (iii) struct is capable of communicating complex propositions, including multiple concurrent goals, negated goals, and nested subclauses."
"author contributions: all authors contributed extensively to the work presented in this paper. y.w. proposed the research idea. m.c. designed the model, interpreted the results and wrote the paper. c.z. edited the paper and prepared the vector road network. x.c. prepared the floating car data of dataset 2."
"in the next set of experiments, we illustrate that struct can solve a variety of complex communicative goals such as negated goals, conjuctions and goals requiring nested subclauses to be output."
"in the trajectory data, the necessary variables include the identifying number (id), time, longitude, latitude and heading direction. missing values of longitude or latitude were estimated using linear interpolation. missing and abnormal values of the heading direction were not addressed to ensure that the robustness of the matching model could be verified. for describing conveniently and clearly in the following context, the trajectory data with interval ranges from 5 s to 150 s was categorized into three groups, i.e., the low, moderate and high sampling rate fcd. similar to previous studies [cit], the interval of low sampling rate fcd is set to exceed 60 s, the interval of high sampling rate fcd is shorter than 40 s, and the moderate frequency fcd is in between. in the road network, the required variables were the road id and direction. the road direction of the vector road network was auto-calculated using the \"linear directional mean\" tool of current geographic information system (gis) software, i.e., arcgis. in one-way roads, the road direction was examined and aligned manually."
"we find that these adjectives which should have been selected immediately are omitted from the output, and that the sentence generated is the best possible under the constraints. this demonstrates that struct is balancing these negated communicative goals with its positive goals. figure 3b (the \"negative goals\" curve) shows the impact of negated goals on the time to generation. since this experiment alters the grammar size, we see the time to final generation growing linearly with grammar size. the increased time to generate can be traced directly to this increase in grammar size. this is a case where pruning does not help us in reducing the grammar size; we cannot optimistically prune out words that we do not plan to use. doing so might reduce the ability of struct to produce a sentence which partially fulfills its goals."
"negated goals. we now evaluate struct's ability to generate sentences given negated communicative goals. we again modify the problem used earlier by adding to our lexicon several new adjectives, each applicable only to the target of our referring expression. since our target can now be referred to unambiguously using only one adjective, our generator should just select one of these new adjectives (we experimentally confirmed this). we then encode these adjectives into negated communicative goals, so that they will not be included in the output of the generator, despite allowing a much shorter referring expression. for example, assume we have a tall spotted black cat, a tall solid-colored white cat, and a short spotted brown cat, but we wanted to refer to the first one without using the word \"black\"."
"though struct has many interesting properties, many directions for exploration remain. among other things, it would be desirable to integrate struct with discourse planning and dialog systems. fortunately, reinforcement learning has already been investigated in such contexts [cit], indicating that an mdpbased generation procedure could be a natural fit in more complex generation systems. this is a primary direction for future work. a second direction is that, due to the nature of the approach, struct is highly amenable to parallelization. none of the experiments reported here use parallelization, however, to be fair to crisp. we plan to parallelize struct in future work, to take advantage of current multicore architectures. this should obviously further reduce generation time."
"where p asp is the aspect constraint probability, β asp is the adjust coefficient, α obs is the heading direction of the observation point, and α candi is the azimuth of the cf relative to the crossover point with the previous cf. equation (9) indicates that the possibility of transmission from the previous cf to the current cf increases if the azimuth of the observation point more accurately approximates that of the current cf."
"the point in the cf that is closest to the observation points is defined as a cp. every cf has only one cp. if the projection of the observation point onto the cf is located between its endpoints, then choose the geometry projection as the cp; otherwise, choose the end point that is closer to the observation point with regard to the euclidean distance, as shown in figure 2 . the projection p b is the cp of p in feature b, and the endpoint p d is the cp of p in feature d. the cfs d and e have the same cp. among the cps, the point that maximally satisfies the matching rules is considered a confident point and added to the resulting list. the corresponding cf is considered a confident feature and added to the results list. the matching rules are presented in the following context."
"dataset 1 was used to verify the matching accuracy of the three models. as shown in figure 5, the matching accuracy of these models in three samplings can be compared. for the total roads, the matching accuracy of ehmm was significantly higher than that of sm and hmm. in the three samplings, the ap mean of ehmm was 0.96. the results of models sm and hmm were similar in the previous two samplings. in the last sampling, the performance of sm was moderate, and the performance of hmm was poor. for simple roads, the three models generally performed well. the ap of ehmm was only 3.4% higher than that of sm and hmm. the corresponding results are shown in figure 6a . three models corrected the gps points to the real roads well. for complex roads, the difference in the matching results among the three models was large. among the three samplings, the matching results of ehmm were satisfactory. the minimal ap of ehmm was 0.9. the next model was the sm model with an ap mean of 0.61. the matching results of the hmm model were satisfactory between the previous two samplings but were unsatisfactory in the last sampling, with a matching accuracy less than 0.55. the matching results among the three models are shown in figure 6b . two roads lay parallel to the nantian road. the positions of the gps points were seemingly close to the inner-ring road a. the sm and hmm models mismatched many points to this road. however, the ehmm model performed well. according to the statistics of ap for simple roads and complex roads, the results indicate that the performance of the three models for simple roads was better than that of the three models for complex roads. figure 7 compares the matching quality of the three models at different sampling intervals based on dataset 1. for the three samplings, the matching quality of ehmm and sm was stable, with an average standard deviation of 0.06 and 0.04, respectively. for hmm, the matching results exhibited a large fluctuation with an average stand deviation of 0.14. with various sampling intervals, the relative positions between the gps points and the roads exhibited randomness. thus, the spatial positions of the gps points had a significant impact on the matching results of hmm but a minimal impact on ehmm and sm."
"the performance of the proposed model, which is termed an enhanced hidden markov map matching (ehmm) model, is compared with that of two popular models."
"online planning in mdps as done by uct follows two steps. from each state encountered, we construct a lookahead tree and use it to estimate the utility of each action in this state. then, we take the best action, the system transitions to the next state and the procedure is repeated. in order to build a lookahead tree, we use a \"rollout policy.\" this policy has two components: if it encounters a state already in the tree, it follows a \"tree policy,\" discussed further below. if it encounters a new state, the policy reverts to a \"default\" policy that randomly samples an action. in all cases, any rewards received during the rollout search are backed up. because this is a monte carlo estimate, typically, we run several simultaneous trials, and we keep track of the rewards received by each choice and use this to select the best action at the root."
"our set of actions consist of all single substitutions or adjoins at a particular valid location in the tree (example shown in figure 1 ). since we are using pltags in this work, this means every action adds a word to the partial sentence. in situations where the sentence is complete (no nonterminals without children exist), we add a dummy action that the algorithm may choose to stop generation and emit the sentence. based on these state and action definitions, the transition function takes a mapping between a partial sentence / action pair and the partial sentences which can result from one particular pltag adjoin / substitution, and returns the probability of that rule in the grammar."
"a grammar contains a set of ptag trees, divided into two sets (initial and adjoining). these trees are annotated with the entities in them. entities are defined as any element anchored by precisely one node in the tree which can appear in a statement representing the semantic content of the tree. in addition to this set of trees, the grammar contains a list of words which can be inserted into those trees, turning the ptag into an pltag. we refer to this list as a lexicon. each word in the lexicon is annotated with its first-order logic semantics with any number of entities present in its subtree as the arguments."
"we note that prior work exists that uses mdps for nlg [cit] . that work differs from ours in several key respects: (i) it considers nlg at a coarse level, for example choosing the type of utterance (in a dialog context) and how to fill in specific slots in a template, (ii) the source of uncertainty is not language-related but comes from things like uncertainty in speech recognition, and (iii) the mdps are solved using reinforcement learning and not planning, which is impractical in our setting. however, that work does consider nlg in the context of the broader task of dialog management, which we leave for future work."
"struct uses a first-order logic-based semantic model in its communicative goal and world specification. this model describes named \"entities,\" representing general things in the world. entities with the same name are considered to be the same entity. these entities are described using first-order logic predicates, where the name of the predicate represents a statement of truth about the given entities. in this semantic model, the communicative goal is a list of these predicates with variables used for the entity names. for instance, a communicative goal of 'red(d), dog(d)' (in english, \"say anything about a dog which is red.\") would match a sentence with the semantic representation 'red(subj), dog(subj), cat(obj), chased(subj, obj)', like \"the red dog chased the cat\", for instance."
"the results of ehmm demonstrate that matching produces superior performance during the verification of real trajectory data relative to the reference models. however, on complex roads, such as multi-layer roads and parallel roads, the challenge of accurate matching is enormous because of the quality of the fcd, the model structure and the corresponding parameters."
"the trajectory data and the corresponding road networks contain three datasets. in the previous study, the first dataset (dataset 1) was collected in guangzhou using a handheld gps device with a sampling interval of 5 s (the other intervals can be acquired by sampling) [cit] . in dataset 1, there are 421 roads from a road network consisting of ordinary roads, elevated roads and the roads below these roads. the actual route, termed route 1 (figure 1a), is divided into two groups: simple roads, also known as ordinary roads, with simple intersections that do not have parallel roads in the buffer of 50 m, and complex roads including multi-layer roads and parallel roads, e.g., an elevated road and its side roads and complex intersections. the route covers 11 different roads and 72 links for a total length of 20,258 m. to effectively evaluate the matching models, this route was sampled three times with 482, 671 and 566 sampling points. the second dataset (dataset 2) was collected from the actual trajectories of freight cars in nantong. in dataset 2, the road network stems from the vectorization of the remote sensing images and involves 14,792 roads that traverse the entire city. the actual routes, termed route 2 and route 3, are shown in figure 1b . both routes contain simple roads and complex roads. route 2 includes 49 links with a total length of 30,564 m. route 3 comprises 92 links with a total length of 45,530 m. on both routes, the sampling interval of each floating carwas30 s."
"the instantaneous status of a vehicle is considered the observation point, i.e., the gps point in the study, which has both location (longitude and latitude) and attributions (e.g., a timestamp, heading direction, and velocity). the sequence of the observed points forms a gps trajectory."
"in our work, we also view nlg as a planning problem. however, we differ in that our underlying formalism for nlg is a suitably defined markov decision process (mdp). this setting allows us to address the limitations outlined above: it is naturally probabilistic, and handles probabilistic grammars; we are able to specify complex communicative goals and general criteria through a suitably-defined reward function; and, as we show in our experiments, recent developments in fast planning in large mdps result in a generation system that can rapidly deal with very specific communicative goals. further, our system has several other desirable properties: it is an anytime approach; with a probabilistic grammar, it can naturally be used to sample and generate multiple sentences satisfying the communicative goal; and it is robust to large grammar sizes. finally, the decision-theoretic setting allows for a precise tradeoff between exploration of the grammar and vocabulary to find a better solution and exploitation of the current most promising (partial) solution, instead of a heuristic search through the solution space as performed by standard planning approaches."
"the first popular model is the ordinary hmm model. in hmm, the observation probability and the transmission probability are separately calculated [cit] . the observation probability is concerned only with the distance between the observation point and the corresponding candidate point. the transmission probability is related to the difference between the distance between two adjacent observation points and the distance between two corresponding candidate points. the smaller the absolute value of the distance difference, the greater the likelihood of transmission from the previous feature to the current."
"though restricted, this nlg problem is still difficult. a key source of difficulty is the nature of the grammar, which is generally large, probabilistic and ambiguous. some nlg techniques use sampling strategies [cit] where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. such approaches naturally handle statistical grammars, but do not solve the generation problem in a goal-directed manner. other approaches view nlg as a planning problem [cit] . here, the communicative goal is treated as a predicate to be satisfied, and the grammar and vocabulary are suitably encoded as logical operators. then automated classical planning techniques are used to derive a plan which is converted into a sentence. this is an elegant formalization of nlg, however, restrictions on what current planning techniques can do limit its applicability. a key limitation is the logical nature of automated planning systems, which do not handle probabilistic grammars, or force ad-hoc approaches for doing so [cit] . a second limitation comes from restrictions on the goal: it may be difficult to ensure that some specific piece of information should not be communicated, or to specify preferences over communicative goals, or specify general conditions, like that the sentence should be readable by a sixth grader. a third limitation comes from the search process: without strong heuristics, most planners get bogged down when given communicative goals that require chaining together long sequences of operators [cit] ."
"though the nlg-as-planning approaches are elegant and appealing, a key drawback is the difficulty of handling probabilistic grammars, which are readily handled by the overgeneration and ranking strategies. recent approaches such as pcrisp [cit] attempt to remedy this, but do so in a somewhat ad-hoc way, by transforming the probabilities into costs, because they rely on deterministic planning to actually realize the output. in this work, we directly address this by using a more expressive underlying formalism, a markov decision process (mdp). we show empirically that this modification has other benefits as well, such as being anytime and an ability to handle complex communicative goals beyond those that deterministic planners can handle."
"following prior work [cit], we consider a series of sentence generation problems which require the planner to generate a sentence like \"the adj 1 adj 2 ... adj k dog chased the cat.\", where the string of adjectives is a string that distinguishes one dog (whose identity is specified in the problem description) from all other entities in the world. in this experiment, maxdepth was set equal to 1, since each action taken improved the sentence in a way measurable by our reward function. numt rials was set equal to k(k + 1), since this is the number of adjoining sites available in the final step of generation, times the number of potential words to adjoin. this allows us to ensure successful generation in a single loop of the struct algorithm."
"multiple goals. we first evaluate struct's ability to accomplish multiple communicative goals when generating a single sentence. in this experiment, we modify the problem from the previous section. in that section, the referred-to dog was unique, and it was therefore possible to produce a referring expression which identified it unambiguously. in this experiment, we remove this condition by creating a situation in which the generator will be forced to ambiguously refer to several dogs. we then add to the world a number of adjectives which are common to each of these possible referents. since these adjectives do not further disambiguate their subject, our generator should not use them in its output. we then encode these adjectives into communicative goals, so that they will be included in the output of the generator despite not assisting in the accomplishment of disambiguation. for example, assume we had two black cats, and we wanted to say that one of them was sleeping, but we wanted to emphasize that it was a black cat. we would have as our goal both \"sleeps(c)\" and \"black(c)\". we want the generator to say \"the black cat sleeps\", instead of simply \"the cat sleeps.\""
"where ap is the point-matching accuracy, m pt denotes the number of observation points matched to the correct features, and n all is the total number of observation points. to analyze variations in the matching accuracy and the running time among the three models at different sampling intervals, a simple linear regression model was employed."
"suppose someone wants to tell their friend that they saw a dog chasing a cat. given such a communicative goal, most people can formulate a sentence that satisfies the goal very quickly. further, they can easily provide multiple similar sentences, differing in details but all satisfying the general communicative goal, with no or very little error. natural language generation (nlg) develops techniques to extend similar capabilities to automated systems. in this paper, we study the restricted nlg problem: given a grammar, lexicon, world and a communicative goal, output a valid english sentence that satisfies this goal. the problem is restricted because in our work, we do not consider the issue of how to fragment a complex goal into multiple sentences (discourse planning)."
"we now describe our approach to solving the mdp above to generate a sentence. determining the optimal policy at every state in an mdp is polynomial in the size of the state-action space [cit], which is intractable in our case. but for our application, we do not need to find the optimal policy. rather we just need to plan in an mdp to achieve a given communicative goal. is it possible to do this without exploring the entire state-action space? recent work answers this question affirmatively. new techniques such as sparse sampling [cit] and uct [cit] show how to generate near-optimal plans in large mdps with a time complexity that is independent of the state space size. using the uct approach with a suitably defined mdp (explained above) allows us to naturally handle probabilistic grammars as well as formulate nlg as a planning problem, unifying the distinct lines of attack described in section 2. further, the theoretical guarantees of uct translate into fast generation in many cases, as we demonstrate in our experiments."
"we formulate nlg as a planning problem on a markov decision process (mdp) [cit] ). an mdp is a tuple (s, a, t, r, γ) where s is a set of states, a is a set of actions available to an agent, t :"
"where p azi is the azimuth constraint probability, which is quantified by the absolute cosine function. the argument α obs is the instantaneous heading direction of the observation point, and α candi denotes the azimuth of the candidate point. as shown in figure 3, the candidate points p e and p f have two directions and one direction, respectively. thus, equation (3) can be used to evaluate the included angle between the direction of the observation point and the cf."
"as we can see in figures 4a, 4b, and 4c, struct successfully generates results for conjunctions of up to five sentences. this is not a hard upper bound, but generation times begin to be impractically large at that point. fortunately, human language tends toward shorter sentences than these unwieldy (but technically grammatical) sentences."
"in addition, the following assumptions are made to compare the running times of the three models. (1) all models are running on the same hardware device, i.e., a personal computer (pc) equipped with a 4-core cpu and a 4-gb memory-chip. (2) all models are running on the same operating system (os) in pc, i.e., 32-bit windows 7 os. (3) all models are designed as single-thread programs and implemented in java."
"for these experiments, struct was implemented in python 2.7. [cit] version of crisp which uses a java-based graphplan implementation. all of our experiments were run on a 4-core amd phenom ii x4 995 processor clocked at 3.2 ghz. both systems were given access to 8 1 we were unfortunately unable to get the pcrisp system to compile, and so we could not evaluate it."
"we have described an accurate and efficient mm model, called ehmm, for matching gps data to a digital map. the model comprehensively considers elements such as the shortest distance, heading direction, traffic regulation, topology and shape similarity. compared with the existing hmm models, the obvious improvements of the ehmm model can be summarized as follows: (1) the ehmm model considers traffic rules and thus can ensure that vehicles obey traffic rules at post-matching points; (2) the ehmm model considers a shape similarity constraint, thereby ensuring that the probability distribution of the next state depends not only on the present state but also on past states. this strategy makes use of more information from historical trajectory points to reduce the matching error caused by the data uncertainty of the present trajectory point; and (3) the ehmm model considers topological information expressed in two different forms. when the sampling rate is high or moderate, explicit topological information is used in the ehmm model. when the sampling rate is low, implicit topological information is considered. based on the ground truth data, the matching performance of ehmm was analyzed."
"we find that, in all cases, these otherwise useless adjectives are included in the output of our generator, indicating that struct is successfully balancing multiple communicative goals. as we show in figure 3b (the \"positive goals\" curve), the presence of additional satisfiable semantic goals does not substantially affect the time required for generation. we are able to accomplish this task with the same very high frequency as the crisp comparisons, as we use the same parameters."
"first, the matching accuracy of ehmm for a high sampling rate was evaluated. the results indicate that the matching accuracy of ehmm was higher than that of the reference models, namely, sm and hmm, for simple roads and complex roads. in addition, ehmm achieved better performance for simple roads than for complex roads; similar results were obtained for the reference models."
"we have proposed struct, a general-purpose natural language generation system which is comparable to current state-of-the-art generators. struct formalizes the generation problem as an mdp and applies a version of the uct algorithm, a fast online mdp planner, to solve it. thus, struct naturally handles probabilistic grammars. we demonstrate empirically that struct is anytime, comparable to existing generation-asplanning systems in certain nlg tasks, and is also capable of handling other, more complex tasks such as negated communicative goals."
"similar to previous studies [cit], the observation probability model refers only to the current status, i.e., the current cf and the current observation point. this status is controlled by the distance and azimuth."
"the weight model uses the weight coefficients to minimize the total mm error in terms of identification of the correct links. for example, [cit] described an enhanced weight-based mm (ewmm) model in which the weights were determined from real-world field data using an optimization method. they introduced two new weights for turn restrictions at the junctions and link connectivity. the matching results revealed that the ewmm model was superior to reference models, particularly at junctions. however, the ewmm model contains numerous parameters and coefficients that are static and restricted to a specific operational environment. developing some methods to calculate the weights for each gps point based on its special circumstances may improve the output [cit] ."
"where p topo is the topology constraint probability, and r is the topology stamp. the r value involves the spatial relation between two cfs. if two cfs are unequal and their intersection is not empty (i.e., if the two features are adjacent), then r is a positive constant. if two cfs are overlaid and the intersection between one feature and the projective point of the observed is not empty (i.e., if the two features are equal and if one feature contains the projective point of the observed), then r is zero. in other cases, r is infinite. equation (7) indicates that larger value of the topology constraint probability corresponds to a greater chance of transmission from the previous cf to the current cf."
"where p obs is the observation probability, which comprehensively considers the constraint conditions, including the distance, the azimuth and the traffic regulation between the observation point and the cf."
"in order to control the search space, we restrict the structure of the mdp so that while substitutions are available, only those operations are considered when determining the distribution over the next state, without any adjoins. we do this is in order to generate a complete and valid sentence quickly. this allows struct to operate as an anytime algorithm, described further below."
"the final component of the mdp is the discount factor. we generally use a discount factor of 1; this is because we are willing to generate lengthy sentences in order to ensure we match our goal. a discount factor of 1 can be problematic in general since it can cause rewards to diverge, but since there are a finite number of terms in our reward function (determined by the communicative goal and the fact that because of lexicalization we do not loop), this is not a problem for us."
"the tree policy needed by uct for a state s is the action a in that state which maximizes:, a) is the estimated value of a as observed in the tree search, computed as a sum over future rewards observed after (s, a). n (s) and n (s, a) are visit counts for the state and stateaction pair. thus the second term is an exploration term that biases the algorithm towards visiting actions that have not been explored enough. c is a constant that trades off exploration and exploitation. this essentially treats each action decision as a bandit problem; previous work shows that this approach can efficiently select near-optimal actions at each state."
"below, we first describe related work, followed by a detailed description of our approach. we then empirically evaluate our approach and a state-ofthe-art baseline in several different experimental settings and demonstrate its effectiveness at solving a variety of nlg tasks. finally, we discuss future extensions and conclude."
"most existing mm models can be characterized as either offline or online [cit] . offline models, which are also known as global models, batch process an entire input trajectory prior to generating a solution. these models attempt to obtain a curve that is as close as possible to a vehicle trajectory in a road network as the most likely road route. to evaluate the quality of the matching result, the fréchet distance between the trajectory and the matched path in the road network is used [cit] ."
"in this section, we describe our approach, called sentence tree realization with uct (struct). we describe the inputs to struct, followed by the underlying mdp formalism and the probabilistic planning algorithm we use to generate sentences in this mdp."
"based on the observation probability and the transmission probability, the hidden markov model is applied to solve the mm problem. the output probability is formed as:"
"where p trans is the transmission probability, which can ensure the logical rationality of the feature transmission. however, the transmission probability is more suited for high sampling rates. for compatibility with low sampling rates, a shortest path algorithm is used to amend the transmission probability and is defined as:"
"despite this issue, struct is capable of generating these sentences. figure 3c shows the score of struct's generated output over time for two nested clauses. notice that, because the exact reward function is being used, the time to generate is longer in this experiment. to the best of our knowledge, crisp is not able to generate sentences of this form due to an insufficiency in the way it handles tags, and consequently we present our results without this baseline."
"a comparison of running times for various sampling intervals on route 1 is shown in figure 9 . the running times among the three models increased as the sampling interval increased. the running time of the sm model was similar to that of the hmm model. the difference in the running time between the ehmm model and the other models is notable. prior to the sampling interval of 40s, the running time of the ehmm model was lower than that of the other models. over this period, the ehmm model calculated that the transmission probability primarily depended on the topological constraint, which worked well, particularly for the high sampling rate. the sm and hmm models adopted the shortest path to calculate the transmission probability. in this study, the candidate radius of the models was dynamic and related to the sampling interval. as shown in figure 10, the running time was significantly positively correlated with the candidate radius for the three models. when the sampling rate was high, calculating the topological relation was much faster than computing the shortest path based on the uniform candidate radius. subsequently, ehmm requires increasingly longer amounts of time. for sampling intervals exceeding 90 s, the difference in the running time between the ehmm model and the other models was significant and averaged 158 ms. as previously mentioned, when the sampling rate decreased, the topological constraint weakened. the ehmm model used the shortest path to correct the transmission probability. the running time of the ehmm model included the above two scenes. thus, the running time of the ehmm model was slower than that of the other models."
"the second popular model is the stm model, which includes two modules, namely, the spatial analysis and the temporal analysis, as discussed in the literature [cit] . the spatial analysis incorporates the observation probability and the transmission probability. similar to hmm, the observation probability and the transmission probability are related to the distance. however, the probability expressions of stm differ significantly from those of hmm. the temporal analysis refers the average velocity. in the present study, obtaining the velocity of the road segment was prohibitively challenging; thus, the temporal analysis was abandoned. for this reason, the stm model is referred to as the sm model hereinafter."
"struct takes three inputs in order to generate a single sentence. these inputs are a grammar (including a lexicon), a communicative goal, and a world specification."
"a world specification is simply a list of all statements which are true in the world surrounding our generation. matching entity names refer to the same entity. we use the closed world assumption, that is, any statement not present in our world is false. before execution begins, our grammar is pruned to remove entries which cannot possibly be used in generation for the given problem, by tran- sitively discovering all predicates that hold about the entities mentioned in the goal in the world, and eliminating all trees not about any of these. this often allows struct to be resilient to large grammar sizes, as our experiments will show."
"the immediate value of a state, intuitively, describes closeness of an arbitrary partial sentence to our communicative goal. each partial sentence is annotated with its semantic information, built up using the semantic annotations associated with the pltag trees. thus we use as a reward a measure of the match between the semantic annotation of the partial tree and the communicative goal. that is, the larger the overlap between the predicates, the higher the reward. for an exact reward signal, when checking this overlap, we need to substitute each combination of entities in the goal into predicates in the sentence so we can return a high value if there are any mappings which are both possible (contain no statements which are not present in the grounded world) and mostly fulfill the goal (contain most of the goal predicates). however, this is combinatorial; also, most entities within sentences do not interact (e.g. if we say \"the white rabbit jumped on the orange carrot,\" the whiteness of the rabbit has nothing to do with the carrot), and finally, an approximate reward signal generally works well enough unless we need to emit nested subclauses. thus as an approximation, we use a reward signal where we simply count how many individual predicates overlap with the goal with some entity substitution. in the experiments, we illustrate the difference between the exact and approximate reward signals."
"the maximum running time, i.e., the time required for the ehmm model to process one gps point, was approximately 7 s, which was substantially shorter than the sampling interval. thus, the ehmm model is also appropriate for the operation."
"we use a modified version of uct in order to increase its usability in the mdp we have defined. first, because we receive frequent, reasonably accurate feedback, we favor breadth over depth in the tree search. that is, it is more important in our case to try a variety of actions than to pursue a single action very deep. second, uct was originally used in an adversarial environment, and so is biased to select actions leading to the best average reward rather than the action leading to the best overall reward. this is not true for us, however, so we choose the latter action instead. with the mdp definition above, we use our modified uct to find a solution sentence (algorithm 1). after every action is selected and applied, we check to see if we are in a state in which the algorithm could terminate (i.e. the sentence has no nonterminals yet to be expanded). if so, we determine if this is the best possibly-terminal state we have seen so far. if so, we store it, and continue the generation process. whenever we reach a terminal state, we begin again from the start state of the mdp. because of the structure restriction above (substitution before adjoin), struct generates a valid sentence quickly. this enables struct to perform as an anytime algorithm, which if interrupted will return the highestvalue complete and valid sentence it has found. this also allows partial completion of communicative goals if not all goals can be achieved simultaneously in the time given."
"in ehmm, the parameters were set to the empirical constants. due to the quality of the fcd and the vector map, the parameters were not always sufficient. thus, optimization of the model parameters warrants future research."
"a second line of attack formalizes nlg as an ai planning problem. spud [cit], a system for nlg through microplanning, considers nlg as a problem which requires realizing a deliberative process of goal-directed activity. many such nlg-as-planning systems use a pipeline architecture, working from their communicative goal through discourse planning and sentence generation. in discourse planning, information to be conveyed is selected and split into sentence-sized chunks. these sentence-sized chunks are then sent to a sentence generator, which itself is usually split into two tasks, sentence planning and surface realization [cit] . the sentence planner takes in a sentence-sized chunk of information to be conveyed and enriches it in some way. this is then used by a surface realization module which encodes the enriched semantic representation into natural language. this chain is sometimes referred to as the \"nlg pipeline\" [cit] ."
"the matching performance of ehmm for the actual trajectories of freight cars was verified. the results reveal that the matching accuracy of ehmm was significantly higher than that of the reference models on actual roads. the running time of ehmm was notably shorter than that of the reference models. the matching results of ehmm retained the topological adjacency between two roads and complied with traffic regulations better than the reference models. moreover, the ehmm model is competitive relative to the other newer and more efficient models in terms of matching accuracy."
"in the nlg-as-planning framework, the choice of grammar representation is crucial in treating nlg as a planning problem; the grammar provides the actions that the planner will use to generate a sentence. tree adjoining grammars (tags) are a common choice [cit] . tags are tree-based grammars consisting of two sets of trees, called initial trees and auxiliary or adjoining trees. an entire initial tree can replace a leaf node in the sentence tree whose label matches the label of the root of the initial tree in a process called \"substitution.\" auxiliary trees, on the other hand, encode recursive structures of language. auxiliary trees have, at a minimum, a root node and a foot node whose labels match. the foot node must be a leaf of the auxiliary tree. these trees are used in a three-step process called \"adjoining\". the first step finds an adjoining location by searching through our sentence to find any subtree with a root whose label matches the root node of the auxiliary tree. in the second step, the target subtree is removed from the sentence tree, and placed in the auxiliary tree as a direct replacement for the foot node. finally, the modified auxiliary tree is placed back in the sentence tree in the original target location. we use a variation of tags in our work, called a lexicalized tag (ltag), where each tree is associated with a lexical item called an anchor."
"-the box-jenkins method; -the dbn network; -the multilayered gmdh algorithm; -the anfis method. the matlab software package was used to train forecasting models using the corresponding methods (a new function was written to implement the proposed algorithm). in order to perform a more comprehensive comparison, multiple possible values for the number of previous time series values to be used as inputs were tested, namely all values from 2 to 10 inclusively. all models were trained to give a year ahead prediction. all methods were used with their default hyperparameters values. 80% of an entire examples set were randomly selected as a training set, the remaining 20% were used to test the forecasting model's performance. comparisons of the results are presented in the following tables. as seen from the results of the comparison, the suggested algorithm's average, min and max mae is the smallest of all the compared methods."
the essence of the proposed approach is to view a multi-layered gmdh algorithm as a pre-training stage of fitting a deep polynomial neural network and then to apply a resilient error backpropagation algorithm (rprop) [cit] ) with dropout to perform finetuning.
"figures with sensitive values, including the quality score, the fraud score, and the number of clicks have been anonymized as follows: the original values have been transformed by arbitrary constants so as to preserve trends and relative differences while obscuring the absolute numbers."
"method for the determination of the dimension of a set is the measurement of the kolmogorov capacity (i.e., box-counting dimension). this method covers the set with tiny cells/boxes (squares for sets embedded in 2d and cubes for sets embedded in 3d space) having size ϵ. the dimension d can be defined as follows [cit] :"
"analysis techniques 3.1. reconstructed phase space analysis of a dynamical system. the phase space is an abstract multidimensional space, which is used to graphically represent all the possible states of a dynamical system [cit] . the dimension of the phase space is the number of variables required to completely describe the state of the system [cit] . its axes depict the values of the dynamical variables of the system [cit] . if the actual number of variables governing the behaviour of the dynamical system is unknown, then the phase space plots are reconstructed by time-delayed embedding, which is based on the concept of taken's theorem [cit] . the theorem states that if the dynamics of a system is governed by a number of interdependent variables (i.e., its dynamics is multidimensional), and only one variable of the system, say, x, is accessible (i.e., only one dimension can be measured), then it is possible to reconstruct the complete dynamics of the system from the single observed variable x by plotting its values against itself for a certain number of times at a predefined time delay [cit] have reported that the reconstructed phase spaces can be regarded as topologically equivalent to the original system and, hence, can recover the nonlinear dynamics of the system. let us consider that all the values of the observed variable x is represented by the vector x."
"for each group and each bucket we compute a percentile threshold, t. in real time, if any publisher receives more than t% of her traffic on this bucket, its traffic from this bucket gets filtered. to set t, we carry out a fine-grain scan of all the possible percentiles of this bucket. for each percentile, p, we aggregate the traffic from all the publishers that received more than p% of their traffic from that bucket, with some binomial confidence threshold. if the quality score of this aggregated traffic is lower than q min, we set p as a candidate threshold. at the end, we pick the threshold, t, to be the candidate threshold that has the highest impact, i.e., the largest number of discarded traffic. this technique takes into account the observed empirical distributions, the number of available samples (ip sizes), and the desired confidence level."
"the remainder of this paper is organized as follows. in section ii, we define the ip size and describe how attacks using machine-generated traffic affect the ip size distribution. in section iii, we describe the data set used in this study. in section iv, we summarize the notation used throughout this paper. in section v, we show how to distinguish a publisher's legitimate traffic from fraudulent traffic. in section vi, we show how to detect fraud at the publisher's level. in section viii, we discuss the strengths and limitations of this work. in section ix, we conclude the paper."
"each figure is a four-dimensional plot. the x-axis represents the bucket of the ip size, while the y-axis represents the probability value. each point is associated with a single publisher and represents the probability that the publisher receives a click of a certain size. in fig. 8(a) and 8(c) the size of data points represents the number of clicks and the color represents the scaled fraud score. fig. 8 (b) and 8(d) display the same points as in fig. 8(a) and 8(c) with the difference that the size represents the number of clicks fed to the quality classifier system, and the color represents the scaled quality score. we chose to plot circles with different sizes to represent different levels of statistical confidence."
"the unbounded time series x t is then split into a number of portions of equal length n, and a straight line fitting is performed to the data using the method of least square fitting. the fluctuation (i.e., the root-mean-square variation) for every portion from the trend is calculated using [cit]"
"each point in the reconstructed phase space of a system describes a potential state of the system. the system starts evolving from any point in the phase space (regarded as the initial state/condition of the system), following the dynamic trajectory determined by the equations of the system [cit] . a dynamic trajectory describes the rate of change of the system's state with time. all the possible trajectories, for a given initial condition, form the flow of the system. each trajectory occupies a subregion of the phase space, called as an attractor. an attractor can also be defined as a set of points (indicating the steady states) in the phase space, through which the system migrates over time [cit] . the 3d attractor of the rri time series (represented in figure 2 ) has been shown in figure 5 ."
"1. \"preliminary\" deep network training (pre-training), the essence of which is to add new layers one by one and during which the weights between two layers are trained separately -most often using the restricted boltzmann machine training algorithm. 2. \"tuning\" of the obtained network structure using the error back propagation algorithm (or some modification of it), sometimes with the use of regularization methods (currently, a dropout algorithm [cit] ) is the most popular regularization method for the training of deep networks)."
"in recent years, ecg is being widely explored as a biometric to secure body sensor networks, human identification, and verification [cit] . as compared to the other biometrics, it provides the advantage that it has to be acquired from a living body. in many previous studies related to the ecg-based biometric, features extracted from the ecg signals were soon-terminating atrial fibrillation and immediately terminating atrial fibrillation a genetic algorithm in combination with svm 100% sensitivity, 100% specificity, and 100% accuracy [cit] amplitudes, durations, and areas of p, q, r, s, and t waves [cit] . however, the extraction of these features becomes difficult when the ecg gets contaminated by noise [cit] . wavelet analysis of the ecg signals was also attempted for the extraction of the ecg features for the identification of persons [cit] . but, it required shifting of one ecg waveform with respect to the other for obtaining the best fit [cit] . recently, fang and chan proposed the development of an ecg biometric using the phase space analysis of the ecg signals [cit] . the phase space plots were reconstructed from the 5 sec ecg signals, and the trajectories were condensed, single course-grained structure. the distinction between the course-grained structures was performed using the normalized spatial correlation (nsc), the mutual nearest point match (mnpm), and the mutual nearest point distance (mnpd) methods. the proposed strategy was tested on 100 volunteers using both single-lead and 3-lead ecg signals. the use of single-lead ecg signals resulted in the person identification accuracies of 96%, 95%, and 96% for mnpd, nsc, and mndp methods, respectively, whereas, the accuracies increased up to 99%, 98%, and 98% for 3-lead ecg signals. earlier, the same group had proposed the ecg biometric-based identification of humans by measuring the similarity or dissimilarity among the phase space portraits of the ecg signals [cit] . in the experiment involving 100 volunteers, the person identification accuracies of 93% and 99% were achieved for single-lead and 3-lead ecg, respectively."
"predicting market demand for air transportation is of great significance for airlines, as well as for investors, since the accuracy of such a prediction has a big impact on investment efficiency [cit] . therefore, airline transportation demand metrics, like revenue passenger kilometers (rpk), are one of the key factors that are considered when preparing an airline's annual operating plan, performing fleet planning and developing the route network [cit] . besides, examining and estimating an airline's transportation demand may likewise help an airline mitigate its risk through an objective assessment of the demand side of the airline business [cit] ."
"a total of n − d − 1 τ number of such vectors are obtained, which can be arranged in a matrix v (9) [cit] ."
"hit inflation attacks represent the biggest threat to the internet advertising industry [cit] . in this paper, we share our experience in building a fraud detection system at google. hit inflation attacks represent a specific application of the techniques and methodologies presented here. however, these can be applied, generally, to detect machine-generated traffic. the main contributions of this work are as follows:"
"2 is the sum of squares of residuals. eq. (2) can be interpreted as the amount of variance captured by proposed model. moreover, in contrast with the r 2 statistic, which does not decrease with more regressors, r 2 penalizes the use of a large number of regressors unless it significantly improves the explanatory power of the model. fig. 11 shows that as we use more statistical tests, the adjusted coefficient of determination increases. this demonstrates that additional features increase the explained variance of the model. when all features are used, the model in eq. (1) captures over 40% of the total variation in the data. this result is particularly significant in a large data set that includes a wide range of patterns of click traffic."
"to accurately characterize the deviation, if any, between the observed and the expected distribution of each entity we use an ensemble of different statistical methods. these can be grouped in four wide categories:"
"currently, main approaches to forecasting problems can be divided into 2 groups: -\"classical\" methods like linear regression, or boxjenkins method [cit], etc. -methods based on artificial intelligence -group method of data handling [cit] ) methods family, artificial neural networks-based methods, genetic algorithms, fuzzy logic-based methods and different hybrid algorithms, like adaptive networkbased fuzzy inference system. let us shortly review the main methods from both groups."
"in this paper, we leverage an internal classifier that takes as input click logs of network traffic and determines the likelihood that the network traffic is fraudulent machinegenerated traffic. we call the score obtained through this system the quality score. this classification system takes as input a variety of features that accounts for different types of user inputs, and different types of anomalies. this classifier provides us with an estimate on the aggregate quality of a large set of clicks. similar classifiers exist for other kinds of attacks depending on the application. for instance, in the case of email spam a classifier can be built on several features of the email. one of the features could be the ratio of users that labeled this email as spam. another feature could be the number of valid and invalid recipient addresses, and so on."
"as observed in sec. ii-b, the type of services provided by the publisher's website and the type of traffic driven to her website affect the ip size distribution of a publisher. furthermore, this is also influenced by the geo-location of the source ip addresses visiting her website. the rationale behind this is that different countries have different ip size distributions due to various reasons, such as heavy use of proxy, population density vs. number of ip addresses available, and government policies."
we analyze a sample of click logs collected for a period of 90 consecutive days. our analysis and development rely on the following fields in each entry:
"the geometrical objects possess a definite dimension. for example, a point, a line, and a surface have dimensions of 0, 1, and 2, respectively [cit] . this notion has led to the development of the concept of fractal dimension. a fractal dimension refers to any noninteger dimension possessed by the set of points (representing a dynamical system) in a euclidean space. the determination of the fractal dimension plays a significant role in the nonlinear dynamic analysis. this may be attributed to the fact that the strange attractors are fractal in nature and their fractal dimension indicates the minimum number of dynamical variables required to describe the dynamics of the strange attractors. it also quantitatively portrays the complexity of a nonlinear system. the higher is the dimension of the system; the more is the complexity. the commonly employed figure 3: computation of the optimal embedding dimension by the method of false nearest neighbours. the optimal embedding dimension was 7, and the corresponding percent false neighbour was 44.83%. the method of false nearest neighbour was implemented using visual recurrence analysis freeware (v4.9, usa), developed by kononov [cit] . figure 5: 3d phase space attractor of an rri time series. the attractor was plotted using the matlab toolbox developed by yang [cit] ."
"1. model identification and model selection: checking the stationarity of variables, checking the seasonality in the dependent series, plotting the autocorrelation and partial autocorrelation functions of the dependent time series to decide which autoregressive or moving average component should be used in the model; 2. parameter estimation using computation algorithms to arrive at coefficients that best fit the selected arima model. the most common methods use maximum likelihood estimation or non-linear least-squares estimation; 3. model checking by testing, to determine whether the estimated model conforms to the specifications of a stationary univariate process. in particular, the residuals should be independent of each other and constant in mean and variance over time. plotting the mean and variance of residuals over time and performing a ljung-box test or plotting autocorrelation and partial autocorrelation of the residuals are helpful to identify misspecification. if the estimation is inadequate, we have to return to step one and attempt to build a better model. group method of data handling (gmdh) is a set of forecasting algorithms that are based on a selection of the best models from the set of trained simple models and the subsequent construction of more complex models using the selected ones. the accuracy of the forecast is improved with the increase in the complexity of the models. the selection criterion is based on the performance of the models on the validation set, while the model parameters are determined using the training set. the simplest models, also called the basis functions, are usually of the following form:"
publicly available time series of australia's major domestic airlines yearly revenue passenger kilometers (rpk) [cit] were used to test the performance of the suggested algorithm compared to:
"today, a large number of internet services such as web search, web mail, maps, and other web-based applications are provided to the public free of charge. at the same time, designing, deploying, and maintaining these services is expensive. they must have high availability, be able to serve any user, anonymous or logged in, and from anywhere in the world. [cit] generated over $22 billion [cit] in the u.s. alone."
"finally, we also use two sets of blacklists, the gmail blacklist [cit] and the spamhaus exploit blacklist (xbl) [cit], to determine whether or not the ip addresses that generate fraudulent ad events are also known to generate other types of abusive traffic. gmail blacklist is a list of source ips that are likely to send email spam. spamhaus xbl is a realtime database of hosts infected by some exploits."
"in the wide area of anomaly detection, [cit] represents a recent survey on various categories of anomaly detection systems. our work in this paper falls in the category of statistical anomaly detection, i.e., we define as an anomaly an observation that is extremely unlikely to have been generated by the probabilistic model assumed. [cit] discusses various fraudulent schemes in telecommunications and possible techniques to mitigate them. [cit] presents a histogram filter similar in spirit to the ip size histogram filter. however, our work differs in both the problem scope and the approach used to measure deviations and compare distributions."
"for these reasons, we group together publishers that provide the same type of service (e.g., web search, services for mobile users, content sites, and parked domain websites), and receive clicks from the same type of connecting device (e.g., desktops, smart-phones, and tablets), and from ip addresses assigned to the same country. for instance, if a publisher receives clicks from more than one type of device, its traffic is split depending on the type of devices, and accordingly assigned to different groups. this provides a fine grained grouping of publishers which takes into account the various factors that affect the ip size."
"the main advantage of the recurrence plot is that it does not require any mathematical transformation or assumption [cit] . but the drawback of this method lies in the fact that the information provided is qualitative. to overcome this limitation, several measures of complexity that quantify the smallscale structures in the recurrence plot have been proposed by many researchers, regarded as recurrence quantification analysis (rqa) [cit] . these measures are derived from the recurrence point density as well as the diagonal and the vertical line structures of the recurrence plot. the calculation of these measures in small windows, passing along the line of identity (loi) of the recurrence plot, provides information about the time-dependent behaviour of these variables. several studies have reported that the rqa variables can detect the bifurcation points like the chaos-order transitions [cit] . the vertical structures in the recurrence plot have been reported to represent the intermittency and the laminar states. the rqa variables, corresponding to the vertical structures, enable the detection of the chaos-chaos transition [cit] . the following discussion introduces the rqa parameters along with their potentials in the identification of the changes in the recurrence plot."
"as per the reported literature, the random signals produced by noise fundamentally differ from the random signals produced from the deterministic dynamical systems with a small number of dynamical variables [cit] . the differences between them cannot be analyzed using the statistical methods. phase space reconstruction-based dynamical system analysis has been recommended by the researchers for this purpose [cit] ."
"we define the ip size as the number of users sharing the same ip address. estimating the ip size is a challenging problem in its own. several users might share the same host machine, or might connect through the same network address translation (nat) device or even a cascade of nats, as illustrated in fig. 2 . moreover, the ip size changes over time as new users join the local network and share the same public ip and others leave, or as the ip address gets reassigned to a different host."
"despite their differences, most attacks share a common characteristic: they induce an unexpected deviation of the ip size distribution. the attacks in fig. 4 represent two opposite scenarios. however, in both cases the attack is revealed as a deviation from the expected ip size distribution. in general, different deviations represent different signatures of attacks."
"to detect deviations between the expected and the observed entity distribution, r and f (p ), that are induced by machinegenerated traffic. fig. 9 illustrates the workflow of the system we implemented at google. the first step is the estimation of the expected ip size distribution of each entity. each group might have a different ip size distribution. however, entities within the same group are expected to share a similar distribution. since the majority of fraudulent clicks are already filtered out by existing detection systems, we use the aggregate distribution of legitimate ip sizes within each group as an estimation of the true (unknown) ip size distribution for that group. next, we use a set of statistical methods to accurately characterize the deviation between the observed and expected distribution. as noted in fig. 4, different attacks result in different deviations in the ip size distribution. finally, we use an ensemble learning model [cit] to combine the outcome of these methods in a signature vector specific to each entity, and we train a regression model that identifies and classifies signatures associated with fraudulent entities."
"-2 k c models of the form: -are trained using a linear regression on the training set; -for each model f, its error is calculated on the validation set:"
"fabio soldo was partly at an internship with the traffic quality team at google inc. and partly at uc irvine while this work was conducted. the work was partially supported by the nsf cybertrust grant 0831530. fig. 1 depicts a simple scenario with three publishers, where each publisher represents a different type of traffic. advertisements on the publisher sites thispagemakesmoney.com and thispagetoo.com receive legitimate traffic, i.e., users interested in the advertisements clicked on them. advertisements on thispagetoo.com also receive fraudulent traffic. for instance, the publisher might ask her friends to repeatedly click on advertisements displayed on her site. finally, in a more sophisticated hit inflation attack, publisher iwontmakemoney.com uses a botnet to automatically generate a large amount of fraudulent traffic. this simple example illustrates the complexity of the problem. three publishers contract with an advertising network to host advertisements for a commission, for each click on these advertisements. the three publishers illustrate three types of traffic: (1) advertisements on the publisher site thispagemakesmoney.com are clicked only by legitimate users (white pointers); (2) advertisements on thispagetoo.com are clicked by both legitimate and fraudulent users (red pointers); and (3) advertisements on iwontmakemoney.com are not clicked by legitimate users-instead, iwontmakemoney.com uses a large botnet to generate fraudulent traffic."
"3.2. lyapunov exponents. the nonlinear dynamical systems are highly sensitive to the initial conditions, that is, a small change in the state variables at an instant will cause a large change in the behaviour of the system at a future instant of time. this is visualized in the reconstructed phase space as the adjacent trajectories that diverge widely from the initial close positions or converge. lyapunov exponents are a quantitative measure of the average rate of this divergence or convergence [cit] . they provide an estimation of the duration for which the behaviour of a system is predictable before chaotic behaviour prevails [cit] . positive lyapunov exponent values indicate that the phase space trajectories are diverging (i.e., the closely located points in the initial state are rapidly separating from each other in the ith direction) and the system is losing its predictability, exhibiting chaotic behaviour [cit] . on the other hand, the negative lyapunov exponent values are representatives of the average rate of the convergence of the phase space trajectories. for example, in a three-dimensional system, the three lyapunov exponents provide information about the evolution of the volume of a cube and their sum specifies how a hypercube evolves in a multidimensional attractor. the sum of the positive lyapunov exponents represents the rate of spreading of the hypercube, which in turn, indicates the increase in unpredictability per unit time. the largest positive (dominant) lyapunov exponent mainly governs its dynamics [cit] ."
"in the last few decades, the ecg signals have been widely analyzed for the diagnosis of the numerous cardiovascular diseases [cit] . apart from this, the ecg signals are processed to extract the rr intervals, which have been reported to divulge information about the influence of the autonomic nervous system activity on the heart through heart rate variability (hrv) analysis [cit] . hrv refers to the study of the variation in the time interval between consecutive heart beats and the instantaneous heart rate [cit] . an important step in the analysis of the ecg signals is the extraction of the clinically relevant features containing all the relevant information of the original ecg signal and, hence, can act as the representative of the signal for further analysis [cit] . features can be extracted from the ecg signals using the time-domain, frequency-domain, and joint time-frequency domain analysis methods including the nonlinear methods [cit] . the analysis of the ecg signals using the nonlinear signal analysis methods has received special attention of the researchers in recent years [cit] . the nonlinear methods of the ecg signal analysis derive their motivation from the concept of nonlinear dynamics [cit] . this may be attributed to the fact that the biomedical signals like ecg can be generated by the nonlinear dynamical systems [cit] . a dynamical system is a system that changes over time [cit] . however, a dynamical system may also be defined as an iterative physical system, which undergoes evolution over time in such a way that the future states of the system can be predicted using the preceding states [cit] . dynamical systems form the basis of the nonlinear methods of the signal analysis [cit] . the highly explored nonlinear signal analysis methods include reconstructed phase space analysis, lyapunov exponents, correlation dimension, detrended fluctuation analysis (dfa), recurrence plot, poincaré plot, approximate entropy, and sample entropy. this study attempts to provide a theoretical background of the above-mentioned nonlinear methods and their recent applications (last 5 years) in the analysis of the ecg signal for the diagnosis of diseases, understanding the effect of external stimuli (e.g., low-frequency noise and music), and human biometric authentication ( figure 1 )."
"the time delay is usually determined using either the first minimum of the average mutual information function (amif) [cit] or first zero crossing of the autocorrelation function (acf) [cit] or empirically. the implementation of acf is computationally convenient and does not require a large data set. however, it has been reported that the use of acf is not appropriate for nonlinear systems, and hence amif should be used for the computation of the optimal time delay [cit] . for the discrete time signals, the amif can be defined as follows [cit] :"
"(vii) laminarity: laminarity (lam) is the ratio of the number of recurrence points forming vertical lines to the total number of recurrence points in the recurrence plot (31) . lam has been reported to provide information about the occurrence of the laminar states in the system. however, it does not describe the length of the laminar states. the value of lam decreases if more number of single recurrence points are present in the recurrence plot than the vertical structures."
a different line of research has proposed a data analysis approach to discriminate legitimate from fraudulent clicks. [cit] focuses on the problem of finding colluding publishers. the proposed system analyzes the ip addresses generating the click traffic for each publisher and identifies groups of publishers that receive their clicks from roughly the same ips. [cit] addresses the scenario of a single publisher generating fraudulent traffic from several ips. the authors propose a system to automatically detect pairs of publisher and ip address that are highly correlated. [cit] presents a detailed investigation on how a large botnet was used to launch click fraud attacks.
"most of the biosignals are nonstationary in nature, which often makes their analysis cumbersome using the conventional linear methods of signal analysis. this led to the development of nonlinear methods, which can perform a robust analysis of the biosignals [cit] . among the biosignals, the analysis of the ecg signals using nonlinear methods has been highly explored. the nonlinear analysis of the ecg signals has been investigated by many researchers for early diagnosis of diseases, human identification, and understanding the effect of different stimuli on the heart and the ans. the current review dealt with the relevant theory, potential, and recent applications of the nonlinear ecg signal analysis methods. although the nonlinear methods of ecg signal analysis have shown promising results, it is envisaged that the existing methods may be extended and new methods can be proposed to improve the performance and handle large and complex datasets."
"in the area of time series forecasting, deep belief networks (dbn) [cit] are showing the most promising results. a deep belief network (dbn) is a deep neural network composed of multiple layers of hidden units, with connections between the layers but not between units within each layer. when trained on a set of examples without supervision, a dbn can learn to probabilistically reconstruct its inputs. the layers then act as feature detectors. after this learning step, a dbn can be further trained with supervision to perform classification or regression. a few articles describing the application of dbns to time series forecasting are [cit], and others."
"each click, c, is associated with a source ip address, ip c, that generated the click, and with a publisher site, p k, that hosted the advertisement clicked. let s c be the ip size associated with ip c, and let n be the number of clicks on advertisements hosted by p k in a certain time period, t ."
"machine-generated attacks are performed in various ways, depending on the resources available, motivations and skills of the attackers. for instance, if an attacker controls a large number of hosts through a botnet, the attack can be highly distributed across the available hosts to maximize the overall amount of traffic generated while maintaining a low activity profile for each host individually. we refer to this type of attacks as botnet-based attacks. conversely, if an attacker controls a few hosts but still wants to generate a large amount of traffic, she can use anonymizing proxies, such as tor nodes, to hide the actual source ips involved. we refer to this type of attacks as proxy-based attacks. botnet-and proxybased attacks are two diverse examples in the wide spectrum of possible attacks using machine-generated traffic, in terms of both the resources required and level of sophistication. fig. 4 illustrates these two attacks and how they affect the ip size distribution associated with a publisher. assume that we have an a-priori knowledge of the expected ip size distribution based on historical data. let the blue curve be the expected distribution of ip sizes. fig. 4 (a) depicts an example of botnetbased attack. bots are typically end-user machines and so have a relative small ip size. intuitively, this is because end-user machines are easier to compromise than large well-maintained proxies. as a result, a botnet-based attack generates a higher than expected number of clicks with small size. analogously, a proxy-based attack skews the ip size distribution towards large ip sizes because a higher than expected number of clicks comes from large proxies, as in fig. 4(b) ."
"next, we set a minimum quality score, q min, that a set of legitimate clicks should satisfy. different websites have different quality scores depending on various factors, such as the services provided and the advertisements displayed. thus, we compute q min as a fixed fraction of the average quality score associated with each group of publishers."
"4. as a result, we simply have a polynomial neural network, so forecasting on new data is performed as usual: the input vector x  is sent on the first network layer, and, after that, the outputs of all neurons are calculated layer by layer, up to the last layer with one neuron, the output of which will be the forecast itself."
"in this section, we consider the ip size distributions associated with entities. an entity can be a user-agent, an e-mail domain, a publisher, a city, a country, and so on. in general, an entity is any dimension that aggregates ad events. for each type of entity, we can build a detection system based on the ip size distribution. this is useful to build several complementary defense mechanisms that protect against different types of attacks."
"-after the updates of all weights, a network's error on the training set is calculated, and, if the error is less than on the previous iteration, the training continues, if the opposite is true, the training stops and weights \"rollback\" to their values on the previous iteration. -according to the dropout regularization procedure, after the training stops, outgoing weights of all neurons are multiplied by value 1-p, where p is the \"deletion\" probability that was used for this neuron during training."
"for the above-mentioned reasons, fraud detection is a critical component for the well-being of many internet services. hit inflation attacks refer to the fraudulent activities of generating charges for online advertisers without a real interest in the products advertised. they can be classified into publishers' and advertisers' attacks. publishers' hit inflation attacks use fraudulent traffic in an attempt to increase publishers' revenues from online advertising. advertisers' hit inflation attacks aim at increasing the overall amount of activities, such as impressions or clicks associated with the advertisements of their competitors. the main objective of advertisers' hit inflation attacks is depleting their competitors advertising budgets. in this paper, we focus on publishers' attacks, but the same discussion applies to advertisers' attacks."
"using a stochastic gradient descent method trained on a small subset of entities, k, which includes legitimate distributions and known attacks provided both by other automated systems, and by manual investigation of the logs. the model in eq. (1) is then applied to a large data set of entities to predict the fraud score as a function of their ip size distribution. analyze the accuracy of our system when all methods are used. next, we iteratively remove the feature that causes the least amount of variation in the prediction accuracy until we are left with a single feature [cit] . we train on 10% of the entities, and test it on the remaining entities. fig. 10 shows that using multiple comparison methods that measure different type of deviations allows us to reduce the prediction errors, down to a 3% error. this is about 3 times lower than when using a single method. moreover, we observe that additional methods improve the accuracy of the model but with decreasing gain. to validate the goodness-of-fit of the model in eq. (1) we also compute the adjusted coefficient of determination,r 2 :"
"later, another method for training multilayered networks was introduced -well known nowadays as backpropagation (lecun, bottou, orr, & müller, 1998) . backpropagation allowed calculating the true gradient of error function with respect to every weight of a multilayer neural network, thus one could use any suitable first-order gradient descent method to perform weights optimization and find local optima. theoretically, this should \"solve\" the problem of training multilayer neural networks, but, in practice, results were not very good for deep networks, see [cit] for an example. the core issue with training deep networks using backpropagation was soon found: vanishing/exploding gradients problem [cit] . essentially, according to the backpropagation method, an error function's gradient for the weights in \"early\" layers (layers, that are closer to inputs) is a sum of the products of terms involving an error function's gradients for the weights in all following layers. when there are many layers that is an intrinsically unstable situation -if many gradients in deeper layers are smaller than 1 in an absolute value, their product will be close to 0, thus leading to a \"vanishing\" gradient for the early layers; and vice versa -if many gradients in deeper layers are bigger than 1 in an absolute value, their product will increase exponentially, leading to \"exploding\" gradients in early layers."
"in this paper, we use advertisement click logs collected at google from a sample of hundreds of thousands of different publisher websites. we use these logs to gain insights into modern machine-generated traffic attacks, as well as to test and evaluate the performance of our system on real data. in this section, we briefly describe the data set and the specific features used in this study."
"in this paper, we present a data-driven approach to combat machine-generated traffic based on the ip size informationdefined as the number of users sharing the same source ip address. our main observation is that diverse machinegenerated traffic attacks share a common characteristic: they induce an anomalous deviation from the expected ip size distribution. motivated by this observation, we implemented a fraud detection system that detects hit inflation attacks at different levels of granularity using statistical learning models. we show that the proposed model can accurately estimate fraud scores with a 3% average prediction error."
"intuitively, the filtered clicks represent regions of high probability for specific publishers, i.e., spikes in their ip size distributions, that also have a significantly lower quality than what we would expect for the same group of publishers and set of advertisements."
"strengths. first, our approach does not require any identification or authentication of the users generating the clicks. it only uses aggregate statistical information about the ip size. second, the proposed system is fully automated, has low complexity (it scales linearly in the amount of data to be processed), and is easy to parallelize. this makes it suitable for large-scale detection. third, the ip size is robust to dchp reassignment. clicks generated from a specific host have the same size regardless the specific ip address assigned. this is particularly useful in practice, since a large fraction of ips are dynamically reassigned every 1-3 days [cit] . fourth, the ip size-based detection is hard to evade. in fact, even if the attacker knows the legitimate distribution of ip sizes for all publishers in her group, and the exact mechanisms used to estimate the ip size, she still would need to generate clicks according to the legitimate ip size distribution. however, the attacker has access only to a limited number of bots. further, even for those bots, she cannot control the activities of legitimate users sharing the compromised machines. this in turn affects the ip size and limits her ability to arbitrarily shape the ip size distribution."
"finally, the focus of this paper is on click traffic. however, we believe that the key features exploited here, namely, the ip size generating the malicious activity, and the techniques we developed, are potentially applicable to a wide range of fraud detection problems. instead of looking at the \"size\" of ip sources generating clicks, we can analyze the size of ips generating other malicious activities, and apply a similar statistical framework for detecting anomalous distributions."
"ip size distributions. fig. 8 (a) through fig. 8 (d) depict two groups of publishers, named here a and b for anonymity purpose. these groups consist of publishers whose websites provide similar services and whose click traffic comes from the same country and the same type of device."
"suppose that a sequence of values (figure 1 ) that is considered in this paper can be formulated as: by using the available data (figure 1, a (figure 1, b) ."
(vi) ratio: it is the ratio of the determinism and the recurrence rate (30) . it has been reported to be useful for identifying the transitions in the dynamics of the system.
"these figures confirm on real data the motivating intuition discussed in fig. 4 . fig. 8(a) and fig. 8(b) show the results on one of the largest groups, comprising hundreds of publishers. despite the complexity of the problem and the variety of possible attacks, fig. 8(a) shows that spikes in the ip size distribution of a publisher are reliable indicators of high fraud score. in fact, most points associated with an anomalous high probability are red, thus indicating that they are known to be fraudulent clicks. as an additional validation, in fig. 8(b) we analyze the corresponding quality score. the spikes corresponding to high fraud score also have very low, or zero, quality score. this confirms that the clicks identified by our systems are indeed fraudulent clicks. fig. 8(c) and fig. 8(d) illustrate a sample group where the ip size distribution detects machine-generated traffic that would have been undetected otherwise. for instance, fig. 8(c) shows the case of a publisher that has about 70% of its clicks in bucket 6. this spike in distribution is particularly suspicious since all other publishers in the same group have 15% or less click of this size. the quality score associated with this point confirms this intuition. in fact, despite the large number of clicks (size in fig. 8(d) ) we observe a very low quality score. similarly, a small group of publishers have most of clicks in buckets 11 or 12. also in this case, the known fraud score is low, but the so is the quality score. this hints towards a previously undetected attacks, possibly orchestrated by a group of colluding publishers."
"first, we aggregate the click traffic received by any publisher within the same group, over a time period τ . to account for the long tail of ip size distributions [cit], we bin the click traffic of each publisher using a function of the actual ip size."
"the ip size histogram filter described in section v can distinguish between a set of legitimate and a set of fraudulent clicks by automatically detecting anomalous spikes in a distribution associated with low quality click traffic. to avoid detection a fraudster could attempt to spread its clicks across various buckets so as to achieve the same overall effect while avoiding generating high probability regions in few buckets. therefore, we need additional methods that look at the entire ip size distribution."
"2. pre-training stage using a multi-layered gmdh algorithm to obtain the original structure and weights of the polynomial neural network: -the entire sample set is randomly divided into training and validation sets; usually 70% of all samples go into the training set and 30% of the remaining samples go into the validation set, but one can choose another ratio -the training set size to validation set size ratio is one of the hyperparameters of this approach;"
"artificial neural networks [cit] are a system of connected and interacting artificial neuronssimplified mathematical models of biological neural cells. ann cannot be programmed in the usual sense of the word: they are trained. during the training, the neural network is able to detect the complex relationship between the input and output data and to perform a generalization. the ability of neural networks to carry out the prediction follows directly from their ability to generalize and find hidden relationships between input and output data. after training, the network is able to predict the future value of a specific sequence based on a number of previous values and/or any current factors."
"the divergence of the vector field of a dynamical system is identical to the sum of all its lyapunov exponents (13) . hence, the sum of all the lyapunov exponents is negative in case of the dissipative systems. also, one of the lyapunov exponents is zero for the bounded trajectories, which do not approach a fixed point."
"each attractor is associated with a basin of attraction, which represents all the initial states/conditions of the system that can go to that particular attractor [cit] . attractors can be points, curves, manifolds, or complicated objects, known as strange attractors. a strange attractor is an attractor having a noninteger dimension."
"in this section, we focus on the general scenario where the click traffic received by a publisher is a mixture of both legitimate and fraudulent clicks. our goal is to automatically detect and filter out the fraudulent clicks."
(ix) maximum length of the vertical lines: the maximum length of the vertical lines (v max ) in the recurrence plot can be defined as follows:
"we also define the fraud score as a function of the ratio between the number of fraudulent clicks and the total number of clicks, with different weights assigned to the fraudulent clicks depending on the reason for tagging them as fraudulent."
"overlap with other blacklists. in fig. 7 we analyze the overlap between ips filtered by the ip size histogram filter, and ips listed in gmail blacklist [cit] and in spamhaus exploit blacklist (xbl) [cit] . for each day, we compile a blacklist of ips that sent fraudulent clicks during that day. the x-axis represents the time difference between the day we compile our blacklist, and the day the gmail and spamhaus blacklists were compiled. a zero value indicates that we compare blacklists associated with the same day. negative values indicate that our blacklist is some days older than the blacklist compiled by gmail or spamhaus xbl. positive values indicate the opposite scenario. the y-axis represents the percentage of ips detected with our system that are also found in other blacklists. interestingly, we observe that a large percentage of fraudulent clicks are generated by ips that also generate other kinds of abusive traffic, such as spam emails. in particular, up to 45% of fraudulent clicks are generated by source ips listed either in gmail blacklist or in spamhaus xbl."
"however, the kolmogorov capacity-based dimension measurement does not describe whether a box contains many points or few points of the set. to describe the inhomogeneities or correlations in the set, hentschel and procaccia defined the dimension spectrum [cit] ."
"analysis of a single bucket. in fig. 5, we focus on bucket 0 of fig. 8(a), as this is the bucket with the largest number of data points. we study how the number of filtered clicks, the fraud score, and the quality score vary with the percentile threshold set by the histogram filter for this bucket. we also analyze the number of incremental fraudulent clicks, i.e., the number of fraudulent clicks detected solely by the ip size histogram filter and not by other systems, as well as the incremental quality score, i.e., the quality score associated with the incremental fraudulent clicks. as we can see from fig. 5, there is a sweet spot around 0.7 that identifies a small fraction of clicks, about 1% of the total number of clicks in this buckets, that have both high fraud score and low quality score. performance over time. fig. 6 shows how the proposed system performs over time. we run the ip size histogram detection every day for a month and we compute the fraud score and quality score of the filtered click traffic. the fraud score is consistently high and stable over time, while the quality score of the filtered traffic remains an order of magnitude lower than the quality score of the unfiltered traffic for the same group of publishers."
"for each publisher, and a given time period t, we measure its ip size distribution. this is defined as the empirical distribution of the sizes associated with advertisements on her website during time period t . different publishers naturally exhibit different ip size distributions. fig. 3 shows two examples of ip size distributions that are typically seen on (1) a website that receives average desktop traffic, and (2) a website that receives average mobile traffic. first, where a website receives mainly desktop traffic, most of the clicks have small sizes because, typically, only a handful of users share the same ip address. as such, the ip size distribution is highly skewed toward the left. second, where a website receives mainly mobile traffic, the ip size distribution exhibits two distinct modes. this is because mobile users typically access the internet either with public ip addresses, which have relatively small sizes, or through large proxies, which are shared by numerous users. generally, different publishers have different ip size distributions depending on both the type of their services, and the type of traffic driven to their websites."
"as shown in fig. 4, machine-generated traffic attacks naturally induce an anomalous ip size distribution. keeping this in mind, we implement a detection system based on the ip size histogram that automatically filters fraudulent clicks associated with any publisher. our system proceeds through the following main steps."
"identifying a proper grouping of publishers is the first fundamental step in combating machine-generated traffic. a good grouping of publishers should ensure that publishers in the same group naturally share a similar ip size distribution, while publishers in different groups might not."
"in general, different methods for comparing probability distributions provide different information as they measure different properties. for instance, if we measure the skewness of a distribution, all symmetric distributions will be considered similar to each other as they have null skewness. however, if we measure other properties, such as, the l 2 distance, two symmetric distributions will, in general, be different from each other. using an ensemble of statistical methods provides a more accurate characterization of the observed deviation than using a single methods. this is particularly important in analyzing massive data sets, comprising a wide range of different patterns."
"the complete set of lyapunov exponents can be described by considering an extremely small sphere of initial conditions having m dimensions, which is fastened to a reference phase space trajectory. if p i (t) represents the length of the ith axis, and the axes are arranged in the order of the fastest to the slowest growing axes, then 12 denotes the complete set of lyapunov exponents arranged in the order of the largest to the smallest exponent [cit] ."
"in sections v and vi, we used the ip size distribution for detecting machine-generated traffic and we evaluated the effectiveness of our detection with respect to various metrics. in this section, we discuss strengths and limitations of our approach beyond those metrics."
"generally, randomness is considered to be associated with noise (unwanted external disturbances like power line interference). however, it has been well reported in the last few decades that most of the dynamical systems are deterministic nonlinear in nature and their solutions can be statistically random as that of the outcomes of tossing an unbiased coin (i.e., head or tail) [cit] . this statistical randomness is regarded as deterministic chaos, and it allows the development of models for characterizing the systems producing the random signals."
"where f represents the vector field of a dynamical system. lyapunov exponents can be calculated from either the mathematical equations describing the dynamical systems (if known) or the observed time series [cit] . usually, two different types of methods are used for obtaining the lyapunov exponents from the observed signals. the first method is based on the concept of the time-evolution of nearby points in the phase space [cit] . however, this method enables the evaluation of the largest lyapunov exponent only. the other method is dependent on the computation of the local jacobi matrices and estimates all the lyapunov exponents [cit] . all the lyapunov exponents (in vector form) of a particular system constitute the lyapunov spectra [cit] ."
"where the rows correspond to the d-dimensional phase space vectors and the columns represent the time-delayed versions of the initial n − d − 1 τ points of the vector x. the two factors, namely, embedding dimension (d) and time delay (τ) play an important role during the reconstruction of the phase space of a dynamical system [cit] . the embedding dimension is determined using either the method of false nearest neighbours [cit] or cao's method [cit] or empirically [cit] . the false nearest neighbour method has been regarded as the most popular method for the determination of the optimal embedding dimension [cit] . this method is based on the principle that the pair of points which are located very near to each other at the optimal embedding dimension m will remain close to each other as the dimension m increases further. nevertheless, if m is small, then the points located far apart may appear to be neighbours due to projecting into a lower dimensional space. in this method, the neighbours are checked at increasing embedding dimensions until a negligible number of false neighbours are found while moving from dimension m to m + 1. this resulting dimension m is considered as the optimal embedding dimension."
"gmdh-like algorithms show good forecasting accuracy for real-life processes, mainly due to their use of an external criterion (i.e. the models are selected using data that were not used for their training)."
the authors declare that they have no conflicts of interest. atrial fibrillation svm optimized with particle swarm optimization 92.9% accuracy [cit] atrial fibrillation unthresholded recurrence plots 72% accuracy [cit]
"(v) entropy: entropy (entr) is the shannon entropy of the probability p l of finding a diagonal line of length l in the recurrence plot (29) . it indicates the complexity of the recurrence plot in respect of the diagonal lines. for example, the uncorrelated noise possesses a small value of entropy, which suggests its low complexity."
"the paper has proposed a new time series forecasting method based on gmdh and deep learning approaches. the proposed algorithm was used to predict australia's major domestic airlines yearly revenue passenger kilometers, together with other forecasting methods, namely: gmdh, anfis, dbn and box-jenkins. as seen from the tables comparing the results, the average mae error of the proposed method is approximately 10% smaller than the average mae of next best method -gmdh, and the best reached mae is approximately 25% smaller than the corresponding best mae of the gmdh. this indicates that the practical application of the method can give good results compared to other well-known methods. the originality of this paper is the combination of the gmdh to perform a network's pre-training and gradient descent together with dropout to perform fine-tuning."
"in recent years, very good results for a wide range of applications have been shown by neural network-based methods such as deep neural networks, including the following applications: image recognition [cit], speech recognition [cit], machine translation [cit], reinforcement learning [cit] and many others. in essence, many of these applications require some kind of function approximation to be performed in order to build a corresponding model, which should indicate the general suitability of deep learning-based methods as a function approximation mechanism. we propose a deep learningbased hybrid approach to forecast the yearly revenue passenger kilometers (rpk) time series of australia's major domestic airlines, which are publicly available (australian domestic airline activity-time series, n.d.)."
to evaluate the quality of the piezo-resistive effect and to determine whether the proposed sensor setup is applicable in robotic hands the manufactured sensor patches are analysed on the testbed. for these tests the patches are covered with an electrically insulating foil to ensure the sole measurement of the effects within the patch.
"to overcome this problem, fred was proposed that improved uniformity by constraining all flows to occupy loosely equal shares of the queue's capacity. aqms that used only congestion metric and not flow information faced the problem of unfairness in handling the different types of traffic. fred is based on instantaneous queue occupancy of a given flow. it provides better protection than red for adaptive flows and isolating non-adaptive greedy flows. combination of flow and congestion metric based aqms like choke, sfb [cit], sfed [cit], faba [cit], stored [cit] were proposed to allocate fair buffer between flows considering the effects of misbehaving or non-responsive flows."
"we produced various test patches with different filler ratios on the injection molding machine. the patches are equipped with cast-in steel wires, see fig. 2 . we found that an estimated filler ratio of about 30 w/w showed the best piezo-resistive effect. manual indentation of the patches resulted in measurable changes in resistance ranging from 3 mω to 100 kω."
"our software on the arduino uses a 'software serial' library to control and communicate with the carbon monoxide sensor, dust sensor, gps chip and the cellular modem. pollutant measurements are read from the analog to digital converter output. the mq-7 sensor has a 30 second sensor heating cycle and a 60 second sampling cycle. gps, carbon monoxide and dust readings are sampled periodically and transmitted to our cloud server. in order to prevent data loss on loss of data connectivity, a store & forward mechanism is included in our implementation."
"aqms that used only the congestion metric faced the problem of unfairness in handling the different types of traffic. to overcome this problem, fred [cit] was proposed that improved uniformity by constraining all flows to occupy loosely equal shares of the queue's capacity. to overcome the shortcomings of the fred, choke [cit] was designed. this paper considers the advantages of the queue-based, load-based and flow-based algorithms to combine into an improvised aqm algorithm. this improvised aqm algorithm called flow based avqchoke (favqchoke) tries to achieve its objective of improving overall performance"
"the node device along with the oxa and clima sensor modules costs about $400. in addition, user's existing iphone device and data plan will be used to transmit data periodically to server. the personal sensing device generates 1536 bytes/minute of data when sampling every 5 seconds. with an average driving time of 2 hours per day for 30 days the msb produces about 5.27 mb of data per month."
"were y j is the output of neuron, x i are the inputs to the neuron, f j is the activation function, w ij are the weights between the inputs and the neuron and θ j is the bias for the output neuron. the applied neural network contains 4 output neurons, each for one indentor geometry. the above fig. 11 . 2-layer feedforward neural network described indentors are used to apply force to the sensor patch with the goal of correct classification of the indentor. on the testbed defined forces ranging from 0 n to 5 n have been cyclicly applied to the patches, and the output of the sensor patch as well as the indentation position and force have been recorded. subsequently, the neural network was trained using two datasets for each indentor containing 208 datapoints for each taxel. in the following, a sequence of two unknown sets for the same indentor are presented to the neural network. this procedure is repeated for the different indentor types (spherical, cylindrical, paraboloid and a variable 1-4 point indentor). the rate of change of the output voltage for the given indentation produced the features, which are used as training input for the neural network, see fig. 12 ."
"congestion control involves the design of algorithms to statistically limit the demand-capacity mismatch, or dynamically control traffic sources when such a mismatch occurs. currently, usage of the internet is dominated by transmission control protocol (tcp) traffic such as remote terminal, ftp, web traffic, and electronic mail. although these applications can tolerate either packet delay or packet losses rather gracefully, congestion remains a major problem that leads to poor performance. internet is still evolving as a high-performance network providing ubiquitous services, including real time voice/video. accordingly, many congestion control approaches have been proposed. however, current internet congestion control methods results in unsatisfactory performance including multiple packet losses and low link utilization as the number of users and the size of the network increases. active queue management (aqm), network algorithms executed by network components such as routers, detect network congestion, packet losses, or incipient congestion, and inform traffic sources either implicitly or explicitly."
"furthermore, it will be interesting to study how general drivers behave when they are presented with fine-grained pollution data collected by our two models. for example, given the information on highly polluted routes, how many drivers will choose to take cleaner route at the cost of longer driving times?"
"favqchoke considers input rate that provides early feedback of congestion resulting in good link utilisation. in case of rate-based aqm, the queue length is less sensitive to the number of connections. so queue length stability is provided by this algorithm. routers with this aqm will provide better protection than other aqms for adaptive (fragile and robust) flows."
the first aqm algorithm red detects congestion by observing the queue state. in red [cit] packet drop probability is linearly proportional to queue length. the aqm algorithm red drops packets before a queue becomes full. this reduces the number of packets dropped. red and its variant uses queue length as a congestion indicator that results in certain drawbacks. in order to overcome the difficulty of relying only on queue length to identify the level of congestion various other aqms are introduced with different congestion indicators.
"to overcome the problems with red, rem [cit] was proposed. this aqm scheme attempts to make user input rates equal to the network capacity. in case of high congestion, sources are indicated to reduce their rates. in contrast to red, rem decouples congestion measure from performance measure which stabilizes the queue around its target independent of traffic load leading to high utilisation and low delay. aqm schemes like green [cit], avq [cit] also depend on arrival rate to control the congestion in the router. avq uses only the traffic input rate for the measure of congestion. this provides early feedback of congestion."
"two approaches to handling congestion are congestion control and congestion avoidance. the former is reactive in which congestion control typically comes into play after the network is overloaded, that is, congestion is detected. the latter is proactive in that congestion avoidance comes into play before the network becomes overloaded, that is, when congestion is expected. in general and throughout this article, the term congestion control is used to denote both approaches."
"the cost of assembling one unit came to about $700. in addition, we signed up for a $25 per month prepaid data plan with 1.5 gb data cap per month. the msb generates 1600 bytes/minute of data when sampling every 5 seconds. with an average driving time of 2 hours per day for 30 days, the msb produces about 5.5 mb of data per month, which is a small fraction of a typical lowend data plan."
"an iphone application uses the node ios framework to scan, connect and communicate with the node oxa and clima sensor modules. we developed a custom ios application using the node open api to read sensor data, tag the data with location information and send the data to the central server. the application allows the user to pair the smartphone with a sensor device of their choice over bluetooth."
"in msb design, sensors are mounted in a box and a small fan draws air into the box for sampling. we plan to evaluate the measurements using the multiple orientations described above to understand how orientation influences the measurements. in addition, we have conducted tests with the personal sensing device placed inside and outside the car. our preliminary studies indicate that pollution readings inside vehicles correlated with those taken outside vehicle on freeways and suburban roads."
there is a multitude of robotic hands being developed by many groups. we present a small subset which is equipped with tactile sensors in table ii .
"our touch sensor is developed with the aim to be eventually mounted on the dlr hand ii. this hand is based on four identical fingers. each finger is equipped with four joints, with the last two joints mechanically coupled. the intrinsic sensors of dlr hand ii provide information about motor-positions, joint-angles and joint-torques as well as temperature. in addition, the fingertips are equipped with custom-made six-dof force-torque sensors based on strain gauges [cit] . with this sensor setup a glass of tea can be prepared and served [cit] ."
"recent advances in robotic hand development allow for increasingly sophisticated tasks to be conducted by robotic hands. integration of tactile sensors into robotic hands results in additional requirements: to extend the dexterity to rolling objects between the fingertips, mostly cylindrical, spherical, or 3d-free-form surfaces are applied at the fingertips. tactile sensors dedicated to the application on those fingertips need to be deformable or conformable to the shape of the fingertips to support the desired dexterity."
"the cloud service must be capable of collecting data from different types of sensors, which may sample different pollutants. in section 2.2.1, we design the data collection so that the cloud can handle different data formats from various participating sensors. as this cloud service also provides community feeds to subscribing consumers, it must be capable of providing spatially or temporally aggregated feeds based on consumer-defined granularity. besides, the service must be extensible and must be capable of supporting additional data dissemination patterns. we will discuss the data dissemination design in section 2.2.2."
"we chose the arduino mega128 microcontroller for our prototype implementation [cit] . the arduino platform, with its large developer community and its reusable open-source libraries, provides a versatile microcontroller platform for rapid prototyping. we used the sim5218 3g/gprs cellular shield with at command support for data transmission over http [cit] . the large number of i/o pins on the microcontroller facilitates the inclusion of many add-on sensors and peripherals to collect a wide variety of data. for the prototype implementation, we chose to measure position, velocity, carbon monoxide concentration and particulate matter concentration. the pmb 648 gps receiver allows reading of position and velocity with high accuracy and can easily interface with the arduino platform [cit] . a sharp dust sensor is used for dust concentration measurements. carbon monoxide concentrations are measured using the mq-7 carbon monoxide sensor from hanwei electronics [cit] ."
"if not, what factors should the clientsensing app consider to decide the best sampling time for accuracy? geography of the road, speed, and changes in traffic are the obvious candidates."
"the share feature proposed in section 2.2 is implemented using google drive api. this api provides the ability to add, modify, or delete permissions for a file that resides in google drive. to perform these actions, the user simply needs to authorize requests using oauth 2.0 and provide the email address of subscribers. in our case, server can create tables for users when necessary and then transfer ownership to the users. in this way, data producers retain full control of their raw data, by taking the role of the owner."
"the dlr crossed-wire approach is based on piezoresistivity as transduction effect. with a focus on easy and low cost manfuacture, mechanically simple designs for the test patches were preferred. therefore, we focused on \"classical\" array setups. the first approach is based on injectionmolded patches with cast in wires."
"rem used both input rate and queue length that illustrated very high utilization but very low throughput compared to queue based red. this scheme stabilizes both the input rate around link capacity and the queue around a small target independent of the number of users sharing the link. blue used packet loss and link utilization as congestion indicator to give a very high throughput and, high utilisation with low queue length stability."
"as opposed to a coarse-grained sensing system, a fine-grained approach would provide more frequent and spatially dense pollutant measurements. a scalable sensing platform could effectively disseminate pollution information to users in need. today, the scarcity of fine-grained air quality information is hindering public awareness of health issues arising from pollution. studies suggest that the health effects among asthmatics from short-term changes in air pollution levels are an important public health problem [cit] . we anticipate that, with the help of finegrained air quality measurements, people could be advised to take actions based on real time pollution levels to accommodate individual health needs."
"the patches are fixed on the testbed and loaded with a trapezoidal indentation path. the linear motor is operated in position control mode and commanded to indent the patch at a certain speed, then stop and maintain a pre-set indentation depth for a defined period of time and then to release the patch, again with a defined velocity."
"several prototypes have been manufactured and tested. as predicted in preliminary tests, the sensor patches exhibit good piezo-resistive behavior during mechanical loading. the investigation of the prototype patches on a specialised testbed to quantify the characteristics of the correlation between indentation and resulting electrical resistance revealed a non-monotonic, velocity-dependent behavior. simulation and sem examination revealed that this behavior most likely results from the low adhesion between the matrix material and the cast-in wires. therefore, future focus for the development of our tactile sensors we lay on the improvement of the adhesion between the basis material and the contacting wires, for example by using non-metal wires."
"in our proposed mobile pollution-sensing schema (shown in figure 1 ), a variety of mobile sensing models can be used to collect data from different scenarios. the sensing models measure the concentration of pollutants, tag the pollution data with relevant information, such as time, speed and gps location 1, and send the data over a cellular data link to the cloud server. raw pollution data is then processed and aggregated by the server to make it available as a pollution map. various devices should be supported to access the map through browsers and mobile apps. users would be able to view illustration of real time pollution data overlaid on map. this would enable users to get fine-grained street level air quality report."
"among the popular cloud storage services, google fusion tables (gft) has proven to be the best fit. it is designed as a new file type within google drive, with all the capabilities associated with a compact database. it supports a special data type for location storage, and supports various visualization tools for large data sets. this provides a convenient data storage in the cloud with google's cloud visualization support."
"the paper is structured as follows: in section ii, we review different approaches to tactile sensing and current designs of dlr -german aerospace center, institute of robotics and mechatronics, germany. contact: michael.strohmayr@dlr.de. this work is supported by the fp6 eu project sensopac (028056) hands equipped with tactile sensors. section iii then introduces the dlr crossed-wire approach, the materials used, and the manufacturing of the prototype sensors. in section iv the sensor patches are evaluated with respect to their behaviour and capabilities for object shape classification."
"as no commercially available granulates with the desired piezo-resistive profile existed, we ran several experiments to determine materials suitable for blending in order to achieve the required properties: easy miscibility with conductive fillers, tuneable mechanical properties and processability in injection molding machines."
"the proposed algorithm is motivated by the need for a stable queue size and fair bandwidth allocation irrespective of the varying traffic and congestion characteristics of the n flows. as discussed in introduction, some of the algorithms arrive at a stable queue size and some of them bring in fairness when the shared link has n flows. the unstable queue size results in high queue oscillation due to the parameter tuning problem in queue based aqms. we are motivated to identify a scheme that penalizes the unresponsive flows with the stable queue size."
"for every mixing ratio a patch is tested applying: for the evaluation of the piezo-resistive effect, time, commanded indentation depth, actual indentation depth, force in z-axis and measured voltage is recorded and stored. the prototype patches clearly show a piezo-resistive effect. but during the test on the testbed the patches also showed an unexpected behavior. if the load is increased faster than 500"
"sampling strategy. the frequency at which measurements are done should relate to the vehicle speed and to the spatial gradient of pollution. for instance, at highway speeds, a frequency of one sample per minute means the measurements are one mile apart (assuming a vehicle speed of 60 mph). is this measurement frequency really necessary given the rate at which pollution level changes relative to distance?"
"today, an increasing number of robotic devices is developed to support humans in a variety of everyday tasks [cit] . in contrast to the well-defined conditions in industrial production lines, the unstructured nature of human environments is particularly challenging for robotic devices."
"to provide efficient visualization, we adopt the visualization support from cloud storage services. we store common requests and their associated results as tables in server's domain. when we detect a match between a request and a table, we can return it"
"it has been shown that humans make extensive use of their sense of touch when grasping and manipulating objects, and that any impairments can severely affect their dexterity. robots face similar problems, as pre-defined object models or visual sensor data might not provide enough information for setting grip forces correctly or fine-grained handling of objects. especially the manipulation of small objects designed for human hands is a challenging problem: safe gripping and manipulation of a shirt button or a match still is an almost unsurmountable challenge for robotic manipulators. sophisticated tactile sensors are needed in order to tackle such problems."
"table in figure 4 .c -the region to road segment mapping tableconsists of the road segments pertaining to a given region. so, the aggregation performed for the rutgers campus is also applicable to the road segments that belong to the campus region."
"in this section different piezo-resistive materials and their applicability in tactile sensor setups are investigated. we first described the mechanical testbed that we used in section iv-a. then, we present the results of simple indentation tests examining the piezo-resistive behaviour of the sensor patches in section iv-b. finally, we present object shape classification results in section iv-c and multi-point discrimination in section iv-d."
"we focused on ultra-soft poly[styrene-b-(ethylene-cobutylene)-b-styrene] (sebs) polymers in order to make the resulting patches compliant and flexible. blending these materials with pre-filled polymers of high conductivity did not prove successful. we also discarded pure graphite as a filler material. however, using carbon black as a filler material, we were able to produce granulate with the desired piezoresistive properties. for these mixtures we used kraiburg thermolast tf0stl (0 shore a) [cit] and cabot vulcan xc72 (carbon black) [cit] ."
"there is an increasing number of promising approaches towards the integration of tactile sensors into dexterous manipulation scenarios. however, up to now there is no commercially available tactile sensor system available that is mountable on 3-d free-form surfaces like robotic fingertips and, at the same time, provides high mechanical compliance to support the dexterous capabilities of the robotic hand."
another aqm scheme blue [cit] does not use queue length as a congestion metrics. blue uses packet loss and link utilization as a congestion indicator. in lred [cit] packet loss ratio is used to design more adaptive and robust aqm. it uses the instantaneous queue length and packet loss ratio to calculate the packet drop probability.
"our cloud server is deployed on amazon ec2 with classic lamp settings. it solves most of the problems mentioned in section 2.2, yet does not support some advanced features such as smart aggregation and automatic device registration. first, we describe some essential technical features, and then, we describe a typical workflow in order to show the data processing procedure."
"if pressure is applied to the patch, the matrix material shuns from the load. due to the difference in elasticity a relative motion between wires and sensor matrix is induced. fast indentation of the patches results in increasing resi- stivity. this effect might be caused by an altering transfer resistance between matrix and cast-in metal wires. when the matrix material is deformed quickly, it partly lifts off from the metal wires and the transfer resistance temporarily increases. possibly the slow indentation allows the material to maintain sliding contact to the wires, which has only minor influence to the transition resistivity. to show the causative effects the behavior of the sebs patches with cast-in steel wires is simulated using ansys [cit] . due to the applied force the matrix material bulges to the sides. in addition, the highly compliant matrix material shuns from the load and lifts off the wire in the x-axis, see fig. 7 ."
"the availability of real-time air quality data could make drivers better educated about driving patterns and how it impacts the environment and increases pollution. better driving habits will lead to reduced pollution. also, more health conscious citizens may choose alternate \"healthy\" routes based on the pollution information. it will benefit them as well as others by reducing pollution concentration in peak roadways so everybody breathes cleaner air."
we designed a protocol to handle such variations. the idea is to group different fields according to their role with respect to server's post processing step. sensors are enabled to design their own data formats without adversely affecting the server processing. any new data format is required to be registered with server a priori.
the main components of the personal sensing device (called psd for short) include a mobile air quality sensor and a smart phone to act as an interface with the central repository hosted on a cloud server. our system uses a node wireless sensor platform available for smart devices from variable technologies [cit] . the device is shown in figure 3 .
"in this section, we will use the packet-simulator ns-2 to simulate the favqqchoke algorithm. in this simulation the network topology in fig. 4 is with a single link of capacity 1mbps that drops packet according to the aqm algorithm. the congestion link is in between the two routers r1 and r2. the link is shared by n tcp flows and n udp flows. end hosts are connected to the routers using a 10mbps link. all links have a small propagation delay of 1ms so that the delay introduced is by the buffer delay rather than the transmission delay. the tcp flows are derived from ftp sessions which transmit large size files. the udp hosts send packets at a constant bit rate of 2 mbps. in the simulation setup we consider 32 tcp flows and 1 udp flow in the network. the minimum threshold min th in the favqqchoke scheme is set to 100 and the maximum threshold max th to be twice the min th and the physical queue size is fixed at 300 packets."
"in this section, we present the data process pipeline through which we convert raw data from our two types of sensors into aggregated data stored in gft repository. we first give two sample measurements and then explain all the processing that takes place. figure 5 gives the two sample data points, which are of different sizes and contain different fields. data point 'a' is from the msb and data point 'b' is sent by the node sensor. the first step is the removal of the device identifiers, which are id, device_id and device_name in these two cases. then, initial geographical aggregation is applied, causing the translation of the location field to road segments and then to the merging of the measurements as two data points for a road segment; shown in figure 5 as 'c'. finally, temporal aggregation is applied. since these two records are assigned the same road segment and they are in the same time frame, they will be joined as shown in figure 5 as 'd'. during this step other post-processing can take place such as the computation of the mean of the measurements or the air quality index value. for example, the mean is calculated in the example shown in figure 5 ."
"a square aluminum plate with a surface of 400 mm 2 is used as indentation probe, see fig. 4 . thus, the probe covers the entire patch avoiding possible influences of a varying indentation location. for the readout of the resistance two crossing, cast-in wires are connected to a pre-amplification board where the resulting current is measured over a reference resistor."
server aggregation. the server needs to implement a smart data aggregation strategy on data provided by multiple cars so that the air quality reports correlate with the ground truth pollution.
red and other aqms are unable to penalize unresponsive flows. as the packets dropped from each flow over a period of time is almost the same. consequently the misbehaving traffic like udp can take up a large % of the link bandwidth and starve out tcp friendly flows as in fig 5. favqchoke identifies and penalizes misbehaving flows effectively compared to the existing aqms as in table 1 with the help of input rate and the flow information.
"immediately. this new design outperforms traditional massive data visualization tools in the sense that data filtering, sorting, aggregation and visualization are either pre-computed, or processed in the cloud."
"the conducted experiments proved the ability of the neural network to classify the 8 sets correctly, if the sensor is loaded with the specified indentation sequence. the first output the shape (i.e. the geometry) of the indentor was correctly classified using simple neural networks."
"another goal in the development of the testbed was maximum flexibility and accuracy at reasonable cost. the testbed consists of a frame to support the test patch and the dlr linear drive cylinder [cit] . the drive cylinder is equipped with position and force sensors and is able to provide up to 300 n at a spacial resolution of 2μm. to enable horizontal movement of the tested specimen a x/y-platform with a spacial resolution of 1μm is applied. to measure the forces exerted on the patch, an optical force torque sensor from spacecontrol [cit] with a force-resolution of 20 mn is applied."
"due to the huge gaps in ground-based networks of air pollution monitors, there is a necessity to obtain fine-grained air quality data. various attempts have been made to employ mobile sensors in order to achieve this goal. the school bus monitoring study [cit] conducted at university of california along with nrdc (national resources defense council) highlights the health hazards posed to school children by their exposure to diesel pollutants. it also emphasizes the urgent need for mobile monitoring of air quality because diesel exhaust is a known carcinogen and a cause of respiratory illnesses. an interesting study was conducted by epa [cit] to measure air pollutant concentrations inside and outside of a truck cabs. the study however used measurement techniques that involved collecting air samples in the truck and later analyzing them in a lab to derive actual air quality values. wireless sensor networks for monitoring personal pollutant exposure [cit], indoor air quality [cit] and hazardous sites [cit] have also been proposed."
"the node sensor platform is customizable with add-on sensor modules. each device can accommodate two sensors on either end of the device. we selected oxa and clima modules to measure carbon monoxide, humidity, temperature, ambient light and barometric pressure. only carbon monoxide sensor was available when we started using this device for our experiments. the mobile sensor for our social model is intended for use anytime, anywhere irrespective of the mode of transport."
"at the same time, the emergence of cheap commodity air pollution sensors and the increase of cellular bandwidth have made mobile sensing platforms capable of real-time air quality data collection increasingly feasible. several manufacturers such as aeroqual or variable technologies have recently introduced handheld pollution measurement devices. these devices are small enough to be carried by walking people for personal use and measure all the criteria pollutants contributed by vehicle emissions [cit] . but none of these off-the-shelf devices has been evaluated with respect to their real-time sensing performance when installed on mobile platforms such as vehicles. to the best of our knowledge, we have not come across any work that study the long-term stability, reliability and impact of realtime pollution monitoring systems using commodity sensors and the problems associated in deploying such systems."
"we start out with the mobile sensing schema, in which we talk about multiple mobile sensing models as well as their data processing in cloud. then implementation is described in section 3. section 4 discusses the preliminary results and some challenges we identified based on our work. section 5 gives related work. finally, we close the paper with our conclusion in section 6."
"we deployed the personal sensing device (node) inside mobile sensing box (msb) so that both the devices can measure the pollution simultaneously and under the same conditions. mounting msb outside the vehicle would be the typical deployment scenario on public transportation infrastructure. figure 6 shows the msb mounted outside the car. the inset picture shows the layout inside the msb. figure 7 shows the correlation between the two platforms in a typical experiment. each vertical represents one sample. data in red is from the node and data in blue is from the msb. in msb, there is a 30 second heating cycle between two sampling cycles, so we can observe gaps in the data from the msb."
"as shown in fig. 1, the wires are arranged in two different layers. those layers are separated by the piezo-resistive matrix, which forms sensitive areas at each crossing point of two wires. one of the advantages of such a design is that the number of connecting cables does not increase linearly with the number of taxels. due to space limitations, processing of the tactile information cannot happen directly on the hand and keeping the number of connections manageable while still providing a high spatial resolution is a key requirement."
"we provide a web portal where users can view real time pollution data. this is implemented as a new map layer, which we call air quality index layer (aqi). there are two types of aqi layers available for different use cases -marker map and heat map. marker map consists of data markers -when the user clicks on them will display all the information associated with each data point like -the time the data point was generated, gps location, all the pollutant concentrations sent by the sensor, etc. heat map shows all available measurements in a heat map style gradient color display where higher pollution is represented by higher ranked color in the color spectrum."
"besides normal aggregation tables, this repository also manages all the temporary aggregation tables. for example, \"busch\" can be the name of the region containing all the route segments within the busch campus of rutgers university. all queries with locations belonging to this area will be mapped to this region. as a result, a table storing the campus pollution data is created for smoother data retrieval. figure 4 shows a typical gft item in the repository. aggregation table in figure 4 .a keeps track of the aggregations performed, owner who initiated the aggregation, the period for which data aggregation was performed, whether spatial and temporal aggregations are performed. in the table above, start and end indicate the time span of data used for aggregation. a value of 1 int spatialagg indicates that a spatial aggregation has been performed and a value of 1 in timeagg indicates that a oneminute temporal aggregation has been performed. the region table in figure 4 .b indicates the region for which the aggregation was performed. in the example above, rutgers campus was the region chosen."
"using the same format as shown in figure 7, we show data from this experiment in figure 12 . we can see that the variations of pollution levels from the two platforms are similar. we performed regression analysis as described in section 4.1 (shown in figure 13 ), which supports the positive linear relationship between the two platforms. however, spearman correlation coefficient is 0.5931, which is lower than the result from section 4.1. more work is required to identify the reasons behind this."
"in the next section, a broad study of all possible aqm schemes is presented to identify the basic schemes that exist and their classification based on congestion metric and flow information. in section 3 the favqchoke algorithm is discussed to bring out the advantages of the proposed algorithm."
during the recent years research activities have come out with various congestion avoidance mechanisms in internet to improve internet traffic. but each of these mechanisms is ineffective especially in heavy traffic network. that has made research a continuous process in identifying the best active queue management algorithm. congestion in routers results in high packet loss leading to high cost that is reduced by the various existing aqm schemes.
"therefore, there is an increasing need for such sensor systems, and a variety of different approaches has been proposed over the last years. one of the problems in the widespread adoption of tactile sensors is difficult integration with existing robotic hardware, mainly robotic hands. often, these manipulators offer only limited or no space for additional sensor processing equipment or bulky rigid parts. ideally, touch sensors should consist of a skin-like soft and flexible material providing a coating to the robotic fingers devoid of rigid or bulky parts."
"in region a the force increases exponentially due to the indentation, while the measured voltage shows an unexpected behavior, it decreases (i.e., the resistance increases). within region b the patch exhibits the expected piezo-resistive behavior, as the voltage increases exponentially while the force increases linearly. during the pre-set hold time (region c) the curve of the force shows a typical course for highly compliant materials. while the given position is maintained the material shuns from the load and the force slowly decreases. due to the applied force an increasing amount of conductive pathways is formed within the piezo-resistive matrix material. therefore, the voltage slowly increases towards a saturation. in region d the voltage decreases proportional to the force. while the force decreases further in region e, the voltage converges to a constant value slightly below its initial value. within region f a negative force is observable, as the linear drive cylinder pulls the patch which adheres to the upper support of the specimen, the measured voltage decreases only slightly. at the end of the indentation test, in region g, the force reaches its initial value and the voltage maintains a value slightly below its initial value. additional experiments revealed a velocity-dependent behavior of the test-patch. the above described trapezoid indentation path was applied to the patch with indentation and release velocities of, 0.1 figure 6 shows the velocity-dependent response of the test-patch."
"in this section, we describe preliminary experiments we performed with the two platforms on highways in new jersey and new york. in the first experiment, both sensing devices are placed outside the car. in the second experiment, node is mounted inside the car while msb is still placed outside."
"to be able to apply the dlr touch sensor as an exteroceptive sensor for the dlr-hand, not only the piezo-resistive characteristics but also the discrimination of different indentation probes at different indentation depth and forces is relevant, highlighting fine-grained tactile capabilities."
"to test the ability of the sensor patch to discriminate between multiple contact indentations of [mm] -scale objects at [mm]-scale deviation the classification of six different indentation scenarios was investigated. to enlarge the information content of the input space, different training sets were generated with two consecutive datasets and a combination of the first and the last dataset. a neural network was trained using the training sets and subsequently tested applying an independent test dataset unknown to the network. the independent test dataset included data for all the indentation positions mentioned above."
the sampling strategy also needs to define the minimal subset of criteria pollutants that must be measured on a mobile platform to reflect the air quality. this is essential to arrive at a cost-effective mobile platform.
"incentives and applications. given these inevitable sources of inaccuracy, we must rely on developing a large pollution sensing community and more advanced pollution sensor technology. how to incentivize users to produce more measurements is yet another challenge. we plan to provide value added services derived from our system to volunteers as a complimentary reward for their participation."
for the injection molding a battenfeld microsystem 50 injection molding machine was used [cit] . this machine is designed for the processing of small volumes and molds in [mm] scale.
for the application of a piezo-resistive material as pressure sensor the desired material properties can be achieved by the optimal selection of the basis material and the adjustment of the tunable properties with a focus to the desired mechanical and electrical properties.
"in order to test this, different indentation probes (cylindrical, spherical, parabolic and a spezialised 1-4 point probe) are applied to the touch sensors, see fig. 9 . the prototype sensor patches are attached to the testbed using double-sided duct tape. the test patch is then loaded according to a defined indentation path. sequentially different areas of the sensor patch are indented with the probe. therefore the support of the specimen can be manually deviated in μm steps. thus the entire sensitive area of the test patch is evaluated with one indentation probe. subsequently the whole test is repeated applying different indentation probes. figure 10 shows the different indentors and the response of the sensor patch. the first set of tests revealed promising characteristics of the sebs thermoplastic patches. with the realised spatial resolution of less than one mm the different indentation probes can easily be discriminated by eye. to test the classification performance of the sensor patches a simple 2-layer feed forward neural network (fig. 11) is applied for the interpretation of the values from the sensor readout system. a single neuron which makes up the network can be described by the following equation 1:"
"the current version of dlr hand ii is not equipped with exteroceptive sensors, which limits the dexterity of the hand, for example when manipulating small objects. therefore we focus our development on flexible sensors that could provide a suitable tactile coating for the hand's outer surface (fingers, especially the tips, and palm)."
"in this paper, we present the development and a first prototype of the dlr touch sensor i. it is based on a novel crossed-wire design, compliant, and robust."
"μm s the measured voltage shows an unexpected dip. contrary to the expected behavior an initial increase of the resistance is observable, see fig. 5 ."
"we assembled a mobile sensing box (called msb for short) as shown in figure 2 . it consists of a microcontroller, dust & carbon monoxide sensors, gps and a cellular modem. the assembled unit can be mounted on any vehicle and can be powered by the vehicle's battery."
"internet congestion occurs when the aggregate demand for a resource (e.g., link bandwidth) exceeds the available capacity of the resource. thus resulting effects of such congestion include long delays in data delivery, wasted resources due to lost or dropped packets, and even possible congestion collapse, in which all communication in the entire network comes to an end. it is therefore obvious that in order to maintain good network performance, certain mechanisms must be provided to prevent the network from being congested for any noteworthy period of time."
"(1) variation in measured content -sensors have different pollution measurement capabilities. as a result, they produce different pollutant measurements with different levels of accuracy and variation. (2) variations in data representation -location, for instance, can be represented in various formats."
"to remove the implementation cost of fred, choke (choose and keep for responsive flows, and choose and kill for unresponsive flows) algorithm was designed that penalizes misbehaving flows by dropping more of their packets. so choke tries to bring fairness for the flows that pass through a congested router. choke provides much better fairness than fred but penalizes high bandwidth flows and does not handle unresponsive flows in case of few packets. flow based aqms with congestion metric are able to discriminate responsive and nonresponsive flows. the malicious flows are identified which might cause congestion at the router."
"as shown in figure 11, the test started from route 440, exit 4 and went through staten island express way (i-278), some local roads and finally returned to the starting point. due to the influence of road constructions, the carbon monoxide concentrations on staten island express way are higher than other places in this experiment."
"in this paper, we present a vehicular-based approach of measuring fine-grained air quality in real-time. we propose two cost effective data farming models -one that can be deployed on public transportation and the second a personal sensing model. we present preliminary prototypes and discuss implementation challenges and experiments. in particular, we found that a personal sensing device conveniently mounted inside a vehicle in front of the vent can measure carbon monoxide levels that correlate well with outdoor values."
"complemetary to the simulations, the sensor patches were examined using scanning electron microscopy (sem), see fig. 8 . the examination with the sem reveals the most fig. 8 . sem of the wire cast into sebs likely reason for the observed phenomena: the low adhesion between the sebs matrix and the cast-in steel wires."
"we also found that pollution on the road is highly sensitive to weather. sensors are also susceptible to produce varying readings during abnormal weather conditions. currently, we segregate pollution measurements under different weather conditions and plan to further investigate the correlation between weather and the ground-truth pollution measurements."
"the current pollution measurement methodology uses expensive equipment at fixed locations or dedicated mobile equipment. the raw data obtained in this manner is used to further extrapolate the extent and concentration of pollution through dispersion models. this is a coarse-grained system where the pollution measurements are few and far in-between. widespread deployment of this measurement paradigm is constrained by its prohibitive cost. in addition, it is desirable to have access to real-time measurements to be able to quickly analyze and identify alarming levels of pollutants. currently, access to such data is limited [cit] if not absent. it is available to and discernable by only a few who are well informed on the subject of pollution."
temporary pollution surge is an additional phenomenon we observe in our measurements. a passing truck that produces high pollution around itself usually causes this; but this effect is temporary and does not reflect the ground truth. algorithms to find and eliminate these outliers need to be designed.
"the conducted classification tests have shown that the realised spatial resolution of the taxels of 0.8 mm enables the reliable classification of very small objects in multiple contact scenarios. experiments were performed for the center of the sensor patch and its four corners. each corner position had a deviation of about 1.5 mm from the center position. it was shown, that the patch resolution enables classifying indentations at different positions correctly with an accuracy of about 98% using a basic 2-layer artificial neural network. the applied neural network was also successful in classifying single and multiple point indentations. even multiple-contact-point indentations adjacent to each other and deviated by only 1.5 mm can efficiently be discriminated by the sensor prototype. comparing the results to the human fingertip performance [cit] we were able to achieve similar spatial resolution for our sensor prototype."
"the server must provide a unified interface for sensors to communicate with. different sensors produce different data formats, and this variation falls into two categories."
"the first challenge was the production of a thermoplastic elastomer exhibiting piezo-resistivity, so we could use this material in injection molding. in piezo-resistive materials the resistivity depends on the concentration of conductive filler particles, geometric form, and distribution. a piezo-resistive behavior can be observed if a non-conducting material is endowed with conductive particles."
"the first sensing model is designed for deployment on public transportation infrastructure such as buses, which have fixed and reliable routes along high volume corridors. for this model, we propose a custom-made mobile sensing box (msb) that includes a microcontroller board with add-on sensors, a peripheral gps receiver and a cellular modem. connecting to the bus battery would provide the power supply needed to operate this model. since sensor bulk is not a primary design constraint in this case, it allows us to pack enough sensors per unit to measure all criteria pollutants. in our current prototype (see section 3.1), we used two pollution sensors to measure carbon monoxide and particulate matter concentrations. however, in this paper we only focus on carbon monoxide. the second sensing model relies on air quality-aware drivers who install a personal sensing device (psd) in their cars, connected over bluetooth to their smart phone. drivers can use this setting to measure the air quality for themselves, or they can register to participate in a social community-based sensing. the pollution data is geo tagged and posted to the central server over cellular network. in this paper, we focus on carbon monoxide sensor but the interchangeable sensors on wireless device can measure various other pollutants. 1 bluetooth enabled obd-ii scan tool can provide additional vehicle state information."
"sensor location. we observed that the orientation of the sensor relative to the vehicle movement influences the measurements. the inline deployment allows air to flow into the sensor body. intuitively, this should provide true ground level measurements as opposed to the transversal deployment of the sensor. however, in cases where sensors use a sensor preheat cycle before sampling, this will affect the measurements."
"the air quality data obtained using such sensing models could serve various applications. patients with respiratory or cardiovascular diseases would find our results valuable to determine less polluted routes. health aware individuals could also take advantage of this form of cleaner route navigation. individuals using our system will become more knowledgeable about the extent of pollution and be motivated to follow better driving patterns, such as not allowing their vehicle to idle for long periods or to drive more environmentally friendly cars. apart from these applications at an individual level, this data could be used as an additional input to large-scale policy making. for example, public health officials and policymakers could use our results to predict potential health impacts based on air quality across various regions to make decisions such as possible locations for a future school or residential community."
"in order to bridge the gap between the sampling phase and the analysis phase, researchers introduced monitoring approaches using commodity sensors, which can provide real time pollution data. n-smarts [cit] and commonsense research conducted jointly by uc berkeley and intel focused on collecting air quality data by attaching sensors to gps enabled cell phones. it also highlights various challenges with the quality of sensor data from networked mobile sensing units such as interference of user behavior, location coverage, calibration accuracy and social aspects of mobile sensing and impact on citizen behavior."
"in this section, we describe the experiment and the results when personal sensing device (node) is deployed inside the car, which would be typical in personal sensing scenario. figure 10 shows the psd (node) mounted inside the car near the vent, while the msb is fixed outside the car (as shown in figure 6 ). during the experiment we kept the fans open to maintain the airflow from outside."
"adoption of sophisticated touch sensors on robotic hands has been hampered by the lack of fully flexible coatings that can easily be attached to existing hardware. instead, touch sensors often rely on rigid parts or bulky processing equipment. in this paper, we presented a novel crossed-wire sensor design that aims to overcome these problems. it is fully flexible and works with only a limited set of connecting wires. it provides a soft coating for improving object grip. spatial resolution can be easily varied to allow for different levels of accuracy, for example on the fingertips and the palm. moreover, it can be produced in large sheets or easily stitched together from smaller ones."
"the table in fig. 14 shows that the training set combination of the first and last data sets gives the minimum error of approximately 5% for the classification of the different indentation positions as compared to the other training set combinations. reduced classification accuracy was obtained with the training sets recorded initially and tested with datasets obtained later w.r.t. time. this is due to the fact that this training set covers the full range of temporal output values for the sensor patch and thus is effective in classification of the indentations. due to the material properties of the sensor patch, such as relaxation behavior over time, significant variation of output values were obtained. these findings will lead to new calibration procedures accounting for the relaxation effects within the patch over time, and thus take into consideration the significant variation of the output values which is caused by the material properties of the sensor."
"the two models presented in this paper, though built with the common goal of air quality monitoring, present varied advantages and challenges. since the public infrastructure model uses buses travelling on fixed routes at scheduled times, it ensures a constant reliable stream of pollution measurements. the social communitysensing model, on the other hand, relies entirely on the participants to generate data. hence, the number of individuals actively participating will determine the breadth of pollution information obtained. but, unlike the bus scenario, where data will always be pertaining to certain fixed routes everyday, the social scenario would collect air quality measurements for multitude of routes across the country and enable wider coverage and provide redundancy."
"for the blending of the polymer and the carbon black, we used a twin-screw extruder. the self-compounded ultrasoft polymers was granulated by cooling it with liquid nitrogen. the resulting granules were then processed by the injection molding machine."
"this paper presents two mobile platforms for fine-grained realtime pollution measurement, mobile sensing box, deployable on public transportation infrastructure and a personal sensing device (node) that can be used to create a social pollution sensing. we conclude that both approaches are feasible. we also show that a personal sensing device can be conveniently used inside the car, yet still producing meaningful pollution measurements. however, more work is needed to arrive at a model that reflects the ground truth pollution values."
"the heat map and the marker map serve as basic visualization tools. unfortunately, color gradients and markers do not help in data analysis and cannot be meaningfully consumed by downstream consumer applications. we accommodate these needs by extending our storage model to store temporarily generated tables and share them with consumer applications."
"a new rate-flow based aqm method, favqchoke is designed to improve the performance of congested routers in ip networks. favqchoke requires the queue, load and flow information to adapt the queue size with regard to the dynamics of traffic in routers. this algorithm enhances the way virtual capacity is adapted to the varying traffic types in ip networks. this aqm also protects adaptive flows from non-adaptive flows resulting in good service under varying traffic. favqchoke algorithm considers the advantages of queue based, load based aqms and flow based algorithm to provide the qos requirements of the ip networks. so the algorithm improves the performance of aqm under heavy load and guarding the adaptive flows from nonadaptive flows to achieve best qos to all users by simulation."
"the vehicle routing problem (vrp) is a well-known combinatorial optimization problem that deals with transportation network management and the scheduling and distribution of vehicles and goods. the vrp is concerned with planning and organizing the distribution of goods to find the appropriate routes to transfer the customers' demands by using one or more homogeneous fleet of vehicles. each vehicle has a limited capacity and it starts its tour from a distribution center (depot), then it transfers goods to customers, and returns to the distribution center. in the literature, several types of the vrp with different complex constraints have been presented and solved over the last 50 years which contributed to minimizing a lot of road transportation problems, such as, pollution and congestion [cit] ."
for another performance comparison simulation for combination of non-linear load and unbalanced linear load has been carried out. the phase-a source current before compensation is shown in the fig. 7(a) and the harmonic analysis is shown in the fig. 7(b) . the thd of source current before compensation for the combination of loads is 14.23%.
"the source is feeding a combination of linear and nonlinear load, where the linear load considered is unbalanced in this case. the measured load current is presented in fig. 12(a) with nonlinear load and unbalance linear load. the shunt apf with vllms is employed for compensation, where the source current is compensated as shown in fig. 12(b) ."
"abstract-the power quality problem in the power system is increased with the use of non-linear devices. due to the use of non-linear devices like power electronic converters, there is an increase in harmonic content in the source current. due to this there is an increase in the losses, instability and poor voltage waveform. to mitigate the harmonics and provide the reactive power compensation, we use filters. there are different filters used in the power system. passive filters provide limited compensation, so active filters can be used for variable compensation. in this paper, a shunt active filter has been made adaptive using a variable leaky least mean square (vllms) based controller. proposed adaptive controller can be able to compensate for harmonic currents, power factor and nonlinear load unbalance. dc capacitor voltage has been regulated at a desired level using a pi controller and a self-charging circuit technique. the design concept of proposed adaptive controller for shunt active filter has been verified through simulation and experimentation."
"where is a revenue of the candidate pair, is the distance from the last vertex ( ) in the route to the candidate pickup customer, is the distance between the candidate customer pair nodes, and is the distance from the candidate delivery customer to the depot. then, the customer pair that has the highest selection ratio is inserted into the route. this procedure is repeated until either the allowed time is consumed or no more customer pairs need to be served. in the c1 heuristic, there is no need to check the precedence and capacity constraints, because each delivery customer is added directly after the related pickup customer."
a highly inductive nonlinear load has been connected to the main grid where the load current is sensed and presented in fig. 14(a) . the highly inductive load current is rich in harmonics which need to be eliminated. the shunt apf with the proposed method is employed for the compensation of the harmonics and to fig. 14(b) .
"to regulate the dc capacitor voltage at the desired level, an additional real power has to be drawn by the adaptive shunt active filter from the supply side to charge the two capacitors. the energy e stored in each capacitor can be represented as (14) if the value of the dc capacitor voltage changes from v dc to v ' dc the change in energy is represented by (15) the charging energy delivered by the three-phase supply side to the inverter for each capacitor will be p: additional real power required t: charging time v rms : value of instantaneous supply voltage v i dc-rms : value of the instantaneous charging current i dc : phase difference between supply voltage and charging current (16) neglecting the switching losses in the inverter and according to the energy conservation law, the following equation holds from (15) and (16) ."
"thus, the ir represents the greedy property of selecting the next customer to be inserted in the route. then, all the candidate customer pairs will be sorted in descending order with respect to their ir values and assigned to the candidate solution set (css). after this, the first half of the candidate solution set is assigned to the restricted candidate list ( ), so that the opportunity of selecting a good pair of customers increases. thus, the random property lies in choosing one customer pair randomly from the . the previous process will be repeated to add the rest of the un-served pairs of customers, while considering the time limit and the vehicle capacity constraint. once the time limit of the vehicle has been exhausted, a new route (vehicle) will be initiated. at the end, we will have an initial solution for the mvppdp which contains a number of routs equal to the number of vehicles. to increase the opportunity of getting better solutions, we decided to use the concept of the population-based metaheuristics, where a number of solutions are generated at the same time and then we select the best one according to a certain evaluation criterion. therefore, the previous method of generating the initial solution will be repeated several times according to the population size to generate a number of candidate solutions. the quality of each candidate solution is evaluated in terms of the value of the objective function (of). this objective function is defined as maximizing the total profit by subtracting the total travel cost from the revenues collected, as shown in equation (1) . after that, the best solution that has the highest value is saved in a matrix to compare it with other best solutions that are selected from different iterations. finally, we select the best overall solution which represents the initial solution for the mvppdp."
various simulations at different power system conditions have been carried out with the proposed vllms based controller. simulations have been done using power system block set under matlab/simulink environment.
"to increase the diversity of solutions, half of the solutions that are found in the css were assigned to the rcl; i.e., if the number of solutions in, then ⌈( ) solutions are assigned to the rcl. this value has been chosen by trial, since choosing a smaller size for the rcl resulted in solutions that are similar to each other and thus lack diversity."
"to analyze the system performance, another condition has been taken into consideration. linear load with unbalanced nature is connected to the source. the unbalanced load current is shown here in fig. 8(a), whereas after compensation with the proposed technique it becomes more sinusoidal as shown in fig. 8(b) . spectral analysis of load current is found to be 20.93% from fig. 8(c) and with the proposed method, the thd has been reduced to 2.21% clearly shown in fig. 8(d) . simulation parameter for this condition has been presented in table ii."
"i. introduction o ver the past few years, rapid increase in the use of nonlinear loads causes many power quality issues, like high current harmonics, low power factor and excessive neutral current. the increased harmonics, reactive power and unbalance cause increase in voltage distortion, line losses and instability when harmonic current travel upstream and produce drop across the line impedance, which leads distortion in power system. usually, passive filters are used for suppression of harmonics but their applications are limited to fixed amount of compensation. passive filters are also not capable in providing solutions in presence of unbalance and variable reactive power compensation. another disadvantage with passive filter is the problem of resonance which amplifies current of certain harmonic frequencies. the solution to above mentioned problem can be realized using a shunt active power filter [cit] ."
"the mvppdp belongs to the type of spdp that is subject to minimizing the travelling costs and maximizing the profits collection. the mvppdp aims to visit only the profitable customer pairs, in order to make the gathering operation as profitable as possible. thus, the mvppdp can be considered as a combination of two types of problems: the feature of selecting a subset of customers belongs to the spdp, while the feature of selecting customers that have maximum revenue belongs to the profitable tour problem (ptp). therefore, the literature review of the mvppdp will be classified into three parts: the mvrppdp, the spdp and the ptp."
"in our proposed method, we use a greedy-randomized procedure that is based on the well-known grasp (greedy randomized adaptive search procedure) to generate initial solutions for the mvppdp. as previously mentioned, the grasp consists of a construction phase (greedy-randomized) and an improvement phase (local search). in this paper we only utilize the construction phase to generate the initial solutions for the mvppdp. any local search method (e.g. hill climbing, simulated annealing, genetic algorithm, etc.) can be used later to improve the constructed solution quality. the framework of the greedy randomized procedure is shown in algorithm 1 [cit] . we adapted the gr procedure to our problem as explained next."
"in this case, an unbalanced linear load has been connected to the main grid. the sensed load current is shown in fig. 13(a) . after compensation by shunt apf with vllms, the compensated source current is shown in fig. 13(b) ."
the paper is organized as follows. section ii discusses in detail about adaptive shunt active power filter. section iii shows the simulation results and discussion. section iv experimentally verifies the proposed algorithm. section v concludes the paper.
"first, we select seed customers for each route based on a greedy approach. then, we fill up the routes with the rest of the customers, based on combining two criteria: greediness and randomness."
the simulation is done using proposed vllms based controller and phase a source current after compensation is shown in fig. 7(c) and the harmonic analysis of the phase a current is shown in the fig. 7(d) . dc side capacitor voltage is shown in fig. 7(e) .
"in this paper, the construction phase of a greedy randomized adaptive search procedure (grasp) was used to build initial solutions for the multi-vehicle profitable pickup and delivery problem (mvppdp). the algorithm uses the concept of a restricted candidate list (rcl) to combine between random and greedy properties, which can help in the diversification of the search, and is especially beneficial for population-based metaheuristics. the performance of our algorithm was compared with two construction heuristics that were previously used to build the initial solutions of the mvppdp. the results proved the effectiveness of the proposed method on small-medium sized instances, where eight new solutions were obtained for the mvppdp. also, the grasp method had a better performance than one of the other construction heuristics on 11 test instances. nevertheless, the proposed method is not currently able to produce good results on large-sized instances (500 and 1000 customers). this is possibly due to decreasing the number of iterations because of time limitations. future work will try to improve the performance of the algorithm by optimizing its runtime. in addition, a second phase may be added to try to insert un-visited customers to increase the profit of the company and improve the quality of the initial solutions. finally, an improvement phase, using a selected populationbased metaheuristic, will be added to improve the initial constructed solutions by the grasp based method."
"step 10: select the best solution that has the highest objective function in the final-best-solutions matrix to be the initial solution for the www.ijacsa.thesai.org done in equation (19) . the customer pair with the highest ir is inserted in the route with respect to the precedence and capacity constraints. the second stage of the algorithm, tries to insert all customers that have not been inserted in the first stage, due to capacity violation. table iii presents the results of constructing initial solutions for the mvrppdp using our grasp method. the results are calculated in terms of the objective function (of) of the solution (i.e., the gained profit), which is equal to total revenue -total cost, as previously shown in equation (1) . thus, the larger the of value, the better is the solution obtained. the results of our grasp were compared with the results of c1 and c2. in table iii, the first column presents the instance name. the following two columns show the results of grasp in terms of the average and best objective value of 5 runs respectively. the third and fourth columns represent the results of c1 and c2 algorithms in terms of the best objective value. for each group of instances of a particular size, the average results are shown in the highlighted row."
"customers with fixed revenue to generate short route ( --s), 100 customers with fixed revenue to generate long route ( --), customers with proportional revenue to generate short route (100-p-s), 100 customers with proportional revenue to generate long route (100-p-l), 100 customers with random revenue to generate short route (100-r-s), 100 customers with random revenue to generate long route (100-r-l). each data instance was tested with different numbers of iterations: 10, 50,100 and 500. the results of testing show that there is no enhancement in the objective function values after 100 iterations. thus, the maximum number of iterations was taken to be 100."
"step 5: put all the candidate customer pairs in the in descending order of ir step 6: assign half of the candidate customer pairs in the to the step 7: pick one customer pair randomly from the and insert it in its best position in the route, as calculated in step 3."
"to maintain the value of each dc capacitor voltage at the reference level v * dc / 2, v dc / 2 is measured and fed back to a pi controller as shown in fig. 3 to manipulate v ' dc / 2. so that it can be used in (17) to compute the required peak value of the charging current i dc from the supply side. the pi controller also helps in reducing (17) the steady state offset between the reference v * dc / 2 and the actual v dc / 2. the pll synchronizes itself with the supply voltage of phase a i.e v a and gives three output sinewaves which are 120° out of phase with each other. these sine waves are multiplied with i dc to obtain three phase i dc . in order to force the supply side to deliver i dc, a term consisting of this i dc is added to the three phase injection currents i inj that can be represented by (18) fig. 3 shows the schematic of three phase self-charging circuit with pi controller. the negative sign indicates the flow of charging current into the vsi. for each phase it lags by an angle of 120°. the reference currents calculated shows that the adaptive shunt apf injects i ln and i l1,q into the line to compensate the harmonic currents and the reactive power respectively, and at the same time it receives the charging current i dc from the supply to regulate the dc capacitor voltage."
the voltage of the dc side capacitor is maintained constant using a self-charging circuit in the vllms based current decomposer. the voltage of the single capacitor on the dc side is shown in the fig. 6 .
"transportation management is considered one of the most difficult problems facing people and governments in different countries all over the world. in our daily life, millions of people use land, sea, or air transport means to commute from one place to another, raising the need to optimize the planning of these services, in order to reduce their cost as well as their negative environmental impacts. therefore, a lot of research has been conducted recently to address these problems in the fields of computer science, operations research, and industrial engineering. land transport, in particular, has received a great interest from researchers, due to its huge volume. research efforts try to optimize the daily use of the means of transportation, such as cars, buses, trucks, trains, motorcycles, trams, etc. among the most known land transport problems are: vehicle routing problems [cit], pickup and delivery problems [cit], bus scheduling problems [cit], truck routing problems [cit], cash transportation problems [cit], railroad blocking problems [cit], and others [cit] ."
"in (1) above, the instantaneous current of the nonlinear load is expanded into 3 terms. the first term is the load instantaneous fundamental phase current i l1,p which is always in phase with the supply voltage. the second term i l1,p is the load instantaneous fundamental quadrature current which is always 90° out of phase with the supply voltage. the third term i ln is the load instantaneous harmonic currents. from fig. 1, it can be shown that (2) in order to have i s that is almost in phase with v and at the same time consists only of the fundamental component, from (2) (3) the dc voltage of each capacitor v dc /2 is also measured and passed to the self-charging circuit to regulate to its reference voltage level v * dc /2. the output signal from the self-charging circuit i dc together with i l1,q and i ln will form the reference injection current of the adaptive shunt active filter i * inj ."
"in the greedy construction heuristic (c1), the customer pairs are added sequentially to the route based on the value of the selection ratio (sr). in each iteration, the sr is calculated for all candidate customer pairs ( ) as follows:"
"the mvppdp is a static problem, where all problem constituents are known in advance. the problem is characterized by having a central distribution location (depot), a predefined number of homogenous vehicles and a set of customers' requests. each request has a predefined pair of customers: a pickup customer and a delivery customer. moreover, these requests include transferring a number of homogenous products (i.e., they have the same characteristics and quality) from pickup customers (origins) to their corresponding delivery customers (destinations) to get profits. fig. 1 represents a simple example of the problem, where there are three vehicles that serve only the profitable customer pairs (i.e., those who make high revenues at the lowest possible cost) among 50 customers [cit] ."
"the outline of the construction phase of our grasp is presented in algorithm 2, where the meaning of each notation is as follows: pop_size: the size of the population, max_iter: maximum number of iterations, num_vehicles: number of vehicles, css: candidate solutions set, rcl: restricted candidate list, us: un-served pairs of customers and sm: solutions matrix that contains the best solutions in the population for each iteration."
"where is the distance from the depot to the candidate pickup customer, and is the distance from the candidate delivery customer to the depot. the pair with the shortest id is selected to be a seed customer for the route. then, the routes are constructed with the rest of unvisited customers by computing the insertion ration (ir) for each customer pair as step 3: for each customer pair, insert it in the best position in the route after checking the precedence, time, and capacity constraints"
"the constraints are the objective function (1) tries to maximize the total profit by subtracting the total travel costs from the total revenues. constraints (2) and (3) mean that each vertex is visited at most once. constraints (4) and (5) mean that the origin depot (starting point) cannot be entered, and the destination depot (end point) cannot be departed by any vehicle . constraint (6) means flow conservation. constraint (7) means that each customer pair (pickup customer and delivery customer) has to be served by the same vehicle. constraint (8) means all vehicles must start from the depot and return to the depot. constraint (9) means exceeding the vehicle capacity is not allowed. constraint (10) means that the load of a vehicle is bounded by . constraint (11) means that all the vehicles start with empty load. constraint (12) means that the delivery customer cannot be visited before its corresponding pickup customer. constraint (13) means that the earliest time to start the service at vertex is given by the beginning of service time at vertex plus the service time at and the travel time between and . constraint (14) means that the maximum time for each vehicle is restricted by . constraint (15) means each vehicle starts at the depot at time 0. finally, decision variables are defined by (16) and (17) . www.ijacsa.thesai.org iv. proposed method most metaheuristic algorithms start solving a problem by generating one or more initial solutions and then they improve them using some local search method. the type of method used in generating the initial solution(s) plays an important role in the efficiency and effectiveness of those algorithms, regardless of the improvement method used. there are two main approaches that can be used to construct initial solutions: random and greedy. the random approach is simple, fast, and can create diverse solutions. however, the produced random solutions maybe of very low quality, which makes the job of the improvement procedure harder. in contrast, the greedy approach, which takes the objective function into consideration while constructing a solution, is often complicated and needs more computation time. in addition, although it overcomes the other approach with respect to the quality of solution, it risks getting stuck in local optima mainly due to lack of diversity [cit] . to combine the advantages of the random and greedy approaches, greedyrandomized (gr) procedures can be used. the main idea behind these procedures in general is to select at random one of the best (greedy) decisions (instead of the absolute best as done in pure greedy methods). this helps in diversifying the search and is especially beneficial for use within populationbased metaheuristics."
an inductor which acts a low pass filter is connected in between the filter and the pcc to eliminate the higher order harmonics. the compensating signals along with the original injecting currents are given to a adaptive hysteresis current controller to generate the switching pulses for the igbts or switches in the inverter to produce the required currents.
"researchers in this field generally aim at minimizing congestion and the environmental damage of the transportation services, caused by harmful emissions, such as carbon dioxide and other greenhouse gases, which cause pollution and global warming, and have a negative effect on people's health. in the shipment sector, for instance, the average fleet emission for delivery trucks and vans is kg of co 2 per gallon of diesel consumed [cit] . furthermore, many trucks are not exploited to their full capacity, where statistics indicate that to of pollution, traffic congestion, and accidents are caused by empty trucks [cit] . besides the environmental damage, the inefficient regulation of the trucks' paths and the non-exploitation of their full capacity has an economic effect on the companies that work in the transport sector. in order for these companies to remain and continue its activities in the market, many solutions are suggested to increase the companies' profits, as well as to reduce their costs."
"because of the importance of selecting seed customers and their clear impact on the performance of the algorithm in general, several heuristics have been proposed. the seed vertex is usually selected according to a specific criterion, often in relation to the depot (e.g., nearest, farthest, related, ..., etc.). in our method, we select the seed customer pairs for each route according to a certain measure that we call the customer benefit (cb). the cb is calculated based on the revenue gained after delivering the demand to the delivery customer, with respect to the distance between the customer pair. using the notations of section 3, the customer benefit of a customer pair ( ) is calculated as shown in equation (18): (18) algorithm 1: the greedy-randomized procedure [cit] where is the revenue gained after delivering the goods to the delivery customer, and is the cost of travelling from the pickup node to the delivery node. thus, the cb tries to rank customers based on their relative benefit versus cost with respect to the operating company. the customer pair that has the highest benefit is selected to be a seed customer."
"the rest of this paper is organized as follows. in section 2, a review of some related work is presented. the definition and mathematical formulation of the tackled problem is given in section 3. in section 4, the proposed method is described in detail. experimental results are discussed in section 5. finally, conclusions and future work are presented in section 6."
"after the updating of the vector of unknown parameter using vllms algorithm, (13) as seen from fig. 1, the current output of the vllms based fundamental extraction circuit is subtracted from the load current. the subtracted output serves as a major component in reference current generation. fig. 2 shows the flow chart of the active component of fundamental current extraction scheme using vllms algorithm."
"the mvppdp is an np-hard problem [cit], which means that exact algorithms can find an optimal solution for only a small number of input data. therefore, metaheuristic algorithms have been often used to find good solutions that are not necessarily optimal, in a reasonable processing time."
"observing the results in table iii, the proposed algorithm demonstrated good performance on average in solving the small and some medium-sized data instances, where we got new best solutions for 8 data instances (bold values). also, we got 8 solutions that were better than either c1 or c2 (underlined values). however, the proposed algorithm was not the best one for the large instances on average, but its performance was acceptable in some cases where we got two solutions that are better than one of the other two heuristics. this is probably due to decreasing the number of iterations for the large instances due to extensive time consumption. also, we observed that when the grasp was better than one of the other heuristics, its performance was better than c1 for instances with fixed and random profits (except 20-p-s). on the other hand, it was better than c2 on instances with proportional profits to its demand. overall, we believe that our method is comparable with the other two methods and has the advantage of being able to construct a number of solutions (a population) that are characterized with the greedy-randomized feature, rather than just one greedy solution as done in the other two construction methods."
"the real validation of the proposed scheme has been verified by hardware experiment performed in the laboratory. ds1103 controller board has been used which is compatible with matlab to control the experimental setup, the controller has been developed in matlab/dspace interactive platform. the pwm signal generated from the control board are passed through the driver circuit which is fed to the inverter. the experimental setup developed in the laboratory includes voltage and current sensor boards, driver circuit, inverters. the hardware setup block diagram is shown in fig. 10 . single phase shunt active power filter is connected in to the distributed system at the load end. the experiment is conducted in the following steps, first the source is feeding the load without the power conditioner connected to the system and the nonlinear load current is sensed which is highly rich in harmonics."
the current source i l is used to model the instantaneous current of the nonlinear load that can be represented by (1) where i l1 is the peak value of the fundamental component and i ln is the peak value of the harmonic component. l1 and ln are the phase angles of the fundamental and the harmonic components. fig. 1 shows the circuit for shunt apf. voltage source v represents the instantaneous supply voltage at the pcc with i s as its instantaneous supply current. the injection current of the shunt active filter is denoted by i inj . the first order low-pass filter in series with the vsi output is represented by inductor l sh with resistor r sh as the inverter losses. v dc /2 denotes the voltage of each capacitor unit.
"so far as signal processing techniques are concerned, least mean square (lms) is a favorable choice. because of fixed step size in conventional lms technique, it has slower rate of convergence. this can be overcome using time varying step size [cit] . since least mean square of error is taken as the cost function in lms algorithm, weights are not bounded and it takes more time to respond because of stalling effect [cit] . to overcome this, leaky lms algorithm is employed where magnitudes of weights are also included in cost function to nullify the stalling and parameter drifting effect [cit] . this paper proposes a vllms based technique for shunt active filter. harmonic currents and reactive component of nonlinear load can be extracted using a circuit based on above technique. both harmonic and reactive current of nonlinear load together with signal from self-charging circuit [cit] form the reference injection current of adaptive hysteresis controller for generating switching signal of three phase igit voltage source inverter. it becomes also able to compensate for unbalanced load currents and bring the power factor of the supply side to become unity. dc capacitance is also maintained at a desired level using a self-charging circuit. the main contribution of the paper is on the implementation of vllms in active power filter circuit for faster adaptation of active filter to any variations in operating conditions. conventional weight updating algorithm is modified by replacing with vllms based weight updating algorithm, which greatly enhances the speed of algorithm and extraction. another contribution of this paper is the use of adaptive hysteresis band current control technique for avoiding acoustic noise uneven switching frequency in hysteresis band current control."
"the performance of the system with non-linear load has been analysed by simulating the shunt apf filtering using both the the matlab /simulink results are presented in fig. 5 . fig. 5 (a) shows the source current of phase-a without any compensation. the thd of this current as shown in fig. 5(b) is 17.93%, which exceeds the ieee standards. the source current of phase-a after compensation using proposed controller is shown in fig. 5(c) . the harmonics are reduced and the source current is almost sinusoidal. the thd of the source current has been reduced to very low value which is permissible (2.84%) and the harmonic analysis of the current is shown in fig. 5(d) ."
"in this paper, a new control design for the shunt active power filter has been presented. the controller design is based on vllms based algorithm for fundamental current extraction. with the use of this proposed algorithm, the performance of shunt active filter has been enhanced. the proposed shunt active power filter compensates balanced and unbalanced nonlinear load currents. self-charging capability has also been integrated into the proposed shunt active power filter for regulating the dc capacitor voltage. simulation and experimental results under various system operating conditions have verified the effectiveness and robustness of the proposed adaptive shunt active filter."
"few number of parameters that needs to be tuned is one of the reasons that encouraged us to choose the grasp metaheuristic. there are only three parameters: the size of the candidate solutions set (css), the size of the restricted candidate list (rcl), and the number of iterations (max_iter). in our method, we added a fourth parameter which is the size of the population (pop_size); that is the number of solutions that were generated using the greedyrandomized approach, and the best one is selected before moving to the next iteration. empirical experiments were performed to select the suitable value for each parameter. six medium-sized data instances have been used to tune the parameters. these are the data instances containing customers that are served by vehicles. all three revenue states were considered in the instances namely fixed revenue for all customers, proportional revenue to demand and randomly selected revenues. also, each revenue case has been tested twice: when the vehicle capacity is units and the total time limit is 6000 to generate a long route, and when the vehicle capacity is and a total time limit is to generate a short route. the details of testing each parameter are as follow:"
"the three phase source is connected to the nonlinear load, which injects harmonics to the distribution system. the harmonic rich load current is measured and presented in fig. 11(a) . the vllms algorithm is applied for the compensation of load harmonics. the source current after compensation is presented in fig. 11(b) ."
"in this paper, we present a new approach to construct initial solutions for the mvppdp. we adapt the construction phase of the well-known greedy randomized adaptive search procedure (grasp) to the underlying problem. grasp is a multi-start metaheuristic that is commonly applied to solve different combinatorial optimization problems. it was first introduced by feo and resende [cit] . it consists of two phases: construction and improvement phase. the construction phase is used to build an initial feasible solution, while the second phase is a local search used to improve the initial solution found in the construction phase to get a local optimum [cit] . there are several advantages of grasp compared to other popular metaheuristics, like genetic algorithms, simulated annealing and tabu search. these include combining the advantages of random and greedy search, which helps the grasp to be fast, competitive and able to find good solutions in a reasonable time. furthermore, the number of parameters that need to be tuned is small, which is another advantage that makes the grasp preferred over other metaheuristics [cit] . also, grasp has successfully contributed to solving different variants of vrp [cit] ."
"the mvppdp can be formally defined as follows [cit] : let ( ) be a graph in which * + defines the vertex set, where the vertex ( ) represents a central depot, while the remaining vertices represent the pickup customers * + and delivery customers * + *( ) + defines the arc set, where each arc is associated with a non-negative routing cost . each delivery vertex i has a revenue to be gained when visiting it. also, each vertex has a supply (pickup, ) or demand (delivery, ) at start, there is no supply or demand in the depot ("
"after selecting the seed customer in the route, several candidate sub-routes will be generated for each vehicle. the following steps will be repeated to fill up each vehicle until either the final allowed time of the vehicle has been reached or no more customer pairs need to be served. firstly, insert all customer pairs that are not served yet, where each pair of customers is inserted individually into a route in the best possible position. that is, the position that leads to the lowest possible total distance. while doing this, in order to achieve the precedence constraint, the best position for the delivery customer will be selected after adding the pickup customer."
"3) the size of the population: the same previous dataset instances were tested again to select the appropriate number of solutions in the population. the population size was increased from 10 to solutions. table ii presents the values of the objective function after setting the number of iterations to . the results showed that increasing the population size did not lead to an improvement in the objective function value. thus, the population size was taken to be 10. www.ijacsa.thesai.org algorithm 2: the pseudocode of the construction phase of grasp"
"this condition is very much essential to test the effectiveness of the proposed method. in this condition balanced and highly inductive loads are connected to the source. parameters considered for this simulation are shown in table iii. inductive load current obtained from the simulation has been shown in fig. 9 (a), after compensation with the application of vllms decomposer control algorithm the source current become sinusoidal as shown in fig. 9(b) . from the spectrum analysis the harmonic content can be known, thd of the load current is 30.50% as per fig. 9 (c) which has been reduced to 2.61% for vllms decomposer control algorithm as shown in fig. 9(d) ."
"the two-stage cheapest insertion heuristic (c2) works as follow: in the first stage, select the seed customer pair for each route, based on the idle distance (id), which is computed for each candidate pair ( ) as follows:"
"in adaptive hysteresis band current controller, since modulation frequency f c, almost remains constant, this improves the pwm performances and apf substantially. calculated hysteresis band using above (21), is applied to hysteresis band current controller as shown in fig. 4 for switching pulse generation to be fed back to inverter."
"beat features give meanings to audio signals in human-recognizable terms which generally reveal the human interpretation or perception of certain audio properties such as mood, emotion, tempo, genre, etc. we used the following four main beat features: beat histogram, beats per minute, beat sum, and strongest beat in the audio signal."
"it is important to clarify that the above attributes are indeed the perceived values of a vacant taxi driver before he/she makes the decision to conduct a customer-search in a particular zone. we assume that every taxi driver has the same perception concerning the attributes. therefore, for the sake of simplicity, the subscript q is omitted in each attribute."
"factor 2 -intra-zonal circulation distance: it represents how far a vacant taxi driver travels in a chosen zone to reach the next customer on average. it is indirectly related to passenger demand. the intra-zonal circulation distance factor 4 -rate of return: the rate of return ( ij r ), also known as profit per unit time, is calculated based on a search cycle that consists of the vacant taxi trip from the destination zone ii  of the previous occupied trip to zone ji  for picking up a customer, and the subsequent occupied trip from zone ji  for transporting the customer to his/her destination. the rate of return equals the mean profit (i.e., the difference between the expected in-pocket profit from an occupied trip and the total operational cost) obtained from the search cycle over the expected time taken to obtain that profit. mathematically, the rate of return for a vacant taxi traveling from zone ii  to zone"
"downloading and purchasing music from online music collections has become part of the daily life of probably the majority of people in the world, and quality of music recommendation affects quality of life of billions of people. the users often formulate their preferences in terms of genre, such as jazz or disco. however, many tracks in existing collections are not classified by genre, or a genre is specified for an artist or an album but not for a particular track. given huge size of existing collections, automatic genre classification is crucial for organization, search, retrieval, and recommendation of music."
"step 4 -adjust the probability of success adjust the probability of success step 5 -update the circulation distance and time use the adjusted probability of success to compute the updated circulation distance after finishing the proposed solution procedure, the balancing factor for zone ji  introduced in equation (15) can be calculated as the product of the reduction and expansion factors in all iterations:"
"in the first stage model, a vacant taxi driver in each zone decides whether to search for customers (1) in the current zone or (2) in one of the adjacent zones, and this decision is modeled by the mnl model. if he/she cannot meet a customer in the zone from his/her first zonal search decision, he/she may make another decision in this zone based on the available search options there. the model assumes that all of the zonal customer-search decisions are made independently without depending on previous and subsequent decisions, and that the decisions are made sequentially. this process is repeated until he/she meets a customer. it allows us to trace the zones to be passed through before a customer is reached, or the search paths (in terms of a sequence of zones visited) taken to reach the customer. the zonal decisions were found to be influenced by the following four factors [cit] ) ."
"this study found that reducing the number of taxi licenses can effectively limit the excessive vacant taxi movements at the cbds, but the hong kong government has no flexibility to adjust this number. therefore, rather than issuing more the permanent licenses in the future, it is suggested issuing temporary taxi licenses (e.g., macau, a twin city of hong kong, issued taxi licenses with validity periods of 8-year and 10-year, and after which no renewal would be allowed). the city government could evaluate the conditions of passenger demand and number of vehicles on urban road networks to decide whether issuing new licenses after the expirations. this approach can reserve flexibility to the city government to control the taxi market to balance the demand and supply, and prevent the traffic congestion and air pollution problems at the cbds to reach an unacceptable level."
"for the case of 1 l , the number of local search decisions anticipated is more than one. vacant taxi drivers make their local search decisions based on both the probability of success in each of the cells considered in the first decision and the additional probabilities gained from the subsequent cells after leaving the cells considered in the first decision. when the driver is currently in cell ' w denotes the set of all local movements. similar to equation (5), every taxi driver was assumed to have the same perception concerning the cumulative probabilities of success for the corresponding connected cells. therefore, for the sake of simplicity, the subscript w is omitted in the attribute. the model form is identical to that of the selection probability of a vacant taxi driver any subsequent cell as indicated in equation (9) and the model has the same calibrated parameter ( l  ) for each cell and zone to maintain a consistent search decision everywhere in the network."
"the entering flow φ h+ j predicts how 'good' alternative x j is in the opinion of dm d h, which with the higher value means the better. on the other hand, the leaving flow φ h− j shows how 'inferior' alternative x j is, which with the lower value is better."
"to calculate the value of net flow, the attribute weights must first be solved. in what follows, a new linear goal programming method is put forward to determine the attribute weights."
"soon, we plan to expand the classification tool on a different level by taking into account also lyrics associated with music tracks. in particular, we will extract conceptual and affective information associated with songs by means of semantic multidimensional scaling [cit] and, hence, add these as additional semantic and sentic [cit] features for classification."
"note that the proposed modeling methodology could be applied to any study area using different sizes of zones and cells. however, in general, the sizes of the zones and cells should be designed carefully based on the required level of modeling accuracy and the detail of taxi movements and the information available to the modeler. some problems can be expected if the zones and cells are either too big or too small. if the sizes were too big, all differing data collected would be aggregated, making a useful analysis and conclusion impossible. an unnecessarily large zone or cell would also create concentrated traffic loading in particular cells or zones of the network, thereby distorting the overall traffic pattern. hence, the taxi movements and operation predicted could not be useful for planning if the sizes are too large. if the sizes were too small, the relevant data collected would be statistically unreliable, and the number of samples in each zone and cell would be insufficient to provide representative means on the model explanatory variables. moreover, some of the information may be unavailable if the sizes are too small because it is too expensive to collect."
"in this section, a real bank recruitment case is analyzed to demonstrate the application of the proposed method of this paper. meanwhile, a comparative analysis is conducted to illustrate the superiorities of the proposed method."
"we can see that svm performed better on choosing between the category with the highest membership value and that of the second highest one. here, we used all features, which corresponds to the last row of table i ."
it can be called an additively consistent fpr. the additively consistent fpr can be obtained by the priority vector of the alternatives. assume a priority vector
"combining the rankings of the two sub-problems, the final ranking is in accordance with the original ranking of undecomposed magdm problem and meets the transitivity. if the original magdm problem is decomposed into other two sub-problems, the same conclusion holds. thus, the obtained results are valid under test criteria 2 and 3."
"the rest of this paper is organized as follows. in section ii, some definitions and distances of ifss and trifns are briefly reviewed. section iii describes the heterogeneous magdm problems with incomplete weight information and gives the normalization methods. in section iv, a new promethee-flpm method is proposed for solving heterogeneous magdm problems. an application in a bank recruitment case and comparison analyses are given in section v. the conclusions are made in section vi."
"with the net flow φ h j, dm d h can make a ranking more persuasive than just using either the entering flow or the leaving flow."
"remark 2: zhu and xu [cit] directly gave the additive fprs for gdm, whereas this paper constructs the additive fprs through the priority vector (21)). this is the most difference between zhu and xu [cit] and this paper."
"factor -cumulative probability of success: the cumulative probability of success ( l y s ) represents the accumulated probability of a vacant taxi driver successfully picking up a customer, if the driver initially selects cell y and prepares to make local search decisions l times to meet a customer. the cumulative probability equals the probability of successfully picking up a customer in the first cell visited plus the additional probability gained from the subsequent cells. this can be expressed as"
"in fact, the second and subsequent decisions are only the initial (or conceptual) plan of the taxi driver made at his/her current position. he/she may not necessarily follow the plan and confine his/her remaining search decisions to the subsequent cells in the plan. the driver re-evaluates his/her plan after reaching the next cell and his/her decision made in each cell is independent of the decision made in other cells. in this sense, the local search decisions made in all cells are independent and are made sequentially."
our approach is a bit different from existing semi-supervised approaches in a manner that we used fuzzy for training supervised classifier in order to map a 10-way classification problem to 2-way or 3-way classification problem.
"in this paper, we proposed a new fuzzy promethee-flp method to effectively solve the magdm problems with heterogeneous information formats involving real numbers, ifss, intervals, tifns and trifns, and incomplete weight information. based on the net flow of promethee method, the attribute weights are determined through constructing a multi-objective programming model which is transformed into a single objective program to resolve. transforming the net flows into individual priorities by a defined conversion formula, additive reciprocal fprs are obtained and used to derive the primary group priorities by the constructed flp model. then the dms' weights are calculated using the defined effect index. the ranking order of alternatives is generated by the adjusted group priorities. finally, a bank recruitment example analysis verifies the feasibility and effectiveness of the proposed method of this paper."
where ij  denotes the toll charge (if any) associated with the trip from zone ii  to zone ji  . j p in equation (2) is the expected in-pocket profit from an occupied trip starting in zone ji  and equals the average taxi fare paid by passengers originated from that zone minus the corresponding average operational cost of the occupied trips. the expected profit can be formulated as
"if any element is bigger than 1, it would be set to be 1. the membership degree w h ij and non-membership degree volume 6, 2018"
"long-time features can be obtained by aggregating the short-term features extracted from several consecutive frames within a time window. we have used derivate, standard deviation, running mean, derivative of running mean, and standard derivative of running mean as the aggregation methods of short-time features."
"to test the possibility of convergence of the iterative solution procedure and the uniqueness of the solution, we attempted inputting different initial (zonal vacant taxi distribution) solutions into the procedure. the procedure achieves the same convergent solution to all these scenarios, but the numbers of iterations required are different which depends on the similarity between the inputted initial solution and the convergent solution. in addition, from table 2, we found that the adjustment coefficient (  ) and the acceptable tolerance (  ) affect the model results and the speed of convergence. we tested five combinations of  and , where different solutions (as reflected by the rmse) are obtained under various settings of these parameters, because these parameters control the set of acceptable solutions. especially, when we adjusted  to 0.5 and maintained  at 0.05, the solution procedure cannot stop because the adjustment rate per iteration is too large to satisfy the requirement of acceptable tolerance. figure 5 shows that a high proportion of vacant taxi trips (30.18%) ended in the highly congested cbds (including western and central district, wan chai district, and yau tsim mong district, which are shaded in the figure). these trips worsened the traffic congestion and air pollution problems there. furthermore, the figure also shows that the taxi service quality to taxi customers at the non-cbds was poor since the taxi supply to those areas was limited. it is therefore important to develop taxi regulation policies to achieve a more even distribution of vacant taxis to all zones and cause a lower proportion of vacant taxi trips ended in the cbds in customer-search. the proposed two-stage model and the solution procedure can be used for this purpose. we considered two types of taxi policies: entry restrictions (adjusting the fleet size) and price controls (adjusting the fare). figure 6 presents the consequences due to adjusting the fleet size (from 50% to 150% of the current level) and fare (from 50% to 150% of the current level) during the morning peak period. the fare structure was assumed to remain the same but all charges, including the initial charge, addition charges, and unit charges for waiting time and travel distance, were adjusted according to the same percentage. (e.g., all the charges were discounted by 20% if the percentage equals 80%.) the solid lines represent the estimated proportion of trips ended in the cbds. from this figure, it is noted that lowering the fare might help reduce the proportion of trips ended in the cbds. however, the effect was not guaranteed. for example, if the fleet size remained unchanged, the proportion of vacant taxi trips ended in the cbds was anticipated decreasing from 30.18% (the current situation) to 30.16% when the fare was reduced by 10%. however, if the fare was further reduced to 50% of the current level, the proportion would climb up to 30.35%. nevertheless, the effect was minimal. if the fleet size was increased but the fare remained the same, the proportion of vacant taxi trips ended in the cbds would decrease, and the proportion ended in the non-cbds (i.e., one minus the proportion ended in the cbds) would increase, meaning that vacant taxis would be more evenly distributed among all the districts and the supplies of vacant taxis among these districts would be closer. increasing fleet size could increase the taxi supplies and hence the service quality at the non-cbds, as the proportions ended in the non-cbds increased with the total fleet size. however, a lower proportion of trips ended in the cbds after increasing fleet size does not necessary mean that fewer vacant taxis ended there. the resultant number of taxis ended in those districts depends on the percentage decrease of the estimated proportion and the percentage increase of the fleet size. it could happen that the resultant number of vacant taxis going those districts increased. for example, if the fleet size was increased by 50% and the fare remained unchanged, the estimated proportion of trips ended in the cbds would be decreased from 30.18% to 29.55% or equivalently the proportion had a 2.09 percentage reduction. this percentage reduction was much less than the percentage increases of the fleet size of 50%. therefore, an increase of about 47% of vacant taxis was expected to circulate in those congested areas."
"by the promethee method, we can get the final net flow φ h j which can be converted into individual priority ϕ h j by a conversion formula defined as"
"comparing with other classifiers we tried several classifiers, such as multi-layer perception (mlp), naïve bayes, and knn. while mlp performed better than naïve bayes and knn, none of them outperformed our two-stage procedure; see table iii ."
"according to the esl model structure, vacant taxi drivers may make more than one zonal search decision to travel across zones for customer-search. therefore, to predict the how trips generated from a zone is distributed to other zones (referred to as trip distribution) as well as the number of vacant taxi trips ended in each zone for customer-search, it is necessary to calculate the cumulative probability of a vacant taxi driver in zone ii  eventually selecting zone ji  (not necessarily to be an adjacent zone of zone"
"the well-known fuzzy c-means clustering algorithm takes as input a set of n data points x 1, x 1, ..., x n described via their coordinates in a p-dimensional feature space:"
"where l is the number of local search decisions anticipated by a vacant taxi driver and its value should be calibrated., and is referred to the probability of success of the driver in cell j yx "
"to forecast the actual effect to the number of vacant taxis at the cbds, the dash lines representing the variation in the number of vacant taxis at the cbds (from +20% to -20% of the current situation) are also presented in figure 6 . these lines can be used as the taxi policy targets of reducing the number of vacant taxis at the cbds to certain levels. for example, if the policy target was to drop down the number of vacant taxis at the cbds by 10%, the fleet size should at least be decreased by about 11% while the fare remained the same."
"to calibrate our proposed model, the gps data of 460 hong kong urban taxis are adopted. a root mean square error (rmse) evaluation is conducted in this study to measure the goodness of fit of the proposed model with respect to observed data, and to compare with that of the existing taxi customer-search models. sensitivity analyses are also accomplished to illustrate the changes in the proportion of vacant taxi trips ended in the cbds in customersearch with respect to the variations in fleet size and fare. potential taxi policies are investigated and discussed according to the findings."
"calibration -first stage zone choice model: according to the esl model structure, decisions are made independently and sequentially. each zonal search decision is indeed described by an mnl model. the zone choice made by a driver in a particular zone is independent to the decision sequence. hence, the variable coefficients"
where l  denotes the non-negative parameter and its value depends on the number of local search decisions anticipated l made by vacant taxi drivers.
"the latter term j c is estimated by the sum of the product of the proportion of trips selecting cell, and the corresponding expected number of cells traveled to reach a customer after reaching that cell,"
"the study area, hong kong territory, is divided into 18 zones according to administrative districts as shown in to calibrate the first stage logit-based choice model, the attributes such as the average values of travel distance and time, revenue, and operational cost for trips between each origin-destination (od) pair were determined. od pairs with less than five trips were considered insufficient to give a representative average value, and were excluded. after the data extraction, about 11,200 vacant trips and 11,600 occupied trips from these 460 taxis were identified for model calibration."
"similar to the modeling concept of the first stage esl model, a vacant taxi driver may not meet a customer in the cell selected once he/she reaches that cell. if so, he/she makes another search decision in this cell based on the available search options at that cell. the search decision made in a cell is assumed to be independent of the decision made previously. this process is repeated until he/she meets a customer. this process implies that the driver circulates in the cell-based network cell by cell to search for customers and the driver is assumed to make independent decisions sequentially to select the subsequent cells during his/her cruising within the cell-based network, with one decision made in each cell."
"in this section, some basic concepts on ifss [cit] and trifns [cit] are introduced as well as their distances. the comparisons between two fuzzy numbers are given. volume 6, 2018"
"the determination of the optimal number of zonal search decisions (m ) is based on a rmse evaluation. the rmse for a particular number of search-decisions is defined as the square root of the average of the sum of the square differences between the estimated vacant taxi zonal distributions (formed by m j q for all zones ji  ) and that observed from the gps data. the lower the rmse, the better the fit of the model to the taxi trip data. hence, the optimal number of zonal search decisions is the value of m that gives the lowest rmse. the objective can be formulated as"
test criterion 1: an effective mcdm method should not change the indication of the best alternative on replacing a non-optimal alternative by another worse alternative without changing the relative importance of each decision criterion.
there are two requirements to implement the promethee method [cit] : (i) information of weights of attributes; (ii) preference functions. the preference function of two particular alternatives x j and x k over attribute f i given by dm d h can be defined as:
"on the basis of the above analysis, a new promethee-flp method is proposed for heterogeneous magdm problems. a step by step procedure is summarized below."
"from table 1, the ranking order of the five candidates is step 4: the individual priorities are obtained based on the net flows shown in table 2 ."
"multiple attribute decision making (madm) method is widely used to choose the most compromise solution from all feasible alternatives assessed on multiple attributes. however, using only one format of information [cit] to describe different aspects of the problem is far from enough. in fact, multiple different types of evaluation information usually coexist in practical decision making problems [cit] . meanwhile, since the knowledge structures, experience, judgment and evaluation methods differ from man to man, more than one decision maker (dm) are involved in the process to improve the accuracy of the decision [cit] . hence, research on heterogeneous multiple attribute group decision making (magdm) problems with different information formats, like real numbers, intuitionistic fuzzy sets (ifss) [cit], triangular intuitionistic fuzzy numbers (tifns), trapezoidal fuzzy numbers (trfns), triangular fuzzy numbers (tfns), intervals and real numbers, seems to be complex and necessary, which has drawn increasing attention of lots of scholars [cit] ."
"from the above sensitivity analysis, it is found that adjusting the fare could not effectively minimize the number of vacant taxis at the cbds. a more effective taxi policy that specifically deals with the vacant taxi trips entering the cbds was hence necessary implemented. for this purpose, we considered the surcharge imposed specifically to the trips entering the cbds, and predicted the effects of this policy on the vacant taxi distribution. it is expected that the operational cost to vacant taxis travel across zones to the cbds would be increased and hence the attractiveness of the associated zone choice for customer-search would be reduced after imposing the surcharge. figure 7 illustrates the effects on the estimated proportion of trips ended in the cbds with surcharge (from none to hk$10) imposed to the trips entering the cbds. the solid lines represent the estimated proportion of trips ended in the cbds. as in figure 6, the estimated proportion kept decreasing when increasing the fleet size only. moreover, figure 7 shows that the proportions also kept decreasing when increasing the surcharge only. hence, the proportion decreased when both the fleet size and surcharge were increased. for example, when the fleet size remained unchanged, the proportion of vacant taxis ended in the cbds was anticipated decreasing from 30.18% (the current situation) to 30.06% while imposing hk$1 surcharge to the trips entering the cbds. the estimated proportion could even reach 29.50% if the fleet size was further increased to 150%."
"where b is the number of parameters in the second stage model. by solving problem (22), we can obtain the optimal number of local search decisions anticipated (l )."
"to make a reasonable decision, the attribute weights should be taken into account. suppose that ω i is the relative weight of attribute f i, then the attribute weight vector"
"in promethee method, a net flow φ h j is used to show how an alternative x j is superior to the others in considering of its inferiority in the view of dm d h as follows:"
"factor 1 -relative passenger demand: the relative passenger demand ( j e ) in zone ji  is defined as the number of customers ( j o ) being picked up in that zone compared with the total number of customers being picked up in the entire study area, where i represents the set of zones in the study area, and can be mathematically represented by"
"this additional term forces the algorithm to increase the membership of a data point in the cluster with the nearest centroid, grouping similar points together."
"the confusion matrices obtained with our procedure are shown in table iv . we can observe that misclassification problems are very rare, and quite similar in both cases."
"the modeling concept of the proposed two-stage model can be summarized in the flowchart in figure 1 . the inputs of the first stage model are (1) zonal passenger demand; (2) the time and distance of cross-zonal travel and intra-zonal circulation for searching for customers; (3) average occupied travel time and distance for transporting passengers; (4) unit operational costs; (5) tolls; (6) fare structure; and (7) the observed origins and destinations of zonal vacant taxi movements, where those with an asterisk are the inputs that can be obtained from the gps data. based on these inputs, we calculate the four model attributes for the first stage model as explained in section 2.1. the output of the first stage model is the zonal distribution of vacant taxis, and influences to the probability of success in each cell in each zone. because a zone with more vacant taxis circulating has a lower probability of success meeting a customer there, in equation (15) is defined as the passenger demand generated from cell j yx  over the availability of vacant taxis in that cell, subject to the condition that the probability is between zero and one inclusively. mathematically, it can be expressed as . this probability is cell specific to allow capturing the variation of the probability of successfully meeting a customer in different cells. equations (15) and (16) are used to calculate the cumulative probability of success in each cell. the cumulative probability of success in each cell is the unique attribute for the second stage model as stipulated in section 2.2. together with the inputs of (1) a spatial distribution of customers; (2) a spatial distribution of vacant taxis; (3) average required intra-zonal circulation time and distance in one cell; and (4) the observed origins of local vacant taxi movements, the second stage model estimates the required intra-zonal circulation time and distance for customer-search in each zone. they are part of the multiple attributes of the first stage model. hence, these two submodels influence each other."
"while huge amount of unlabeled data is readily available, labeled data-tracks with the genre reliably assigned by human annotators-are scarce. in this paper we propose to use for genre classification a methodology that was proven to work well in a similar situation: affective labeling of words in natural language texts, where, similarly, unlabeled texts abound but few words have a manually assigned affective label [cit] . for brevity we refer to this methodology as semi-supervised learning, to emphasize that it uses two kinds of data: few labeled examples and a large quantity of unlabeled data; however, internally our two-step procedure works differently from a typical semi-supervised learner. we show that this methodology outperforms a number of standard supervised learning techniques, such as support vector machine (svm) and k-nearest neighbor (knn)."
"experienced taxi drivers indeed would know an approximately average distance and time required to travel to meet their next customers. for this reason, they would not simply make their local customer-search decisions depending on the attractiveness (i.e., the probability of successfully find a customer) of the adjacent cells, but would also consider the subsequent possible choice of cells that could be reached later. the cumulative probability of success is hence introduced to represent the accumulated attractiveness to vacant taxi drivers searching on the way, and it was found to be the factor that affects the local movements of vacant taxi drivers in search of customers [cit] b) ."
"the main drawback of unsupervised methods is that the clusters are not labeled and the boundaries between clusters are not reliably defined. in this paper, we rely on a genre taxonomy well-defined by music experts and well-known to the users, which suggest using a supervised approach."
"where p h i (x j, x k ) is the preference indicator expressing the degree to which x j is better than x k over attribute f i, assessed by the distance of these two alternatives over attribute f i ."
"that is, when the number of zonal search decisions ( (note that zone k must be either adjacent to zone ii , or zone ji , or both of them.) because there can be more than one possible search path linking zone ii  and zone ji , it is necessary to aggregate the probability associated with each search path."
"if the fare was also increased in addition to the fleet size, the proportion ended in the cbds would be further reduced compared with the case of just increasing the fleet size, implying that the supplies among districts would be more even and the supplies to the non-cbds would be further increased. in the extreme case, if both the fleet size and fare were increased by 50%, the estimated proportion of trips ended in the cbds would decline to 29.44%, almost the minimum percentage in the ranges of fare and fleet size considered. however, about 46% more vacant taxis would circulate in the congested areas, and hence the traffic congestion and air pollution problems at the cbds could not be relieved in this way."
"all tracks are 22,050 hz mono 16-bit audio files in . [cit] from a variety of sources including personal cds, radio, and microphone recordings, in order for a variety of recording conditions to be represented."
"in our evaluation, we consider a label to be assigned correctly if the evaluation dataset assigns this label to the music file. to choose only one class for a token under classification, we used a two-step process."
"(1) this paper firstly combines the promethee method with fuzzy linear program to solve the heterogeneous magdm. it provides for us with a new perspective to study the heterogeneous magdm problems. (2) propose a new method to determine the attribute weights. through maximizing the sum of differences of net flows between each two alternatives, a multiobjective program is constructed and transformed into a single objective program. thus, the attribute weights are determined reasonably. (3) with the net flows obtained by the promethee, a conversion formula is defined to transform them into the individual priorities. according to the characteristic of the additive reciprocal fuzzy preference relations (fprs), we can easily get the complete fpr for each dm. (4) the group priorities of alternatives are derived by a fuzzy linear programming model. thereby, an effect index is defined to obtain the weights of the dms, which can be used to make an adjustment to the group priorities. thus, the ranking order of alternatives can be generated by the adjusted group priorities."
"v y x is the probability calculated from equation (10). the optimal number of local search decisions anticipated (l ) is determined based on the bayesian information criterion (bic) evaluation. the model with the lowest bic value is the most preferable as it has the best fit to the taxi trip data or involves the fewest explanatory variables, or both. therefore, the optimization problem can be formulated as"
"− hard clustering: the ambiguity of the fuzzy assignment of category labels to data items is resolved, leaving each data item assigned to exactly one category using a supervised technique."
"the average number of cells traveled in equation (14) should be based on the predicted decisions made by taxi drivers rather than the plans in their minds, because the drivers may change their mind once reaching the next cell"
"such procedure does not guarantee for a hard cluster to be non-empty, for the majority voting not to result in a tie, or for two clusters not to share the same genre label, in which case some labels would not be assigned at all. however, this is low probable and did not happen in our experiments. moreover, correctness of the obtained mapping of the classes to genre labels is confirmed by the fact that we obtained over 90% accuracy of the final results, which is not possible with incorrectly mapped labels."
"compared with the classical promethee, the proposed method utilizes different formats to describe the attribute assessment information, determines the attribute weights objectively, and generates the ranking order of alternatives by the group priorities. in contrast with the gdm with fprs, the proposed method can evaluate all the alternatives with heterogeneous information formats, and get the more fair individual preference relations by promethee. there is no need to consider that the fprs are complete or incomplete. hence, the proposed method integrates the advantages of both promethee and gdm with fprs, provides a new perspective for solving heterogeneous magdm problems. however, the proposed method neglects the fuzzy truth degrees of comparisons of pair-wise alternatives. the future work is to extend the proposed method to heterogeneous magdm problems with the fuzzy truth degrees of comparisons of pairwise alternatives."
"(1) the heterogeneous information considered in this paper includes real numbers, intervals, ifss, tifns and trifns. although most of existing research [cit] studed the heterogeneous madm or magdm problems, there is no research involving tifns and trifns. in fact, tifns and trifns are more flexible and useful than ifss in dealing with ill-known quantities in decision data [cit] . thus, incorporating tifns and trifns into the heterogeneous magdm is of great importance for scientific research and real applications, which is the biggest motivation of this paper. (2) most of prior research [cit] employed the idea of linmap to solve heterogeneous magdm. these achievements failed to consider the advantages of promethee method. the promethee method can build an outranking between different alternatives through comparing each pair of alternatives for each attributes. the information requested by promethee is particularly clear and easy to define for both decision-makers and analysts. taking these advantages into account, this paper intends to utilize promethee to solve heterogeneous magdm."
"as the deviation parameter d h can be used to characterize the preference provided by different dms, there exists a relationship between d h and the weight of dm d h . the smaller the value of d h, the bigger the value of v h, which can be described by a basic unit-interval monotonic (bum) function for simplicity. given a basic bum function defined as f :"
"to achieve more compact clusters in which the most similar elements are clustered together, we incorporated an additional term in the original objective function (2):"
"for feature extraction, we used the jaudio toolkit [cit], a music feature extraction toolkit written in java, freely available for research purposes. 2 as we have mentioned, we used the following three kinds of features: short-time features, long-time features, and beat features."
"then with all the alternatives compared to x j, an entering flow φ h+ j and a leaving flow φ h− j can be obtained as follows:"
"the rest of the paper is organized as follows: section 2 describes related work. section 3 gives an overview of our method. section 4 presents the dataset and the features used in our experiments. sections 5, 6, and 7 describe the three main steps of our algorithm: fuzzy clustering, mapping of the obtained clusters to labels, and the final hard categorization. section 8 gives the experimental results and evaluation. finally, section 9 presents conclusions and future work."
"− assigning random values to all µ ik, normalized to satisfy the constraints (1); − iteratively re-calculate the values for all v i and then all µ ik according to (3);"
"for each data point, we chose k classes for which the fuzzy clustering gave the highest value of the membership function. the hard clustering technique used afterwards was only allowed to choose between those k labels preselected for a given music file."
"in order to test the validity of the results obtained by the proposed method using test criterion 1, alternative x 2, a nonoptimal alternative, is replaced by the worse alternative x 2 shown in table 3 ."
"by comparing figures 6 and 7, we found that imposing hk$1 surcharge to the trips entering the cbds could reduce the proportions ended in those districts more than increasing the fare by 10% (which is equivalent to at least hk$1.8 increase in fare for the initial charge of hk$18). in general, we found that imposing the surcharge was more effective than adjusting the fare on influencing the proportion of vacant taxi trips ended in the cbds. this observation is expected because the surcharge was imposed specifically to the trips entering the cbds to increase the operational cost of the trips and reduce the attractiveness of the districts, but the fare increment was not."
"as a dataset for the music genre classification task, we used the one presented by tzanetakis [cit] . the dataset is publicly available for research purposes. 1 it consists of 1000 audio tracks, each being 30 sec. long, classified into 10 genres: blues, classical, hiphop, country, disco, pop, rock, jazz, metal and reggae. each genre is represented by 100 tracks. we followed this taxonomy for our classification."
"each zone ji  was divided into many equally sized square cells to form a cell-based network. each cell was connected to surrounding cells in at most four cardinal directions. when an urban taxi driver circulates in a zone locally for searching for new customers, he/she has two categories of choices: (1) searching for a customer in the current cell within the zone; and (2) traveling towards one of the adjacent cells in at most four cardinal directions."
"based on the preceding flowchart, we introduce an iterative solution procedure for the two-stage model to obtain a convergent solution of a vacant taxi trip distribution and also the balancing factor. the estimated circulation distance and time in each zone are determined in the second stage model to update the inputs to the first stage zonal distribution model. it is noted that a zone that requires a shorter distance and time finding the next customer attracts more vacant taxis going there, and hence reduces the overall probability of successfully being hired. therefore, we propose applying an adjustment factor for zone ji  to the probability of success in each cell in that zone under different conditions to achieve an equilibrium zonal vacant taxi distribution and the required circulation distance and time. the adjustment factor can be expressed as follows:"
"this paper proposed a two-stage model to predict vacant taxi movements in search of customers. the first stage model was formulated as an esl model, in which the vacant taxi trip distribution to adjacent zones was modeled as functions of the relative passenger demand, the cross-zonal travel distance, the intra-zonal circulation distance, and the rate of return. the second stage model is a cell-based logit-opportunity model, which captures the local customer-search movements of vacant taxi drivers within a single zone. these two sub-models were designed to influence each other, and hence an iterative solution procedure was introduced to obtain a convergent vacant taxi trip distribution. the two-stage model was calibrated by the gps data of 460 hong kong urban taxis. the results show that all of the explanatory variables in both the first and second stage models were significant at the 1% level, and the proposed model offered a great improvement in terms of goodness of fit over the existing taxi-customer search models, which gave that the lowest rmse value of 47 as compared with the observed trip distribution. the sensitivity analyses also illustrated the model capabilities to predict the changes in vacant taxi trip distribution under the implementations of various taxi policies. the effects on the fleet size and fare to the proportion of trips ended in the cbds for customer-search were also explained. potential taxi policies were investigated and discussed to achieve the taxi policy target on achieving a more even distribution of vacant taxis to all zones and cause a lower proportion of vacant taxi trips ended in the cbds in customer-search. the proposed model allows us developing simulationbased models and simulation-based optimization models for depicting and managing taxi flow on local streets in future research."
"nowadays, the competition between banks, coming from both banking and non-banking world, is becoming fiercer. hence, commercial banks need to continuously carry out business and management innovation to improve its customer service level and competition ability. information technology (it) plays a decisive role in the development of modern banks, as it can provide substantial support to the development of modern bank in the field of customer relationship management, asset liability management, performance and value management, risk management and so on. so it is needed to increase investment in the it. banks should not only invest in the it outsourcing to get the appropriate service project, but also improve their it departments by choosing a number of potential and technical employees."
"a two-stage taxi customer-search model is therefore proposed in this study to predict vacant taxi movements to address the above issues by integrating and formulating the zonal and local search problems of vacant taxi drivers. in fact, the integrated transportation modeling concept has been widely adopted for formulating multi-stage transportation problems to ensure that the obtained outcomes from different stages are consistent with each other. [cit] attempted to calibrate an integrated model for urban taxi services in a congested road network with elastic passenger demand. [cit] further extended the model for multiple user classes, multiple taxi modes, and customer hierarchical modal choice. their efforts of integrating different stages of transportation model into a single and simultaneous mathematical model gave inspirations to this study."
"in our implementation we constructed these sets on the fly while re-calculating the positions of the centroids according to (6) below, which is a modification of (3). i.e., when re-calculating v 2, we considered in (5) already re-calculated value for v 1 ."
"− mapping: the obtained fuzzy clusters are one-to-one identified with the c target categories. the classes are identified through a majority voting technique, performed within each of the clusters. in our case we have all annotated music samples but for that situation when we have maximum number of unlabeled data and much lower number of labelled data we can still carry out our method by taking all of those unlabeled and labelled data for clustering and determine the fuzzy classes of the clusters through the majority voting with the help of available labelled data taking part in the clustering step."
"where   is the coefficient associated with the rate of return for the search cycle. to ensure that the selected zone is either adjacent to the starting zone or the starting zone itself, the recursive equation (5) sets a criterion that if the selected zone is not the starting zone and not adjacent to the starting zone, then the probability that the trip will travel to the selected zone from the starting zone is zero. otherwise, the probability is determined based upon the mnl model with the alternatives comprising the starting zone and its adjacent zones."
"the iterative solution procedure in section 2.4 was applied to solve the two-stage taxi customer-search model. with an adjustment coefficient (  ) of 0.05 and an acceptable tolerance (  ) of 0.05, the proposed iterative solution procedure provides a convergent solution of a zonal vacant taxi distribution as shown in figure 4"
"is the adjustment factor for zone ji  in iteration k.   are applied to the probability of success because in such cases, the total number of assigned trips is obviously higher and smaller than those predicted, respectively."
"to form a cell-based network for each zone for the second stage local customer-search model, each zone was divided into many identical squares, in which the length of each square equals 200 m. there are imaginary bidirectional links connected between each pair of adjacent cells, representing taxi movements between these cells. the vacant taxi trajectories extracted from the gps data disclosed the cells and links involved during a customersearch process. in the case if these vacant trips were operating at high travel speeds and thus bypass the adjacent cells in our gps records, the search trajectories of the cells that were passed through were estimated by the interpolation method. cells and links with no recorded taxi activity were considered invalid and were excluded. the whole week (7 days a week, 24 hours a day) of gps trip data was extracted to develop a cell-based urban taxi operating road network, in which the 2-hour (07:30 to 09:30) data in each day of the week was used for the model calibration and examining the local taxi customer-search behavior during the morning peak period. fig. 3 . the cell-based taxi operating road network for the zones in hong kong island and kowloon peninsula figure 3 shows the cell-based urban taxi operating road network for the zones in hong kong island and kowloon peninsula. the shaded cells represent the valid cells in the developed network. the cells without taxi service activities are left blank and were excluded from further analysis. darker dots in the cells represent more trips involved in those cells. based on the darkness of cell distribution indicated in this figure, it is noticed that the sampled urban taxis mostly concentrated in the urban areas of hong kong island and kowloon peninsula in search of customers. this could happen because the urban taxis charge a higher fare and are less competitive to operate in rural areas than new territories and lantau taxis [cit] ) . therefore, the developed cell-based urban taxi operating road network mainly covers the urban areas."
"the proposed decision making method is applied to a recruitment of a bank in nanchang city of china who is preparing to expand its human resource of it department. in the recruitment, there are five candidates x 1, x 2, x 3, x 4 and x 5, applying for the position. for the fair, the interview panel consists of five dms: bank president d 1, two vice presidents d 2 and d 3, minister of hr department d 4 and minister of it department d 5 . as it department is a special department which involves both banking business and information technology, the advice of dm d 5 is as important as the other dms. therefore, we give the dms the same priority coefficient which will be used in the calculation of attribute weights. to evaluate every candidate more properly, there are seven attributes including age f 1, experience f 2, adaptability f 3, appearance and behavior f 4, comprehensive quality f 5, professional knowledge f 6, and communication skills f 7 ."
"impact of different feature combinations table i shows that we obtained better accuracy when we used long-time features than using short-time features. however, a much lower accuracy was obtained when we used only beat features. the highest accuracy was obtained when we used all three types of features: long-time, shorttime, and beat features. we have done the evaluation in two ways. in one experiment we performed tenfold cross-validation on each of the 120 and 45 classifiers mentioned in section 7.2 using all 1000 music files. the result and the corresponding confusion sets are given in section 8. using all three feature sets along with fuzzy membership vector as a feature, we obtained 97.10% accuracy."
"in another experiment we split our dataset into 60% training and 40% test data. using 60% training data we trained our 60 or 45 classifiers correspondingly, depending on the value of k, and tested them on the unseen test data. with this, we obtained 91.50% accuracy, which is probably explained by smaller size of the training data."
"we have proposed a method of music genre classification in a large music dataset using a two-stage classification methodology. the methodology consists in fuzzy clustering followed by disambiguation using a hard classifier. as features of musical data, we used the values obtained with the jaudio toolkit."
"this definition assumes that the pickup rate is the same for each zone. in that case, even though the pickup rate is not 100%, the relative passenger demand equals the relative actual passenger demand, because the actual passenger demand multiplied by the pickup rate gives the number of customers picked by taxis, and the pickup rate in each zone is cancelled out when we calculate the relative actual passenger demand."
"means there is no difference between x i and x j (x i ∼ x j ). as usual, u h satisfies the additive reciprocity property:"
"where m g is the total number of vacant trips started (or equivalently, the total number of occupied taxis dropping off their customers) in zone mi  . according to equation (7), the total number of predicted vacant trips ending in zone ji  equals the sum of the product of the total number of trips generated from each zone times the corresponding probability that the trips end in zone ji  ."
"preference ranking organization method for enrichment evaluaion (promethee) [cit] to solve madm problems through comparing each pair of alternatives for each attributes. the promethee method is a multi-criteria decision aid system that permits the building of an outranking between different alternatives. the information requested by promethee is particularly clear and easy to define for both decisionmakers and analysts. it consists in a preference function associated to each criterion as well as weights describing their relative importance. literature review reveals that existing research mainly focuses on extending linmap to solve heterogeneous magdm. however, there is no investigation on how to apply promethee for heterogeneous magdm. to overcome this shortage, we propose a new promethee-fuzzy linear programming (promethee-flpm) method for solving the heterogeneous magdm problems with incomplete weight information in which the information formats are represented as real numbers, intervals, ifss, tifns [cit] and trifns [cit] . the main motivations of this paper are outlined as follows:"
the first stage model is developed through an esl modeling approach. the study area was divided into several zones. each zone was connected to its adjacent zones which include both (1) the zones that can be reached directly from it via for example expressways or highways without passing the urban roads of other zones and (2) neighbor zones that are next to the current zone.
"the dash lines representing the variation in the number of vacant taxis at the cbds (from +20% to -20% of the current situation) are also presented in figure 7 . if the taxi policy target was to drop down the number of vacant taxis at the cbds by 10% (same as the previous numerical example), the fleet size should only be decreased by about 8% while imposing a hk$10 surcharge to the trips entering the cbds. moreover, if the fleet size were further reduced, the surcharge required could be smaller."
"the first step in our process is unsupervised: we cluster the music files into 10 categories, given that we consider 10 genres. on output, we define for each music file and each of the ten classes the membership value between 0 and 1 with which the given music file belong to the given class."
"the determination of the dms' weights is an essential topic. motivated by pang and liang [cit], zhu and xu [cit] defined an effect index to determine the dms' weights."
"test criterion 3: when a mcdm problem is decomposed into smaller problems and the same mcdm method is applied on smaller problems to rank alternatives, combined ranking of alternatives should be identical to the original ranking of un-decomposed problem."
"in order to verify the effectiveness of the proposed apso for tracking the occluded target, the 210th frame to 250th frame are selected in forward looking sonar image sequences. fig. 5 shows the tracking results of occluded target when frame is 215th, 225th, 235th, and 245th respectively. fig. 6 shows the center position error curves (210th frame to 250th frame) of occluded target to further compare the experimental results more clearly."
"results. figure 2 shows the number of correct queries and answers, the average time spent on each question and the number of participants who had a correct query for at least one question of each category. for example, in category \"visualization\", the first two questions had 20 correct answers and queries; the third question had 10 correct answers and 13 correct queries; all the 20 participants had a correct query for at least one question of the category; the average re- fig. 2 . average time and number of correct queries and answers for each question sponse times were respectively 43, 21, and 55 seconds. the difference between the number of correct queries and correct answers is explained by the fact that some subjects forgot to set the focus on the whole query after building the query."
"if a precedes exam 1 and b precedes exam 1 and there are no common topics that occur in both a and b, then exam 1 can be split into two exams, exam 1a and exam 1b such that a precedes exam 1a and b precedes exam 1b, and a and b are independent of exam 1b and exam 1a, respectively. see figure 5 . this transform must be applied after t1. currently enable applies both these semantic rules using graph transformations. there are additional meaningful transformations to be explored in the future work of this project, but we give them here to show the power of the approach."
"sus questionnaire. table 2 shows the answers to the sus questions, which are quite positive. the first noticeable thing is that, despite the relative complexity of the user interface, subjects do not find the system unnecessarily complex nor cumbersome to use. we think this is because the principles of qfs are very regular, i.e., they follow few rules with no exception. the second noticeable thing, which may be a consequence of the first, is that subjects felt confident using the system and found no inconsistency. finally, even if it is necessary for subjects to learn how to use the system, they thought that the system was easy to use, and that they would learn to use it very quickly. the results of the test demonstrate that they are right, even for features that were not presented in the tutorial (the inverse category)."
"with the growing amount of available resources in the semantic web (sw), it is a key issue to provide an easy and effective access to them, not only to specialists, but also to casual users. the challenge is not only to allow users to retrieve particular resources (e.g., flights), but to support them in the exploration of a knowledge base (e.g., which are the destinations? which are the most frequent flights? with which companies and at which price?). we call the first mode retrieval search, and, following marchionini [cit], the second mode exploratory search. exploratory search is often associated to faceted search [cit], but it is also at the core of logical information systems (lis) [cit], and dynamic taxonomies [cit] . exploratory search allows users to find information without a priori knowledge about either the data or its schema. faceted search works by suggesting restrictions, i.e., selectors for subsets of the current selection of items. restrictions are organized into facets, and only those that share items with the current selection are suggested. this has the advantage to provide guided navigation, and to prevent dead-ends, i.e., empty selections. therefore, faceted search is easy-to-use and safe: easy-to-use because users only have to choose among the suggested restrictions, and safe because, whatever the choice made by users, the resulting selection is not empty. the selections that can be reached by navigation correspond to queries that are generally limited to conjunctions of restrictions, possibly with negation and disjunction on values. this is far from the expressiveness of query languages for the semantic web, such as sparql 3 . there are semantic faceted search that extend the expressiveness of reachable queries, but still to a small fragment of sparql (e.g., slashfacet [cit], browserdf [cit], sor [cit], gfacet [cit] ). for instance, none of them allow for cycles in graph patterns, unions of complex graph patterns, or negations of complex graph patterns."
"not all items in canvas have due dates. in the sample cs0 course, 8 out of 49 learning items do not have due dates associated with them. these undated learning items include lecture notes, videos, and frequently asked questions. it is likely these items are informational materials that are most beneficial when preceding other items in the same unit or other items of the same topic. as topics extend across a larger time period than units, associating the undated learning items with other items in the unit is preferred. these learning items are dated two days before the first dated item in their unit."
"existing approaches to semantic faceted search often have additional limitations, which are sometimes hidden behind a lack of formalization. a same facet (a property path) cannot be used several times, which is fine for functional properties but not for relations such as \"child\": p(., f 1 ∩ f 2 ) is reachable but not p(., f 1 )∩p(., f 2 ) (e.g., browserdf, gfacet). a property whose domain and range are the same cannot be used as a facet (e.g., /facet), which includes all family and friend relationships for instance."
"this raw count of how many times a term occurs in a document can be more informative if it is weighted. the weighting approach used by enable is tf-idf. tf-idf starts with the tf and then multiplies it by the inverse of the document frequency. the document frequency (df) of a term is the count of how many of the documents in the corpus contain that specific term. if the df is high, the term is very common so the fact that it shows up in a document is not as significant as a term that is less common. when the df is low, the occurrence of the term in a document is more significant. by multiplying tf by the inverse document frequency (idf) the resulting value results from a weighting based on the relative frequency of the term in the corpus. the enable system computes tf-idf using log weighting of the tf count and log inverse frequency weighting on the document counts [cit] ."
"to verify the superiority of the proposed apso in searching ability, sphere function and griewank function are used to test position distribution of particles. sphere function is unimodal and contains only one global optimal solution. griewank function is multimodal and contains many local optimal solutions, but only one global optimal solution [cit] . fig. 1 shows position distribution of particles in the proposed apso and pso. the relevant parameters are as follows. the dimension of the solution space is 2, the population size is 30, and the maximum iterative times is 5."
"where fitness (i) is the optimal fitness value of the current particle i, and fitness (r) is the optimal fitness value of the random particle r. when the optimal fitness value of the current particle i is greater than or equal to that of the random particle r, the fitness value of fit 1 is fitness (i), and the value of fit 3 is set to 0, which means, the current particle's position is updated according to pbest and gbest. when the optimal fitness value of particle i is smaller than that of random particle r, the value of fit 1 is set to 0 and the value of fit 3 is set to fitness (r). that is, the current particle's position is updated according to rbest and gbest."
"when graphing the temporal relations the precedes relation is used. nodes represent learning items and edges are the relations between them. this is a directed graph with the arrow of the edge on the node with the later due date, expressing that one node precedes the other node in time."
the unit relations represent a grouping of the current organization. this is represented as a bipartite graph with one set of nodes representing the units and the other set of nodes representing the learning items. this is a directed graph with the arrow of the edge on the learning item nodes expressing that the unit includes the learning item.
"after the topic-based precedes elimination rule was first applied to the sample cs0 course, the instructor reviewed the resulting graph. several unexpected findings were encountered."
"from the tracking results in fig. 5 and fig. 6, the pf has serious underwater target tracking error after the 215th frame, and the algorithm is ineffective. the pso-pf and pso also have serious tracking errors when the target is occluded at the 230th frame. although adso and the proposed apso can track the position of the underwater target, the proposed apso has better tracking accuracy and stability."
"the inertia weight w is a very important parameter, which is used to balance between exploration ability and exploitation ability of particles [cit] . when the value of w is large, the particle has strong exploration ability, while small w means the particle has good exploitation ability. in the early search stage of particle swarm, larger w can be used to avoid the particles falling into the local optimum. while in the late stage of search, smaller w can be used to find the optimal solution. the premature convergence problem in pso means that the particles cannot find the global optimum and fall into the local optimum. in order to solve the premature convergence problem in pso, an adaptive inertia weight is proposed."
"in order to demonstrate the effectiveness of the proposed apso for tracking the normal target, the 1st frame to 60th frame are selected in forward looking sonar image sequences. fig. 3 shows the tracking results of the normal target when frame is 15th, 30th, 50th, and 60th respectively."
"by itself, the temporal relation seems trivial, and yet it is the predominant relation presented to students. an educator who has designed and implemented a course is aware of other relations between the learning items such as how they are grouped together to create a unit of learning, how they are related by a single topic or a group of topics, and prerequisite recommendations. although the educator may consider these other relations more significant, the learning management systems currently available use the temporal relation as the dominant organizational aspect when presenting learning materials. even when the module tool is used in canvas to group learning items together into units, the student view presents learning items in a linear format based solely on temporal relations in the assignments page, the gradebook, and the syllabus."
"to identify the topical relations the text of each learning item is gathered. canvas provides a title and a text description of each learning item. these become the basis of the text. this text is analyzed to see if there is a link to a file. canvas has a specific way of referencing files that have been uploaded making it possible to use text parsing and regular expressions to identify these references. once a filename is found, the file extension is considered. currently enable adds .txt and .pdf files to the text description. pdf files are converted to text before being added. canvas has a category of items identified as quizzes. these contain questions in addition to the text description. for these types of learning items, the questions are added to the text description."
"as can be seen from fig. 1, the solution scope is relatively larger in the proposed apso, and the proposed apso more easily obtains the global optimum. therefore, compared with pso, the proposed apso can increase population diversity and improve searching ability."
"to graph the topical relations, a bipartite graph is used with one set of nodes representing the topics and the other set of nodes representing the learning items. this is a directed graph with the arrow of the edge on the learning item nodes expressing that the topic occurs in the learning item. for readability, these edges are labeled oi."
another way to reduce the cognitive load for an exam is to split an exam temporally. this leads to the transform shown in figure 7 .
"in this section, we generalize in a natural way the set of queries compared to section 2. this defines a query language, which we call lisql (lis query language). we then define a set of query transformations so that every lisql query can be reached in a finite sequence of such transformations. this is in contrast with previous contributions in faceted search that introduce new selection transformations, and leave the query language implicit. we think that making the language of reachable queries explicit is important for reasoning on and comparing different faceted search systems. in section 3.3, we give a translation from lisql to sparql, the reference query language of the semantic web. this provides both a way to compute the answers of queries with existing tools, and a way to evaluate the level of expressivity achieved by lisql."
"the definition of lisql allows for the arbitrary combination of intersection, union, complement, and crossings. in order to further improve the expressiveness of lisql from tree patterns to graph patterns, we add variables (e.g., ?x) as an additional construct. variables serve as co-references between distant parts of the query, and allows for the expression of cycles. for example, the query that selects people who are an employee of their own father can be expressed as a person and father : ?x and employee of ?x, or alternately as a person and ?x and employee of father of ?x. the semantics of queries with variables is given with the translation to sparql in section 3.3, because it cannot be defined like in the table of definition 1."
"the key notion we introduce to reconcile this finite vocabulary, and the reachability of arbitrary lisql queries is the notion of focus in a query."
"many types of relations exist between the learning items, but here we focus on three basic ones: temporal, topical, and unit coherence. these describe the chronological order, similarity of topics, and presentation organization of the learning items, respectively."
"when combined with topical and unit relations the temporal relations add some information. for example, if there are two assignments that cover the same topic and one precedes the other it is likely that there is a non-commutative relation between the two learning items and it is important that the first is completed before the other."
"currently enable produces eight different course maps based on the relations between learning items. each of these graphs can be rearranged by the instructor to produce many organizations. the course maps have been designed to minimize the congestion of learning items and relations. the system does not, however, guarantee a minimum number of edge crossings. there is congestion of undated learning items as they are all positioned temporally at the beginning of a unit. this is a problem in one of the 13 units in the sample course. another congested area is produced when exams are split. the due date and unit assignments are the same for each of the split exams. this locates them close together. these congested areas are easy to resolve by dragging the nodes to new locations. when the nodes are moved the connecting edges automatically move with them keeping the visual representation of relations intact."
"we have developed an automated system that constructs an initial course organization graph based on information provided by canvas, a standard learning management system (lms). a variety of types of material are represented in the nodes of the graph and initially only their chronology is known. a detailed analysis of the materials based on the text contained within each learning item allows a more informed representation which captures the topic relations among the items. a set of graph transformations is then defined which convert the (basically) linear structure of the course to a graph structure which makes evident the dependencies and independencies of the learning items. a specific test case, cs0, was transformed in this way to demonstrate the power of the method."
"a more general query language, lisql, can be obtained simply by merging the syntactic categories of features and queries in the grammar of section 2, so that every query can be used in place of a feature."
the graph in figure 11 is clustered by units. the similarity between the graph arranged by topic and this one indicates that the units in the original organization grouped learning items into units by topic. order of the units is not restricted. there are precedes relations between the the five units in center of the graph. in the first and second grouping there are edges going in both directions.
"score (on a 0-4 scale) i think that i would like to use this system frequently 2.8 agree i found the system unnecessarily complex 0.8 strongly disagree i thought the system was easy to use 2.6 agree i think that i would need the support of a technical person to be able to use this system 1.5 disagree i found the various functions in this system were well integrated 2.9 agree i thought there was too much inconsistency in this system 0.6 strongly disagree i would imagine that most people would learn to use this system very quickly 2.5 agree i found the system very cumbersome to use 1.0 disagree i felt very confident using the system 2.8 agree i needed to learn a lot of things before i could get going with this system 1.7 neutral to solve the inverse questions with a reasonable success rate, and a decreasing response time."
"through the above verification and comparative experimental analysis, the proposed apso algorithm has better tracking accuracy and faster tracking speed in forward looking sonar image sequences, and it still has effectiveness and adaptability for the occluded target, the target with large contrast changes, the weak and small target, and the target affected by noise to some extent."
"in the following, when it is necessary to refer to a focus in a query, the corresponding subquery is underlined with the focus name as a subscript, like in mother of ? φ . foci are used in qfs to specify on which subquery a query transformation should be applied. for example, the query (f 1 and f 2 ) or (f 3 and f 4 ) can be reached from the query (f 1 and f 2 ) or f 3 by applying the intersection with restriction f 4 to the subquery f 3, instead of to the whole query. similarly, the query p 1 : (f 1 and f 2 ) and p 2 : (f 3 and f 4 ) can be reached by applying the intersection with restriction f 4 to the subquery f 3 . this removes the problem of unreachable selections in set-based faceted search presented in section 2. moreover, this removes the need for a strategy in the ordering of navigation steps. for example, the query a woman and mother of name : \"john\" can be reached by first selecting a woman, then by selecting mother of ? φ, then by inserting name : \"john\" at the focus φ."
"information about the existing course is provided by each step in this process. the enable system gathers existing information from what is available about the course in the lms and represents it in a visual way. this sheds light on what students currently have available through their access to the learning materials in the lms. the most significant finding was how entrenched the precedes relation is in the presentation of course material. this relation often adds little meaning to how learning items are related and yet it is the predominant organization strategy used when displaying information to students. when comparing the visual representation of that organization, see figure 1 (upper) to the alternative organizations produced by enable, see figures 10 and 11, it is clear that there is significant room for improvement in how the educational community presents learning material to students. although this first phase was designed to inform instructors about the many organization options available when making changes, the feeling of the authors is that the effort to develop a graphical, non-linear representation of a course could have significant impact on how the students perceive and interact with course materials."
this illustrates one of the many benefits of gaining another perspective when considering changes to current courses. this process of identifying correlations between topics provided new insights into possible changes to the topic lists. these insights were not recognized when the original topic lists were made. this process led to the reduction of topics from the original fifteen to the following ten:
"in an effort to meet the changing landscape of education many departments and universities are offering more online courses -a move that is likely to impact every department in some way [cit] . this will require more instructors create online courses. other innovations in instructional strategies are also widely impacting engineering educators [cit] including peer instruction, flipped classrooms, problem-based learning, just-in-time teaching, and a variety of active learning strategies. implementing any of these strategies requires changes to existing courses. sometimes an educator is so familiar with the current course organization that it becomes a stumbling block for visualizing alternative options."
"methodology. the subjects consisted of 20 graduate students in computer science. they had prior knowledge of relational databases but neither of sewelis, nor of faceted search, nor of semantic web. none was familiar with the dataset used in the evaluation. the evaluation was conducted in three phases. first, the subjects learned how to use sewelis through a 20min tutorial, and had 10 more minutes for free use and questions. second, subjects were asked to answer a set of questions, using sewelis. we recorded their answers, the queries they built, and the time they spent on each question. finally, we got feedback from subjects through a sus questionnaire and open questions [cit] . the test was composed of 18 questions, with smoothly increasing difficulty. table 1 groups the questions in 7 categories: the first 2 categories are covered by standard faceted search, while the 5 other categories are not in general. for category visualization, the exploration of the facet hierarchy was sufficient. in category selection, we asked to count or list items that have a particular feature. in category path, subjects had to follow a path of properties. category disjunction required the use of unions. category negation required the use of exclusions. category inverse required the crossing of the inverse of properties. category cycle required the use of co-reference variables (naming and reference navigation links)."
"the first 2 categories corresponding to standard faceted search, visualization and selection, had a high success rate (between 94 and 100) except for the third question. the most likely explanation for the latter is that the previous question was so simple (a man) that subjects forgot to reset the query between the questions 2 and 3. all questions of the first two categories were answered in less than 1 minute and 43 seconds on average. those results indicate that the more complex user interface of qfs does not entail a loss of usability compared to standard faceted search for the same tasks."
"during the course of this work, we determined that even though the precedes relations have been restricted to those that have common topic relations, they still express limited information. we believe that a more informative relation is the prerequisite relation that expresses a recommendation that one learning item be completed before another learning item. the precedes relation limits the connections between learning items and does not allow flexibility in ordering. it is easy to identify cases when this representation is too limited to express how the learning items are actually related. for example, there may be several learning items that are designed to prepare a student to complete a particular homework assignment such as a lecture, a class activity, a video, and a reading assignment. using precedes relations, a graphical representation would look similar to that shown in figure 12 . representing it this way indicates a specific ordering between the learning items when in fact this ordering is not required."
"we now discuss the translations of lisql queries compared to sparql in general. they have only one variable in the select clause because of the nature of faceted search, i.e., navigation from set to set. from sparql 1.0, lisql misses the optional graph pattern, and the named graph pattern. optional graph patterns are mostly useful when there are several variables in the select clause. lisql has the not exists construct of sparql 1.1. if we look at the graph patterns generated for intersection and union, the two subpatterns necessarily share at least one variable, x. this is a restriction compared to sparql, but one that makes little difference in practice as disconnected graph patterns are hardly useful in practice."
"the first time the topic-based exam splitting rule was applied there were fewer exam splits than expected. upon closer review it was discovered that the exam asked questions about a topic without using a topic word explicitly. this seemed pedagogically sound. for example, one question about computer science history was \"why was the invention of the integrated circuit important?\" although this question does not use the term history, it is clearly assessing the student's familiarity with the computer science history covered in the course. these missing meaningful relations can be included by adding text that includes the missing topic words to the description using enable's file upload tool. this adds the text to the enable system without altering the exam itself."
"using these lists of topic words, a term frequency vector is created for each learning item document. term frequency (tf) is a count of how many times a term occurs in the learning item document [cit] . the document in this case is the description of the learning item. this description includes any text available in canvas or uploaded by the instructor. the frequency count of terms found in a topic list are combined to produce a single tf count for each topic."
"the syntax and semantics of the lisql constructs is defined in the following table, where r is a resource, c is a class, p is a property, s 0 is the set of all items, and q 1, q 2 are lisql queries s.t."
"we have introduced query-based faceted search (qfs) as a search paradigm for semantic web knowledge bases, in particular rdf datasets. it combines most of the expressiveness of the sparql query language, and the benefits of exploratory search and faceted search. the user interface of qfs includes the user interface of other faceted search systems, and can be used as such. it adds a query box to tell users where they are in their search, and to allow them to change the focus. it also adds a few controls for applying some query transformations such as the insertion of disjunction, negation, and variables."
"considering the growing requirements of underwater target tracking, the apso was proposed to track the underwater target in real forward looking sonar image sequences. specifically, adaptive inertia weight and velocity update strategy were applied to solve the loss of particle diversity and avoid particles falling into the local optimum. for occluded target, a based-adso update strategy was proposed to update the particle's position according to the level of occlusion, which can quickly and effectively relocate the target. the experimental results demonstrate that the proposed apso can accurately complete underwater target tracking. compared with other algorithms, the proposed apso has higher tracking accuracy and faster tracking speed, and it has a certain effectiveness and adaptability for underwater target under different conditions. therefore, the proposed method can provide better underwater target tracking and has important theoretical and practical value."
"for graph transformation, enable uses agg, a development environment for attributed graph transformation [cit] . it is based on an algebraic approach to graph transformations. the implementation of this approach closely follows the formal, theoretical foundation of algebraic graph transformation and so provides validation support [cit] and sound behavior concerning graph transformation [cit] . agg has non-deterministic rule and match selection but provides control of this with rule layers."
"once the t1 and t2 transforms have been applied, many of the original organizational limitations have been removed. this opens the way for alternative arrangements of the learning items."
units are a set of learning items that are grouped together. unit relations come directly from the modules tool in canvas. this tool allows an instructor to group learning items into units. many different groupings are used. some instructors group the material based on a textbook such as a unit for each chapter. others use it to organize temporally such as one unit for each week in the course. another approach is to organize by specific topic coverage. current grouping in these modules reflects groupings that are in some way meaningful to the instructor. the unit grouping of learning items is used as the y-value in figure 1 (upper and middle) and figure 3 . this visually shows how learning items are related by unit.
"as can be seen from table 3, the average center position error of the target tracking using the proposed apso is 4.1754, which is lower than the adso, pso-pf, pf, and pso (6.3591, 15.4991, 56.0263, 24.0126), and has better tracking accuracy. moreover, when the underwater target is occluded, center position error of the proposed apso is still smaller than other algorithms. the proposed apso has a certain adaptability. compared with adso and pso-pf, the proposed apso can reduce the error rate of 34.34% and 73.06% respectively. therefore, the proposed apso has a certain effectiveness for underwater target tracking in forward looking sonar image sequences. table 4 shows that the average processing time is 0.1831 in the proposed apso for each frame, which is lower than the average tracking time of adso, pso-pf, pso by 32.16%, 66.45%, 44.53% respectively. according to the analysis of the tracking results in fig. 3 to fig. 12, although the pf has the lowest average running time, it cannot effectively track the underwater target. therefore, the proposed apso algorithm has better tracking accuracy and faster tracking speed."
the graph in figure 10 shows the learning items organized by topic. this arrangement separates the learning items in several distinct topic groups. the large group in the middle reflects the interrelated nature of several topics. this provides a visualization of how topics are related and how they might be rearranged. there is no visualization of includes relations.
"is the center point of the region represented by the particle, w i, h i are the width and height. according to the target region information and particle information. the features of the target and particle region can be obtained. the particle's fitness can be calculated according to the fitness function, then target is tracked by eq. (10) and eq. (11) ."
"a new update strategy is proposed to update the particle's position, which can be used to solve the problem of poor tracking performance in case of occlusion. the algorithm regenerates new random particles in search space according to the level of occlusion. when the target is occluded, new particles are generated to explore the target position, and the target position can be quickly relocated. in this way, the diversity of particle in the swarm can be guaranteed. the new update strategy is:"
"enable uses a .txt file to store a series of topic lists. these lists contain topic words, word groups, and variations. each line in the file represents a single topic. individual topic variations are separated by a comma. the original list of topics for the sample cs0 course includes:"
"as seen from in fig. 4, the pf has the phenomenon of tracking error, and the pso-pf can solve the tracking error problem of pf, but the tracking accuracy is not high and unstable, the proposed apso, adso, and pso can get a good tracking result. especially, the proposed apso has better tracking accuracy and stability."
"the system is able to exactly reproduce the temporal precedes relations of all dated learning items. for those learning items that do not have a due date, a date is used that is related to the earliest date in the unit in which the undated item is included. it is also possible to extract and identify the unit or includes relations in all cases where the modules tool in canvas is used. no testing was done with courses that did not use the modules tool. it is anticipated that input from the instructor would be required to identify unit relations if they are not identified in the lms. topical occurs in relations are produced from the text. in the example course the automated assignment of occurs in relations was correct in 472 of the 490 topic to learning item combinations. in eight cases none of the topic word variations was found in the learning item but they were in fact related to the topic. for nine combinations a topic word was encountered in the description but the learning item was not related to the topic. in one case the topic word was used to instruct the students to not use the topic on this specific assignment. these discrepancies were corrected with input from the instructor. involving the instructor in the analysis process is one of the strengths of the enable system. it not only improves the validity of the results but also improves the possibility of increasing the instructor's potential for understanding the possibilities for change."
"from the formal definition of navigation graphs, we can now formally state safeness and completeness theorems. those theorems have subtle conditions w.r.t. focus change, and the main purpose of this section is to discuss them. for reasons of space, lemmas and proofs have been removed, but they are fully available in a research report [cit] (the presentation is slightly different but equivalent)."
"there was one case where the review of the exam exposed the possibility of adding a word to the topic list. the word occupation was used in the exam that covered careers in computer science. this word was also used in other learning items about the topic. it was determined that adding this word to the topic list would add clarity. adding the term to the topic list resolved this missing relation. figure 9 shows the result of the application of the transforms t1 and t2. the revised graph affords much greater leeway in the organization, presentation, and order of selection of material for the instructor and the student."
"the contribution of this paper, query-based faceted search (qfs), is to define a semantic search that is (1) easy to use, (2) safe, and (3) expressive. easeof-use and safeness are retained from existing faceted search systems by keeping their general principles, as well as the visual aspect of their interface. expressiveness is obtained by representing the current selection by a query rather than by a set of items, and by representing navigation links by query transformations rather than by set operations (e.g., intersection, crossing). in this way, the expressiveness of faceted search is determined by the expressiveness of the query language, rather than by the combinatorics of user interface controls. in this paper, the query language, named lisql, generalizes existing semantic faceted search systems, and covers most features of sparql. the use of queries for representing selections in faceted search has other benefits than navigation expressiveness. the current query is an intensional description of the current selection that complements its extensional description (list of items). it informs users in a precise and concise way about their exact position in the navigation space. it can easily be copied and pasted, stored and retrieved later. finally, it allows expert users to modify the query by hand at any stage of the navigation process, without loosing the ability to proceed by navigation."
"where c 1, c 2, and c 3 are learning factors. three learning factors in the proposed apso are adaptively adjusted according to three fitness values in this paper. the three learning factors can be defined as follows:"
"the target tracking with pso is considered as an efficient algorithm, not only can it archive relatively high tracking accuracy but also it requires less computation. however, when target is occluded, the particles will fall into a local optimum [cit] . because all particles move toward to the local optimum in pso, it is not easy for the particles to jump out of the local optimum and cannot track the target correctly. to overcome this drawback, an apso algorithm is proposed to track the underwater target in forward looking sonar image sequences in this paper."
"with the deep understanding of the ocean, the strategic position of the ocean has become more and more important. sonar as a kind of underwater detection equipment, has important significance for the exploration, research and development of the sea in the human beings [cit] . forward looking sonar has been developed rapidly in recent years, it is mainly used in underwater target location, underwater target tracking and obstacle avoidance [cit] . in particularly, underwater target tracking in forward looking sonar image sequences has gradually attracted more and more attention."
the unit relations are loosely expressed by using the unit value to compute the vertical location of the nodes in the starting graph. this provides a visual representation of how the learning items are grouped into units but does not include any edges that connect items in a unit. figure 1 (middle) shows the graph structure produced for the learning materials in sample cs0 course when these relations are combined.
"when anticipating change it is valuable to see how existing learning materials can be organized and used in new ways. the purpose of the enable project is to provide assistance in making informed changes. enable is not an acronym, rather a name that reflects the purpose to enable the implementation of quality educational strategies. the two major contributions of the current enable system are that it:"
"this section shows numerical examples to validate the generality and effectiveness of the proposed apso for underwater target tracking in real forward looking sonar image sequences. all experiments use the same features and the same fitness function in this paper. based on the reference papers and our past experience, the relevant parameters are as follows:"
"a pearson correlation was done between the tf-idf values of the topics. for each correlation that was greater than 0.8, the topics were considered for combining. in the sample cs0 course there was a correlation between the html, attribute, element, and tag topic lists. combining these was obvious once the correlation pointed them out. these are all parts of the html language. the other topics that were highly correlated were javascript, functions, and textboxes. although functions is a topic that exists outside of javascript, in the context of this course, functions are only discussed or used in javascript. this correlation made the instructor aware that their broader view of the computer science curriculum was reflected in this separation of topics and would best be adapted to fit the content of this specific course. this provided the instructor a fresh perspective informed by feedback from enable."
"1) t1: topic-based precedes elimination rule: the major restraint is the precedes relation. it restricts any change in the order of learning items. however, many of the precedes relations are not necessary and can be removed without changing the necessary relations. the first step is to remove unnecessary precedes relations. as discussed earlier, precedes relations by themselves have little meaning. the fact that one learning item comes before another provides only limited information. now that the temporal and topical relations have been combined into a single graph the system can identify precedes relations that have no topical connections and can be removed."
"to further demonstrate the advantages of the proposed apso, table 3 and table 4 respectively show the average tracking center position error and the average tracking time per frame of apso, adso, pso-pf, pf, and pso."
"the syntactic form of restrictions are features. the syntactic form of selections are queries whose answers are sets of items, i.e., subsets of s 0 . the above tables implicitly define a grammar for features and queries:"
"this process of analysis and transformation provides insight for the instructor about a variety of ways the course materials can be organized. as the instructor sees existing course materials presented in a variety of ways their perception of the possibilities for making innovative changes like using peer instruction, flipping the classroom, adding blended learning, implementing more active learning, etc. is increased. this increase in possibilities facilitates change."
"when the focus is in the scope of an union, only the alternative that contains the focus is used in the flipped query. this is necessary to have the correct set of restrictions at that focus, and this is also useful to access the different subselections that compose an union. for example, in the query a man and (firstname : \"john\" φ or lastname : \"john\"), the focus φ allows to know the set of men whose firstname is john without forgetting the second alternative. when the focus is in the scope of a complement, this complement is ignored in the flipped query. this is useful to access the subselection to be excluded. for example, in the query a man and not father : ? φ, the focus φ allows to know the set of men who have a father, i.e., those who are to be excluded from the selection of men."
"dataset. the datasets were chosen so that subjects had some familiarity with the concepts, but not with the individuals. we found genealogical datasets about former us presidents, and converted them from ged to rdf. we used the genealogy of benjamin franklin for the training, and the genealogy of george washington for the test. the latter describes 79 persons by their birth and/or death events, which are themselves described by their year and place, by their firstname, lastname, and sex, and by their relationships (father, mother, child, spouse) to other persons. places are linked by a transitive part-of relationship, allowing for the display of place hierarchies in sewelis."
"from the tracking results in fig. 11 and fig. 12, when the target is not affected by noise, the proposed apso has better stability and tracking accuracy. the target is affected by serious noise from 1120th frame, the proposed apso also has a certain tracking error during the tracking process, but it still has better tracking accuracy."
"to compare the experimental results more clearly, the center position error curves (1st frame to 60th frame) of normal volume 6, 2018 target are shown in fig. 4 . the center position error is the euclidean distance between the real position of the underwater target and the position of the underwater target tracked by the algorithm, which is used to determine the accuracy of the tracking algorithm."
"from the tracking results in fig. 7 and fig. 8, both pf and pso have tracking errors during the tracking process. by comparing the proposed apso, adso, pso-pf, the proposed apso has better stability and tracking accuracy in the case of the target with large contrast changes, to verify the effectiveness of the proposed apso for tracking weak and small target, the 730th frame to 770th frame are selected in forward looking sonar image sequences. fig. 9 shows the tracking results of weak and small target when frame is 740th, 750th, 760th, and 770th respectively. it can be seen from fig. 9 and fig. 10, the pf and pso cannot track the target effectively. meanwhile, it has serious tracking errors in the tracking process. the adso, pso-pf, and the proposed apso can effectively track the weak and small underwater target."
"some useful selections that can be defined in terms of set operations are not reachable by set-based faceted search. for example, the following kinds of selections are not reachable: unions of complex selections. e.g., (r 1 ∩ r 2 ) ∪ (r 3 ∩ r 4 ); or intersection of crossings from complex selections, e.g., p 1 (.,"
"this section reports on the evaluation of qfs in terms of usability 5 . we have measured the ability of users to answer questions of various complexities, as well as their response times. results are strongly positive and demonstrate that qfs offers expressiveness and ease-of-use at the same time."
"hu moment invariant feature [cit] have the features of invariance such as translation, rotation and scaling. according to the characteristics of forward looking sonar image, the hu invariant moment feature is more suitable for feature extraction. in each iteration, feature extraction is performed for each particle, and the similarity measure of the target feature is used to calculate the fitness value of the particle. in this paper, the correlation coefficient method is used as the fitness function. it is expressed as:"
"as depicted in fig. 3, the proposed apso and other algorithms can track the target position normally before 50 frames. from the tracking results between 50th and 60th frame, it can be seen that pf algorithm has a serious tracking error during the tracking process."
"we define a restraint as an unnecessary constraint between two items. thus, restraints are removed in order to open up more possibilities for the relations between learning items. when removing restraints it is important to maintain the integrity of the course representation."
"note that a selection s 1 ∩ p(., s 2 ) cannot in general be obtained by first navigating to s 1, then crossing forwards p, navigating to s 2, and finally crossing backwards p, because it is not equivalent to p(., p(s 1, .) ∩ s 2 ) unless p is inverse functional. therefore, not all combinations of intersection, union, and crossing are reachable, which is counter-intuitive and limiting for end users."
"we here propose a (naive) translation of lisql queries to sparql queries. it involves the introduction of variables that are implicit in lisql queries. as this translation applies to lisql queries with co-reference variables, it becomes possible to compute their set of items."
"querying languages for the semantic web are quite expressive but are difficult to use, even for specialists. users are asked to fill an empty field (problem of the writer's block), and nothing prevents them to write a query that has no answer (dead-end). even if users have a perfect knowledge of the syntax and semantics of the query language, they may be ignorant about the data schema, i.e., the ontology. if they also master the ontology or if they use a graphical query editor (e.g., semanticcrystal [cit], scribo graphical editor 4 ) or an auto-completion system (e.g., ginseng [cit] ) or keyword query translation (e.g., hermes [cit] ), the query will be syntactically correct and semantically consistent w.r.t. the ontology but it can still produce no answer."
"in the future we hope to transform learning outcomes by (1) facilitating deep student learning in science and engineering by providing the student feedback resulting from behavior models based on monitoring paths taken through the online course graph and linking that to performance in the class; and (2) providing effective tools for the instructor to monitor the effectiveness of the course material and its organization. the innovative use of a bayesian inference network, a technology currently applied in many intelligent systems, will be developed and applied in a real-world learning environment to create a predictive computational model for individual learners and educators. by identifying operational student learning processes it may be possible to detect how knowledge gaps are a consequence of less successful learning strategies and tactics. developing learning strategies can be challenging, thus an effective learning environment to support this must be designed and developed. as a next step we propose to integrate a bayesian inference network within the enable system to provide synthesized data about learners' activities, behaviors, and performance."
"if a precedes b and b precedes c and there are no common topics that occur in both a and b, the precedes relation from a to b can be removed. when removing this relation it is important to keep the relation that a precedes c and b precedes c. note, however, that the net number of precedes relations is reduced by 1 as there was an implied p(a,c) before the application of t1."
"where fit 1 is the optimal fitness value of the current particle i, fit 2 is the global optimal fitness value, and fit 3 is the optimal fitness value of the random particle r. for each particle in the particle swarm, it adjusts learning factors adaptively according to fit 1, fit 2, and fit 3, when the current particle's individual optimal fitness value is larger, the value of c 1 is larger, the particle learn more from pbest. when the global optimal fitness value is larger, the value of c 2 is larger, the particle learn more from gbest. when the random particle's individual optimal fitness value is larger, the value of c 3 is larger, the particle learn more from rbest. the values of fit 1, fit 2, and fit 3 are given by:"
"as depicted in fig. 2, whether it is unimodal function or multimodal function, the fitness values of the proposed apso are always smaller than the pso in the iterative process. therefore, the global optimum of the proposed apso is always better than the pso. the optimization results further show that the searching ability of the proposed apso is better than the pso."
"2) t2: topic-based exam splitting rule: one result of building course organization based on temporal relations is illustrated by exams. commonly, an exam is written to assess the material that has been covered over a specific period of time. for example, since the last exam or since the beginning of the semester. this time-based connection is not required for assessment. therefore it is possible to divide the material assessed in an exam by topic. separating the temporal grouping inherent in exams provides additional possibilities for change. the split exams rule is applied after the remove precedes rule has been applied. enforcing this rule application order prevents any exams being split when preceding learning items are topically related. agg allows the user to specify which rule layer a specific rule is in. it enforces rule ordering by applying all the rules in one layer before applying rules in the next layer. this, then, is another example of a restraint: when exams tie learning items together that are not related in any other way."
"similarly, in order to further verify the effectiveness of the proposed apso for tracking the target with large contrast changes, the 355th frame to 435th frame of forward looking sonar image sequences are selected. fig. 7 shows the tracking results of target with large contrast changes when frame is 370th, 380th, 400th, and 425th respectively. fig. 8 shows the center position error curves (355th frame to 435th frame) of target with large contrast changes."
"computing the set of restrictions is equivalent to set-based faceted search, i.e., amounts to compute set intersections between the set of items and the precomputed set of items of features. the same datastructures and algorithms can therefore be used. as features are lisql queries, their set of items can be computed like for queries, possibly with optimizations given features are simple queries. finally, determining the set of navigation links requires little additional computation. a navigation link is available for each focus of the query (focus change), and each restriction (intersection). three navigation links for exclusion, union, and naming are always available. only for reference navigation links it is necessary, for each variable in the query, to compute the set of items of the target navigation place, in order to check it is not empty. this additional cost is limited as the number of variables in a lisql query is very small in practice, and is bounded by the number of foci of the query."
"in the target tracking process, the balance between exploration ability and exploitation ability is an important condition to avoid particle falling into the local optimum. in pso, as the number of particle iterations increases, the inertia weight should be reduced continuously, so that pso has a strong exploration ability in the early iteration and has strong exploitation ability in the late iteration. for each particle in particle swarm, when the particle's fitness value is small, the particle should have strong exploration ability for searching a global optimal solution. when the particle's fitness value is large, the particle should have strong exploitation ability to keep particles close to the global optimal solution. therefore, an adaptive inertia weight is proposed according to iterative times and fitness value of particles. it is expressed as follows:"
"5) t5: change of topic detection transform: in a standard classroom setting, a sequence of material on one subject will eventually give way to a change of topic and a new set of materials. we believe that this can be detected in the initial course graph due to the overlap pattern of related topics among the learning items. for example, a learning item that sits at the end of a sequence of topic-related items, and at the start of a distinct topic-related set of items, is most likely a transition item. this leads to the transform shown in figure 8 ."
"because enable identifies alternative course structures that maintain the relations between learning items it becomes necessary to transform the graph while still keeping the meaningful relations intact. graph grammars and graph transformation systems provide a means for doing this. there is much research and many successful applications based on the research in this area [cit] . one of the application areas of graph transformation systems is model transformations. this area of model transformation has become important to the field of software engineering [cit] . the models used in software engineering have enough similarities to the graphical representation of learning materials to allow model transformation as the graph transformation technique used by enable. these similarities include typed nodes, node attributes, and edges that represent different types of relations."
"there were five learning items that had no topic relations. upon examination, two of the items were truly not related to any topic, an assignment in which students were to submit which team they wanted to be on, and the teacher evaluation. both items were left unchanged with no topical relations."
"sequences of query transformations are analogous to the use of graphical query editors, but the key difference is that answers and restrictions are returned at each step, providing feedback, understanding-at-a-glance, no dead-end, and all benefits of exploratory search. despite the syntax-based definition of navigation steps, those have a clear semantic counterpart. intersection is the same as in standard faceted search, only making it available on the different entities involved in the current query. in the above example, intersection is alternately applied to the person, his birth, his birth's place, his father, etc. the set of relevant restrictions is obviously different at different foci. the union transformation introduces an alternative to some subquery (e.g., an alternative birth's year). the exclusion transformation introduces a set of exceptions to the subquery (e.g., excluding some father's birth's place). in section 4, we precisely define which query transformations are suggested at each navigation step, and we prove that the resulting navigation graph is safe (no dead-end), and complete (every \"safe\" query is reachable)."
"2) learning items with meaningless topical relations: there were six topical relations that connected learning items to topics mistakenly. in five of these learning items, the topic words occurred but were being used in a more general way. for example, one of the topics is content. this is specifically related to selecting content when creating a web site. however, the word content was used in its more general way in three of the learning items. in the other case the instructions included a restriction to not use javascript which was a future topic. these relations were manually removed."
"each navigation step from a navigation place (q, φ) requires the computation of the set of items items(q, φ), the set of restrictions restr (q, φ), and the set of navigation links as specified in definition 8. in many cases, the set of items can be obtained efficiently from the previous set of items, and the last navigation link. if the last navigation link was an intersection, lemma 1 shows that the set of items is the result of the intersection that is performed during the computation of restrictions, like in standard faceted search. for an exclusion or a naming, the set of items is unchanged. for a reference, the set of items was already computed at the previous step. otherwise, for an union or a focus change, the set of items is computed with a lisql query engines, possibly reusing existing query engines for the semantic web (see section 3.3)."
"once the initial course graph is available, it becomes possible to begin a conversion process from a linear (chronological) style class organization to a more non-deterministic, multi-path organization of the learning items more suitable to online delivery. it is now necessary to determine the types of desirable transforms and their meanings. we begin with the consideration of how to eliminate unnecessary precedes relations."
"in these regards, to obtain more accurate tracking results and faster tracking speed, this paper presents an apso to track the underwater target in real forward looking sonar image sequences. using an adaptive inertia weight firstly to balance the exploration and exploitation abilities according to iterative times and fitness value of particles. then, a velocity update strategy is proposed to update the velocity of particle using a random particle selected from particle swarm. on this basis, one of the key advantages of this paper is a new update strategy is used to update the particle's position. compared with the results of adso, pso-pf, pf and pso, the proposed apso can achieve better tracking accuracy and faster tracking speed for underwater occluded target. moreover, it still has a certain effectiveness and adaptability for target with large contrast changes, weak and small target, and target affected by noise. therefore, the proposed apso has important theoretical and practical value."
"46838 volume 6, 2018 figure 6. position errors of image sequences (210th frame to 250th frame). where h 1 (k) is the target moment invariant feature, h 2 (k) is the moment invariant feature of the particle, n is the size of the feature vector."
"in order to further solve the problem of particle falling into the local optimum, a random particle selected from particle swarm is used to update the velocity of the particle. the position of the particle at the next moment is determined by the particle's current position, pbest and gbest in pso [cit] . however, as all particles move towards to pbest and gbest in particle swarm, the diversity of the particles will be lost and they will easily fall into the local optimum, and thus the target position cannot be accurately found. in order to solve this problem, this paper proposed a velocity update strategy based on a random particle selected from particle swarm. in the iterative process, the particles are no longer only based on individual optimal information and the global optimal information to evolve and update, particles will learn from a random particle in the swarm. specifically, if the particle's individual optimal fitness value is better than random particle's individual optimal fitness value, particle will update according to the pbest and gbest. otherwise, particle will update according to the optimal position of random particle (rbest) and gbest. the velocity and position update of particles is shown as:"
"in order to demonstrate the effectiveness of the proposed apso for tracking target affected by noise, the 1085th frame to 1155th frame are selected in forward looking sonar image sequences. fig. 11 shows the tracking results of target affected by noise. fig. 12 shows the center position error curves (1085th frame to 1155th frame) of target affected by noise."
3) t3: material splitting transform: it may be determined from analysis of student success on homework problems and exams that there is too much material in some learning items. this leads to the transform shown in figure 6 .
"we have generalized the query language by allowing complex selections in place of restrictions: e.g., s 1 ∩ s 2 instead of s ∩ r. however, because the number of suggested restrictions in faceted search must be finite, it is not possible to suggest arbitrarily complex restrictions. more precisely, the vocabulary of features must be finite. in qfs, we retain the same set of features as in section 2, which is a finite subset of lisql for any given dataset."
"all subjects but one had correct answers to more than half of the questions. half of the subjects had the correct answers to at least 15 questions out of 18. two subjects answered correctly to 17 questions, their unique error was on a disjunction question for one and on a negation question for the other. all subjects had the correct query for at least 11 questions. for each question, there is at least 50 percent of success. the subjects spent an average time of 40 minutes on the test, the quickest one spent 21 minutes and the slowest one 58 minutes."
"for other categories, all subjects but two managed to answer correctly at least one question of each category. within each category, we observed that response times decreased, except for the cycle category. at the same time, for path, disjunction and inverse, the number of correct answers and queries increased. those results suggest a quick learning process of the subjects. the decrease in category negation is explained by a design flaw in the interface. for category cycle, we conjecture some lassitude at the end of the test. nevertheless, all but two subjects answered correctly to at least one of cycle questions. the peak of response time in category inverse is explained by the lack of inverse property examples in the tutorial. it is noticeable that subjects, nevertheless, managed"
"when computing tf for all the terms in a corpus of documents, this process produces high-dimensional, sparse vectors [cit] . techniques such as the application of singular value decomposition (svd) to a topic similarity matrix (i.e., spectral graph analysis) may allow the reduction of dimension to make computationally intensive text analysis more efficient [cit] . in the cs0 example here, the limited number of specific terms found in the topic lists produced tf vectors for which no dimensional reduction was possible."
the temporal and topical relations go well together since the temporal relations are entirely in the set of learning items. this combined graph includes all the topic nodes and learning item nodes with both the precedes and the occurs in edges included.
"qfs has been implemented as a prototype, sewelis. its usability has been demonstrated through a user study, where, after a short training, all subjects were able to answer simple questions, and most of them were able to answer complex questions involving disjunction, negation, or co-references. this means qfs retains the ease-of-use of other faceted search systems, and gets close to the expressiveness of query languages such as sparql."
"to further verify the effectiveness of the proposed apso in searching ability, the fitness values are calculated by sphere and griewank functions in the proposed apso and the pso. the optimization results are shown in fig. 2 . the relevant parameters are as follows. the dimension of the solution space is 10, the population size is 30, the maximum iterative times is 100."
the other three clearly were related to a specific topic but none of the terms in the topic list were found in the description. this could be remedied by using enable's file upload tool that provides a way to add additional text to the description of a learning item. this also caused the instructor to consider the value of more frequently using the topic terms explicitly in the textual presentation of the learning items.
"a simpler view of the temporal relations displays only the precedes relations that come immediately before a given node. this graph displays an edge from a node to the node it immediately precedes in time. this reduces the number of edges to n-1 and makes a much more readable graph. since these relations are transitive, no connection is lost. the meaning of the precedes relation is limited. learning items connected in this way are not necessarily related by topic or grouped in the same unit. note that precedes does not mean it is a prerequisite. this relation expresses nothing more than how learning items are laid out in time in the original course."
"the paper is organized as follows. section 2 discusses the limits of set-based faceted search by formalizing the navigation from selection to selection. section 3 introduces lisql queries and their transformations. in section 4, navigation with qfs is formalized and proved to be safe and complete w.r.t. lisql, and efficient. section 5 reports about a usability evaluation, and section 6 concludes."
"temporal relations express the relation in time between learning items. the word precedes is used to express this relation. a learning item (item a) precedes another learning item (item b) when the due date of item a is before the due date of item b. these relations are transitive such that if item a precedes item b and item b precedes item c, then item a precedes item c. when all these relations are included on a directed graph the learning item at location k in the sequence has k − 1 in-edges and n − k out-edges, where n is the number of learning items. these graphs are too cluttered to be informative. see figure 3"
"we assume a certain well-formedness criteria on our filter functions, such as that they do not change the form of an action (e.g., an output action remains an output action), and that whenever they map to silent actions, τ, these are not decorated; the filter functions in example 4.1 satisfy these criteria. through this latter requirement we re-obtain the standard silent τ-action at the lts level."
"where the first constraint imposes that the control-traffic capacity demand is always satisfied, while the second imposes that the amount of capacity assigned to a data flow does not exceed the demand. moreover, fairness is introduced by normalizing the utd for the data-traffic demand of each flow, d d v,w . the aim is to have almost the same percentage of utds for different flows."
"the primary contribution of this paper is a unified formal framework for studying different monitoring strategies. we present a location-aware calculus supporting explicit monitoring as a first class entity, and internalising behavioural traces at the operational level rather than at a meta-level. we show the expressivity of the calculus by using it to model different distributed system monitoring strategies from the literature. we also present a novel architecture in which contract monitors migrate across locations to keep information monitoring local, while limiting remote monitor instrumentation in certain situations. the versatility of the contract-supporting calculus is later illustrated by showing how it can model different instrumentation strategies. in particular, we show how behavioral contracts expressed using regular expressions can be automatically translated into monitors of different monitoring strategies."
"basic events have the form (c,v)@k indicating that a communication on channel c with valuev occurs at location k. we adopt a semantics allowing for multiple matches, rather than opt only for the shortest match 2 and thus any trace terminating with a communication c!v at location k is considered to be a violating trace. the other operators are the standard ones used in regular expressions: e.f corresponds to the traces which can be split into two, with the first matching e, and the second matching f; expression e * corresponds to traces which can be split into a number (possibly zero) parts, each of which satisfies e; and e + f corresponds to the set of traces which match either e or f."
δ sys ψ o (e)) ω ntg ≈ (δ sys ψ c (e)) ω ntg ≈ (δ sys ψ m (e)) ω ntg and prove it by giving witness bisimulations defined by induction on the structure of e. one can prove similar results on the lines of the equivalences given in section 4.4.
"the timestamps of trace-set (1) record the fact that the output on c 1 was consumed before that on c 2, whereas those of trace-set (2) record the opposite. however, in both of these trace-sets, the timestamp assigned to the trace-entity relating to c 3, recorded at location k, does not indicate the order it was consumed, relative to the outputs on c 1 and c 2, which occurred at location l."
"which checks that equal values are communicated on channels c 1 and c 2 . thus it is important that trace entities, such as l t(c 2, v 2, n + 1), are persistent and not consumed once analysed, as in the case of outputs in a message passing setting."
"this was repeated 4 times for each repair and the suture was cut flush to the tendon surface each time, so no barbs were present on the exposed tendon. to facilitate a solid, nonslip repair, each barbed suture strand was placed in an opposing direction to the one beside it. a core suture purchase of 10 mm was made for the barbed technique as well."
". figure 7 represents the fairness index of the proposed scheme as a function of the number of small cells in the area. we can see that the proposed scheme achieves a higher proportional fairness, while the other two schemes have almost the same performance. indeed, even if the hc approach allows distribution of the traffic mainly in high-capacity links, it does not take into account the fair distribution of utds among the flows. the behavior is almost constant with the number of cells, there is a little increase, because there are more paths, and hence, a reduction of utds that can be better distributed."
"m orch monitors for this property in orchestrated fashion, querying traces at both l and k from a remote central location h; this monitor is well-aligned with location l to start with, but has to explicitly re-align with location k once monitoring shifts to that location. m chor is an instance of a choreographed monitor setup, instrumenting local monitors at each location where trace querying needs to be performed, namely l and k. these local monitors synchonise between them using remote communication on the scoped channel d . note that the monitor at k updates its context upon channel synchronisation on d to ensure a temporal ordering on analysed trace records; without synchronisation, the monitor would potentially be reading past parts of the trace which may lead to unsound sequentiality conclusions. finally, m mig is a case of a migrating monitor, that starts monitoring at location l but then migrates to location k if it needs to continue monitoring there, re-aligning its index to that of the destination location."
"repaired tendons were mechanically tested with a zwick z005 tensiometer (zwick z005, ulm, germany). to simulate the forces that act on an immobilized tendon during active flexion, the upper clamp of our material testing machine was set with a preload of 1.5n and an advancement rate of 20 mm per minute. 33, 34 these settings have been used previously in biomechanical tests on flexor tendons. 19 the repaired tendons were secured tightly in the upper and lower sandpaper covered clamps before mechanical testing. there was no slippage of the tendon ends during testing."
"this paper dealt with backhaul routing in 5g udn. udn paradigm is considered a pillar of future 5g networks but its deployment on large scale is challenging. in particular, udn deployment could lead to a massive traffic forwarding on the backhaul network that may not be able to support all access-point traffic demands. this paper investigated the use of the sdn paradigm to have efficient and flexible routing policies. a problem specific for udns backhauling has been presented, and a new solution based on a modified dijkstra algorithm has been proposed with the aim of reducing the traffic demand that the backhaul network is not able to support. the solution is defined for a centralized implementation and for splitting control and data planes following the sdn-approach. in particular, first, the control-flow routes in the backhaul network are identified using a bandwidth-constrained dijkstra algorithm. the control flows must be always satisfied by the network. the second step allows finding of the routes for the data flows. the algorithm searches successively for each cell for the route that has the minimum cost and that satisfies the data-flow request. if this route does not exist the algorithm looks for the path that minimizes the weighted utd, i.e., the amount of traffic that the backhaul network is not able to support due to the congestion of one or more links. the utd is weighted to achieve a fair distribution of the unsatisfied demand among different data flows that are allocated on the congested link."
"assuming that locations l and k have the respective timestamp counters n and m, once all outputs are consumed we can obtain either of the following possible sets of trace entities (for simplicity, we assume that v 1 v 2 ):"
"the optimal solution of the proposed problem is not feasible with an affordable complexity, hence, we divide the problem in two sub-problems: (i) finding the solution for the bandwidth-constrained problem of the control traffic, and (ii) finding the paths for the data flows that minimize the utds."
"on the other hand, the choreography control logic can be placed at any location. for instance one may choose to locate them at the node where the next input will be expected, or where the last one occurred. for a particular choice of locations l and h, choice e + f is compiled as follows:"
"more and more systems are deployed in a distributed fashion, whether out of our choice or necessity. distribution poses major design challenges for runtime monitoring of contracts, since monitors themselves can be distributed, and trace analysis can be carried out remotely across location. this impacts directly various aspects of the system being monitored, from the security of sensitive information, to resource management and load balancing, to aspects relating to fault tolerance. various alternative solutions have been presented in the literature, from fully orchestrated solutions where monitors are located at a central location, to statically distributed monitors where the contract monitor is statically decomposed into different components hosted at the location where system traces are generated."
"rule outt models trace actions as output labels with tags t : k, l : n, where the timestamp of the trace, n, is recorded in the tag as well. crucially, the trace entity is not consumed by the action (thereby acting as a broadcast), and its persistence allows for multiple monitors to query it. this action can be matched by a query action, int, expressed as an input action with a matching tag t : k, l : n where the source location of the trace entity, k, and time stamp n must match the current monitoring context (k, n). since the action describes the fact that a trace entity has been matched by the monitor query, the timestamp index of the monitoring context is incremented, (k, n + 1) to progress to the next entitity in the local trace log."
"moreover, sdn allows use of a centralized algorithm as the one we propose. indeed, in this case a distributed algorithm would be impracticable, because all routes are calculated/updated together so that to fairly distribute the utds. this operation can be done only by a central entity that has a global view of the network. as an example, figure 4 reports the path-reconfiguration that follows the increase of the data-traffic demand of the node d. in particular, three nodes, (d, e, f), are involved in the network changes. the routes are depicted only for these nodes, and we suppose, in this example, that only data traffic is present directed toward the cn."
"scope extrusion of channel names may occur both directly, through process or monitor communication, or else indirectly through trace querying; these are both handled by the standard scoping rules open and res. all three forms of communication i.e., process, monitor and trace, are also handled uniformly, this time by the communication rule com1 (we here elide its symmetric rule). communication yields a silent action τ γ that is decorated with the corresponding tagging information from the constituent input and output actions of the premises. this tagged information must match for both input and output actions and, in the case of the trace tags, t : k, l : n, this also implies a matching of the timestamp n. when, for a particular timestamp, querying does not match the channel of the trace entity at that timestamp, rule skip allows the monitor to increase its timestamp index and thus querying to move up the trace-chain at that location. finally, sync allows monitors to realign with a trace at a particular location, geti and seti allow for explicit manipulations of the monitoring context whereas go describes monitor migration."
"note how migration (thus monitor instrumentation) is delayed and happens only once the start signal on channel s is received. initially, the monitor can be chosen to reside anywhere. for a particular location choice h, the migrating monitor approach for a contract e would be the following:"
"for simplicity, our calculus only records effected output process events in traces (which dually imply effected inputs); we can however extend traces to record other effects of computation in straightforward fashion. in order to extract realistic temporal ordering of traces across locations, the calculus provides two mechanisms for monitor re-alignment: the coarser (and real-time) sync operation is used to start monitoring from a particular instant in time; the more explicit context update seti enables hard-coded control of relative timing at the level of monitors. for instance, together with geti, it enables decomposed monitoring to hand over tracing at a specific index in a local trace. this mechanism gives more control and can improve distributed monitoring precision, but may also lead to unsound monitoring."
"we define the semantics of mdpi in terms of a number of related labelled transition systems (ltss), which are then used to compare systems through the standard notion of weak-bisimulation equivalence, denoted here as ≈. this framework allows us to state and prove properties from a behavioural perspective about our monitored systems. for instance, we could express the fact that, ignoring monitoring location, m orch and m chor from example 3.3 monitor for the same properties wrt. sys, using the statement:"
"the instrumentation of contracts as distributed monitors is non-trivial and can easily lead to unsound contract monitoring. in this section we illustrate how the instrumentation of contracts, expressed using a simple regular expression-based temporal logic specifying violation traces, can be safely automated according to different monitoring approaches. the syntax of the contract language is:"
"we are currently working on an implementation in erlang [cit], guided by the design decisions made for our calculus. this should give us insight into practical issues, such as that of addressing trust issues when installing monitors and the avoidance of indirect data exposure due to monitoring. we are also studying mdpi further, addressing issues such as clock boundaries and real-time operators. as the calculus stands, the monitoring component is non-intrusive, in that it reads system events but does not otherwise interact with it. to handle reparations and enforcements upon contract violation, and to be able to express monitor-oriented programming [cit] we require potentially intrusive monitoring. we believe that our bisimulation approach can also handle reasoning about monitor intrusiveness."
"as systems continue to grow in size and complexity, the use of behavioural contracts is becoming crucial in specifying, regulating and verifying correctness. various notions of contracts have been used, but most prevailing variants enable the regulation of the behaviour of a system, possibly with consequences in case of violations. such contracts can then be used in multiple ways, from system validation and verification, to conflict analysis of the contract itself. one important use of contracts is in runtime monitoring: system traces are analysed at runtime to ensure that any contract violating behaviour is truncated before it leads to any further consequences, possibly applying reparations to recover from anomalous states."
"orchestration places the monitor at some predefined central location, h. as stated earlier, the lack of a global clock prevents it from deducing with certainty the order of events happening across different locations. nevertheless, our translation attempts to mitigate this imprecision for sequence of events occurring at the same location using the following mechanism: when a basic event is matched, the monitoring context, (k, n), at that moment is recorded using geti and passed as arguments on the match signaling channel; this allows subsequent matching to explicitly adjust the monitoring context to these values using seti in cases where the location of subsequent events does not change; in cases where the location changes, this information is redundant and alignment is carried out using the coarser sync command."
"using different ltss, the same system could be assigned more restricted behaviour. for instance, this is useful to ensure that the monitor m mig of example 3.3 does not perform remote querying at any stage during its computation by establishing the comparison:"
"the traffic generated by cells (both data and coordination traffic) must be routed through the backhaul network toward the core network (cn) and other cells, often with stringent requirements in terms of capacity, latency, availability, energy, and cost efficiency. the consequent massive traffic forwarding is a great challenge for future 5g backhaul networks that may represent the capacity bottleneck. moreover, the 5g backhaul network is complex and must have the ability to dynamically 2. related literature sdn has been widely investigated for wired networks, where its benefits have been proved both in terms of performance and management. as a consequence, there is an increasing interest in evaluating if also mobile networks can take advantages from sdn [cit] . benefits and challenges of the software-defined-networking in wireless networks are analyzed in several papers that present high-level architectures [cit] . in particular, there is a great interest in sdn-based backhauling due to the dominant role that these networks will have in future 5g systems, especially in presence of udns and new paradigms such as cloud-ran, caching, fog/edge computing [cit] . in this area, recent papers focus on specific services provided by an sdn-based backhaul network, such as mobility management [cit], multi-tenant network slicing [cit], caching [cit] and cross-layer coordination [cit] ."
"since the control traffic is bandwidth-constrained, the control-traffic demands are always satisfied. moreover, in high-capacity network scenarios, all traffic demands are satisfied (i.e., utd is zero). for these reasons when we show results in terms of utds, we refer only to data traffic in low-and medium-capacity networks. the high-capacity network is considered for delay evaluations. figure 5 reports the total amount of utds for the data traffic as a function of the mean number of cells in the area when the network has medium (a) and low (b) capacity. figures report the behavior of the proposed algorithm (fudm) and the two benchmarks (mnh, hc). we can see that the proposed algorithm always outperforms the two benchmarks. in particular, the effectiveness of fudm algorithm is more evident when the network has low capacity (or similarly, is overloaded), indeed in this case the gain in terms of utds increases. this means that the proposed method is more able to distribute the traffic among the available paths, thus reducing the links' congestion. in all cases we can observe that increasing the number of cells introduces only a slight increase of the utds value because also the number of possible paths in the network increases accordingly. to see how the degree of connection of the network impacts on system performance, figure 6 reports the total amount of utds when the mean number of edges per node varies (i.e., low, medium and highly connected network), for both low-and medium-capacity networks. as the network connection degree increases, also the number of edges in the network increases. hence, the data traffic has more alternative paths to reach the destination and the links' congestion decreases. this is particularly true for the proposed method that can better distribute the traffic among the possible routes. benefits are more evident in critical situations, i.e., when the network has low-capacity links. moreover, the proposed algorithm aims at fairly distributing utds among different data flows. to evaluate this aspect, we have derived the jain index of the percentage of the data-traffic demand that is not satisfied (i.e., following the proportional fair approach). jain index provides a measure of the fairness of a given performance x, and is defined as"
"in addition to stripping τ action tags, the second filter function, ω prc, allows only process external actions, filtering out the p component in this case. the function is partial as it is undefined for all other prelts actions. this is useful when we do not want to discriminate configurations based on the tracing and monitoring actions. the final filter function, ω ltr, removes all tags but uses them to prohibit silent tracing actions where the two locations in the tag are distinct; this in effect rules out remote trace querying, thereby enforcing localised trace monitoring."
"we have shown how one can formulate different monitoring strategies of the same contract using mdpi. the contract language and its compilation procedure have intentionally been kept simple to avoid their complexity from obscuring the underlying monitoring choices. the different approaches mostly differ only in the location of the monitors. the migrating monitor approach also allows for straightforward setting up of new contracts at runtime, including references to locations not known at compile-time. furthermore, the migrating approach procrastinates from setting up monitors in remote locations until necessary. in contrast, on a choreographed approach, monitors are set up at all locations, even though some of them may never be triggered."
we define different macro-scenarios where network parameters are random variables with a certain probability distribution as reported in table 1 . we have averaged numerical results over several realizations of a given scenario. in that way results depend on the macro-scenario but not on a specific distribution of cells and traffic-loads.
"a caliper with a preset of 2 mm was set up alongside the tensiometer. when a 2-mm gap at the repair site occurred during testing as measured by the caliper, the corresponding force that created this gap was recorded. the ultimate failure load of each repair was also recorded. this is the greatest force that occurs immediately before tendon repair failure. the mode of final failure (rupture or pullout) was also recorded."
"we have described a novel technique for flexor tendon repairs using a barbed suture device in which the directionality of the barbed suture is used to hold the repair. we found our barbed repair much easier and faster to perform and required fewer hand movements than the adelaide repair. the barbed device was easier to handle than the polypropylene as we found that the barbs act as a grip. we also noticed that if technical errors were made during the barbed repair method, they were easy to rectify by pulling the suture out in the direction of the barbs. this is in contrast to more complex barbed suture repairs that have been described, where technical errors are not as easy to correct, as multiple suture passages of the suture within the tendon substance make it more difficult to extract the suture. 6 we did not observe any significant difference in the tensile strengths of both repair methods as both repair methods failed at comparable forces. gap formation is a common complication post flexor tendon repair that can adversely affect the end result and prolong tendon healing. 37 there was a statistically significant difference in the 2-mm gap formation force required in the 2 groups. the barbed repair method was able to withstand more force than the traditional repair method before a 2-mm gap formed at the repair site. we did not pretension either material before repair so this may have had an effect on the gapping forces for the adelaide group. 38 repaired flexor tendons should be able to pass freely through the flexor sheath. repairs typically add bulk to the repair site due to bunching of the tendon ends and the presence of the knot and suture material. this increased csa can impair tendon gliding and overall outcome. 17 in our study, we found that the barbed suture group had a significantly reduced csa than the adelaide group. this would allow smoother gliding through the pulley system and could reduce the postoperative rupture rate."
"tracing: whenever communication occurs (possibly across two locations) on some channel c with valuesv, a trace entity of the form t(c,v,n) is produced at the location where the output resides. the output location, which keeps a local counter, assigns the timestamp n to this trace entity and increments its counter to n + 1. local timestamps induce a partial-order amongst all trace entities across the system. in particular, we obtain a finite (totally ordered) chain of traces per location."
"using an lts that does not express observable monitor actions, the property that a monitor, say m orch, does not affect the observable behaviour of the system sys could be stated as:"
"to solve the considered problem, the routing algorithm must take into account both the capacity of each edge and the edge's congestion. toward this goal we have modified the dijkstra algorithm for an sdn implementation that requires a global knowledge of the network and is performed centrally at the sdn controller. being the goal the minimization of utds with a fair distribution we refer to the algorithm as fair unsatisfied demands minimization (fudm)."
"example 4.2. using the filter functions defined in example 4.1, we can formally state and prove equivalences (5), (6) and (7) outlined earlier, for a localised clock-set δ including locations l and k:"
"an alternative approach is that of using migrating monitors, which adequately supports dynamic contracts whilst still avoiding orchestration; in particular, it limits instrumentation of distributed monitors in cases were monitoring is dependent on computation. using this approach, monitors reside where the immediate confidential traces reside, and migrate to other subsystems, possibly discovered at runtime, when information from elsewhere is required i.e., on a by-need basis. this enhanced expressivity also permits support for dynamic topologies and contracts learnt at runtime. example 2.1. consider the hospital system contract:"
"the preservation of the property 'whenever a value e is communicated on the first output on c 1 at location l, then this value is not output on a subsequent output on channel c 2 at k' in general cannot be adequately determined statically, due to the non-deterministic nature of the computation, as exhibited by the possible traces (3) and (4). however, the property can be monitored at runtime in a number of ways:"
"for each vertex of the graph, the dijkstra algorithm [cit] generates a shortest-path tree (a tree is a graph with one and only one path between every two nodes) with the given vertex as root, i.e., the node that originates the path. two sets are maintained, one contains vertices included in the shortest-path tree and the other contains those not included. at every step the algorithm finds a vertex not yet included in the shortest-path tree that has minimum cost from the source. at the beginning only the root (i.e., the selected vertex of the graph) is included in the shortest-path tree."
"a nurse will have access to a patient's records after requesting them, as long as his or her request is approved by a doctor assigned to the patient. we assume that (i) the nurse requests (and eventually accesses) the patient's data from a handheld device, (ii) the information about which doctors have been assigned to which patients resides at the central site, and (iii) the patient's information is stored on the doctors' private clinic systems, where doctors can also allow nurses permission to access patients' data."
"the combined monitors are located at the predefined central location h, with a dummy initial monitor context continuation parameters (h, 1). the monitor induced for a contract e is thus:"
"data traffic path selection is always based on a modified dijkstra algorithm. however, the algorithm is not constrained and introduces a proportional fairness concept. at the beginning of this phase, the residual capacity matrix, r, has the value achieved at the end of control-flow allocation."
"in either case, a listener triggers a signal with the updated index of the trace on the match channel for every event c with datav. specifically, the macro trg(c,v, f ) repeatedly reads from channel c, outputting the monitoring-context information on the matching channel f every time the data traced matchesv:"
"the paper is organized as follows. first we introduce the main concepts of sdn in section 3, then the considered scenario is described in section 4. section 5 presents the problem formulation whose solution is proposed in section 6. finally, numerical results are presented in section 7 and conclusions are drawn in section 8."
"v,w represents the amount of capacity that has been allocated to the flow from node v to node w for the control and data traffic, respectively."
"asynchronous messages are initiated by fds to update the sdn controller of events and changes (e.g., a new flow, status change, etc.). consequently, an sdn can timely react to the network changes. indeed, in an sdn-based network the convergence time of routing algorithms is low and quite invariant respect to the network size [cit] . convergence time depends only on the reactivity of fds to notify a change in the network to the sdn controller. only the sdn controller must be advised, then it computes new routes and simultaneously instructs all the involved fds. conversely, in a traditional network if a change occurs, the fd first updates its routing tables, then sends a message to its neighbors. these on their turn updated their tables and send the message to their neighbors and so on. the message propagates in the network. then each node updates its routing tables and sends its best paths to the other routers. this messages flooding introduces a significant delay because all fds must be advised, then updated routing information must be propagated. obviously, the convergence time and the delay increase with the network dimension."
"instead of instrumenting the whole monitor at a single central location, a choreography-based approach decomposes the monitor into parts, possibly placing them at different locations. once again, monitors are made up of two kinds of components: (i) the event listeners; and (ii) the choreography control logic made up of comb and bifurc components. the event listeners are located locally, where the event takes place, but are otherwise exactly the same as in the orchestrated approach:"
"a limitation of our study is that we used a caliper preset to 2 mm to calculate the 2-mm gap formation force. as distraction occurred at a rate of 20 mm/min, it was difficult to accurately assess the 2-mm gap formation force by visual estimation. it would have been preferable to carry out our experiments using identical suture materials, as we directly compared a polybutester barbed suture to a polypropylene monofilament suture. our biomechanical testing used a linear load to failure as one of the primary outcomes. to better replicate physiological conditions, it would have been interesting if angular tensile strengths or cyclical loading studies had been carried out."
"formalising the compilation of regular expression contracts into mdpi also gives us opportunities to formally verifying certain properties. for instance, as a generalisation of (8) we can state and prove that, for arbitrary expression e, different compilation approaches give the same monitoring result. we can state this as:"
"there are a number of issues relating to these different monitoring approaches that are unresolved. for instance, it is somewhat unclear, at least from a formal perspective, what added benefits migration brings to distributed monitoring. there are also issues relating to the monitoring of consequential properties across locations, which cannot be both sound and complete: in example 2.1, by the time the monitor migrates to the doctor's system, the doctor may have already approved the nurse's request. distribution precludes precise analysis of the relative timing of the traces, and one has the option of taking a worst or best case scenario, avoiding false positives or false negatives respectively. this problem is also prevalent in both orchestrated and choreographed approaches. we therefore require a common formal framework where all three approaches can be expressed. this would, in turn, permit rigorous analysis and evaluation with respect to these issues."
"we find that the second translation, m 2, lends itself better towards illustrating how monitors can be distributed in different ways across locations -for example, in an orchestrated approach, we would place all the monitoring processes in a single location, while in a choreographed approach, we would distribute the processes as required. a sequential approach such as m 1, may be more approriate in an orchestrated approach (since it avoids unnecessary parallelism), but would not be possible to distribute to enable choreographed monitoring without further manipulation."
"for the migrating monitors technique, we use a simplified translation where the monitors generated are similar to the ones used in orchestration, except that the monitor migrates when required to the relevant location (using the go operator). ψ m is defined identical to ψ o except for basic events:"
"we consider here a heterogeneous network where micro-, pico-, and femto-cells (generally referred as small cells or access points in what follows) are densely and randomly deployed in the macrocell area. the total number of cells is c (i.e., small cells plus macrocell). each cell base station (bs) provides access to ues connected with it, hence, the cell bs has a certain amount of traffic generated by the associated-ues that must be routed in the backhaul network. due to differences in cell coverage areas the amount of traffic generated by different cells can significantly vary. following the sdn principle, we consider two types of traffic flow: data and control. indeed, cell coordination can be split in control plane coordination, which allows optimization of network parameters (e.g., interference management, load balancing, mobility management), and data plane coordination, which allows data exchange and resource scheduling. the two flows are routed following different principles, and control traffic has higher priority."
"as described before, the 2-0 v-loc device is a single-ended unidirectional barbed suture with a welded loop at the end. there are 26 barbs per centimeter of material, and they are distributed circumferentially around the suture at 120° rotations with each barb 0.38 mm in length. our barbed repair technique used 4 strands. the welded loop was not required for our repair method and so was cut off from the suture. the needle was passed through the tendon surface on one side, across the repair site and then out through the surface on the other side. each pass was made through the midsubstance of the tendon, and they were made 2 mm apart from one another."
"the noted advantages of barbed sutures for wound closure have been the lack of knot and the ease and speed of closure. until now, no previous flexor tendon repairs using barbed devices have been simple and quick."
"our calculus describes distributed, event-based, asynchronous monitoring. monitoring is asynchronous because it happens in two phases, whereby the operational mechanism for tracing is detached from the operational mechanism for querying the trace. this two-set setup closely reflects the limits imposed by a distributed setting and is more flexible with respect to the various monitoring mechanisms we want to capture. monitoring is event-based because we chose only to focus on recording and analysing discrete events such as communication."
"although necessary to encode extended information of system execution, the prelts presented is too discriminating. for instance, the internal action τ γ is now compartementalised into distinct silent actions, each identified by the tag information γ, which complicates their use for weak actions when verifying bisimilar configurations. similarly, external actions differentiating between a process or a monitor carrying out that action may also be deemed to discriminating. finally, we may also want to disallow certain actions such as remote trace querying."
we suppose that first the algorithm (mnh or hc) is carried out for the control traffic with a bandwidth constraint: the selected path is the one that has minimum cost and has sufficient bandwidth to accommodate the control flow. then the algorithm is performed again for the data traffic without any constraint.
we want to underline that varying the network capacity while maintaining the traffic load constant leads to the same considerations as fixing the network capacity and varying the network load.
"when the network has very high capacity, all the traffic (both data and control) is supported, with any algorithm. nevertheless, in this case it is interesting to evaluate the average delay of the network. we can see that using a more efficient routing algorithm allows reduction of the delay because traffic is routed in less congested links thus speeding up the transmission. with a rough approximation the average delay of the control and data traffic has been estimated as table 2 reports the average network delay of the network for the proposed and benchmark methods when the network has high capacity and utds is zero for both data and control traffic. we have considered a medium-connected network."
"as stated before, dense and random deployment of the network infrastructure imposes new challenges in the backhauling, which becomes the capacity bottleneck and may not be able to support all the traffic demanded by cells. as consequence, we formulate a new routing problem that is particularly suited for this scenario."
"notation: note that filter function applications are essentially abstractions of the prelts. ltss obtained in this manner can effectively be indexed by their respective filter function, ω, and for clarity we denote a configuration c subject to a behaviour obtained from the prelts and a filter function ω as c ω . we also denote transitions obtained in this form as"
"equivalence (8) formalises the behaviour expected for (5) using an lts whose actions prohibit distinctions based on action tags; including monitoring location, i.e., ω ntg . since in (6) we wanted to analyse how monitors affect process computation, in its corresponding equivalence (9) we use an lts that tags process external actions with location information while prohibiting any actions relating to tracing or monitoring. finally (10) compares m mig with itself, subject to a restricted semantics where remote monitoring is prohibited, i.e., ω ltr ."
"the technical development in sections 4.2 and 4.3 allows us to immediately apply weak bisimulation [cit] as a coinductive proof technique for equivalence between ltss obtained for our prelts and well-formed filter functions. two (filtered) ltss, c ω 1 and d ω 2, are bisimilar, denoted as c ω 1 ≈ d ω 2, if they match each other's transitions; we use the weak bisimulation variant, ≈, as this abstracts over internal τ-actions which yields a more natural extensional equivalence."
"in this paper, we adopt the maximally parallelised approach, primarily to be able to observe similarities and distinctions between different compilation approaches. in particular we use this translation for a compilation strategy corresponding closely to standard approaches used in hardware compilation of regular expressions [cit], producing circuits with two additional wires: an input which is signaled upon to start matching the regular expression, and an output wire which the circuit uses to signal a match with the regular expression. in our case, the wires correspond to channels: basic events (c, v)@k with start channel s and match channel f would be translated into an expression which waits for input on channel s, then outputs on channel f when an instance of c with v occurs at location k. our translations also employ two standard monitor organisations for funneling two output signals into one and forking channel communication onto two separate ones; these are expressed below as the macros comb and bifurc:"
"from above we can summarize that routing policies, specifically designed for udns backhauling, have not been yet widely investigated. the literature mainly refers to routing in wired and fixed networks, where the common goal is to minimize the maximum link use, thus avoiding that some links are particularly congested while others are underused. however, sufficient capacity to admit a new flow is always assumed."
"closer inspection of the comparisons (5), (6) and (7) reveals that the different ltss required are still expected to have substantial common structure; typically they would differ with respect to either the information carried by actions and/or the type of actions permitted. for instance, in (5) we would want actions that restrict information relating to the location of where monitoring is carried out, as this additional information would distinguish between the two monitors. on the other hand, for (7) we would want to prohibit actions relating to remote monitoring. we therefore construct these related ltss in modular fashion through the use of a prelts, i.e., an lts whose transitions relate more systems, and whose action labels carry more information than actually needed. the excess transitions and label information are then pruned out as needed by a filter function from actions in the prelts to actions in the lts required."
"note that unless all the locations enable the execution of new (monitoring) process at runtime, the contracts must be known at compile-time, which is guaranteed in the simple regular expression logic we are using."
"we have presented a novel process calculus framework in which distributed contract monitoring can be formalised and analysed. we have shown it to be expressive enough to encode various distributed monitoring strategies. to the best of our knowledge, it is unique in that it traces are first class entities rather than meta-constructs. we modularly developed various semantics for this calculus, using transition abstraction techniques that enable selective reasoning about aspects such as locality of communication and distinctions between monitor and process actions."
"we suppose that the admission of an access point to the network is constrained by the control traffic, i.e., a small cell can be admitted only if there is at least one path on the backhaul network that supports its control-traffic bandwidth demand. by contrast, the data traffic demand can be only partially satisfied (i.e., it is not constrained). the main concept is that control traffic is mandatory for an access point to function, hence, it cannot be limited. conversely, if the backhaul network is not able to support all the data traffic demand, the sdn controller instructs the small-cell bs to take actions at ran level to reduce its data traffic such as call blocking, ue's throughput reduction and load balancing policies [cit] . in general, in a udn, ues are under the radio coverage of multiple cells, hence, they can be forced to connect to different cells, thus operating load balancing directly at the ingress nodes. as a consequence, we want to jointly solve:"
"as already stated the proposed routing algorithm runs at the sdn controller that supervises the fds behavior by means of openflow protocol. when a change occurs in the network, the sdn controller recomputes traffic-routes following the steps described before, and sends a modify-state command to fds affected by changes. fds change corresponding entries in the forwarding tables. in particular, three types of messages are supported: controller-to-switch, asynchronous and symmetric."
"a migrating monitor starts on the nurse's system; upon receiving a patient-information request, it migrates to the hospital system, decomposes, and spreads to the systems of the patient's assigned doctors to check for permissions allowing the nurse access to the records. finally, if the permission is given, the decomposed monitors migrate back to the nurse's device to check that the records are available. as with choreographed monitoring, and in contrast to orchestration, migrating monitors can ensure that monitoring is performed locally. the main difference is that instrumentation of monitors can be performed at runtime. for instance, when monitoring the hospital contract clause, no monitor is installed on a doctor's system unless a nurse has made a request for information about a patient assigned to that doctor, which is less intrusive."
"we used 40 fresh porcine flexor digitorum profundus tendons for our study. porcine tendons are routinely used in flexor tendon studies, and they best represent the zone ii flexor digitorum profundus tendon of the human middle finger in terms of biomechanical properties. 17, [cit] all tendons were examined carefully and any found with defects or abnormalities were discarded."
"the main pillar of sdn is decoupling control and data planes: the first one is represented by the sdn controller that aggregates and performs control decisions, while the second is represented by forwarding nodes and links connecting these nodes, and simply becomes packet forwarding. in particular, the main elements of an sdn network are: in an sdn the control logic is moved on an external entity and is programmable via software. the sdn controller supervises the behavior of the forwarding elements through an open programming interface such as openflow. the controller can translate the application requirements down to the data plane, and to provide an abstracted view of the physical network to applications. more in detail, the sdn controller defines forwarding rules that are pushed down to the fds, which simply match the rules with the incoming data flows. simplifying the functionality of fds, these can be implemented on general-purpose hardware and controlled with standard protocols. this leads to a significant flexibility increase respect to traditional networks where each router must perform control functions and implements proprietary interfaces that cannot be reconfigured via software. moreover, if significant changes frequently occur (like in case of udns), traditional networks present a significant signaling increase and longer convergence time of routing algorithms. differently, centralized control operations require less communications (i.e., signaling) and shorter delays. sdn controller must have an updated global view of the network connectivity and throughput. when a change occurs in the network, the sdn knowledge is updated."
"controller-to-switch messages are initiated by the sdn controller and used to manage and observe the fds' state. hence, these messages are used for feature/status requests and configurations. for example, the sdn controller periodically performs traffic monitoring (read-state message) to collect links' statistics, in order to react in real time. sdn controller sends a flow-status request to fds, which answer with a flow-status reply that contains flow-duration and byte-count, thus, to evaluate the link throughput."
"note that using this approach entails minimal local monitor instrumentation since this happens on a by-need basis: the translation avoids installing any monitor at location k unless c 1 !v 1 happens at l. even within this simplistic formal setting, migrating monitors can be seen to be more versatile than a choreographed approach. for instance, if our contract language is extended with variables and a binding construct, ∃x.e, we could express a more dynamic form of contract such as ∃x.(c 1, x)@k.(c 2, v)@x; in such a contract the location of the second event depends on the location communicated in the first event and, more importantly, this location is not known at compile time. because of this last point, this contract cannot be handled adequately by traditional choreographed approaches which would need to preemptively instrument monitors at every location. however, in a migrating monitor approach, this naturally translates to a single runtime migration."
"manner that may have a negative impact on the vascularity and healing capabilities of the tendon. 3, 9, 10 the advent of barbed suture technology could conceivably avoid some of the limitations of traditional repair methods. the potential advantage to barbed sutures for tendon repairs is that the load is distributed evenly throughout the intratendinous suture length rather than creating stress points at the locking zones. furthermore, this even distribution of load would reduce the constricting action of the suture on the tendon. 3 the use of a barbed device for tendon repair was first described by mckenzie 11 [cit], but there has been little reported on their use until quite recently. there are now several commercially available barbed suture devices on the market, but they are not licensed for tendon repairs as of yet. their main use has been in wound closure 12, 13 and abdominal 14 and gynecological procedures. 15 barbed suture devices have been described before for flexor tendon repairs, 3, 6, 11, [cit] and the results have been promising. a major advantage in using barbed sutures is that there is no requirement for a knot. this creates a reduced csa at the repair site and also permits a quicker repair. 17 a major concern with most barbed repair techniques described to date has been the presence of barbs on the outer surface of the tendon. these have the potential ability to cause attritional damage to the pulley system in vivo, which increases gliding resistance and promotes adhesion formation. 3 we have compared a simple and quick barbed suture repair without any exposed barbs on the tendon surface to a traditional 4-stranded adelaide repair using a monofilament suture."
"the added expressivity of migrating monitors requires a trust management infrastructure to ensure safe deployment of received monitors. various solutions can be applied towards this end, from monitors signed by a trusted entity showing that they are the result of an approved contract negotiation procedure, to proof-carrying monitors which come with a proof guaranteeing what resources they access. this issue will not be discussed further here, but is crucial for the practicality of migrating monitors."
"all three distributed monitors in example 3.3 are sound wrt. the property stated, in the sense that they never falsely flag a violation. they are nevertheless incomplete, and may miss out on detecting property violations. for instance, m orch may realign with location k after the trace k t(c 2, v, m) is generated by k, which sets the monitor timestamp index to (k, m + 1). this forces the monitoring to start querying the trace at k from index m + 1 and will therefore skip the relevant trace item k t(c 2, v, m) . this aspect is however not a limitation of our encoding, but rather an inherent characteristic of distributed computing as discussed earlier in section 2."
"this work showed the effectiveness of a centralized routing protocol that follows the sdn principles. however, the work should be extended to evaluate the network reactivity, meaning that the reaction time to changing conditions should be evaluated taking into account also the overhead introduced by the sdn message flow. funding: this research received no external funding."
"initially, the node d requires 30 mbps that can be entirely routed toward the cn by means the direct link. the network can satisfy all the traffic demands (i.e., no utds). when node d increases its traffic demand up to 50 mbps, the shortest path (i.e., the direct path toward the cn) would lead an utd for the node d of 20 mbps. the proposed algorithm instead looks for the solution that minimizes the total utd. if the traffic of node d toward the cn is forwarded by node c, the total utd results to be 10 mbps (i.e., the edge form node c to node a has 100 mbps capacity but the total request of nodes c and d is 110 mbps). moreover, the proposed algorithm distributes the utd among all the nodes that route their traffic on the congested link, in this example the 10 mbps are distributed among the nodes c and d as represented in figure. in particular, the utd of the two nodes are derived by the (5), which allows distribution of the unsatisfied requests among all the data flows allocated on the congested edge proportionally to their data-traffic demand as previously detailed."
"consider the trace-set (1) from example 3.1. a (local) monitor determining whether, from timestamp n onwards, a value v 2 was communicated on the first output on channel c 2 at location l, can be expressed as:"
"fifth-generation (5g) wireless networks are expected to support a high variety of services and applications, with different quality-of-service requirements and network functions, in a cost-effective manner [cit] . disruptive changes in mobile networks are needed for supporting such an evolution. in particular, access-point densification (i.e., ultra-dense networks, udn) is one of the key elements of 5g to achieve the desired increase of the network capacity. the transmitter and the receiver are moved closer together, creating a multi-layer heterogeneous architecture where many low-power small cells, such as microcells, picocells, and femtocells are overlaid on high-power macrocells. small cells can be deployed anywhere by operators and end users [cit] . however, small-cell deployment on large scale is challenging. a certain level of centralization is required to manage inter-cell interference, mobility, load balancing and coordinated transmission schemes [cit] ."
"an sdn-based backhaul network is composed by a set of forwarding elements as a traditional network. however, while in traditional networks the control is distributed in every node (i.e., routing protocols are executed in every node), in sdns devices are simply forward nodes without any control function."
"this section provides numerical results to validate the effectiveness of the proposed sdn-based routing approach. as stated in section 5, the aim of our paper is to minimize the total amount of utds, which is the requested capacity that the backhaul network is unable to support due to links overloading. as a consequence, we are interested in evaluating the total amount of utds achieved with the proposed method."
"where the lefthand system is subject to an lts allowing remote querying whereas the righthand monitor is subject to an lts that prohibits it. intuitively, if the behaviour is preserved when certain internal moves are prohibited, this means that these moves are not used (in any useful way) by the monitor."
"differently, this paper focuses on udns taking into account that: (i) udns backhaul network may not be able to support all the traffic demand generated by cells, (ii) udns backhaul network is characterized by an irregular topology, changing in time and space, made by heterogeneous links and multi-hop connections."
"where v 2 is the set of all the possible vertex-pairs, (v, w), whose dimension is v x v. if the edge capacity, c e i,j, is lower than q e i,j, the link e i,j is not able to support all the traffic demand."
"before to explain the proposed approach, we shortly describe the methods considered to be benchmarks. being the shortest-path routing protocols the most widely used, we have considered two of these algorithms. in particular, we consider the dijkstra algorithm [cit] to find the path with the minimum cost in two different cases:"
"we define three compilation strategies, ψ o, ψ c and ψ m, corresponding respectively to monitoring using orchestration, static choreography and migrating monitoring as discussed in section 2. the compilation procedures use three parameters: two control channels (used to notify when the regular expression is to start being matched, and to notify when it has matched) and the expression to be compiled. for simplicity, all three translations follow a similar pattern shown by the block diagrams in figure 3, varying only in the location placement of the monitors and synchronisation strategy."
"the knotless barbed repair method we have described is a quick and easy technique that provides a strong repair without significantly increasing the bulk of the repair site. our novel 4-strand knotless barbed technique had comparable tensile strength with a reduced csa at the repair site in relation to the traditional 4-strand adelaide repair. furthermore, our barbed technique has no exposed barbs on the tendon surface, so there should be no attritional damage to the pulley system in vivo. the use of barbed devices for flexor tendon repairs shows promise, but further studies using animal models are warranted to examine their clinical applicability."
"existing approaches for distributed system monitoring can be broadly classified into two categories: orchestration-based or choreography-based. orchestration-based approaches relegate monitoring responsibility to a central monitor overhearing all necessary information whereas choreography-based typically distribute monitoring across the subsystems. orchestration, used traditionally in monolithic systems, is relatively the simplest strategy and its centralisation facilitates the handling of dynamic contracts. the approach is however susceptible to data exposure when contacts concern private information; it also leads to considerable communication overhead across locations, and poses a security risk by exposing the monitor as a central point of attack. by contrast, choreography-based approaches push verification locally, potentially minimising data exposure and communication overhead. communication between localised monitors is typically substantially less than that induced by the remote monitoring of a central monitor. choreography is however more complex to instrument, as contracts need to be decomposed into coordinating local monitors, is more intrusive, burdening monitored subsystems with additional local computation, and is applicable only when the subsystems allow local instrumentation of monitoring code. choreographed monitors are also instrumented upfront, which may lead to redundant local instrumentation in the case of consequential contracts; if monitoring at location k is dependent on verification at location l, and the check at l is never satisfied, upfront monitor instrumentation at k is never needed."
"proof. let n be the size of t. loop 2-4 requires the lca mapping between t and s, and the identification of ad and nad vertices. as the lca mapping can be computed in linear time [cit], testing whether a tree t is md-consistent with s can be tested in time o(n). clearly, loop 6-8 can be executed in time o(n). as for loop 9-13, it has the time complexity o(c) of wmast. therefore, the complexity for one execution of the recursive algo-rithm correct-tree is o(c). as in the worst case the algorithm can be executed (n) times, the total worst case running time is o(nc)."
"finally, we formulate the generalization of the mast problem to weighted trees as follows, where the value v(w ) of a weighted tree w is the sum of its leaves' weights."
"the goal of the desired genetic algorithm would be to find a table where for each sign neighborhood pattern (v k ) we have a sign prediction (s i,j ) for coefficient c i,j . there is no an univocal relationship between a neighbor sign combination, i.e not always for a same v k pattern, s i,j is always positive or negative. however, it is possible that for a v k pattern, s i,j is more probably to be positive or negative. but, the problem is still more complex, because a sign prediction for a neighbor sign pattern could fit well for an image and not for others. therefore, the idea is to find suboptimal neighbor sign pattern predictions that better fit for a representative set of images."
"in our method, we use a traditional blur assessment method as an initial quality estimator for each image, and adapt its result to the particular context at a later stage."
"we believe that the proposed approach is the rst step towards user-aware photo assessment methods. modeling the entire spectra of user preferences is indeed a big challenge, and in this research we attempt to address one of the multi-dimensional quality aspects. however, adapting to the collection context already allows us to model the photographer's intentions, skills and capturing environment to some extent. in this work, the three context levels are considered as equally important. nevertheless, further study of the in uence of each context level on the nal selection of users could provide useful insights on the relative importance of z-scores from di erent levels in the weighting step. finally, evaluating the performance of our approach with other image quality assessment metrics is also an interesting direction for future research, as well as searching for a solution that would allow us to combine different types of quality scores into a more complete image quality assessment."
"constraint c: for each nad vertex x of t, if y is an ancestor of x that is a duplication vertex, then y is a nad vertex."
"another interesting observation that can be made from the roc curves concerns the maybe labels. both the independent method and our context-dependent method suggest performance that is close to labeling by chance for that label. indeed, the introduction of the maybe labels makes the task of automatic pre-labeling more challenging. despite the ambiguity that a maybe label may introduce, we have opted for including it in our pre-labelling of images to facilitate the task of the end user. our approach is aimed as a pre-process prior to the user's nal selection of images, enabling them to browse, and decide which images they want to keep or not more e ciently. given that our approach relies on objective image assessment criteria and therefore cannot consider more subjective preference aspects, using a binary pre-labelling would oblige users to review all photos for making their nal selection. in contrast, using a maybe label, allows us to use the more de nite accept and reject labels with higher con dence. as such, the end user in this case would only need to verify images with a maybe label assigned. table 2 ). higher value indicates better performance."
"as the initial photo albums are of di erent size, for our analyzed collections we extract 100 photos from each album, in their original consecutive order, hence keeping the scene context unaltered. [cit] pixels. in this manner, ve collections, each of 100 photos, are created. example clusters of three collections along with the statistics on their sharpness level (section 3.2) can be seen in fig.4."
"the merging process continues recursively until only a single cluster is left, where all clusters are merged together into one hierarchical binary tree. it is worth noting that in our case there will be pairs of images with no matches present between them, thus leading to in nite similarity distance (denoted as x in fig.3a) . in this case, the merging stops when one cluster is achieved or when the distance between all remaining clusters is in nite. thus, it is possible to obtain multiple, unconnected hierarchical trees. an example of such case is demonstrated in fig.3b, where two trees are obtained, due to the absence of similarity between the two scenes. note that the depth of branches in the hierarchical tree corresponds to the similarity distances between the two connected clusters."
"in order to determine the compression performance of the proposed sign coding scheme alone, we have selected a wavelet non-embedded encoder (ltw), and one embedded encoder like spiht. as no sign coding technique is used, the sign information is raw encoded, so it would require so many bits as the number of significant coefficients (one bit per significant coefficient). the sign coding results were obtained for different image target bit-rates, counting the total number of significant coefficients to be encoded just after quantization. for example, the ltw encoder at 1 bpp has to encode 45,740 significant coefficients requiring 45,740 bits for coding sign information. if we include the proposed sign encoding technique the encoded sign information will be reduced to 37,804 bits, representing a 17.35% of bit-rate savings. to show the savings on the other selected wavelet encoder (spiht), we proceed in a similar way. first we obtain the total number of significant coefficients after quantization. then we perform a lineal estimation of bits saved, assuming that there is the same relative gain than in ltw, since both encoders use the same dwt filter. in table 3 we show the relative compression gains with respect to the original encoder due only to the sign coding capability for several test images. as we can see, the maximum sign compression gain is 17.35% for barbara image at 1 bpp. as expected, the compression gain is higher at low compression rates because the sign prediction model has been performed using all wavelet coefficients sign information. as the compression rate increases, the number of non-significant coefficients increase, loosing prediction performance since the nsps with some non-significant neighbour would be more and more dominant what it is just the opposite situation when computing the sign prediction table. this effect results in lowering the compression gains up to a 5.42% for lena image at 0.125 bpp."
input: a weighted tree w on g and a species tree s for g; output: a weighted tree w max included in w such that it is md-consistent with s and of maximum value.
"x, such an operation should result in removing the duplication vertex y. in other words, x and y should map to the same vertex s in s. moreover the result of the leaf removal from t i x should result in a different lca mapping for x and y. indeed, removing leaves from the corresponding subtree in t i does not contribute to eliminating any nad from t i . it follows that s should exhibit the phylogeny ((a, b, c), d), which is a contradiction with the result of the last paragraph."
"using the obtained context information, an independent quality score is adapted according to surrounding photos and transformed into a user friendly selection label. for demonstrating our framework, we focus on sharpness as the quality criterion, however our system can be extended to include further assessment criteria. fig.2 shows an overview of our clustering and assessment framework."
"we have opted to demonstrate our framework using sharpness as the underlying image quality criterion. accordingly, a photo is suggested to be removed when its sharpness is not su cient (or, in other words, a photo is too blurred) in the context of the collection. among the criteria leading to image rejection, blur in its di erent forms is one of the most important factors [cit], as it can a ect both professional and amateur photographs and it is hard to remove in post-processing. at the same time, the sharpness requirements largely vary depending on content type and user intentions, hence the need for context-aware adaptation."
the genetic algorithm takes as input data a set of test images to determine the best prediction of wavelet coefficient signs. the corresponding fitness function will score individuals whose predictions maximize sign compression rate.
"in this section, we present a general algorithm, that is exact in the case of a uniquely leaf-labelled gene tree (section \"uniquely leaf-labelled gene trees\") or a gene tree satisfying constraint c (section \"no ad above nad\"), and a heuristic in the general case. we first introduce preliminary definitions. for a given tree u (weighted or not), consider the two following properties on u:"
"based on observations pointing to nad vertices of a gene tree as indicating potentially misplaced genes, we developed a polynomial-time algorithm for inferring the minimum number of leaf-removals required to transform a gene tree into an md-tree, i.e. a tree with no nad vertices. the algorithm is exact in the case of a uniquely leaf-labelled gene tree, or in the case of a gene tree that does not contain any ad vertex above a nad vertex. in the general case, our algorithm exhibited near-optimal results under our simulation parameters. unfortunately, nad vertices can only reveal a subset of misplaced genes, as a randomly placed gene does not necessarily lead to a nad vertex. our experiments show that, on average, we are able to infer 40% of misplaced genes. however, the additional damage caused by a misplaced leaf leading to a nad is an excessive increase of the real mutationcost of the tree. therefore, removing nads can be seen as a preprocessing of the gene tree preceding a reconciliation approach, in order to obtain a better view of the duplication-loss history of the gene family. another use of our method would be to choose, among a set of equally supported gene trees output by a given phylogenetic method, the one that can be transformed to an md-consistent tree by a minimum number of leaf removals."
"finally, we would like to emphasize that our context-adaptive assessment does not aim to achieve the best results in the task of blurred image detection itself. since our adapted scores rely on the original scores provided by the independent quality assessment method used in each case, they are implicitly dependent on the e ectiveness of the image quality criterion chosen. the exibility of our framework however, allows for di erent criteria to be used interchangeably."
"a more conservative strategy that can be used to reduce the risk of inferring a wrong species tree, is to remove the minimum number of species from g such that the forest f restricted to the new genome set is an md-forest."
"generally, as a pre-selection step, users tend to remove clearly awed photos, such as for example blurred or wrongly exposed ones, to facilitate the subsequent step of subjective selection, where aspects related to the depicted scene or people present in the photo are assessed. detection of such low-quality photos is generally assumed to be an objective process that can be performed on each image independently."
"a more general definition is given in the literature, where the mast problem is defined on a set of uniquely leaf-labelled trees as the largest tree included in each tree of the set. this definition is equivalent to ours in the case of a gene tree t and a species tree s."
"in our work however, we aim to assess the photos at a very early stage of pre-selection, when a large collection of photos is just captured and needs to be organized. the recently proposed photo triage dataset [cit] partially addresses these problems, as this dataset contains photos taken by a wide range of users and is organized into series of similar shots from the same scene. nevertheless, these photo series still lack the context of the corresponding containing collections, which better re ect the overall intentions and skills of a photographer."
"finally, a subtree u x of u, for a given vertex x, is said to be a maximum subtree of u verifying a given property p if and only if u x verifies property p and, for any vertex y that is an ancestor of x, u y does not verify property p."
"whether relying on speci c criteria or a combination of multiple factors, most methods assess general image quality or particular defects by analyzing each image independently, and not considering the individual preferences, the photographer's skills, the capturing environment, or the camera properties."
"the lca mapping induces a reconciliation m(t, s) between t and s, where an internal vertex t of t leads to a duplication vertex in m(t, s) if and only if t is a duplication vertex of t with respect to s. in other words, the duplication cost of m(t, s) is d(t, s) (see for example [cit] for more details on the construction of a reconciliation based on the lca mapping). moreover, m(t, s) is a reconciliation that minimizes the duplication, loss, and mutation costs [cit] ."
"similarity-based distance. each obtained temporal window is further clustered in a hierarchical manner, providing scene-level clusters and near-duplicate clusters of images. before clustering, a similarity-based distance metric is computed between all images. our distance metric is based on the sift features [cit] ], due to their advantage in identifying the image matches even in presence of distortions (which often appear in the series of similar photos). for each pair of images within a temporal window, two sets of sift descriptors are compared using the euclidean distance."
"in addition, we would like to discuss a few aspects of our results visualization, which provides a user-friendly way of viewing the photo collection. previous approaches that rely on hierarchical clustering for organizing photographs present images in a tree browsing structure, where one representative image replaces the contents of the cluster [cit] . however, in the typical photo viewing and managing software (e.g. picasa, lightroom) at representations are preferred, with ags and other identi ers used to label photographs."
"in this section, we consider a tree t containing no ad vertex above a nad vertex (figure 3a ). more precisely, t satisfies constraint c below:"
"notice however that a misplaced gene in a gene tree t does not necessarily lead to a nad vertex. in other words, nad vertices can only point to a subset of misplaced leaves. however, in the context of reconciliation, the damage caused by a misplaced leaf leading to a nad vertex is to significantly increases the real mutation-cost of the tree, as shown in figure 2 ."
"in order to determine the impact of the proposed sign coding technique in the encoder overall performance, we implemented our sign coding proposal in the ltw wavelet encoder [cit], calling it s-ltw. after that, we performed several experimental tests comparing s-ltw encoder with ltw and spiht (spiht 8.01) in terms of r/d. the test images used in the evaluation were: lena (512x512), barbara (512x512), bike (2560x2048), goldhill (512x512), cafe (2560x2048), peppers (512x512), zelda (512x512) and woman (2560x2048)."
"with our approach, we construct a at representation and rely on enclosing borders to identify clusters at di erent context levels (temporal, scene-level, near-duplicate), as shown in fig.4 . the selection labels can be assigned to images, similar to decision ags in popular photo management software, allowing the user to quickly understand the structure of their collection and perform their nal decision, without having to navigate several levels within a tree structure."
"(1) collection level statistics can be used to re ect the features of the capturing devices and skills of a photographer. (2) scene level statistics focus on images of the same general scene but not necessarily completely overlapping, and aim to re ect environment properties, such as in uence of illumination."
"first, for each image, an independent image quality score is computed based on the selected quality criteria (fig.2a) . second, an input collection is grouped into temporal windows, and then each temporal window is hierarchically clustered into scene-level clusters and near-duplicate clusters (fig.2b) . these clusters are used [cit] ."
"the hl subbands of a multi-scale 2-d wavelet decomposition are formed from low-pass vertical filtering and high-pass horizontal filtering. the high-pass filtering detects vertical edges, thus the hl subbands mainly contain vertical edge information. oppositely defined are the lh subbands that contain primarily horizontal edge information."
"to that end, we propose a novel framework that facilitates the process of photo pre-selection by evaluating image quality in the context of the collection in which it belongs and the relevant photos captured in the same scene. given a collection of photos, originating from the same user and equipment, and representing a single event (e.g. trip, holiday, party, wedding), we employ a hierarchical clustering approach to organize collection into similarity-based clusters and de ne the context of each image."
"we define the nad-border of u as the set of roots of the maximum subtrees of u verifying property only-nad, and the ad-border of u as the set of roots of the maximum subtrees of u verifying property only-ad."
"where z i c is an image score on the collection level, z i sc is a scene level score, z i n d is a near-duplicates level score, and n z is the number of de ned z-scores for this image (number of non-singleton clusters). unde ned z-scores for a given image are set to 0 for this calculation."
"algorithm correct-tree is a recursive algorithm that takes as input a gene tree t and a species tree s, and outputs a number of leaf removals transforming t into a tree that is md-consistent with s. it proceeds as follows:"
"since our approach performs photo evaluation based on multilevel context rarely available in publicly available datasets, we decided to evaluate its e ciency with a user study, performed on 5 photo collections of di erent content type. the rst collection consists of travel photos, taken mostly outdoors, with a small number of low-quality images. the second collection consists of wedding photos, captured indoors in di cult lighting conditions, and containing a noticeable number of blurred pictures. the third collection is a photo session conducted by a professional photographer (the photos are extracted before any processing is applied to them), with a consistent level of photo quality. the fourth collection covers a sport event (a volleyball match), where multiple pictures present motion blur due to the players' movements. finally, the fth collection is taken during a halloween party and presents cases of out-of-focus and motion blurred photos. all collections, except the professional collection, were acquired from the photo albums in yfcc100m dataset [cit] . the professional photo session is acquired directly from a photographer."
let t be a gene tree for a gene family on g. we suppose that t is not an md-tree consistent with s (i.e. there is at least one duplication vertex of t that is a nad vertex). we begin by describing special classes of gene trees for which exact polynomial-time algorithms have been developed for the minlrr and minsrr problems.
"in other words, solving the minlrr problem on t is equivalent to solving the wmast problem on t i . we show in the proof of theorem 2 that wmast can be solved by the traditional mast algorithms with no change in the asymptotic running time."
"the use of genetic algorithms to compress the sign of wavelet coefficients is twofold. first, when the number of neighbors used to analyze the sign correlation grows or when there is a great number of images to be used in the analysis, the search space is excessively wide. second, it is not intuitive to find a way of combining the predictions obtained for several images."
"after individual image quality scores are obtained, they can be adapted to the context of each image. this allows our framework to consider the quality of the image in relation to images surrounding it (same scene, taken at a similar time and so on). in this section, we describe how the context of each image is determined. as a rst step, the entire collection is organized into time-based groups. then, inside each time group, a hierarchical tree is constructed, using the proposed image similarity metric. finally, each time group is clustered into two levels: scene level and near duplicate level, which group the photos according to the visual similarity between them."
"the latter two optimization problems (minsrr and minsri) are the subject of the next section. section \"algorithms for the minimum removal reconciliation problems\" focuses on the two optimization problems related to reconciliation (minlrr and minsrr)."
"in a previous work [cit] we have observed that the sign of a wavelet coefficient may be strongly correlated with the sign of some neighbor coefficients. however, this relationship is not uniform and constant for any image, or even consistent within the same image. thus, although a careful analysis for the target image could be done in order to get the most accurate sign relationships and therefore better sign prediction and improved compression rates, these sign relationships would be only useful for this image. by increasing the number and kind of images under analysis, the relationship between the signs of the neighbor coefficients may be generalized."
(3) near-duplicate level information focuses on images that depict the same scene or object with minor variations due to occasional user mistakes and other arbitrary changes between similar images.
"on the other hand, let y be the parent of x in t i . as an optimal solution of the wmast problem on t i removes leaves from the subtree t i"
"using the similarity distances computed in the previous step, the clustering is performed in a bottom-up approach, where each image initially belongs to a cluster containing only the image itself. at each step, the two clusters with the lowest similarity distance are merged together into a new cluster. the distance between singleimage clusters can be directly retrieved from the similarity distance matrix. the distance between two clusters with multiple images c 1 and c 2 is de ned as the shortest distance over all possible pairs of images, where one image from a pair belongs to the cluster c 1 and another image belongs to the cluster c 2 (this way of cluster merging is also known as the single-linkage criteria)."
"the user study has been performed with 15 participants (9 male and 6 female), where each participant regularly takes personal photos in every-day life and is familiar with the task of photo selection and organization. two of the participants occasionally use professional cameras, and can be considered as experts. every participant was presented with each of the collections (selected in a random order), and was given the task of labeling sharp and non-sharp photos. the user could assign one of three labels: accept if he considered a photo sharp enough to keep, reject if a photo was too blurry and not worth keeping, and maybe if a photo was not absolutely sharp but still worth keeping or if they could not otherwise make a decision. a user could freely browse through the entire collection during the labeling process, as in a typical real-life selection scenario, and zoom to view the images at full resolution."
"almost all genomes which have been studied contain genes that are present in two or more copies. duplicated genes account for about 15% of the protein coding genes in the human genome, for example [cit] . in practise, homologous gene copies (e.g. copies in one genome or amongst different genomes that are descended from the same ancestral gene) are identified through sequence similarity; using a blast-like method, all gene copies with a similarity score above a certain threshold would be grouped into the same gene family. using a classical phylogenetic method, a gene tree, representing the evolution of the gene family by local mutations, can then be constructed based on the similarity scores. however, macroevolutionary events (duplications, losses, horizontal gene transfer) affecting the number and distribution of genes among genomes [cit], are not explicitly reflected by this gene tree. having a clear picture of the speciation, duplication and loss mechanisms that have shaped a gene family is however crucial to the study of gene function. indeed, following a duplication, the most common occurrence is for only one of the two gene copies to maintain the parental function, while the other becomes nonfunctional (pseudogenization) or acquires a new function (neofunctionalization) [cit] ."
"by linking the species tree inference problem to a supertree problem we have been able to prove that deciding whether a gene tree t is an md-tree can be done in polynomialtime [cit] . we used a constructive proof based on a min-cut strategy, which has been largely considered in the context of supertrees [cit] . in this section, we develop a greedy heuristic for minsri based on a minimum vertex cut strategy."
input: a uniquely leaf-labelled gene tree t on g and a species tree s for g; output: a tree t max included in t such that it is mdconsistent with s and of maximum size.
"applying a classical phylogenetic method to the gene sequences of a given gene family leads to a gene tree t that is different from the species tree s, mainly due to the presence of multiple gene copies in t, and that may reflect a divergence history different from s. the reconciliation approach consists in \"embedding\" the gene tree into the species tree, revealing the evolution of the gene family by duplications and losses."
"when the considered gene family contains at most one gene per genome, the gene tree t is uniquely leaf-labelled. in this case, minimizing the number of leaves, or equivalently species, that should be removed from t to obtain an md-tree consistent with s is equivalent to finding the maximum number of genomes that lead to the same phylogeny in t and s. in other words, it is immediate to see that the minlrr problem reduces, in this case, to the mast problem given below."
"given a tree u on a genome set g, a leaf removal consists of removing a given leaf from u, along with its parental node x, and if x is not the root joining the parent of x and the remaining child by a new edge. a tree u obtained from u through a sequence of leaf removals is said to be included in u. the tree u restricted to a subset g of g is the tree u obtained from u through a sequence of leaf removals that removes all the leaves with labels in g \\ g ."
"in the general case, nad-border(t i ) is not restricted to a single vertex, and loop 9-13 can be executed many times. moreover, at the end of loop 9-13, the resulting tree is not guaranteed to be md-consistent with s, as nad vertices higher than those in nad-border(t i ) may exist. algorithm correct-tree may therefore be applied many times. theorem 2. algorithm correct-tree has time-complexity o(n 2 lg n), where n is the size of t."
"several parameters should be taken into account when training a genetic algorithm: the population size, the individuals initialization, the number of iterations performed, the mutation probability, the crossover point, the crossover method, the selection criteria of the best sequences, etc. we have performed lots of tests varying these parameters to tune the genetic algorithm [cit] . the parameters used to obtain the sign prediction are: population size (100), individuals initialization (randomly), number of iterations (1000), mutation probability (0.075%), mutation policy (inversion of a randomly chosen gen), crossover point (randomly) and crossover method (best two fitness individuals over four randomly selected parents)."
"in this paper we evaluate our sign coding technique proposal based on the use of genetic algorithms. we developed a genetic algorithm that will find the best sign prediction. it will work with a large set of test images in order to obtain a good sign prediction that be image independent. the evaluation will be performed over a non-embedded wavelet base encoder called ltw [cit], showing the benefits of minimizing the entropy of sign information resulting from the sign prediction coding process."
"to extract this necessary context information, we need to nd natural boundaries between di erent captured scenes within a collection, which can be achieved using clustering methods. photo album clustering is typically obtained using temporal [cit] ] or similarity [cit] information. among clustering approaches, hierarchical clustering takes a prominent place. as it does not require a pre-de ned number of clusters, it is versatile in its application and suitable for di erent types of data, and, in particular, images. hierarchical clustering is often applied for creating tree-like representations for image browsing, for example based on color histogram similarities [cit] or on geo-location information [cit] . in our proposed method we also rely on hierarchical clustering, but, in contrast to previous methods, our clustering approach is based on similarity computed from matches de ned over sift descriptors. more importantly, instead of a typical tree-like browsing structure, we obtain a at representation of the entire collection, which can serve as a convenient way of image browsing (which is demonstrated in section 4.4)."
"in this manner, a z-score is computed on three levels, where each level provides an estimation how good the photo in the context of entire collection, its containing scene and among very similar nearduplicate photos. if the image is unique at its level (for example, no other similar photos have been taken in the same scene), then no other images are available for the computation of the statistics information, and we consider its z-score as unde ned."
"although a number of image datasets for aesthetic assessment (with corresponding evaluations by users) is publicly available, such as the photo.net dataset [cit] or the ava dataset [cit], these datasets are created to address the task of independent image assessment. thus each image is given without any context information about the original collection. moreover, these datasets are acquired from photographers' peer-review social networks, hence the majority of presented photos are high-quality post-processed photos, which represent the nal outcome of a photographer's work."
"suppose this is not the case. let y be the vertex of t representing the least common ancestor of all leaves labelled s in t. then y is an ad node. as a leaf i labelled s is removed from t, such removal should contribute to resolving a nad vertex x of t. from constraint c, such a vertex should be outside the subtree of t rooted at y. moreover, it should clearly be an ancestor of y (otherwise removing i will have no effect on x)."
"the rest of the paper is organized as follows: section 2 describes our contextbased sign coding framework and introduces a brief description about genetic algorithms and their application to sign coding prediction. in section 3, we show"
"the mast problem arises naturally in biology and linguistics as a measure of consistency between two evolutionary trees over species or languages [cit] . in the evolutionary study of genomes, different methods and different gene families are used to infer a phylogenetic tree for a set of species, usually yielding different trees. in such a context, one has to find a consensus of the various obtained trees. considering the mast problem, introduced by finden and gordon [cit], is one way to obtain such a consensus. [cit] showed that computing a mast of three trees with unbounded degree is np-hard. however, in the case of two binary trees, the problem is polynomial. the first polynomial-time algorithm for this problem was given by steel and warnow [cit] . it is a dynamic programming algorithm considering the solution for all pairs of subtrees of t and s; it has a running time of o(n 2 ), where n is the number of leaves in the trees. later, [cit] developed an o(n lg n) time algorithm, which, as far as we know, is the most efficient algorithm for solving the mast problem on two binary trees. we use this result in the minlrr version of our algorithms. in the case of k binary trees, the current fastest known algorithms run in o(kn 3 ) time [cit] . we use this result in the minsrr version of our algorithms."
"with digital photography and low storage costs, users have the ability to capture many photos of the same scene, object or event with little overhead in terms of cost or e ort -an estimated 1.1 [cit] alone 1 . however, this increase in the number of photos captured, brings a signi cant overhead in (d) an independent image score is re-estimated for the three context levels, leading to a nal scoring and labelling of the photo collection."
"existing techniques are capable of assessing sharpness or blur in individual images, but they cannot be easily adapted to the nature of particular collections. for example, a collection captured in di cult illumination conditions by an amateur photographer might exhibit a large number of blurred photos. however, typical blur assessment methods are pre-trained on a wide range of photos, thus they can underestimate and reject many photos in a low-quality collection."
"this is easy to see; take the species tree s given by the instance of minsrr and add it to the forest f for the minsri problem. the solution to minsri gives a species tree that must be a subtree of s. thus, any algorithm for minsri can be used to solve minsrr."
"however, this is not always true, in other words, a duplication vertex of t with respect to a species tree s is not necessarily an ad vertex. we call such a duplication vertex a non-apparent duplication vertex, or simply a nad vertex. for example, the tree of figure 1b contains one nad vertex, indicated by a square, and thus t is not md-consistent with s."
"our complete solution was implemented in matlab, and timings for processing photo collections were measured on an intel core i7 pc 2.80 ghz with 16 gb ram, running windows 7 64-bit. using our current framework, a collection of 100 images of hd resolution can be fully processed in about 90 seconds, with a linear growth to 460 seconds for 500 images. the clustering step takes around 40% of the execution time, and the sharpness estimation step takes around 55% of the execution time."
"the cluster-based context visualization of collections is implemented in a form of auto-generated html pages, which show image thumbnails with suggested labels, in a manner similar to image browsers (examples are shown in fig.4) . the html page can be freely browsed and zoomed without a ecting the frames delimiting photo clusters."
"nevertheless, users implicitly consider the characteristics of the entire collection, as well as surrounding photos, to make a decision for a particular image. for example, photos taken by a nonexperienced user with a smartphone camera might present more blur and other artifacts, compared to photos acquired by a professional photographer in a studio. in a non-professional scenario, the user is likely to keep the best photos of their collection as a representation of an event, even if they are not objectively perfect photos. as an opposite example, pre-selection of sharp photos taken in a professional photo session presents stricter quality requirements, especially when numerous similar photos from the same scene are available. regardless of the user skills and collection characteristics, if many photos of a particular object, landscape or moment are available, the user might apply stricter criteria to select whether to keep each instance or not. at the same time, they might be more lenient in their decision if only a single version is available. therefore, the notion of quality context is important, and computation of context characteristics should be considered in the process of photo pre-selection."
"hierarchical tree construction. we chose the hierarchical clustering approach, as it does not require a pre-de ned number of clusters, and it allows to create a speci c cluster structure, where clusters of di erent similarity are enclosed into each other, as it explained below. such structure allows to re ect natural organization of photos, where photos are usually considered in connection with similar photos from the same scene."
"fleiss' kappa agreement to demonstrate the e ectiveness of our approach, we have conducted a user study, where people's pre-selection decisions have been collected for various types of collections. then, the obtained data has been used to verify the bene t of our context-aware method in comparison with independent methods for detection of low quality images, which do not adapt to the collection properties."
"score. furthermore, an incorrect score assigned to one image from a multiple-images cluster can negatively a ect score adaptation for the surrounding images. as a second challenge, the context extraction by clustering can be unreliable in some conditions. photos of low quality, especially captured in low-light conditions, often lack reliable information for the sift matching, thus leading to unreliable clustering and inaccurate context-adaptation as a result."
"the nal step of our framework is the adaptation of individual image scores computed in section 3.2, according to the context extracted at di erent levels (section 3.3). the context of each image is de ned on three levels: collection level, scene cluster level and near-duplicate cluster level. in the following we refer to the three levels using the c, sc, n d identi ers. to obtain context-dependent image scores, we utilize the z-score [cit] ] (based on the assumption that the independent scores follow gaussian distribution), which can be computed from the independent image score s i as follows:"
"removing the minimum number of species instead of leaves can also be considered in the case of reconciliation, a scenario that may be applicable when full confidence is not put in the species tree."
the obtained sharpness score is used as an individual image score in our approach. sharpness histograms for three di erent photo collections are shown in fig. 4 . context-based adaptation of the obtained score is described in section 3.4.
"the per-image independent quality score computed initially is then re-estimated for the three context levels, de ned by collection, scene-level and near-duplicate level statistics (fig.2d) . finally, the weighted sum of context-based scores for di erent levels is used to obtain a global scoring and subsequent labeling of each image, agging images as accept, reject or maybe, similar to common photo editing work ows."
a complete example of the algorithmic methodology used for solving the minlrr problem on t and s is given in figure 3 . the algorithm will be detailed in the next section.
"in this section, we assume that a species tree s is known for the genome set g. for simplicity, we present the algorithms in the case of a single gene tree t, although it is straightforward to generalize them to the case of a forest of gene trees."
"most wavelet image codecs do not consider the use of sign coding techniques since the wavelet coefficients located at the high frequency subbands form a zero-mean process, and therefore they are equally likely positive as negative."
"our framework can be adapted to other independent metrics or extended to include multiple factors, such as exposure issues or noise. in e ect, any existing method can be used to obtain initial independent image quality scores, as long as the scores provided are de ned within a limited range of values (e.g. 0 to 1, not 0 to in nity)."
"the intuition behind the z-score is that it normalizes score values around the mean value of the corresponding level, therefore adapting to its quality statistics. examples of collection-level distributions are shown in fig. 4, showing the necessity of such adaptation: overall sharpness can vary signi cantly across di erent collections."
"non-maximum suppression is applied to process all maps into the equivalent scale, using scale-dependent local windows. depending on the edge properties across scales, each pixel is assigned an edge or non-edge value, and each edge pixel is further assigned one of four edge structure types: dirac-structure, roof-structure, astep-structure and gstep-structure. in particular, dirac-structure and astep-structure pixels are considered sharp (as they tend to disappear in presence of blur), and their total number n da, along with the total number of edge pixels n ed e is used to compute a nal sharpness score:"
"additional performance analysis was conducted using receiver operating characteristic (roc) curves, which were computed for each of three labels over the all evaluated collections. the shape of the curves, shown in fig.5, as well as the area under the curve measures, given in table 3, demonstrate better performance of our method for each label type. it can be noted that the independent method tends to correctly identify photos pre-labeled by users for rejection, while it does not show high performance on two other labels. this agrees with our intuition that the non-adapted method is likely to reject the lower quality photos, while users try to nd acceptable photos even among imperfect ones."
"in this work, we propose a novel context-aware approach of photo assessment, where an independent image quality metric can be adapted to the content of an analyzed photo collection. our approach extracts the context of each photo by means of hierarchical clustering, where each level re ects di erent degrees of similarity between photos. along with statistical information, our clustering method provides a at collection representation with intuitive indications of the scene boundaries, which can be used to assist the user in the task of photo pre-selection. the extracted context information is used to adapt the initial image quality score for each level of context and compute a nal weighted score for every image, from which an output photo label is obtained. the conducted experiments demonstrate that we are able to model user pre-selection behavior, and that the context-adapted score performs signi cantly better than the original computed score in the scenarios where pre-selection decision cannot be made independently."
"in this section we analyze the behavior of the proposed sign coding technique. for this purpose, we will first analyze the bit-rate savings over a set of test images of different sizes and target bit-rates. so we will evaluate the potential bit-rate reduction that whatever wavelet encoder without sign coding algorithm could obtain if our sign coding proposal is included. before performing the experiments we have trained and tuned our proposed ga, and we have run it three times, one for each subband type over the whole kodak image set [cit] in order to get the corresponding prediction tables. those tables were codewired in both encoder and decoder."
"we have presented a study about sign coding for non-embedded image encoders. we propose a simplified context model formation that it is oriented to maximize the successful prediction of the sign for every significant wavelet coefficient. in order to obtain a good sign prediction, we employ a genetic algorithm specially trained and tuned with a representative image set. the prediction result is encoded with an adaptive arithmetic encoder to compact the sign information as much as possible. we have implemented it over the ltw encoder in order to evaluate the sign context model behavior. the new proposed sign coding technique is able to reduce the sign information with a compression gain up to 17.35% which lead in a r/d performance increase up to 0.37 db (as experimental results show), being greater the improvement at low and medium compression rates. also, we have shown that other encoders like spiht will obtain similar improvements when including the sign prediction model, being directly applicable to whatever nonembedded wavelet encoder."
"to evaluate the performance of the proposed approach, the output labels of our method were compared with the labels given by each participant in the experiment described above. the metrics of accuracy, precision, recall and f-measure [cit] ] were calculated for each label and user separately, and average values of these metrics were computed across all participants. the performance comparison is given in table 2 . for each metric, a higher score indicates better performance. both precision and recall values are important to estimate the correctness of label prediction in our task, therefore the f-measure is a good indicator of the overall performance, since it is computed as the harmonic mean between precision and recall."
"the described preferences-based approaches perform the analysis of photos independently, where the information about other photos, captured in the same conditions, is not taken into consideration. the knowledge of the photo context -characteristics for the entire album and speci c scenes -could potentially assist the process of automatic photo assessment and selection, even without explicit user modeling."
"based on the computed pair-wise image similarities d(i, ), we obtain a similarity distance matrix (an example is shown in fig.3a) . the distance matrix is computed within each temporal window and then used to perform hierarchical agglomerative clustering [cit] ."
"the goal of our framework is to simultaneously organize and assess the quality of photographs belonging to a photo collection by considering not only each image independently but also its surrounding context. to achieve this, our approach consists of the following stages, which are illustrated in fig.2 ."
"in figure 1 we show the genetic algorithm pseudo code for sign prediction. first of all we define each individual, containing a sign prediction for each 3 3 nsp, then each nsp sign prediction of each individual of the universe is randomly initialized as a positive or negative sign. then, during evolution, sequences mate and mutate to generate new individuals in the population, being selected the best ones for survival on the basis of their fitness function. the mating of sequences is performed through crossover operator, where parents are randomly selected and its gens (nsps) are mixed. the best two individuals, the ones that exhibit best prediction performance, are selected for survival. individuals can also undergo mutation, where a sequence prediction is randomly modified. finally, after performing the maximum iterations, the algorithm finishes, obtaining an optimal/suboptimal sign prediction for each nsp."
"if t satisfies constraint c, then nad-border(t i ) is also reduced to the root of the whole tree, and thus loop 9-13 is just executed once. in this case, the methodology is the one following theorem 1, and illustrated in example 3."
"in contrast, in our framework we consider the complete collection as well as di erent levels of image context simultaneously. by performing hierarchical similarity clustering, the necessary multi-level context is obtained, which gives an intuitive collection representation and provides a base for adaptive image scoring and labeling. while several previous methods are based on a prede ned criteria for image quality assessment, in our work we opt for a more exible approach that can adapt any image quality criteria to consider the context of each photo. this way, our method can provide the evaluations that are closer to user's expectations."
"for example, the tree of figure 1c is a reconciliation between the gene tree t of figure 1b and the species tree of figure 1a . such a reconciliation between t and s implies an unambiguous evolution scenario for the gene family, where a vertex of r(t, s) that satisfies property (d) represents a duplication (duplication vertex), a vertex that satisfies property (s) represents a speciation (speciation vertex), and an inserted subtree represents a gene loss."
"this task is commonly addressed with individual image aesthetics assessment approaches, where an input image is evaluated irrespective of its context [cit] . however, these approaches often tend to be modeled over average user preferences, extracted from a vast, generic range of content, thus diminishing the subjective aspect of selection and neglecting the context of other photos present in the same collection."
"this clustering, in addition to o ering a convenient structure for browsing photos, also provides the context in which quality may be evaluated, using any desired criteria. the context is modeled on three levels, with the following intuition behind each level:"
"recently, sophisticated techniques developed in the programming languages field have begun to be applied to automate repetitive and structured tasks in education, including problem generation, solution generation, and feedback generation for computer science and logic topics [cit] . closer to the subject at hand is the automated generation of mathematical word problems that are organized around themes of interest to kids, such as \"school of wizardry\" [cit] . the method allows the student to specify personal preferences about the world and characters, and then creates mini \"plots\" for each word problem by enforcing coherence across the sentences using constraints in a logic programming paradigm combined with hand-crafted discourse tropes (constraints on logical graphs) and a natural language generation step. a sample generated word problem is professor alice assigns elliot to make a luck potion. he had to spend 9 hours first reading the recipe in the textbook. he spends several hours brewing 11 portions of it. the potion has to be brewed for 3 hours per portion. how many hours did elliot spend in total?"
"salman kahn, the creator of kahn academy, talks about the \"swiss cheese\" model of learning in which students learn something only partly before they are forced to move on to the next topic, building knowledge on a foundation filled with holes, like the cheese of the same name [cit] . this is akin to learning to ride a bicycle without perfecting the balancing part. in standard schooling, students are made to move one from one lesson to the next even if they only got 70, 80, 90% correct on the test. by contrast, mastery learning requires a deep understanding, working with knowledge and probing it from every angle, trying out the ideas and applying them to solve real problems."
"in our moocchat work with triad discussions we observed that more workers will change their answer from an incorrect to a correct one if at least one member of the group starts out correct than if no one is correct initially . we also noticed that if all group members start out with the same answer -right or wrong -no one is likely to change their answer in any direction. this behavior pattern suggests an interesting idea for large scale online group discussions that are not feasible in in-person environments: dynamically assign students to groups depending on what their initial answers to questions are, and dynamically regroup students according to the misconceptions and correct conceptions they have. rather than building an intelligent tutoring system to prompt students with just the right statement at just the right time, a more successful strategy might be to mix students with other poeple who for that particular discussion point have the just the right level of conceptual understanding to move the group forward."
"these technologies have only become feasible recently because of the combination of multimedia, fast audio and image processing, fast network connectivity, and a connected population. nlp researchers may want to let their imaginations consider the possibilities that arise from this new and potent combination."
"beyond learning to write, new technology is changing other aspects of language learning in ways that should excite nlp researchers. in order to write well, a student must have a good vocabulary and must know syntax. learning words and syntax requires exposure to language in many contexts, both spoken and written, for a student's primary language was well as for learning a second language."
"there is a revolution in learning underway. students are taking massive open online courses as well as online tutorials and paid online courses. technology and connectivity makes it possible for students to learn from anywhere in the world, at any time, to fit their schedules. and in today's knowledge-based economy, going to school only in one's early years is no longer enough; in future most people are going to need continuous, lifelong education. students are changing too -they expect to interact with information and technology. fortunately, pedagogical research shows significant benefits of active learning over passive methods. the modern view of teaching means students work actively in class, talk with peers, and are coached more than graded by their instructors."
"learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases [cit] . but another important component of learning with others refers to learning with other students. literally hundreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning [cit] . in the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale [cit] . even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online [cit] ."
think. this is a confusing question. s1 i thought e contradicted the statement the most s2 me too s3 i loving hits with other mturkers table 1 : transcript of a conversation among three crowdworkers who discussed the options for a multiple choice question for a gmat logical reasoning task. note the meta-discussion about the prevalence of robots on the crowdsourcing platform.
"our work on a game for children called wordcraft takes this idea one step further [cit] ) (see figure 1) . children manipulate word cards to build sentences which, when grammatically well formed, come to life in a storybook-like animated world to illustrate their meaning. preliminary studies of the use of wordcraft found that children between the ages of 4 and 8 were able to observe how different sentence constructions resulted in different meanings and encouraged children to engage in metalinguistic discourse, especially when playing the game with another child."
"of course, for decades, the field of intelligent tutoring systems (its) [cit] has developed technology for this purpose, so what is new about what i am suggesting? first, we know as nlp researchers that language analysis requires specific technology beyond standard algorithms, and so advances in intelligent tutoring systems on language problems most likely requires collaboration with experts in nlp. and, apparently such collaborations have not been as robust as they might be [cit] . so there is an opportunity for new advances at the intersection of these two fields."
recent techniques pair with learning techniques like inductive logic programming with instructor editing to induce logic rules that describe permissible answers with high accuracy [cit] .
"perhaps the least useful feedback that an instructor writes next to a block of prose on a learner's essay is 'awkward'. we know what this means: something about this text does not read fluently. but this is not helpful feedback; if the student knew how to make the wording flow, he or she would have written it fluently in the first place! useful feedback is actionable: it provides steps to take to make improvements."
unfortunately most approaches require quite a large number of students' answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500-800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers [cit] . this high initial cost makes the development of hundreds of practice questions for a given conceptual unit a daunting task for instructors.
"this inversion can apply to other aspects of nlp technology as well. [cit] have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named \"helping our own\" shared task [cit] . using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. another option is to target practice questions tailored for learners based on errors in a fun manner (as described below)."
"the reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce activities and incentives to help students help one another. s2 i think e is the right answer s1 hi, i think e is right, too s3 hi! this seems to be a nurture vs nature question. s3 can scent be learned, or only at birth? s2 yeah, but answer a supports the author's conclusion s1 i felt that about a too s2 but the question was, which statement would weaken the author's conclusion s3 so i choose a, showing that scent can be learned at not only at birth. s2 that's why i think e is right s3 are you real, or fake? s2 real s1 i didn't think that b or d had anything to do with the statement s3 actually what you said makes sense. s1 so, do we all agree that e was the correct answer? s2 i think so, yes. s3 but i'm sticking with a since \"no other water could stimulate olfactory sites\" abd i suggests that other water could be detected. s3 *and s1 i thought about c for awhile but it didn't really seem to have anything to do with the topic of scent s3 it has to be a or e. other ones don't have anything do do with the question. s2 but that \"no other water\" thing applies equally well to e s3 e is still about spawing ground water, i"
there is much nlp research to be done to enhance the online dialogues that are associated with student discussion text beyond the traditional role of intelligent tutoring systems. one idea is to monitor discussions in real time and try to shape the way the group works together [cit] . another idea is to automatically assess if students are discussing content at appropriate levels on bloom's taxonomy of educational objectives [cit] .
"recent research in learning at scale has produced some interesting approaches to improving \"feedback at scale.\" one approach [cit] ) uses a variation on hierarchical text clustering in tandem with a custom user interface that allows instructors to rapidly view clusters and determine which contain correct answers, incorrect answers, and partially correct answers. this greatly speeds up the markup time and allows instructors to assign explanations to a large group of answers with a click of a button."
"grammar checking technology has been excellent for years now [cit], but instead of just showing the right answer as grammar checkers do, a grammar coach should give hints and scaffolding the way a tutor would -not giving the answer explicitly, but showing the path and letting the learner fill in the missing information. when the learner makes incorrect choices, the parser can teach principles and lessons for the conceptual stage that the learner is currently at. different grammars could be developed for learners at different competency levels, as well as for different first-second language pairings in the case of second language learning. this suggests a different approach for building a parser than what is the current standard. i am not claiming that this has not been suggested in the past; [cit] designed a parser to explain errors to learners. however, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics."
"and second, the newly expanded interest in online learning and technology makes possible the access of information about student writing behavior on a large scale that was not possible in the past. imagine thousands of students in cascaded waves, tasked with writing essays on the same topic, and receiving real-time suggestions from different algorithms. the first wave of student responses to the feedback would be used to improve the algorithms and these results would be fed into the next wave of student work, and so on. students and instructors could be encouraged to give feedback via the user interface. very rapid cycles of iteration should lead to accelerated improvements in understanding of how the interfaces and the algorithms could be improved. a revolution in understanding of how to coach student writing could result! algorithms could be designed to give feedback for partially completed work: partially written sentences in the case of a parser; partially completed paragraphs in the case of a discourse writing tool, and so on, rather than only assessing completed work after the fact."
"going beyond phonemes, the duolingo second-language learning application [cit] teaches syntax as well as vocabulary through a game-based interface. for instance, one of duolingo's games consists of a display of a sentence in one language, and a jumbled list of words in the opposing language presented as cards to be dragged and dropped onto a tray in the correct order to form a sentence. in some cases the user must select between two confounding choices, such as the articles \"le\" or \"la\" to modify french nouns."
another way that nlp can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. there has been significant work in this space [cit] . the standard approach builds on the classic successful model of essay scoring which compares the student's text to model essays using a similarity-based technique such as lsa [cit] or careful authoring of the answer [cit] .
"when we tested the synchronous small-group discussions in a live mooc we found that, for those students that were successfully placed into a group of 3 for discussion, they were quite positive about the experience [cit] . however, there are significant challenges in getting students to coordinate synchronously in very large low-cost courses [cit] ."
"for example, the tip tap tones project [cit] attempts to help learners reduce the the challenge of mastering a foreign phonetic system by microtasking with minute-long episodes of mobile gaming. this work focuses in particular on helping learners acquire the tonal sound system of mandarin chinese and combines gesture swipes with audio on a smartphone."
"current practice uses the output of scorers to give students a target to aim for: revise your essay to get a higher score. an alternative is to design a system that watches alongside a learner as they write an essay, and coaches their work at all levels of construction -phrase level, clause level, sentence level, discourse level, paragraph level, and essay level."
"in many cases, mastery learning also requires practicing with dozens, hundreds, or even thousands of different examples, and getting feedback on those examples. automation can help with mastery learning by generating personalized practice examples that challenge and interest students. automatically generated examples also reduce the cost of creating new questions for instructors who are concerned about answer sharing among students from previous runs of a course."
"in this paper i am suggesting inverting the standard mode of our field from that of processing, correcting, identifying, and generating aspects of language to one of recognizing what a person is doing with language: nlp algorithms as coaches rather than critics. i have outlined a number of specific suggestions for research that are currently outside the mainstream of nlp research but which pose challenges that i think some of my colleagues will find interesting. among these are text analyzers that explain what is wrong with an essay at the clause, sentence, discourse level as the student writes it, promoting mastery learning by generating unlimited practice problems, with answers, in a form that makes practice fun, and using nlp to improve the manner in which peers learning takes place online. the field of learning and education is being disrupted, and nlp researchers should be helping push the frontiers."
"in our moocchat research, we were interested in bringing structured peer learning into the mooc setting. we first tried out the idea on a crowdsourcing platform, showing that when groups of 3 workers discussed challenging problems together, and especially if they were incentivized to help each other arrive at the correct answer, they achieved better results than working alone. (a sample conversation is shown in table 1 .) we also found that providing a mini-lesson in which workers consider the principles underlying the tested concept and justify their answers leads to further improvements, and combining the mini-lesson with the discussion of the corresponding multiple-choice question in a group of 3 leads to significant improvements on that question. crowd workers also expressed positive subjective responses to the peer interactions, suggesting that discussions can improve morale in remote work or learning settings."
a challenge for the field of nlp is how to build writing tutors or coaches -as opposed to graders or scorers. there is a vast difference between a tool that performs an assessment of writing and one that coaches students to help them as they are attempting to write.
"in this new world of education, there is a great need for nlp research to step in and help. i hope in this paper to excite colleagues about the possibilities and suggest a few new ways of looking at them. i do not attempt to cover the field of language and learning comprehensively, nor do i claim there is no work in the field. in fact there is quite a bit, such as a recent special issue on language learning resources [cit], the long running acl workshops on building educational applications using nlp [cit], and a recent shared task competition on grammatical error detection for second language learners [cit] . but i hope i am casting a few interesting thoughts in this direction for those colleagues who are not focused on this particular topic."
"the tonewars app [cit] ) takes this idea one step farther by linking second language learners with native speakers in real time to play a tretis-like game against one another to better learn chinese pronunciation. the second language learner feels especially motivated when they are able to beat the native speaker, and the native speaker contributes their expert tone recordings to the database, fine-tunes their understanding of their own language, and enjoys the benefits of tutoring others in a fun context."
"although computerized vocabulary tools have been around for quite some time, the rise of mobile, connected applications, the serious games movement, and the idea of \"microtasks\" which are done during interstices of time while out and about during the day, opens the door to new ways to expose students to repetitive learning tasks for acquiring language [cit] . some of the most innovative approaches for teaching language combine mobile apps with multimedia information."
"why is the completion rate of moocs so low? this question vexes proponents and opponents of moocs alike. counting the window shopping enrollees of a mooc who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out [cit] . that said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting with other people."
"a karaoke-style video simulation is used by the engkoo system to teach english to chinese speakers [cit] . the interface not only generates audio for the english words, but also shows the lip and facial shapes necessary for forming english words using a 3d simulated model lipsyncing the words in a highly realistic manner. to generate a large number of sample sentences, the text was drawn from bilingual sentence pairs from the web."
"an entirely different approach to providing feedback that is becoming heavily used in massive open online courses is peer feedback, in which students assign grades or give feedback to other students on their work [cit] . researchers have studied how to refine the process of peer feedback to train students to produce reviews that come within a grade point of that of instructors, with the aid of carefully designed rubrics [cit] . however, to ensure accurate feedback, several peer assessments per assignment are needed in addition to a training exercise, and students sometimes complain about workload. [cit] experimented with a workflow that uses machine grading as a first step. after training a machine learning algorithm for a given assignment, assignments are scored by the algorithm. the less confident the algorithm is in its score, the more students are assigned to grade the assignment, but high-confidence assignments may need only one peer grader. this step was found to successfully reduce the amount of feedback needed to be done with a moderate decrease in grading performance. that said, the algorithm did require the instructors to mark up 500 sample assignments, and there is room for improvement in the algorithm in other ways, since only a first pass at nlp techniques was used to date. nonetheless, mixing machine and peer grading is a promising technique to explore, as it has been found to be useful in other contexts [cit] ."
"results are close in terms of comprehensibility and solubility to those of a textbook. the project's ultimate goal is to have the word problems actually tell a coherent story, but that challenge is still an open one. but the programs can generate an infinite number of problems with solutions. other work by the same research team generated personalized algebraic equation problems in a game environment and showed that students could achieve mastery learning in 90 minutes or less during an organized educational campaign [cit] ."
"novel internet-based indoor localization technologies will be developed for this purpose. these will enable multi-robot navigation requiring minimum or no installation of infrastructure on building walls, ceilings and floors, hence ensuring minimum sensors and communications buses. a typical sensor installation costs over 10,000 euros and requires several days for installation and integration. hence, this approach will considerably reduce the cost and the time needed for infrastructure installation of indoor logistic systems by fusing state-of-the-art wireless sensors on board the robot with robust simultaneous localization and mapping (slam) algorithms. a fundamental problem is that there is currently no open-source platform to seamlessly integrate fingerprint measurements from different types of indoor (and robot on-board) localization technologies (e.g., wi-fi, ble, uwb, etc.) [cit] . our approach will be to extend the open-source anyplace (anyplace.cs.ucy.ac.cy) architecture with components and specialized localization algorithms, which will enable fine-grain infrastructure-free localization using internet of things (iot) -enabled devices. we will particularly redesign the algorithmic components necessary for indoor localization, indoor localization accuracy estimation [cit], api channel optimization techniques [cit], and visual analytic cues necessary to provide a rigorous fingerprint management studio for crowd-sourced indoor signals. the infrastructure-less multi-robot navigation will be implemented by a software architecture that will combine ros packages for navigation and path planning implemented onto the commercially available robotnik (www.robotnik.eu) rb1-base as shown in fig. 1, and java and javascript modules for the gui and the fleet management system implementation."
"with the emergence of small low-cost imu (inertial measurement unit) chips, imu-based indoor positioning technology has developed quickly [cit] . however, the imu-based method has an accumulation error that is proportional to the third power of time [cit] . to reduce the speed of error accumulation, one of the common approaches is to use the zupt (zero velocity update) algorithm [cit] to constrain the positioning drift. this is because it can not only utilize the fact that the velocity is zero at the moment when the foot touches ground but also constrain the positioning drift when the zero velocity is taken as the observation of the ekf (extended kalman filter) algorithm. the application of this technology has improved the positioning precision and algorithm stability. however, since the imu-based algorithm can only acquire the relation between the relative position and the initial state, the positioning error will still grow linearly over time, despite the adoption of the zero velocity update algorithm. more importantly, although the pdr (pedestrian dead reckoning) algorithm has good positioning precision in the case of rectilinear movement, it will result in big angular deviation overall due to the accumulation of slight angular deviations at the corners. after that, the trajectory will deviate even more seriously. therefore, the performance of the pdr algorithm will be degraded by the presence of many corners. in view of this, one method is to use a magnetometer for the constraint [cit] . however, as the magnetometer can be easily affected by many factors, such as large metal pieces, pedestrians, and walls, the constraint effect is not ideal. then, to acquire a stable and precise positioning result after a long time, it's necessary to introduce data from other sensors to impose a constraint on the positioning result for the control of error growth."
"where f k and g k represent the state transition matrix and the system noise gain matrix, respectively. in addition, the system state covariance matrix p k can be updated according to the following formula:"
"this experiment uses the landmark-based visual positioning system to obtain the real trajectory of the current system state. this approach uses the square fiducial marker with a given dimension (the edge length) as the tag to calculate the relative position between the camera and the fiducial marker [cit] . before the use of this system, it's necessary to deploy cameras to take pictures of all landmarks in the entire experimental field. if there are two or more two fiducial markers in the same frame of an image, the constraint relation (which can be expressed by 6-dof rotation or translation) between these fiducial markers can be established. to ensure that the system state in a unified coordinate system can be recovered based on the picture of a random fiducial marker, this paper adopts the graph optimization [cit] method to determine the fiducial marker and the pose of the camera when the pictures of multiple fiducial markers are taken as the vertices of a graph for optimization. in addition, when the rotation and translation from the fiducial marker to the camera at each moment is taken as the edge, the relative relation between all of the fiducial markers can be obtained globally after the iterative computation of the minimum errors. in the positioning process, the picture of all fiducial markers in an image frame is used to estimate the current pose of the camera and exclude all of the estimation results with apparent errors, to obtain the true value of the current system state. based on this method, the indoor positioning precision can reach 7 cm [cit] . since the graph optimization method is adopted in this process, the accumulated error will be limited by the closed-loop detected in the whole experiment. the pose has similar scale to the covariance throughout the whole process. since the pose is necessary for calculating a relatively accurate position, this feature plays an important role in error analysis of the uwb observation values."
"to alleviate this usual problem, adaclassifier makes optional almost every input file (this includes the dtm, the inventories and the horizontal components). each time the application is run, it analyzes the dependencies of each sub-algorithm and decides which of these may be executed as a function of the inputs provided by the user. consequently, it is necessary to add the \"not checked\" value as one of the possible outputs of each classification process. note that this behaviour makes the tool much more flexible: while concentrating the detection of six deformation processes in a single application, it may be used to check only one of these, just providing the available set of data."
"only one of the applications developed and tested, adafinder, relies on a procedure for which previous results already existed. therefore, it was the only one for which a performance reference was available to compare with. since such procedure was executed manually (a series of steps performed by an operator using the tools offered by a gis) a noticeable im- there were no performance references for the two other tools (los2hv and adaclassifier) so no specific expectations about performance improvements existed. all tests took place using a computer with the following characteristics: windows 10 64-bit, intel core i5-5300u @ 2.3 ghz, 2 cores, 4 threads, 8 gb ram, 500 gb magnetic (non-ssd) hard disk. table 1 shows the performance of the three tools."
"as indicated by get result, a sub-process is utilized to output the average position after the observation values of the uwb sensor and imu sensor are considered; in the corresponding flowchart, the positioning result is outputted by the particle filter before resampling. it can be used as the positioning result of the whole positioning algorithm after the average weighting of the particle"
"geographic information is everywhere. the rise of mobile search, whose volume is now reported to have surpassed that of desktop search, 1 makes the importance of differentiating relevance between mobile, location-based from a static, information-seeking context all the more important [cit] . for many everyday activities related to, and often occurring in space (e.g., looking for a bar that is still open or finding an optimal route home in congested traffic), users require search results that take account of local context. furthermore, many location-based services rank information, and display only a subset of possible options to users to display uncluttered results, often on a map, often simply using a combination of distance and feature type for ranking. essentially, this task can be seen as analogous to that of traditional search-users require geographically relevant information about geographic entities in their environment in order to make appropriate choices. however, traditional information retrieval (ir) and geographic information retrieval (gir) approaches, which have been predominantly concerned with retrieving documents, fall short in addressing factors relevant to mobile search."
"the los2hv tool is targeted at the computation of the separate horizontal and vertical components of the ground displacement measured with psi technologies along the satellite's los. both ascending and descending datasets-that is, files with pss derived from sar (synthetic aperture radar) images captured when the satellite is traveling from south to north and from north to south respectively-are required. los2hv, in its current version, is not able to compute such horizontal and vertical components when only one dataset (ascending or descending) is available."
"standard robot localization or location tracking approaches often rely on state estimation algorithms working in a stochastic framework; they usually show weak performance when in presence of data outliers, systematic errors or invalid model assumption. besides, most of these approaches require initial robot position to be known at least approximately, hence cannot solve the lost robot problem. we will develop a more robust and effective alternative, which is offered by set membership estimation and data fusion techniques that work in the bounded-error framework [cit] . these techniques can operate efficiently with sparse, asynchronous and heterogeneous measurements while being robust to the presence of non-consistent measurements, inaccuracy in environment modelling, and drift and inaccuracy in the robot evolution model. furthermore, they can naturally track multihypotheses about robot pose, hence offer a solution technique for the robot global localization problem [cit] . fostering on these approaches, effective data fusion and state estimation algorithms will be develloped for indoor robust localization and mapping-update in actual environments."
"several researchers have suggested that location alone is not sufficient for capturing a mobile user's context, and claim that there are other fundamental dimensions [cit] b; [cit] ), such as time [cit], activity (e.g., [cit], movement, and visibility [cit] . other work has tried to go beyond lbs and its technological focus, and study mobile geographic information usage from a more fundamental perspective. [cit] was probably the first to introduce the idea of relevance of geographic entities on maps. he proposed a simple abstracted function for computing this relevance as a weighted linear combination of user characteristics and context parameters (e.g., spatial relevance, topical relevance, etc.), although he did not detail the parameters to be used. [cit] discussed a very high-level and abstract perspective on gr. his conceptualization of gr encompassed an individual \"attention\" and an \"influence\" dimension, as well as their relations. the individual attention is an expression of geographic information needs, which may be either allocentric or egocentric. the influence dimension of geographic objects in the environment is either space-or place-related."
"the toolset has been implemented in c++ to boost performance. other popular languages, as python, have been avoided precisely for that reason. although developed using microsoft's visual studio, special precautions have been taken to make the source code portable, particularly for the most popular c++ compiler used in the linux operating system, i.e. gcc. choosing c++ opened the door to using several open source libraries on which the toolset relies. the most relevant ones are:"
"aiming at the state estimation problem, one of the common approaches is bayesian filter technology, which provides a general framework for estimating the system state based on its observed value. it uses a state and the corresponding confidence coefficient (in the form of a covariance matrix) of this state to estimate the system state. for the linear problem, the kalman filter uses a gaussian distribution to describe the state distribution. the ekf algorithm also uses the gaussian distribution to describe the state distribution for the linear approximation of a nonlinear system. this is because a gaussian distribution is obeyed only in a linear system. in contrast, the particle filter can address all non-linear or non-gaussian problems when many of the particles distributed in the state space approximate the state probability distribution."
"at this point, w k is still a second-order diagonal matrix. assume that δv b t and δθ b t are independently and identically distributed. then we have:"
"during system operation, five levels of supervision and control naturally emerge, dictated both by time requirement, i.e. the loop-rates, and the level of abstraction of the signals which are involved. this leads to hierarchical architecture composed of levels for controlling motors of the wheels, the robot modules, behaviors and tasks, and is described next in a bottom-up sequence."
these values are saved to the corresponding output files. note that the user may select to store points (centroids of the tesserae with data) or squares (boundaries of the tesserae). the second kind of output (squares) is the one used by adaclassifier. the centroid output is provided for whatever other purposes.
"the european union identified the development of ehealth [cit] and mhealth [cit] systems and services as a top priority for the advancement of the quality of healthcare provision, leading to the advancement of patients' quality of life, while saving significant healthcare expenditures. placing the citizen at the center of innovations, the development of cloud-based ehr systems, facilitating the complete patient's medical history, will allow medical experts for more informed, timely decisions, hence advancing the level of current clinical practice. for achieving such a radical transformation, it is essential for different healthcare systems to be able to communicate and share information. in other words, install the necessary interoperability mechanisms that will allow medical experts from different specialties, practicing medicine in different hospitals, and using different systems, to connect to a patient's ehr and update it with the relevant clinical information. [cit] /1302 [cit] recommended 27 ihe -integrating the healthcare enterprise (www.ihe.net) -profiles describing the different layers of interoperability at a european level, thus providing software vendors with the appropriate power to develop truly interoperable ehr systems for storing and sharing medical data. going one-step further, appreciating the greater socioeconomic impact of such endeavor, the eu has acknowledged the necessity of interoperable ehr systems by incorporating them under the umbrella of the connecting european facility (cef) eu instrument towards forming interconnected and trans-european digital infrastructures and networks. currently ongoing cef program for cross-border healthcare provision, simultaneously being developed in 17 member states, will facilitate the exchange of the minimum patient summary (ps) between countries, while accommodating electronic prescription (eprescription) and dispensing (edespensing) [cit] /24/eu (joint action to support the ehealth), [cit] ."
"finally, these two aggregated scores, mobility and geographic environment, are combined with the topicality score to give a geographic relevance score (gr, figure 1 ), in the following referred to as scoregr (table 1) ."
"2. 8xco2 might be interesting, but (hopefully!) is not policy relevant. i think this would stretch any linearity of the system and not be useful for policy targets. i would expect most esm groups therefore not to do this one, although emic groups, less limited by cpu, may well do."
"our experiment also highlights the importance of an adequate approximation of topicality. while simple category filtering is computationally efficient and often an adequate approach, by relying on syntax only, it can be too exclusive and discard too many entities as nonrelevant. we recommend using semantic similarity to topicality instead of an exclusive filtering based on category labels (e.g., [cit] ) . for example, instead of excluding all entities that are not of the category \"hotel,\" entities of similar kinds, for example, \"hostel,\" \"guesthouse,\" \"motel\" should be treated as semantically similar and be included as partly relevant. it is fair to assume that most humans would accept certain trade-offs in terms of space, semantics, and functionality, for example, a nearby motel versus a distant hotel. this is supported by various empirical studies that show that similarity is context-dependent and malleable (e.g., [cit] ) . however, one key problem remains as how to define and implement semantic similarities of geographic features. the solution implemented for scoregr is based on the normalized google distance [cit], and offers plausible and meaningful scores for topicality, but is strongly dependent on the underlying data set. it can also produce inadequate results: for example, that pubs are almost as similar to hotels as hostels are to hotels. topical relevance assessment becomes even more difficult if the user cannot explicitly specify a category name (e.g., \"restaurant\"), but only his/her intention or objective for a planned activity (e.g., \"dinner\"). in general, a possible solution to such problems can be to establish the objective behind a user query, deduce possible activities, and match them with respective affordances of the geographic features. however, such a solution requires sophisticated activity analysis and a systematic understanding of commonsense knowledge [cit], especially concerning affordances associated with different types of geographic features [cit] ."
"for accurate manipulation of the tissue, a firm grasping mechanism is necessary. in our previous work, we developed a motorized force-sensing micro-forceps module that fits onto micron without interfering with its operation [cit] . the unit is a \"drop-in\" module carrying all the necessary actuators and sensors (fig. 2) . the actuation is provided by a linear micro motor (squiggle-rv-1.8 by new scale technologies inc., victor, ny), which slides the tubular tool shaft up and down along the tool axis to respectively open and close the forceps jaws. the forceps jaws are fixed to the module body via a set screw, and can easily be replaced. this enables the use of various jaw profiles for handling various surgical tasks, such as the thicker profile-1 in fig. 2a for peeling dense epiretinal membranes and the slimmer profile-2 for delamination of finer internal limiting membranes. in addition, since the forceps jaws can wear and deform with use, easy jaw replacement enables use in prolonged or repeated tests without deterioration of the grasping quality."
"the next level is the planning level controller. this involves robot's actions and reactions; it is responsible for on-line path and motion planning, and transforms from the current state of the robot to a new state."
"after particle sampling according to the previous particle state and the output u t of the virtual odometer at time t, the prior distribution of particles is obtained. the particle update must conform to the distribution p(s"
"furthermore, we found that crowdsourcing was a useful complementary approach for testing gr assessment methods, beyond controlled lab studies. although the outcome of the experiment generally supports our claim that gr is distinct from concepts of relevance in ir and gir, and that scoregr is an effective and valid method for assessing gr, we need further and extended empirical studies. extensions should not only encompass more criteria and different scenarios, but additional propositions on combining scores."
"the bandage phantom was prepared by slicing sticky tabs from 19 mm clear bandages (rite aid corp.) into 2 mm wide strips (fig. 5d) . repeated peeling tests using a single bandage strip on the setup shown in fig. 5a revealed that the adhesion between the bandage strip and its backing decays with each peel initially (fig. 6) . however, after approximately the 10th peel, the bandage sticks back on consistently, requiring similar amount of delamination force for a prolonged time. this enables the use of each strip numerous times by brushing it back in place with its consistent level of adhesion after each peel, and provides a very repeatable platform for conducting multiple tests."
"this stable geographic relevance is strongly influenced and modified by a second, dynamic component of gr. the dynamics of the environment and the mobility of a user can further modulate the basic, configurational geographic relevance through direction of movement or spatiotemporal accessibility of entities. this component corresponds to mobility in our computational model (see figure 1) . we understand mobility as a means to perform activities at various places, thus spatiotemporal proximity of entities is central to assessing gr with respect to such mobile user activities. the results for scenario 1 indicate that planning may also play a substantial role in gr assessment. a few participants judged a supermarket further away from the destination and with less time for shopping as more relevant than one that was open longer and located closer to the destination. comments by these participants reveal that they planned to go to the second-and still open-supermarket on their way to the destination, if they could not find what they were looking for in the first one. although it is hard to generalize, it suggests that for modeling gr the geographic environment of entities is as important as the context in which the user is seeking information to satisfy her/his needs."
"instead of buying multiple medical devices for the same service, the e-diagnostic module gathers all of them in one place allowing thus a gain of time and money. fig. 3 . a schematic view of the e-diagnostic module components and communication interfaces."
"the momit project-(multi-scale observation and monitoring of railway infrastructure threats), see [cit] for details-aims at developing and demonstrating a new use of remote sensing technologies for railway infrastructures monitoring. momit solutions will mainly aim at supporting the maintenance and prevention processes within the infrastructure management lifecycle. the overall concept underpinning momit project is the demonstration of the benefits brought by earth observation and remote sensing to the monitoring of railways networks both in terms of the infrastructure and of the surrounding environment, where activities and phenomena impacting the infrastructure could be present. momit will leverage on state of the art technologies in the fields of space-based remote sensing and rpas (remotely piloted aircraft systems) based to perform different kind of analysis thanks to the wide variety of sensors they could be equipped with."
"to compute the geographic relevance score of a geographic entity, we first operationalize the elicited criteria by mapping each criterion to a distance function. these distance functions (δ) take for each criterion a user query and a geographic entity as input. the computed distance increases as the relevance of a geographic entity in a specific user context diminishes (see figure 1, left side). furthermore, we normalize on an interval scale between 0 (no relevance) and 1 (maximal relevance) to obtain quantitative relevance scores."
i agree it is good to make sure these are assessed. in fact rcp2.6 [cit] erl paper c3 (http://iopscience.iop.org/article/10.1088/1748-9326/11/9/095012 ). in there we show explicitly a sequence of how human and nature sinks/sources gradually transition from positives to negatives and the interesting dynamics of the earth system. to some extent therefore this scenario can achieve (but not in a clean idealised way) they same sequence that you get via your logistic pathway. -1% ramp-down -other (faster/slower than 1%) idealised % runs there were also desires to run other scenarios as well as the idealised cases (e.g. an emissions-driven rcp2.6). we also tried to align with other mips -such as lumip.
"it is important to remark that the results obtained by adafinder with this (and any other real) dataset are the same than those obtained when computing the results in a manual way. the situation of adaclassifier and los2hv is different. these applications are much newer and no previous real datasets were at hand, so, at the time of writing this paper, still no actual results based on real datasets were available. nonetheless, the university of alicante, in cooperation with the university of florence, and using datasets kindly provided by this last institution, is currently and actively testing these tools. the areas covered are zeri, abbadia san salvatore and campiglia marittima, all of them located in italy (see figure 7) ."
"my main question really is not just the choice of scenario -what do you recommend about an analysis technique. you do not mention performing coupled/uncoupled simulations with the logistic pathway -so how would you look at climate-carbon and co2carbon feedbacks? would you still want to do cou, bgc and rad versions of the logistic pathway? (which would increase computational cost of course). how do these metrics (beta/gamma) evolve in time?"
"finally, the highest level of the hierarchy is the gui level. the gui level and the task level can be grouped since, these will run on the cloud, whereas all the rest will run on board."
"the overall differences between crowdsourced and scoregr ranks are smaller in scenarios 1 and 2 than in scenario 3 (see figure 6 ). in scenario 1 we observe that two entities closest to the top ranked show a difference in rank of note. the label \"irr\" refers to entities identified as irrelevant. *significance on 5% and ** on 1% level."
"to restrain the error accumulation of an imu-based algorithm, the algorithm can be combined with other positioning methods without error accumulation to achieve better results. in my opinion, these methods can be grouped into two major types: the laser or visual positioning system based on a given map and the positioning system based on wireless sensors. the former is mainly characterized by centimetre-level precision [cit] . however, such a visual or laser-based positioning system requires the consumption of computing resources, so generally, real-time calculations can only be performed on devices with powerful computing abilities. since it uses a complex process to build a global map in the early stage, it's not applicable to rapidly changing scenes, as the accuracy of the global map greatly influences the positioning precision, and any change in the map will also affect the positioning result. for the latter, the bluetooth and wifi-based positioning method achieves a slightly lower accuracy, which only reaches approximately 3 m [cit], while the uwb-based algorithm achieves a better positioning result with decimetre-level precision [cit] . nevertheless, since a wireless signal-based positioning system with certain robustness in a dynamic environment does not have a high requirement for computing resources during the positioning calculation, it's highly applicable to the construction of a low-cost, indoor positioning system with the integration of imu. based on a similar idea, some researchers have already integrated the odometer (which is also an incremental positioning system) with uwb (ultra-wideband) technology for the positioning of a wheeled robot [cit] . for a wheeled robot, as the uwb sensor can be easily connected with the odometer, such a constraint will facilitate the fusion. however, in the case of pedestrian positioning, in the integration of the imu-based positioning system with the uwb-based positioning system, the following factors must be taken into account: the method for placing both of the sensors and the establishment of a relationship between them. for the purpose of zero-speed detection, the imu must be mounted on a foot. however, the uwb sensor must be mounted on the head or shoulder to reduce the blockage of the uwb signal by the human body due to its great influence when it's mounted on a foot. therefore, this paper adopts a fusion method by mounting the uwb on the head and imu on a foot. based on the choice of the imu as the core, many researchers take the uwb data as positional observed values and add them into the ekf algorithm, which is based on zero velocity, to realize the fusion [cit] . nevertheless, as the ekf is, in essence, the linear approximation of the observation equation, it can hardly achieve a good approximation of the uwb sensor [cit] based on a highly nonlinear observation model. therefore, this paper adopts a particle filter algorithm to fuse the data from both sensors. it takes the imu-based calculation result as the prior information for the particle filter, and uses the uwb observation as the observed value of the particle. section ii introduces the general framework for positioning based on uwb and imu, and describes uwb and imu separately. section iii provides the relevant experimental results and compares the positioning precisions obtained based on the fusion algorithm and the pure application the data from each sensor, in addition to the analysis of the error source during the positioning. the last section is the conclusion of this paper."
"in simulated ophthalmic procedures, auditory force feedback was shown to help in maintaining the exerted forces below potentially dangerous levels [cit] . in addition, there are motivating applications in other fields that may help in reducing forces, such as inserting a biopsy needle, where reciprocation of the needle was shown to facilitate the advance of the needle through tissue and penetration of the site of interest [cit] . recently, we have shown that inducing micro-vibrations on the tool tip can facilitate delamination of membranes as well [cit] . however, deciding on the optimal frequency and amplitude of these vibrations during the surgical operation, and updating this information based on exerted forces in real time is not trivial."
"as previously explained, the diagnostic support module will be mounted on the robot chassis allowing the overall robot to become an e-diagnostic mobile station. equipped with a set of non-invasive sensors/devices (ecg, blood pressure, oximeter, temperature), this mobile station will then be functionally capable of performing medical exams, generating diagnostic reports and securely forwarding them to the patients cloudbased ehr. this module will have to be easily pluggable and benefit from all the existing interfaces available on the robot that are necessary to its own use."
"a first objective of the experiment is to validate our proposed scoregr and grbm25 methods against human relevance assessments. a second objective is to establish whether the baseline methods provide a sufficient approximation of gr, even if they do not explicitly implement the criteria spatiotemporal proximity, cluster, and colocation as in scoregr and grbm25. to meet these objectives, we designed three scenarios of mobile information seeking, involving (a) clusters of geographic entities (supermarkets, hotels, restaurants), (b) colocation rules (pharmacies next to supermarkets), and (c) spatiotemporally inaccessible entities (supermarkets, restaurants). we chose these three scenarios to balance the complexity of our evaluation tasks between tasks that reflect the core of gr (i.e., cluster, directionality, colocation), tasks that are atomic enough to be understood, tasks that are doable by the assumed population of workers, and tasks that still are ecologically valid. we did not consider simpler scenarios (e.g., a user searching for a type of geographic entity that is not involved in cluster or colocation rules), because in such cases the additional criteria implemented by scoregr and grbm25 do not influence the output rank, by design. therefore, in such scenarios, scoregr and grbm25 would resemble the output of the baseline methods, except for the combination of the score of the individual criteria."
"first, this paper directly uses the imu data to perform a calculation based on the pdr algorithm. then, it takes the result of the pdr algorithm as a virtual odometer to obtain a prior estimate on the whole movement. finally, it uses the uwb data as the observation to constrain the prediction result and obtain the posterior estimate on the overall movement state to avoid a large difference between the output frequencies of the two data sources. meanwhile, the constraint that the human body as a whole moves in the same direction establishes a connection between the sensors. thus, their fusion can be realized."
"b. gonenc, r. h. taylor, and i. iordachita are with cisst erc at johns hopkins university, baltimore, md 21218 usa (e-mail: bgonenc1,rht,iordachita@jhu.edu) (corresponding author: berk gonenc, phone: 360-975-1676; e-mail: bgonenc1@jhu.edu)."
"whereθ f is the direction of the vector from the location of the foot at t 0 to the location of the foot at t 1 . so far, we have established the formulable relation between the imu sensor and the uwb sensor."
"participants. a total of 416 [cit] . participants were gathered through the online service crowdflower (www.crowdflower.com). this service passes the tasks over to the crowdsourcing platform amt. we assume that our sample falls into typical amt demographics [cit], that is, computer-literate people with no particular expertise in geography. as all participants connected to the service from a u.s. internet protocol (ip) address, we considered them to be familiar with mobile information-seeking tasks in an urban context and being valid candidates for our study, as they are assumed to represent the general public."
"the specific objectives of endorse are described next: 1) to develop a cost-effective logistics robotics system for healthcare indoor environments. a key point for the achievement of this goal is the minimization of installation time and of the cost of the system. 2) to push forward the frontier of human-robot interaction (hri), especially for the cases where robot should safely navigate in crowded spaces, resolving deadlocks that involve humans. 3) to develop robust localization methods for infrastructure-less indoor robotic fleet localization and supervision, which will result in installation time minimization. 4) to introduce novel cloud services that bring together solutions from healthcare, robotics, and logistics, offering new business models for indoor logistics and high data security standards. 5) to develop an integrated mobile e-diagnostic station, which will endow modularity to the endorse system, and will facilitate a mobile e-diagnostic service. the objectives will be achieved by tackling specific scientific and technological challenges through cross-sector collaboration taking advantage of complementary competences on the fields of robotics, machine vision, logistics, it and product design. the proposed solution will be modular and reconfigurable and thus applicable to different fields and applications of commercial spaces, while it will focus-in on the needs of the healthcare sector. the developed solution will be piloted in healthcare center fundació ave maría in sitges, spain."
"indoor spaces in hospitals and healthcare centers offer advantages that would facilitate the deployment of mobile robots. they are required by law to meet stringent building codes and therefore, from a robotics perspective, the navigation space exhibits some structure which moderates the technical challenges, e.g., with respect to home environments. moreover, most commercial spaces have reliable and predictable communications infrastructure, e.g., wi-fi and cellular connectivity, since it is required for smooth business operation [cit] . thus, spaces in hospitals and healthcare centers are rightfully considered the next great field of robotic deployment, following the industrial spaces. despite these advantages, today, only a handful of solutions exist for hospital and healthcare spaces -unlike the case of industrial spaces-and these solutions do not trigger widespread acceptance by the market. this is because they require costly and time-consuming localization infrastructure, e.g., significant peripheral sensor installation, mapping, etc.; they do not easily integrate to corporate it solutions; as a result, they do not address satisfactorily data protection and cybersecurity threats; they are not adaptable and reconfigurable and therefore are limited to a single type of service. existing solutions for small and medium commercial spaces such as hospitals and healthcare centers, developed by aethon (aethon.com) and swisslog (www.swisslog.com) are very expensive, they cost more than 75,000 euros plus installation costs, and exhibit operational limitations. they lack motion flexibility and therefore the design and implementation of their route becomes challenging in small spaces; they are not customizable and modular and therefore perform no tasks other than transfer of goods; hence they lack the capability to replace their carts with other functional units such as an ehealth diagnostic station or a uv-light sanitizer. these systems are difficult to integrate with existing enterprise resource planning (erps) and business intelligence systems typically installed in healthcare buildings, and therefore the data they collect and the tasks they execute are not registered in the corporate records and data management system. cybersecurity threat is another deterrent factor, especially in cases where humans are involved."
"an accurate assessment of spatiotemporal proximity is crucial to capture topicality well. [cit] that direct distance is a good first estimation of distances along a route network. although we could show that direct distance implemented in scoregr is an effective and efficient approximation for spatiotemporal accessibility in our experiment, it may be too simple in other cases, especially where movement is constrained by a network. it is limited by the assumption of uniform speed of movement that hardly holds in reality. for mobile users, the relevance assessment should at least discern different means and modes of transport, including public transport, and real-time traffic situations. any spatiotemporal accessibility assessment method is an estimation, and includes computational costs for increased accuracy. further empirical studies will have to consider cost/benefit ratios for degrees of increased accuracy and associated computational costs."
"overall, our crowdsourcing results show that our proposed scoregr method is capable of effectively assessing gr for the implemented scenarios. human relevance ranking is similar to ranking computed by scoregr. the correlation ranged from .50 to .851 for the three scenarios. for scenarios 1 and 3, where participants weighted the criterion colocation lower than cluster, the strength of correlation is weaker. as scoregr was designed to weigh these two criteria equally, the observed difference suggests that cluster plays a more important role than colocation when humans assess gr. at the same time, it also implies that the importance of these two criteria may vary for different scenarios. overall, the results suggest that cluster and colocation are essential when computing the final gr score and that the importance we assigned to them in the combination method of scoregr is too low compared to the crowdsourced ranks. entities that highly satisfy the cluster and colocation criteria received higher crowdsourced ranks than the computed scoregr ranks. at the same time, both fig. 6 . differences of ranks between crowdsource and scoregr for scenarios 1, 2, and 3. note: the colored circles represent the differences in ranks between crowdsourced judgments (figure 4 ) and ranks obtained with the scoregr method ( figure 5 ). green colors indicate positive, red colors negative deviations. yellow circles stand for no difference. [color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com.] baseline methods produced no similar ranks with respect to human judgments. they perform better in scenario 2, where spatiotemporal proximity is reduced to spatial distance (as the time factor is not important in that scenario), and when directionality is excluded. however, even in scenario 2 the ranks produced by the baseline methods do not resemble the crowdsourced ranks. these empirical results are coherent with previous work and conceptualizations of gr [cit] ."
"a mechanism to quickly compare the results produced by the manual and automated solutions was devised; basically, it consisted on exporting the values of the attributes to check for both outputs (manual and automated), sorting these to easily match the attributes in each file and then computing the differences of their values, which, in all cases, were under the threshold set by the precision of the typical 8-byte ieee 754 double (around the 15th decimal position). this means, from the numerical standpoint, that the results (for instance, the coordinates of the output points) were equivalent. the values of the attributes standing the level of certainty were checked for absolute equality since these were represented by integer (non-floating point) magnitudes."
"however, and although it is not possible to publish any definitive conclusions now, the preliminary results obtained up to now are very promisingconfirming, at least for the time being, the positive results of the formal tests."
the selection of the programming language has direct implications on how the logic of these applications may be embedded in other software components. see section 4.2 for further details.
"where t 0, t 1, and t 2 represent the moments when the foot starts to move, the moment it stops moving, and the moment it begins to move again, respectively. it is clearly shown in figure 2 that the right foot is only moved in the period between t 0 and t 1, and the whole body is moved throughout the entire period (from t 0 to t 2 ). since the whole body is connected with the right foot, the displacements of these two parts are equal; thus, the average velocity of the right foot between t 0 and t 2, which is on the right-hand side of equation (1), must equal to the average velocity of the whole body, which is on the left-hand side of the equation. in addition,v f can be obtained by dividing the foot displacement in the period between t 0 and t 1 by the time difference t 2 − t 0 . similarly, there is no firm correlation between the direction of body movement and the direction of foot movement. however, there is a relationship between the mean moving direction of the footθ f and the mean moving direction of the bodyθ b :"
"our results describe the interaction between the main parameters that influence membrane peeling forces: membrane properties, peeling speed, microvibration amplitude and frequency. the viscoelastic mechanisms behind these findings have yet to be fully explored. based upon the identified behavior, our current efforts focus on implementing an adaptive control scheme for optimal selection and real-time update of the micro-vibration parameters to minimize delamination forces. before this method can be proposed as a clinically feasible assistance option, the effect of micro-vibrations on underlying retinal tissues needs to be critically examined. upon system integration, our future studies will inspect the effects on live tissues, and feasibility in animal models."
"we also developed an alternative assessment method for gr that takes into account the distribution of the distance values of the geographic entities [cit] . for this purpose, instead of scores, we calculate probabilities for the distribution of the distances for the same five criteria as for scoregr. in analogy to the okapi bm25 model (spärck [cit] ), we refer to this method as grbm25 (table 1 ). the similarity function is defined as follows:"
"although [cit] . while these tasks are certainly plausible and prototypical for everyday information-seeking tasks, future studies will have to carefully design tasks that capture information needs in the real world and employ tasks that participants will accomplish in the real world. selection of such tasks could be informed by an analysis of mobile search queries. in particular, such tasks should cover more complex mobile situations and information needs triggered by linked activities and influenced by their dynamic coordination and planning. however, this would require field studies with even less control and fewer participants."
"the second application in the toolset is adaclassifier. this tool takes the output of adafinder, that is, a set of adas, and tries to determine the kind(s) of deformation process(es) undergone by each of them."
"note that one of the four values is \"not checked\". this is so because adaclassifier may decide not to apply one or more of the six detection processes because of the lack of data. as stated above, a noticeable number of inputs is required. just the inventory files already amount to five. taking also into account the dtm and the horizontal components of the movement, it is easy to realize that many will be the situations when the full set of files will not be available."
"the aforementioned procedures usually rely on the heavy use of a gis (geographic information system) tool and the expertise of its operator; therefore, these are time-consuming, error-prone processes, which require qualified human resources. this paper presents an implementation of the three methodologies and algorithms above, incarnated as the above-mentioned adafinder, adaclassifier and los2hv applications. the goals of such implementations are (1) to automate the respective procedures to avoid unnecessary human errors, (2) to reduce the time needed to identify and preclassify the adas, thus opening the door to more frequent updates and analysis, and (3) to reduce the expertise required to obtain such results, being possible to integrate the process in an semi-automated production workflow, if necessary and possible."
"the solution to avoid this problem are the \"readmap files\" defining how the relevant attributes in a shapefile are organized. of course, the attributes needed by the tools to work properly must always be present in the files, but thanks to these read-map files it is possible to deal with changes in the positions where these appear."
"in the experiment, the uwb and imu data have been collected along two paths, where the walking velocity has been controlled within 1.5~2.5 m/s. throughout the calculation process, all of the initial values for the algorithms are given by the reference trajectory. with the zupt-based algorithm, the initial movement direction is calculated based on the real trajectory in the first three seconds. path i is relatively simple, along which the tester walks around the whole experimental field with a moving trajectory similar to a rectangle. path ii is more complicated. part of the path has a large uwb observation error based on path i. it's created to reflect the better robustness of the fusion algorithm in an nlos environment. the detailed paths will be described separately in the following sections."
"the membrane inside the raw chicken egg shell was sliced similarly using a razor (fig. 5c ). both the inner and the adherent outer shell membranes are cut together. thus, while peeling the cut strips, both membranes need to be delaminated off the egg shell surface, which requires a larger force as compared to the removal of ism alone. in contrast to the bandage phantom, each membrane strip can be used only once, limiting the total number of tests on this phantom. yet, assuming that the membrane structure does not vary significantly between the eggs, this phantom provides a consistent platform for studying the effect of microvibrations on peeling biological tissue."
(airborne fraction etc). can you also derive and show tcre? you may find that this is actually better behaved in terms of being more constant in time and between scenarios. which is a nice feature of it in fact.
"the output ada shapefile includes among its attributes the results of the quality assessment of the deformation detection process. such asessment is represented by a four-level variable whose values measure the certainty (\"very sure\", \"sure\", \"not so sure\" and \"not sure at all\") of the deformation detection process. figure 2 shows some adas; their colours correspond to just two of the certainty levels described (\"very sure\", green, \"sure\", blue)."
"to constrain the error accumulation based on the time sequence, it's necessary to add an observation to the state update as a constraint to increase the positioning precision. in pure imu-based pdr positioning, generally a zero-velocity detector is adopted to obtain a zero-velocity observation to constrain the current result. as such a constraint can effectively reduce the speed of error accumulation, this paper adopts the acceleration-magnitude detector [cit] as the zero-velocity detection algorithm based on the generalized likelihood ratio test (glrt). through the zero-velocity detection, a virtual movement velocity of zero will be obtained as the observation value, that is to say,"
"in general, both personally and as a co-chair of c4mip, i find this level of analysis and engagement very pleasing to see, and it will certainly help drive the further evolution of c4mip in the future (i'm not yet ready to think about cmip7 though!). c4mip is explicitly aimed at esms, although we welcome emic participation. but perhaps for a next generation we should more explicitly engage with emics and provide additional simulations which emics can lead on to supplement joint esm/emic runs. in fact it was a requirement of cmip6 that no mips added new experiments which had not been tried by at least some models. they (very reasonably) wanted to avoid too many brand new experiments being suggested and possibly wasting time of model groups. so it is really positive to see suggestions like this also being tested with a model. i list below some comments which i hope will be useful both for the improvement of this manuscript and also in general as part of the evolving discussion. there are some areas of literature which can be helpful, and there are some issues which are relevant to esms more than emics (mainly around computational expense). but overall i very much like this paper and would recommend publication with only minor amendments."
"esri shapefiles, although standardized, may include variable sets of attributes. even when a shapefile includes the full set of attributes needed by a tool (for instance, the x and y coordinates as well as the velocity or the deformation time series in the case of adafinder), they may appear in different columns of the attribute (.dbf) file. this usually depends on the provider of the files."
"geography has not played a major role in ir for long. [cit] with the concept of situational relevance. however, this concept was still targeted to classic document-based ir. more recently, the interdisciplinary field of geographic information retrieval (gir) has studied the retrieval of documents where the query and documents retrieved contain spatial references and are often connected through spatial relations. the focus of gir, however, is still on documents, and thus relevance in gir is typically understood as relevance of documents with a spatial reference [cit] ."
"the micro-forceps module is capable of sensing the transverse forces exerted at its tip via the 3 fbg strain sensors attached evenly around the tool shaft. the calibration setup and protocol of the force sensor follow [cit] . the wavelength shift in each fbg sensor normally depends linearly on both the local strain and the temperature variation. during calibration, the effect of temperature change was removed by subtracting the mean wavelength shift from each sensor measurement. resulting temperature-compensated sensor readings exhibit a linear reproducible behavior during both the x-and y-axis calibration procedures, as shown in fig. 3a . the slopes of the response curves form the calibration matrix (k). the moore-penrose pseudo-inverse of this matrix (k + ) is used in the linear relationship (1) to compute the transverse tool tip forces (f x and f y ) from fbg wavelength shifts (δs) during the operation."
"one of the major innovations and tangible technical outcomes of the project will be the development and testing of the modular interfaces and of the e-diagnostic mobile station integrated on-board the mobile robot and tested in a healthcare environment. this will require research and development in both non-invasive sensors/devices (ecg, blood pressure, oximeter, temperature) and dedicated software applications. to this end, modular hardware interfaces (mechanical, electrical and data interfaces) mounted on the chassis of the robotnik rb1-base robot will be developed (see fig. 1 ), so that different hardware-based functional modules can be easily swapped. this would be the first time that a medical diagnostic functionality is integrated on a mobile robot. the integrated robot will serve as an e-diagnostic mobile station, which will be able to produce a diagnostic exam report that will be then securely forwarded to update and complement the patients' cloud-based electronic health records (ehr). the software of the e-diagnostic mobile station will run on-board the robot."
"for the first two scenarios, scoregr is able to correctly identify the most relevant geographic entity as judged by users, while for the third scenario it ranks the second most relevant entity first, and vice versa (see figures 4 and 5, respectively). scoregr also correctly identifies irrelevant entities in the first and third scenario, that is, entities that are spatiotemporally not available. only in scenario 2 does scoregr yield different results. while study participants classified three geographic entities located very close to the user's position as irrelevant, because they belong to categories not matching the user's need, scoregr does not identify these as irrelevant entities. the reason for this is that the underlying measure identifies a semantic similarity between their categories and the user query, although the assigned ranks are very low (i.e., they are classified among the least relevant entities). while scoregr performs quite well, baseline1 and baseline2 classify an irrelevant, spatiotemporally not available entity as the top-ranked one, because it is closest to the user's movement path in scenario 1."
"as indicated by get result, a sub-process is utilized to output the average position after the observation values of the uwb sensor and imu sensor are considered; in the corresponding flowchart, the positioning result is outputted by the particle filter before resampling. it can be used as the positioning result of the whole positioning algorithm after the average weighting of the particle state. it's worth noting that this weight is, in effect, the result of normalizing all weights. the key to the particle filter step is the selection of the state transition equation and observation model. for the former, it requires the output from the virtual odometer system to estimate the change in motion state. for the latter, it requires the uwb observation data to correct the state estimation. they function together to obtain the posterior distribution of the state represented by a particle and the corresponding weight, which will be discussed in section 2.4. figure 1 shows the overall algorithm flowchart. if imu data are received, the state transition model is used to update the current system state and estimate if it's in a zero-velocity state. if the velocity is zero, then the zero velocity is updated to correct the state and the output from the visual odometer is calculated based on the updated state. nevertheless, if uwb data are received, the uwb data are used as the observation values to obtain the posterior distribution of the current position and the state estimate is output after the updating of the particle based on the information from the visual odometer. finally, re-sampling is performed to alleviate the particle degeneracy problem."
"unlike formal tests, where it is possible to predict the outcomes of the tools because of a controlled set of input data, real datasets pose a much more difficult challenge due to the complexity of the information they contain. therefore, the expertise of seasoned experts in the field is required; validating the correctness of the output is a slow task; furthermore, obtaining data to check the six different deformation processes is not easy-inventories, for instance, are very hard to find."
"in conclusion therefore -in order to not end up with way too many model years required from model groups, we selected a small and succinct set. it is highly likely, as you suggest, that this is not perfect and there will be value in other simulations too. for cmip7 we can certainly open this discussion again and evolve our thinking once more."
"up to six different kinds of deformation phenomena are detected. these are: landslides, sinkholes, land subsidences, constructive or consolidation settlements, expansive soils and temperature effects. a different algorithm (or sub-methodology) has been devised for each of these. for instance, figure 3 roughtly depicts how landslides are detected."
"in order to measure the exerted forces inside of the eye, a family of force-sensing instruments was developed at johns hopkins university using fiber bragg grating (fbg) strain sensors. these tools are able to capture the forces at the tool figure 2 . (a) the motorized force-sensing micro-forceps module [cit] . the tool provides firm grasping functionality, can accommodate various jaw profiles for handling different types of tissues (profile-1 for thick epiretinal membranes, and profile-2 for internal limiting membranes), and senses the transverse forces exerted at its tip via the embdedded fbgs. (b) the microforceps module is compatible with the handheld micromanipulator, micron. tip without any adverse effect from tool-to-sclera interaction. first, a single degree of freedom (dof) force-sensing tool [cit] and then a 2-dof pick-like instrument [cit] were built. intuitively, compared with a pick tool, the forceps provide more controlled manipulation of the tissue by firmly grasping it. this enables easier removal of the membrane from the eye in a single step [cit] . with this motivation, tool development continued with a manual pair of 2-dof forcesensing forceps [cit], followed by a 2-dof forceps that can be used with the steady-hand robot [cit] . we recently presented a 2-dof force-sensing micro-forceps for micron. this design was shown to be sufficiently compact and lightweight for micron to operate properly, and the benefits of the resulting device was demonstrated on artificial bandage phantoms [cit] ."
"in order to suppress involuntary hand motion, and induce assistive micro-vibrations during membrane peeling, our system uses a handheld micromanipulator: micron [cit] . this device is normally designed to cancel the physiological hand tremor of the surgeon. the position of its handle is determined by its custom microscale optical tracking system, namely the asap (apparatus to sense accuracy of position). after sensing the tool motion, it is filtered into its voluntary and involuntary (tremulous) components [cit] . then activating its 3 piezoelectric actuators, micron moves its tip to counteract the involuntary motion component within a workspace of approximately a 1x1x0.5 mm volume centered on the handle position. the control software for this operation mode was already implemented in labview. for our system, we extended the existing control loop by adding controlled pulses onto the filtered (non-tremulous) tool tip trajectory with variable frequency and amplitude ( fig. 1) . in order to identify the individual effect of frequency and amplitude on delamination forces, these variables are currently set manually. but the ultimate aim is to develop an adaptive control law-based upon the delamination response to each parameter-that will tune the frequency and amplitude automatically according to measured tool-to-tissue forces."
"within the context of the endorse project, the objective is to augment our existing ehr system [cit] complying with the afore-described interoperability requirements with vital signs data coming from the e-diagnostic support module detailed below. for that purpose, the necessary apis will be developed relying on the fast healthcare interoperability resources (fhir -www.hl7.org/fhir) standard, the most widely used standard for developing smartphone-based mobile health applications today. lightweight apis will acquire data coming from different sensors, at preset sampling rates and using the suitable drivers residing in the e-diagnostic modules communication interface, and assemble them into json or xml -like format for communicating them to the cloudbased ehr. naturally, the appropriate security and device authentication procedures will be installed, while the ehr system will facilitate appropriate auditing schemas for registering each communication and data update session. the latter functionality will rely on accumulated experience from fis-tar (future internet for social and technological alignment) and resulting fiware-specific enablers that were previously developed. additional ehr system functionality will allow generating relevant reporting documents, will accommodate statistical analysis, and support export features for further data analytics purposes."
"evaluating gr brings with it several problems. largescale evaluations in ir often use benchmarks, that is, test collections of documents for which relevance has been assessed by human experts. for gr, no such benchmarks are available. although the contextual suggestion track of the text retrieval conference (trec) 2012 shares similar goals, it is not applicable to gr, as the adopted descriptions of context and granularity of spatiotemporal information are too coarse, and it was established for document-based ir. similar problems apply to evaluation testbeds developed in gir, such as geoclef [cit] . moreover, large evaluation campaigns, such as trec, are not always affordable for small interest groups, focused on subfields of ir. in the last few years, crowdsourcing [cit] has emerged as an alternative route to ir evaluation [cit] . crowdsourcing has been applied to particular ir tasks, such as video annotations [cit], music similarity assessment (urbano, morato, marrero, & martín, 2010), and news search [cit] . more recently, crowdsourcing has been applied in geographic information science for evaluating spatial formalisms of simple spatial overlap situations (wallgrün, [cit] ) . such evaluations can be effectively crowdsourced through commercial providers such as amazon mechanical turk (amt), where any questionnaire or user experiment that can be incorporated into a web page, can be run as an internet service without the need for further equipment. these platforms provide evaluators with tools to create and submit small human intelligence tasks (hits in amt) to a wide audience of registered users, known as workers in amt. as the tasks are rather often short and simple, and the number of workers is large, response times are commonly rather short."
"in retinal microsurgery, surgeons manipulate extremely delicate tissues by applying very small forces that are routinely below the human tactile sensation threshold. a prototypical vitreoretinal task is membrane peeling, where the surgeon delaminates a very thin fibrous membrane (micron scale) adherent to the retinal surface, using either a pick or micro-forceps. successful execution of this task requires extensive experience, and is extremely difficult to master due to suboptimal visualization, inconsistent tissue properties, surgeon's physiological hand tremor, fatigue and involuntary patient motion. during the critical steps in the operation, the instruments need to be moved very slowly, within a range of 0.1-0.5 mm/s, in an extremely delicate environment, to minimize deleterious force transfer to tissue. furthermore, the required forces for delamination routinely lie below the surgeon's sensory threshold. these forces were shown to be below 7.5 mn in porcine cadaver eyes and only 19% of events with this force magnitude could be felt by *research supported in part by the national institutes of health under r01 eb007969, and r01 eb000526, and in part by johns hopkins university internal funds as research to prevent blindness, the j. willard and alice s. marriott foundation, the gale trust and mr. bill wilbur."
"when the relationship between the real system state x k, the solutionx k of the navigation algorithm, and the system state perturbation can only be represented by a nonlinear function, denoted a γ, it can be expressed as follows:"
"the computation of the distance functions for the criteria topicality, directionality, cluster, and colocation follows a similar approach [cit] . next, we compute a mobility score by combining spatiotemporal proximity and directionality scores. for this we use cpl [cit]"
"p. gehlbach is with the wilmer eye institute at the johns hopkins school of medicine, baltimore, md 21287 usa (e-mail: pgelbach@jhmi.edu) surgeons [cit] . application of forces beyond this level can damage retinal veins [cit] and give rise to serious complications such as iatrogenic retinal injury and breaks [cit], vitreous hemorrhage, or subretinal hemorrhage [cit] leading to potentially irreversible damage and loss of vision."
note that the command-line as well as the gui versions of the applications are just interfaces calling the classes in the library that actually implement the logic of the processes.
"using the arithmetic product to combine individual relevance values (e.g., spatial relevance, topical relevance, etc.) and compute gr does not seem likely to be a valid approach, since this method is noncompensatory, that is, one low score is enough to yield a low aggregated score. this could generate invalid results, as the strong \"and-ness\" of the method would cause possibly relevant entities to be scored as absolutely irrelevant. [cit] applied a geometric, compensatory, combination method, taking account of both thematic and geographic relevance. nonetheless, it may still rank topically nonrelevant documents highly, where no other documents are available, by overestimating the importance of the geographic environment component."
"t − 1 is the corresponding weight of the ith particle at time t − 1. assume that the current system state only depends on the system state at the previous moment and the current input. then, the system state can be updated according to the following steps, which are illustrated by the flowchart in figure 1:"
"in this section, two experiments are conducted to verify that the fusion algorithm (denoted as fusing) outperforms the particle filter algorithm using only the uwb data (denoted as only-uwb) and the zero-velocity-update-based ekf algorithm (denoted as zupt) using only imu data in terms of the positioning precision in an nlos (not line of sight) environment. since the general control-point-based positioning precision calculation method cannot reflect the positioning error very well at every time point in the process, this paper adopts the landmark-based approach to calculate the real trajectory. based on the high-precision position at each moment, reference distances from the tag to each beacon are calculated and play an important role in explaining the benefits of the fusion algorithm in the section 3.3.2."
"the results of our experiment not only confirm our previous studies [cit], and underline the main claim by raper (2007, p. 837 ) that \"situational relevance concepts as currently articulated do not deal sufficiently with concepts of mobility and geography, and that these concepts are essential to the understanding of mobile information seeking.\" at the same time, our empirical results do not support another claim made by raper (2007, pp. 842-843) that whatever is in the vicinity of the user is topically relevant, simply because it is in the \"accessibility envelope or surroundings of the user.\" the relevance judgments collected in our experiment clearly show that geographic entities close to a user's current position are assessed as nonrelevant if their semantics are not relevant to the user task at hand."
"design. as the number of entities in an open street map data set is very large, it is not feasible to collect relevance judgments for all entities in a given geographic area. therefore, we reduced the number of judgments to be made for all three scenarios applying a pooling approach commonly used in ir [cit] . we precomputed the relevance of all entities and ranked them for all four methods to be evaluated (see table 1 ). in a second step, those entities in the top-k list of at least one of the methods were included in the set of entities to be judged. the underlying assumption is that a relevant geographic entity would be recognized by at least one of the methods. in practice, many of the elements in the top-k lists of the four methods were common to at least two or three methods, a strong indication that relevant entities have been identified by the pooling approach. a manual check of the data set provided no evidence that any obvious relevant entity had been excluded."
or are you suggesting keeping the 1% run for the feedback separation and using the logistic run to look more at emissions/tcre/af? it would be good to be clear on the intended use as well as scenario that you are suggesting.
"integrating the findings from our previous work [cit] with the results obtained from the experiment presented here, we argue that geographic relevance expresses multifaceted relationships between a mobile user's geographic information needs and the geographic entities in the user's environment. the two main criteria defining the strength of the gr relationship are the spatiotemporal accessibility of an entity with respect to a user's mobility (spatiotemporal proximity), and the topicality of an entity's affordances with respect to the information need for a particular activity. furthermore, the strength of the gr relationship is strongly influenced by the geographic context of an entity's location, such as spatial clusters and colocation of other relevant geographic entities. therefore, gr is distinct from the concept of relevance commonly used in ir. our empirical data indicate that gr cannot be adequately calculated by simply combining category filtering and direct distance-based ranking, that is, baseline1 and baseline2. in contrast, our proposed method scoregr proved to be effective in assessing gr for the considered scenarios."
"crucially, most mobile information needs implicitly entail a temporal dimension. for instance, if a user intends to go grocery shopping and asks \"show me the closest supermarket,\" they implicitly assume that a supermarket should not only be nearby, but also open. if an entity is not spatiotemporally accessible with respect to a user's space-time constraints, the entity is not relevant for the shopping activity. in such cases, participants ranked the obsolete entities as irrelevant. this implies that spatial relevance needs must be considered with temporal relevance, that is, the time a user intends to or needs to spend for a particular activity at a specific location."
"additionally, each iteration includes up to three check comparisons. check comparisons are duplicates of one of the comparisons in each iteration, where either the order of presentation of the entities, the labeling of the entity a or b, or both have been swapped. moreover, we applied a latin square design to produce orderings that start with a different comparison, but otherwise follow the same order. procedure. the iterations described earlier were submitted to crowdflower, which allocated workers to tasks, that is, the previously generated iterations (see the previous section, design). a minimum of 40 participants was allocated per iteration and at least 4 participants for each distinct order. figure 2 shows a task example, that is, one iteration, as presented to a worker on crowdflower. in each iteration participants were asked to make a comparative assessment of two entities a and b (see figure 2 ) and select one of the two entities a or b as more relevant. participants could also classify entities a and b as equally relevant, or both as nonrelevant. we also requested participants to explain their judgment in a text box. as this entry is mandatory, the collected qualitative data may help to better understand the rationale behind participants' judgments and serves as quality check."
"in this paper, we explored the influence of the microvibration parameters on average and maximum membrane peeling force using two models; one a dry phantom and the other a biological model. we combined a force-sensing micro-forceps tool with a micromanipulator, micron, to provide firm tissue grasping and vibratory tool motion at 3 levels of frequency (10, 30 and 50 hz) and amplitude (50, 100 and 150 µm). our observations in this study are focused on tool-to-tissue interactions, which are not specific to the used system (micron), and remain still valid with the other available robotic systems, such as the steady-hand robot. upon introducing micro-vibrations, the average peeling forces were lowered for both the bandages and the egg shell membranes, reaching a minimum mostly at 30 hz for the bandages and at 50 hz for the egg shell membranes. the force-frequency trend varied depending on the phantom type, peeling speed and the vibration amplitude. increasing the vibration amplitude within the explored range (50-150 µm) resulted in a consistent decay in the average peeling force at higher frequencies (30 and 50 hz). nevertheless, because larger vibrations result in higher force fluctuations, there remains a potential risk that the peak force value may exceed the safety limits (even when the average force is lower) if the vibration amplitude is not carefully tuned."
"as stated in section 5, two real datasets were used to check the correctness of the adafinder application. the first one corresponds to tenerife, one of the canary islands; the other one, the glòries dataset covers one of the neighbourhoods of the city of barcelona. the results related to the most representative one, tenerife, are briefly presented here. glòries [cit] 1 pss only and covering a much smaller area, so it was used primarly to test the correctness of the application taking almost no time to obtain results (about 2 s); it will not be presented here."
"the importance of the role of geography has been stressed [cit] and spatial relationships have been proposed, such as, for example, spatial clusters for assessing the neighborhood of a relevant entity in terms of other similar entities nearby, or colocation by analyzing typical patterns of nearby entities belonging to different categories [cit] ."
"the results of the experiment show that scoregr effectively calculates gr for the scenarios we tested. they also reveal that the selected criteria (topicality, spatiotemporal proximity, directionality, cluster, and colocation) combined to a single gr score can be used to effectively rank geographic entities, as humans would do using the same criteria. kendall's τ coefficients (table 2) are only significant for the correlation between the crowdsourced rank and scoregr, while there are no significant correlations between the crowdsourced rank and grbm25, or the two baselines. this also suggests that the latter three models are not adequate for complex scenarios, as tested in this experiment."
"besides, a comparison and a selection of on-board sensors, that are employed for localization, will take place, in order to improve accuracy and stability of global localization, since localization techniques fail and become less accurate in large spaces where lidar or rgb-d have travel long distances, and reduce the computational burden of slam, which might result in deployment of less expensive hardware. to this end, the key tradeoffs will be identified on the requirements both on the type and nature of the sensors embedded or installed on-board the robot and on the environment. imu will be considered for dead reckoning, cameras for line or feature following, as well as wheels encoders. moreover, the use of rssi measurements will be investigated (existing wi-fi beacons or deploy ble or uwb). ideally, only on-board sensors will be utilized hence minimizing installation time of the endorse system. however, alternative solutions, using barcodes or qr codes on the wall in well-chosen areas, will also be deployed, to make localization easier for the case of docking."
"crowdsource judgments for scenarios 1, 2, and 3. note: the red-colored circles represent the ranked entities; irr refers to entities identified as irrelevant. in scenario 1 (left) and scenario 3 (right) the arrow indicates the user's route direction from start to end; the flag represents the user's current position in scenario 2 (middle) and scenario 3 (right). in scenario 1 (left) the blue circles represent other supermarkets and the green crosses represent pharmacies. in scenario 2 (middle) the brown circles represent other hotels, the blue circles restaurants, and the green crosses pharmacies. in scenario 3 (right) the green circles represent other restaurants and the blue symbols represent pubs."
"future studies could focus on evaluating gr methods in scenarios where the criteria cluster or colocation are not as influential for the gr score, in order to test for the robustness of the proposed methods. for instance, the discussed methods can be tested in a scenario depicting a user searching for a hotel, in an area where hotels are not clustered and not satisfying any of the colocation rules. finally, we recommend testing the validity of the gr methods described as well as the criteria they are based on in field studies with users moving in the real world."
"2) how will it come?: a possible architecture for the diagnostic support module is a pluggable device powered by the robot. this device will consist of a set of non-invasive medical sensors connected to a supervising unit. this unit will be in charge of managing and handling the sensors, but also serve as a preprocessor for the medical data and a gateway to the patients cloud-based ehr. as the robot already features a communication interface, the supervising unit should be able to use this interface to transmit diagnostic reports to the cloudbased ehr."
"the correlation between scoregr and crowdsource is lower for scenario 1 and 3, where participants seem to have weighted the criterion cluster slightly higher than colocation. however, we treated the criteria cluster and colocation as equally important, when combining them to compute the geographic environment component of scoregr. for instance, entity 9115 is ranked second by scoregr, but fifth by participants in scenario 1, because it satisfies the colocation rule (pharmacies next to supermarkets) involved in the scenario well, but it is not part of a cluster. in all scenarios, the top-ranked entities belong to a cluster, according to the crowdsourced ranks. a possible explanation for this is that visual clusters are preattentively processed by the human visual system, and thus clusters are very salient [cit] . aiming for an even better approximation of the human-based ranks, further implementations of the scoregr method might therefore require a higher importance to be assigned to the criterion cluster."
"in the following we present first related work, before describing the methods by which we calculated gr and our crowdsourced relevance judgments. we then present and interpret our results, before discussing them in the context of our research questions and their broader implications."
"so far, we have calculated the movement trajectory of the foot-mounted imu and corrected the movement trajectory based on the zero-velocity state. the pseudo code for the whole process is provided in algorithm 1."
"note that in the case of adaclassifier, the process includes the identification of six different classes of deformation phenomena (that is, the test covers all the possible classification processes available). testing for fewer options will reduce the time needed to process data. [cit] )."
"the moving trajectory is relatively simpler along path i, where the tester directly walks around the hall three times in the clockwise direction. figure 4 shows the reference trajectory along path i and the different positioning results obtained based on the various algorithms. the red line indicates the reference trajectory along the path, the purple line represents the path calculation result based on the only-uwb-based algorithm and the blue line is the calculation result based on the zupt-based algorithm. meanwhile, the green line is the positioning result based on the fusion algorithm, the black squares stand for the pillar obstacles and the red diamonds represent the beacon positions. it's very obvious that both the only-uwb-based algorithm and the fusion algorithm significantly outperform the zupt-based algorithm in terms of the positioning precision. what's more, the fusion algorithm can better reflect the real movement trajectory locally than the only-uwb-based algorithm. this is because although the only-uwb-based algorithm can achieve a smoother estimate of the trajectory, which to some extent might improve the positioning process after the addition of the velocity and direction of movement, the assumption of uniform linear motion must be added into the algorithm due to the lack of the other data sources on the changes in the reference velocity and direction of movement. if this assumption has a high confidence coefficient (a smaller sampling range), although it guarantees that the only-uwb-based algorithm will have a strong anti-jamming capability for movement along a straight line, it leads to a slow response to the inflection point. in this case, it takes quite a long time for the positioning result based on the correct uwb measurement to return to the position near the real coordinates after the inflection point or a deviation from the real trajectory. however, if this assumption has a low confidence coefficient, positioning precision might be easily affected by observation noise of the uwb sensor. in contrast, the fusion algorithm won't lead to large error at the inflection point due to the availability of the reference provided by the imu data. moreover, it is highly robust against the incorrect uwb data when the movement is in a straight line. the details will be introduced in section 3.3.2. obvious that both the only-uwb-based algorithm and the fusion algorithm significantly outperform the zupt-based algorithm in terms of the positioning precision. what's more, the fusion algorithm can better reflect the real movement trajectory locally than the only-uwb-based algorithm. this is because although the only-uwb-based algorithm can achieve a smoother estimate of the trajectory, which to some extent might improve the positioning process after the addition of the velocity and direction of movement, the assumption of uniform linear motion must be added into the algorithm due to the lack of the other data sources on the changes in the reference velocity and direction of movement. if this assumption has a high confidence coefficient (a smaller sampling range), although it guarantees that the only-uwb-based algorithm will have a strong anti-jamming capability for movement along a straight line, it leads to a slow response to the inflection point. in this case, it takes quite a long time for the positioning result based on the correct uwb measurement to return to the position near the real coordinates after the inflection point or a deviation from the real trajectory. however, if this assumption has a low confidence coefficient, positioning precision might be easily affected by observation noise of the uwb sensor. in contrast, the fusion algorithm won't lead to large error at the inflection point due to the availability of the reference provided by the imu data. moreover, it is highly robust against the incorrect uwb data when the movement is in a straight line. the details will be introduced in section 2.3.2. figure 5 demonstrates the cumulative error probability distribution of path i. it clearly reveals that the fusion algorithm has a big advantage in positioning precision over the only-uwb-based algorithm and the zupt-based algorithm. table 1 provides the operation time, the mean positioning error, and the standard deviation of errors for each algorithm. these results reveal that the fusion algorithm can achieve a smaller mean error than the only-uwb-based algorithm with a lower standard deviation of errors. this means that if the computing time is of the same order of magnitude, the fusion algorithm can provide a higher precision with better stability and similar algorithmic complexity. figure 6 shows the comparison of the relevant trajectories along path ii. the direction of the movement along path ii is the same as that along path i. however, for a long time, there is a significant signal error due to the shielding of beacons by the pillar in the extended section of the path. due to the presence of many corners in the movement process, the zupt-based algorithm can't provide high precision, which is reflected by a significant error in the trajectory direction calculated based on the zupt-based algorithm after some corners. however, it can still achieve reasonable accuracy in the displacement distance. that is to say, the zupt-based algorithm is still of high reference value in the estimation of moving velocity. meanwhile, as the zupt-based algorithm can ensure that the angle calculation result for a single corner does not vary much from the real value, it's still of significant effectiveness when acting as the input of the fusion algorithm. the only-uwbbased trajectory demonstrates the similar characteristics of the paths. in other words, it can provide high accuracy under los conditions. however, great deviations will arise when there is a barrier nearby. this is mainly because, at this moment, there is a large error in the uwb observation value, which can be observed in figure 7, where the red line represents the distance between the beacon and the tag after the calculation based on the reference trajectory. the rms errors of the beacons over the whole process are 0.456, 0.626, 0.737, and 0.323. the blue line stands for the measurement value of the uwb sensor. table 1 provides the operation time, the mean positioning error, and the standard deviation of errors for each algorithm. these results reveal that the fusion algorithm can achieve a smaller mean error than the only-uwb-based algorithm with a lower standard deviation of errors. this means that if the computing time is of the same order of magnitude, the fusion algorithm can provide a higher precision with better stability and similar algorithmic complexity. figure 6 shows the comparison of the relevant trajectories along path ii. the direction of the movement along path ii is the same as that along path i. however, for a long time, there is a significant signal error due to the shielding of beacons by the pillar in the extended section of the path. due to the presence of many corners in the movement process, the zupt-based algorithm can't provide high precision, which is reflected by a significant error in the trajectory direction calculated based on the zupt-based algorithm after some corners. however, it can still achieve reasonable accuracy in the displacement distance. that is to say, the zupt-based algorithm is still of high reference value in the estimation of moving velocity. meanwhile, as the zupt-based algorithm can ensure that the angle calculation result for a single corner does not vary much from the real value, it's still of significant effectiveness when acting as the input of the fusion algorithm. the only-uwb-based trajectory demonstrates the similar characteristics of the paths. in other words, it can provide high accuracy under los conditions. however, great deviations will arise when there is a barrier nearby. this is mainly because, at this moment, there is a large error in the uwb observation value, which can be observed in figure 7, where the red line represents the distance between the beacon and the tag after the calculation based on the reference trajectory. the rms errors of the beacons over the whole process are 0.456, 0.626, 0.737, and 0.323. the blue line stands for the measurement value of the uwb sensor."
"the endorse project's localization solution will be geared towards infrastructure-less multi-robot navigation. in that context, the robustness of slam techniques used by en-dorse will be enhanced, so that the navigation capabilities of the fleet will be fostered."
"a major distinction has to be made between objective (1) and subjective (2-5) relevance. the former has a long history of use in ir as a measure for the effectiveness of the retrieval process and is typically captured in precision and recall. the underlying assumption of the system or algorithmic relevance is that a system is objectively capable of assessing the relevance of documents from independently of the user. many researchers (e.g., [cit] suggested a more flexible approach going beyond simple binary relevance by measuring the semantic similarity of terms found in documents and query terms and then ranking the documents accordingly."
"otherwise, i list some comments below which i hope you find useful. it would be great to involve you in future discussions around c4mip analysis and experimental design."
"accurate measurement of membrane peeling forces in the presence of micro-vibrations requires not only sub-mn forcesensing resolution but also a very fast responding force sensor. the transient response of the force-sensing tip was monitored using the setup shown in fig. 4a . first, the microforceps module was mounted onto micron, and the forceps tip was held between two elastic rubber bands. then, micron was given a step input to move the tool tip towards one side laterally while the resulting reaction force was recorded. the tests were repeated for 3 levels of step amplitude (50, 150 and 250 µm). in all case, the measured force profile matched a first order system response with 0.005 s time constant as shown in fig. 4b, proving a fast enough response to track rapid force variations even in the presence of high frequency micro-vibrations (in our case up to 50 hz)."
"the output is made of two esri shapefiles, the first one containing the polygons defining the boundaries of the adas; the other contains the set of pss passing the set of filters set by the methodology. figure 1 depicts the user interface of the gui (graphical user interface) version of the adafinder tool."
"generally, the output frequency of a uwb tag is between 2 and 4 hz, while the output frequency of an imu is between 100 and 1000 hz. since there is a big difference between the data output frequencies of the sensors, it is necessary to align them on the time axis before integrating them. the algorithm in this section constructs a transformation between these two sensors without a rigid connection."
"thus, a read-map file is just an extra options file (see section 4.3) where, by means of couples of labels and values, the positions of the attributes on which each tool rely are specified."
"there are various factors affecting the forces in membrane peeling. some of these pertain to tissue properties, such as tissue width and thickness, while some are related to the motion of the peeling instrument, such as the peeling speed. in order to isolate the influence of micro-vibrations, all other factors affecting the peeling force need to be eliminated in a very reproducible experimental setup. using a handheld micromanipulator, it is hard to keep the peeling speed constant. to avoid peeling speed alterations during and between trials, we fixed the micron handle to a clamp, and used a linear stage to drive phantoms relative to micron (fig. 5) . the micro-forceps module was attached onto micron for grasping the phantom before peeling. this setup is only for identifying the effect of microvibrations in membrane delamination. the practical use of our system will work based on this identified behavior, but will be handheld (not with a linear stage) as shown in figs 1 and 2."
"endorse is a running three-year project under the umbrella of the marie skłodowska-curie action (msca) -research and innovation staff exchange framework (rise) [cit], hence this concept paper described research work-in-progress. endorse implementation will offer a great opportunity to exchange knowledge, know-how and experience between the smes and the creative academic groups constituting the endorse consortium. the ultimate goal is the successful research and commercial follow-up emanating from a challenging r&d topic that intends to address major issues in the industry in general and in healthcare provision in particular."
"the current finding in hospital is that numerous trolleys or carts with medical devices are disseminated in every corridor of the building, often left alone without any supervision. nurses are losing time searching for them and this can unfortunately happen when they need it urgently. another risk is that patients having an easy and free access to medical devices on the trolleys can increase the risk of equipments deterioration or degradation. the e-diagnostic module brings a first answer to this problem gathering in one place and at the right moment all the necessary medical equipment a nurse could need for a medical check-up. another asset is that all the embedded sensors in the module are already powered on and can be used immediately, which is not the case when all the medical devices are left on a trolley. as all the patients' medical data are instantly sent to a cloud-based server, this also limits human mistakes or loss of papers."
"i found this a useful and thoughtful paper which makes some very salient comments about existing experimental design and offers some insights into the limitations of the c1 standard experiments compared to new \"logistic\" co2 pathways. the paper show cases the new pathways using the uvic emic."
"in the project, a creation of an api framework will be accomplished, which will integrate logistic robotics into the corporate it environment, and enable end-to-end automation (automatic traceability of the transferred goods, update of the erp databases when transactions are carried out, etc.). moreover, communication and security standards will be investigated in order to securely transfer data from the robot to the patients cloud-based ehr. to realize this task, ros-based software integration will be pursued, where all system modules will be represented by ros packages and ros nodes. non-ros modules (i.e. java or javascript applications) will have apis that create objects that can be translated by ros bridge package into ros standard objects (topics, services, actions, etc.)."
"chris jones 1. in several places, including the abstract and conclusions the paper mixes up features of the models/results with features of the experiment itself. for example you say sinkto-source transition is \"absent from the 1% experiment\". i think you should be a bit stricter in which phrasing you use -the sink-to-source transition is neither present nor absent in the 1% experiment -but it will depend on the results. it may or may not occur depending on the model. you might be able to say it is more likely in one set of model runs than another, but it is not a \"feature of the experiment\"."
"the toolset has undergone a very strict testing process, using either real datasets and the comparison of manual and automated results to check their validity or synthetic ones when these were not available-that is, for adaclassifier and los2hv. in this case, however, the software is being actively tested nowadays, and, although no results are available yet, the partial outcomes are promising, thus confirming, at the time being, the positive results of the formal validation process."
"industrial infrastructures and transit warehouses, where they offer significant economic and administrative advantages. in contrast, in spaces such as hospitals, nursing centers, hotels, museums, malls, commercial spaces, offices and retail stores, which herein for economy of space we call commercial spaces, the use of autonomous robots is very limited. the use of existing robotic systems designed for large industrial and warehouse spaces is not applicable to indoor commercial spaces because of the different specifications and different constraints. a fundamental difference is that large industrial and warehouse spaces are characterized by very structured and predictable environments where robots move in predefined paths and interaction with humans can easily be avoided. contrary to the industrial warehouses, commercial spaces are less structured, the environment is much more dynamic and the interaction with humans is frequent. also, the load specifications are different and the interior architectural specifications are much more constrained. automation of the supply chain in commercial spaces could have a high economic impact, given the large number of daily transfers taking place in these premises. for example, in a hospital or in a health care center, several tenths of person months are spent annually on transfer of goods, linens, biological samples, medical equipment, pharmaceuticals, mail parcels, and medical waste, amounting to more than 850 man hours per week in a 500 beds hospital [cit] . these transfers are currently carried out manually by pushing carts for long distances by nurses or carers. this is a no-added-value job which deprives the patients from valuable nursing duties. the same arguments apply to the majority of commercial spaces listed earlier. in addition, automation of the supply chain will enable enhanced traceability of the goods and of the products from the time of the request until the moment of delivery to the recipient. in other words, a well programmed logistics system will improve the services offered to the end-user (i.e., the hotel customer or the patient and the carer in a hospital), by reducing the response times for each request. hence, the indoor supply chain is an application that calls for automation and at the same time has high commercialization potential because the involved corporations and organizations hold a huge part of the market, they have specific practical problems, and seek specific solutions for which they are willing to pay."
at the lowest level of hierarchy is the signals level which involves the process of raw signals and motor control such as the wheel motors. the second level of hierarchy is that of the motion level. this represents information in terms of the module and its environment. each module's controller accepts asynchronous commands to move and turn with respect to a common reference frame.
"although commonly used criteria such as in ir (topicality) (e.g., [cit], in lbs (spatial proximity) [cit], or in gir (topicality and spatial proximity) [cit] seem appropriate and effective by respective applications, crowdsourced responses from our experiment indicate that they are not sufficient for understanding gr. spatial proximity (represented by baseline1 and baseline2) commonly implemented in current mobile information services, such as lbs, show weak correlations with human judgments, and suggest that spatial distance alone is not enough for assessing the relevance of geographic entities for specific context-dependent and realistic information needs."
due to its integrative nature this task will be implemented iteratively and will be tightly inter-leaved with safety humanrobot interaction protocols. an initial integrated software prototype will be developed given the prioritized system requirements and the output of the implementation. then this initial prototype will be integrated with the cloud-based services. the evaluation will be done in an iterative manner resulting in intermediate versions. the special requirements of the healthcare development are already identified and the development process will use them as a solid basis.
"several software applications are needed in the context of this demonstrator. this paper describes those developed at cttc, namely, adafinder, adaclassifier and los2hv. the first one, adafinder, is a tool to detect and update adas (active deformation areas) using sentinel-1 imagery and psi. the navarro, j., cuevas, m., tomás, r., barra, a. and main goal of such application is to update and assess the geohazard activity (volcanic activity, landslides or ground subsidence among other phenomena) of a given area. adaclassifier goes a step beyond adafinder, trying to classify the kind of deformation process undergone by the adas -that is, is the detected deformation process a landlisde, a sinkhole, or what else? finally, los2hv computes the horizontal and vertical components of the movement measured along the los (line of sight). the horizontal component of such movement is one of the inputs of adaclassifier; therefore, a tool to compute it was needed."
"peeling tests on the bandage phantom were done in 2 sets, each set having a different speed setting (0.15 mm/s and 0.3 mm/s). in each set, a total of 10 operational modes were examined. in the first mode, delaminating forces during regular peeling were monitored. the remaining modes explored the effect of micro-vibrations at 3 frequencies (10, 30 and 50 hz) and 3 amplitudes (50, 100 and 150 µm). 15 trials per mode were completed using a single bandage strip for each speed setting. each bandage was peeled and brushed back 10 times before starting the trials, so that the adhesion between the bandage and its backing remained consistent throughout the experiments (fig. 6 ). for the egg trials, the experimental conditions were limited to 1 speed setting (0.15 mm/s), 2 frequencies (30 and 50 hz) and 2 amplitudes (100 and 150 µm). ten shell membrane strips were peeled for each setting, and each strip was used only once."
"this study builds on our previous work [cit], explores the effect of the main micro-vibration parameters on membrane peeling forces, and aims to establish an adaptive control algorithm ( fig. 1) for regulating micro-vibrations during the procedure. in the following sections, we will first present the force-sensing micro-forceps system. this will be followed by the experimental investigation of micro-vibrations during membrane peeling on two types of phantoms: artificial bandages and raw chicken eggs."
"with regard to the integrated filtering, or in other words, the filtering of the imu-based result and the uwb observation, the pf (particle filter) framework is adopted as indicated in figure 1 . the main reason is that the uwb observation model is a highly non-linear model and cannot be well described by a linear model. meanwhile, there is a large error when the gaussian distribution is used to approximate the observation error probability distribution. through the particle filter algorithm, a large number of particles can perform very well in approximating the system state under non-linear conditions. ekf algorithm can reduce the consumption of computing resources. hence, it's feasible to adopt the ekf algorithm in the process where the imu data are used to calculate the velocity and direction of pedestrian movement. the key to this process mainly lies in the linear representation of the state transition equation and observation equation with the specific procedure provided in section 1.3. with regard to the integrated filtering, or in other words, the filtering of the imu-based result and the uwb observation, the pf (particle filter) framework is adopted as indicated in figure 1 . the main reason is that the uwb observation model is a highly non-linear model and cannot be well described by a linear model. meanwhile, there is a large error when the gaussian distribution is used to approximate the observation error probability distribution. through the particle filter algorithm, a large number of particles can perform very well in approximating the system state under non-linear conditions. represents the state of the th particle at time − 1 and weight t − 1 [ ] is the corresponding weight of the ith particle at time − 1. assume that the current system state only depends on the system state at the previous moment and the current input. then, the system state can be updated according to the following steps, which are illustrated by the flowchart in figure 1: 1. as indicated by sample in the corresponding flowchart, perform the sampling according to the distribution state transition equation. 2. as indicated by evaluate in the corresponding flowchart, update the weight of every particle according to the observation model and the observation values. 3. as indicated by resample in the corresponding flowchart, use the resampling method to restrain the particle attenuation."
"as the zupt-based pdr algorithm can be linearized perfectly, assume that the observed value is the zero velocity obtained from the zero-velocity detection. then, the observation equation is linear. therefore, the ekf algorithm has the minimum linear error among these types of algorithms. meanwhile, as the frequency of outputting data from an imu is very high, the adoption of the simple ekf algorithm can reduce the consumption of computing resources. hence, it's feasible to adopt the ekf algorithm in the process where the imu data are used to calculate the velocity and direction of pedestrian movement. the key to this process mainly lies in the linear representation of the state transition equation and observation equation with the specific procedure provided in section 2.3."
"in real scenarios, the zero-velocity phenomenon can intermittently occur when people are walking. in normal cases, the human head moves roughly in the same direction and at the same speed as the human body. to estimate the movement state of the head based on the foot movement, the following relation can be utilized. assume that v f is the velocity of the foot movement, θ f is the direction of the foot movement, v b is the velocity of the body movement and θ b is the angle of the body movement. although it's very difficult to express the relationship between the instantaneous velocity and angle of the foot movement with the velocity and angle of the body movement, without loss of generality, we can assume that the average velocity v f from the moment the foot starts to move to the moment it stops moving is equal to the velocity of the body movement v b . then, according to the foot and body movement rule, which will be explained later, we obtain the following equation:"
"through the above formulas, the approximation of the prior distribution at time t can be achieved. then, we need to update the particle weight according to the observation to obtain the posterior distribution of the particle. in addition, we update the particle weight based on the weight after the particle sampling according to the data from the virtual odometer and the current uwb observation. in other words, we multiply the prior probability by the likelihood probability to obtain the posterior probability with the update formula provided below:"
"the robotic revolution is as much about getting the machines to adapt to humans as it is about humans adapting to the machines. this convergence requires intuitive hri so that untrained personnel in a healthcare center can smoothly interact with the robot to avoid deadlocks, while the robot itself is also capable of adapting to the crowd and conducting human-aware navigation. security aspects also play an important role, so that logistic tasks are addressed to the authorized personnel, leveraging communication and message activation systems that further minimize stress and maximize the comfort of people interacting with the robot. these functionalities will be developed and accommodated by the ros perception packages that run on-board the robot."
"the cpa operator combines the \"mandatory\" spatiotemporal score with the \"desired\" directionality score in a conjunctive manner, that is, the spatiotemporal score is a starting value and incremented or decremented, depending on whether the directionality score is greater or lower than the spatiotemporal score, and on the \"and-ness\" of the partial conjunction. if the spatiotemporal score is zero, the combined output will also be zero. the scores for cluster and colocation are to a score for the geographic environment, respectively."
"the first auxiliary function computes the average distance for a given context dimension c, while the latter computes the number of objects with equal or shorter distance to a user for a given object. for comparison, we include two additional methods as baselines relying on simpler assessment models. the first baseline method reflects a simple lbs approach and will be referred to as baseline1 (table 1) in the following. given a query, baseline1 [cit] not match a user query, and orders the remaining entities according to the length of user's movement path (i.e., the distance from the user's current location to the location of the entity, and then to the destination). the second baseline method is referred to as baseline2 (table 1) . baseline2 combines the topicality score with a distance score computed as the inverse of the length of a user's path (i.e., normalized in the range [cit], dividing it by the maximum obtained value), using the geometric combination method employed in the spirit project [cit] . table 1 summarizes the methods of gr assessment tested in the experiment and the relevance criteria included in our proposed methods. our evaluation follows the common benchmark-based testing of ir systems, where system relevance output is compared to relevance from judgments from humans, set as a benchmark. [cit], which uses simple pairwise comparisons of preference judgments of similar music pieces against an item chosen as a pivot and also allows for judging items as equally relevant. in order to evaluate the validity and effectiveness of these two gr assessment methods, scoregr and grbm25, we designed a user-based experiment. we defined effectiveness as the similarity between the relevance rank produced by our gr computational model and the entities ranked using relevance judgments performed by experiment participants through crowdsourcing, which we consider as ground truth."
"a diagnostic support module and the electronic health records (ehr) interfacing will be also included. this will require research and development in both non-invasive sensors/devices (ecg, blood pressure, oximeter, temperature) and dedicated software applications. these sensors could be based on embedded microcontrollers processing units, with 32-bit arm-cortex soc. this chip also will incorporate a complete communication layer that make the device compliant with ble4.2, ble5.0, ipv6, ant/ant+ and nfc-a tag protocols. in accordance of other magnitudes, the device could be equipped with a 9-axis inertial measurement unit (imu) that makes possible monitoring the activity and real-time orientation. when the diagnostic support module is mounted on the robot chassis the overall robot will serve as an ediagnostic mobile station, which will be able to produce a diagnostic exam report that will be securely forwarded to update and complement the patients cloud-based ehr."
"as we also assume that utility grows less than linearly with decreasing distance, we define an auxiliary function dstprox as a square root function of the inverse of the distance (see equation [cit] ). utility is zero if an entity is not available for the time a user has available to accomplish the activity."
"the next level is the task level where a software platform of the fleet management system runs, i.e., task planner, robot dynamic routing, multi-map manager, elevator controller, etc."
"the solution to the aforementioned limitations, and the overall goal of the endorse concept, is to design and develop integrated logistic robotic systems that will enable efficient, cost-effective and safe indoor logistic services for hospital spaces."
"taking account of local, geographic context, and retrieving geographically relevant information for users in a given context is thus a key challenge for information science. mobile users often seek to solve spatial problems or answer spatial questions in the physical world and therefore establish relationships between spatial concepts in their minds, objects in physical space, and their representation on a mobile device. this degree of situatedness goes beyond topicality and includes concepts that are particular to information seeking in a mobile context, such as personal mobility opportunities and limitations, environmental factors, simultaneously available activities, and affordances of places in the physical world. having recognized this need, the notion of geographic relevance (gr) was introduced in geographic information science [cit] . gr refers to the relevance of a representation of a geographic entity (i.e., a physical entity, such as a restaurant or a mountain), given a specific context of interaction with its representation, such as a point of interest on a digital map embedded in a specific, typically mobile, usage context. gr is thus expressed as the relation between a geographic entity and a human information need. thus, although geographically referenced documents and documents containing geographic information may be a source of information in judging the gr of an entity, they are not the objective of the relevance assessment."
"four levels of certainty are defined by the methodology: \"it is an x\", \"it could be an x\", it is not an x\" and \"x has not been checked\", where \"x\" stands for each of the six deformation detection processes executed by adaclassifier-for instance, when talking about landslides, \"it is a landslide\" would be one of the values of the corresponding attribute."
"the tool accepts (input) and produces (output) esri shapefiles for compatibility reasons. on output, los2hv creates two files, to store, respectively, the horizontal and vertical components of the movement as observed along the los."
"our findings support both the assumed static influence of the spatial layout of the geographic features and the dynamic influence of the user's context and mobility on gr assessment. this is coherent with prior conceptualizations of static and dynamic context in the literature (e.g., [cit] . the static gr component corresponds to the geographic environment in our computational model (see figure 1 ). as part of the context component of relevance [cit] it encompasses the spatial configuration and spatial relations of entities beyond spatial proximity, such as colocations, clusters, connectedness, etc. our results also confirm that the relevance of a single entity increases if there are several entities of the same category in a neighborhood (cluster) [cit] or if it is located next to an entity of a related category that is typically in close spatial proximity (colocation) [cit] . these criteria are \"the differences in situational contexts and research task requirements\" [cit] ) that separate gr from the concept of relevance commonly applied in classic document-based ir. we believe that the geographic environment, that is, the spatial configuration of geographic features, defines a kind of basic or elementary geographic relevance."
"t, which represents that the movement velocities on all three axes are zero. at this moment, the observation equation is linear, which can be expressed as follows:"
"cloud computing delivers ubiquitous, 24/7 access to software-as-a-service (saas) solutions, capable of adapting to varying degrees of user load. moreover, cloud services must guarantee what is known as a service level agreement (sla). typically, slas have been customarily specified as some percentage time during which the functionality of the service is available. guaranteeing the sla of a service is, thus, intrinsically related to the management of the service fig. 1 . the commercially available robotnik rb1-base robot navigating in a healthcare environment life cycle, and the events driving it: initial deployment, reconfigurations, upgrade deployment, fault handling, re-scaling. automation makes it possible to manage the data centres so that they deliver portions of themselves (in the form of virtual machines and networks) in a cost-effective manner. cloudbased applications will offer to the enterprise users a high memory service, data sharing and security. cloud servers will be able to connect with different individual applications and enterprise legacy systems. the cloud service proposed will contain a repository of algorithms, which will be fed by the data gathered through the cloud. the algorithms in the cloud will allow supporting the management of fleet, including the assignment of tasks to the robots, the queuing functionalities, the generation of dynamic routing and traffic control of the robots, and the supervision the fleets task execution. developing a cloud oriented software platform will facilitate the development of apis for integrating endorse software with corporate erp solutions and heath record systems that are often provided as a service in a cloud environment. most importantly the endorse applications will comply with the high safety standards for data sharing and data storage which are offered by the cloud infrastructure. cloud-based services will run on a cloud server and will be interfaced by the user through a web-based application."
"the results are shown in figures 6a and 6b . the left side of the figure depicts the distribution of pss and their velocities. on the right side, the 72 adas detected by adafinder are shown. note the qi (quality index) values is measuring the reliability (certainty) of the detection process. the meangins of these values are: (1, red): very reliable; (2, orange): reliable; (3, yellow): not so reliable; (4: green): not reliable."
"the output of adaclassifier is another file with adas, where the set of attributes characterizing each of them has been extended to include six more. each of them state the possibility that the ada belongs to the corresponding deformation process. this is so because all the detection algorithms are applied to each ada. therefore, and although incongruous, some adas might be classified as positives in more than one deformation process."
"here, we give a concrete example for the criterion of spatiotemporal, showing how we compute a distance and the respective relevance score. we assume that a user submitting a space-time query at a given location in space and time wishes to reach a destination within a given time. based on the concept of space-time prisms [cit] ), we take into account the time needed to reach an entity, and a time budget available to perform an activity at that location, also considering the temporal availability of the entity. we thus define the δstprox distance function as a ratio between the time needed to fulfill an activity, and the time a user is able to spend at a location of a given entity, while the entity is also available in terms of time of day"
"3. i'm not sure of the value of plotting the compatible fossil fuel emissions for either the 1% or logistic scenarios. to me this is not a relevant quantity. i think experiments should either be \"realistic\" -i.e. follow a plausible scenario to try to derive useful information about how the real world may unfold, or be \"idealised\" -i.e. stripped down or simplified in some way to aide understanding of the system. both have great value, but shouldn't be mixed. the fossil fuel emissions that would be required to follow a scenario are only really an interesting quantity in the first case. in the second case i don't c5 think they have either scientific nor policy interest. so i would stick to showing more process-based quantities, such as the land/ocean components, the airborne fraction etc. but not the fossil emissions."
"aiming at fusion positioning based on uwb and imu data, this paper proposes an approach to fuse the results based on a particle filter with the calculation of the virtual odometer which, in turn, is based on the imu data. in addition, the effectiveness of the fusion algorithm is verified and illustrated through an experiment. the fusion positioning can achieve high precision in an nlos environment when the data on the movement trend are obtained through the calculation of the imu data. meanwhile, with the addition of the prior information from the imu, the sampling accuracy in every step is improved, which makes it possible to approximate the real posterior probability distribution with a smaller number of particles. therefore, the fusion algorithm can achieve high precision with fewer particles. it's very clear that the fusion algorithm can achieve a stable positioning result based on a smaller number of particles, as it can make good use of the zupt-based result as the prior information for the sampling process. compared with the only-uwb-based algorithm, the fusion algorithm only requires an additional computation based on the zupt-based algorithm. however, the zupt-based algorithm, in essence, is an ekf-based algorithm with far lower time consumption than the particle filter algorithm, but it does not have a big influence on the total computing time of a particle filter-based algorithm. all of these prove that the fusion algorithm can achieve a higher positioning precision with lower time consumption."
"speed, flexibility and adequate gui interfaces make the toolset a very apt tool for research activities, where different scenarios-usually defined by variations on the input parameters-may change the results. on the other side, the availability of the command-line versions of the tools makes possible to integrate them in more production-oriented environments, thanks to the use of option files. furthermore, it is possible to use the toolset as a black box to include it in higher level software components, since c++ classes have been included in the library implementing the logic of the three applications. last, but not least, the tools, in their gui versions, may be integrated quite easily in either arcgis or quantum gis-there is no need to modify the source code; this may be done using the mechanisms provided by these gis tools. in this way, the applications become an extra set of tools available in the regular work environment of a gis operator."
"we first transformed the crowdsourced answers into ranks for each scenario, to be used as \"ground truth.\" to compare crowdsourced ranks with the ranks generated by baseline1, baseline2, scoregr, and grbm25 we computed the kendall's τ correlation coefficient. the resulting statistics for the three scenarios are reported in table 2 ."
"in the case of adafinder, the manual methodology had been used for some time in gis environments when this tool was developed, so datasets including both inputs and outputs were available. therefore, the tests consisted essentially in comparing the results of the manual procedure with those created by the tool."
", where f is the state transition equation. generally, pedestrian movement can be described perfectly as uniform motion or uniformly variable motion. considering that the direct application of the movement velocity and angle requires the alignment of both coordinate systems and the relationship between both sensors is not currently clear, it's feasible to take the rates of change in the velocity and angle as the connection between these two coordinate systems to simplify the operation and modelling. therefore, the particle state includes the coordinate, linear velocity, and angle. at this point, the particle state vector"
"the first tool in the set is adafinder. this application is used to identify those areas that are undergoing a deformation process, whatever it is. additionally, an assessment of the quality of the detection process is performed."
"both the command-line and gui versions of the applications in the toolset rely on option files to retrieve the information defining how to proceeed-input or output files, thresholds controlling some condition, etc. note that this is so for the gui-based tools too; in fact, the gui is just a mechanism to fill the gaps in an option file template. this simplifies the design of the classes implementing the logic, since only one interface (the option file) needs to be taken care of. the commmand-line incarnations of the three tools have a single parameter: the name of the options file with the program's parameters."
"aiming at fusion positioning based on uwb and imu data, this paper proposes an approach to fuse the results based on a particle filter with the calculation of the virtual odometer which, in turn, is based on the imu data. in addition, the effectiveness of the fusion algorithm is verified and illustrated through an experiment. the fusion positioning can achieve high precision in an nlos environment when the data on the movement trend are obtained through the calculation of the imu data. meanwhile, with the addition of the prior information from the imu, the sampling accuracy in every step is improved, which makes it possible to approximate the real posterior probability distribution with a smaller number of particles. therefore, the fusion algorithm can achieve high precision with fewer particles."
"the tools are also flexible, being able to adapt themselves to variations in the format of the input shapefiles thanks to the use of the so-called \"read-map files.\""
"1. the system or algorithmic relevance independent of the context and measures how well the query topic and document topic match; 2. topical or subject relevance (aboutness, topicality); 3. cognitive relevance or pertinence (informativeness, novelty); 4. situational relevance or utility (usefulness in decision making, reduction of uncertainty); and 5. the motivational or affective relevance (satisfaction, success)."
"material. all three scenarios are set in an urban environment. as map data we used unlabeled open street map data for madrid, spain (see figure 2), including points of interest and a street network [cit] . we rotated the original map data by 105 degrees counterclockwise and displayed it at a large scale, that is, ∼1:8,000. with these measures we address the problem that users' familiarity might confound our results, since we assume it unlikely that participants recognize the represented geographic area. in the first scenario, a user searches for a supermarket while returning home from work. in the second scenario, a user is searching for a hotel in the area where she/he is attending a conference. in the third scenario, a user is searching for a restaurant. an example of a question for scenario 3 is shown in figure 2 ."
"the need for a comprehensive concept of gr applying to representations of real-world geographic features was largely ignored until mobile technology advanced 3g mobile networks were implemented making location-based services (lbs) viable propositions [cit] b) . many lbs provide simple relevance filtering [cit] ) based on a user's current position (e.g., show the nearest 10 restaurants). [cit] ."
"since the cell response can be only a non-negative value in accordance with the basis of the biological theory, the method of half-wave rectification is used in processing; that is,"
"in this and the next two sections, we describe the results, in terms of runtime speed-ups, comparing the query plan generated by hybrid inference with a non-hybrid solution. table 2 summarizes the results of hybrid inference for different queries over different models."
"in this section, we first describe ie models that are cyclic (e.g., the skip-chain crf model) and review the way that simple relational queries can often induce cyclic models-even over text that is itself modeled by simple linear-chain crfs. such cyclic models call for an efficient general-purpose inference algorithm such as an mcmc algorithm. next, we describe our efficient in-database implementation of the gibbs sampler and mcmc-mh. finally, we discuss query-driven sampling techniques that push the query constraints into the mcmc sampling process."
"the query-driven geninitworld() function generates an initial world that satisfies the constraint. the first \"qualified\" sample can either be specified according to the query or generated from random samples."
"in contrast, our query-driven sampling approach pushes the query constraints into the mcmc sampling process by restricting the worlds generated by geninitworld(), genproposals() and gensamples() functions, so that all the samples generated satisfy the query constraint. one of the advantages of the mcmc algorithms is that the proposal and sample generation functions can naturally deal with the deterministic constraints, which might induce cliques with high tree-width in the graphical model. such cliques can easily \"blow up\" the complexity of known inference algorithms [cit] . we exploit this property of mcmc to develop query-driven sampling techniques for different types of queries."
"(v) a script to estimate the lysine pool dynamics from independent measurements, such as those that can be obtained from the blood serum (fitdirectpool.m); (vi) a script to generate simulated datasets that can be used for testing purposes (simdataset.m); and (vii) a series of supporting scripts that are used by previous scripts (readdataxls.m, writedataxls.m, fitsetwritexls.m, fitsingle.m, timemsg. m, gentemplatexls.m, fitglobalpar.m and fitset.m). all these scripts are based on matlab, fully annotated and entirely inspectable and modifiable based on the needs of specific applications. we compiled all the scripts, and we explain them stepwise in this protocol in a way that allows their utilization by users with no previous experience with matlab."
"in the ilp method, sentence selection is done by considering the concepts that a sentence contains. it is difficult to add indicative features in this framework to explicitly represent the sentence's salience, and more importantly, its novelty for the update summarization task. this information is only captured by the weights of the bigrams using the method described above. therefore, we propose to use a two-step approach, where an initial ilp module first selects some sentences and then a reranking module uses sentence level features to rerank them to generate the final summary. we expect this step of modeling sentences directly can complement the bigram 1 note that we do not use all the sentences in the ilp module. the 'relevant' sentences are those that have at least one bigram with document frequency larger than or equal to three. centric view in the first ilp summarization module. for the first step, we use the ilp framework with our supervised bigram weighting method to obtain a summary of n words (n is greater than the required summary length l). note that the ilp model selects these output sentences as a set that optimizes the objective function, and there are no scores for each individual sentence. second, we use sentence level features listed in table 1 to rerank the can-didate sentences. this is expected to better evaluate the salience and the novelty of the sentences. we use a regression model (svr) for this reranking purpose. when training the model, a sentence's rouge2 score compared with the human generated summary is used as the regression target. after reranking, we just select the top sentences that satisfy the length constraint to form the final summary. in this work we do not use any redundancy removal (e.g., mmr method). this is because the ilp decoding process tries to find a global optimal set maximizing the concept coverage, subject to the length constraint, and thus already considers redundancy among sentences. typically when the initial set (i.e., the output from the first ilp step) is not too big, redundancy is not a big problem."
"in the group by and the having clause, we can specify conditions on an entire text \"document\". an example of such aggregate condition over a bibliography document can be that all title tokens are in front of all the author tokens. following the possible world semantics [cit], the execution of these relational operators involve modification to the original graphical models as will be shown in section 4.1."
"the size of the label space of the model is also an important factor of the runtime of all the inference algorithms. the runtime of the viterbi and sum-product algorithms is quadratic in the size of the label space, while the runtime of the gibbs algorithm is linear in the label space because each sampling step requires enumerating all possible labels."
"we envision that the query parser takes in a \"sql+ie\" query and outputs, along with others non-hybrid plans, a query plan which applies hybrid inference over different documents. the algorithm in figure 7 generates a hybrid inference query plan for an input \"sql+ie\" query q, consisting of the relation part qre, and the subsequent inference operator q inf . in line 1, the relational operators in qre are applied to the crf models underlying the base tokentbl tables, resulting in a new crf model crf * . in lines 2 to 3, selection and join conditions on the deterministic attributes (e.g., docid, pos, token) are applied to the base tokentbl tables, resulting in a set of tuples t, each of which represents a document or a document pair. in line 4, the model instantiation is applied over t using crf * to generate a set of \"ground\" models groundcrfs. in line 5, a split operation is performed to partition the groundcrfs according to their model structures into linearcrfs, treecrfs and cycliccrfs."
"belongs, the bihcd algorithm of this paper uses low-level visual cues in the same way; therefore, it has characteristics such as fast speed and database-independent detection performance. in contrast to the canny algorithm, which uses only a single low-level image feature, our model uses the colour opponency characteristics of early vision to enhance the detection of colour boundaries and luminance boundaries while simultaneously combining with the surround inhibition effect under the modulation of multiple features, such as colour, distance, and direction, to effectively suppress the unwanted texture inside the image contours. therefore, the contour detection indices-f-measure and ap-value-are greatly improved."
"where c i and s j are binary variables that indicate the presence of a concept and a sentence respectively. l j is the sentence length and l is the maximum length (word number) of the generated summary. w i is a concept's weight and occ ij means the occurrence of concept i in sentence j. the first two inequalities associate the sentences and concepts. they ensure that selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept only happens when it is present in at least one of the selected sentences."
"lines 6 to 19 capture the rules for choosing inference algorithms we described in section 5.3. finally, a union is applied over the result sets from different inference algorithms for the same query."
"c critical step rodents have the tendency to eat fecal pellets. to avoid the influence of the unlabeled amino acids from the feces on the labeling, it is important to make sure that the unlabeled feces are not eaten, because this might interfere with the experiment outcome. once the feces are mostly labeled (after~4-5 d), it is not so critical to eliminate fecal pellets. an alternative is to use metabolic cages, which collect the fecal pellets in a compartment inaccessible to mice, although this might affect mouse behavior. 6 after the desired pulsing time, prepare the dissecting tools, the materials and the solutions that will be used for the chosen tissue preparation and subcellular fractionation procedure. 7 kill mice using the appropriate method, in conformity with local guidelines. 8 collect mouse tissues of interest rapidly in 15-ml conical tubes filled with ice-cold hbss buffer. 9 wash the blood away from the organs thoroughly, changing the ice-cold hbss buffer 2-3 times."
"protocol 76 use the 'generate' button to create an excel file containing the headers necessary for the correct import of the data and save it in the same directory where the scripts are. 77 copy all experimental heavy/light ratios into the corresponding columns of the generated file, directly under the header rows. copy the corresponding protein ids into the left column. save the modified file. c critical step make sure all experimental data points in the excel file are heavy-versus-light ratios. if correctly input, the ratios will be internally converted to heavy-versus-total during the import step. missing conditions and replicates will be ignored. 78 in the 'import' tab of the graphical user interface (fig. 8), load the excel file created in step 77."
"the correlation strength is the relative strength of the transition correlation between different label variables over the state correlation between tokens with their corresponding labels. the correlation strength does not influence the accuracy or the runtime of the viterbi or the sum-product algorithm. however, it is a significant factor in the accuracy and runtime of the mcmc methods, especially the gibbs algorithm. weaker correlation strengths result in faster convergence for the gibbs sampler. at the extreme, zero transition correlation results in complete label independence, rendering consecutive gibbs samples independently, which would converge very quickly."
"1. apply query over model: apply the relational part of the query qre over the underlying crf model; 2. instantiate model over data: instantiate the resulting model from the previous step over the text, and compute the important characteristics of the grounded models; 3. partition data: partition the data according to the properties of grounded models from the previous step. in this paper, we only partition the data according to the shape of the grounded model. more complicated partitioning techniques, such as one based on the size of the maximum clique can be considered for future work; 4. choose inference: choose the inference algorithms to apply according to the rules in section 5.3 over the different data partitions based on the characteristics of the grounded models; 5. execute inference: execute the chosen inference algorithm over each data partition, and return the union of the results from all partitions."
"based on our experience in implementing different inference algorithms, we also present four design guidelines for implementing statistical methods in the database in the appendix."
"secondly, when the resulting crf model is instantiated (i.e., grounded) over a document, it could result in a grounded crf model with drastically different model characteristics. for example, the crf model, resulting from a join query over a linear-chain crf model, when instantiated over different documents, can result in either a cyclic or a tree-shaped model, as we have shown in figure 5(a) and (b) ."
"? troubleshooting 72 in the 'template' tab of the graphical user interface of the turnover script ( fig. 7), fill in the names of all unique conditions (treatments and/or tissues) in the 'condition(s)' field (using a new line for each condition, with no empty lines). 73 fill in all unique experimental pulse times (in days) in the 'pulse times' field. these are the same time pulses as those presented in fig. 2b or in supplementary table 1 . 74 fill in the number of biological replicates. these would probably correspond to the number of single animals used, unless more than one mouse is pulled for subcellular fractionation. 75 fill in the number of ms machine replicates. in our experience, two machine replicates are a good compromise between machine time usage and precision."
"typically, for a given model and dataset, a single inference algorithm is chosen based on the characteristics of the model. in this section, we first show that in the context of sql queries over probabilistic ie results, the proper choice of ie inference algorithm is not only model-dependent, but also query-and text-dependent. thus, to achieve good accuracy and runtime performance, it is imperative for a pdb system to use a hybrid approach to ie even within a single query, employing different inference algorithms for different records."
"for a typical ie application, the label space is small (e.g., 10), and the correlation strength is fairly strong. for example, title tokens are usually followed by the author tokens in a bibliography string. moreover, strong correlation exists with any multitoken entity names (e.g., a person token is likely to be followed by another person token). thus, the above rules translate in most cases in ie to: choose viterbi and sum-product over mcmc methods for acyclic models for top-k and marginal queries respectively; choose gibbs sampling for cyclic models unless the cycles are induced by query constraints, in which case choose query-driven mcmc-mh. in this paper, we use heuristic rules to decide the threshold for a \"small\" label space and for a \"strong\" correlation for a data set. developing a cost-based optimizer to make such choices based on the data and model is one of our future directions."
"first of all, the relational sub-query qre augments the original model with additional random variables, cross-edges and factor tables, making the model structure more complex, as we explained in section 4.1. the characteristics of the model may change after applying the query over the model. for example, a linear-chain crf model may become a cyclic crf model, after the join query in q1 or the query with aggregate constraint in q2."
"in this query, we want to compute the marginal distribution or the top-k extraction over an underlying skip-chain crf model as shown in figure 4 . the query is simply:"
"as expected, we could see a correlation between the accuracy of the turnover determination (confidence interval) and the number of peptides identified per protein. proteins that were identified"
"-accession codes, unique identifiers, or web links for publicly available datasets -a list of figures that have associated raw data -a description of any restrictions on data availability not applicable"
"the query we use in this experiment is q4 over the skip-chain crf model, described in section 6.3.1. given that the viterbi is more than 10 times more efficient than gibbs with zero computation error, as we showed in section 7.3, the speed-up enabled by the hybrid inference for q4 is determined by the percentage of the documents that do not contain duplicate non-\"stop-word\" tokens."
"the query used in this experiment is q2 with an aggregate constraint described in section 6.3.3. we performed this query over the dblp dataset. out of all the top-1 extractions of the 10, 000 bibliography strings, only 25 of them do not satisfy the aggregate constraint that all title tokens are in front of all author tokens. thus, although the aggregate constraint in the query induce a big clique in the crf model, which calls for mcmc algorithms, the mcmc is not needed for most of the cases. to perform q2 over dblp, mcmc only needs to be performed over 25 out of 10, 000 documents, which leads to a 10-fold speedup. summary: the results in section 7.1 and section 7.2 show that mcmc algorithms can be implemented in database, achieving comparable runtimes to the scala/java implementation. the querydriven sampling techniques can effectively generate more samples that satisfy query constraints for conditional queries. section 7.3 and section 7.4 show that the viterbi and sum-product algorithms are by far more efficient and more accurate than the mcmc algorithms over linear-chain and tree-shaped models in ie. lastly, based on the text analysis over nytimes, twitter and dblp datasets, we conclude that the query plans with hybrid inference can achieve up to 10-fold speed-up compared to the non-hybrid solutions."
"in this paper, we first explore the in-database implementation of a number of inference algorithms suited to a broad variety of models and outputs: two variations of the general sampling-based markov chain monte carlo (mcmc) inference algorithm-gibbs sampling and mcmc metropolis-hastings (mcmc-mh)-in addition to the viterbi and the sum-product algorithms. we compare the applicability of these four inference algorithms and study the data and the model parameters that affect the accuracy and runtime of those algorithms. based on those parameters, we develop a set of rules for choosing an inference algorithm based on the characteristics of the model and the data."
"in the query plan with hybrid inference, the viterbi algorithm runs first to compute the top-k extraction without the constraint. then, the results are run through the aggregate: those that satisfy the constraint are returned as part of the results, and those that do not satisfy the constraint are fed into the query-driven mcmc-mh algorithm."
"a method for precise determination of protein lifetimes in mice in vivo to overcome these limitations, we have recently introduced and thoroughly validated a technique that enables the measurement of protein lifetimes 4 while taking advantage of a commercially available protocol nature protocols diet for silam 25 . the approach is an extension of the stable-isotope labeling of amino acids in cell culture (silac), which has been largely adopted for protein quantification in dissociated cells 26 . approaches using a silam scheme for absolute protein quantification have been around for more than a decade, including those of several important in vivo works 25, 27, 28 . our in vivo pulsing strategy workflow utilizes an isotopically stable 13 c 6 -lysine, which is an essential amino acid that needs to be absorbed through the diet. in addition, lysine is not turned directly into other amino acids but is catabolized to specific metabolites such as pipecolic acid 29, reducing the isotopic labeling of other amino acids to virtually zero. following metabolic labeling, the incorporation of 13 c 6 -lysine is quantified through shotgun ms. this is coupled with a proteome-level description of the lysine pool reuse based on both actual measures and mathematical modeling. overall, our strategy comes with several improvements for the determination of lifetimes, including a simple and robust interpretation of ms results, which allows the obtainment of data for many peptides for each protein. this is a major advantage with respect to approaches based on 15 n diets that limit the analysis to a handful of arbitrarily selected peptides 4, 7 . importantly, even if the users modify this strategy by altering the pulsing and chasing schemes, our reliable analysis framework will still enable them to extract reliable protein lifetime parameters. for this reason, future results will be easily shared and replicated by other laboratories. this is an important aspect for the development of the field, because lifetime evaluations in vivo have been largely qualitative or comparative [cit] . moreover, this refined strategy includes several labeling pulsing times and allows the accurate monitoring of protein labeling over different time scales. one additional improvement of this strategy is the precise evaluation of the reuse of labeled lysine, which enables a study of the stability of the entire proteome. as a result, this experimental design, coupled with the description of lysine reuse, enables investigators to determine the exact half-lives of proteins and can easily be extended to different mouse genetic lines and possibly to other animal models. in principle, one could also use it for other species such as bacteria or plants, if lysine is made into an essential amino acid by targeted mutations."
the correlation strength of the crf model depends on the dataset on which the crf model is learned. the model we learned over nytimes and dblp dataset both contains strong correlation strength.
"in this work, we show the in-database implementations of two mcmcbased inference algorithms. the in-database implementations enable efficient probabilistic query processing with a tight integration of the inference and relational operators. this work also demonstrates the feasibility and potential of using a query optimizer to support a declarative query language for different inference operations over probabilistic graphical models. results from three reallife datasets demonstrate that hybrid inference can achieve up to 10-fold speed-up compared to the non-hybrid solutions. as future work, we intend to explore the development of a cost-based optimizer that can balance the efficiency and accuracy in answering probabilistic queries. in addition, we intend to support other text analysis tasks, such as entity resolution, and learning algorithms. acknowledgements: this work was supported by amplab founding sponsors google and sap, and radlab sponsors amazon web services, cloudera, huawei, ibm, intel, microsoft, nec, netapp, and vmware, nsf grants 0722077 and 0803690, and the european commission under fp7 [cit] -rg-249217 (heisendata)."
"as previously mentioned, silac food costs need to be considered when planning metabolic pulsing experiments. the price of 1 kg of 13 c 6 -lysine food is on the order of~$/€10,000; thus large experiments require very careful planning. a minimal labeling design can be used to limit the costs of the experiments (fig. 2b ). an important aspect to consider is that mice will tend to be skeptical of new diet formulations for some days. to prevent the pulsing procedure from being biased by this effect, it is useful to first habituate mice with a less expensive 12 c 6 -lysine formulation that is identically produced and packaged, which allows the mice to get used to the new food in a few days. we recommend monitoring of food consumption and mouse weight across the entire labeling procedure to avoid any chance of labeling biases that could compromise the entire outcome of the workflow. usually, mouse cage setups are designed to hold relatively large quantities of food. for this reason, food pellets are provided in abundance and the weight of the food itself is sufficient for the animal to grasp and eat the food. when small amounts of food are used, the weight of the food itself might not be sufficient for the animals to correctly grasp the pellets and eat them. to overcome this problem, a simple solution is to add a steel weight that presses the food down and allows its correct consumption ( fig. 2a ). owing to the importance of the metabolic pulsing to the overall outcome of the entire workflow, several other details are thoroughly discussed in the protocol and presented in fig. 2 ."
"in general, the peripheral area of the aforementioned crf has a certain surround inhibition effect on the response of the central receptive field, thereby highlighting the texture of isolated edge groups and achieving the reduction of background texture edges while better highlighting the boundaries of the target region. the ncrf introduces an important viewpoint concerning the local environmental background, and the environment can have specific features. it also enlarges the receptive field of the nerve cells by multiple folds, providing an affective neural mechanism for the visual system to perceive image features under complex natural scenes. it is exactly the interactions based on the complex nonlinearity between crf and ncrf that lead to the diversity of cell response characteristics, and when there is texture in the environment, the ncrf will generate an obvious suppression modulation effect on the crf response, thereby reducing the response of unwanted textures in the cluttered background to highlight the boundaries of the contour region. this kind of suppression modulation effect depends on the degree of difference in features such as luminance, distance, direction, and colour between the centre of the receptive field and the surrounding environment. here, a variety of local features that affect the texture suppression modulation effect are extracted. first, we adopt the distance-related weighting function as:"
"to obtain precise lifetimes, it is necessary to describe the change from the incorporation of normal 12 c 6 -lysines to the labeled 13 c 6 -lysines over the duration of the experiment. the mathematical modeling and the fitting of the data are summarized in fig. 3 . briefly, we have shown that lysines exist in free solution ('s') or are incorporated into proteins ('p'), in either a non-labeled (light (l)) or a labeled form (heavy (h)) 4 . the average interactions between these four pools can be approximated by a fast process (dominated by food absorption and excretion, rate parameter b) and a slower process (corresponding to protein synthesis and degradation, rate parameter a). together with the pool size ratio (r; which indicates bound versus free lysines), four coupled differential equations are solved analytically, using the information gathered with a pulsing experiment 4 . the change of the lysine pool can thus be described as a bi-exponential process ( fig. 3c ) with three unknown pool parameters [a,b,r]. the labeling of an individual protein of interest (h poi ) is influenced by the pool and has a specific turnover time constant, which is analytically solved. for each [a,b,r] pool parameter triplet, all data obtained by ms can be fitted, producing residuals for each protein that can be summed for all proteins (fig. 3c, lower panels). the residuals are then minimized during a global pool fit, repeating the individual h poi fits for different [a,b,r] triplets until optimal pool parameters are found. the best parameters can then be used for obtaining all individual lifetime measures (half-lives)."
"in this paper, we use a multi-feature suppression and integration method under the guidance of multi-scale information (see reference [cit] for more details). first, the strength of surround inhibition is integrated according to:"
"where θ c denotes the gaussian-weighted mean vector of the orientation vector θ in the crf, and θ s represents the dog + -weighted mean vector of the orientation vector in the ncrf. finally, the surround modulation weight of colour orientation selectivity is:"
"we describe the query processing steps that employ hybrid inference for different documents within a single query. then we describe an algorithm, which, given the input of a \"sql+ie\" query, generates a query plan that applies the hybrid inference. finally, we show the query plans with hybrid inference generated from three example \"sql+ie\" queries to take advantage of the appropriate ie inference algorithms for different combinations of query, text and crf models."
"the ms analysis we describe here in detail is based on the initial separation of the samples through page 40, followed by in-gel digestion and reverse-phase hplc-ms. this allows the measurement of relative incorporation of 13 c 6 -lysine at the proteome level and the heavy-versus-light values for each protein, corresponding to the amount of labeled 13 c 6 -lysines versus the amount of unlabeled 12 c 6 -lysines. these are the required values for the interpretation of the results and the lifetime measurements. several other possible shotgun ms approaches could in principle be used to obtain similar values. aware of this, we have provided scripts for computational analysis (see below) that are compatible with the results from other approaches that provide the heavy-versus-light fraction for each protein as an output."
"the query used in this experiment is the join query q1, described in section 6.3.2. given that the sum-product is more than 6 times more efficient than gibbs with zero computation error, as we showed in section 7.4, the speed-up enabled by the hybrid inference for q1 is determined by the percentage of the \"joinable\" document pairs that share only one pair of common non-\"stop-word\" tokens."
"in this section, we first test the edge extraction performance and analyse the basic characteristics of the proposed model on several simple images. then, we further investigate the response characteristics of the crf centre and the multi-feature guided texture surround inhibition of the ncrf peripheral with a complex human image. finally, we give experimental results on a popular natural image dataset (i.e., bsds300/500) [cit], respectively evaluate the performance of our model from subjective and objective perspectives, and analyse the influence of model parameters on the test results."
"in this section, we describe three example queries and show the query plans with hybrid inference generated from the algorithm in figure 7, which take advantage of the appropriate inference algorithms for different combinations of query, text and crf models."
"this experiment compares the runtime and the accuracy of the gibbs, the mcmc-mh and the viterbi algorithms in computing top-1 inference over linear-chain crf models. the inference is performed over 45, 000 tokens in 1000 bibliography strings from the dblp dataset. we measure the \"computation error\" as the number of labels different from the exact top-1 labelings according to the model 1 . the viterbi algorithm only takes 6.1 seconds to complete the exact inference over these documents, achieving zero computation error."
"(vi) derivatize the sample with 15 µl of methoxyamine hydrochloride solution for 4-48 h at rt. (vii) for the next derivatization step, add 30 µl of mstfa, vortex and spin for 2 min at 20,000g at rt. (viii) transfer the sample to gc-ms vials and incubate for 1-8 h."
"we run the query-driven mcmc-mh and the vanilla mcmc-mh algorithm over a randomly picked 10 documents from the dblp dataset. figure 12 shows the number of \"qualified\" samples that are generated by each algorithm in 1 second. as we can see, the querydriven mcmc-mh generates more \"qualified\" samples, roughly 1200 for all the documents, and for half of the documents the query-driven mcmc-mh generates more than 10 times more qualified samples than vanilla mcmc-mh."
"digestion buffer mix 6.5 ml of lichrosolv-grade water, 6.5 ml of 100 mm abc and 650 μl of 100 mm cacl 2 c critical prepare shortly before use."
"the algorithm works by passing real-valued functions called messages along the edges between the nodes. these contain the \"influence\" that one variable exerts on another. a message from a variable node yv to its \"parent\" variable node yu in a tree-shaped model is computed by summing the product of the messages from all the \"child\" variables of yv in c(yv) and the feature function f (yu, yv) between yv and yu over variable yv:"
"one of the most competitive summarization methods is based on integer linear programming (ilp). it has been widely adopted in the generic summarization task [cit] b; [cit] ) . in this paper, we use the ilp summarization framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. first, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ilp model. second, we design a sentence reranking component to score the summary candidate sentences generated by the ilp model. this second reranking approach allows us to explicitly model a sentence's importance and novelty, which complements the bigram centric view in the first step of ilp sentence selection. our experimental results on multiple tac data sets demonstrate the effectiveness of our proposed method."
"for update summarization, intuitively we need to not only identify the salience of the bigram, but also incorporate bigrams' novelty in their weights. therefore, only using the document frequency as the weight in the objective function is insufficient. we thus propose to use a supervised framework for the bigram weight estimation in the ilp model. the new objective function is:"
"lastly we evaluate the effect of the summary length from the ilp module on the two-step summarization systems. figure 1 shows the performance when n changes from 150 to 400. we can see that there is some difference in the patterns for different data sets, and the best results are obtained when n is around 150 to 250. when the first ilp module produces many sentence candidates, it is likely that there is redundancy among them. in this case, redundancy removal approaches such as mmr need to be used to generate the final summary. in addition, for a large candidate set, our current regression model also faces some challenges due to its limited features used in sentence reranking. addressing these problems is our future work."
"while this work is an important step towards building a probabilistic declarative ie system, the approach is limited by the capabilities of the viterbi algorithm, which can only handle top-k-style queries over a limited class of crf models: linear chain models, which do a poor job capturing features like repeated terms. different inference algorithms are needed to deal with non-linear crf models, such as skip-chain crf models, complex ie queries that induce cyclic models over the linear-chain crfs, and marginal inference queries that produce richer probabilistic outputs than top-k. the broader problem of effectively supporting general probabilistic inference inside a pdb-based declarative ie system remains open."
"on the other hand, surround inhibition strength decreases also with the increasing feature difference between the crf and non-crf. in this paper, we evaluate the contribution of local luminance and luminance contrast in contour detection with surround inhibition. we calculate the luminance and luminance contrast in local patches formed by a raised sine-weighted window:"
"for null hypothesis testing, the test statistic (e.g. f, t, r) with confidence intervals, effect sizes, degrees of freedom and p value noted for manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers. we strongly encourage code deposition in a community repository (e.g. github). see the nature research guidelines for submitting code & software for further information."
"this experiment compares the runtime and the accuracy of the gibbs, mcmc-mh and the sum-product algorithms over tree-shaped graphical models induced by a join query similar to q1, described in section 4.1. the query computes the marginal probability of the existence of a join result for each document pair in dblp, joining on the same 'publisher'. the query is performed over a set of 10, 000 pairs of documents from dblp, where the two documents in each pair have exactly one token in common. the sum-product algorithm over these 10, 000 tree-shaped graphical models takes about 60 seconds. as an exact algorithm, the sum-product algorithm achieves zero computation error. we measure the \"computation error\" as the difference between the marginal probabilities of join computed from the mcmc-mh algorithms and the sum-product algorithm, averaging over all document pairs."
"another limitation of the method, which is shared with other shotgun ms approaches used to quantify proteins, is a relatively poorer measurement precision for proteins with low relative abundance. there are two possible solutions to this problem: the first is to simplify the complexity of the sample that is analyzed, for example, fractionating the sample based on its biophysical features so that the most abundant proteins do not obscure the detection of the less abundant ones. here, we detail a thoroughly time-tested method based on page for this purpose 40, 41 (steps 15-51) to simplify the application of this protocol for laboratories that do not have access to other more recent ms solutions to this issue, such as high-ph reverse-phase peptide fractionation 42 . the second solution is to enrich the protein or the structure of interest either by specific immunoprecipitation or by subcellular fractionation. we have applied both methods to samples that were metabolically pulsed, and we confirmed the compatibility of these approaches with this method 4 . owing to space limitations, in this protocol we do not provide an extensive description of these steps, although we refer to the appropriate protocols for these purposes (step 10). having a reasonably high amount of protein in the sample is a very strict requirement for this method. we advise using an initial amount of 100 µg of total protein for each sample. if this amount is exceedingly high, for example, because the biological sample under scrutiny is too small (e.g., a small brain area), one could collect similar samples from several mice and measure the pooled samples, although this increases the number of analyzed mice and thus the price of the labeling procedure."
"this solution is 0.1% (vol/vol) fa and 95% (vol/vol) acn in lichrosolv-grade water. this buffer should be prepared fresh, but it can be stored at 25°c for up to 6 months."
"the characteristics of v1 crf are analysed in figure 5, but the v1-cell receptive field has unique structure; that is, the receptive field consists of sensitivity-oriented crf and selective-suppressionoriented ncrf."
"the exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement a statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly"
"as mentioned above, surround inhibition strength decreases also with the increasing feature difference between the crf and non-crf. to effectively represent the orientation difference between the texture patterns within crf and non-crf, the difference in the colour orientation of the receptive field centre-periphery by further considering the distance-weighted factors is defined as:"
"we did the same analysis on the twitter dataset. the number sentences that contains non-\"stop-word\" duplicate tokens is 10.0%, which leads to a similar 5-fold speedup. on the other hand, for dblp dataset, the number of documents that contains non-\"stopword\" duplicates is as high as 96.9%, leading to a 3% speedup."
"update summarization has attracted significant research focus recently. different from generic extractive summarization, update summarization assumes that users already have some information about a given topic from an old data set, and thus for a new data set the system aims to generate a summary that contains as much novel information as possible. [cit] . it is very useful to chronological events in real applications."
"the main complexity in the application of this method concerns the computational aspects and the correct calculation of the lifetimes. to circumvent this issue, we provide in this protocol several tools that simplify all computational steps (supplementary data 1). these include (i) a graphical user interface script (turnovergui.m) used to create a simple file template for inputting the data, import data, estimate the lysine pool dynamics from proteomic data, calculate protein lifetimes and confidence intervals based on the pool of lysines, and compare lifetimes from different conditions and obtain statistically relevant differences; (ii) a script to predict the labeling efficiency for proteins containing two lysines (predictdoublelabel.m); (iii) a script to predict the relative labeling of proteins in a chase experiment for more advanced applications (predictpulsechase.m); (iv) a script to compare the goodness of fit for multiple datasets using different lysine pool dynamics (comparepools.m);"
"step 93a: evaluation of lysine labeling in the plasma through gc-ms. although the global fitting approach is a successful method for estimating the change in lysine labeling, at the experimental level the pool of available lysines can be measured directly from the amount of free lysines in the plasma (or in other biological fluids). it is therefore critical to remove any lysines that have been incorporated into proteins before performing the analysis. depending on the volume of the sample, several approaches can be used for this purpose, all of which are based on protein precipitation followed by high-speed centrifugation to remove the precipitate. the average interactions between these four pools are described by a fast process (food intake and excretion, rate b, with amplitude a and time constant τ 1 ) and a slower process (protein synthesis and degradation processes, rate a with time constant τ 2 ). together with the pool size ratio (r), four coupled equations can be defined and analytically solved 4 . b, the function for free heavy lysine h s (t) can thus be described as a bi-exponential process with the three unknown pool parameters a, b and r, using the terms c, a, τ 1 and τ 2 . the labeling of an individual protein of interest (h poi ) follows h s using a simple, protein-specific turnover time constant τ, which is analytically solved. c, during global fitting, used to find the correct parameters of the lysine pool, for each pool parameter triplet, theoretical h poi curves can be fitted to the experimental (h/total) ratios (upper three graphs). for each protein, these fits produce residuals and sum of squared residuals (the middle and lower three graphs, respectively). the sum of squared residuals is then minimized during the global pool fit procedure, repeating the individual h poi fits for small steps of the different pool parameter triplets until the optimal pool parameters are found. the best parameters, exemplified in the 'good' situation in the center of this panel, are finally used for the full experimental dataset to obtain all individual time constants τ (or half-lives:"
"c critical step from this step onward, all the liquids will be pooled and collected in the 96-well microtiter plate underneath. do not discard the liquids. 38 add 20 μl of trypsin buffer to each gel piece. c critical step avoid shaking the plates when the trypsin is added because if the trypsin does not remain in the gel long enough, the proteolytic step will be inefficient. while extracting the peptides with the 96-well plate kit, make sure that the plate topology of the upper 96-well filter plate matches that of the 96-well microtiter plate underneath. 39 incubate the gel pieces at 4°c for~30 min to allow the trypsin to diffuse into the gel. 40 after trypsin diffusion, cover the gel pieces with 40-80 μl of digestion buffer. 41 fig. 4 ). c critical step apply a pressure of 40-60 pa. higher or lower pressures are not compatible with the filter membrane of the 96-well microtiter filter plate. 45 repeat steps 43 and 44 once more to extract as many peptides as possible. 46 to make sure that the peptides have been completely extracted from the gel pieces, add 50 μl of water to each well of the 96-well filter plate and incubate it for 15 min at 37°c, with shaking at 300 r.p.m. 66 after efficient trypsinization, there are only two types of peptides in the samples, i.e., the light version and the heavy version. for this reason, once it is confirmed that the trypsinization is complete (troubleshooting), one can specify a multiplicity of '2' in maxquant (fig. 5 ). for peptides containing more than two lysines, refer to the validation part of the protocol (step 93b)."
"lines 11 to 14 deals with a special set of queries, which compute the top-k results over a simple query with aggregate conditions that induce cycles over the base linear-chain crfs. the intuition is that it is always beneficial to apply the viterbi algorithm over the base linear-chain crfs as a fast filtering step before applying the mcmc methods. in line 12, it first computes the top-k extractions res without the aggregate constraint using viterbi. in line 13, it applies the constraint to the top-k extractions in res, which results in a set of top-k extractions that satisfy the constraints in res1. in line 14, the query-driven mcmc-mh is applied to the documents in t with extractions that do not satisfy the constraint: (res-res1).t. an example of this special case is described in section 6.3.3. complexity: the complexity of generating the hybrid plan depends on the complexity of the operation on line 5 in figure 7, where the groundcrfs are split into subsets of linearcrfs, treecrfs and cycliccrfs. the split is performed by traversing the ground crfs to determine their structural properties, which is linear to the size of the ground crf o(n ), where n is the number of random variables. the complexity of choosing the appropriate inference (lines 6 to 21) is o(1)."
"this experiment shows that mcmc algorithms performs relatively better in computing marginal distributions than in computing top-1 extractions. however, sum-product algorithm still outperforms mcmc algorithms in computing marginal probabilities over tree-shaped models: more than 6 times faster with about 1% less computation error."
"the inference part q inf, of a \"sql+ie\" query, takes the docid, the pos, and the crf model resulting from qre as input. the inference operation is specified in the select clause, which can be either a top-k or a marginal inference. the inference can be computed over different random variables in the crf model: (1) a sequence of tokens (e.g., a document) specified by docid; or (2) a token at a specific location specified by docid and pos; or (3) the \"existence\" (exist) of the result tuple. the \"existence\" of the result tuple becomes probabilistic with a selection or join over a probabilistic attribute, where exist variables are added to the model [cit] ."
"again, typically, for such a high tree-width cyclic model, mcmc-mh algorithms are used over all the documents to compute the top- k extractions that satisfy the constraint. such a non-hybrid query plan is shown in figure 10 (b). however, this query falls into the special case described in the query plan generation algorithm in section 6.2 for hybrid inference. the query is to return the top-k extractions over the cyclic graph induced by an aggregate constraint over a linear-chain crf model. thus, the resulting query plan is shown in figure 10(a) ."
(v) analyze the data according to steps 71-92. check that the h/l ratio of the overexpressed exogenous protein corresponds to the incorporation as predicted by the global fitting approach used (step 79-84).
"we have implemented four inference algorithms over the crf model for ie applications: (1) viterbi, (2) sum-product, and two samplingbased mcmc methods: (3) gibbs sampling and (4) mcmc metropolishastings (mcmc-mh). in table 1, we show the applicability of these algorithms to different inference tasks (e.g. top-k, or marginal) on models with different structures (e.g., linear-chain, tree-shaped, cyclic)."
"to train the model (feature weights), we use the average perceptron strategy [cit] to update the feature weights whenever the hypothesis by the ilp decoding process is incorrect. binary class labels are used for bigrams in the learning process, that is, we only consider whether a bigram is in the system generated summary or human summaries, not their term or document frequency. we use a fixed learning rate (0.1) in training."
"the following table that is also part of the main manuscript compares mhns and cbns on three cytogenetic cancer datasets [cit] . below are plots showing (a) the raw data, (b) the learned cbns, and (c) the corresponding mhns. note that the mhns show mutual dependencies as well as inhibiting edges, features that a cbn does not have. our observation that mhns compare favorably in terms of both cv-log-likelihood and aic can be interpreted as evidence that such mutual dependencies between progression events exist in all three types of cancer. the models show striking overlap as well as marked differences."
"at the same time, input to the two sub-areas of the simple-cell double-opponent receptive field receives the output of lgn single-opponent nerve cells with two opposite polarities, respectively; therefore, the v1-simple-cell receptive field response is given by:"
"before starting, the algorithm first designates one node as the root; any non-root node which is connected to only one other node is called a leaf. in the first step, messages are passed inwards: starting at the leaves, each node passes a message along the edge towards the root node. this continues until the root has obtained messages from all of its adjoining nodes. the marginal of the root note can be computed at the end of the first step."
"once sterile filtered, this buffer can be stored at 25°c for up to 1 year. this buffer can be divided into aliquots and stored at −20°c for up to 1 year."
"the procedure is summarized in fig. 1 and consists of five modules: mouse metabolic labeling and tissue collection (steps 1-10); sample processing (steps 11-51); ms analysis (steps 52-70); data interpretation, lifetime determination and statistics (steps 71-92); validation and controls (step 93; several options available). the aspects from steps 1-70 are based on existing procedures that we have specifically tailored for the purpose of measuring protein turnover in mice. in these three initial modules of the protocol, we cover in detail the timing, the quantity of isotopic food and other aspects while offering practical tips to render the measurements reliable and robust. the data interpretation and the final validations (steps 71-93) are the most complex steps. for this reason, we provide in the protocol the extensive biochemical methods, computational tools and scripts that enable the data interpretation and validation, along with practical examples that might help with troubleshooting the most common problems."
"parameters are found by minimizing the sum of residuals from a simultaneous fit of multiple proteins. it is computationally expensive to include the total set of proteins for this step, and, in our experience, including weak and noisy signals from proteins that are detected only in a small number of experiments does not substantially improve the quality of the pool fit. for this reason, a threshold can be set for the number of required data points for proteins to be included into the global fit. 80 fill in the starting parameters (a,b,r) for the pool fit, or use the predefined values that were measured in the key reference work 4 . c critical step the parameters describe global protein generation/degradation rates (a), feeding/excretion rates (b) and the ratio of protein-bound versus free lysine pools (r). in addition, derived parameters (tau1, tau2, a) are shown to the right for a better interpretation of the pool curve: tau1 is the fast time constant of the exponential rise, tau2 the slow time constant, and a is the amplitude of the fast component (and 1−a the amplitude of the slow component). 81 fill in the starting time constant for the fit of individual proteins (poi tau0). a value in the vicinity of the real protein time constant will improve the speed and robustness of the fit. a plot in this tab shows the temporal evolution of the pool using the current pool parameters and will be"
"finally, all the conceptual and practical knowledge that is included in this detailed protocol can be used to extend these lifetime measurements to other frequently used animal models such as rat, fruit flies, nematodes and zebrafish, for which the silac food is already commercially available."
"traditionally, this problem has been approached by using custom diets and advanced analysis tools, which are not readily accessible to most laboratories 5 . for these reasons, although approaches to measurement of proteome-wide turnover have existed for more than a decade [cit], measurement of protein lifetimes has not yet become common practice in ms laboratories. at the same time, previous approaches have been difficult to reproduce between laboratories 4 . nevertheless, protein stability in vivo remains of great importance and keeps attracting interest in the scientific community 5, [cit] . we have, therefore, developed tools to render the measurement of protein lifetimes in mice in vivo not only precise but, importantly, easily reproducible in virtually any laboratory with access to shotgun proteomic technologies 4 . the strategy described here is based on a pulsing workflow that utilizes the isotopically stable 13 c 6 -lysine during metabolic labeling. this essential amino acid is absorbed through the diet and is incorporated into proteins depending on their relative turnover. the analysis of protein labeling is complicated by the reuse of unlabeled amino acids, as we describe in detail in this protocol."
"as mentioned above, surround inhibition strength decreases also with the increasing feature difference between the crf and non-crf. to effectively represent the orientation difference between the texture patterns within crf and non-crf, the difference in the colour orientation of the receptive field centre-periphery by further considering the distance-weighted factors is defined as:"
"more importantly, we study the integration of relational query processing and statistical inference algorithms, and demonstrate that, for sql queries over probabilistic extraction results, the proper choice of ie inference algorithm is not only model-dependent, but also query-and text-dependent. such dependencies arise when re-lational queries are applied to the crf model, inducing additional variables, edges and cycles; and when the model is instantiated over different text, resulting in model instances with drastically different characteristics."
"for example, the following query computes the marginal inference marginal(t1.docid,t2.docid,exist), which returns pairs of docids and the probabilities of the existence (exist) of their join results. the join query is performed between each document pair on having the same token strings labeled as 'person'. the join query over the base tokentbl tables adds cross-edges to the pair of linear-chain crf models underlying each document pair. figure 5 another example is a simple query to compute the top-k extraction conditioned on an aggregate constraint over the label sequence of each document (e.g., all \"title\" tokens are in front of \"author\" tokens). this query induces a cyclic model as shown in figure 5 (c). next, we describe general inference algorithms for such cyclic models."
"sum-product (i.e., belief propagation) is a message passing algorithm for performing inference on graphical models, such as crf [cit] . the simplest form of the algorithm is for tree-shaped models, in which case the algorithm computes exact marginal distributions."
"in this section, we first compare the characteristics of the four inference algorithms we have developed over the crf model. next we introduce parameters that capture important properties of the model and data. using these parameters, we then describe a set of rules to choose among different inference algorithms."
"in the past decade, there has been a groundswell of work on probabilistic database systems (pdbs) [cit] . as shown in previous work [cit], graphical modeling techniques can provide robust statistical models that capture complex correlation patterns among variables, while, at the same time, addressing some computational efficiency and scalability issues as well. in addition, [cit] showed that other approaches to represent and handle uncertainty in database [cit], can be unified under the framework of graphical models, which express uncertainties and dependencies through the use of random variables and joint probability distribution. however, there is no work addressing the problem of effectively supporting and optimizing different probabilistic inference algorithms in a single pdb, especially in the ie setting."
"protocol 88 modify the desired thresholds in their respective 'sel' edit fields, or left-click/left-drag the mouse pointer in the plot to change the selection thresholds directly. 89 to change the axis limits, edit the respective 'max' edit fields. 90 use the 'export stats' button to export the current comparison as an excel file. the comparison will contain all the proteins. 91 to export only the proteins that are selected by the threshold, activate the 'threshold' check box before the export; otherwise, all proteins will be exported. 92 at the end of the session, you can save all data under the 'session' tab with the 'save session' button."
"for the nytimes dataset, about 6.0% \"joinable\" sentence pairs share more than one pair of non-\"stop-word\" common tokens. thus, the sum-product algorithm can be applied to the other 93.6% of the sentences, achieving a 4.5 times speedup compared to the nonhybrid approach of running gibbs over the joint crf model of all the document pairs."
"where c θ denotes the gaussian-weighted mean vector of the orientation vector in the crf, and s θ represents the dog + -weighted mean vector of the orientation vector in the ncrf."
"this implementation achieves similar (within a factor of 1.5) runtime compared to the scala/java implementation of the mcmc algorithms, as shown in the results in section 7.1."
"in this experiment, we explore how different correlation strengths, one of the parameters we discussed in section 5.2, affect the runtime and the accuracy of the inference algorithms. as we explained earlier, the correlation strength does not affect the accuracy or the runtime of the viterbi algorithm. on the other hand, weaker correlation between different random variables in the crf model leads to faster convergence for the mcmc algorithms. the setup of this experiment is the same as in section 7.3."
"natural images are complex and varied. to verify the actual performance of the bihcd model in this paper, we selected the standard image database (berkeley segmentation data set 300/500 (bsds300/500)) provided by the university of california, berkeley, to test the performance of image contour detection in complex natural scenes. bsds300 datasets contains 300 natural images (200 training and 100 test images) and bsds500 additionally adds 200 test images. each image provides 5-10 human-marked standard contour (human) data, which lead to the presence of greater artificial subjective factors in the contours marked in the human map, as shown in figure 6 . for example, in the mushroom sample in the last row of figure 6, cluttered background such as weeds are marked as contours. therefore, in subsequent experiments, we will average all human map of the same image as its 'ground truth'. in combination with the human map, we calculated the aforementioned evaluation index (f-measure) in the experiment and drew a diagram of the corresponding precision-recall (p-r) curve, which directly reflected the consistency between the contours detected and obtained by different algorithms and the human map."
"1. data size: the size of the data is measured by the total number of tokens in information extraction; 2. structure of grounded models: the structural properties of the model instantiations over data: the data size affects the runtime for all the inference algorithms. the runtime of viterbi and sum-product algorithms is linear to the data size. the mcmc algorithms are iterative optimizations that can be stopped at any time, but the number of samples needed to converge depend linearly on the size of the data."
"in this experiment, we evaluate the effectiveness of the query-driven mcmc-mh algorithm described in section 4.3 with the vanilla mcmc-mh in generating samples that satisfy the query constraint. the query we use is q2 described in section 4.1, which computes the top-1 extractions that satisfy the aggregate constraint that all title tokens are in front of the author tokens."
"the response of the v1 complex cells can be obtained by integrating the output of the simple cells. at present, there are three main integration models for the simulation of complex cells: the max model, which takes the maximum value; the energy model, which seeks the sum of squares; and the learning model with data training. to maintain better edge extraction characteristics, the max model is selected in this paper; that is, the v1 complex cell output of this channel is defined by the maximal value of the response of simple cells with n orientations from this given group, as shown in equation (6):"
"finally, we describe example queries and experiment results showing that such hybrid inference techniques can improve the runtime of the query processing by taking advantage of the appropriate inference methods for different combinations of query, text, and crf model parameters."
"for example, the inference marginal(t1.docid,t1.pos), for each position (t1.docid,t1.pos) computed from qre, returns the distribution of the label variable at that position. the inference marginal(t1.docid,exist) computes the marginal distribution of exist variable for each result tuple. we can also specify an inference following a join query. for example, the inference top-k(t1.docid,t2.docid), for each document pair (t1.docid,t2.docid), returns the top-k highest probability joint extractions that satisfy the join constraint."
"in the context of sql queries over probabilistic ie results, the proper choice of the ie inference algorithm is not only dependent on the model, but also dependent on the query and the text."
"step 93c: validation through pulse-and-chase approaches. the data can be further validated through pulse-and-chase approaches. because there is an infinite number of possible designs, we included a small script that can be used to predict the relative labeling of a protein for which the pulse-and-chase parameters have been defined. this is particularly useful for estimating the relative labeling of a protein following a known chase time in an animal that was previously pulsed for a determined number of days."
this solution is 100 mm abc (ph 8) c critical prepare freshly and discard within a day. check the ph of the buffer with a ph paper indicator.
"for the mcmc algorithms, we measure the computation error and runtime for every 200k more samples, starting from 200k to 2 million samples over all document pairs. as we can see in figure 14, the probability difference between gibbs and sum-product converges to zero quickly: at 400 second, the probability difference is dropped to 0.01. the mcmc-mh on the other hand, converges much slower than the gibbs."
"in conclusion, much remains to be investigated about the functional role and the underlying mechanisms of surround inhibition and connections in neurons in contour detection of natural scenes. the bio-inspired models above only mimic part of the visual processing mechanisms in hvs. in this paper, we are especially concerned with incorporating recent knowledges of the physiological"
"to describe the suppression effect of the ncrf periphery on the difference in the colour orientation of the receptive field at the centre of the crf, the maximal value of the response in each direction of the four opponent channels is taken as the response of that colour orientation-that is:"
"for most organizations, textual data is an important natural resource to fuel data analysis. information extraction (ie) techniques parse raw text and extract structured objects that can be integrated into databases for querying. in the past few years, declarative information extraction systems [cit] have been proposed to effectively manage ie tasks. the results of ie extraction are inherently uncertain, and queries over those results should take that uncertainty into account in a principled manner. research in probabilistic databases (pdbs) has been exploring scalable tools to reason about these uncertainties in the context of structured query languages and query processing [cit] ."
"to achieve good accuracy and runtime performance, it is imperative for a pdb system to use a hybrid approach to ie even within a single query, employing different algorithms for different records. in the context of our crf-based pdb system, we describe query processing steps and an algorithm to generate query plans that apply hybrid inference for probabilistic ie queries."
"we implement ie algorithms over the crf model within a database using the relational representations of text and the crf-based distribution in the token token table: the token table tokentbl, as shown in figure 1(b), is an incomplete relation r in db p, which stores a set of documents or text-strings d as a relation in a database, in a manner akin to the inverted files commonly used in information retrieval."
"an additional potential limitation of the methods that use silam labeling is the fact that, upon trypsinization, some peptides do not contain lysines (because they can be cleaved on arginine residues). in theory, this problem can be circumvented by using the endoproteinase lysc, which cleaves peptide bonds only at the carboxyl side of lysine. in reality, this solution is suboptimal because the proteome coverage may be compromised as a consequence of the decreased number of peptides generated. in our workflow, trypsin digestion provides a median of six peptides containing lysine per protein identified and allows us to determine the turnover of~80% of identified proteins. this is superior to techniques based on 15 n diets or 2 h 2 o labeling, in which peptide analysis is more cumbersome and allows determination of turnover rates for only~50-60% of the proteins identified 21, 43 ."
"in this paper, we adopt the supervised ilp framework for the update summarization task. a set of rich features are used to measure the importance and novelty of the bigram concepts used in the ilp model. in addition, we proposed a re-selection component to rank candidate sentences generated by the ilp model based on sentence level features. our experiment results show that our features and the reranking procedure both help improve the summarization performance. this pilot research points out new directions for generic or update summarization based on the ilp framework."
"a direct application of the protein stability measurements is to determine the turnover of a protein of interest in specific organs or in subcellular structures 4, 30, and eventually under genetic or pharmacological modulations of the pathway of interest. this would reveal the proteins that have compromised stability in a set of relevant modulations, in the same manner in which such proteins are currently identified in protein abundance studies. importantly, in lifetime measurements, each hit contains an internal control because, for each protein, the ratio between the 'heavy' pulsed peptides and their 'light' counterparts is measured. as a result, the precision of these measurements is generally higher than that of protein abundance measurements 4 . this implies that small differences that might be overlooked in protein abundance studies would be readily detected with a turnover approach. therefore, protein turnover measurements would be useful as a complement to, and in nature protocols protocol some cases a replacement for, protein abundance studies. this can be particularly efficient because the same samples can be used for both purposes by combining turnover and label-free quantifications 31 ."
"to circumvent these limitations, the approach we have developed takes into consideration the reuse of lysines in the organism and enables a precise determination of protein lifetimes. compared to the approaches used in the past, the main advantages of this workflow are (i) rapid implementation for any laboratory using protein ms; (ii) robust analysis and reliable results that can be easily achieved with the scripts we include in this protocol; (iii) the flexibility of the method and the validation steps, which can be fine-tuned for each practical application; (iv) the modular applicability to specific biological questions, such as neurodegenerative diseases and genetic mouse models; and (v) the possibility of validating lifetimes with high precision."
"corresponding author(s): eugenio f. fornasiero last updated by author(s): jun 14, 2019 reporting summary nature research wishes to improve the reproducibility of the work that we publish. this form provides structure for consistency and transparency in reporting. for further information on nature research policies, see authors & referees and the editorial policy checklist."
"this solution is 0.1% (vol/vol) fa in lichrosolv-grade water. this buffer should be prepared fresh, but it can be stored at 25°c for up to 6 months."
"our recent work [cit] has proposed a pdb system that natively supports a leading statistical ie model (conditional random fields (crfs)), and an associated inference algorithm (viterbi). it shows that the in-database implementation of the inference algorithms enables: (1) probabilistic relational queries that returns top-k results over the probabilistic ie outcome; (2) a tight integration between the relational and inference operators, which leads to significant speed-up by performing query-driven inference."
"we can see from the tables that the supervised ilp model outperforms the unsupervised one. after including the novelty related features, the model can assign higher weights for the bigrams with novel information, resulting in improved summarization performance. there is further improvement when using our 2-step approach with the sentence reranking model. our proposed method (ilp followed by sentence reranking, and using all the features) [cit], and also yields competitive results in the other data sets. the gain of rouge-2 of our proposed system compared with the ilp baseline is statistically significant based on rouge's 95% confidence. when using sentence ranking on the entire document set, without the ilp pre-selection step, its performance is worse than our proposed method. this shows the benefit of doing pre-selection using the ilp module. finally, for all the methods, adding the novelty related features always performs better than that without them, proving the effect of our novelty features for update summarization."
"in addition to isotope-labeled amino acids, deuterium labeling has been used successfully as an alternative approach to labeling proteins 6 . in a typical deuterium workflow, animals receive intraperitoneal injections of deuterated water ( 2 h 2 o) and have ad libitum access to 8% 2 h 2 o in the drinking water supply for different time intervals 21 . during the synthesis of nonessential amino acids, the animals incorporate the deuterium into the c-h bonds, thus labeling the newly synthesized proteins 6, along with all other biomolecules in the organism. following protein preparation, the metabolic labeling of proteins can be measured over time by analyzing the shift in peptide isotope weights toward higher-end masses 21 . freeware software is available for analyzing the shift in peptide isotope composition 22, 23, although it is important to clarify that while 2 h 2 o labeling is in theory simple and costefficient, the exact time course of amino acid production degradation and the reuse of 2 h cannot be formally modeled in an organism. this is because the quantitative aspects of the biotransformations linked to 2 h are largely unknown, as 2 h 2 o can be incorporated into virtually all biomolecules 24 . the current models work by approximating the problem of protein labeling with 2 h to a mono-exponential saturation toward an equilibrium level. this assumption neglects the fact that the body of a mammal works as a reservoir for precursor molecules that are constantly reused for the synthesis of new proteins, which makes it difficult to accurately define protein lifetimes with 2 h 2 o labeling."
"in figure 15, we show the runtime-accuracy graph of the viterbi and the gibbs algorithm to compute the top-1 extractions over models with different correlation strengths. we synthetically generated models with correlation strengths of 1, 0.5, 0.2 and 0.001 by dividing the original scores in the transition factors by 1, 2, 5 and 1000 respectively. as we can see, the weaker correlation strengths lead to faster convergence for the gibbs algorithm. when correlation strength is 0.001 the computation error reduces to zero in less than twice that of the viterbi runtime."
"for the twitter datset, around 25.6% \"joinable\" sentence pairs share more than one pair of tokens. this is much lower than nytimes dataset mainly because tweets contain a lot of common shorthands. thus the speedup is around 2.6 times. for dblp dataset, on the other hand, the speedup is little due to the high percentage of document pairs contain more than one pair of common words."
"however, the existence of such skip-edges in the grounded models, instantiated from the documents, is dependant on the text! there exist documents, like the one shown in figure 4, in which one string appears multiple times. those documents result in cyclic crf models. but, there also exist documents, in which only unique tokens are used except for \"stop-words\", such as \"for\", \"a\". those documents result in linear-chain crf models. the query plan generated with hybrid inference is shown in figure 8(a) . after the model instantiation, the ground crf model is inspected: if no skip-edge exists (i.e., no duplicate strings exist in a document), then the viterbi or the sum-product algorithm is applied; otherwise, the gibbs algorithm is applied to the cyclic ground crfs. compared to the non-hybrid query plan, the query plan with hybrid inference is more efficient by applying more efficient inference algorithms (e.g., viterbi, sum-product) over the subset of the documents, where the skip-chain crf model does not induce cyclic graphs. the speedup depends on the performance of viterbi/sum-product compared to gibbs sampling, and on the percentage of such documents that instantiate a skip-chain crf model into a grounded linear-chain crf models."
"the final design of the experiments will depend on the exact purpose of the study, the budget that can be dedicated to the project and the constraints on the amount of the protein(s) of interest in the analyzed samples. in this protocol, we describe a minimal workflow, which includes the feeding of one mouse for each of three different time periods for a single experimental condition (for a total of three animals). if the samples analyzed meet the optimal protein amounts necessary for ms analysis (100 µg), this workflow will allow the precise fitting of protein lifetimes for large proteomic datasets. thus, it can be used for comparing the effect of specific modulations on protein lifetimes by measuring a set of three mice for any additional genetic or pharmacological treatment of interest."
"the best measures for the lifetimes obtained through fittings are means with confidence intervals 49 . comparing means when the errors are represented by confidence intervals is not standard practice in statistical workflows. thus, to obtain statistical differences between two conditions, we provide a statistical script that performs the initial comparisons of protein turnover results."
"previous work [cit] has developed query-driven techniques to integrate probabilistic selection and join conditions into the viterbi algorithm over the linear-chain crf model. however, the kind of constraint that viterbi can handle is limited and specific to the viterbi and potentially the sum-product algorithm. in this section, we explore query-driven techniques for the sampling-based mcmc inference algorithms. query-driven sampling is needed to compute inference conditioned on the query constraints. such query constraints can be highly selective, where most samples generated by the vanilla mcmc methods do not \"qualify\" (i.e., satisfy the constraints). thus, we need to adapt the mcmc methods by pushing the query constraints into the sampling process. note that our adapted, query-driven mcmc methods still converge to the target distribution as long as the proposal function can reach every \"qualified\" world in a finite number of steps."
"based on analysis in the last section on the parameters, the following are the rules to choose an inference algorithm for different data and model characteristics, quantified by the three parameters, and the query:"
"in this example, we use the join query q1 described in section 4.1, which computes the marginal probability of the existence of a join result. the join query is performed between each document pair on having tokens with the same strings labeled as 'person'. such a join query over the underlying linear-chain crf models induces cross-edges and cycles in the resulting crf model. a typical non-hybrid query plan, shown in figure 9 with black edges, perform mcmc inference over all the documents."
"use the data import script (included in supplementary data 1) to import the data obtained from the ms analysis. start the graphical user interface by typing turnovergui into the matlab command window without parameters (and without the quotation marks). for evaluation purposes, an example is given in supplementary data 2."
"ultimate 3000 rslcnano hplc gradient (step 59) c critical because the entire procedure relies on the correct pulse feeding of mice, all experiments critically depend on this process. to avoid affecting mouse behavior, the entire procedure is designed for minimal interference with mouse physiology. to this aim, mice are fed ad libitum. because metabolic diets are expensive, it is important to design the feeding scheme and calculate the required amount of food. as a reference value, an adult male mouse will eat~3-4 g of food per day, whereas a female under normal conditions will eat slightly less. as a rule of thumb, each mouse eats 1/10th of its weight in dry pellet food daily. c critical see supplementary table 1 for timing details. 1 if you fear food competition among the animals, before the beginning of the labeling procedure, divide the animals by placing them in separate cages. 2 track animal weight and possibly food consumption daily during the labeling procedure (at the same hour of the day). sudden changes in weight or eating behavior should be carefully monitored. c critical step for ensuring the correct labeling of mice, it is necessary to evaluate weight differences across the labeling procedure and exclude from the study any animals with abnormal feeding behavior. c critical step mice become accustomed to specific diet formulations, and a change in the administered diet usually results in weight loss. for this reason, it is necessary to habituate the animals to the lys(0)-silac-mouse mock diet for some days before the start of the experiment. to speed up the habituation step, some pellets can be inserted directly into the mouse cage. c critical step it is not advisable to leave too much food in the cage because it will become spoiled and the mice will end up playing with rather than eating the pellets. at the same time, most cages are not compatible with the small quantities of food pellets that are used for metabolic labeling. mice should be able to grasp food from the bottom of the pellet rack, and if the food weight is not sufficient, they will not be able to eat correctly. to this purpose, a stainless-steel cube can be used to weight the food pellets (see cage setup scheme in fig. 2a ). 4 following habituation with the lys(0)-silac-mouse mock diet, proceed to metabolically labeling the mice with the lys(6)-silac-mouse labeling diet. c critical step the time and day of the beginning of the metabolic labeling should be planned carefully because the tissue collection and possibly the cell fractionation should be performed at precise time intervals following this step. for a standard protein turnover evaluation, we advise collecting tissues from three cohorts of animals after short (5 d), medium (14 d) and long (21 d) pulsing times (see also supplementary table 1) . for more precise evaluations of long-lived proteins and ellps 4, 9, 10, longer time intervals might be required. 5 during the first days of the labeling procedure, remove the fecal pellets or change the bedding daily."
"a virtually unlimited number of tissue and protein fractionation steps can be combined with this metabolic labeling approach. owing to space limitations, in this protocol we avoid engaging in a detailed description of all possible fractionation methods. for subcellular fractionation and, in particular, for brain organelles such as synaptosomes, synaptic vesicles and mitochondria, we refer to previously published protocols [cit] . in principle, the study of cell-type-specific proteomes could also be combined with this technique 4, 48, allowing the evaluation of the differential turnover of the same protein species in different cells. importantly, for all preparations, we advise collection of 100 µg of a metal weight can be used to allow mice to grasp and eat the food correctly. mice should be placed in single cages in the case that competition for access to food is feared. monitor food consumption and animal weight to ensure correct food consumption. because mice have a tendency to eat fecal pellets, which can interfere with labeling, it is useful to change the bedding or remove them. b, estimated food consumption for a minimal experimental design. refer to supplementary table 1 for more complex experimental designs. the total amount of food for a mouse over 40 d will correspond to~150 g of silac food. with this minimal experimental design, the cost of the food can be maintained under~$/€2,000. it is safe to always have an extra amount of labeled food in case the mice eat slightly larger amounts."
the core idea of the ilp based summarization method is to select the summary sentences by maximizing the sum of the weights of the language concepts that appear in the summary. bigrams are often used as the language concepts in this method. [cit] stated that the bigrams gave consistently better performance than unigrams or trigrams for a variety of rouge measures. the association between the language concepts and sentences serves as the constraints. this ilp method is formally represented as below (see [cit] ) for more details):
"c critical step because mstfa derivatization should not exceed 8 h, the gc-ms system should be ready to measure samples in this time frame. therefore, it might be necessary to prepare samples in batches to ensure proper derivatization time."
"the general flowchart of the contour detection algorithm proposed in this paper is summarized in figure 1 . the proposed model mimicking the human visual processing stages is a hierarchical visual architecture. according to differentiation between anatomical structure and physiological function, the frontal cortex or brain areas responsible for contour and boundary detection in the hvs include the retina, lgn, v1, and v2, etc. first, the input image is perceived and captured by singleopponent ganglion cells and lgn cells, which also carry out a convolution operation to form an image with four paths of different colours for the opponent channels; it is then further processed by the double-opponent v1 cells with different orientation sensitivities to preliminarily obtain the local edges of colour and luminance for the image. next, to better solve the problems of texture suppression and contour integration in complex natural scenes, we use a multi-feature suppression and integration method under the guidance of multi-scale information and put forward a distanceweighted colour-oriented encoding method that automatically modulates the suppression strength of each pixel position using the surround inhibition weight obtained after integration. finally, the feedforward and feedback processing of the v2 in response to the edges and textures of the cells in v1 is introduced, and methods such as non-maxima suppression and hysteresis threshold are used for processing the contours, such as refinement and integration, to obtain the hierarchical expression of the final image contours. although the receptive field of the lgn cells is larger than that of the ganglion cells, the characteristics of the receptive fields for these two types of nerve cells are basically consistent. therefore, a great deal of research works [cit] merge the two for studying and view them as one layer. the receptive field of cells in this layer has the single-opponent and spatially low-pass properties. biological experimental studies have shown that the visual system transfers visual information such as image colours through the two opponent channels of red-green (r-g) and blue-yellow (b-y). the single-opponent characteristic can be used to segment the different contour areas in the image through their luminance and colour information. the cells in this layer receive the retina outputs, and their responses can be described by:"
"c critical step blood is a 'liquid organ' and as such will contaminate protein measurements from other organs. to further decrease the contamination of blood, it is possible to cut the organs nature protocols protocol into 1-mm cubes before these washing steps. a more laborious alternative is to perfuse the animal with hbss buffer before dissection 52 . 10 (optional) decrease the complexity of the sample by performing further tissue or subcellular fractionation steps [cit] (experimental design). note that several tissue or subcellular fractionation approaches can be coupled with this method. a limiting factor can be the tissue size and the total protein content of the collected samples. for reliable ms measurements, we advise collection of at least 100 µg of total protein for each sample to be analyzed. the protein amount should be measured by a reliable assay before addition of the sample buffer. if one animal is not sufficient, the subcellular fractions of several animals can be pooled."
"in many ie tasks, good accuracy can only be achieved using nonlinear crf models like skip-chain crf models, which model the correlation not only between the labels of two consecutive tokens as in linear-chain crf, but also between those of non-consecutive tokens. for example, a correlation can be modeled between the labels of two tokens in a sentence that have the same string. such a skip-chain crf model can be seen in figure 4, where the correlation between non-consecutive labels (i.e., skip-chain edges) form cycles in the crf model. in simple probabilistic databases with independent base tuples, the \"safe plans\" [cit] give rise to tree-structured graphical models [cit], where the exact inference is tractable. however, in a crf-based ie setting, an inverted-file representation of text in tokentbl inherently has cross-tuple correlations. thus, even queries with \"safe plans\" over the simple linear-chain crf model, result in cyclic models and intractable inference problems."
"so far, we have described the implementation of the mcmc algorithms, and the query plans for the hybrid inference algorithms. we now present the results of a set of experiments aimed to (1) evaluate the efficiency of the sql implementation of the mcmc methods and the effectiveness of the query-driven sampling techniques; (2) compare the accuracy and runtime of the four inference algorithms: viterbi, sum-product, gibbs and mcmc-mh, for the two ie tasks-top-k and marginal; and (3) analyze three real-life text datasets to quantify the potential speedup of a query plan with hybrid inference compared to one with non-hybrid inference. setup and dataset: we implemented the four inference algorithms: viterbi, sum-product, gibbs and mcmc-mh in postgresql 8.4.1. we conducted the experiments reported here on a 2.4 ghz intel pentium 4 linux system with 1gb ram."
"the query-driven genproposal() and gensamples() functions are called iteratively to generate new samples that satisfy the constraint. the next \"qualified\" jump (i.e., new sample) can be generated by restricted jumps according to the query constraints or from random jumps."
"for nytimes dataset, we use the sentence breaking function in nltk toolkit [cit], and the full-text stop-word list from mysql [cit] . over all sentences, only about 10.3% contain duplicate non-\"stopword\" tokens. thus, the optimizer will use the viterbi algorithm for 89.7% of the sentences, while using the gibbs algorithm for the rest. this hybrid inference plan can achieve a 5-fold speedup compared to the non-hybrid solution, where the gibbs algorithm is used over all the documents."
"the naive way to answer those conditional queries using mcmc methods is to: first, generate a set of samples using gibbs sampling or mcmc-mh regardless of the query constraint; second, filter out the samples that do not satisfy the query constraint; last, compute the query over the remaining \"qualified\" samples."
"exact stoichiometry of the individual components 32 . this would reveal differences in situations in which various compensatory processes might still maintain a constant protein level, even though the protein turnover, i.e., the speed at which a protein is produced and degraded, might be altered. along the same lines, this approach can be used for the study of any of the proteins that are thought to be important for the regulation of degradation pathways, such as specific e3 ligases or regulators of the proteasome or the lysosome pathways. several other practical applications could be envisaged, including the study of specific protein modifications on lifetimes, the effects of autoimmune disease on the stability of receptors and cellular structures, and the metabolism of cancerous tissues at different stages or under different pharmacological treatments. among the most promising future developments of this technique, we would like to mention the possibility of combining it with more advanced ms workflows, including data-independent acquisition methods 33 and similar approaches that have recently become more accessible and reproducible 34, 35 . another obvious future development will be the use of multi-pulsing strategies adding other specific amino acids. these more complex pulsing schemes might be instrumental in shedding light on the in vivo behavior of proteins that might be replaced with unusual dynamics 36 . this is especially relevant for proteins that have completely different turnover profiles at different developmental steps during animal development. in fact, some proteins, such as the crystallins in the eye lens, are synthesized only very early in development and are virtually never exchanged during the animal lifetime 37 . along these lines, we have previously applied a second metabolic pulse with arginine10 ( 13 c 6 -15 n 4 arginine) 4 . even if arginine deprivation is linked to growth impairments 38 and can thus be considered an essential amino acid, in vivo it is efficiently metabolized to proline. this complicates the ms analysis and, in the absence of a precise description of arginine metabolism, restricts the analysis to peptides devoid of proline. a possible future circumvention of this problem could be based on neutron-encoded stable isotope labels 39, which are commercially available and might be adapted to pulse silac strategies to measure protein turnover."
"the main general limitations of protein turnover measurements in vivo at the moment are the lack of a simple workflow for reproducible measurements and the absence of an experimental framework that can be followed by non-experts in the field. these aspects have rendered the protein turnover field a closed community predominantly concentrating either on technical aspects of protein labeling 7, 16, 17 or on the proteins that are most easily addressed by simple labeling approaches, such as the ellps 9-11 ."
"as we can see in figure 11, the runtime of the mcmc algorithms grow linearly with the number of samples for both the sql and the java/scala implementations. while the scala/java implementation of mcmc-mh can generate 1 million samples in around 51 seconds, it takes about 78 seconds for the sql implementation of the mcmc-mh, and about 89 seconds for that of the gibbs sampling. this experiment shows that the in-database implementations of the mcmc sampling algorithms achieve comparable (within a factor of 1.5) runtimes to the java/scala implementation."
"troubleshooting advice can be found in table 1 . spectrum) and will therefore not yield reliable ratios. because we combine three biological replicates and three machine replicates for three different time points for each type of sample (e.g., cortex), the coverage scales up to~53,000 peptides, from which~26,000 heavy-versus-light peptide ratios were measured, corresponding to the determination of lifetimes for~3,300 proteins. in each condition, 2,100 of the calculated lifetimes yielded a reliable confidence interval, which serves as a measure of the lifetime precision."
"93 in this section, we describe four different procedures for validating the obtained results. follow option a for evaluating the labeling of the lysine pool in the plasma of animals. this validation might be necessary if the labeling across different conditions is largely heterogeneous. follow option b for an additional validation through miscleaved peptide analysis. this step is easy to implement in the case that miscleaved peptides are found in the database. follow option c for an additional experimental validation through a pulse-and-chase approach. this option is useful for estimating complex labeling trajectories required by some specific experimental designs. follow option d in the case that it is necessary to check for the labeling of an inducible protein, which might be relevant in the context of cell-specific proteomes (see the 'experimental design' section for a more detailed description of these options)."
"the metabolic labeling of mice with silac food was introduced more than a decade ago 25, 27, 28 . for our purposes, the pulsed metabolic labeling of mice is performed with a rather simple setup ( fig. 2a ), but there are a number of important aspects that should be considered in order to perform it correctly."
"both the gibbs sampler and the mcmc-mh algorithm are iterative algorithms, which contain three main steps: 1) initialization, 2) generating proposals, and 3) generating samples. they differ in their proposal and sample generation functions."
"renal cell carcinoma: both models identify -3p as an initiating event which is the locus of vhl, a tumour suppressor which regulates the hypoxia response pathway [cit] ) and plays a known initiating role in rcc [cit] . the models also agree that -3p promotes -6p. while both models find that -4q is related to -4p, -6q, -13q and +17q, these edges point away from -4q in the cbn and point towards -4q in the mhn. similarly, cbn finds that +17q promotes +17p, while mhn finds that +17p promotes +17q. figure s5 : breast cancer: (a) shows the raw data, where rows are copy number alterations and are sorted by frequency, while columns are event tumours whose 0/1-patterns are sorted lexicographically. (b) shows the cbn estimated from this data, where edges denote that all parent alterations must have occured before the child alteration can. afterwards this alteration happens with the rate annotated in the corresponding node. (c) shows the mhn estimated from this data, where alterations initially happen with rate annotated in the corresponding node. once an alteration has occured, it multiplies the rate of other events by the factor annotated on the edges. figure s6 : colorectal cancer: (a) shows the raw data, where rows are copy number alterations and are sorted by frequency, while columns are event tumours whose 0/1-patterns are sorted lexicographically. (b) shows the cbn estimated from this data, where edges denote that all parent alterations must have occured before the child alteration can. afterwards this alteration happens with the rate annotated in the corresponding node. (c) shows the mhn estimated from this data, where alterations initially happen with rate annotated in the corresponding node. once an alteration has occured, it multiplies the rate of other events by the factor annotated on the edges. figure s7 : renal cell carcinoma: (a) shows the raw data, where rows are copy number alterations and are sorted by frequency, while columns are event tumours whose 0/1-patterns are sorted lexicographically. (b) shows the cbn estimated from this data, where edges denote that all parent alterations must have occured before the child alteration can. afterwards this alteration happens with the rate annotated in the corresponding node. (c) shows the mhn estimated from this data, where alterations initially happen with rate annotated in the corresponding node. once an alteration has occured, it multiplies the rate of other events by the factor annotated on the edges."
"as we can see, viterbi, gibbs and mcmc-mh can all compute top-k queries over the linear-chain crf models; sum-product, gibbs and mcmc-mh can all compute marginal queries over the linear-chain and tree-shaped models; while only mcmc algorithms can compute queries over cyclic models. although there are heuristic adaptations of the sum-product algorithm for cyclic models, past literature found mcmc methods to be more effective in handling complicated cyclic models with long-distance dependencies and deterministic constraints [cit] . in terms of handling query constraints, viterbi and sum-product algorithms can only handle selection constraints, gibbs sampling can handle selection constraints and aggregate constraints that do not break the distribution into disconnected regions. on the other hand, mcmc-mh can handle arbitrary constraints in the \"sql+ie\" queries."
"the stability of the proteome is an essential aspect of living organisms. the mechanisms that regulate the maintenance of a functional proteome are important in several aspects of biology, especially in connection with pathologies such as neurodegeneration, protein accumulation, prion diseases and aging 1 . measuring protein turnover in vitro (i.e., in cell cultures) using isotope-labeled amino acids is an accessible technique that can be applied by virtually any laboratory with moderate experience in cell culture and ms 2 . the situation in vivo is markedly complicated by the sizeable reuse of amino acids within the animal organism during physiological protein metabolism 3, 4 ."
"in this paper, we drew lessons from the perceptual characteristics of the cells at early visual stages for features such as edges, shapes, and colours and proposed a hierarchical contour extraction method based on the human vision mechanisms. our key contributions can be summarized as follows: (i) exploring the receptive field computational models of various cells from retina to v2 and proposing a new biologically-inspired framework for image contour detection; (ii) analyzing and extending current neural models for contour detecting by accounting for feedback connections and pooling mechanism between v1 and v2; (iii) introducing a center-surround mechanism based model by combining multiple local cues; (iv) accounting for the property that salient and meaningful contours are more likely to be retained at different scales by integrating the information at multiple scales. we quantitatively compared our model to current state-of-the-art algorithms on both synthetic images and benchmark dataset, and our results show a significant improvement compared to the other biologically-inspired models while being competitive to the machine learning ones. experimental results also demonstrate that our model achieves a good trade-off between edge detection and texture suppression. this paper mainly simulated the hierarchical perception characteristics of different brain regions in capturing the image contours, which is mainly still based on the processing of low-level, local visual information. actually, the visual cells have comparatively strong dynamic perception characteristics, and higher-level cells have a special information feedback effect on low-level cells (such as v4). therefore, our future work will be based on the study of a rapid contour extraction method driven by the target information and global features in complex visual scenes, and we will integrate a higher level of a variety of visual features to improve contour detection performance and achieve other computer vision tasks."
the constrained top-k inference can be computed by a variant of the viterbi algorithm which restricts the chosen labels y to conform with the evidence s.
"however, mhn identifies mutual exclusivity between +17q and -16q (red) which the cbn cannot. interestingly, gains at 17q, the locus of the oncogene erbb2, are associated with a poor prognosis [cit] while losses at 16q are associated with a good prognosis [cit] . moreover, in the cbn the event +1q predisposes cancers to a subsequent 16q loss. the mhn model agrees that the two events are related but interprets their interplay differently: here +17q inhibits both +1q and -16q and their association can be explained away by the absence of +17q. hence the mhn does not see a driver event in +1q which facilitates a subsequent 16q loss and thus a favorable course of progression."
"the visual information transfer process is an information processing flow with a basis in the hierarchical structure of the visual perception system. in addition to the aforementioned feedforward and lateral connections, there is a large amount of feedback information transfer in the visual information flow between the cells of various levels and areas. therefore, in combination with the contents of the study, we considered only the edge information feedback from v2 to v1 in this work corresponding to the well established fact that global shape influences local contours [32, [cit] . by maximizing the responses of v2 cells in all four channels (i.e., rg, gr, yb, by), we simulated the global shape and sent it as feedback to v1. this feedback is processed only one time same as all other inputs to v1. the final edge response of our model is obtained according to:"
"where r ncrf represents the spatial scope of the ncrf determined by the dog + in equation (6) . in connection with the distance, direction, and colour features of a given image, we propose integration of the three for processing. this is consistent with the biological characteristics of cells in the visual cortex-that is, the complexity of the surround inhibition effect of the cells. in addition, compared with some existing methods [cit], features such as colour, distance, and direction are fused in this paper, and a distance-weighted colour-oriented encoding method is proposed, which overcomes the noise interference that taking a single extremum from the local area is easily susceptible to and solves the problem of difficulty in detecting the colour boundaries of weak texture (as shown in figure 2 )."
"viterbi, a special case of the max-product algorithm [cit] can compute top-k inference for linear-chain crf models. viterbi is a dynamic programming algorithm that computes a two dimensional v matrix, where each cell v (i, y) stores a ranked list of partial label sequences (i.e., paths) up to position i ending with label y and ordered by score. based on equation (1), the recurrence to compute the top-1 segmentation is as follows:"
"the structure of the grounded model can be quantified with three parameters: shape of the model, maximum size of the clique and the maximum length of the loops. the first parameter determines the applicability of the models, and is also the most important factor in the accuracy and the runtime of the inference algorithms over the model. although not studied in this paper, the maximum clique size and the length of the loops play an important role in the runtime of several known inference algorithms (including, for example, the junction tree and the loopy belief propagation algorithms) [cit] ."
"in this example, we use q2, the query with an aggregate constraint, described in section 4.1. as shown in figure 5 (c), the aggregate constraints can induce a big clique including all the label variables in each document. in other words, regardless of the text, based on the model and the query, each document is instantiated into a cyclic graph with high tree-width."
"although silac diets are commercially available, a practical limitation for the application of the approach is the high cost of the isotopically labeled mouse food. aware of this limitation, in this protocol, we have streamlined the design of experiments and we provide different experimental designs, based on the most common needs, that would minimize animal number, labeling time and costs (see supplementary table 1 ). as an example, a minimal design could restrict the use of labeled food to a few hundred grams, enabling reliable results with an affordable investment (on the order of €/$1,500)."
", where w u and w b are unigrams and bigrams respectively in sentence s. feature 18 and 19 are variants of features 11, where instead of document frequency (df in the formula above), bigram and unigram's novelty and uniqueness values are used. among these features, the feature values of feature 4, 5 and 6 are discrete. in this study, we discretized all the other continuous values into ten categories according to the value range in the training data."
"having implemented a number of inference algorithms in the database, including viterbi dynamic programming for linear-chain crf, sumproduct belief propagation, and the mcmc methods described above, we have developed a set of design guidelines for implementing statistical methods in the database:"
"c critical step the multiplicity corresponds to the number of labels present in the sample. for a pulse with 13 c 6 -lysine (lys6), the multiplicity will be 2: lys0 (light) and lys6 (heavy). ? troubleshooting 67 select 'lys6' from among the heavy labels ( fig. 5) . 68 select the database in 'global parameters' (fig. 5 ). 69 untick or switch 'off' in the 'requantify' option."
"step 93b: validation through miscleaved peptide analysis. validation of the data can be performed by analyzing the labeling profile of peptides that contain two (or more) lysines. following metabolic labeling in animals, unlabeled lysines from the tissues ( 12 c 6 -lysines) mix with the heavy lysines from the food ( 13 c 6 -lysines). as a result, peptides containing two lysines can be found in one of three different states: (i) both lysines are unlabeled or 'light' (l; two 12 c 6 -lysines); (ii) both lysines are labeled or 'heavy' (h; two 13 c 6 -lysines); and (iii) one lysine is labeled and one is unlabeled; thus the peptide is considered 'medium' (m; one 12 c 6 -lysine and one 13 c 6 -lysine). we provide a script for predicting the labeling status of a peptide based on its estimated lifetime. the predicted result can be compared to the experimentally measured labeling status as described in the protocol."
"we have previously published the average behavior of the lysine pool in mice 4 . the values can be immediately used for initial lifetime estimations and are included as pre-sets in our script. at the same time, it is possible that specific pharmacological or genetic manipulations will affect the lysine pool. for this reason, in this protocol we include a specific script that can be used to estimate the pool of available 13 c 6 -lysines from a global fit of the labeling results. once the most accurate lysine pool parameters are chosen, the heavy-versus-light ratios can be used to calculate protein lifetimes."
"owing to the wide range of applications of protein turnover measurements, each specific experiment might require a set of tailored validation steps. we include in this protocol a gallery of four controls that can be useful for these validation steps (summarized in fig. 1e and detailed in the protocol). these include (i) a gas chromatography-ms (gc-ms) method for the direct measure of free 13 c 6 -lysines from biological fluids (such as blood plasma) or tissue samples (option a), which is useful for monitoring the availability of labeled lysines during the labeling procedure; (ii) a protocol and a script that allow definition of the labeling profile of double-labeled peptides ('miscleaved' peptide analysis) (option b; this enables the experimenter to monitor and validate the overall labeling of the proteome while also eventually including the results of double-labeled peptides in the computation of the lifetimes); (iii) a script to predict, on the basis of the lifetime results, the relative labeling of specific proteins in a chase experiment (option c); (iv) a method that allows the measurement of the labeling of an exogenous protein after genetic induction (option d). this method can be used to confirm that the available 13 c 6 -lysine concentration in a particular organ corresponds to the specific value calculated from a global fit of the labeling results (as discussed in the previous paragraph). these final validations may be required only by laboratories with special experimental needs. the paragraphs below provide a more detailed description of each of the validation approaches."
"contour detection refers to the identification of the boundary between the target and the background in an image, focusing on the dividing lines outside the targets [cit] . as an important mid-level visual task, contour detection can greatly reduce the image dimension while retaining important structures and rejecting a large amount of weakly related information, which lays an important foundation for advanced computer vision tasks (such as target interpretation [cit] and image analysis [cit] )."
"the general flowchart of the contour detection algorithm proposed in this paper is summarized in figure 1 . the proposed model mimicking the human visual processing stages is a hierarchical visual architecture. according to differentiation between anatomical structure and physiological function, the frontal cortex or brain areas responsible for contour and boundary detection in the hvs include the retina, lgn, v1, and v2, etc. first, the input image is perceived and captured by single-opponent ganglion cells and lgn cells, which also carry out a convolution operation to form an image with four paths of different colours for the opponent channels; it is then further processed by the double-opponent v1 cells with different orientation sensitivities to preliminarily obtain the local edges of colour and luminance for the image. next, to better solve the problems of texture suppression and contour integration in complex natural scenes, we use a multi-feature suppression and integration method under the guidance of multi-scale information and put forward a distance-weighted colour-oriented encoding method that automatically modulates the suppression strength of each pixel position using the surround inhibition weight obtained after integration. finally, the feedforward and feedback processing of the v2 in response to the edges and textures of the cells in v1 is introduced, and methods such as non-maxima suppression and hysteresis threshold are used for processing the contours, such as refinement and integration, to obtain the hierarchical expression of the final image contours. with multifeature-based surround modulation. furthermore, we will explore the neural connections and pooling mechanism between visual areas with new physiological findings."
"the efficient implementation of the gibbs sampler is shown in figure 6, which uses the feature of window functions introduced in postgresql 8.4. mcmc-mh can be implemented efficiently in a similar way with some simple adaptations."
"however, as we see in figure 5 (a) and (b), depending on the text, the joint crf model can be instantiated into either a cyclic graph or a tree-shaped graph. the red edge in figure 9 shows the query plan with hybrid inference for the join query q1. as we can see, instead of performing mcmc methods unilaterally across all \"joinable\" document pairs (i.e., contain at least 1 pair of common tokens), the sum-product algorithm is used over the document pairs that contain only 1 pair of common tokens. compared to the non-hybrid query plan, the hybrid inference reduces the runtime by applying the more efficient inference (i.e., sum-product) when possible. the speedup depends on the performance of sum-product compared to the mcmc methods, and the percentage of the \"joinable\" document pairs that only share one pair of common tokens that are not \"stop-words\"."
"in a more complex situation, where the rate of an event does not depend equally strong on its parent events, we see a similar behaviour. for the cbn most transitions between genotypes are forbidden, while an approximating mhn assigns the highest rates to the same transitions but also allows transitions with smaller rates from states where only some parent events have occured ( figure s2 : a cbn (left) and its approximating mhn (right) defining the markov processes shown below them. for illustration we only show transitions that introduce event 1 to the genotype."
the statistical test(s) used and whether they are one-or two-sided only common tests should be described solely by name; describe more complex techniques in the methods section.
"general information about the dataset is shown in a text window. a testing dataset for evaluation purposes is included as supplementary data 2. 79 fitting of pool parameters (steps 79-84). in the 'pool fit' tab of the graphical user interface (fig. 9), select the reference condition from which the pool parameters are to be calculated (usually control conditions). set the minimum number of data points as the threshold for protein inclusion. the number of proteins thereby selected will be displayed on the top in the threshold drop-down menu. c critical step this part of the script will be used to calculate the pool of lysines available for protein synthesis with a global fitting approach from the labeling data as previously described 4 . the pool of available lysines has two components: a fast component deriving from the rapid incorporation of the 13 c 6 -lysines from the labeled food and a slower component that depends on the slow exchange of the labeled 13 c 6 -lysines with the unlabeled 12 c 6 -lysines deriving from the degradation of the proteome. for the determination of individual protein lifetimes, the exchange of the underlying pool of available heavy and light lysines must be estimated. the optimal pool"
"this section covers our definition of a probabilistic database, the conditional random fields (crf) model and the different types of inference algorithms over crf models in the context of information extraction. we also introduce a template for the types of ie queries studied in this paper."
"a description of all covariates tested a description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons a full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) and variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)"
"finally, figure 9 shows a comparison of the p-r curves for the bsds500 image database from our bihcd model and other contour detection models. it can be seen in the figure that our algorithm attains a higher recall ratio and accuracy rate on the p-r curve than the classical canny and pb algorithms and the sco and mci models based on the biological vision mechanism, better illustrating that our algorithm simultaneously has better results in edge detection and texture suppression. figure 9 . an overall performance comparison of the p-r curve on bsds500."
"over the crf-based ie from text, the queries we consider are probabilistic queries, which inference over the probabilistic attribute label p in the tokentbl table. each tokentbl is associated with a specific crf model stored in the mr table. such crfbased ie is captured by a sub-query with logic that produces the ie results from the base probabilistic tokentbl tables. the subquery consists of a relational part qre over the probabilistic token tables tokentbl and the underlying crf models, followed by an inference operator q inf . a canonical \"query template\" captures the logic for the \"sql+ie\" sub-query in figure 3 . it supports spj queries, aggregate conditions and two types of inference operators, top-k and marginal, over the probabilistic tokentbl tables."
"many inference algorithms are known that can answer the above inference queries over the crf models, varying in their effectiveness for different crf characteristics (e.g., shape of the graph). in the next sections, three inference algorithms will be described: viterbi, sum-product, markov chain monte carlo (mcmc) methods."
"these results are in line with the observation that the turnover measurements are overall quite reproducible. as an example, the average coefficient of determination (r 2 ) between technical replicates is 0.95 ± 0.01 (s.e.m.) and is 0.91 ± 0.03 for biological replicates, highlighting the small variation among experiments. it is likely that the high reproducibility of these measurements arises from the fact that each turnover measure is based on a ratio that arises from the detection of 'heavy' and 'light' values, which is much less error prone than, for example, absolute quantification of protein amounts."
"historically, the main strategy for estimating protein turnover in living organisms has been based on pulsing isotopically labeled amino acids (or amino acid precursors) and monitoring their appearance in the proteins that are synthesized in animals 7, 9, 12, [cit] . for simplicity, protein turnover can be considered as a kinetic process in which an old protein population is replaced with a new population by neosynthesis and degradation of the older proteins. in a system in which all the new proteins are synthetized from a new pool of fully labeled amino acids, this process can be approximated to a simple first-order kinetic. the situation in vivo is complicated by the fact that the pool of old (unlabeled) amino acids cannot be quickly replaced by labeled amino acids. this is because in vivo amino acids can be reused and the old (unlabeled) amino acids can become incorporated into new proteins. if the exact amino acid reuse is neglected, one can observe only relative differences in turnover between proteins by metabolically labeling animals for different amounts of time, followed up by measurements in which one assesses the speed at which each protein species becomes labeled."
"different inference algorithms over the probabilistic graphical models have been developed in a diverse range of communities (e.g., natural language processing, machine learning, etc). the characteristics of these inference algorithms (e.g., applicability, accuracy, convergence rate, runtimes) over different model structures have since been studied to help modeling experts select an appropriate inference algorithm for a specific problem [cit] ."
"compared with some existing methods [cit], features such as colour, distance, and direction are fused in this paper, and a distance-weighted colour-oriented encoding method is proposed, which overcomes the noise interference that taking a single extremum from the local area is easily susceptible to and solves the problem of difficulty in detecting the colour boundaries of weak texture (as shown in figure 2 ). to describe the suppression effect of the ncrf periphery on the difference in the colour orientation of the receptive field at the centre of the crf, the maximal value of the response in each direction of the four opponent channels is taken as the response of that colour orientation-that is:"
"for the mcmc algorithms, we measure the computation error and runtime for every 10k more samples, starting from 10k to 1 million samples over all documents. as we can see in figure 13, the computation error of the gibbs algorithm drops to 22% from 45, 000 to 10, 000 when 500k samples are generated. this takes around 75 seconds, more than 12 times longer than the runtime of the viterbi algorithm. the mcmc-mh converges much slower than the gibbs sampling. as more samples are generated, the top-1 extractions generated from the mcmc algorithms get closer and closer to the exact top-1, however very slowly. thus, viterbi beats the mcmc methods by far in computing top-1 extractions with linear-chain crf models: more than 10 times faster with more than 20% fewer computation errors."
"at the same time, considering the attenuation of the suppression effect by the luminance and contrast of pixel points at different distances in the periphery on the centre point as distance increases, and finally, under the modulation of luminance and contrast, the suppression weight coefficients of the periphery on the crf centre at (x, y) are, respectively:"
"protein stability measurements are important to several domains of biology, encompassing a number of pathological alterations such as cancer and neurodegeneration. mice are extensively utilized as genetic models, and protein turnover measurements have the potential to become as widely used as protein abundance measurements for general screening purposes. in general, the assessment of protein turnover parameters should be considered as an additional technical option in the palette of tools available for molecular biologists and could be used both as a validation and a discovery tool."
"protocol total protein for optimal results. if a pooling strategy cannot be used, it is possible to apply an ms method that does not require fractionation (see below), although this results in a limited amount of detected proteins and scarce proteome coverage."
"to dissect the benefits of constraint propagation independently from multiview learning, we evaluated the use of constraint propagation in traditional single-view clustering scenarios. we found that using the constraints inferred by propagation can also improve performance in single-view clustering, further explaining its performance in multi-view scenarios and also demonstrating the high quality of the inferred constraints."
"as shown in figure 4, constraint propagation clearly performs better than the baseline of single view clustering, and better than cluster membership for inferring constraints in all cases, except for when learning with few constraints on letters/digits. constraint propagation also yields an improvement over direct mapping for each percentage of instances mapped between views, as shown in figure 5 . unlike direct mapping, constraint propagation is able to transfer those constraints that would otherwise be discarded, increasing the performance of multi-view clustering. the performance of both constraint propagation and direct mapping improve as the mapping becomes more complete between the"
"our approach, given as algorithm 2, iteratively clusters each view, infers new constraints within each view, and transfers those inferred constraints across views via the mapping. through this process, progress in learning the model for one view will be rapidly transmitted to other views, making this approach particularly suited for problems where different aspects of the model are easy to learn in one view but difficult to learn in others."
"additionally, like other multi-view algorithms, we found constraint propagation to be somewhat sensitive to the cutoff thresholds t v, but this problem can be remedied by using cross-validation to choose t v . too high a threshold yields performance identical to direct mapping (since no constraints would be inferred), while too low a threshold yields the same decreased performance as exhibited by other co-training algorithms. for this reason, we recommend setting t v to optimize cross-validated performance over the set of constrained instances."
"these results show that the constraints inferred by propagation within a single view can also be used to improve clustering performance in that view. both mpck-means and pck-means show improved performance using the constraints inferred by propagation to augment the original constraints. the maximum improvement from constraint propagation occurs with a moderate number of constraints; as we would expect, constraint propagation provides little benefit when the number of original constraints is either very small or very large. even in the single-view case, the number of constraints inferred by propagation is roughly linear in the number of original constraints (figure 9 ). we also found that the inferred constraints have very high precision with the true cluster assignments in all single-view scenarios ( figure 10) . as in the multi-view experiments, the inferred cannot-link constraints have slightly higher precision than the inferred must-link constraints."
"due to the intuitive nature of must-link and cannot-link constraints for user interaction, constrained clustering has been applied to the problem of interactive clustering, where the system and user collaborate to generate the model. [cit] present interactive approaches in which a user iteratively provides feedback to improve the quality of a proposed clustering. in both of these cases, the user feedback is incorporated in the form of constraints. this interactive process is a useful extension that could permit user knowledge to be brought into a multi-view clustering algorithm. additionally, as we discuss in section 7, a user could specify these constraints in one view of the data where interaction is quick and intuitive (such as images). our multi-view clustering algorithm could then be used to automatically propagate and transfer these constraints to affect the clustering of other views where interaction may be more difficult (such as text or other data modalities). interaction could be further improved by the use of active learning to query the users for specific constraints [cit] ."
"we measure performance using the pairwise f-measure -a version of the information-theoretic f-measure adapted to measure the number of same-cluster pairs for clustering [cit] . the pairwise f-measure is the harmonic mean of precision and recall, given by"
"since each inferred constraint represents an estimate of the minimal strength of the pairwise relationship. the partitioning for view v can then be computed by clustering the data x v subject to the constraints inc v (step 10). the ckmeans subfunction computes the clustering that maximizes the log-likelihood of the data subject to the set of constraints, thereby completing the m-step of algorithm 2."
"we found that for high-dimensional data, the curse of dimensionality causes instances to be so far separated that constraint propagation is only able to infer constraints with a very low weight. consequently, it works best with a low-dimensional embedding of the data, motivating our use of spectral feature reduction. other approaches could also be used for creating the low-dimensional embedding, such as principal components analysis or manifold learning."
"the overall computational complexity of algorithm 2 is determined by the maximum number of em iterations and by the complexity of the ckmeans function, which depends on the chosen clustering algorithm. besides these aspects, the constraint propagation step (step 11) incurs the greatest computational cost. to make this step computationally efficient, our implementation relies on the independence assumption inherent in equation 11 between the two endpoints of the constraint. to efficiently compute the weight of all propagated constraints, we memoize the value of each endpoint's propagation"
"is the mahalanobis distance between x i and x j using the metric m. the first term of j mpck attempts to maximize the log-likelihood of the k-means clustering, while the second and third terms incorporate the costs of violating constraints in c."
"this article proposes the first multi-view constrained clustering algorithm that considers the use of an incomplete mapping between views. given an incomplete mapping, our approach propagates the given constraints within each view to pairs of instances that have equivalences in the other views. since these propagated constraints involve only instances with a mapping to the other views, fig. 1. an illustration of multi-view constrained clustering between two disjoint data views: text and images. we are given a very limited mapping between the views (solid black lines) and a set of pairwise constraints in the images view: two must-link constraints (thick solid green lines) and one cannot-link constraint (thick dashed red line). based on the current clustering, each given constraint is propagated to pairs of images that are in close proximity to the given constraint and can be mapped to the text view. these propagated must-link and cannot-link constraints (thin solid green and dashed red lines, respectively) are then directly transferred via the mapping to form constraints between text documents and influence the clustering in the next co-em iteration. (best viewed in color.) they can be directly transferred to instances in those other views and affect the clustering. the weight of each propagated constraint is given by its similarity to the original constraint, as measured by a local radial basis weighting function that is based on the current estimate of the clustering. this process is depicted in figure 1 . our approach uses a variant of co-em [cit] to iteratively estimate the propagation within each view, transfer the constraints across views, and update the clustering model. our experiments show that using co-em with constraint propagation provides an effective mechanism for multiview learning under an incomplete mapping between views, yielding significant improvement over several other mechanisms for transferring constraints across views. we also demonstrate that constraint propagation can improve clustering performance even in single-view scenarios, further demonstrating the precision of the inferred constraints and the utility of constraint propagation."
"four quadrants is a synthetic data set composed of 200 instances drawn from four gaussians in r 2 space with identity covariance. the gaussians are centered at the coordinates (±3, ±3), one in each of the four quadrants. quadrants i and iv belong to the same cluster and quadrants ii and iii belong to the same cluster. the challenge in this simple data set is to identify these clusters automatically, which requires the use of constraints to improve performance beyond random chance. to form the two views, we drew 50 instances from each of the four gaussians, divided them evenly between views, and created mappings between nearest neighbors that were in the same quadrant but different views."
"constrained clustering algorithms [cit] incorporate side information to influence the resulting clustering model. most constrained clustering research has focused on using side information given as a set of constraints that specify the relative cluster membership of sets of instances. typically, these algorithms use both must-link constraints, which specify sets of instances that belong in the same cluster, and cannot-link constraints, which specify instances that belong in different clusters. depending on the algorithm, this labeled knowledge may be treated as either hard constraints that cannot be violated, or soft constraints that can be violated with some penalty."
"to yield diagonal matrices of eigenvalues in λ m −1 h and λς h . to derive the scale factor α h, we ensure that both first principal components have equal variances, which occurs when"
"in cases where the data set does violate these conditions, we have observed that the clustering is primarily driven by the provided constraints rather than the k-means component. one of the strengths of the mpck-means and pckmeans algorithms is that in situations where k-means does poorly on the data, large numbers of constraints can overcome the k-means component and ensure the proper clustering. in terms of the pck-means/mpck-means objective function (equation 1), the second and third terms of the equation (those involving the must-and cannot-link constraints) dominate the first term (k-means) in these situations. when the data set does not meet these conditions, the cluster distributions specified by the learned centroids and metrics have little in common with the resulting partitions. during the clustering process, we can examine the learned cluster distributions for disagreement with the resulting partitions; the presence of a large disagreement is one indication that the data set may be inappropriate for constraint propagation."
"as mentioned earlier, our approach can be combined with any constrained clustering method. our current implementation supports the pck-means and mpck-means [cit] ) algorithms; we give results for both methods in section 6. in the remainder of this section, we provide a brief overview of these methods; further details are available in the original papers."
"since the ordering of the instances matters in the propagation, we compute both possible pairings of constraint endpoints (x the e-step of algorithm 2 (step 11) uses equation 10 to propagate all given constraints within each view to those instancesx v with cross-view mappings, thereby inferring the expected value of constraints between those instances given the current clustering. using this set of expected constraints p v, we then update the current clustering model in the m-step, as described in the next section."
"is the squared mahalanobis distance between x v and µ v h according to the cluster's rescaled metric σ h −1 . we assume that each constraint should be propagated with respect to the current clustering model, with the shape (i.e., covariance) of the propagation being equivalent to the shape of the respective clusters (as given by their covariance matrices). additionally, we assume that the propagation distance should be proportional to the constraint's location in the cluster. intuitively, a constraint located near the center of a cluster can be propagated a far distance, up to the cluster's edges, since being located near the center of the cluster implies that the model has high confidence in the relationship depicted by the constraint. similarly, a constraint located near the edges of a cluster should only be propagated a short distance, since the relative cluster membership of these points is less certain at the cluster's fringe."
"constraint propagation has the ability to improve multi-view constrained clustering when the mapping between views is incomplete. besides improving performance, constraint propagation also enables information supplied in one view to be propagated and transferred to improve learning in other views. this is especially beneficial for applications in which it is more natural for users to interact with particular views of the data. for example, users may be able to rapidly and intuitively supply constraints between images, but may require a lengthy examination of other views (e.g., text or audio) in order to infer constraints. in other cases, users may not have access to particular views (or even data within a view) due to privacy restrictions. in these scenarios, our approach would be able to propagate user-supplied constraints both within and across views to maximize clustering performance."
"beyond our approach, there are a variety of other methods that could be adapted for learning with a partial mapping between views, such as manifold alignment and transfer learning. further work on this problem will improve the ability to use isolated instances that do not have a corresponding multi-view representation to improve learning, and enable multi-view learning to be used for a wider variety of applications."
"in order to examine the performance of our approach under various data distributions, we use a combination of synthetic and real data in our experiments. [cit] to create these multi-view data sets by pairing classes together to create \"super-instances\" consisting of one instance from each class in the pair. the two original instances then represent two different views of the super-instance, and their connection forms a mapping between the views. this methodology can be trivially extended to an arbitrary number of views. these data sets are described below and summarized in table 1 ."
"this problem arises in many industrial and military applications, where data from different modalities are often collected, processed, and stored independently by specialized analysts. consequently, the mapping between instances in the different views is incomplete. even in situations where the connections between views are recorded, sensor availability and scheduling may result in many isolated instances in the different views. although it is feasible to identify a partial mapping between the views, the lack of a complete bipartite mapping presents a challenge to most current multi-view learning methods. without a complete mapping, these methods will be unable to transfer any information involving an isolated instance to the other views."
"we first survey related work on constrained clustering and multi-view learning in section 2, and then present details in section 3 on the specific constrained clustering and co-em algorithms on which we base our approach. section 4 describes our problem setting and mathematical notation. we develop our multiview constrained clustering algorithm in section 5, describing the constraint propagation and clustering processes in sections 5.1-5.2, its extension to more than two views in section 5.3, and implementation efficiency in section 5.4. section 6 evaluates the performance of our approach in several multi-view scenarios (section 6.3), and then analyzes the performance of constraint propagation inde-pendently through traditional single-view (section 6.4) clustering. we conclude with a brief discussion of constraint propagation and future work in section 7."
"to address this problem, we propose a method for multi-view learning with an incomplete mapping in the context of constrained clustering. constrained clustering [cit] ) is a class of semi-supervised learning methods that cluster data, subject to a set of hard or soft constraints that specify the relative cluster membership of pairs of instances. these constraints serve as background information for the clustering by specifying instance pairs that belong in either the same cluster (a must-link constraint) or different clusters (a cannot-link constraint). given a set of constraints in each view, our approach transfers these constraints to affect learning in the other views. with a complete mapping, each constraint has a direct correspondence in the other views, and therefore can be directly transferred between views using current methods. however, with a partial mapping, these constraints may be between instances that do not have equivalences in the other views, presenting a challenge to multi-view learning, especially when the mapping is very limited."
"given the expected constraints p u between instances inx u, we transfer those constraints to the other views and then update the clustering model to reflect these new constraints. these steps together constitute the m-step of algorithm 2."
"2 views, with constraint propagation still retaining an advantage over direct mapping even with a complete mapping, as shown in all data sets. we hypothesize that in the case of a complete mapping, constraint propagation behaves similarly to spatial constraints [cit], warping the underlying space with the inference of new constraints that improve performance. on these data sets, the number of constraints inferred by constraint propagation is approximately linear in the number of original constraints, as shown in figure 6 . clearly, as the mapping between views becomes more complete, constraint propagation is able to infer a larger number of constraints between those instances inx v . the improvement in clustering performance is due to the high precision of the propagated constraints. figure 7 shows the average weighted precision of the propagated constraints for the 100% mapping case, measured against the complete set of pairwise constraints that can be inferred from the true cluster labels. the proportion that each propagated constraint contributed to the weighted precision is given by the constraint's inferred weight w. we also measured the precision of propagated constraints for various partial mappings, and the results were comparable to those for the complete mapping. the constraints inferred through propagation show a high average precision of 98-100% for all data sets, signifying that the propagation method infers very few incorrect constraints."
"cross-view mapping fig. 5 . the performance improvement of constraint propagation over direct mapping in figure 4, averaged over the learning curve. the peak whiskers depict the maximum percentage improvement."
direct mapping transfers only those constraints that already exist between instances inx v . this approach is equivalent to other methods for multiview learning that are only capable of transferring labeled information if there is a direct mapping between views. cluster membership can be used to infer constraints between instances in x v . this approach simply considers the relative cluster membership for each pair of instances inx v and infers the appropriate type of constraint with a weight of 1. it represents the direct application of co-em to infer and transfer constraints between views via the partial mapping. single view performs constrained clustering on each of the individual views in isolation and serves as a lower baseline for the experiments.
"in kumar and daumé's work, an em co-training approach is used to perform spectral clustering within one view and use this as a constraint on the similarity graph in another view . in subsequent work, they used a regularization approach to optimize the shared clustering . in effect, these approaches are propagating the entire clustering across views, in contrast to our method, which only propagates the explicit constraints to nearby pairs of instances. also, unlike our approach, these works assume a complete bipartite mapping between views. other multi-view clustering variations are based on cross-modal clustering between perceptual channels [cit] and information-theoretic frameworks [cit] ."
"recently, several researchers have studied ways to develop mappings between alternative views. [cit] describe a method for learning from what they refer to as multiple outlooks. outlooks are similar to views in that they each have different feature spaces; however, there is no assumption that instances would appear in multiple outlooks. (harel and mannor mention the possibility of shared instances but do not present any results.) they learn an affine mapping that scales and rotates a given outlook into another outlook by matching the moments of the empirical distributions within the outlooks. this work assumes that the outlooks can be mapped to each other globally with a single affine mapping, whereas our work assumes only local, potentially nonlinear mappings between the views, based on the learned clustering structure. [cit] introduce a technique for projecting multiple views into a common, shared feature space, producing a joint distance function across the views. their work assumes that each instance appears in every view. they use a neighborhood relationship that defines \"similar\" instances to optimize a similarity function in the shared feature space that respects the neighborhood relationships. the problem setting in their work is rather different from ours, since they assume a complete bipartite mapping between views, and the provided input consists of neighborhood sets rather than pairwise constraints."
"we propagate a given constraint x 7). this ensures that the amount of propagation falls off with increasing distance from the centroid, in direct relation to the model's confidence in the cluster membership of x v u . the covariance matrix for the constraint propagation function is then given by"
"to determine whether the data set meets the second condition, we can also directly examine the resulting clusters for overlap. since constraints are propagated based on rbf distance from the given constraint and the relative cluster membership of the endpoints, constraint propagation may not infer constraints correctly between points belonging to overlapping clusters. we can detect overlap in the clusters by measuring the kl divergence [cit] between their learned distributions; the presence of significant overlap is another indication that the data set may violate the conditions necessary for successful constraint propagation. currently, we simply alert the user if the data set may be inappropriate for constraint propagation based on either indicator; we leave the robust determination of whether constraint propagation is guaranteed to improve performance to future work."
"in each trial, we consider performance as we vary the number of constraints used for learning and the percentage of instances in each view that are mapped to the other views. our results are shown in figure 4, averaged over 100 trials."
"other applications of multi-view clustering include image search [cit], biomedical data analysis [cit], audio-visual speech and gesture analysis [cit], multilingual document clustering [cit], word sense disambiguation [cit], and e-mail classification [cit] )."
"h andς h, we compute α h as the scale such that the variances of the first principal component of each matrix are identical. we take the eigendecomposition of each matrix"
p r o t e i n ( p c k -m e a n s ) l e t t e r s / d i g i t s ( p c k -m e a n s ) r e c / t a l k ( m p c k -m e a n s )
"since all terms in the rhs summation are positive, we can compute them incrementally and stop early once the sum exceeds −2 ln t v, since we will never need to evaluate any propagation weight"
"for the base constrained clustering algorithms, we use the pck-means and mpck-means implementations provided in the wekaut extension 1 to the weka machine learning toolkit [cit] with their default values. for pck-means, constraint propagation uses the full empirical covariance for each cluster; for mpck-means, it uses the diagonal weighted euclidean metrics that are learned on a per-cluster basis by mpck-means."
"within x, there are pairs of instances that correspond to different views of the same objects. we denote this connection between two instances x a u and x"
"current multi-view algorithms typically assume that there is a complete bipartite mapping between instances in the different views to represent these correspondences, denoting that each object is represented in all views. the predictions of a model in one view are transferred via this mapping to instances in the other views, providing additional labeled data to improve learning. however, what happens if we have only a partial mapping between the views, where only a limited number of objects have multi-view representations?"
"the models generated by pck-means/mpck-means are equivalent to particular forms of gaussian mixtures [cit], and so our work is also closely related to research in constrained mixture modeling. [cit] learn a soft constrained gaussian mixture model using constraints to influence the prior distribution of instances to clusters. more recent work has focused on incorporating constraints into nonparametric mixture models [cit] ."
"in this section, we present background on the basic methods on which our approach builds: the pck-means and mpkc-means constrained clustering methods, and the co-em algorithm."
"we evaluated multi-view constrained clustering on a variety of data sets, both synthetic and real, showing that our approach improves multi-view learning under an incomplete mapping as compared to several other methods. our results reveal that the constraints inferred by propagation have high precision with respect to the true clusters in the data. we also examined the performance of constraint propagation in the individual views, revealing that constraint propagation can also improve performance in single-view clustering scenarios."
"the feature set and dimensionality d v may differ between the views. we will initially focus on the case of two views, given by x a and x b, and extend our approach to handle an arbitrary number of views in section 5.3."
"as mentioned in the previous section, we found that constraint propagation can decrease performance when applied to data sets that violate the assumptions of k-means clustering. the two conditions assumed by k-means that are most necessary for successful constraint propagation are that: (1) the clusters are globular under the cluster metrics, and (2) the clusters are separable (i.e., have little overlap). however, it may be difficult to determine a priori whether a particular data set meets these conditions, and is therefore an appropriate candidate for constraint propagation. for this reason, it is important to determine whether the data set satisfies these conditions during the clustering process; we next discuss two potential methods for making this determination."
"unlike co-training, co-em does not require the views to be independent in order to perform well. the co-em algorithm also learns from a large set of data with noisy labels each iteration, in contrast to co-training, which adds few labeled instances to the training set at each iteration. [cit] than the co-training algorithm. the approach we explore in this article uses a vari-ant of co-em to iteratively infer constraints in each view and to transfer those constraints to affect learning in the other views."
"within each view, we use the true cluster labels on the instances to sample a set of pairwise constraints, ensuring equal proportions of must-link and cannot-link constraints. the weight of all sampled constraints w is set to 1. we also sample a portion of the mapping to use for transferring constraints between views. both the sets of constraints and the mapping between views are resampled each trial."
"when the covariance matrix σ x v u is diagonal, we can further reduce the computational cost through early stopping of the gaussian evaluation once we are certain that the endpoint's propagation weight will be below the given threshold"
"using multiple different views often has a synergistic effect on learning, improving the performance of the resulting model beyond learning from a single view. multi-view learning is especially relevant to applications that simultaneously collect data from different modalities, with each unique modality providing one or more views of the data. for example, a textual field report may have associated image and video content, and an internet web page may contain both text and audio. each view contains unique complementary information about an object; only in combination do the views yield a complete representation of the original object. concepts that are challenging to learn in one view (e.g., identifying images of patrons at an italian restaurant) may be easier to recognize in another view (e.g., via the associated textual caption), providing an avenue to improve learning. multi-view learning can share learning progress in a single view to improve learning in the other views via the direct correspondences between views."
"we ran several additional experiments on multi-view data sets with poor mappings and distributions that violated the mixture-of-gaussians assumption of k-means clustering. on these data sets, constraint propagation decreased performance in some cases, due to inferring constraints that were not justified by the data. this would occur, for example, in clusters with a nested half-moon shape or concentric rings, where constraint propagation would incorrectly infer constraints between instances in the opposing cluster. in these cases, clustering using only the directly mapped constraints yielded the best performance."
"the mean field approximation is generalized to two populations of neurons having the same variances of weights and thresholds, but different mean weights due to ltp/ltd, which leads to eq. (29)."
"in fig. 3, the identified patterns are shown. in all cases, the dimensionality was set to 5, however to reduce clutter in the figures only the three patterns that were analyzed in detail are shown. it is clear that the matrix factorization methods and cogaps identify the same three patterns in the data. the two patterns not shown do differ between the methods, with cogaps producing two relatively flat patterns that capture biosynthesis and metabolic patterns (based on gene membership), and the nmf methods both producing a pattern spiking at 9 and 24 hours and a pattern with a broad peak at 12 − 18 hours. we refer to the three patterns shown in fig. 3 as \"falling\" (black line), \"transient\" (red line), and \"rising\" (blue line) respectively."
c t t t s d d d d d d d d                              t d  (9)
"recently a new set of methods have been introduced that address multiple regulation directly during trn estimation. these techniques should be considered along side factorization methods when extending trn models. an issue yet to be addressed is the problem of non-time series data, which will dominate the medically-relevant available data. the extension of trn results to humans will require integration of model organism time series data with extremely sparsely sampled human clinical data."
"14 the image of a well of energy is thus not representative of the underlying dynamics. if the initial condition of a trial is taken into a certain ball when the basin boundary is fractal, the trajectory has a certain probability of ending in either of the coexisting attractors [final state sensitivity [cit] ]. this phenomenon could introduce non-reproducibility of neural response as observed in recordings of single neurons and allow some transitions between attractors. it is expected that a chaotic active state produces a deterministic pattern of activity until it switches to another attractor when the trajectory approaches a fractal basin tongue [cit] . switching between neural states can be triggered by noise [cit], oscillations [cit] or other mechanisms such as short-term plasticity [cit] . the existence of fractal basins could be tested experimentally by recording the final state sensitivity of a cortical network."
"further work is needed to determine whether the bifurcations described in this article can also occur in more realistic models such as leaky integrate and fire neurons, quadratic integrate and fire neurons with adaptation which exhibit chaotic solutions [cit] and in biological networks. the effects of short-term plasticity [cit] or neuro-modulators [cit] should also be taken into account for modeling both short-term memory and working memory [cit] . to model long-term memory and simulate networks that can create several successive assemblies, consolidation mechanisms such as late ltp [cit] should also be considered."
"the mathematics of the decomposition (or factorization) of eqn. 1 introduces a number of issues. p can be considered basis vectors, there are an infinite number of sets of these bases that fit the data equally well. second, it is often the case that the actual number of dimensions that are required to fit the data within the noise is less than the smaller dimensionality of the y matrix, but the minimum is often unknown. third, the matrix x is unchanged by exchanges of magnitude between columns of a and rows of p."
"it still remains to integrate the results of matrix factorization techniques into trn estimation formally. this will require linking two inference mechanisms in a unified framework capable of linking inference from the data decomposition and inference on the graphical model expressing the trn. in addition, data from tfbs studies, chip methods, methylation measurements, and literature mining should be integrated to improve inference."
"however, while mcmc methods are designed to escape local maxima in the probability distribution and sample the distribution more fully, they are computationally expensive. for instance, cogaps analysis of the gist data took several hours, while the 500 runs of the nmf methods took only a few minutes. the apparent superiority of mcmc methods in tf activity estimation suggests that recent methods developed in machine learning to replace the inherently slow mcmc approach in other disciplines may be worthy of study in expression analysis and trn estimation (e.g., [cit] ."
"homeostatic plasticity is the change of synaptic connectivity onto each post-synaptic neuron based on its activity level [cit] . homeostatic plasticity acts at a slower time scale than ltp/ltd. in the following, only synaptic scaling is considered, under which synapses to a particular neuron are changed by an amount proportional to their strength [cit] ). we consider that the target firing level of each neuron is regulated so that it remains within a bounded interval (a ''target zone'' [cit] ). when the timeaveraged internal state, over a window of size t 2 ) t 1, of a neuron i reaches the lower or upper boundaries, s i -and s i ?, respectively, the weights are scaled towards their original magnitudes. similar to the notation described in section ''long-term synaptic potentiation/depression'', the time-averaged internal state over the window t 2 is given as"
"the chaotic active states are usually robust to noise. however, we have shown that in certain parts of the state space, transitions between attractors are possible due to the fractal nature of the basin boundaries. in case 1, the basin of attraction is regular and the new attractor can be interpreted to be a well of lower potential. this case is usually described in the attractor network formalism [cit] . in cases 2 and 3, the basin can be irregular and fractal."
"the evolution of the steady state curves obtained by solving eqs. (35) and (36) for case 1 is shown in fig. 7 . the first iterate of the map f does not give any information about the bifurcations related to cases 2 and 3, because the bifurcations involved concern the second iterate f 2 of the map. similar to eqs. (35) and (36), the steady-state curves are obtained for the second iterate f 2 under cases 2 and 3 as shown in figs. 8 and 9, respectively. the intersections of fig. 8 evolution of steady-state curves of the second iterates of map for case 2. a there are initially three fixed points. two of these fixed points correspond to the chaotic baseline, and constitute an unstable cycle. the middle fixed point is unstable. b the unstable cycle is now a stable period-two cycle. a subcritical pitchfork bifurcation occurs at the middle fixed point, which then becomes stable. there are thus five fixed points; the two newly created fixed points are unstable and correspond to an unstable period-two cycle. the periodic attractor corresponds to the baseline, whereas the stable fixed point to the persistent activity. c the stable and unstable period-two cycles collide via tangent bifurcations. only the stable fixed point remains (36) for case 1. a initially, there is only a single stable fixed point, corresponding to the baseline activity. b just after a tangent bifurcation, there are three fixed points. the two stable fixed points correspond to the baseline and persistent activity, respectively. c just after a second tangent bifurcation, there is again only a single stable fixed point because of very strong reverberatory excitation, the baseline activity has disappeared and only the attractor of persistent activity remains these curves give the fixed points for the first and second iterates of the map."
"if the ''border effects'' of the network are ignored (i.e. i varies from à1 to þ1) or equivalently if a ring network is considered [cit], the approximation of eq. (22) shows that the change in y i (n) is identical for all neurons. 12 contrary to the case of a constant external input (in section ''creation of persistent activity: case of constant input''), all neurons here belong to the same group. in this case, the synaptic weights are not independent, and therefore, a mean field approximation cannot be applied. simulations show that there is a critical value for which persistent activity appears (see fig. 14) . the bifurcation involved can be studied by considering the first two terms in the fourrier decomposition of the synaptic efficacy x ij [cit] ."
"it can be considered that the regions of the brain that receive sensory information [visual cortex [cit], central, arcuate sulcus and principal sulcus [cit], limbic system [cit], etc.] usually encode information from sensory organs in the form of a continuum. the stimuli are thus ''perceived,'' without being classifiable into discrete states (i.e., ''fuzzy'' states), and change rapidly as new information flows. contrariwise, higher cognitive functions such as sequence of motor actions [cit] or cell assemblies encoding words [cit] tend to be discrete events. they thus might participate in the process of abstraction and be maintained in memory for longer periods to allow for their combination with previously stored memories. if these two memories are the result of the same learning process, it raises the fundamental problem to determine the conditions under which one or the other type is formed. the answer to this question is important for understanding what constitutes ''abstraction,'' and how the brain can transform a continuum of stimuli into higher cognitive signals."
"this block is needed for error correction in channel. bch (bose, ray-chaudhuri, hocquenghem) coding has been selected for channel coding. the inputs of applied bch code are 12 bits that convert to 32 bits and are transmitted in channel. this code can correct 5 or fewer random errors in receiver [cit] . in each data transmission, a training sequence to which allocates 10% of the first transmitted sequences itself is multiplexed with the data sequences before the qpsk modulation. the main purpose of the training sequence is to provide the receiver with a known sequence which can be used for phase estimation and synchronization in decision feedback equalizer."
"according to the real data obtained from field measurements in the persian gulf and the simulations to which applied, at the first place, we present (9) as a suitable empirical formula describing the sound speed profile in the strait of hormuz. secondly, on the basis of the presented patterns, considering a five-path channel is appropriate for mentioned region. thirdly, in order to achieve reliable and band-width efficient communica- tions over an uwa short-range shallow water cha similar to the persian gulf, using linear prediction coding to compress acoustic signals and rc5 cryptography algorithm, because of the opportunity for great flexib in both performance characteristics and the level of security, has a good performance. finally, employing dfe successfully copes with isi and phase variations due to multipath propagation in the persian gulf channel."
"on the basis of extensive laboratory and field experiments [cit], the attenuation due to the absorption effects of boric acid (b(oh) 3 ), magnesium sulphate (mgso 4 ), and pure water (h 2 o) is considered. the total loss is the sum of individual losses due to each material. the experimental measurements and the resulting profile for each material as well as the total loss are shown in figure 5."
"when synaptic scaling is taken into account, a novel boundary where switching [cit] ) can occur is added to the dynamical system. in particular, we consider the time steps n 1 for which the time-averaged internal state ! i ðn 1 þ crosses either s i ? or s i -. because ! i ðnþ varies very slowly, we can expect that the internal states cross the boundaries only once, and then be scaled (recurrent modes at the boundaries s i"
it is possible that in the future this error estimation may be further improved through increased understanding of translational and regulatory processes related to individual genes. this may allow for gene-specific models that include a priori modeling of the probability of the translation of the mrna to protein and potential inclusion of models of alternative splice variants and their proteins.
"the knowledge that a tf is active, either through the inference noted above or by other predictive methods, permits one to estimate the probability that a novel target, potentially a tf itself, is activated by the tf. the most straightforward way of estimating this is to use the strength of the scores for the known targets, either the distribution of z-scores for the targets of a tf or the distribution of the mean values in a pattern p. this then allows the other genes in pattern p to be compared."
"where r & 1 is the synaptic scaling ratio. the term rk ij ðnþ in eq. (13) corresponds to the effects of ltp/ltd. to ensure that the modeled synaptic scaling is much slower than ltp/ltd, r is chosen such that the term (r -1) x ij (n) is negligible as compared to rk ij ðnþ: thus, the following condition is required:"
"overall, the results of this mammalian data analysis suggest that cogaps is more effective at identifying correct tf activity, although both nmf methods appear equally good at determining the patterns. the reason for the failure in detection of tf activity is unclear, but it is consistent with the previous poor results in detection of biological processes in the yeast compendium data [cit] . in both cases, the a matrix is far larger than the p matrix, and it is possible that solutions remain too smooth in this domain. careful tuning of the nmf methods may create appropriate constraints to guide the methods to useful a matrices as well as p matrices, however such tuning often is counterproductive in biological studies as the methods tend to get fit to the peculiarities of individual data sets. interestingly, bfrm, which was not used here, was also good at finding biological process signatures in the yeast data, so that it is possible that the ability of mcmc methods to more fully explore the posterior probability distribution is important."
"on the basis of extensive laboratory and field experiments and the results obtained from deferent simulations, to improve the bandwidth efficiency, using the coherent modulation methods such as quadrature amplitude modulation (qam) and phase shift keying (psk) is the best approach in underwater operations [cit] . depending on the method for carrier synchronization, phasecoherent systems fall into two categories; differentially coherent and purely phase coherent. the advantage of using differentially coherent detection is the simple carrier recovery which it allows. its disadvantage is performance loss as compared to coherent detection. while bandwidth-efficient methods have successfully been tested on a variety of channels, the real-time systems have mainly been implemented for applications in vertical and very short range channels, where little multipath is observed and the phase stability is good [cit] . in this paper, for the purpose of compensating for the multipath effects and inter-symbol interference (isi), since the persian gulf channel is shallow and horizontal and the qpsk modulation method in comparison with other methods has proper bit error rate despite low bandwidth, we have used the qpsk modulation method, which is purely coherent. the block diagram of the transmitter is shown in figure 1 . this transmitter includes blocks for producing the qpsk symbols. the resulting qpsk symbols are then passed through a pulse shaping filter. the rectangular pulses are not practical to send and require a large bandwidth. hence, we replace them with shaped pulses that convey the same information but use smaller bandwidths and have other good properties such as inter symbol interference rejection. in continuation, we completely explain each of the blocks."
"matrix factorization will provide an indication of the activity of a tf through interpretation of the p and a matrices. using the association of genes (columns of a) with patterns (rows of p) permits estimation of when a tf is active. this can be done through a gene set analysis of the scores associated with a gene in a column of a for a set of genes that are known to be regulated by a tf. specifically, for methods that generate error estimates for the matrix elements, a z-score test can be used, where the z-score for a tf within a pattern p is defined from the measured z-scores of its targets,"
"as traditional nmf techniques do not account for uncertainty information, overfitting of the data can be an issue. in addition, the treatment of all variances as equal raises a potential problem for eukaryotic data. least-squares nmf (lsnmf) introduced new updating rules, effectively replacing the criterion for distance minimization with a minimization of the χ 2 error [cit], given by"
"where y i is the expression for gene i, with the superscript indicating the time point, tf j is the activity of the j th transcriptional regulator with the superscript indicating the time point, r is the total number of regulators, and aff ik is the binding affinity for the k th transcriptional regulator on the i th gene. this is effectively a log-linear model where the transcriptional binding affinity is taken as a measure of the strength of gene activation, and each regulator effectively leads to a multiplicative increase in gene expression."
"in this article, we studied the effects of ltp/ltd and synaptic scaling with respect to a non-trivial baseline attractor of activity. when the baseline is not a fixed point, learning depends upon the instantaneous structure of the attractor. in the case of a chaotic baseline attractor, learning is thus a priori sensitive to initial conditions [cit] . however, it is known that synaptic connections are modified at a much longer time scale than that of membrane dynamics. therefore, ltp/ltd and synaptic scaling act with information on the history of network states fig. 14 bifurcation diagram of the baseline attractor y i (n) for constant x ij ðnþ. the changes in weights are indexed using the bifurcation parameter m. at the critical step m c, a novel line attractor is created (not shown here). for m [ m c, the system is bistable between a chaotic attractor and a line attractor fig. 15 activity of neuron x i (n) in terms of i and time steps n in the case of the line attractor for two different inputs in a and b. the activity level x i (n) is indicated by levels of gray. the activity profile x i (n f ) at the final step of the trial n f is also plotted. initially, neurons have chaotic activity. after the external input, neurons respond in the shape of a persistent ''bump'' centered around the center position of the external input on the line attractor (i.e the structure of the attractor). we have shown that the larger is the averaging window t 0, the smaller is the uncertainty related to the observable y i (n). for states separated over a large time scale t 1, a large enough window t 0 allows for approximation of the baseline attractor deformation. under certain conditions, the averaged internal state y i (n) is modified in the same direction irrespective of the nature of the attractor (fixed point, chaotic, etc.). the change in weights due to ltp/ ltd ultimately deforms the baseline attractor until persistent activity is created due to reverberation."
"similar to section ''distribution of weights'', each external input i always results in an increase in the weights of connections within the group of currently excited neurons (for i 2 c þ ðiþ), a decrease otherwise (for i 2 c à ðiþ). note that because all external inputs are considered identical, p ? m neurons are excited by an external input i. considering the distribution of external inputs at a time step n,"
"when applying nca to microarray data, the relative strength of the tf in regulating its target must be determined. for each gene and each tf, the gene regulation is assumed to be proportional to the binding affinity of the tf to the promoter for a gene. since each gene can be regulated by multiple regulators, the expression of a gene at a time point must be estimated as a combination of the regulation from different factors. for each time point, this is estimated as"
"the bifurcation diagrams of the internal state y i (n), given by microscopic equations (not the mean field approximation), under cases 2 and 3 shown in fig. 6, are shown in fig. 10 . only the effects of ltp/ltd are taken into account in this figure. the microscopic states y i (n) bifurcate as predicted by the macroscopic mean field approximation."
"to understand the formation of these complex memories (parametric [cit], spatial [cit], pair-associated [cit], sequential [cit], assemblies encoding higher cognition processes such as language [cit], etc.), we must consider various biological mechanisms [cit] that can act in combination at different time scales. the change in the dynamics that characterize the creation of assemblies can then be described by formulating canonical models to explain the underlying bifurcations [cit] ."
"chaos in neurons has been reported in squid giant axons [cit], in which spikes need not necessarily to be all-or-none. this motivated the development of the chaotic neural network model [cit] ), which models chaos in neurons. [cit], which originates from the mcculloch-pitts equations [cit] and considers the influence of the refractoriness effect. it can be interpreted as the discretization of the leaky integrate-and-fire neurons [cit], with a reset level after spiking that depends on the state of the neuron before firing. in the following, this chaotic neural network model is used to implement an initial chaotic baseline attractor. because this study focuses on a canonical description of neural activity, rather than the reproduction of quantitative aspects, the use of this model affords good balance between analytical simplicity and realism."
"seawater acts as an acoustic waveguide in which sound waves travel. the sound channel, as a sound waveguide, is a channel with random parameters; however, this doesn't mean that its behavior is unpredictable. the most important characteristic of the seawater is its inhomogeneous nature, which on the whole, can be classified into regular and random varieties. regular variations of sound speed in different layers of water lead to the formation of sound channels and this phenomenon facilitates long distance sound propagation. random inhomogeneities cause the scattering of sound waves and result in sound field fluctuations."
"we use this modified raised cosine filter in the transmitter and receiver. in figure 2, for the assumed sequence of \"00110001101111011\", the output of the i and q"
"the first issue of infinite many solutions can be resolved by considering fig. 2, in which we imagine a distribution in three dimensions that can be described by the standard cartesian system (labelled x, y, z). naturally, any rotation of the standard system will also describe the data, or alternatively a non-orthogonal system (labelled a, b, c) can be used provided a, b, c are non-colinear. ideally these non-orthogonal bases will be related to the biological processes active within the samples. as pca and svd define dominant directions in the data and force orthogonality in the basis vectors along directions of maximum residual variance, each vector would then necessarily combine signals from many processes active in the biological system, confounding inference of multiple regulation in analyses relying on these methods."
"the deformation in the direction i is thus dependent on the cumulative correlation between neurons j and i over t 1 time steps and is scaled by a ''forgetting'' ratio r. because weights reach a limit value due to synaptic scaling, it can be considered that the approximation of eqs. (22) and (23) hold for large steps n. in section ''distribution of weights'', we verify this approximation in the case of a constant external input."
"similar to the case of the constant external input given in section ''creation of persistent activity: case of constant input'', the proper proportion between ltp/ltd and synaptic scaling allows for excitatory connections to maintain persistent activity. however, the system has a continuum of stable states and the chaotic baseline attractor. figure 15 shows that an external input produces a ''bump'' [cit] of activity around the center of the input (the center of the mexican hat function), which is persistent, from the chaotic baseline attractor."
"in this subsection, the external input is assumed to be constant. when there are no external inputs, if the condition described in eq. (24) holds, the synaptic weights are constant, i.e., learning does not happen. the proportion of units activated by an external input is noted p ? . when an external input is applied to the network at n 0, m p ? neurons are excited for y i (n 0 ) [ d, and the other neurons are inhibited (see fig. 3 ). according to eq. (9), self-connections to the excited group (noted c ? ) are strengthened, and connections to the inhibited group (noted c -) are weakened. then, before synaptic scaling is activated, the approximation of eq. (21) shows that y i (n) increases for i 2 c þ and decreases for i 2 c à . therefore, in the next step n 0 ? 1, the changes in weights will be identical. thus, using mathematical induction, it can be concluded that the change in synaptic weight is constant in this case; namely"
"devarajan notes that sparseness is a critical aspect for \"parts-based\" decomposition in nmf, as it provides localized patterns, so that the whole can be reconstructed from these localized features [cit] . this sparseness may need to be enforced by penalizing non-sparse solutions. in addition, the use of cophenetic correlation is discussed as a method to estimate dimensionality, similar to scree plots in svd or pca."
"overall this suggests that the bayesian factorization methods, which naturally include sparseness, or sparse nmf methods should be the first choices for matrix factorization when the goal is trn estimation."
"initially, neurons have chaotic activity. after the external input, neurons respond in the shape of a persistent ''bump'' centered around the center position of the external input. [cit] 6:499-524 517 changes slowly, the basin of attraction of the learnt state can grow momentarily before synaptic scaling acts (see fig. 12 ), and thus promote the activation of this memory on time scales of the size of t 2 . synaptic scaling creates a forgetting effect for variations at a time scale larger than that of t 1 at a rate of r t 1 per t 1 time units. the model described in this article is thus related to a fading memory. this mechanism could promote recently learnt memories at time scales much larger than short-term plasticity. these hypotheses could be tested by observing experimentally if the mechanisms of synaptic scaling are still active long after learning. in the case of an abnormal ratio between the time scales of ltd/ltd and synaptic scaling, this transient increase in synaptic weights is likely to induce imbalance between excitation and inhibition and possibly result in epileptic seizures [cit] ."
"in a second study, they used the rosetta compendium of yeast deletion mutants [cit] and functional annotation to study which methods recovered biological behaviors from the data when coupled to gene set analysis of 15 well-studied processes [cit] . after factorization, signatures of enrichment were identified for each method. in this case bd performed best, identifying 7 terms, with bfrm identifying 5. clustering methods identified 4 terms, while nca, pca, and ica all did more poorly. this may reflect the lack of sparseness as snmf was not used in this study."
"equation (9) is the modified version of the medwin formula that we have presented. in this equation, the de- pendence of sound speed to salinity and temperature is incorporated into the medwin formula. however, in this case, the sound speed versus depth is approximated by a 10 th order polynomial. figure 6 illustrates the output of the medwin formula, the output of the obtained modified formula, and the ex- perimental data. in this figure, the approximation error (3%) for each point is well acceptable."
"in the following, t 1 is the slower time scale at which the weights x ij (n) increase (t 1 ) t 0 ). the following generalized learning is exploited, considering both ltp and ltd [cit] :"
"the change in connectivity over an interval t 1 is of the order of a very small : we consider that for a small number of steps n, y i (n) is approximately time-averaged over the trajectory of a quasi-static dynamical systemfðnþ 4 as follows:"
"as we have seen in the bifurcation analysis, if ltp/ltd act for long enough to create very strong self-excitatory connections, the previously created attractors with the persistent activity are destroyed until there is a change in the baseline attractor. this can also be observed by considering the growth of the basins of attraction for all cases shown in fig. 6 . in the case of tangent bifurcations (case 1, see fig 11a), the basin of attraction of the persistent activity grows as c increases. the loss of the baseline state corresponds to the growth of the basin of the new attractor over the whole phase space. the growth is also observed in the case of generalized period doubling (case 2). however, the basin boundary is irregular. the basins of attraction are not simply connected and ''islands'' grow as learning progresses. under case 3, the basin boundary is fractal around the region where the attractor of persistent activity is located, and becomes regular through a basin boundary metamorphosis [cit] ). when the baseline attractor touches its basin boundary, it loses its stability. thus the basin of the attractor of persistent activity does not simply grow as in case 1, but can disappear suddenly."
"if external inputs change rapidly in a random manner, the combined effects of ltp/ltd and synaptic scaling can result in very different distributions of weights from that in the case described in section ''creation of persistent activity: case of constant input''. if the centers of the external inputs (mexican hat functions) are distributed uniformly over a certain interval, 11 then the resulting synaptic couplings strengths decrease with the distance to the preferred cue [cit] (see fig. 13 ). thus, the network implements ''line'' attractors [cit], which, for example, may encode continuous stimuli in the spatial working memory [cit] ."
"in order to address problems similar to those arising in multicellular gene expression data, new matrix factorization techniques coupled to dimensionality reduction were introduced simultaneously by ourselves in bayesian decomposition (bd) for spectral imaging [cit] and by lee and seung in nonnegative matrix factorization (nmf) for image processing [cit] . both techniques aimed to address the limitations of analytical methods in handling inherently positive data where the natural basis vectors to describe the data were non-orthogonal. the techniques developed to deduce the non-orthogonal basis vectors showed particular potential in inferring multiple regulation for trn inference."
"the previous work on the subject primarily relied on simulations or treated the case of fixed-point attractors [cit] (brunel, 2003 [cit] . few models provide a detailed analytical description of the effects of plasticity from the viewpoint of general attractors [cit] (periodic/chaotic, modulated, or disrupted by noise), and formulate mechanisms that result in the creation of discrete or continuous attractors because of neural plasticity [cit] . the one aspect that all these studies have in common is that the time scale of neural plasticity is a determining factor in the creation of cell assemblies. intuitively, the mechanisms of plasticity should be tuned properly so that the characteristics of the input patterns can be learnt as opposed to the variations in the patterns due to the chaotic baseline attractor, modulation, or noise. neural systems must indeed be able to manage these time scales adequately."
"the encoder accepts the sequence of the input binary data. it has two outputs; in-phase, i, and quadrature, q. for each distinct pair of input binary data a unique combination of i    and"
"while matrix factorization will provide insight into multiple regulation by identifying coordinated patterns that can be linked to tf activity, it does not directly solve trn structure. this relies on more complicated modeling approaches that balance adherence to the underlying biology against computational complexity and parameter explosion. a review of the methods utilized for trn prediction from expression data and prior knowledge is beyond the scope of this work, however karlebach and shamir summarized widely-used techniques previously [cit] ."
"we also discussed in this article how novel attractors of persistent activity are created when the baseline either is periodic or chaotic. in the case of a constant external input, the novel attractor encoding for persistent activity can appear via tangent bifurcations (case 1), period-doubling bifurcations (case 2), and tangent bifurcations of the second iterates, or crises (case 3)."
"we have shown that under a uniform distribution of external inputs, two types of memories (discrete and continuum) can result from the combined effects of ltp/ltd and synaptic scaling. the relation between the time scales of ltp/ltd and synaptic scaling could thus be tuned in normal conditions to promote a specific type of attractor. we have shown that if the frequency of the change in external inputs f c is small compared with that for ltp/ ltd, discrete attractors are created. on the contrary, if f c is very large, a uniform distribution of external inputs is learnt by means of a line attractor. in the intermediate case, the continuous attractor breaks down into a set of discrete attractors. trajectories drift to the nearest attractor. this case is more realistic than line attractors since it does not require fine tuning of the system parameters."
"to explain these observations, various models have been proposed. some recent models focus on the formation of assemblies rather than the characterization of their existing states [cit] (brunel, 2003 [cit] ). long-term potentiation and depression (ltp/ltd) have been identified as important biological mechanisms underlying the creation of assemblies [cit] . ltp/ltd have been shown to lead the network to a bistable regime [cit] ) (baseline and persistent activity) through a saddle-node bifurcation [cit] . the formation of pair-associative memory and prospective activity [cit] ) could thus be modeled by considering the effects of plasticity on the dynamics of the network."
"there are three primary factors that complicate trn prediction in multicellular organisms. first, gene reuse in multiple biological processes is dramatically increased in higher organisms. this leads to multiple regulation of genes by multiple tfs, which introduces mathematical complexity to the determination of the tf responsible for a change in expression of a target. second, many genes are regulated post-transcriptionally, either through translational regulation or post-translational modification. for instance, many tfs require post-translational modification or cofactor binding to initiate transcription. third, epigenetics, such as silencing by chromatin formation or dna methylation, play a far larger role in multicellular systems than in prokaryotes and yeast. this substantially complicates the relationship between tf activity and target expression."
"we have reviewed the principal methods and their extensions. sparseness appears to be a recurring theme for obtaining good results, whether in identifying coregulation of genes as desired for trn estimation, or for gene set analysis to identify biological processes active in the system under study. bayesian methods implement sparseness through a prior on the potential a and p matrices, while nmf methods tend to introduce an additional matrix in the decomposition or to penalize non-sparse solutions. either approach appears to improve recovery of coregulation, although the bayesian methods appear somewhat superior over all, perhaps because they explore the parameter space more fully."
"where a can be viewed as factor loadings for latent factors p [cit] . the h matrix provides known covariates in the data, and the mean vector, µ, provides a gene specific term that adjusts all genes to the same level. the patterns then are those needed after accounting for mean behavior and covariates. like bd and cogaps, bfrm seeks to minimize structure in a and p. bfrm also addresses the issue of the number of patterns through an evolutionary stochastic search. the algorithm attempts to increase the number of patterns by thresholding the probability of inclusion of a new factor. the model is refit with the additional pattern with retention of the additional dimension if there is an improvement according to the criterion chosen. evolution ceases when no additional factors are accepted. the bfrm software also allows running without this evolution."
"the period of hamming window is 2t that t is the period of each symbol. this window has 99.96% of its energy in the main lobe, with side lobes of over 20 db down from the peak [cit] . thus impulse response of filters is corrected:"
"where a represents assignment of genes to patterns and p the patterns as in eqn. 1. for the case of linear ica, the estimation of p can be formulated as"
"moreover, the attractor of persistent activity can be more complex, 13 or irregular, than the baseline activity. this attractor can be either periodic or chaotic. it is known that cortical neurons do not fire according to a poisson process [cit] and it has been argued that neural activity can be chaotic [cit] . in this case, the transition from baseline to persistent activity can modify the dynamical regime from low to higher complexity [cit] . the increase in variability of the interspike interval observed during mnemonic activity in vivo [cit] and at the onset of a stimulus [cit] could be explained by a change in dynamics. we have proposed in the article a putative mechanism for these observations: the transition of the mean field dynamics to an attractor with higher or lower complexity. in a chaotic active state, the network generates reproducible patterns of activity, as observed in cortical networks [cit] . here, these patterns can result from the intrinsic dynamics in local populations of neurons. chaotic patterns of activity can be exploited to achieve motor tasks such as motion control [cit] ) and tracking [cit] ."
"the second issue of dimensionality estimation remains problematic. numerous methods that have been developed in other fields have had only limited success in applications to gene expression data. both ad hoc methods [cit] and formal methods [cit] have been proposed and applied, however a definitive method remains elusive. many of these dimensionality estimates rely on strong assumptions about the error model in y, which are inapplicable to the strongly correlated and deeply structured microarray data. the best approach may be to try many dimensionalities and look for robust patterns in the results, similar to robust clustering methods."
"on results for the channel hose speed profile was shown in figure 4 are prech ray in 2 dith cases of multi-path propagation ha ots) and very fine sa es, the strengths of all rays constitu obability of err figure 15 shows the delay spread of ea in this section, the simulati w sented. the simulated channel characteristics are given in table 1 . in this case, the transmitter and receiver use the qpsk modulation with a bandwidth of 5 khz and a carrier frequency of 27 khz. also, the transmitter and receiver are placed at a depth of 5 m and 70 m from the surface, respectively. figure 11 illustrates the constellation diagram of qpsk symbols and the power spectrum density of the band-pass signal with a 27 khz carrier frequency, before the transmission through the channel. the constellation diagram of the received qpsk symbols after entering the fading channel is obtained according to figure 12 . as shown, the amount of inter symbol interference is high in this figure. also, figure 13 illustrates the power spectrum density of the transmitted signal in the channel for each of the special paths. as expected, in the rsrbr path (figure 13(d) ), the largest attenuation takes place, and the signal in this path experiences 23db loss. figure 14 shows the ensemble of channel response, for the fifth path and the remaining paths after it the signal is strongly attenuated, as can be seen from comparing figure 14(a) and 14(b) . mensional views. for the 8 path, it is about 0.07 sec in comparison with the first (direct path)."
"these studies suggest that some consciousness related information could be encoded in the firing rates of neuronal groups, or cell assemblies [cit] . however, the characterization of these cell assemblies remains difficult [cit], and requires reconsideration of their original definition in order to understand them within the framework of dynamical systems theory [cit] ."
"a second bayesian approach to matrix factorization, bayesian factor regression modeling (bfrm) [cit], although it had been applied to microarray data in clinical studies prior to this publication. bfrm induces sparse solutions by removing the mean behaviors in the data, which tends to highlight differences between samples."
"note that if the frequency of changes in the external inputs is small, the average weight change does not reach its expected value during the window t 1 . in this case, the continuous attractor usually decomposes into a set of discrete attractors. we consider in the following a case that the deviation around the expected value is very small. when synaptic scaling is taken into account, i.e., n ) t 1, the expected value of the weights for a time step n is given as follows (see eq. (11)):"
"to calculate the loss due to wave scattering at the surface, we use the probability density function of the gaussian normal distribution for the surface displacement variable. in the simulation, the mean surface reflection coefficient is calculated from (11) [cit] ."
"(1) recorded persistent activity has low levels of irregular firing [cit], and shows timedependency [cit] . moreover, it has been argued that baseline activity may not be a fixedpoint attractor, but a chaotic one as observed in the olfactory bulb notably [cit], and squid giant axons [cit], modulated by some frequency; theta oscillations in the hippocampus [cit], gamma waves [cit], and further perturbation by noise [cit] ). (2) single neurons can reach not only one level as described using fixed point attractor models, but several levels of persistent activity [cit] . in the management of analog rather than discrete states, some memories can be better understood as ''line'' attractors [cit], which involve the encoding of a continuum of states."
"to overcome the difficulties of time-varying multipath dispersion, the design of commercially available underwater acoustic communication systems has relied so far mostly on the use of noncoherent modulation techniques and signaling methods which provide relatively low data throughput. recently, phase-coherent modulation techniques, together with array processing for exploitation of spatial multipath diversity, have been shown to provide a feasible means for a more efficient use of the underwater acoustic channel bandwidth. these advancements are expected to result in a new generation of underwater communication systems, with at least an order of magnitude increase in data throughput [cit] ."
"the fundamental problem of factoring a matrix to find structure to explain the physical world recurs in numerous fields, which has led to the development of similar methods under many names. following the broader history in the development of matrix factorization techniques, the first methods that were widely used in microarray studies included the standard statistical techniques of singular value decomposition (svd) and principal component analysis (pca) [cit], showing that this significantly improved inference on the yeast cell cycle [cit] . later studies demonstrated the value of bd when applied to human patient data [cit], and we developed an open-source algorithm, cogaps, linked to r to simplify applications [cit] . while bd can be considered a form of independent component analysis (ica), it is driven to inherently sparse solutions, which appears important for inference on expression data."
over the last few years the interest in nmf and other factorization methods for high-throughput biological data has increased. a number of extensions have been made to the basic techniques in order to address issues in biological data that can impact use in trn estimation.
"because the changes in the synaptic efficacies k ij ðnþ depend on y i (n), the time average over a window t 0, it is necessary to describe the evolution of y i (n). however, the interdependence of y i 's is complex, and thus, they cannot simply be explicated without considering some approximations."
"there are a number of techniques closely related to matrix factorization that have been applied to expression data. blind source separation (bss) was developed to isolate the signal coming from a single source within a background of multiple sources, such as the speech of a single individual in a crowded room. [cit] . biclustering is a technique focused on identifying subsets of similar behavior within a matrix, where these subsets may overlap. unlike matrix factorization, it does not lead to a decomposition into two matrices that can reconstruct the fundamental fitting that occurs in the techniques focused on here are the derivation of a and p matrices which combine to form a denoised model, x, of the measured data. x comprises n rows of genes and m columns of samples."
"auto-associative networks [cit] allow for the modeling of cell assemblies as local minima of a lyapunov function (or wells with lower energy) rather than modeling them directly by detailing synaptic connectivity. this is possible when the weight matrix in a fully connected network is symmetric. these associative models are well-understood [cit], and the assemblies created have simply fixed point attractors, which can be recalled by providing the corresponding (incomplete) external inputs."
"these three complications require new approaches and sometimes new data sources when building trns. the multiple regulation issue is being addressed through matrix factorization methods, which we will focus on in this review. the post-transcriptional regulation of genes leads to several issues. perhaps most critically, it leads to many genes not being under transcriptional control, leading to substantial variance in transcript levels for these genes independent of protein level changes and functional consequences. this suggests a need to incorporate estimates of random variability in expression, which can be incorporated into individual matrix factorization techniques. the epigenetic factors influencing tfbs site access and transcriptional availability of genes requires techniques that limit the strength of priors from tfbs data to insure accurate inference in multicellular systems. in addition, integration of data measurements, such as methylation status of tfbs elements, can provide additional information to guide trn estimation from expression data."
we propose to analyze the dynamics associated with the creation of cell assemblies for a general case of a chaotic baseline attractor by implementing ltp/ltd and synaptic scaling and taking into account the different time scales of neural plasticity. the combined effects of these two plasticity principles are shown to result in the creation of both discrete and continuous (periodic/chaotic) attractors that coexist with the (periodic/chaotic) baseline in accordance with the relation between the characteristic time of neural plasticity and the rate of change of the external inputs. we thus illustrate the dependence of memory formation on the relation between the time scales of plasticity phenomena and the external stimuli.
"convolution functions map atoms to matrix elements allowing preferred correlations between matrix elements to increase in probability. through the convolutions, a set of values (e.g., a) can be constructed from a family of measures, φ (the atoms), using kernels, k. in the simplest case, an atom simply maps to a single matrix element. the probability for each combination of a and p is determined from bayes rule,"
"in the second block, data is encrypted with rc5 algorithm. rc5 is a symmetric block cipher designed to be suitable for both software and hardware implementation. it is a parameterized algorithm, with a variable block size, a variable number of rounds and a variable-length key. this provides the opportunity for great flexibility in both performance characteristics and the level of security [cit] . in this paper, block size has been selected 32, number of rounds are 16 and key length is 10."
"the effects of plasticity rules are summarized in fig. 1 . the external inputs used for the simulations are mexican hat functions of fixed amplitude, which simultaneously excite and inhibit different populations of neurons. this choice is motivated by the observation of lateral inhibition in the cortex. the external inputs are sufficiently strong to force the internal states y i (n) over the threshold d. the shapes of the stimuli are not an important factor in the current analysis, and similar results would be obtained with gaussian, sinusoidal, or even heaviside-step functions. the center of the mexican hat of each external input varies randomly (uniformly distributed) over the population of neurons."
"homeostatic plasticity [cit] may act in combination with ltp/ltd and limit the growth of weights. this mechanism could explain the selectivity of neurons, i.e., the shape of the tuning curves in the visual cortex [cit] . further, this mechanism could stabilize synaptic connectivity that allows the formation of line attractors, and avoid drifts due to small deviations in this distribution of weights [cit] . the combined effects of ltp/ltd and synaptic scaling could thus explain both the continuum of memories and discrete memories."
"ts x i ðnþ; t s ( t 0 . indeed, the terms x i (n) and x i (n -t s ) average almost identical trajectories, and therefore do not describe the attractors of two different dynamical systems (perturbedgðnþ and unperturbedgðn à t 1 þ), but the difference between consecutive steps of the same attractor. in that case, these consecutive steps could be at a distance that is equal to the size of the attractor. that the change in weights remains small; thus, the approximation remains true for all n. note that if ''hard'' types bifurcations or interior crises [cit] occur during the route to chaos, then the approximation may eventually be wrong for some bifurcation points, but true otherwise. we therefore consider only the ''smooth'' deformation of the baseline."
"bandwidth-efficient digital underwater acoustic communications can be achieved by employing equalization of quadrature amplitude modulation (qam) and phase shift keying (psk) signals. the receiver structure that has been found useful in many applications is a multichannel decision-feedback equalizer. due to the nature of the propagation channel, the required signal processing is often prohibitively complex. reduction in computational complexity can be achieved by using efficient adaptive algorithms, such as the low-complexity lms algorithms with improved tracking properties [cit], and by reducing the number of adaptively adjusted receiver parameters [cit] ."
"is the impulse response of the low pass filter. the filtered signals are then multiplied by a carrier frequency, added and transmitted through the persian gulf underwater acoustic channel."
"so that we need to find a matrix w (the unmixing matrix), such that the rows of matrixp are statistically independent though not orthogonal. the process of finding the unmixing matrix can be performed by different algorithms, based on different metrics of statistical independence. pournara and wernisch provided a thorough review of ica and other factor analysis approaches in trn estimation [cit] ."
"a number of recent studies support hebb's hypothesis [cit] ) that persistent activity in the working memory could be the result of reverberatory excitation, i.e. selfsustained excitation due to feedback connections, in local neuronal populations [cit] ."
" is assigned. we consider each qpsk symbol as a complex number, i + jq, whose real and imaginary components are the outputs of the in-phase and quadrature channels respectively, to describe those four points separated in the complex plane  . the encoded data stream of i and q is then used to modulate a sequence of impulses in which are transmitted every signaling period; t. to limit their bandwidth such modulated sequences are then low-pass filtered by lpfs. the same low pass filters are applied at the receiver. the in-phase and quadrature signals at the output of low pass filter are:"
"moreover, to determine the exact nature of the bifurcations in the system and to verify whether the transversality conditions are satisfied, the following two equations are also solved separately, using the given sets of parameters c and l h :"
"here we review the basics of matrix factorization methods applied to gene expression data, focusing on those methods that have been applied widely to gene expression data."
"kossenkov and ochs generated a simulated yeast cell cycle data set with errors based on real data and compared a wide variety of methods in terms of recovery of known coregulation in the background of multiple regulation [cit] . bd and snmf performed best, with aucs of 0.98 and 0.94 respectively. nmf, lsnmf, and ica also performed well, while pca and clustering techniques performed relatively poorly."
"transcriptional regulatory networks (trns) provide a method for cells to reprogram their functions as needed for survival or multicellular interactions. these networks comprise transcriptional regulators (or transcription factors, tfs) and their target genes. as one of these genes may encode an additional tf, a propagation of primary, secondary, tertiary, etc. transcription can occur, with a branching out from the activation of an initial tf to a growing set of tfs and targets. overall, the tfs so induced form the trn."
"for this work, we reanalyzed gastrointestinal stromal tumor (gist) cell line data using svd, ica, two nmf methods, and bd/cogaps, since this data had a number of tf activities validated by direct phosphoprotein measurements [cit] . in brief, the data comprised triplicate growth of gist cells in the presence of imatinib, the targeted tyrosine kinase inhibitor of the kit receptor tyrosine kinase. imatinib, also known as gleevec, is used therapeutically in gist patients, and the inhibition of kit phosphorylation was validated experimentally in the cell line. cells were harvested at nine time points and agilent whole human genome microarrays were run on each of the three samples at each time point. data was processed to provide mean and standard deviations for each gene at each time point. [cit] .4 were retained, providing a y matrix of 1363 genes (rows) by 9 samples (columns) together with standard deviations on each element."
"the network considered in the following is a local group of m fully connected neurons, possibly as a ''hypercolumn'' [cit] . each unit (or neuron) in the model is the sum of inhibitory interneurons and excitatory pyramidal cells with synapses that share the same characteristic time scale, 1 so that synaptic connections can be either positive or negative."
"the initial baseline attractor can be periodic or chaotic (see fig. 2 for examples of temporal neuronal activity). when the connectivity of the system is changed slowly, ltp/ltd act to deform the baseline attractor. in the following, we approximate the ''smooth'' deformation of the attractor by quantifying the changes in the time-averaged internal states y i (n) to extract the dependence of y i (n) on system parameters. these approximations hold for the generalized ltp/ ltd rule described in eq. (7), but only under particular conditions of the scaling parameters. these approximations can easily be adapted to describe the ''smooth'' deformation of the baseline attractor due to external inputs."
"to understand the creation of persistent activity under the effect of ltp/ltd, the set of bifurcation points of the mean field approximation are drawn in the parameter space fc; l h g."
"matrix factorization can provide an important insight for trn estimation. the patterns will indicate when corresponding genes are active, and as shown in sections iv.b and v.a, the genes associated with patterns can provide inference on tf activity. since the factorization methods can be designed to appropriately utilize variance estimates and epigenetic measurements, the tf activity estimates should be more robust."
"second, for expression data, it applies a positive mapping from the inherently positive atomic domain to a and p limiting the solutions to positive matrices. third, dimensionality reduction is typically used to limit the number of parameter estimates needed, so that p min(n, m ). in order to determine how to place and size atoms within the atomic domain, a markov chain monte carlo (mcmc) procedure is used. atoms are created ex vacuo according to the prior, and atoms can be resampled, destroyed, moved, or have flux moved to neighboring atoms (see [cit] for details). using"
"if the external inputs have a low frequency f i (compared to the inverse of the time scale t 0 of ltp/ltd), their time average a i over t 0 may be small. when the increase in y i (n) due to external inputs is smaller than d, i.e., the ltp/ ltd threshold, there are no changes in synaptic connectivity. in this case, the ''perceived'' external input is too weak, or varies too slowly to induce any change in the baseline attractor, i.e., there is no learning."
"to avoid the disadvantages of the side lobes and to reduce the isi we transmit data with shaped pulses instead of rectangular pulses. therefore, the obtained qpsk symbols are passed through a modified raisedcosine filters (lpfs) with a roll-off factor 1   and impulse response . the transfer function of the baseband channel has the form:"
"in this review, we focus on the development of matrix factorization in the analysis of microarray data. we highlight particularly the value of these methods to trn prediction and address the value of including error modeling within the analyses."
creation of persistent activity: case of rapidly changing external inputs it is considered now that external inputs vary rapidly compared to the changes in synaptic efficacies.
"as with bd and cogaps, nmf provides an inherent reduction in dimensionality. in an nmf simulation, random matrices a and p are initialized according to some scheme, such as from a uniform distribution u [cit] . the matrices are updated iteratively using"
"network component analysis (nca) uses information on the binding of tfs to dna to reduce the possible a and p matrices [cit] . essentially, a two layer network is created, with one layer populated by transcriptional regulators and the other by their gene targets. edges then connect each tf to the genes they regulate."
"determination of transcriptional regulatory networks (trns) provides important insight into numerous biological processes driven by cellular reprogramming. in mammals, evolution has driven high levels of gene reuse, leading to multiple regulation of genes, which greatly complicates trn estimation. one potential path forward is to integrate matrix factorization methods, which have been developed specifically to address multiple regulation, into trn estimation."
"on the other hand, external inputs repeated at a higher frequency may change synaptic connectivity until deforming the baseline attractor such that, even if no external inputs are applied in future trials, y i (n) become larger than the threshold d of ltp/ltd. in this case, synaptic plasticity is activated even in the absence of an external input and weights are strongly increased, which induces a high activation of the neurons. the condition under which this does not occur is expressed as follows:"
"one method closely related to matrix factorization is independent component analysis (ica). like typical applications of pca to microarray data, ica performs matrix decomposition by projecting the data onto a lower dimensional space, using statistical independence between components rather than orthogonality. since the observed microarray signals are a result of a mixture of underlying biological processes, the factorization of the data matrix, y, can be expressed as"
"in the past five to 10 years, there has been a growing interest in the underwater acoustic communications in various application areas such as remote control in off-shore oil industry, pollution monitoring in environmental systems, collection of scientific data recorded at ocean-bottom stations, speech transmission between divers, and mapping of the ocean floor for detection of objects, as well as for the discovery of new resources [cit] . wireless underwater communications can be established by transmission of acoustic waves but they are not the only means for wireless transmission of signals under water. radio waves that will propagate any distance through conductive sea water are the extra low frequency ones (30 hz -300 hz) which require large antenna and high transmitter powers. optical waves do not suffer so much from attenuation, but they are affected by scattering. consequently, transmission of optical signals requires high precision in pointing the narrow laser beams. while the laser technology is still being perfected for practical use, acoustic waves remain the single best solution for communicating under water in applications where tethering is unacceptable [cit] ."
"the third issue of flexibility of solutions to exchange of flux between the a and p matrices is generally solved by normalization of either a column of a or a row of p. either is a valid mathematical solution, and we have argued above for normalization of rows of p to increase biological interpretability. however, the best choice will depend on the particular application."
"we have seen in this section that ltp/ltd alone induce a bifurcation that allows for persistent activity. if the effects of long-term plasticity are not controlled, the basin grows until the baseline attractor is destroyed. synaptic scaling is thus necessary to limit the growth of weights and settle the regime of the network in the bistable region."
"1 it is important to note that phenomena that are related to spike timing (such as synchrony of spikes) are not taken into account here; however, they may play an important role in cognition studies (a temporal-coding hypothesis). synchronous firing may disrupt persistent activity [cit] because of the different time scales for the receptors (primarily ampa and gaba a ), and create oscillations [cit] . all receptors are assumed to have an identical time scale for simplicity; thus only asynchronous persistent activity is considered in the following. 2 [cit] . here, the average activity over the population of neurons, which is given by the mean field approximation, is chaotic (see appendix 4)."
"in the performed simulation, the number of channel paths varies as a multiple of four. in the considered model, for the wave propagation from the transmitter to the receiver, we use four eigen rays which are shown in figure 8 . the transmitted wave either follows one of the four eigen rays or a multiple of them. in the case of multiple reflections, after several reflections, the wave reaches the receiver in one of the four ways shown in figure 8 ."
"compared to m water acoustic channels are generally band-limited and so involve relatively low data rates. despite this, the channel distortions commonly encountered require complex signal processing in the receiver, resulting in high computational loads and the need for power-hungry, high-speed hardware. consequently, the design of a computationally-efficient receiver is crucial for practical implementations. adaptive signal processing, in the form of adaptive equalization, has established itself as an integral part of nearly every digital communication receiver in use today. digital acoustic communication is limited by severe isi associated with shallow underwater channels. the isi is caused by multipath propagation resulting from surface and bottom reflections [cit] . to overcome the effects of inter-symbol interference; an adaptive equalization method employing a mean square error (mse) criterion is introduced. computer simulations are carried out to verify the effectiveness of the equalization scheme for high data rate communication."
"witten, tibshirani, and hastie provided a generalized penalized matrix decomposition framework [cit] . this framework allows specification to a number of specific forms, including sparse svd and non-negative sparse coding, similar to snmf. an r package is now available for implementing many nmf approaches [cit] ."
"? and s i -are not considered here). in this case, it is shown, in appendix 3, that the approximation given by eq. (21) can be used to arrive at the following approximation:"
"in the first block of transmitter, acoustic signal is compressed with lpc-10 algorithm. sampling frequency is 8 khz and for any sample 8 bit is appropriated. therefore sampling bit rate is 64 kbps. lpc-10 algorithm reduce bit rate to 2.4 kbps, consequently small bandwidth is needed and frequency attenuation is well reduced [cit] ."
the time-averaged internal states are thus scaled at rate r t 1 for t 1 steps towards the limit value y i ð1þ; which is given as follows when the external input is constant:
"the remainder of this article is organized as follows. first, we quantify the ''smooth'' deformation of the chaotic baseline attractor induced by learning, by approximating the combined effects of ltp/ltd and synaptic scaling in the case of non-trivial trajectories of activity. the conditions under which the baseline activity does not reach the ltp/ltd threshold (i.e., the system does not learn the baseline attractor characteristics when no external inputs are applied) are given. then, the bifurcations induced due to this change in neural connectivity are studied. these bifurcations can be involved in the creation of persistent activity (tangent, generalized period-doubling [cit], and crisis bifurcations when the baseline is a chaotic attractor). finally, the relationship between the time scales of neural plasticity and the rate of the external inputs is clarified so as to understand the conditions under which discrete or continuous attractors can be created."
"trn prediction often relies on two additional pieces of information beyond gene expression data. the first piece of information is tfbs data and prediction, though as noted above this should not provide too strong a prior in multicellular systems. tfbs data can be generated through chipchip measurements, and coupling of this data with expression data allowed modeling of the yeast cell cycle [cit] . more recently tfbs data has been generated through chip-seq measurements, where the immunoprecipitation pulldown is directly sequenced. the second piece of information is the result of text mining, with a focus on identification of targets of tfs as reported in the literature. while all these data sources can provide direct evidence of regulation, context often plays a role through epigenetics, so care must be taken in the use of this information as well."
"the matrix factorization methods presented here have a number of tunable parameters making complete comparison difficult. comparisons have been made through simulations, where the ground truth is well-defined but errors do not reflect likely strong correlations between genes and processes, and through analysis of data, where ground truth is generally not known but data is realistic. we present a brief summary of previous comparisons and then provide an example from our work in cancer, where several tfs were validated by western blots using phosphoantibodies."
"a number of reviews have focused on comparison of different nmf methods [cit], the ability of nmf to recover biologically meaningful patterns [cit], different methods for identifying coregulation [cit], and methods for gene set analysis [cit] ."
"where r indexes the genes that are targets of tf t . this can then be compared to a random sample of genes from pattern p to determine the significance of the z-score and the probability of activity of the tf. the amplitude of the corresponding row of p then gives an indication of the level of activity of the tf (see [cit] for an example in cancer). this statistic was used to generate estimates in section iv.b. alternative statistics can be generated from the a and p matrices for cases where only mean estimates of the element values are obtained. at the simplest level, these can include threshold values based on targets, although analyses based on sparse components or binary models are likely to be more robust."
"during the learning, and change at a certain frequency f c, i.e., the external input changes every t c steps, with t c ) t 0 . the average external input per window t 0 at step n is given as a i (n) & a i (n)f i, and is the ''perceived'' external input. therefore, here we consider external inputs that change at an intermediate time scale between those of ltp/ltd and synaptic scaling."
"when these conditions are verified for a limited number of steps n, i.e., nk ( \" x; and \" xp ( 1 (and g )"
"there are two approaches to incorporate improved inference of regulation from matrix factorization into trn estimation. we refer to these as reverse inference and forward inference. the former term refers to estimation of tf activity from the behavior of known tf targets, while the latter refers to inference of tf targets given tf activity. in either case, methods that extend the known targets (see below) can be used to extend the inferences."
"we then looked at the specific signaling process readouts used in the original publication. first, mapk and pi3k signaling were downregulated due to imatinib suppression of kit signaling, which appears as upregulation of the tfs downstream of mapk and pi3k in the falling pattern. second, the transient pattern showed strong upregulation of p53. upregulation of p53 was validated through western blots of the dna damage response proteins and p53 across the time series. third, the rising pattern showed upregulation of elk1 and stat3, which was validated by phosphoprotein antibodies as well. to visualize the results, we converted a permutationbased z-score statistic (see section v.a) for each tf based on its known transfac targets to a strength of activity, with high activity reflected as bright yellow and low activity as dark blue. the results for cogaps and the nmf methods are presented in fig. 4 . it is clear that in all cases the nmf methods do not provide either a consistent view of downstream activity (i.e., for the falling pattern) nor accurate prediction of individual tf activity. on the other hand, cogaps provides consistent estimation of the full network readout downstream of kit (i.e,, sp1, elk1, myc, e2f1, ap1, creb high and foxo low), as well as correct identification of tf activity in the other two patterns."
"given that t 0 is sufficiently large for x i (n) to approximately describe the entire attractor of the systemgðnþ; under ergodicity, it can be considered that x i (n) depends on the structure of the attractor of the quasi-static dynamical systemgðnþ. then, by considering two dynamical systems gðnþ andgðn à t 1 þ; we make the following assumption:"
"bd applies several approaches to identify those a and p matrices that best explain the data y for gene expression data. first, it applies a sparseness criterion through use of an atomic prior that is penalized for the addition of structure [cit] ."
"in the future, it will be possible to provide additional data to guide matrix factorization and trn inference. methylation data can provide prior distributions on the probability that a given tf target can avoid upregulation by an active tf, so that epigenetically silenced genes do not adversely affect trn estimation. identification of alternative splice variants, as from rna-seq, can aid in identifying alternative tf isoforms that may have different gene targets, deconvolving potentially confusing cases where the tf targets change unexpectedly due to presently unmeasured regulatory shifts."
"studying the interactions between the time scales of neural plasticity is important for understanding the creation of complex memories. spatial memory is likely to be a compromise between discrete and continuum attractors; sequential memory, a compromise between stability and instability of attractors [cit] ."
"this paper is organized as follows. in the second section, transmitter is designed. because of the strong frequency attenuation, channel bandwidth is limited, therefore in transmitter, we have used lpc-10 (linear prediction coder-10) algorithm to compress speech signal. after that, for more safety in transmission, rc5 (rivest cipher) cryptography algorithm has been used to encrypt data. then, for reduction of bit error rate (ber), channel coding is used. in the last section of transmission, data is modulated. in the third section, the model for the sound channel is provided. in the fourth section, the block diagram of the receiver and its performance is discussed."
"it is observed in vivo that ltp/ltd act on a slower time scale than the time scale of neuronal dynamics. when the baseline is irregular (chaotic, modulated, noisy, etc.), the above mentioned property is fundamental. if ltp/ltd depend on an average over the window t 0 of internal states, then the change in weights depends on the following properties:"
"one complexity is the likely activity of multiple tfs within any pattern. in the case where the activity of the tfs differ in different patterns, the behavior of the targets for a tf across all patterns can be used to isolate the genes that are tied to the specific tf. if the activity of a subset of tfs agree across all patterns, then it is necessary to have additional information to infer the probability that one of the active tfs is responsible. this would be an excellent use of comparative tfbs information, as in this focused case it is likely that the additional presence of a tfbs could resolve the potential multiple driving tfs. however, in this case the tfbs data used for forward inference must be sufficiently independent to ensure the identifiability of the inference algorithm."
"although the statistical independence requirements of ica are not as strict as the orthogonality requirements of svd and pca, the assumption of independence between the underlying processes may not be fully justified in most microarray data due to multiple regulation and coordinated activation of biological processes. [cit] for spectral imaging [cit], and an open-source approach linked to r/ [cit] ."
"and the algorithm proceeds as with nmf to find a local maximum in probability according to the minimization of eqn. 8. all nmf methods fail to explore the posterior probability distribution, due to the inability to escape local maxima. the approach to this problem has been to routinely begin with 50 to 200 different initial random a and p matrices, then to look for the solution which provides the best fit to the data, as measured by eqns. 7 or 8 [cit] . alternatively, robust solutions can be looked for within the factorizations, providing an indication of reliable patterns (i.e., metagenes) within the data. however, in our experience, the metagenes obtained from reasonably complex data sets can vary in terms of their χ 2 fit to the data by two orders of magnitude. as such, care must be taken to make sure that an adequate number of simulations using an nmf method have been attempted before interpreting the results."
"the considered local group of neurons also receive longrange connections from other neuronal groups. these external inputs a i are of short duration, i.e., the term a i is non-null only for a few steps n. in the following, we assume that external inputs have a time scale approximately equal to the neural dynamics, are repeated many times at a frequency f i )"
"where at includes all potential a matrices. by requiring t to be diagonal, a and p are unique up to a scaling factor. diagonality of t can be guaranteed if 1) a is full column rank, 2) removal of a tf yields a network where a is still full column rank, and 3) p is full row rank. criterion 3 demands that a set of tfs be considered linearly independent, which is reasonable biologically. the solutions, a and p, are determined by minimizing eqn. 7, just as for nmf. as with lsnmf, this could be easily extended to gene and array specific errors by inclusion of specific error terms. as with bd and cogaps, the rows of p are normalized so that each row provides the average effect of a regulator."
"in this subsection, all the hardware elements that make up the entire cell will be exposed. the fms, in its current configuration, is formed by the following elements: in the top of the automation and supervision system, a master terminal unit (mtu) consisting of a pc-based scada system communicates with the plc of station 1 (plc-st1 in figure 10 ). this plc plays the role of master plc of the fieldbus profibus. so, the remaining plc in the network (plc-st7, plc-st8, and plc-est in the same figure) act as slaves. to this aim, an ethernet communication module, cp 343-1, is directly coupled to such master plc through its internal bus. details about the mtu-plc communication are developed in section 3.2."
"for the training of operators without the need of direct connection to the process. the use of external databases allows access both from the monitoring environment and from other applications through standard languages, which results in the most convenient way to integrate industrial computer systems. structured query language (sql) is one of the most widespread languages and is adopted by most manufacturers and suppliers of industrial software. in this work, the process of archiving information of interest was solved through visual basic (vb) scripts that are generated by variable value change events."
"as previously commented upon, the cp module is responsible for establishing such a connection, both for the programming of the first station plc and for communication with the scada system. the cp includes a rj45 port to accommodate an ethernet wire. another capability is a customizable web page hosted by this module that allows web-enabled diagnostics via hypertext transfer protocol (http) clients. it should be noted that in the present work, this capability has not been exploited."
d dev is the development dataset used to determine the decision threshold by using the proposed anti-spoofing system. more details about the algorithm are given in algorithm 1.
"as future work, we are planning to test our proposed anti-spoofing system under different scenarios to enhance its performance. we will also implement our system with other biometric modalities such as face, gait, hand, etc., as a multimodal anti-spoofing system to enhance the performance of age verification systems. we are also planning to test the compatibility of the proposed anti-spoofing system with other biometric traits for age verification in real time."
"the automatic operation mode of the fms is programmed in the monitoring fc (fc2). this mode is similar in the four stations, although the plc program of station 1 (profibus master) includes some additional functions to control the other bus devices. this fc contains the following secondary fc:"
"the plc used for the automation of each station is located on the electrical panel mounted on a standardized deutsches institut für normung (din) rail. the plc incorporates 16 digital i/o, both working and load memory, profibus and mpi interface, and integrated counters and timers. each slave plc executes control tasks over the corresponding station and exchanges data with the others via profibus. in addition, it must be noted that each plc acts as an interface to link the s&a with the fieldbus. in the case of station 1, s&a are connected directly to the master plc."
"using targeted attacks and clothing impersonation can trick a gait recognition system. no artificial detection exists for such kinds of attacks, and this is especially extremely challenging."
"the plc used for the automation of each station is located on the electrical panel mounted on a standardized deutsches institut für normung (din) rail. the plc incorporates 16 digital i/o, both in the top of the automation and supervision system, a master terminal unit (mtu) consisting of a pc-based scada system communicates with the plc of station 1 (plc-st1 in figure 10 ). this plc plays the role of master plc of the fieldbus profibus. so, the remaining plc in the network (plc-st7, plc-st8, and plc-est in the same figure) act as slaves. to this aim, an ethernet communication module, cp 343-1, is directly coupled to such master plc through its internal bus. details about the mtu-plc communication are developed in section 3.2."
"the remainder of this paper is structured as follows. section 2 deals with the description of the used fms. the proposed solution to integrate the s&a network with the scada system is reported in section 3. the achieved results are expounded and discussed in section 4. finally, the main conclusions are provided."
"the assignment of the ip addresses is made using the tia portal suite. particularly, the ip address of the cp is defined in the device configuration menu (figure 14) . the subnet mask must also be specified. within this menu, the properties of the cp include the parameterization of the profinet interface, where the assignment of the ip address is performed. in this regard, it should be noted that this interface allows the use of fieldbus profinet or industrial ethernet. in the present case, the latter one has been applied as aforementioned. other configurable features are related to the mpi network, but this connectivity has not been exploited for this application. concerning the parameterization of the ethernet-based communication, the master plc and the mtu are connected in a local area network (lan). the configuration of this communication requires defining the ip address in both nodes. despite the fact that the supervisory application is presented in the next section, it has been considered more convenient to expose here such a configuration for a better exposition of the developed solution."
"once the scada runtime starts, the home screen containing the user identification by name and password, and access icons to the three operating modes of the system appear, as can be seen in figure 17 . it should be noted that user administration is a paramount functionality in the exploitation of scada systems. with the establishment of access permissions, the protection of important data of the process is guaranteed and the execution of certain functions in runtime is regulated. users and user groups are created and specific access rights called \"authorizations\" are assigned to them."
"the performance evaluation methods presented in this article were based on standard methodologies existing in the state-of-the-art for anti-spoofing systems. in this study, we presented techniques to secure our existing system for human age estimation using auditory perception considering its vulnerability to spoofing."
"from the maintenance fc (fc1) calls to the manttransfer, mant1, mant7, mant8, and mant9 functions are made. these fc correspond to the maintenance tasks of the transfer and the four stations, respectively. they are contained in the program of station 1. as aforementioned, this is the station which the scada system accesses to show the activation and deactivation status of the s&a of both the transfer system and the four stations."
"the auditory perception-based system for age estimation and classification showed promising results. although it was very sensitive to spoof attacks, a subject could easily fool the system with respect to age. two scenarios exist to trick the system:"
"each of the stations is constituted by a table-like structure on which the different components of each process are arranged, such as robots, pneumatic cylinders, pneumatic distributors, motors, and sensors. on the front of each station, the electrical and electronic components that carry out the control of the station are arranged: switched voltage sources of 24 vc, protection elements, drivers for servomotors, plc, etc., along with the buttons and indicators used for the manual operation of the station. figure 2 shows the front panel and keypads of one of the stations. the research group has three of the eight stations that make up the entire cell available, namely, station 1, station 7, and station 8, as well as an empty station, which is currently being equipped. the tasks of each station are now explained:"
"direct attacks [cit] are possible by generating synthetic samples, and this is the first unsafe point at the sensor level of a biometric security system. for direct attacks, no particular information is needed"
"in the literature on musical interfaces, design principles for musical controllers have been outlined with the aim of improving the experience of playing interactive composition systems [cit], either individually or in collaboration. jordà [cit] points out the relationship between new musical controllers and new music-making paradigms. according to winkler [cit], interactive music interfaces must provide feedback and interaction support. cook [cit] suggests some artistic principles such as \"instant music, subtlety later\" or \"make a piece, not an instrument or a controller\". some of these are particularly relevant in collaborative interaction, e.g., the use of certain metaphors such as \"catch and throw\" facilitates the idea of a dialogue between several musicians, in which the musical material is received, modified and sent in real time [cit] . in the context of democratic collaborative network music, new design principles related to engagement have also been recommended, such as facilitating the awareness of contributions, relationships between performers or dialogues mediated by technology [cit] . musical tabletops present an ideal setting for collaborative engagement because they make possible visual feedback, individual vs. shared spaces, and real-time multi-dimensional interaction [cit] ."
"in this subsection, all the hardware elements that make up the entire cell will be exposed. the fms, in its current configuration, is formed by the following elements:"
"the interface design displays a number of circles with different ui controllers inside each (see figure 3) . each circle represents a task such as recording/playing or transforming/mixing. the large circle on the right of figure 3 shows four tracks. for each track it is possible to play, record or stop a sample, as well as to modify its volume. the large circle on the left allows participants to modify global con- trols such as the global volume and to manipulate a set of filters such as band-pass filter or reverb, among others. filters affect only the tracks that are in play mode, and they are applied in sequence to the global output. given that all the tracks are looping, and start at the same time, the circle in the middle indicates the start of the global loop with a bright yellow light pulse. this circle also permits changes in the global pitch shift."
"to this aim, first, the configuration of the communication between wincc software and sql server is addressed. the access to an sql database in wincc runtime software is done through scripts. to access an sql database, it must be previously created on the sql server. wincc runtime software acts as an sql client. a communication using the open database connectivity (odbc) standard with the scada application establishes data streaming towards the database. the goal of odbc is to allow access to any data from any application, no matter what database management system (dbms) stores the data. now, through scripts, the developer can create new registers for each new piece, update the information for the current task of the cycle, or allow the user to search a specific register by entering the identifier in the input field. figure 20 shows the result obtained for the database using the aforementioned scripts. in this way, the information of the fms operation is successfully stored and can also be accessed by other software applications like those devoted to erp, mes, cam, or webbased remote visualization. figure 21 depicts, in a simplified way, this availability of databasesupported information. tracking of the product (traceability), the comparison of campaigns, or their use as a virtual test bench for the training of operators without the need of direct connection to the process. the use of external databases allows access both from the monitoring environment and from other applications through standard languages, which results in the most convenient way to integrate industrial computer systems. structured query language (sql) is one of the most widespread languages and is adopted by most manufacturers and suppliers of industrial software. in this work, the process of archiving information of interest was solved through visual basic (vb) scripts that are generated by variable value change events."
"the problem to tackle is the enhancement of the connectivity of the fms automation system. the existent plc support communication through the fieldbus profibus and the proprietary protocol mpi. these protocols are well-proved reliable and robust in industrial environments, but they lack the possibility of being directly connected to ethernet networks. the ethernet connectivity is a paramount requirement for i4.0, as discussed in the introduction. therefore, it was necessary to add such connectivity to start the migration process of the legacy system. the proposed solution to share data between the s&a network and the scada system is reported in this section. to begin with, it is necessary to describe the automation and supervision system that performs the operation of the fms. after that, in section 3.2, the abovementioned solution to integrate both systems is fully developed. to provide a whole perspective, figure 9 depicts the block diagram of the proposal."
"the ability of a human to receive and interpret different sounds that reach the ears or the human auditory system through audible frequency waves transmitted through air or other means is known as auditory perception [cit] . auditory perception has a high correlation with human age. the auditory perception response varies with age; for example, as age increases, the highest audible range of frequency decreases. the decrease in the highest audible frequency leads to hearing loss."
"the touchtr4ck prototype was developed using open source tools. the computer vision framework reactivision [cit] was used for the multi-touch finger tracking, and the table hardware (e.g. infrared illumination, camera and projector) was built according to the requirements of this framework. the audio software was built using the programming language for real-time audio synthesis supercollider 3 [cit], and the graphics and control management with the programming language processing [cit] . thus, the prototype is divided into model, view and control modules: the sound synthesis engine (e.g. playing, recording and transforming sound) is defined and managed in supercollider (the model), whilst the graphic interface and the interaction control of the tuio messages sent by reactivision are managed in processing (the view and the controller)."
"regarding the robots, the control of the movements of the abb irb 120 robot is carried out by abb's compact controller irc5. such a controller is connected to the digital i/o module of the station 7 plc. on the other hand, the cartesian robot of station 8 is controlled by two mitsubishi servomotors, which are connected to the digital i/o module of the associated plc."
"biometric systems allow us to identify a person and provide authentication based on an identifiable and verifiable dataset, which is uniquely specific to the person. it can be used for surveillance, access control, and security systems [cit] . due to scientific growth and technological advancement, in the fields of pattern recognition, computer vision, machine learning, data storage, data processing, and data acquisition, it is now very much possible to identify and verify a subject. several biometric modalities such as face, iris, fingerprint, veins, blood flow, and auditory perception can successfully allow a subject's identification and authentication. in parallel, several spoofing techniques have also been introduced to crack such biometric systems."
"the anti-spoofing dataset was used to assess the vulnerability of the proposed anti-spoofing system with the required threshold τ value, and 410 trials were conducted for the anti-spoofing dataset."
"concerning the software level, db are memory positions of the plc where measurements and signals are stored. these blocks can be of two types, global or instance db. the access to the latter one is restricted to a particular function block (fb). the first type has been the selected one in order to have access to the hosted information by any part of the plc program. taking advantage of the ethernet connectivity afforded by the cp, the data shared through the fieldbus and the db-based storage, a seamless integration between the scada system and the s&a network has been developed and implemented."
"in this way, the master unit gathers all the information of the fms and makes it available for the scada. in other words, the information related to all s&a and internal parameters of plc are \"concentrated\" in two db for each station and can be shared with higher-level applications. therefore, this information is available not only for supervisory interfaces, but also for other software applications devoted to the control of production, enterprise resource planning (erp), manufacturing execution systems (mes), computer-aided manufacturing (cam), and so forth. as a sample of this scheme, figure 13 shows the aspect of the db designed to exchange write values between the scada system and the plc of station 8, named db80, with the master plc acting as the intermediate layer. when parameterizing the profibus communication, a transfer area (ta) is defined to act as a buffer of the shared information. in the presented approach, two ta for each plc have been defined: one for input data and the other for output data. as a consequence, two db have been created for each plc, where one db is devoted to read values and the other one hosts write values. read values are the signals from s&a, which constitute information to be read by the scada system. on the other hand, write values correspond to command signals that the operator/user introduces through the supervisory interface. the empty station nowadays does not require any db because there is no information to be shared. hence, a total number of 6 db are used, as can be observed in this figure."
"future works include the addition of more i4.0 features like cloud storage and open source technologies under the service-oriented architecture (soa) paradigm. the inclusion of the opc ua specification will also be approached to handle the heterogeneity of entities. another further guideline consists of introducing other operational scenarios related to i4.0, taking advantage of the incorporated connectivity."
"in this section, we will discuss the dataset collected for decision threshold optimization and the dataset for testing the anti-spoofing system with the standard value of the decision threshold."
"usability: the anti-spoofing system based on auditory perception was easy to use and was more secure as compared to the existing approaches. this depends on the requirement of the system, e.g., some systems are highly secure and accept a higher value of frr as compared to far. our proposed system is easy to optimize according to the requirements of biometric systems. the anti-spoofing system has not yet been implemented in real-time applications and requires more time for testing."
"this paper has presented the design and implementation of a communication link between an s&a network and a scada system within a fms. the proposal consists of a middle layer involving hardware and software elements. such an fms is used to investigate i4.0, so an enhancement of its connectivity capabilities according to the i4.0 main principles has been carried out."
"as briefly indicated in the introduction, a scada system is a software application specially designed to work on computers in production control, providing communication with field devices (s&a, plc, etc.) and controlling the process automatically from the screen. it also provides all the information generated in the production process to diverse users, both at the same hierarchical level as other supervisory users within the company (supervision, quality control, production control, data storage, etc.)."
data storage: this last option permits the access to an external database with the most important parameters of the work processes performed on each piece.
"the scada system is composed of 14 screens, through which the user can navigate to access all the options and is able to track the process of the fms. the scada system has the following structure:"
"in the following section, some background on both computer-supported collaborative work and collaborative musical experiences is given. after that, an overview of some design principles for democratic collaborative music making are presented. next, the design process for the prototype is introduced: concept, interface design, implementation and interaction mappings. finally, an informal test of the approach is discussed and future work highlighted."
"a middle layer composed of a cp and a software-defined structure to store operational data has been developed. on the one hand, the hardware linkage is implemented by a cp module that enables the integration of the master plc in an ethernet-based network. on the other hand, an array of data blocks (db) in the master plc makes the sharing of information between the scada and the s&a network independently of their location possible."
"this work constitutes a step forward in the direction of enabling i4.0 features for reliable legacy automation systems. moreover, the experimental nature of the fms affords an added value in the sense that it is not a theoretical proposal; instead, it is a real system effectively working."
"as more features of i4.0 are included within the fms, it will be necessary to manage the heterogeneity of components (software and hardware). this issue will be approached by means of the aforementioned opc protocol, namely the unified architecture (ua) specification, whose advantages are able to address the challenges introduced by the industry 4.0 [cit] ."
"the overall efficiency of a biometric system is the main concern, and there is always an assessment of unconventional performance. the first configuration (blue line) was specified as the baseline configuration, while the forth configuration (black line) was simple with no anti-spoofing system and open to spoof attacks. a separate configuration needed to be applied which can support the licit scenario under spoof attack. however, the third configuration (cyan line) was the baseline system with zero-effort imposters (attackers with no spoofing background). the second configuration (red line) allowed the system to stay secure under spoof attack and could be evaluated for overall performance."
"the first spoofing attack for gait signature is that an individual can walk behind the genuine target by copying his/her moves. this kind of spoofing can be identified as a spoofing attempt that has a lower match score in comparison to the genuine gait. the second spoofing attack is related to the reaction of an accelerometer sensor connected to the leg, which is projected on a wall. a spoof attacker can visualize and try to match the moves of the target, and this is used for identification. this technique has an accuracy of 60% to trick the gait biometric system. for the third spoofing attack, an accelerometer is used like the previous approach and focused on one's performance achieved via practice. only those spoof attackers were found to be successful that were closely matched to the genuine signature gait of the target, and with practice, the performance of the signature gait can affect the system. the fourth study showed how a an attacker impersonated the clothing of a genuine subject to trick the gait recognition system. it is among one of the most straightforward and robust methods used for spoofing. it was used to enter a secure environment where formal types of dress or uniforms are common. impersonation of clothes is also one of the most efficient techniques used to spoof gait signature [cit] ."
"for all biometric modalities such as iris, face, gait, etc., there are spoofing detection methods. all the existing systems are based on a deep feature extraction and provide a solid direction that allows us to develop a more efficient anti-spoofing system. in this paper, we present an anti-spoofing system for auditory based human age estimation and classification."
"regarding the mtu, the employed ethernet interface is a common peripheral component interconnect (pci) ethernet card. in wincc, its ip address in established in an equivalent procedure to that followed for the cp. in addition, for a proper communication mtu-plc, it is necessary to parameterize a connection within the wincc, as shown in figure 15 . as can be seen, the addresses of both devices belong to the same range within the lan."
"in the field of biometrics, we consider a binary classification to distinguish genuine and spoof trials. like other biometric systems, this also gave rise to two other kinds of errors, false fake rate (ffr) and false living rate (flr). ffr represents the value of genuine trials misclassified as spoofed trials, while flr represents the value of spoofed trials misclassified as genuine trials."
"the comparison of the human age estimation system under the licit scenario and spoof scenario is shown in figure 2 . the efficiency of a biometric system can be evaluated by calculating the equal error rate (eer). the lower value of err indicates the higher accuracy of the biometric system, while a higher value of eer indicates worse performance. the eer value for the human age estimation system under the licit scenario was nearly 2%, while under the spoof scenario, it increased to 60%. we concluded that the existing system for human age estimation was vulnerable to spoofing and required a strong anti-spoofing system to overcome this challenge."
"until now, all the biometric systems have been facing the mutual issue of spoof attacks. in this article, we introduced an anti-spoofing system based on auditory perception with promising knowledge and a standardized evaluation method. our proposed anti-spoofing system was tested in real time by different volunteers of different ages and genders. we concluded that our proposed anti-spoofing system was robust by having an eer value of 5.5% under the spoofing scenario. this position contributes to a range of forward-looking study strategies, including merged countermeasures and classification techniques. as it is a new trait, more challenging systems are needed to keep biometric systems safe from spoof attackers. however, it is hard to estimate the effectiveness of an anti-spoofing system without implementing it in a critical situation. this includes not only the capacity to identify spoof attacks, but also the effect on the suitability of the model."
"the program used for the design and implementation of the scada system is wincc, included in the tia portal platform. using the aforementioned program, the following functions are carried out: browsing through different screens, control of process variables, start-up of the system, user administration, warnings system, collection of process data and transfer to external files (historical data), scheduled tasks, etc."
"a fully functional and experimental system has been deployed, which, in fact, is presently working. the reported results prove the feasibility of the proposed solution in order to seamlessly connect the s&a network and the scada system, taking advantage of the db-based structure in the master plc and the ethernet cp. all the signals of the s&a are effectively shared and managed through the network, available for higher hierarchical level applications."
"virtualization of manufacturing processes by means of virtual or augmented reality is also a merging movement, envisioned to enhance human-machine interaction in the i4.0 context [cit] . ethernet connectivity will facilitate the sharing of information between the virtualized environment, for instance a 3d virtual world, and the experimental facility."
"as a sample of this scheme, figure 13 shows the aspect of the db designed to exchange write values between the scada system and the plc of station 8, named db80, with the master plc acting as the intermediate layer. concerning the parameterization of the ethernet-based communication, the master plc and the mtu are connected in a local area network (lan). the configuration of this communication requires defining the ip address in both nodes. despite the fact that the supervisory application is presented in the next section, it has been considered more convenient to expose here such a configuration for a better exposition of the developed solution."
"the awareness of others' actions is supported by the division of labour incorporated in the interface design. each of the large circles is devoted to either editing or mixing, so its proximity to the subject indicates who is in charge of each task. moreover, audiovisual feedback indicates the state of the tracks or filters, that is, whether they are mute, active or selected."
"the robotized station is in charge of the adjustment of the four screws placed in the body of the turning device. nevertheless, due to the fact that the research group does not own all the stations that compound the whole cell, the robot will carry out operations of assembly and disassembly of shafts and covers. two different operating cycles have been programmed in the robot, whose execution the research group has three of the eight stations that make up the entire cell available, namely, station 1, station 7, and station 8, as well as an empty station, which is currently being equipped. the tasks of each station are now explained:"
"regarding that last trend, a drawback was found when designing the scada system with the tia portal due to the associated license costs. this aspect is important because the available budgets are continuously decreasing. to solve this situation, a research line is focused on developing a supervisory system using open source software. in fact, open source hardware and software projects are key accelerators for the industry adoption of iot [cit] . in the same regard, the fms is able to act as a benchmark to research the integration of plc and open source hardware devices in the context of legacy systems. an example consists of connecting an arduino microcontroller through an ethernet shield or an inbuilt port, so a communication channel is established."
"the practice of composing electronic music tends to be an individual activity, even though musical composition and music making may also have a social dimension. curiously, among the vast market of musical controllers available for musicians, few are designed for facilitating collaboration in the creative process. a study of the compositional processes of electronic musicians [cit] found that this community would prefer a free and exploratory approach during the initial stages-when an idea can still change substantially-, which is not addressed by the traditional digital audio workstation (daw) software they use. within a spectrum from exploration to linearity in the creative process, interactive composition systems [cit] become a trade-off between the two approaches, hence they offer an explorative approach under permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. certain constraints. using these real-time computer music systems, the process of composing and performing music happens simultaneously. with ableton live, for example, both processes can become one. but, even though collaborations can emerge from laptop ensembles using this software [cit], musical dialogues in real time are more fully exploited by tangible user interfaces (tui) such as the reactable [cit] or the squeezables [cit] ."
"each user is able to add or reduce the number of tracks or filters in order to adapt to his or her expertise, within the limits of the interface design. this flexibility permits novices and experts alike to use the system, whilst maintaining, as much as possible, a democratic setting."
an old subject can impersonate a young one just by finishing the experiment with a high frequency in the first test and respond after some seconds as the second test of the experiment.
"in the present era, the threat of malicious actions is among the major challenges that biometric systems confront. the main type of malicious action uses a conventional type of attack, known as \"spoofing\", to trick a biometric system. biometric spoofing is a technique to deceive a biometric system. in this technique, a false object such as a fingerprint mold made of artificial material that copies the unique biological features of a subject is presented to the biometric scanning tool. the system computes the features in a manner that the biometric system will otherwise not be able to recognize the artifact from the genuine biological target. therefore, the aim of spoofing a biometric system is to present the spoof attacker as a real user by producing a fake identity to fool the biometric sensors. anti-spoofing systems thus are required in order to reject the spoofing attacks [cit] . biometric systems without an anti-spoofing system pose a greater threat to the security of users' data [cit] . in previous studies, eight different points have been highlighted regarding spoofing attacks [cit], and they are categorized into two major groups such as direct attacks and indirect attacks."
"to this aim, first, the configuration of the communication between wincc software and sql server is addressed. the access to an sql database in wincc runtime software is done through scripts. to access an sql database, it must be previously created on the sql server. wincc runtime software acts as an sql client. a communication using the open database connectivity (odbc) standard with the scada application establishes data streaming towards the database. the goal of odbc is to allow access to any data from any application, no matter what database management system (dbms) stores the data. now, through scripts, the developer can create new registers for each new piece, update the information for the current task of the cycle, or allow the user to search a specific register by entering the identifier in the input field. figure 20 shows the result obtained for the database using the aforementioned scripts. in this way, the information of the fms operation is successfully stored and can also be accessed by other software applications like those devoted to erp, mes, cam, or webbased remote visualization. figure 21 depicts, in a simplified way, this availability of databasesupported information."
"the proposed solution is generalist in the sense of being valid if other scada software is used. for instance, the widely known labview environment could also be used, so particularities of such software should have to be configured but the communication bridge would remain the same. in this sense, the communication between the master plc and the scada application should be conducted through the standard open platform communications (opc), but the middle layer implemented in this work would be the same. likely, information storage in databases is a widely supported functionality regardless of the specific supervisory software suite."
"this paper presents the first step for the migration of a legacy system, an fms, towards the i4.0 concept. namely, an ethernet-based communication solution has been developed in order to integrate the s&a network and the scada system. this approach is conceived as a middle layer involving hardware and software elements."
"in this section, the achieved results are expounded and discussed. once the ethernet connection and the db structure have been configured, all the operational information of the s&a network and of plc is available to be monitored. to this aim, a scada system has been developed and deployed, demonstrating the feasibility of the proposed approach. furthermore, a database has also been generated to afford this information to other software applications."
"in view of the achieved results, the following discussion is conducted. this work constitutes the first step in the migration of a legacy system, namely an fms, towards the i4.0 concept. as expounded in the previous subsections, the experimental results prove the feasibility of the proposed solution."
"in this section, the achieved results are expounded and discussed. once the ethernet connection and the db structure have been configured, all the operational information of the s&a network and of plc is available to be monitored. to this aim, a scada system has been developed and deployed, demonstrating the feasibility of the proposed approach. furthermore, a database has also been generated to afford this information to other software applications."
"the development dataset was utilized to decide the optimization point of the threshold for efficient performance at a specific operational value. the total number of trials for the development dataset was 360, for both genders (male and female)."
"from the point of view of the system operation, there are two network levels in the proposed system, namely: profibus and industrial ethernet (ie). fieldbus profibus serves as a communication network among plc (master and slave units) and distributed input/output (i/o) modules. each of the aforementioned devices includes a communication port for this fieldbus, so, the physical layer of this configuration is already resolved. in this way, the master plc has access to all the s&a of the manufacturing cell. the communication between the master plc and the scada system is carried out by the standard communication network ethernet. to this goal, a middle layer that acts as a bridge to link the fieldbus with the scada system enables the communication between the master plc and the scada system. such a middle layer is composed of a communication processor (cp) and a software-defined structure to store operational information. data of s&a are available in the fieldbus through plc or dedicated interfaces. there are also s&a linked directly to the plc of station 1. from the point of view of the system operation, there are two network levels in the proposed system, namely: profibus and industrial ethernet (ie). fieldbus profibus serves as a communication network among plc (master and slave units) and distributed input/output (i/o) modules. each of the aforementioned devices includes a communication port for this fieldbus, so, the physical layer of this configuration is already resolved. in this way, the master plc has access to all the s&a of the manufacturing cell. the communication between the master plc and the scada system is carried out by the standard communication network ethernet. to this goal, a middle layer that acts as a bridge to link the fieldbus with the scada system enables the communication between the master plc and the scada system. such a middle layer is composed of a communication processor (cp) and a software-defined structure to store operational information. data of s&a are available in the fieldbus through plc or dedicated interfaces. there are also s&a linked directly to the plc of station 1."
"iris recognition/verification has gathered significant attention due to its well established architecture, with high precision and operational performance. the viability of spoofing attacks was recognized for the first time by daugman [cit] . he used fast fourier transform for the verification of high frequency spectral measures inside the frequency domain. in the literature, several solutions are available to detect the liveness of iris, which rely on special acquisition hardware [cit], as well as software based solutions that use the pattern of someone's iris on contact lenses to analyze the textural effects of the spoof attacker [cit] . software based solutions have also investigated pupil construction [cit], cosmetic contact lenses [cit], and multiple biometrics, eeg and iris together [cit] ."
"the robotized station is in charge of the adjustment of the four screws placed in the body of the turning device. nevertheless, due to the fact that the research group does not own all the stations that compound the whole cell, the robot will carry out operations of assembly and disassembly of shafts and covers. two different operating cycles have been programmed in the robot, whose execution depends on the coding of the transport pallet that supports the set. the so-called short cycle (code: 000) only implies that the cover is changed and the screwing is done. on the contrary, in the long cycle (code: 101), the cover and the shaft are changed and the screwing is conducted."
"in this section, the performance of human age estimation and classification is briefly explained along with the vulnerability of the system to spoof attacks. the proposed approach of human age classification based on auditory perception showed a good classification rate of 92% and 86% for three to five age groups, respectively. a robust regression model was also designed for human age estimation, and it had a root mean square of error value of 2.6 years."
a young subject can impersonate an old one by finishing the first test with lower frequency and respond with a higher delay for the second test.
"the remainder of this paper is structured as follows. section 2 deals with the description of the used fms. the proposed solution to integrate the s&a network with the scada system is reported in section 3. the achieved results are expounded and discussed in section 4. finally, the main conclusions are provided."
"once the scada runtime starts, the home screen containing the user identification by name and password, and access icons to the three operating modes of the system appear, as can be seen in figure 17 . the monitoring-devoted screens are described below. with these screens, the behavior of the fms working in automatic mode can be supervised. this mode consists of a main view (figure 17 ) in which the basic indicators appear to check the operation of the global system, as well as the buttons for set-up and to reset the counters of correct parts and of the full warehouse batch. finally, from the main view, it is possible to access the warnings to visualize the operating states and the present faults the monitoring-devoted screens are described below. with these screens, the behavior of the fms working in automatic mode can be supervised. this mode consists of a main view (figure 17 ) in which the basic indicators appear to check the operation of the global system, as well as the buttons for set-up and to reset the counters of correct parts and of the full warehouse batch. finally, from the main view, it is possible to access the warnings to visualize the operating states and the present faults and alarms in the installation. obviously, the empty station has not been fully included in this screen since it does not perform operations over the piece, and only allows the cyclic transport of the pallet."
"the time and effort devoted to implementing the proposed approach has required deep expertise related with automation to configure the plc, whereas the deployment of the ethernet link with the scada system has not required high level knowledge about networking over ethernet. the structure of db also implies a profound skill for managing distributed i/o signals in the plc and the fieldbus profibus. it must be taken into account that the usage of industrial fieldbuses imposes detailed parameterization, whereas the ethernet means does not require low level configurations. this issue is considered as a benefit in order to facilitate the migration towards i4.0."
"in figure 6, each profile represents a specific configuration profile according to the baseline approach. the blue profile (first line from the bottom) shows the baseline performance (auditory perception based age estimation) of the biometric system under the licit scenario (genuine trials). the black profile (the highest line) shows the efficiency of the system while having the same configuration of the baseline under the spoofing scenario. the cyan profile (second from highest) shows the performance of the baseline system under the licit scenario with zero-effort imposters. the red profile (second from bottom) shows the performance of the baseline system, equipped with auditory perception based anti-spoofing under spoofing attack. in order to quantify how many genuine users were misclassified as spoof attackers and to recapitulate all of the above configurations, we needed to study the system under a complex contact of the integrated system with a spoofing attack."
"o programming of the plc that controls the work cycle of the station. the plc commands the irc5 controller to initiate the sequence that has been programmed for the robotic arm. the ir5 controller returns an end-of-cycle signal when all its instructions have been completed. it also provides information about the execution of the different phases of the cycle (screwing, changing the cover, etc.)."
"meanwhile, four different spoofing mechanisms have been explored. the gait motion has been captured through an accelerometer sensor, which provided the gait signature. traditional approaches such as a vision based gait recognition system have more potential and are more practical compared to a sensor based approach."
"the master plc communicates with the rest of the components via profibus. these components are configured as slaves and correspond to the mitsubishi and wago distributed i/o modules and the remaining plc. eventually, for the initial programming and configuration tasks, the pc could also be connected to the plc through an mpi connection, as indicated in figure 9 in an orange color."
"before addressing the description of the proposed approach to communicate the s&a network and the scada application, it is necessary to include, as a starting point, a general description of the fms of the company smc (tokyo, japan) [cit] . the automated flexible manufacturing cell performs assembly and storage of a turning mechanism. to perform these tasks, the complete system consists of eight stations using components from different technologies (pneumatics, electrical engineering, robotics, etc.). the turning mechanism assembled in the different stations of the cell consists of the following elements: body, bearing, shaft, cap, and screws. these constituents are shown in figure 1a, whereas the pallet where the set is transported appears in figure 1b . each of the stations is constituted by a table-like structure on which the different components of each process are arranged, such as robots, pneumatic cylinders, pneumatic distributors, motors, and sensors. on the front of each station, the electrical and electronic components that carry out the control of the station are arranged: switched voltage sources of 24 vc, protection elements, drivers for servomotors, plc, etc., along with the buttons and indicators used for the manual operation of the station. figure 2 shows the front panel and keypads of one of the stations. each of the stations is constituted by a table-like structure on which the different components of each process are arranged, such as robots, pneumatic cylinders, pneumatic distributors, motors, and sensors. on the front of each station, the electrical and electronic components that carry out the control of the station are arranged: switched voltage sources of 24 vc, protection elements, drivers for servomotors, plc, etc., along with the buttons and indicators used for the manual operation of the station. figure 2 shows the front panel and keypads of one of the stations."
"the vulnerability of the system was quantitatively measured and expressed in terms of spoof false acceptance rate (sfar). sfar is the percentage value of the spoofed trials that are classified as genuine for a given decision threshold τ. an example is presented in figure 7, which shows the score distributions for subjects of genuine, impostor, and spoofed trials. it can briefly illustrate the sfar profile with a threshold function τ. the difference among the impostor and spoofed trial distributions showed the potential impact of spoofing on the quality of biometric verification, and the correlation of genuine and spoofed trial score distributions was significantly greater than that of genuine and impostor distributions. in figure 8, the det plot presents both the spoof (sfar vs. frr) and licit (far vs. frr) scenarios. expressing the vulnerability at a certain point is very important; thus, the eer for sfar and far for a common frr is shown in figure 8 . the far under the licit scenario was 2.3% for a baseline system, while sfar was 5.5% under the spoof scenario, which means there was a chance for nearly five trials to be misclassified among one hundred trials. the results showed that our proposed anti-spoofing system was highly secure and that it was difficult for a spoof attacker to deceive this system."
"as briefly indicated in the introduction, a scada system is a software application specially designed to work on computers in production control, providing communication with field devices (s&a, plc, etc.) and controlling the process automatically from the screen. it also provides all the"
"interactive tabletops have been studied within the field of computer-supported collaborative work (cscw). in the cscw literature, facilitating the awareness of others [cit] and enriching of existing work processes [cit] are factors that have been considered when approaching co-located collaboration. thus, our approach focuses on enhancing the workflow of collaborative music creation with special attention to facilitating communication among musicians by supporting the awareness of others."
"the presented development has enabled the integration of an s&a network with a scada system. to sum up, the legacy feature of the existing plc has been overcome by means of an ethernet link between the supervisory interface and the master plc. therefore, all data of the s&a network can be effectively retrieved by such a supervisory interface. the designed scada system affords the relevant information with user-friendly and easy-to-use features."
"as frr and far are inversely related, the decision threshold can be fixed to equalize the ratio of far and frr. therefore, the a posteriori performance criterion to minimize can be the value of the half total error rate (hter):"
"the control of the modular transfer of stations 1 and 8 is carried out with the mitsubishi distributed i/o modules. these modules, located below their corresponding station, are connected to the fieldbus and have eight inputs and four digital outputs for each one. in the same way, the wago modules control the modular transfer segments of stations 7 and empty, also linked via profibus."
"in this way, the master unit gathers all the information of the fms and makes it available for the scada. in other words, the information related to all s&a and internal parameters of plc are \"concentrated\" in two db for each station and can be shared with higher-level applications. therefore, this information is available not only for supervisory interfaces, but also for other software applications devoted to the control of production, enterprise resource planning (erp), manufacturing execution systems (mes), computer-aided manufacturing (cam), and so forth. figure 12 depicts the information flows regardless of the physical medium where it occurs. a continuous data flow takes place between the different elements. for instance, signals from s&a of the transfer are interfaced with profibus via the distributed i/o modules, and exchanged with the master plc through profibus. once these stages are completed, the supervisory program accesses the signals by means of the ethernet channel."
"monitoring: this second option makes it possible to carry out the monitoring of the productive process without the need to be in situ next to the stations thanks to the implemented ethernet link. in this way, it is possible to know at each moment where the platform is located, what sensors are activated, and what action is being taken. 3."
"modiability of others' actions is allowed through the use of the shared controls such as the global pitch shift. in addition, the division between the tasks implies that users can only have partial control of the musical result."
"among the research and development (r&d) lines carried out in the automation and industrial computation laboratory of the university of extremadura (spain), i4.0 is receiving a lot of attention in order to study its implications from the perspective of automation and supervision. with this aim, an experimental advanced i4.0-compliant system is required to act as a benchmark to investigate the migration of legacy systems towards this challenging concept."
"remote access for online supervision is afforded by the remote connection options of the scada system. in the present case, wincc offers web connections, as well as a virtual network computing (vnc) desktop interface. in the developed system, this function has not been exploited in order to avoid unauthorized intrusions in the network of the university."
"the face based anti-spoofing techniques are categorized into four different groups [cit] such as user behavior modeling, data driven characterization methods, relying on user corporation, and relying on extra devices. users' behavior modeling concerns the behavior of a user in front of the camera, and some researchers considered motion detection such as the unintentional movement of different parts of the head and face [cit] and eye blinking [cit] . these methods rely on extra devices such as allowing a user to utilize specific anti-spoof hardware, and thermal or infrared images could be deployed [cit] . multiple 2d cameras or 3d cameras have been used, which can also provide additional protection [cit] . the physical characteristics of materials relating to their unique reflective qualities have also been presented as a measure of distinction between a real face and a printed face on paper as a 2d image. polarized light (light that vibrates in one direction) can be utilized to distinguish reflections. stokes' parameters have been applied to generate stokes images, which have then been utilized to create the final picture, known as the stokes degree of linear polarization (sdolp). statistically, the strength of an sdolp image has been studied, and promising results have been demonstrated between skin and a paper mask in the material classification [cit] . from many decades, people have been wearing masks or facial disguises so as not to be identified. in the present era, the use of plastic surgery is a newer trend to modify one's appearance. the procedure of plastic surgery is performed because of its cost and time effectiveness to achieve perfection. despite all this, recently, a robust algorithm was designed to detect the facial surgery changes [cit] . however, the problem of face recognition after going through an operation of plastic surgery is still a challenging task [cit] . even without going through a permanent treatment, temporary make-up can also affect the efficiency of a face based biometric system [cit] . adaptive gradient location and orientation histogram (agloh) based extraction features have also been introduced for successful plastic surgery face recognition. the features indicated have been omitted from the granular region of the face [cit] . all the above techniques mentioned such as face masks, make-up, and plastic surgery are used to hide the identity of a person. adults try to impersonate a child; males can try to impersonate females, etc. it has also been demonstrated that a female intruder can impersonate a male successfully by wearing some make-up [cit] ."
"both frequencies are registered in a database, and the system then calculates the mean of the two frequencies (first test frequency and second test frequency). the three frequencies are the feature vectors, which are used to describe the response of auditory perception for every test subject. while performing this experiment, two objectives were achieved:"
"hence, the value of flr/ffr can decide the overall efficiency of a biometric system under the required conditions. the performance depends on the requirement and security of the system such that for some applications, flr is more important than frr and vice versa. the eer value for the baseline under the licit scenario, the baseline under the licit scenario with zero-effort imposters, the baseline under spoof attacks, and the baseline with anti-spoofing system was 2.7%, 43%, 60%, and 5.5%, respectively. these results demonstrated that the anti-spoofing system based on auditory perception showed promising accuracy for age verification."
"vulnerability: as our purposed anti-spoofing system was well equipped with a solid design, every biometric system shows a weakness for spoof attacks. although some biometric modalities such as gait, fingerprints, etc., claim more secure behavior as compared to other biometric systems, that does not mean these approaches are reliable in the presence of spoof attackers. thus, in this article, we also presented a secure anti-spoofing approach, which can be used for other biometric modalities."
the performance of the anti-spoofing system based on auditory perception was evaluated with the spoofing scenario such that the subset of zero-effort imposters was replaced by spoofed trials. it can be illustrated from the score distribution that the overlap between spoofed trials and genuine was greater than imposter and genuine trials.
"consistent with these three objectives, the touchtr4ck prototype allows musicians to record up to four samples and mix these collaboratively on a multi-touch tabletop surface. the prototype offers a plug and play approach, where sounds can be recorded and modified, and disruption of the workflow between editing and mixing is avoided by looping all tracks and showing changes in real time, which seems ideal for exploration. democratic collaboration is supported by dividing these two main tasks into two modules which can be executed in parallel, and the possibility of modifying others' contributions. additionally, the awareness of others is provided by real-time visual feedback. finally, collaborative engagement seems to be associated to the personal motivation and the control level of the interface, hence using basic ui controllers (both discrete and continuous such as knobs, sliders or buttons) is intended to afford ease of learning and use at a general level."
"second test: the second test starts automatically. in this case, the sound is generated from higher frequency (20,000 hz) to lower frequency (20 hz). the subject can complete the second test (e.g., keyboard action) once the subject starts detecting the sound."
"in summary, we have provided a set of design principles for democratic collaborative music on interactive systems, and we have built a prototype upon these. after an informal evaluation, promising results have been obtained: firstly, collaborative experience has been facilitated by awareness and modifiability of others' actions using a shareable interface; and secondly, experts found the musical controller playful and experimental, an approach which tends to be offered to novices only. future work will involve, on the one hand, carrying out a formal testing with more users, in order to strengthen the design concept; and, on the other hand, to improve the prototype by providing more accuracy of control, more support for relationships between performers and more awareness of contributions. furthermore, the benefits of a flexible design should be examined precisely. as a final remark, this approach specifically enhances the relationships between performers because collaborative work processes are facilitated which, in turn, affect the musical output."
"as a proof of concept, figure 18 shows the main monitoring screen under real operation of the fms. as can be seen, the working station is indicated in the top right corner, and in the presented case, station 1 is processing body pieces (see the corresponding green indicator). the pallet is placed in this station, which is shown by a green indicator in the section devoted to the transfer state. concerning the automatic warehouse, an alarm state is reflected, showing that the warehouse is full. the maximum number of stored pieces (30) has been reached, so it is signaled by means of a red led indicator in order to inform the operator. this human-requesting situation is solved by resetting the counter (button reset warehouse) once the positions have been manually released."
"the motivation for this work arose when implementing the automation system of the fms in order to be exploited for i4.0-related r&d. it was necessary to improve some functionality of the fms, mainly enhancing its networked communication options and data management. concretely, the installed plc do not provide ethernet connectivity by default, whereas the fieldbus profibus is natively supported, as well as a proprietary protocol of siemens (multi-point interface, mpi). a scada system could be linked to the s&a network via mpi connection or using a profibus adaption card coupled to the pc where the supervisory application runs. though, in a certain sense, under the i4.0 vision, these options imply an isolated operation regarding modern devices which use widespread ethernet communication and facilitate the inclusion of other i4.0 functionalities. this boundary was considered as an opportunity to study the legacy problem. consequently, the presented proposal takes advantage of the already available components, fostering their orchestration following the i4.0 scheme."
"before addressing the description of the proposed approach to communicate the s&a network and the scada application, it is necessary to include, as a starting point, a general description of the fms of the company smc (tokyo, japan) [cit] . the automated flexible manufacturing cell performs assembly and storage of a turning mechanism. to perform these tasks, the complete system consists of eight stations using components from different technologies (pneumatics, electrical engineering, robotics, etc.). the turning mechanism assembled in the different stations of the cell consists of the following elements: body, bearing, shaft, cap, and screws. these constituents are shown in figure 1a, whereas the pallet where the set is transported appears in figure 1b . the target group of this paper is researchers and practitioners in the scopes of industrial automation, s&a networks, and i4.0."
"informal testing was done with two expert musicians using the prototype as a proof-of-concept. the two participants formerly played in a band together, and currently they make electronic music using mainly daw workstations, along with individual musical controllers. both interacted with the prototype for an interval of ten minutes. a playful attitude was observed during the whole session. moreover, the musicians contributed similarly, using both shared and individual controls. after the session, an informal discussion was carried out, in which both described the prototype as an experimental tool not fully controllable, and which provided unexpected results that can be useful when composing music. they both agreed on the ease of use of the prototype, although commented about the need of more accuracy when recording."
"for gait recognition, spoof attacks have not been studied as intensively as needed. high quality video of a legitimate volunteer replayed in front of the camera can affect the system. gait is a behavioral trait, and it may not remain the same, especially over a longer period of time, due to changes in body weight, and particular injuries [cit] . synthetic attacks cannot affect the performance of gait biometrics."
"within such a laboratory, a flexible manufacturing system (fms) is found. it is composed of a set of stations interconnected mechanically by means of a transport conveyor, and logically through a fieldbus. such a network acts as backbone where s&a, control units, and a supervisory system exchange data in real time. fms has a modular architecture that allows the work stations to be reorganized for different processes and operations. as it is evident, fms are ideal environments in which to research the i4.0 concept since they include characteristics like networked interconnection, data gathering, and distributed intelligence."
"as previously commented upon, the cp module is responsible for establishing such a connection, both for the programming of the first station plc and for communication with the scada system. the cp includes a rj45 port to accommodate an ethernet wire. another capability is a customizable web page hosted by this module that allows web-enabled diagnostics via hypertext transfer protocol (http) clients. it should be noted that in the present work, this capability has not been exploited."
"there are also screens to access the monitoring of each station separately. through animations and boolean indicators, the evolution of the process can be observed in real time. other actions that can also be performed are resetting the counters of accepted and rejected parts, restarting the abb robot system, or resetting the warehouse positions. for instance, figure 19 shows the screen dedicated to monitor the first station. the reflected case corresponds to that commented on for the previous screen. this one provides illustrative information about the operation of the s&a of the first station. namely, the pallet is detected, the correct position of the base is verified, and the vacuum pad works properly to place the base over the pallet."
"concerning data storage, it must be remarked that the continuous recording of data allows its subsequent use and, therefore, also its graphic representation, comparison, creation of statistics, analysis, etc. consequently, the registration of process data (historical) and its possible exploitation is a basic functionality of the supervisory system. for this purpose, the monitoring systems are linked to databases, usually external to them. in this way, the monitoring systems allow the historical tracking of the product (traceability), the comparison of campaigns, or their use as a virtual test bench for the training of operators without the need of direct connection to the process. the use of external databases allows access both from the monitoring environment and from other applications through standard languages, which results in the most convenient way to integrate industrial computer systems. structured query language (sql) is one of the most widespread languages and is adopted by most manufacturers and suppliers of industrial software. in this work, the process of archiving information of interest was solved through visual basic (vb) scripts that are generated by variable value change events."
"in this section, we will explain the mechanism of the existing system for human age estimation using auditory perception and its vulnerability to spoof attacks."
"to this aim, first, the configuration of the communication between wincc software and sql server is addressed. the access to an sql database in wincc runtime software is done through scripts. to access an sql database, it must be previously created on the sql server. wincc runtime software acts as an sql client. a communication using the open database connectivity (odbc) standard with the scada application establishes data streaming towards the database. the goal of odbc is to allow access to any data from any application, no matter what database management system (dbms) stores the data. now, through scripts, the developer can create new registers for each new piece, update the information for the current task of the cycle, or allow the user to search a specific register by entering the identifier in the input field. figure 20 shows the result obtained for the database using the aforementioned scripts. in this way, the information of the fms operation is successfully stored and can also be accessed by other software applications like those devoted to erp, mes, cam, or web-based remote visualization. figure 21 depicts, in a simplified way, this availability of database-supported information."
author contributions: a.j.c. conceived and developed the presented systems; i.g. and a.j.c. performed the experimental validation; i.g. analyzed the data; i.g. and a.j.c. wrote the paper. all authors have seen and approved the manuscript.
"to allow access to the data of all the stations and s&a, a software structure based on db has been created in the master plc of station 1 (figure 12 )."
"the master plc communicates with the rest of the components via profibus. these components are configured as slaves and correspond to the mitsubishi and wago distributed i/o modules and the remaining plc. eventually, for the initial programming and configuration tasks, the pc could also be connected to the plc through an mpi connection, as indicated in figure 9 in an orange color."
"in an exercise of self-criticism, the presented fms still has a long way to go until it becomes a fully i4.0-compliant system. in fact, innovative trends can be incorporated like radio frequency identification (rfid), virtual/augmented reality, cloud computing, cyber security means, or open source resources."
"the flowchart of the proposed auditory perception based age estimation and classification approach is shown in figure 1 . first, the auditory system is stimulated via dynamic frequency sound waves. the audible frequencies are registered and utilized for age estimation of a person. after, the responses of the auditory perception are registered in a dataset to analyze the separability between the different age groups, to classify the perceived responses into an age group, and estimate the age of the subject."
"empty station: this one is intended as a reserve station for future extensions. its objective is to close the modular transport system (transfer), so that the transport pallet can return from station 8 to station 1, and thus be able to carry out a cyclic and continuous execution of the assembly process."
"the problem to tackle is the enhancement of the connectivity of the fms automation system. the existent plc support communication through the fieldbus profibus and the proprietary protocol mpi. these protocols are well-proved reliable and robust in industrial environments, but they lack the possibility of being directly connected to ethernet networks. the ethernet connectivity is a paramount requirement for i4.0, as discussed in the introduction. therefore, it was necessary to add such connectivity to start the migration process of the legacy system. the proposed solution to share data between the s&a network and the scada system is reported in this section. to begin with, it is necessary to describe the automation and supervision system that performs the operation of the fms. after that, in section 3.2, the abovementioned solution to integrate both systems is fully developed. to provide a whole perspective, figure 9 depicts the block diagram of the proposal."
"an anti-spoofing system extracted different kinds of features for each biometric trait. ridgelets was used to extract features from the face, while from fingerprint level 1 such as local orientation and frequencies and level 2 (minutiae) extracted the required characteristics. the local ternary pattern was calculated for iris. finally, all the features were fused and fed to the classifier for classification. a multimodal biometric system with the fusion of three biometric modalities (face, fingerprint, and iris) was also designed based on a convolutional neural network with promising results [cit] ."
"consequently, this work aims to humbly contribute to the migration path of legacy systems towards the challenging i4.0 scenario. in this sense, authors would like to remark that the proposal has constituted a preliminary stage in order to make a legacy system a candidate to fulfil i4.0 requirements. therefore, further efforts must be conducted to make the system fully i4.0-compliant."
"case, station 1 is processing body pieces (see the corresponding green indicator). the pallet is placed in this station, which is shown by a green indicator in the section devoted to the transfer state. concerning the automatic warehouse, an alarm state is reflected, showing that the warehouse is full. the maximum number of stored pieces (30) has been reached, so it is signaled by means of a red led indicator in order to inform the operator. this human-requesting situation is solved by resetting the counter (button reset warehouse) once the positions have been manually released. there are also screens to access the monitoring of each station separately. through animations and boolean indicators, the evolution of the process can be observed in real time. other actions that can also be performed are resetting the counters of accepted and rejected parts, restarting the abb robot system, or resetting the warehouse positions. for instance, figure 19 shows the screen dedicated to monitor the first station. the reflected case corresponds to that commented on for the previous screen. this one provides illustrative information about the operation of the s&a of the first station. namely, the pallet is detected, the correct position of the base is verified, and the vacuum pad works properly to place the base over the pallet. there are also screens to access the monitoring of each station separately. through animations and boolean indicators, the evolution of the process can be observed in real time. other actions that can also be performed are resetting the counters of accepted and rejected parts, restarting the abb robot system, or resetting the warehouse positions. for instance, figure 19 shows the screen dedicated to monitor the first station. the reflected case corresponds to that commented on for the previous screen. this one provides illustrative information about the operation of the s&a of the first station. namely, the pallet is detected, the correct position of the base is verified, and the vacuum pad works properly to place the base over the pallet."
"here, the decision threshold τ was set in order to equalize the false rejection rate (frr) and the false acceptance rate (far). frr is defined as the ratio of the number of false rejections divided by the number of verification attempts; while far is defined as the ratio of the number of false acceptances divided by the number of verification attempts. the baseline performance of the algorithm can also be illustrated as a function of the decision threshold τ eer ."
"when parameterizing the profibus communication, a transfer area (ta) is defined to act as a buffer of the shared information. in the presented approach, two ta for each plc have been defined: one for input data and the other for output data. as a consequence, two db have been created for each plc, where one db is devoted to read values and the other one hosts write values. read values are the signals from s&a, which constitute information to be read by the scada system. on the other hand, write values correspond to command signals that the operator/user introduces through the supervisory interface. the empty station nowadays does not require any db because there is no information to be shared. hence, a total number of 6 db are used, as can be observed in this figure. this division of information facilitates modifications or maintenance tasks, both for current legacy equipment or future new devices."
"a middle layer composed of a cp and a software-defined structure to store operational data has been developed. on the one hand, the hardware linkage is implemented by a cp module that enables the integration of the master plc in an ethernet-based network. on the other hand, an array of data blocks (db) in the master plc makes the sharing of information between the scada and the s&a network independently of their location possible."
"multimodal biometrics can also be defined as a fusion of a matcher and liveness detector or multiple biometric systems without liveness detection [cit] . multimodal biometric systems are considered more secure as compared to unimodal systems by making it difficult for the intruder to spoof the trait of a genuine subject [cit] considered a biometric system combining face and fingerprint modalities, and the likelihood rate and weighted sum were used as score fusion rules. the performance evaluation results showed a lower value of false acceptance rate (4.33% and 4.71%)."
"the user program of each of the plc that make up the automation system is structured in a series of blocks, called functions (fc). the following diagram shows a flow chart that portrays the structure of the master plc program (figure 11) ."
"the assignment of the ip addresses is made using the tia portal suite. particularly, the ip address of the cp is defined in the device configuration menu (figure 14) . the subnet mask must also be specified. within this menu, the properties of the cp include the parameterization of the profinet interface, where the assignment of the ip address is performed. in this regard, it should be noted that this interface allows the use of fieldbus profinet or industrial ethernet. in the present case, the latter one has been applied as aforementioned. other configurable features are related to the mpi network, but this connectivity has not been exploited for this application."
"the biometric system for age estimation and classification based on auditory perception was vulnerable and easy to spoof, as briefly explained in section 3. therefore, we present an anti-spoofing system based on auditory perception responses, and the flowchart of the proposed system is shown in figure 3 . experimental design: a test subject was required to take the test for age estimation using auditory perception, as shown in figure 1 . as the auditory perception based system estimated the age, our proposed anti-spoofing system would verify the age of the test subject. a system was designed to generate ten random frequencies of sound according to our standard database by taking the estimated age of the subject as an input. among these ten sound frequencies, some were audible and some were inaudible for the test subject. to make it more secure against spoofing attacks, some of the audible frequencies were repeated to ensure that the test subject provided the same feedback. according to our previous study, the minimum and the maximum values of audible and inaudible sound frequencies for each age were assigned from a reference database. it was hard for a spoof attacker to guess the audible and inaudible sound frequencies in the set of generated sound frequencies. every feedback for each generated sound frequency had a value of 1/b to calculate the final score, where b is the total number of randomly generated sound frequencies, as shown in algorithm 1. the final score must be greater than a decision threshold τ to prove that the subject is genuine and verify the input age."
"in our opinion, approaching the aspects that facilitate a musical controller to be more democratic is a key element for successful collaboration 1 . [cit] by stockhausen (reported by blaine and fels [cit] ). this piece is meant to be played by six musicians in three pairs: one pair of percussionists plays a tam-tam instrument; another pair records the resulting sounds with microphones; and the third pair applies filters to the output. although the musicians follow an instructional score, and thus the performance requires musical expertise, it can be seen as a seminal example of a democratic piece because each performer's musical influence depends on the rest of the team."
three main interaction factors are identified for a democratic collaborative music making on multi-touch surfaces: awareness of others' actions; modifiability of others' actions; and the distinction of users' musical expertise.
"the software platform used for the development of this project is totally integrated automation portal v13 sp1, which has been recently referred to as the foundation of digital factories [cit] . the programming of the automation system has been carried out with the step 7 professional package, an engineering software for programming and configuring siemens controllers included in the tia portal platform. the programming of the supervisory system has been resolved with the wincc package from siemens (munich, germany), also included in the tia portal platform."
"address of the cp is defined in the device configuration menu (figure 14) . the subnet mask must also be specified. within this menu, the properties of the cp include the parameterization of the profinet interface, where the assignment of the ip address is performed. in this regard, it should be noted that this interface allows the use of fieldbus profinet or industrial ethernet. in the present case, the latter one has been applied as aforementioned. other configurable features are related to the mpi network, but this connectivity has not been exploited for this application. regarding the mtu, the employed ethernet interface is a common peripheral component interconnect (pci) ethernet card. in wincc, its ip address in established in an equivalent procedure to that followed for the cp. in addition, for a proper communication mtu-plc, it is necessary to parameterize a connection within the wincc, as shown in figure 15 . as can be seen, the addresses of both devices belong to the same range within the lan. as a result, the network view of tia portal depicts the configured network, including both the profibus segment and the ethernet link ( figure 16 ). one can observe that both networks correspond to those depicted in figure 10 . as a result, the network view of tia portal depicts the configured network, including both the profibus segment and the ethernet link ( figure 16 ). one can observe that both networks correspond to those depicted in figure 10 . as a result, the network view of tia portal depicts the configured network, including both the profibus segment and the ethernet link (figure 16 ). one can observe that both networks correspond to those depicted in figure 10 ."
"1. design and implementation of a generic (km-technique agnostic) simulation engine to assess the quality of the km and decision-making techniques. 2. partitioning the resource allocation problem for cloud infrastructures into several subproblems by proposing escalation levels that structure all possible reaction possibilities into different subproblems using a hierarchical model. 3. design, implementation and evaluation of two km techniques for one escalation level, i.e., vm resource configuration: cbr, and the rule-based approach. 4 . application of the rule-based approach to real-world monitoring data from scientific workflow applications in the field of bioinformatics."
"green-orange heavy over-consumption is forbidden. all applications that consume more than τ % (threshold to be specified) of the agreed resource slo are restrained to τ /2% grouped by a clustering algorithm we choose the one with the highest utility and execute exactly the same action as in the chosen case. afterwards, this action, the resulting measurement and the utility of the action is added to the initial measurement, and stored as a complete case."
this section presents a methodology of dividing the resource allocation problem into smaller subproblems using a hierarchical approach. it demonstrates which actions can be executed in what level to achieve sla adherence and efficient resource allocation for cloud infrastructures.
"in this paper we have hierarchically structured all possible reallocation actions, and designed, implemented, and evaluated two knowledge management techniques, case based reasoning and a rule-based approach to achieve the aforementioned goal for one reallocation level, i.e., vm reconfiguration. after a comparison, we determined the rule-based approach to outperform cbr with respect to violations and utilization, but also to time performance. furthermore, we applied the rule-based approach to a realworld use case evaluating a scientific workflow from the area of bioinformatics. we showed by simulation that the rule-based approach can effectively guarantee the execution of a workload with unpredictably large resource consumptions."
"this paper studies the constellation diagram design for a recently introduced communication system that is based on the concept of near-field direct antenna modulation (nf-dam). unlike the conventional architectures, the signal is modulated in a nfdam system after the antenna by varying the electromagnetic boundary conditions of the antenna via a passive controller. one of the major challenges in designing an optimal nfdam system is to find the coverage area of the signal constellation diagram. it is shown that this coverage area is always a convex region that turns into a circle if no constraints are imposed on the parameters of the system. later on, a linear matrix inequality (lmi) optimization method is proposed to approximate the coverage region by a polygon with any prescribed accuracy. a similar analysis is performed for the identification of the coverage area of the antenna input admittance."
"as to the utilization of the resources, it is clearly higher when a lot of violations occur, so scenario 1 naturally achieves high utilization. this is the case, because when a parameter is violated, then the resource is already fully used up, but even more of the resource would be needed to fulfill the needs. on the opposite, scenario 3 naturally achieves low utilization, as a lot of resources are over-provisioned. scenarios 2.* achieve a good utilization that is on average in between the two extremes and ranges from 70.6% (scenario 2.1) to 86.2% (scenario 2.8). furthermore, we observe some exceptions to this ''rule'' when considering individual parameters. so, e.g., for memory we achieve a utilization of 85.0% with scenario 2.8 or 80.0% with scenario 2.6, which is higher than the utilization in scenario 1 (77.4%). the same is true for cpu utilization rates of 85.5% as compared to 84.3% for the scenario 1 and 2.8, respectively. only for storage the utilization of all but one of the scenarios 2.*, which is at 85.9%, is smaller than for scenario 3 (90.1%)."
"in this section we describe how the km approach can be integrated within a more holistic cloud management project that, e.g., also consists of a monitoring component. yet, the km approach does not depend on the specific used monitoring framework, as long as it correctly measures the current values of the parameters specified in the sla."
"the simplest type of control unit that one can think of is likely a switching network, which corresponds to circuit 2 given in figure 5a . we have shown in our recent work [cit] that deciding whether or not there exists a switching control unit that makes the above constraints be satisfied is an np-complete problem. to alleviate this issue, we have proved that dealing with reciprocal passive control units leads to a convex problem in the form of lmi. thus, it is henceforth assumed that the control unit being designed is a general reciprocal (linear) passive network (see circuit 3 in figure 5b )."
"it is worth mentioning that the optimization problem proposed in theorem 1 is indeed an lmi problem. in what follows, an immediate corollary of this theorem will be presented."
"in fig. 10, the green boxes represent simplified sub-tasks of the workflow application, whereas the blue boxes represent the data transferred between the sub-tasks. the first sub-task aligns input reads to the given genome using the bowtie program [cit] . (c) reconfiguration actions. fig. 11 . violations, utilization and reconfiguration actions for ten autonomic management scenarios using bioinformatics workflow. unaligned reads are then divided into shorter sub-sequences which are further aligned to the reference genome in the next subtask. if sub-sequences coming from the same read were aligned successfully to the genome, that may indicate that this read was straddling a 'gap' in the gene, falling on a so-called splicejunction. after verification of candidate reads falling on splice junctions, these and the reads that were aligned in the first subtask are combined to create an output with a comprehensive list of localized alignments. we demonstrate by simulation that the rule-based approach can guarantee the resource requirements in terms of cpu, memory and storage for the execution of the workflow in a resourceefficient way."
"in the following we briefly describe the bioinformatics workflow in more detail. we here consider next generation sequencing (ngs), a recently introduced high-throughput technology for the identification of nucleotide molecules like rna or dna in biomedical samples. the output of the sequencing process is a list of billions of character sequences called 'reads', each typically holds up to letters that represent the individual dna bases determined. lately, this technology has also been used to identify and count the abundances of rna molecules that reflect new gene activity. we use the approach, called rna-seq, as a typical example of a scientific workflow application in the field of bioinformatics."
"since the invention of the radio in the end of nineteen century, there have been major revolutions in the architecture of the radio but there has been a central component that all of them had in common. in all of these systems, the information is generated before the antenna and the role of the antenna is only to efficiently transmit the signal. different schemes are developed to add information to a carrier signal (e.g. a sinusoidal waveform). the act of adding information to a carrier signal, known as modulation, can be achieved by altering some properties of the carrier signal such as its frequency, amplitude, or phase. in a broad range of wireless communication systems, the information is generated in low frequencies (base-band frequency region), and then upconverted to a carrier frequency (rf) via a mixer that acts as a multiplier."
"with scenarios 2.* we can reduce the sla violations to a minimum. we completely avoid violations for storage in all subscenarios, as well as for memory in all but one sub-scenarios. also cpu violations can be reduced to 0.6% for sub-scenarios 2.1 and 2.4, and still achieve a maximum sla violation rate of 2.8% with scenario 2.8. the average sla violation rate can be lowered to 0.2% in the best case. scenario 3, of course, shows no violations. however, it is unlikely to know the maximum resource consumption before workflow execution."
"as the crucial parameters for cbr and the rule-based approach differ, we define scenarios for both approaches separately, but still compare them to the aforementioned six performance indicators."
"higher values for α strengthen the utilization of resources, whereas lower values the non-violation of sla parameters. we further note that c(x) describes a case only with respect to parameter x. e.g., we say that a violation has occurred in c(x), when in case c the parameter x was violated. we define the violation function for every parameter x as follows:"
"the remainder of this work is divided as follows: in section 2 we present related work. section 3 gives some background information by explaining the mape-k loop and the fosii project. in section 4 we structure the problem into the mentioned escalation levels, and in section 5 we describe how to use the two km techniques (cbr and rules) to tackle the resource allocation problem for a certain escalation level. section 6 shows the evaluation of both approaches, especially focusing on the rulebased approach. section 7 concludes this contribution and points out future work."
"the objective of the current paper is to study a nfdam system with the aim of finding the set of points that can be generated on the signal constellation diagram. as a secondary goal, it is desired to investigate the variation of the input impedance of the antenna and identify the region that the antenna input impedance belongs to."
"the vision of cloud computing is to provide computing power as a utility, like gas, electricity or water [cit] . for the underlying infrastructure this means that it has to deal with dynamic load changes, ranging from peak performance to utilization gaps. this brings up two issues: on the one hand, the management of a cloud computing infrastructure has to guarantee pre-established contracts despite all the dynamism of workload changes. on the other hand it has to efficiently utilize resources and reduce resource wastage. as to the former, the pre-established contracts, so called service level agreements (slas), contain service level objectives (slos) that represent quality of service (qos) goals, e.g., ''storage should be at least 1000 gb'', ''bandwidth should be at least 10 mbit/s'' or ''response time should be less than 2 s'', and penalties that have to be paid to the customer if these goals are violated."
"a huge advantage of scenarios 2.* is that they do not run into any crucial sla violation (except for scenario 2.3), but achieve a higher utilization as compared to scenario 3. as to the reallocation actions, of course, scenario 1 and 3 do not execute any, but also for the autonomic management in scenarios 2.*, the amount of executed reallocation actions for most scenarios stays below 10%. only scenario 2.7 executes actions in 19.8% of the cases on average of the time. five out of eight scenarios stay below 5% on average."
"where id represents the sla id, and m i and p i the measured (m) and provided (p) value of the sla parameter x i, respectively."
"this work can be integrated into the foundations of selfgoverning ict infrastructure (fosii) project [cit], but is on its own completely self-sufficient. the fosii project aims at developing an * correspondence to: argentinierstrasse 8/184-1, 1040 wien, austria. tel.: +43 1 58801 18457."
as to the first problem we assume that each sla has a unique identifier id and a collection of slos. slos are predicates of the form
"a utilization utility of 1 is retrieved if less over-provisioning of resources takes place in the final case than in the initial one, and a utilization utility of −1 if more over-provisioning of resources takes place in the final case than in the initial one. the whole cbr process works as follows: before the first iteration, we store the mentioned initial cases consisting of an initial measurement, an action and a resulting measurement. then, when cbr receives a new measurement, this measurement is compared to all cases in the kb. from the set of closest cases table 3 resource policy modes."
"at first, in the analysis of rna-seq data, the obtained sequences are aligned to the reference genome. the aligner presented here, tophat [cit], consists of many sub-tasks, some of them have to be executed sequentially, whereas others can run in parallel (fig. 10) . these sub-tasks can have different resource-demand characteristics: needing extensive computational power, demanding high i/o access, or requiring extensive memory size."
"the rest of the paper is organized as follows. some preliminaries on nfdam systems are provided in section ii and the problem is introduced accordingly. the main results are then presented in section iii. the efficacy of this work is elucidated in a practical example in section iv. concluding remarks are drawn in section v. finally, a proof is given in the appendix."
"if there exist such matrices m, n satisfying the above constraints, then one candidate for the admittance of the passive control unit at the frequency ω 0 is:"
"the first part of theorem 2 states that the feasibility constellation region d u is simply a circle with known radius and center. this result significantly benefits the design of an optimal modulation scheme, because finding a maximal set within a circle can be performed systematically. furthermore, the second part of theorem 2 says that the feasibility admittance region is again a circle, which is a useful fact for the modulation design. to be more precise, denote the impedance of the input source and the input impedance of the antenna system with z d in and z in, respectively. the reflection coefficient at the input of the antenna, denoted by t, is equal to to maximize the power accepted by the antenna, the norm of this reflection coefficient must be minimized. since the feasibility admittance region is a circle, the feasibility impedance region, denoted byq, is also a circle whose radius and center can be obtained in terms of (o"
the main challenge in this work is to evaluate km techniques for autonomic sla enactment in cloud computing infrastructures that fulfill the three following conflicting goals: (i) achieving low sla violation rates; (ii) achieving high resource utilization such that the level of allocated but unused resources is as low as possible; and (iii) achieving (i) and (ii) by as few time-and energy-consuming reallocation actions as possible. we will call this problem the resource allocation problem throughout the rest of the paper.
"we then group these actions into so called escalation levels that we define in table 1 . the idea is that every problem that occurs should be solved on the lowest escalation level. only if this is not possible, the problem is tried to be solved on the next level, and again, if this fails, on the next one, and so on. the levels are ordered in a way such that lower levels offer faster and more local solutions than higher ones. at every level it has to be decided, whether the proposed action should be executed or not, because it is important to know when to do nothing, since every reallocation action is time and energy consuming. in fact, for every level there is the possibility not to execute the proposed action. if the proposed action is not executed, then the decision-making process will stop and not evaluate whether the next escalation level should be considered or not."
"it is well-known that a broad class of problems in circuits, electromagnetics, and optics can be formulated as an optimization over the parameters of a multi-port passive network javad lavaei and john c. doyle are with the department of control and dynamical systems, california institute of technology (emails: lavaei@cds.caltech.edu, doyle@cds.caltech.edu)."
"the method receivemeasurement inputs new data into the kb, whereas the method recommendactions outputs an action specific to the current measurement of the specified sla. the simulation engine traverses all parts of the mape-k loop as can be seen in fig. 6 and described in section 3. the simulation engine is iteration based, meaning that in one iteration the mape-k loop is traversed exactly once. (in reality, one iteration could last from some minutes to about an hour depending on the speed of the measurements, the length of time the decision making takes, and the duration of the execution of the actions, for example migrating a resource intensive vm to another pm.) the monitoring component receives monitoring information from either synthetic or real-world workload from the current iteration. it forwards the data into the knowledge base (1). the knowledge base contains representations of all important objects in the cloud and their characteristic information. these objects are the running applications, the virtual machines, and the physical machines with the current state of their cpu power, memory, storage, etc., the corresponding slas with their slos, and information about other clouds in the same federation. furthermore, the kb also has representations of the inserted measurements, and the available actions to execute (these have to be pre-defined). finally, the kb also contains a decision mechanism that interprets the state of available objects in order to recommend a reconfiguration action. this mechanism can be substituted by any km technique; as already mentioned, we used cbr and a rule-based mechanism. the next step in the mape loop is the analysis component, which queries the kb for actions to recommend (for a specific sla id) (2); these actions are then returned to the analysis component (3) . the planning component schedules the suggested actions, and the execution component executes them. the changed state configuration of the cloud objects are automatically reflected in the kb (4) . the monitoring and the execution components are simulated. this means that the monitoring data is not measured on a real system during the simulation, even though it handles input measured at a real system or synthetic workloads generated beforehand (see sections 6.3 and 6.4). the execution component updates the object representation of the manipulated objects in the kb, but obviously does not actually manipulate real-world objects."
"a more general approach also taking into account the cost of actions represents the definition of a generic cost function that maps sla violations, resource wastage and the costs of executed actions into a monetary unit, which we want to call cloud eur. the cost function is currently not evaluated within the simulation engine, it is a value calculated after the simulation for comparison reasons. thus, the recommended actions do not depend on the specific functions we assumed. however, it could be incorporated into the kb in order to adjust and learn the tts for every resource r."
"summing up, the simulation shows that learning did take place (and cost some time) and that cbr is able to recommend right actions for many cases, i.e., to correctly handle and interpret the measurement information that is based on a random distribution not known to cbr. fig. 9 shows the same evaluation for the rule-based approach evaluating the aforementioned eight scenarios. from fig. 9(a) we learn that in terms of sla violations scenario 1 achieves the best result, where only 0.0908% of all possible violations occur, and scenario 8 yields the worst result, with a still very low violation rate of 1.2040%. in general, the higher the values are for tt high, the worse is the outcome. the best result achieved with cbr was at 7.5%. thus, the rule-based approach achieves an up to 82 times better performance with the right tts set, and still a six times better performance in the worst case. fig. 9(b) shows resource utilization. we see that the combination of high tt low and high tt high (scenario 8) gives the best utilization (84.0%), whereas low values for tt low and tt high lead to the worst utilization (62.0% in scenario 1). still, compared to cbr which scored a maximum of 80.4% and a minimum of 51.8%, the rule-based approach generally achieves better results."
"therefore, we define the sla shown in table 5 for tophat with the maximum amount of available resources on the physical machine on which we are executing it. the physical machine has a linux/ubuntu os with a intel xeon(r) 3 ghz cpu, two cores, 9 gb of memory, and 19 gb of storage. for cpu power, we convert cpu utilization into mips based on the assumption that an intel xeon(r) 3 ghz processor delivers 10 000 mips for 100% resource utilization of one core, and linearly degrades with cpu utilization."
"the idea of this rule-based design is that the ideal value that we call target value tv(r) for utilization of a resource r is exactly in the center of region 0. so, if the utilization value after some measurement leaves this region by using more (region −1) or less resources (region +1), then we reset the utilization to the target value, i.e., we increase or decrease allocated resources so that the utilization is again at"
"define d u and q u to be the feasibility constellation and admittance regions, respectively, in the case when the constraints given in (1) do not exist. in other words, d u and q u are meant to characterize the sets of all possible v 1 and y in which could be generated via a passive control unit. note that d and q are contained in d u and q u, respectively, and therefore finding d u and q u leads to understanding how restrictive the imposed constraints are. this is particularly important for a practical design because if the constraints turn out to be too restrictive, the designer may need to relax the constraints to better utilize the system. the following lemma will be later used to study the shapes of d u and q u ."
"to compare the achievable performances of switching and passive control units, denote with d s the feasibility region for v 1 under switching control units. finding the exact shape of d s requires computing v 1 for all possible switchings, i.e. 2 50 combinations. since this may not be possible, a number of switching networks are generated at random and the corresponding values of v 1 are plotted in figure 8a . it can be seen that even though a passive network has far more free parameters than a switching network, the region d u is a fairly good approximation of d s, which can be used for finding the hard limits on the switching performance."
"cloud infrastructures have to be taken into account, too, and negotiations have to be started with them as well. also the rule-based approach benefits from this hierarchical action level model, because it provides a salience concept for contradicting rules. without this concept it would be troublesome to determine which of the actions, e.g., ''power on additional pm with extra storage and migrate vm to this pm'', ''increase storage for vm by 10%'' or ''migrate application to another vm with more storage'' should be executed, if a certain threshold for allocated storage has been exceeded. the proposed km approaches will present a solution for escalation level 1. fig. 2 visualizes the escalation levels from table 1 before and after actions are executed. fig. 2 (a) shows applications app1 and app2 deployed on vm1 that is itself deployed on pm1, whereas app3 runs on vm2 running on pm2. fig. 2(b) shows example actions for all five escalation levels. the legend numbers correspond to the respective numbering of the escalation levels."
"the base-band data forms a series of complex numbers that can be separated into real and imaginary parts. the first set is called the in-phase signal (i) that is the real part of the complex signal and the second set is called the quadrature-phase signal (q) that is the imaginary part of the complex signal. a simple constellation diagram can be used to represent this complex signal, where each point of the diagram corresponds to some information symbol. after modulating an incoming signal, a conventional antenna propagates the modulated signal in almost all directions. this signal can ideally be received in different directions after some time delay and power attenuation. this implies that the signal cannot be transmitted securely only in some desired directions, and indeed the underlying transmission technique does not differentiate between desired and undesired directions. this fact is illustrated in figure 1 ."
"the number of violations in scenario 1 reach 41.7% for cpu and memory, and 49.4% for storage, which leads to an average of 44.3%. (for better visibility, these results have been excluded from fig. 11(a) .) thus, we experience violations in almost half of the cases. this is especially crucial for parameters memory and storage, where program execution could fail, if it runs out of memory or storage, whereas for a violation of the parameter cpu, we would ''only'' delay the successful termination of the workflow."
"an important question arises as to what modulation point α and input impedance y in can be generated via a passive control unit. addressing this significant problem is the core of the present work. define d to be the set of all modulation points α that can be produced by a passive network in circuit 3 such that the constraints given in (1) are satisfied. likewise, define q to be the set of all feasible input admittance y in . note that each of the sets d and q can be identified by a planar region, because every complex number has a 2-d representation. the sets d and q will be referred to as feasibility constellation region and feasibility admittance region, respectively. it is critical to know the regions d and q before designing an optimal modulation/demodulation scheme. the rest of the paper aims to investigate the shapes of these regions."
"for the rule-based approach we first introduce several resource policy modes to reflect the overall utilization of the system in the vm configuration rules. dealing with sla-bound resource management, where resource usage is paid for on a ''pay-asyou-go'' basis with slos that guarantee a minimum capacity of these resources as described above, raises the question, whether the cloud provider should allow the consumer to use more resources than agreed. we will refer to this behavior as overconsumption. since the consumer will pay for every additional resource, it should be in the cloud provider's interest to allow over-consumption as long as this behavior does not endanger the slas of other consumers. thus, cloud providers should not allow over-consumption when the resulting penalties they have to pay are higher than the expected revenue from over-consumption. to tackle this problem, we introduce five policy modes for every resource that describe the interaction of the five escalation levels. as can be seen in table 3 the policy modes are green, green-orange, orange, orange-red and red. they range from low utilization of the system with lots of free resources left (policy mode green) over a scarce resource situation (policy mode orange) to an extremely tight resource situation (policy mode red), where it is impossible to fulfill all slas to their full extent and decisions have to be made which slas to deliberately break and which applications to outsource."
in is the admittance of the source delivering power to the transmitting antenna. the constraint (1a) is intended to match the input admittance of the transmitting antenna to that of the input source in order to minimize the reflected power.
"in this section we evaluate the two presented approaches with several different synthetic and real-world workload data. for this purpose, we present a km-agnostic simulation engine that implements the autonomic control loop and simulates executed actions and evaluates their quality responding to the workload data at stake."
"yet, evaluating the km system on a real environment is not a trivial task because of two reasons: first, cloud infrastructures usually are huge data centers consisting of hundreds of pms and even more vms. thus, a first step is to simulate the impact of autonomic management decisions on the cloud infrastructure to determine the performance of the km decisions. consequently, we designed and implemented a simulation engine that mimics the mape-k cycle on large clouds. second, workload data for a large number of vms has to be provided as input for the simulation. we decided to go two ways: on the one hand, we generated synthetic workload data categorized into different workload volatility classes. these workload volatility classes are determined by the speed and intensity of workload change. on the other hand, we gathered real world data from monitoring scientific workflow applications in the field of bioinformatics [cit] . these workflows need a huge, yet unpredictable and varying amount of resources, and are thus -due to the needed flexibility and scalability -a perfect match for a cloud computing application [cit] ."
the goal of this research field is to enact slas in a resourceefficient way with little human-based interaction in order to guarantee the scalability and strengthen the dynamic behavior and adaptation of the system. autonomically governing cloud computing infrastructures is the investigated method leading to this goal.
"the percentage of all executed actions as compared to all possible actions that could have been executed is shown in fig. 9(c) . one observes that the greater the span between tt low and tt high is, the fewer actions have to be executed. most actions (60.8%) are executed for scenario 7 (span of only 5% between tt values), whereas least actions (5.5%) are executed for scenario 3 (span of 60% between tt values). cbr almost always recommended exactly one (out of two possible) actions and hardly ever (in about 1% of the cases) recommended no action."
"fourthly, compared to other sla management projects like sla@soi [cit], the fosii project in general is more specific on cloud computing aspects like deployment, monitoring of resources and their translation into high level slas instead of just working on high-level slas in general service-oriented architectures."
"when it comes to the overall costs of the scenarios (cf. fig. 12(a) ), all 2.* scenarios approach the result achieved by the best case scenario 3. scenario 1 sums up costs of 4493.6, and has therefore been omitted in the figure. furthermore, the lowest cost is achieved using scenario 2.6, which is even lower than the cost for scenario 3. this is possible, because scenario 2.6 achieves a very good utilization and sla violation rate with a very low number of reallocation actions. also resource allocation efficiency for scenarios 2.* as shown in fig. 12(b) achieves unambiguously better results than for scenario 1 (rae of 48.2%). furthermore, all scenarios of the second category achieve a better rae than the rae of scenario 3 (69.3%)."
"a vast majority of problems in circuits, electromagnetics, and optics can be regarded as the analysis and synthesis of linear systems in the frequency domain. these systems, in the circuit theory, consist of passive elements including resistors, inductors, capacitors, ideal transformers, and ideal gyrators [cit] . since the seminal work [cit], there has been remarkable progress in characterizing such passive (dissipative) systems using the concept of positive real functions. this notion plays a vital role not only in circuit design but also in various control problems [cit] ."
"the above algorithm obtains an approximating polygon after solving at most 2 algorithm 1 presents an efficient optimization-based method to approximate the unknown region d by a polygon. a derivative of this algorithm can be used for designing an optimal rectangular quadrature amplitude modulation (qam) scheme. given a positive constant d, consider a constellation diagram j in the from of rectangular qam with the points:"
"by applying lemma 1 to the constraints (13) and (15), it can be concluded that d u is a circle with the aforementioned properties. the proof for the set q u can be carried out in the same line, after noting that:"
"utilized (2) table 2 . first, we deal with the measured value (1), which represents the amount of a specific resource that is currently used by the customer. second, there is the amount of allocated resource (2) that can be used by the customer, i.e., that is allocated to the vm which hosts the application. third, there is the slo agreed in the sla (3). a violation therefore occurs, if less is provided (2) than the customer utilizes (or wants to utilize) (1) with respect to the limits set in the sla (3). considering table 2 we can see that rows 1 and 3 do not represent violations, whereas row 2 does represent an sla violation. in order to save resources we envision a speculative approach: can we allocate less than agreed, but still more than used in order not to violate an sla? the most demanding questions are how much can we lower the provisioning of resource without risking an sla violation. this heavily depends on the characteristics of the workload of an application, especially its volatility."
"one of the imminent problems that come up when dealing with the mape-k loop is to define possible actions that can be executed at the end of the loop. due to the plethora of possible reconfiguration actions in clouds, e.g., increasing/decreasing available memory or storage for virtual machines (vms), choosing vms to migrate to selected physical machines (pms), determining pms to power on/off, etc., it is not trivial to identify the most beneficial action in a certain situation. on the one hand it is not trivial to retrieve and store all necessary information in a cloud infrastructure. on the other hand, and more important in our work, dealing with the complexity of recommending an action based on this information is, as we will see, in most cases np-hard. to tackle this, we structure all possible actions and organize them in a hierarchical model of so called escalation levels."
") and r ′ . now, the minimization of the reflected power amounts to finding a point z in in the circleq such that ∥t ∥ is minimum. this problem has a simple analytic solution. in other words, the optimal input impedance that the antenna system accepts by using a passive control unit can be obtained routinely."
"listing 1: rule ''storage_increase'' 1 rule \" s t o r a g e _ i n c r e a s e \" 2 when 3"
"concerning related work, we have determined four different ways to compare our work with other achievements in this area. whereas the first level compares other works dealing with sla enactment and resource efficiency, the second one considers the area of knowledge management, and the third one compares commercial products to our approach. fourthly, the fosii project is briefly related to other projects in this field."
"thus, we conclude that by using the suggested autonomic management technique, we can avoid most costly sla violations, and thus ensure workflow execution, together with a focus on resource-efficient usage. all this can be achieved by a very low number of time-and energy-consuming vm reallocation actions for many of the autonomic management scenarios."
"thirdly, commercial cloud iaas platforms such as amazon ec2 [cit], rackspace [cit] or rightscale [cit] have a very limited choice of preconfigured and static vm resource provisioning types. amazon ec2 only offers vm instance types such as small, medium or large with predefined storage, computing units, and memory without the possibility of reconfiguring or fine-tuning them beforehand, not to mention during runtime. rackspace only offers storage on the iaas level, and rightscale focuses more on integrating different iaas platforms such as amazon ec2 or rackspace into a holistic view."
"note that if the optimization problem in theorem 1 is feasible, thenṽ 1 andṽ 2 turn out to be equal to the two subvectors of the output voltages, i.e.:"
"case based reasoning is the process of solving problems based on past experience [cit] . in more detail, it tries to solve a case (a formatted instance of a problem) by looking for similar cases from the past and reusing the solutions of these cases to solve the current one. in general, a typical cbr cycle consists of the following phases assuming that a new case was just received: to adapt cbr to our problem, three issues have to be solved. first, it has to be decided how to format an instance of the problem. second, it has to be decided when two cases are similar. third, good reactions have to be distinguished from bad reactions."
"consider the antenna configuration depicted in figure 6, which consists of a transmitting dipole antenna, 10 metal reflectors each with 5 ports (antenna parasitic elements), and a receiving dipole antenna located at the far field in the upward direction. there are 52 ports as follows:"
we assume that customers deploy applications on an iaas cloud infrastructure. slos are defined within an sla between the customer and the cloud provider for every application.
"the goal of the simulation engine is to evaluate the quality of a km system with respect to the number of sla violations, the utilization of the resources and the number of required reallocation actions. furthermore, the simulation engine serves as an evaluation tool for any km technique in the field of cloud computing, as long as it can implement the two methods of the kb management interface: the parameter slaid describes the id of the sla that is tied to the specific vm, whose provided and measured values are stored in the arrays provided and measurements, respectively (cf. section 5.1). the list violations contains all sla parameters being violated for the current measurements."
"for an iaas provider omitting escalation level 2, the sequence of these escalation levels is quite obvious: if vm sizes are not changed in escalation level 1, there is no need to trigger escalation level 3 as vms have not changed, and no better allocation of vms to pms can be found, if the previous one was already optimal. however, if vm sizes were changed, escalation level 3 can still come to the conclusion that vm migrations are unnecessary. on the other hand, if vm migrations were recommended, some pms could be then turned off in escalation level 4. similarly, if no migrations were triggered, thinking about turning off pms is unnecessary, as no pms run idle now that have not been running idle before. finally, if all the previous actions were successfully executed without the help of another cloud provider, there is no need to consider outsourcing applications. only if the last possibility failed, outsourcing applications should be considered. (other business incentives for outsourcing applications such as cheaper execution costs in other clouds, etc., are not considered here.) for providers of other cloud delivery models such as saas or paas, the sequence of placing application migration after vm reconfiguration is arguable; another model could also propose an inverse sequence for these two levels."
"(21) on the other hand, since the eigenvalues of a hamiltonian matrix are symmetric, it follows from the inequality (8) that the eigenvalues of the symmetric matrix given in the left side of (8) are all in the interval (−1, 1). hence:"
"in this case, the fosii project will serve as a running example. we will describe how the km approach relates to other components of the fosii project. generally, the project distinguishes between system set-up and run time. during system set-up, applications, their corresponding slas and used infrastructure are tailored and adapted. once the application is deployed, we consider monitoring, knowledge management and execution phases during run time. in this section, in particular, we focus on the adaptation, monitoring, and knowledge management phases, as shown in fig. 1 . thus, the mape-k loop is extended to the a-mape-k loop, where the additional a stands for the adaptation phase during system setup. this adaptation phase, however, should not be confused with later adaptation and re-configuration of resources during system run time. quite evidently, we especially focus on the knowledge management phase in this paper. the three mentioned phases are described as follows:"
"the next step will be to move from simulation to a real cloud testbed. furthermore, the presented methods still involve some user-interaction for parameter tuning. thus, it will be of great interest to autonomically determine crucial parameters of the presented methods and to adapt them based on current performance."
"19) where z represents the set of integer numbers. it is desired to obtain the intersection of d and j, denoted by q p . note that q p consists of those points in the qam diagram j which can be generated by a passive control unit. the following algorithm can be used for this purpose."
"that d is a convex set has an important practical implication: to design an optimal modulation scheme, it is required to identify a maximal set of points in the feasibility constellation region with the minimum point-to-point distance greater than d, where d is a given positive number. if d were a non-convex set with a complicated shape, finding such a maximal set would be a highly complex problem. in contrast, the convexity of the set d simplifies the design problem significantly, as noted below."
"the quality of the decision making can ultimately be judged by the number of occurred sla violations, resource wastage and the number of needed reallocation actions."
"content based image retrieval (cbir) system has been an active research area for more than two decades now, still the field is evolving and there is huge scope for researchers to establish it as a matured system. the cbir system comprises of feature extraction of images followed by matching and retrieving relevant images in order of its similarity with the query image. features of an image are extracted, considering image as pixel matrix and processed to represent the information. feature representation plays very important role in cbir systems to obtain relevant images. the handcrafted feature set for an image, may not exactly represent human perception, this is termed as semantic gap. for all these years since its inception, researchers have been working and proposed various feature representation techniques for cbir systems, but this still remains a challenging issue because of the semantic gaps between low-level pixel information of images and highlevel human perception. although the handcrafted features are proven to be efficient for the systems but have limitations in terms of covering all the characteristics of an image."
"the ca3306 is a cmos parallel adc designed for applications demanding both low-power consumption and high speed digitization. it is a 6-bit 15 msps adc with a parallel read out with single 5 v supply. the power consumption is as low as 15 mw, depending upon the operating clock frequency. it may be directly retrofitted into ca3300 sockets, offering improved linearity at a lower reference voltage and high operating speed with a 5 v supply. the high conversion rate of this adc is ideally suited for digitizing high speed signals in shm application. if a higher resolution is needed, the overflow bit makes the connection of two or more ca3306s in series possible to increase the resolution. also, two ca3306s may be used to produce a 7-bit high speed converter that doubles the conversion speed; this will increase the sampling rate from 15 mhz to 30 mhz [cit] ."
"the manual operation of fd delivery and deployment leads to an uncertain axial displacement during the interventional procedure. an axial displacement test (adt) is designed to investigate the robustness of a given fd structure. for a given fd structure, we sequentially added the variable θ �, ranging from −π to π, to the starting phase θ of each helix to mimic the axial displacement along the centerline of the parent artery. a cfd simulation was subsequently performed to calculate the difference in velocity resulting from the displacement."
"in the last few years, there is an emerging paradigm in cbir systems which allows system to learn feature extraction kernel. this is made possible by the use of dl, cnns and its architecture that embeds feature extraction with in. here in the presented work, we trained cnn kernels for feature extraction, and obtained the feature map from the middle layers. the statistical parameters like contrast (standard deviation) and entropy of the feature map are calculated to use it as feature set for the purpose of image comparisons. feature map as it is, and its statistical parameters, one at a time was used as features in conventional cbir model. retrieval results was compared in both the cases. it was observed that the results are better when statistical parameters of feature map obtained from cnn are used as features. this finding opens up a new alley in the field of image feature representations. more evocative parameters can be experimented to represent the high dimension feature map from the cnns that improves the retrieval performance."
"the tone signal in each burst is used to measure doppler frequency shift by identifying the peak of the fft of the signal and a maximum likelihood estimator (mle) of frequency [cit] . the if of the ring alert signal is 28.270833 mhz. it repeats every 48 frames, which means that the repetition the bandwidth of this burst is approximately 26.6667 khz, and the signal duration is approximately 6.8 ms. the if acquired by mle is 28.244935683 mhz, which means that the corresponding doppler-shift is 25897.317 hz. we can see that the iridium next signal consists of three parts, which are tone (no modulation), unique word (bpsk modulation) and information (qpsk modulation), as shown in the bottom-right plot. similar results can still be obtained after performing the same step for the primer message signal. actually, the duration of the real collected burst signals are between 6.5 ms and 20.32 ms, and the duration of the tone is approximately 2.6 ms. on the other hand, for the ring alert channel, the two bursts with corresponding intervals that are rigorously 90 ms and multiples of 90 ms in the actual data capture means that they are from the adjacent and the non-adjacent beams of the same satellite. if not, they are transmitted by different satellites."
"in addition, our optimization results have revealed a practical approach for the conventional homogeneous fd devices to improve its flow diversion efficiency, that is, compacting fd wires into boi areas during deployment may achieve a marked difference in blocking the aneurysmal inflow."
"the physical significance of dpdop, as well as dhdop and dvdop, is the time spent by the doppler positioning solution shifts from the true position to the estimation due to the existence of measurement errors."
"in today's world with large set of images being added every moment, it is observed that hand-designed features will take huge effort and time in image retrieval. whether it is audio, visual or text data, in every application features plays an important role. to address this issue, andrew ng and his team are actively working on dl algorithms which automatically learns feature representation (from unlabelled data) [cit] . inspired by the cortical (brain) computations the algorithms based on massive artificial neural networks are developed. andrew, as a part of his work found and led a project at google for building a huge dl algorithm which resulted in highly distributed neural network with more than 1 billion parameters which were trained on 16,000 cpu cores. this resulted into self learning of high-level concepts like 'cats' from the unlabelled youtube videos."
"the entire 30 min of data are divided into 5000 data blocks, and the doppler-shift will be acquired if the data block contains the burst. the left and right plots in fig. 12 show the doppler-shift curves of the 7 and 11 downlink-only channels. the receiver can see 7 iridium next satellites during 30 min, and at most 2 satellites can be viewed simultaneously. the first visible satellite just passes the top of the receiver. the three longer curves show that the corresponding satellites are on the same track as the first visible satellite. the corresponding subsatellite points of these four satellites the top left plot of fig. 13 shows that the north-direction position error has a smaller mean value. however, the solutions are bad in the east direction, the corresponding mean error is relatively large because it is almost 180 m, and the fluctuations in error magnitude can be 380 m. the statistical results of height aiding are shown in the bottom left and right plots of fig. 13 . the largest mean errors of east-direction and north-direction are approximately 46 m and 24 m, respectively, and the east-direction error has a relatively larger fluctuation that is caused by the special iridium orbital characteristic."
"the measurements are taken from the acquisition process since the iridium satellite burst signals are discontinuous. generally, the signals are much stronger than the noise. doppler coarse and fine measurements are obtained by the fast fourier transform (fft) and maximum likelihood estimator (mle) algorithms [cit], where the latter can improve measurement accuracy which are effected by the fft resolution. iridium satellite orbital data can be provided in norad two-line element set format (tle), which consists of orbital parameters and time information. the sgp4 prediction model is used for the purpose of reconstruction. iridium tle sets are periodically updated once or twice every day and can be acquired directly from the norad website celestrak.com. the iridium satellite position errors based on tle can be from 100 m to several kilometers, and the corresponding satellite velocity errors can be approximately 3 m/s, and their values are rather smaller in the radial direction than for cross and along direction."
"another limitation from the viewpoint of clinical practice is that sending the optimized fd to the aneurysm location could be a challenge for interventionists, since a larger device displacement after deployment may considerably increase the aneurysmal inflow. this might be solved in the future by embedding a reliability test into the optimization loop to achieve improved stability."
"piezoelectric sensors are the most commonly used sensors; they can transmit and receive guided waves such as lamb waves in solids that can be used for damage detection. these lamb waves are much more cost effective as well as reliable as they are sensitive to change in structures. to identify the damage, the difference signal is acquired and compared with a damage-free signal. for structural damage localization, irrespective of the geometrical or imaging method used, the key to this process is the acquired time of flight and amplitude of the response to the signal. these factors directly determine the precision of damage localization. the time of flight is directly dependent on the properties of the material such as modulus of elasticity and modulus of rigidity. using piezoelectric sensors, there are two basic active sensing techniques used to detect damage in structures, namely, pitch-catch and pulse-echo. in the pitch-catch technique, two pzt sensors are used: one as transmitter that located before the damage and the other as a receiver that located after the damage. the first pzt sends a signal, and the received signal by the second pzt is then used to determine the location and/or the extent of the damage [cit] . on the other hand, the pulse-echo technique relies on the reflected wave from the damage, and one ptz transducer is used for both transmitting and receiving the signal. figure 1 shows both techniques."
"shm is a vital tool to improve the safety and maintainability of critical structures such as bridges and buildings. shm provides real time and accurate information about the structural health condition. it is a process of nondestructive evaluations to detect location and extent of damage, calculate the remaining life, and predict upcoming accident. shm has become challenging task with the increase in development and construction of structures along with the complexities involved in them. the demand for shm has also increased due to increase in the necessity to ensure safety of the structures as well as the human lives associated with it. shm is capable of detecting the damage early and make it feasible to take action before any loss occurs [cit] . diverse theories have been proposed and implemented to meet distinct requirements of structures. integration of the diverse theories has helped not only to improve the efficiency and performance of the shm systems but also to reduce the computational time and costs [cit] . in order to share data and ensure reliability, shm systems use networks [cit] ."
"we used the scalar parallel computing system [cit] at the institute of fluid science, tohoku university. the computational times for one stage of cfd simulation (using 256 cores) were approximately 30, 45, and 70 min for the s, c, and r models, respectively."
"it should be noted that we addressed merely the hemodynamic factors that may affect the fd performance, while the mechanical and material properties of the modified fds have not been investigated. in future work, we plan to include these parameters as a part of objective functions for optimization to improve fd's hemodynamic compatibility."
"we performed adts to investigate the robustness of the wire structures. the flow reductions achieved by the optimized fds were greater than those obtained using the homogeneous fds within displacement ranges of −0.25 to 0.25, −0.5 to 0.25, and −0.75 to 0.75 mm for the s, c, and r models, respectively (fig. 4) . it is indicated that a homogeneous wire configuration can nonspecifically prevent a strong inflow jet from passing through an aneurysm orifice; however, its flow-diversion efficiency is inferior to that of the optimal wire configuration when the device was desirably deployed."
"the doppler curves of the iridium ring alert signals and the position result are shown in fig. 15 and fig. 16, respectively. although the visible times of satellites are less than those under open sky, the snr is still optimistic and the doppler measurements are enough for positioning by the new positioning method. the estimation is 108m from the receiver position. fig. 17 shows the real-time processing result of gps l1 signals. at most time, only one satellite can be seen, and it is not kept being tracked by the static receiver, which means sometimes no gps satellite can be seen by receiver. as a result, the gps receiver does not work under such circumstances."
"the helix radius r varied with respect to the discrete points along the centerline of the parent artery and was associated with the maximum inscribed sphere radius (misr) corresponding to each point. the coordinates of discretized points and their corresponding misrs were measured using the open library vascular modeling toolkit vmtk v1. 2 [cit] . given the above fd parameters, the fd porosity can be calculated according to a previously defined equation [cit] where s total and s metal denote the surface area of the fd's generalized cylinder and the fd's metal wires, respectively. the device porosity was fixed at 80% in this study."
"first, the relationship between dhdop and satellite positions is verified. the positions of satellites s 1, s 2 and s 3 remain unchanged, and the elevation of satellite s 4 is changed by adjusting the corresponding true anomaly. the solid line in fig. 7 shows the dhdop as a function of the s 4 position. the dotted line represents the changing curve of dhdop when s 4 is located on the top of the receiver, and the true anomalies of s1, s 2 and s 3 are changed simultaneously. second, the relationship between dhdop, dvdop and the satellite velocities is verified. keeping the locations of s 1, s 2 and s 3 unchanged, the satellite velocity directions of s 3 and s 1 are changed simultaneously by adjusting the corresponding orbital inclinations. the curves in fig. 8 show the relationship between the doppler dop and the satellite velocity directions, and the x-axis represents the angular between the orbital inclination of s 1 or orbital inclination of s 3 and the orbital inclination of s 2 . finally, the relationship between doppler dop and satellite positions is verified. without loss of generality, the true anomalies of s1, s 2 and s 3 are all 3 degrees. the changing curves of doppler dop when the s 4 elevation is increasing are drawn in fig. 9 ."
"under the non-stent condition, a strong inflow jet and two outflow jets were observed (fig. 3d) . the boi was located in the proximal area of the neck, whereas the boos were symmetrically dispersed along both sides of the aneurysmal neck. after the initial fd implantation, the magnitude of the velocity of the boi decreased considerably (fig. 3b), and the outflow bundles were disrupted. for the optimized fd, the velocity magnitude of the boi was further reduced. a denser strut distribution inside the boi region was observed in the optimized case (fig. 3a) ."
"the ca3306 is operating at 5 v, which means that we cannot connect it directly to the raspberry pi which operates at 3.3 v. accordingly, a level converter in between is needed. the simplest way to do level conversion is to use a buffer such as cmos 74hc4050. as the buffer runs at 3.3 v, so it is necessary to place a pull-up resistor to 5 v behind the clock buffer. figure 10 shows the circuit diagram of both the adc and the buffer."
"the proposed iot platform consists of wi-fi module, raspberry pi, adc, dac, buffer, and pzt as shown in figure 9 . the two piezoelectric sensors are mounted on the structural and connected to a high speed adc. in the real case implementation, we will deploy the sensors in a way to catch all the possible damage. a buffer is used as a level conversion and to protect the raspberry pi. the raspberry pi generated the excitation signal and the dac converted it to analog. in addition, the raspberry pi, using the proposed shm technique, is used to detect if the structure has damage or not and the location of the damage if it is existing. moreover, the raspberry sends the structure health status to the internet server. the data is stored on the internet and can be monitored remotely from any mobile device. moreover, the internet server sends an alert if there is a damage in the structure. standards. the wi-fi module is used to send the data to the cloud."
"in order to evaluate the proposed platform, a healthy aluminum sheet and another sheet with damage were used. the unhealthy sheet has damage that is 30 cm far from the exciter piezoelectric sensor and 7 cm width as shown in figure 11 . the proposed platform is used to check both sheets and send the data to the internet server. these two sheets are tested separately by the proposed platform [cit] . figure 12 shows the test bed as located in the lab. the health status was sent to the internet server that hosted on thingworx [cit] . thingworx is a technology platform designed for the iot framework. it simplifies the creation and deployment of applications for smart, connected products by giving developers the tools they need to connect, build, analyze, experience, and collaborate about their \"things.\" figure 13 shows the results on thingworx. table 1 shows the percentage error of both the damage location and damage width between the actual sheets status and the data on the internet. results show that the proposed iot shm platform successfully checked if the sheet is healthy or not with 0% error. in addition, the proposed platform has a maximum of 1.03% error for the damage location and a maximum of 8.43% error for the damage width."
"in consideration of the possible correlation between porosity and post-stenting stenosis, we chose the starting phase θ of each fd helix as the modification parameter."
"the mcp4725 is a low-power, high accuracy, single channel, 12-bit buffered voltage output dac with nonvolatile memory (eeprom). its onboard precision output amplifier allows it to achieve rail-to-rail analog output swing. the mcp4725 is an ideal dac device where design simplicity and small footprint are desired and for applications requiring the dac device settings to be saved during power-off time [cit] . 6.6. piezoelectric sensors. the pzt transducer converts mechanical energy to electric signals or vice versa. they can work as an actuator to excite an elastic lamb wave based on the electrical signal applied to the pzt crystal. it can also be used as a transducer to transform the responding elastic lamb waves into an electrical signal. two ptzs sensors are mounted on structure: pzt1 (excitation) will send the excitation signal and pzt2 (receiver) will receive the signal."
"the raspberry pi 2 is a single-board computer. it features a full linux operating system with diverse programming and connectivity options. the onboard 900 mhz quad-core arm cortex-a7 cpu allows for swift computation and analysis of data obtained from several nodes and transducers. the operating system of raspberry pi is linux 3.18.5-v7+ and has 4 processors in one chip which is cpu armv7 processor rev 5 (v7l). it has a ram of 1 gb and maximum clock speed 900 mhz. normally, current drawn by raspberry pi 2 is 200 ma. the raspberry pi will be used to collect the structure health and push it to the cloud using wifi module."
"we begin to coexist and interact with smart interconnected devices that are known as the iot. the iot brings new opportunities for our society. with the maturity of the iot, one of the recent challenges in the structural engineering community is development of the iot shm systems that can provide a promising solution for rapid, accurate, and low-cost shm systems. moreover, the combination of shm, cloud computing, and the iot enabled ubiquitous services and powerful processing of sensing data streams beyond the capability of traditional shm system. cloud platform allows the shm data to be stored and used intelligently for smart monitoring and actuation with smart devices. in this paper, a complete shm platform embedded with iot is proposed to detect the size and location of damage in structures."
"for geometrical analysis of the effects of the satellite orbital errors on position accuracy, three effects must be taken into account. first, there is the effect of the satellite tangent velocity error. if the true satellite tangent velocity is v satreal, and v sat is the satellite tangent velocity error, then"
herex is the mean of the intensity values of all bins of four quantised histogram. it describes the brightness of the image. x i is the intensity value of each pixel and n is the total number of pixels. entropy measures the uncertainty of the intensity level distribution of the histogram bins. the high value of entropy indicates the distribution among the greater intensity levels in the image. the entropy is low for simple image and high for the complex image. [cit]
"where and are the mean values of the two sets of signal and and are standard deviations of the signature signal sets and r, respectively. if the two signals are similar, cc ≈ 1, which mean that the structure is healthy; otherwise, the structure has damage. in order to determine the size and location of the damage, a mathematical model is proposed based on the dimensional diagram shown in figure 3 . first, the speed of the wave must be calculated using a healthy model as in"
"deep learning (dl) which is evolving as a promising branch of machine learning, demonstrates high-level abstractions using deep networks which has multiple non-linear transformations. dl networks acts like human brains and learns concepts similar to human perception. dl learns image features automatically and maps the images directly to the output using its domain knowledge; thus overcoming the limitations of handcrafted features."
"according to (14), the greater the distance r sat − r user is, the larger the dacgply value is. the effect of measurement error v on dacgply influences the doppler dop value indirectly. since the distance of line-of-sight between the receiver and satellite is always too far, dacg-ply value is large. as a result, the dpdop is often larger than the geometric dilution of precision (gdop) of pseudorange positioning."
"satellite orbital errors and doppler observation errors are the main error sources for the iridium sops positioning. the satellite velocity information is also used in doppler positioning, which means positioning error analysis is more complex than that of the traditional pseudorange positioning because the effects of satellite velocity are need to be considered. doppler geometric dilution of precision (doppler dop) is not only related to constellation geometric distribution but also dynamic satellite trajectory, and it is a physical quantity with dimensionality. decreasing the doppler dop by reasonably choosing gnss satellites has significance for improving position accuracy. there is few research in the current literature on the issues mentioned above. we present deep research on these topics, focusing on the iridium sops positioning technique."
"where is the time it takes for a wave to travel to the damage and reflect back. after calculating the position, the next step would be determining the damage width. first, the proportion of the damage position to the total length, 1, could be determined by"
"generally, at least four satellites are needed for instantaneous doppler positioning. for the static receiver, doppler measurements of the same or different satellites at different moments can be used for positioning since the receiver coordinates are unchanged over time. however, the receiver clock frequency drift maybe changed at different moments due to the instability of clock. as a result, it is modeled with a three order polynomial and six unknowns need to be calculated. in this context, at least six doppler measurements of the same or the different satellites at different moments are needed in this case. with height aiding, the common unknowns can be reduced to five, and five doppler measurements are enough for positioning."
"for each case, optimization was performed until the lower temperature limit was reached; this required 916, 1035, and 976 iterations for the s, c, and r models, respectively. in all three cases, the fr improved markedly during the initial few hundred iterations and then stabilized (fig. 2) . the intra-aneurysmal average velocity without fd intervention and the r f values after fd implantation with the initial and optimal configurations are shown in table 2 . figure 3b, d depict the streamlines (color-coded by velocity magnitude) and iso velocity surface (corresponding to 0.01, 0.015, and 0.1 m/s for the s, c, and r models, respectively) of the three geometries with no stent and stents before and after optimization. figure 3a, c illustrate the velocity components perpendicular to and velocity vectors fig. 2 the sa procedure of s, c, and r models, respectively (vertical axis: r f, horizontal axis: sa iteration) generated from the aneurysm orifice. figure 4 illustrates the intra-aneurysmal average velocity differences with respect to the axial displacements for both the initial and optimized fd structures. aneurysmal inflow is observed as a flow bundle entering from the aneurysm orifice. the concept of bundle of inflow (boi) area describes the inflow feature of an aneurysm [cit] . likewise, the bundle of outflow (boo) area indicates the region(s) where the bloodstream exits an aneurysm. to demonstrate the unique features of different flow patterns, the red and blue arrows/circles are used in fig. 3b, c to indicate the boi and boo areas, respectively; the yellow circles with dotted lines in fig. 3a, b, d depict the"
"equation (4) reveals that satellite tangent velocity error has an effect on idccs similar to that of measurement errors. they both change the field angle of the corresponding idccs. second, there is the effect of direction error of the satellite tangent velocity. let β denote the angular deviation of satellite tangent velocity; then, the idccs1 with error is deviated β from the real idccs, as shown in the middle plot of fig. 4 ."
"if the receiver is located at the earth surface, the estimations from instantaneous doppler positioning are distributed in a region that is the intersection between dacgs and the earth surface. the relationship between measurement error v and field angle error θ can be written as"
"this article investigated the use of iridium signals that are treated as the noncooperative sops for positioning. the key innovation was the development of a new geometric analysis method of positioning errors of the instantaneous doppler positioning and the finding of the relationship between the constellation geometric distribution and the positioning accuracy. the new concept of iridium sops positioning and the effects of constellation geometric distribution on positioning errors were then tested using a real iridium next data set and stk simulations, respectively. the key findings from this work are described below."
"where is the total time the wave takes to travel from exciter to the sensor with the presence of the damage. from these equations, a triangle can be made as seen in figure 3 . is the base and 2 is hypotenuse of this triangle. 1 and 2 are very close in value and only equal if 1 is 1/2. 2 can be determined using the following:"
"we present some experimental results based on simulation data and real data. the simulations are carried out to verify the theoretical analysis of doppler dop above, and the real iridium next data collection and analysis are used to test the iridium sops positioning, as described below."
"in sect. 2, we describe the principle of the iridium satellite sops positioning technique. the instantaneous doppler positioning algorithm and a new method of geometric analysis of positioning error is proposed in sec. 3. the effect of constellation geometric distribution on position accuracy is discussed in sect. 4. experimental results are described in sect. 5. finally, the conclusions are presented in sect. 6."
"these equations are used if the damage has the same width above and below the axis. if the damage is not symmetrical, the two waves might reach the sensor at slightly different times resulting in two s, two s, and two 2 s to calculate the upper and lower part of the damage. from the above mathematical model, the damage location can be detected using (5) and the damage width can be determined using (7). one unique advantage of the proposed mathematical model is that it does not require high computational processing unit, allowing for implementation almost anywhere, anytime, and also easier integration with an iot platform. figure 4 shows the flow chart of the proposed shm technique."
"the analysis here considers two aspects, which are the character of single satellite dacgply and the relative position relationship of the dacgs of multiple satellites. point o is the satellite subpoint, and the solid line l and the dotted line l' denote the satellite trail and subtrack, respectively, as shown in the bottom plot of fig. 5 . line uu' is perpendicular to line l' and passes through point o. the dotted lines am and an are perpendicular to line l' and line uu', respectively. the field angle of true idccs is denoted by θ. point c in triangle amo and point d in triangle ano are symmetrical with respect to line ao. first, the receiver is on track l'. the larger θ is, the smaller the distance r sat − r user is, and then dacgply will be small. second, the receiver is on line uu'; then, field angle θ is constant. the closer the distance between the subpoint and receiver is, the smaller the distance r sat − r user is. as a result, dacgply will be small. third, the receiver at station a moves towards uu' along an. the field angle θ increases, and the distance r sat − r user decreases, which means that dacgply will be small. if the receiver moves towards l' along am, the field angle θ and distance r sat − r user decrease. whether dacgply gets larger or smaller is all in accordance with specific conditions. the dacgply at point d will be smaller than that at point c. finally, the leo satellite dacgply is smaller than the meo satellite dacgply when considering the above three conditions due to the characteristics of the leo orbit. the dacgply is also smaller for near-equatorial users, since the influence of the earth rotation on the relative motion of the satellite is greater."
"the geometric analysis shows that the effects of measurement errors and satellite velocity errors on the instantaneous doppler positioning are the changing of the idccs profile, while the effects of satellites' position errors shift the position of idccs. the positioning accuracy is mainly affected by satellite velocity errors instead of satellite position errors."
"the 3d patient-specific geometry of a human internal carotid artery (ica) with an aneurysm was reconstructed (r model, fig. 1a ). the parent artery had an inlet diameter (d 2 ) of 3.8 mm [cit] ."
"this section mainly presents a new analysis method for positioning errors: geometric analysis. we begin with a look at the principles of the instantaneous doppler positioning technique and the corresponding doppler dop. next, the influences of measurement errors and orbital errors on position accuracy are analyzed with the new method, and the mathematical models of position errors are presented."
"the robustness of an optimized fd is associated with the wire coverage of an aneurysm orifice. after homogeneous fd implantation, if the boi areas are axially distributed along the orifice (e.g. r model, fig. 3c ), the robustness of an optimized fd might be superior. in contrast, if the boi areas are radially distributed (e.g. s and c models, fig. 3c ), the optimized fd robustness could be inferior. the optimized fd wires concentrate in boi regions, whereas only a small number of wires are assigned in the remaining areas where large holes can be found. when axial displacement occurs, the inflow jet may cross the orifice through the holes, causing considerable fluctuations of r f ."
"this study has several limitations. we applied steady flow and newtonian fluid assumptions to reduce the computational cost, since the objective of this study is to develop a feasible and manufacture-oriented optimization approach for fd device. when computational cost is no longer a problem, the setting is readily changed by adopting time-dependent boundary conditions and a non-newtonian rheology in the lb solver."
"in this paper, a complete real-time iot platform for shm was proposed. the proposed platform consists of a wi-fi module, raspberry pi, dac, adc, buffer, and pzts. the two pzts are mounted on the structure and connected to a high speed adc. a buffer was used as a level conversion and to protect the raspberry pi. the raspberry pi generates the excitation signal and the dac converts it to analog. in addition, the raspberry pi was used to detect if the structure has damage or not. moreover, the raspberry was used to send the structure health status to the internet server. the data was stored on the internet server and can be monitored remotely from any mobile device. the system has been validated using a real test bed in the lab. results show that the proposed iot shm platform successfully checked if the sheet is healthy or not with 0% error. in addition, the proposed platform has a maximum of 1.03% error for the damage location and a maximum of 8.43% error for the damage width."
"to quantitatively evaluate the flow-diversion efficiency of a given fd structure, a flow reduction (fr) rate index was introduced as where v w/o and v withfd are the intra-aneurysmal average velocities without fd intervention and after fd implantation of a given wire configuration, respectively."
"the open source library palabos (version 1.4) [cit] based on lattice boltzmann method (lbm) was used as the cfd solver for its high flexibility and parallelism. lbm is a mesoscopic approach which exhibits good agreement along with its similar numerical stability to other cfd tools [cit] . in lbm, fluid is described in terms of the density (2) counterclockwise :"
"damage detection in shm systems is classified into two types: local-based and global-based damage detection [cit] . localbased damage detection is used for screening structures and global-based damage detection is used for vibrational characteristics. various sensors are used to detect the size and location of the damage in shm systems [cit] . sensors like piezoelectric, optical fiber, ultrasonic, laser, image detection, and vibrational sensors are commonly used because of their leverage over other sensors."
"he is currently a full professor with beihang university, china. he published more than 125 peer-reviewed papers which are indexed by sci or ei and 3 books. he has presided more than 20 major projects in total, such as the national natural science foundation of china, national 863 program, and bds key project. his research interests include automatic test systems, fault diagnosis, and satellite navigation. chao zhao is currently pursuing the ph.d. degree with the school of electronic and information engineering, beihang university, china. his research interests include positioning with combining navigation and opportunistic satellites. volume 7, 2019"
"to control the random modifications progressing towards the optimal solution, a sa procedure ( fig. 1 ) was implemented to identify the fd structure with the lowest intraaneurysmal average velocity within a certain range of temperature drop [cit] . we selected intra-aneurysmal average velocity as the objective function of sa because of its possible correlation to thrombotic occlusion [cit] . optimization began with the homogeneous fd structure and was completed when the lower temperature limit was reached. the initial and lower temperature limits were decided in this manner: (1) prior to the sa procedure, hundreds of random modifications were performed to calculate the mean alteration of objective function for assigning the initial temperature an acceptance probability of 0.5; (2) the optimization was assumed to involve 60 decreases in temperature and the lower limits for all cases were calculated accordingly as shown in table 1 . during optimization, the fd structure was first modified, and cfd was then performed to obtain the corresponding intra-aneurysmal average velocity in each stage."
"due to the damage being present, the wave takes longer to travel from exciter to sensor, which implies that the wave travels a further distance. this distance, h, can be determined by"
"for the purpose of automatic feature learning, this paper concentrates on dl using cnns. unlike neural network, the cnn has its neurons arranged in three dimensions, i.e. width, height, and depth as can be seen from fig 1. f there are three main layers in cnns."
"commercially available, braided fd devices are usually made of helix-like woven wires with uniform structural arrangements. in this study, the fd was assumed to comprise eight helices (four clockwise and four counterclockwise); wire thickness and width were both 50 μm. each helix trajectory of the deployed fd was individually described by the following equations:"
"a previous study suggested that the peak value of hemodynamic parameters computed for pulsatile flow matches those of the corresponding steady flow [cit] . therefore, we performed steady flow simulation with standard d3q19 lattice topology [cit] in consideration of hundreds of cfd simulation steps in the following optimization procedure."
"in conclusion, for satellites with high elevations, velocity directions that are equally spaced and satellites with low elevations, the corresponding subtracks are close to the receiver and should be used for instantaneous doppler positioning. with altitude aiding, low elevation satellites can be ignored when the horizontal positioning precision is in a receivable scope, which means that only satellites with larger elevations are used for positioning."
"theta is one of the parameters required in fd design and manufacturing process. for design optimization, using θ as modification parameter could maintain the device porosity at a pre-defined value. our results demonstrate (fig. 2) that modifying θ effectively improves the flow reduction rate of a conventional homogeneous fd structure."
"in fact, θ is not a constant. suppose that idccs2 and idccs1 correspond to the boundary values of measurement errors, and all the idcces obtained by the receiver are between idccs1 and idccs2 and have the same vertex. as a result, the influence of measurement errors changes the idccs to a special geometry, and its bottom is an annulus. we define this regular geometry as doppler annulus cone geometry (dacg)."
"neural networks have been around from last 25-30 years and they have always been good for learning weights in the network with one hidden layer. [cit] the authors used the three-layered neural network for training, i.e., colour, texture and edge features of the image and retrieved the relevant images based on this trained data. the set of features are trained resulting into fused feature set and then the similarity is measured over this fused feature. they have found improvement in the recall and retrieval rate after the classification using feed forward back propagation neural network."
"in this study, we demonstrate an automated optimization method on a conventional, homogeneous, helix-like fd to adapt its wire structure to an assigned aneurysm. the proposed optimization was designed to rearrange the starting phases of fd wires, so that the original device porosity was kept to maintain the metal-to-arterial tissue ratio. after the fd structure with the highest flow-diversion efficiency was identified, its robust performance was then investigated by axial displacement test. figure 1 shows the scheme of our proposed optimization method, which includes vascular model reconstruction, fd modeling, random modification, computational fluid dynamic (cfd) simulation, and a simulated annealing (sa) procedure."
"one can conclude that the dacgply is small if the receiver is at a direction orthogonal to the satellite velocity direction. its value is even smaller for the receiver near the subpoint in this case. conversely, the satellites with corresponding subpoints that are closer to the receiver and corresponding angles between satellite velocity and line-of-sight that are larger have smaller dacgply, which means that these satellites are more suitable for instantaneous doppler positioning."
"we present some experimental results based on real iridium data collected. every iridium next data blocks are collected by a static receiver in the parking area of buaa university for 30 min, and the wide antenna with 9 db gain is under open sky, as shown in fig. 10 . the if data collector receives the raw data with a sampling rate of 112 mhz at a rf center frequency of 1626.25 mhz, and the center frequency of the recorded data is 28.25 mhz. in practice, the received signals are from two downlink-only channels. however, only one channel signal is used for positioning. since no orbital information can be decoded from the signals, tle data will be involved. the results of iridium signal acquisition and iridium sops positioning are presented, as described below."
"aneurysmal local hemodynamics is sensitive to the morphological characteristics of the parent artery. thus, three vascular geometries were used to investigate the proposed optimization method under various hemodynamic conditions."
"we employed sa procedure to identify a global optimum for the average velocity as shown in fig. 2 . to prevent optimization from resulting in a local optimum, modifications to fd structures with inferior flow diversion performance might also be accepted according to our pre-defined cooling schedule. our optimization approach could accept other objective functions, as long as the initial temperature and cooling schedule are well established [cit] ."
"fiber optical sensors are used in fiber bragg grating (fbg) to detect the dynamic loads on bridge decks [cit] . vibration based shm systems have become an area of focus in recent studies, as it is used to detect damage that cannot be visually detected and damage hidden within the internal areas of the structure. the vibration of the structure changes along with the stiffness of the body when it is damaged and this can be detected by using vibrational shm [cit] . the usage of wireless sensor networks (wsn) in shm systems has increased in recent times due to its low installation and maintenance costs [cit] . compared to other data acquisition systems, wsn has various benefits such as low maintenance cost, flexibility, and ease of deployment [cit] ."
"in table 2 each row indicate the result for each of the 21 classes of image in the data set. the values in the table indicate the average value of the rank of retrieval results for all the 100 images, taken as query image one at a time, in each class. here the improvement in the rank value for each image category is clearly observed for the features with the contrast and entropy values as compared to that of pooled feature set. the rank value in case of building is 0.4419 for pooled feature which is improved to 0.2394 and 0.2418 when calculated for the contrast and entropy features, respectively. in case of normalised rank higher value means higher deviation from the ideal condition and lower value means better performance. the decreasing value demonstrates the improving performance. 1 6 5 2 10 7 2 13 10 3 16 12 3 18 14 river 2 4 5 2 6 7 2 7 9 3 8 10 3 9 11 runway 1 7 7 2 9 9 2 10 10 2 10 11 3 11 12 sparse residential 2 5 6 2 8 8 3 10 10 3 12 12 3 13 13 storage tanks 6 4 5 8 6 7 10 7 8 12 9 10 15 10 11 table 2 retrieval results for the dataset in terms of rank"
"the robustness of an optimized fd may also relate to the shape of an boi area. when boi is strong and concentrates in a small region (e.g. c model, fig. 3c), axial displacements may result in jet flow entering aneurysm cavity through areas with less wires. it is implied that the boi characteristics of the fd recipient needs to be investigated before an optimized device can be applied."
"a practical optimization method for commercially available helix-wire fds was developed in this study. by rearranging the starting phase of each helix subset, the structure of fd can be tailored to efficiently block the inflow for a patient-specific aneurysm. using this optimization method, three optimized fd structures with unchanged device porosity were obtained corresponding to three different vascular geometries. the developed method potentially enhances the study of the patient-specific design of fd devices."
"compared with the s and c models, the non-stent r model has a strong and sharp inflow jet, as seen in fig. 3d . the bloodstream flows into the deep aneurysm sac through the orifice and circulates inside the cavity, finally flowing out of the aneurysm as a wide and strong boo. after the fd implantation of the initial structure, the width and velocity magnitude of boi were reduced; both the size and volume of the isovelocity surface decreased (fig. 3d) . after the implantation of the optimized fd, the flow circulation was drastically modified. the depth traveled by the inflow jet into the aneurysm sac decreased, and rotational flow circulation was clearly observed inside the sac (fig. 3b) . the sac circulation and parent flow were further separated, and both boi and boo were split into two weak and thin streams (fig. 3c) . similarly, the wire concentration inside the boi area of the optimized fd structure can be seen in fig. 3a ."
"the iridium system uses a combination of sdma, fdma, tdma and tdd. the tdma frame is in total 90 ms long, and the simplex channel is 20.32 ms. the system uses l-band frequencies of 1616 to 1626.5 mhz for the user link. the fdma scheme divides the simplex channel bandwidth into 12 channels of 41.67 khz for a total of 0.5 mhz from 1626.0 to 1626.5 mhz [cit] . each iridium signal burst has an unmodulated tone, unique word and information data [cit] . the iridium next data that were collected by us are the ring alert signal and primer message signal."
here r i -position of the relevant image among the retrieved. n r -no of relevant images in the database n-total no of images the value of the rank is ranging from 0 to 1 where 0 indicates perfect match and 1 indicate the complete mismatch. the above expression calculates the deviation from the ideal condition when all the n r relevant images are at the top positions.
"damage was considered along the diagonal of this plate starting with a distance starts at 85 mm and ends with 481 mm from the exciter. different simulation scenarios with different damage location along the plate's diagonal have been done. figure 6 shows the percentage error between the actual damage location and the damage location calculated by the proposed technique. the results show that the proposed model determines the damage location precisely with a maximum of 0.9% error. in order to evaluate the damage width calculation, different simulation scenarios for different damage width have been done. first, we considered damage located at center of the plate along with the diagonal. second, the simulation was done for differed damage widths. figure 7 shows the percentage error between the actual damage width and the damage width calculated by the proposed technique. the results show that the proposed technique was able to determine the damage width with an average of 17.5% error."
"shm process incorporates sensors, signal processing, and hardware implementation. many sensors, for example, accelerometer, ultrasonic, laser, vibration, camera, fiber optical, and piezoelectric have been used. terrestrial laser scanning (tls) technique uses laser sensor to get the threedimensional coordinates of the target structure and monitor its health. laser doppler velocity meter is also used in shm using noncontact and low frequency lamb-wave detection [cit] . the detection of the structural damage can be made more robust by using a triaxis, multiposition scanning laser vibrometer [cit] . a displacement measurement model has been incorporated with it to reduce the error of tls by using linear variable displacement transducer (lvdt), electric strain gages, and long gage fiber optic sensor. to monitor the structural health, a 3d finite element model takes the periodic measurement of the deformation structure and performs inverse structural analysis with the measured 3d displacements [cit] ."
"over the years since the advent of cbir various features are defined and designed to capture distinguishing characteristics in an image. these features are grouped into colour, texture and edge. amongst these three feature categories, colour feature has always been more obvious and intuitive. different colour features which researchers have already experimented are colour coherence vector (ccv) [cit], colour histogram, and so on. edge features are captures the edges in the image. the edge descriptors are better able to define the edges in an image. [cit] are like classic features which exist for years now. these feature sets have been used in texture identification so satellite images is one of the domain where this has been used. both these texture descriptors are time tested in gray scale images. [cit] . morphological covariance as operator is used to find textures. circular covariance histogram and rotation invariant point triplets (rit) [cit] . there are many more feature sets introduced, the limitation of this feature sets is that they do not adapt with the image dataset. also the feature set do not adapt with the concept of image classification in the same image dataset. [cit] which indicated the limitation of low-level feature set and need of incorporating learning, for efficient cbir systems. in the last few years, researchers have started working on a completely new dimension called dl, which is expected to change the way the authors perceive the working of cbir systems. dl in neural networks (nns) [cit] . human learns the patterns by sequentially directing attentions to various relevant portions of the available data unlike dl systems."
"image processing technique involved in multipath ultrasonic guided wave imaging is used for complex structures, inhomogeneous and anisotropic materials. this technique gives an improved image quality using fewer sensors. it takes maximum advantage of using a large number of echoes and reverberations to perform localization and damage detection. this system helps not only to increase the performance but also to reduce complexity and cost [cit] . vibrational sensors and video cameras used together in wireless sensor networks (wsn) send distributed data interpretation to detect the local data trends like normal vibrations, abnormal vibrations, and structural tilts. the video camera is prompted to zoom/tilt into the local area for monitoring in case of critical events. this video camera data is synchronized with the sensor data [cit] . the self-diagnosis system helps to find out the failed piezoelectric sensors and reconfigure the network with existing sensors in large shm systems [cit] . piezoelectric wafer active sensors (pwas) are used in a variety of damage detection methods [cit] . pwas are nonresonant devices and can be used in various guided wave modes [cit] . piezoelectric sensors have extended its groves in multiple fields like aircraft, aerospace health conditioning, metallic plate stiffener disbanding, and structural health of wind turbines [cit] . pwas are very useful in large class of shm systems like guided wave ultrasonic and electromechanical and passive detection [cit] ."
"in this study, we have demonstrated an optimization approach to improve the flowdiversion efficiency of conventional homogeneous fd stents. our optimization involves rearranging the starting phases of homogeneous helix wires, thereby modifying fd structure without altering its porosity, which finally enables the optimal wire configuration to maximally block the aneurysmal inflow."
"if satellite doppler-shift is recorded by a static receiver on the earth surface at some moment, without considering any errors, all points with the same measurement form a circular conical surface, as shown in fig. 2 . point n denotes the projection of the satellite on the cone undersurface. in the following sections, this special cone is defined as an iso-doppler circular conical surface (idccs), and the angle θ between the directions of satellite velocity and line-of-sight is defined as the field angle of this idccs. the doppler-shift is expressed as"
"the conventional, commercially available fd devices such as pipeline embolization device (ped; irvine, ca, usa) and silk (balt, montmorency, france) are constructed by homogeneous helix-like wires. the porosity of fd device is associated with poststenting stenosis as suggested by prior studies [cit], since a high metal-to-arterial tissue ratio resulted from low device porosity may pose the risk of vascular injury. animal experiments, on the other hand, have confirmed that a lower device porosity promotes a complete thrombotic occlusion of an aneurysm [cit] . therefore, simply modifying fd wires by decreasing or increasing the porosity may result in an increased risk of post-stenting stenosis or long-term thrombosis formation. to accelerate thrombotic occlusion and avoid post-stenting stenosis, a possible solution may be the application of patient-specifically tailored fds with porosity maintained at a high level. attempts of introducing optimization to fd structures have been made to improve the flow-diversion efficiency [cit] . however, a practical optimization strategy that can be feasibly applied to the conventional fds has not yet been developed."
"as described in subsection 3.1 all the 2100 images with given class, participate in the learning process. as an outcome of the learning processes, the weights are learnt in such a way that when convolved with the images gives appropriate features. an example of the feature maps obtained as the output of the convolution layer can be seen in figure 3 . the figure describes the output of each of the 400 kernels for image number 38 of agriculture category. these 400 blocks visible in the figure are the output of layer one, when a test image is operated with learned kernel of cnn. figure 4 is the the set of some selected feature maps, and is quite apparent that the kernels of cnn generates the patterns, which are very similar to a human visualisation of the patterns. figure 4 shows four different images of agricultural category and their patterns are captured by three different kernels numbered 20, 93 and 202 . it can be concluded that these kernels have captured the patterns in a similar way as perceived by human. these output of the convolution layer is then sub sampled, which in turn becomes the input to the fully connected neural network. input to the fully connected neural network is the feature set, and the authors have used this feature set for cbir systems. the results obtained using the pooled data set (output of pooled layer) yields not so good results when measured in terms of rank and retrieval images as can be seen in tables 1 and 2 . to improve the retrieval result and to make the feature set compact, the authors have used the concepts contrast and entropy."
"in recent years, dl, specially convolutional neural network (cnn) has proven its capabilities in the field of image classification and pattern recognition, but its application in the field of cbir is still limited. in this paper, the authors have demonstrated use of cnn for cbir systems. the feature maps obtained using cnn are used for cbir application."
"the theoretical analysis and simulation results show that the satellites with high elevations and velocity directions that are equally spaced are more suitable for improving the horizontal positioning accuracy, and for the satellites with low elevations, the corresponding subtracks that are close to the receiver should be used to restrict the vertical positioning errors."
"systems tool kit (stk) software of analytical graphics inc. is used to simulate the leo satellite orbits, and the orbital altitude is 780 km. the receiver is located on the x-axis of the eci coordinate system, and the altitude is zero during the initial time of the stk project, three satellites, s 1, s 2, and s 3, are all located above the receiver, the corresponding orbital inclinations are 10 degrees, 45 degrees and 80 degrees, and the anomalies are all 0 degrees. the orbital inclination of s 4 is 135 degrees, and the corresponding anomaly is −24 degrees. the constellation of leo satellites and receiver are shown in fig. 6 . three cases are presented in the experimental verification."
"zizhong tan is currently pursuing the ph.d. degree with the school of electronic and information engineering, beihang university, china. his research interests include navigation and positioning with leo satellites, opportunistic navigation, snapshot positioning, and gps satellite navigation."
"v sat is the satellite tangent velocity, and θ is the true field angle. the v sat and v are constants for the moment. when the satellite tangent velocity is orthogonal to the line-of-sight, θ achieves the minimum value. the case in the top plot of fig. 5 corresponds to an elevation less than 90 degrees, and point b denotes the receiver position. drawing a plane perpendicular to segment sb, that plane intersects segments sd and sc at points e and f, respectively. segment ef is defined as the ply of the corresponding dacg (dacgply); the expression is"
"the changing of true anomaly from left to right in fig 7 means that the satellite elevation is increasing and then tapering. the solid line shows that the elevation of s 4 has almost no influence in dhdop when other satellites are on the top of the receiver. because the width of the horizontal intersection region between different dacgs is decided by higher elevation satellites, the horizontal positioning error is restricted within a scope. from the dotted line, we can see that the dhdop values decrease obviously with increasing s1, s 2 and s 3 elevations when s 4 is on the top of the receiver. the changing trend of dhdop is in connection with the other three satellite positions and satellite velocities. dhdop is smaller for the higher elevation satellites because the width of the intersection region between all the dacgs and the earth surface is smaller, which means that the horizontal intersection will not be restricted by only one high elevation satellite. as a result, more satellites near the top of the receiver should be used to constrain the horizontal position errors, since it includes 2-d errors. fig. 8 shows that the more dispersive the satellite velocity directions are, the smaller the doppler dop is. the curve of dvdop is almost a straight line since the vertical intersection area is not changing. the results in fig. 9 show that dpdop and dvdop are sensitive to the s 4 satellite elevation. the vertical intersection gets larger as the elevation of satellite s 4 increases, which means that the elevation variation primarily affects the up-direction position errors. the dhdop is almost unchanged since the remaining satellites restricted the horizontal position accuracy. the effects of constellation geometrical distribution on position accuracy are verified by the tests. using the analysis proposed above, we are able to obtain a better doppler dop."
"before fd stent implantation, the flow enters the aneurysm from the distal end and exits through the proximal end. after fd implantation with the initial structure, the symmetric flow distribution was disrupted; the flow circulation inside of the aneurysm sac became irregular and sparse. the boi area shifted from the central distal end to the proximal end with a negative y offset; meanwhile, the outflow zone switched to the distal end (fig. 3c) . a marked reduction in both the breadth and magnitude of the boi was observed for the optimized fd structure (fig. 3b) . rotational flow circulations were found inside the aneurysm. the optimized wire structure showed a concentration of fd wires inside the boi area, resulting in a denser strut distribution in the proximal orifice end (fig. 3a) ."
"non-gnss satellite signals, as the new sops, have some important advantages in terms of high signal strength, low cost and better coverage. plentiful non-gnss satellites are operational today; iridium, globalsatr and orb-comm are typical representatives. [cit], and it launched a total 75 iridium next satellites. [cit], it also announced that the service termed satellite time and location (stl) based on the iridium satellite network can provide a solution for assured time and location which can aid gps or even replace gps, but the technical details have not been released [cit] . the deployment of the 24 [cit], and 15 [cit] . numerous scholars have further studied using additional signals to augment gnss from low earth orbit [cit], oneweb and boeing announced the production of new constellations of large numbers of leo satellites in the future [cit] . china has also presented a similar plan, for example, the honyan constellation and honyun project."
"two idealized aneurysm models-the straight (s, fig. 1a ) model and the curved (c, fig. 1a ) model-were constructed; for both models, the aneurysmal diameter (d) was 4.8 mm, the neck diameter (n) was 2.8 mm, and the arterial diameter (d 1 ) was 3.5 mm. the curvature radius (r) of the c model was 6.0 mm [cit] ."
"the leo satellite sops can work in severe environments, such as when rushing to deal with an emergency, fire control inside deep buildings and in combat. it can also be a full standalone backup of the gnss; aspn, presented by darpa, is a typical gps backup research project. the concept of iridium sops positioning is presented in figure 1 . an antenna with a frequency band from 380 mhz to 20 ghz and a 5 db gain connects to a radio frequency (rf) front end. there are typically three main processes in the software module: doppler acquisition, satellite orbit reconstruction and positioning solution. the receiver position is obtained with the instantaneous doppler positioning, and the position accuracy can be further improved by height aiding. the dotted part in fig. 1 shows the new analysis methods, which include error analysis and the relationship between the geometrical distribution of satellites and positioning error. more details are presented in the following sections."
"it can be noticed that the proposed technique's error increases when the damage width decreases. thus, evaluating the proposed technique for a fixed damage width at different locations is very important. figure 8 shows the percentage error between the actual damage width and the damage width calculated by the proposed technique for a fixed width at different locations. the results show that the proposed technique is able to determine the damage width with an average of 10.4% error. results show that the proposed mathematical model is accurate. moreover, it does not need high computational processing unit. thus, this mathematical model could be integrated with iot platform."
"for a given satellite, as a result of the doppler-shift measurement process of a static receiver, the user must locate somewhere on the corresponding idccs. if the receiver repeats the same process with other satellites simultaneously, the receiver is at the intersection of these idccses. as a result, the receiver position can be calculated by the doppler measurement equation when enough satellites are available. the navigation equations of instantaneous doppler positioning can be obtained by different methods. van [cit] differentiated the linearized pseudorange positioning equations, and the user position is solved by the least squares algorithm. although the vector expressions of the navigation solution from different methods are different, the scalar expressions are the same. doppler positioning takes receiver clock frequency-drift as an unknown quantity instead of a receiver-clock error. if one cannot acquire the precise transmitted time of the signal, time error can be regarded as an unknown variable. this method is called the snapshot technique in pseudorange positioning. fernández-hernández and borre studied this in his paper, and the experiments showed that the method was advanced [cit] ."
"matrices k nr and m nv are the differential coefficient matrices corresponding to the satellite position and velocity. with the 83416 volume 7, 2019 aid of (6), the error covariance p dx of estimation is"
"contrast of an image represents the difference in the intensity values in case of grey scale, which is represented by standard deviation . standard deviation of the data set represents the measure of variation or dispersion from the average (mean or expected value). a low contrast indicates that the data points tend to be very close to the mean, whereas high contrast indicates that the data points are spread out over a large range of values. [cit]"
"our optimization generated modified fd designs with inhomogeneous wire structure. similarly, previous studies also reported the benefits of inhomogeneous and asymmetric fd designs. [cit] showed that a local low-porosity design can decrease flow velocity inside an in vitro model. by animal experiments, [cit] showed the good performance of asymmetric stents in occluding rabbit elastase aneurysms. these studies suggest the possibility of using inhomogeneous device to achieve favorite treatment outcomes. in this study, we demonstrated how a conventional homogeneous fd can be tailored to an inhomogeneous one by merely changing the values of 'θ', which is applicable to be modified without affecting the manufacturing possibility."
"honglei qin received the b.s. [cit], the m.s. degree in electrical engineering from the harbin institute of technology, china, and the ph.d. [cit] ."
"in this paper, a combination of pulse-echo and pitch-catch techniques is proposed as shown in figure 2 . pitch-catch technique is used to determine presence of damage. if any damage is found, the pulse-echo technique will be used to determine location of the damage. this method uses two pzt sensors. the first transducer (pzt1) excites the signal towards the second transducer (pzt2) and picks up any waves that have been reflected back from damage that could not reach the second transducer, which uses the pulse-echo technique. the second, ptz2, will receive wave feedback in a pitchcatch manner. the proposed technique is good for any metal structure, for example, aluminum and steel. in order to detect if the structure is healthy or not, a cross correlation (cc) algorithm is used to check the similarity between excitation signal, e, at ptz1 and received signal, r, at ptz2. the cc value is used as a linear damage index and is defined as"
the effects of constellation geometric distribution on position accuracy are taken into account here. the constellation geometric distribution not only includes the relationship of the relative position of a satellite and receiver but also that of the relative velocity directions of different satellites. we provide the related analysis after introducing a geometrical concept.
"to improve the position accuracy, the kalman filtering (kf) is used to process the least squares (ls) solutions fig. 14) . the iridium signal spectrum is shown on the right computer in fig. 14 . clearly, the power of iridium ring alert signals is higher than the noise for about 8 db."
"the analysis above concerns the characteristics of dacg-ply; however, the position errors are also related to the different dacg positions. the intersection region of different dacgs depends on satellite positions and satellite velocity directions. the width of the intersection region between dacg and the earth surface is larger than the corresponding dacgply, since satellite elevation is often less than 90 degrees. three cases are in place to discuss the characteristics of the intersection of different dacgs. first, two satellites are adjacent, and the elevations are almost equal. the corresponding vertical intersection region of two dacgs will be large when the satellite velocity directions approach each other; as a result, larger position errors exist in the vertical direction. second, considering the same condition, the vertical intersection of dacgs will be larger if two satellite elevations increase, which means that the vertical position error will be larger. at the same time, the width of the intersection arc region between each dacg and the earth surface will be smaller. however, the horizontal intersection region of two dacgs may be large, and it all depends on the satellite velocity directions. finally, consider the condition that the two satellite elevations are small; for instance, the receiver and two satellites are in the same plane. then, the vertical intersection of dacgs will be relatively smaller than for the above two cases regardless of the satellite velocity directions. the horizontal intersection will be small if the line-of-sight directions are perpendicular. at the same time, the vertical intersection of dacgs will be even smaller if the receiver is located on the satellite subtrack."
"the problem of heritage goods preservation is a challenging task. indeed, many objects made up of wood, as well as paper and cloth of artistic or cultural value, are seriously damaged by a variety of pests that may produce aesthetic, physical, and chemical damages that in most cases are irreversible. during the last few decades, the scientific community has tackled this issue, focusing the attention on several complications related to the preservation of works of art, perceiving the fundamental role of the cultural heritage (ch) as an inestimable value for mankind. regardless of the origin, or value, the ch domain represents the evidence of human achievements from the past in terms of architectural, historical, technological and functional progress. keeping their authenticity, as well as warranting their survival for posterity is mandatory."
"the exact solution of the problem is obtained starting from the maxwell equations by using the vectors potential, with the spherical symmetry, and expressing the plane wave as an infinite sum of spherical waves. there are two solutions: the fields around the sphere and those on the inside. for the continuity principle of the electric and magnetic field tangential components, on the surface of separation of the two domains, the two solutions have to get the same results. this condition allows the coefficients fields' evaluation. by considering a spherical coordinate system and the mathematical expression of a plane wave, as shown in figure 3, that proceeds along the direction of the z-axis with the field polarized in the x-axis:"
"by replacing the expressions for a and f, the final expressions are obtained. however, the incident field is given by the plane wave that invests the sphere. for the diffused fields outside the sphere:"
"the previous expressions can be encoded with any programming language (fortran, pascal, c, matlab, mathematics, etc.) for the values evaluation of the electromagnetic field in all points of the sphere and the surrounding space. the result obtained is not satisfactory from the point of view of the study of irradiation and distribution of density power because it has an asymmetrical situation. this situation is given by the plane wave from a well-defined direction (z-axis). by summing the fields from the same model with the change of the direction of the plane wave and its direction of propagation, a situation more adherent to what occurs in a reverberation chamber is obtained. by considering three axes, the two ways of propagation, there are a total of six plane waves."
"the advantage of creating a model of the object for a faithful simulation of the reality is twofold; on the one hand, it is possible to determine the presence of areas not hot enough to ensure the effectiveness of the disinfectant treatment. on the other hand, one can predict if there are too hot zones in which irreparable damage of the good to be treated might occur [cit] . additionally, a second benefit of the simulation is to be able to determine the relationship between the temperature in the interior points to the object, and therefore inaccessible in practice, and the surface ones that are easily monitored even during the actual treatment. thanks to these relationships, the evaluation of the temperature can be conducted at each point in real time, in order to ensure the effectiveness and safety of the treatment. finally, another advantage of the simulation is to evaluate the possible resonances of biological agglomerates subject to the electromagnetic field; this effect could create an unexpected overheating condition that could seriously damage the treated good. the methodology and treatment protocol were developed according to laboratory tests, exposing some samples of different essences of wood into a reverberation chamber. we tested the microwave application in order to evaluate the management of disinfestations of wooden works, monitoring temperature and incident power [cit] . some samples were prepared to test the use of microwaves on wooden works of art. the samples were very similar to real artistic objects, both painted and not painted. we did not observe any significant modifications in the samples [cit] . for this kind of treatment, 2.45 ghz ism frequency is largely used."
whereâ x is the unit vector of the x-axis and β is the angular wave number. the expression of plane waves in terms of spherical coordinates becomes:
"the heat flux wood-sphere depends on the sphere dimension. in case of fir wood, the behaviour is the same, even a decrease of wood heat for its low conductibility could be noticed (figure 10 ). for an equal time treatment, the wood remained at the starting temperature, the sphere, instead, warms in case of resonance. figures 11 and 12 depict the graphics in the resonance case. in case of spheres, varying the radius value of 10% from the resonance value has involved a halving of the heating effect. in case of an ellipsoidal model, moving away from the resonance value, the heating decrease is much blander. figures 15 and 16 depict the results in case of resonance."
"the resonances model study was developed from a strong theoretical base to make its application as wide as possible. this way was supplied by mie [cit] that introduced an exact solution of maxwell equations, in order to estimate the electromagnetic field. figure 2 shows a schematic of a block in the shape of parallelepiped with losses that contains a sphere of dielectric material with losses too, all invested by a plane wave [cit] . in our specific case, the block is made of wood and the sphere is made of water. this choice is due to the dielectric characteristics of biological agents, infesting wooden artifacts, which is similar to water [cit] ."
"the rest of the paper is organized as follows: the second section is devoted to the description of the methods and mathematical assumptions derived to postulate the model. in section 3, the main results achieved are shown, while, in the last section, concluding remarks and future developments are presented."
"this requires comprehensive studies for their maintenance and conservation, to be done by a multidisciplinary team including conservation architects, engineers, scientists, specialists, conservators, archaeologists, art historians, etc. [cit] . therefore, the need to intervene with effective disinfestation treatments is every day more and more imperative. the disinfestation is the activities intended to destroy small animals, especially arthropods, either because they are parasites, disease carriers or hosts for infectious agents or are causing annoyance. the disinfestation process results effective for preserving the ch goods. the disinfestation of works of art is important for preserving them from time passing, air and pest, as well as controlling the atmosphere damages, thus reducing toxic substances. nowadays, several methods are available for eliminating the pests, including fumigation, irradiation, hot-water, hot air, vapour, heat and cold treatments. the current technologies employed in disinfestation of works of art present big limits: among the others, the very long treatment times, the risks of pollution for both the operator and for the environment, in addition to the possible damages for the treated objects. one of the most widespread techniques employed is the fumigation, which is the use of extremely toxic and polluting gases, such as methyl bromide, ethylene oxide and formaldehyde. besides fumigation, also anoxic treatments in controlled atmosphere are being used for disinfestation. they are based on the use of inert gases as nitrogen, argon and helium and of mixtures of co2 that, though not highly toxic, can interact with the treated materials; moreover, another weak point of such treatments is that it requires very long time periods (from 7 to 30 days) [cit] . further alternative technologies are those employing physical means (e.g., uv rays, high and low temperatures), whose applications are still experimental, proving scarcely practical and valid [cit], and also those employing a combination of different techniques [cit] ."
"given the above, it is clear that microwaves constitute a safe and effective methodology for this kind of treatment, although, in some situations, it will be necessary to carefully evaluate any unwanted effects [cit] . for the evaluation of such effects, it is useful to proceed to the heating simulation, avoiding unwanted effects during the real treatment, this is significant for development of treatment procedure of important goods [cit] . several models reproduce the electromagnetic field in a reverberation chamber equipped with a mechanical mode stirrer, and some numerical results are compared with experimental measurements with a good agreement. mode stirred reverberation chambers (msrc) guarantee a controlled electromagnetic environment around biological materials for exposure to electromagnetic fields [cit] ."
"the presence of biological organisms in the works of art can be extremely dangerous for their preservation; however, treatments of disinfection, by means of microwaves, can lead to problems of permanent damage if their dimensions and characteristics are such that they will bring to the resonance frequency of the pests."
"where µ and η are, respectively, the magnetic permeability and the magnetic permittivity of the block. the potential vectors are defined by the following expressions:"
"microwave heating is a well-established practice in the conservation of the works of art. in many situations, when the object that must be treated is valuable, it may be useful to predict the behaviour of the reverberation chamber, in order to avoid its damage and the presence of hot spots for particular dimensions of pests. the hot spots are points that are very hot in which overheating of the material occurs. it is necessary to investigate the possibility of the occurrence these situations. the simulation consists in the study of the microwave heating of pests, which are equivalent to a sphere of water, surrounded by a block of wood invested by six plane waves. table 1 lists the dielectric characteristics of the materials used in this study. making the average of power density absorbed from waves coming from several directions, a peak corresponding to a sphere radius equal to 6.84 mm was observed. this measure is close to half the electromagnetic field wavelength of the sphere. the electromagnetic field wavelength is:"
"with j n (r) spherical bessel function, p n is legendre polynomials and where: the electromagnetic field can be found by using the scalar helmholtz equation:"
"the solution of this equation can be obtained with the method of separation of variables, and the functions that satisfy the solution can be expressed as:"
"to date, mw treatments proved to be a valuable method, and the possibility to know in advance the effects of the treatment opens up an interesting research path. notwithstanding, no data are available in order to standardize the model and extend it to every object, or pests. because of this, forthcoming investigations will be conducted by conservationists, archeologists and art historians in order to effectively validate the model with laboratory tests. further efforts will be also dedicated to explore how different complex shapes of the object can influence the heating distribution. moreover, thanks to this model, we will develop an application that simulates the heating of an object in a reverberation chamber. this application makes it easy for non-electromagnetic personnel to employ the mathematical models, it can visualise the distribution of temperature in objects to be treated and it is possible to insert the power to be transmitted in the chamber [cit] ."
"works of art are an important national wealth for each country. therefore, they should be protected and preserved by all the possible causes of damage. dealing with wooden artifacts, a significant cause of damage is made by the pests that can erode the structure."
"the hankel functions of the second kind replace the bessel equations. in fact, the bessel equations accept the argument equals zero, which is necessary in this circumstance, because it represents the center of the sphere. in the last two expressions, the coefficients b n and c n should be defined, while η is the characteristic impedance of the block that contains the sphere. the total field, sum of incident and scattered fields will depend on the sum of the two contributions:"
"the proposed methodology is useful to calculate the response of biological materials subject to microwave exposure and to prevent the risk of resonance. in fact, peak of resonance, as well as localized overheating, may cause irreversible damage to the good. the numerical model described in these pages has been applied to different shapes (both spherical and ellipsoidal) in order to better fit with the reality. however, even if the model has proved to be affordable to predict the heating behaviour, a preliminary diagnosis is mandatory in order to know in advance the characteristics and the model parameters. this is because their prediction is not easily implemented, mainly due to the uncertainty of these events. in these situations, it is desirable to be sure that the process is effective, but not risky for the integrity of the goods. consequently, it is mandatory to investigate a priori the possible presence of these harmful conditions using techniques of experimental investigations such as nuclear magnetic resonance, axial tomography, and x-rays; in addition this, the study of possible pests species, size and their dielectric properties, in all the forms in which they can aggregate, is fundamental."
"in this context, this paper outlines a prediction model, developed in order to envisage the distribution of heating power into the objects to be treated, even of complex shapes enabling one to define a priori the exposure time. the model can also predict the behavior of irradiation when the object is infested by other entities such as nails or pests. the data to be provided for performing a simulation are the geometry of the object (or objects in case one performs a multiple loading) and their dielectric characteristics. as a result, we obtain the distribution of heating power [cit] ."
"the main contribution of the proposed methodology, in addition to those described, is that it could be extended to objects of historical-artistic interest made up of different materials, from very simple mono-components to complex structures integrating inorganic and organic matters."
