text
"to detect place cells, we calculated 'spatial information' 34 -the mutual information rate between spikes and location-using the same formula as above (equation 2), but using the 1,600 spatial bins instead of time bins. place cells were defined as cells for which the information rate exceeded 99% of the values for the shuffled samples."
"regarding user experience, there were no significant differences in questionnaire responses. the lack of differences is actually encouraging, for example, in terms of social acceptability of operating the glove in the wild, since it was not considered significantly more obtrusive than operating the smartphone."
"the system is based on a tactile glove (see fig. 2 ), which is equipped with vibrotactile actuators and a set of sensors, and an android mobile device. the glove has been constructed of thin elastic fabric -to make it adaptable to different hand sizes -and several electronic components. two arduino microcontrollers (arduino pro mini) process the sensor signals and commands, and control the vibrotactile feedback. for sensing hand orientation, the glove is equipped with a 9-axis inertial measurement unit (imu, invensense mpu-9150), which consists of a gyroscope, an accelerometer, and a compass. flexible bend sensors (spectra symbol flex sensor) are deployed on three fingers (thumb, index, and middle finger). a set of three vibrotactile actuators (precision microdrives 10mm shaftless vibration motor) is mounted for providing notification cues and directional guidance on the thumb, index, and middle fingers. the glove communicates wirelessly over bluetooth with an android mobile phone running the application software. the glove has been built for right-handed interactions."
"video analysis shows that glove users seemed to be having a walk around the city. when they got a notification of approaching a poi, they slowed down smoothly and simply concentrated on the vibrations. when the audio description was playing, they looked at the poi or took quick glances around. interestingly, even when they showed some uncertainties in interpreting the tactile guidance, none of the participants ever looked down at the glove. overall, the glove did not appear as an external device put onto the user's body, but rather seemed well integrated with the flow of actions. users operated it with the same naturalness as if they were making a movement with their hand."
"in this paper, a tested ac microgrid with five identical dgs and some linear loads was simulated in matlab/simulink, which is designed to testify the effectiveness of the proposed distributed secondary control approach. the topology of the communication network with five nodes employs ring structure as figure 2, that is, all dgs only communicate with its adjacent nodes and the associated laplace matrix l is:"
"to quantify theta precession, we considered all firing fields that occurred between the press and the release of the joystick. for each field, we considered spike times, linearly warped between 0 and 1 (corresponding to the joystick press and the release, respectively) and the phases of theta at the spike times (measured between 0° and 360°). values of theta phase were then circularly shifted in 1° increments, with values exceeding 360° wrapping back to 0°. for each shift from 1 to 360°, linear regression was fit to the relationship between theta phase and the warped time. the shift for which this linear regression had the smallest mean squared error was chosen, and the slope of the theta precession was determined from the linear regression at that shift."
"sliding mode control (smc) is efficiently applied for the stabilization control of several linear / nonlinear dynamic systems such as power converters [cit], singular systems [cit], quad-rotor aircraft [cit], inverted pendulum [cit], seesaw systems [cit], robotic manipulators [cit], electronic circuits [cit], power processing [cit], electro-hydraulic servo system [cit], secure communication [cit], vehicular following system [cit], chaotic systems [cit], etc. smc is a powerful control technique and has the ability of the achievement to the desirable performance in the presence of disturbances and uncertainties. the important properties of smc are the superior transient performance, insensitivity to the bounded disturbances and robustness to the uncertainties in comparison with the other control methods. smc design procedure is separated into these phases: (i) sliding phase, (ii) reaching phase. a switching surface is specified in the sliding phase so that the controlled system displays promising dynamic performance. a sliding mode controller is used in the reaching phase to converge trajectories of the states to the switching surface. for the purpose of the influence of the switching surface on the stabilization and transient response, the design procedure of the switching surface is the most significant subject. smc uses a discontinuous control signal to drive the states to a predesigned sliding surface on which the desired performance and system's stability are achieved."
"the vibrotactile guidance follows the metaphor of pulling the hand towards the vibrations. if the user is pointing too much to the right, they will feel continuous vibrotactile bursts (100 ms activation with 400 ms silence) on their thumb, indicating they need to point more to the left. conversely, the guidance cue is presented on the middle finger to guide pointing to the right. when the user is pointing at the target, all three actuators vibrate with a faster rate (100 ms activation and 200 ms silence). the spatial left/right mapping for the tactile cue was chosen because it gives an immediate indication of the direction, which would be lacking in an intensity-based hot/cold metaphor approach."
"the key control parameters and operation cost coefficients for each dg are given in tables 1 and 2, respectively. table 1 . specifications of the microgrid system."
"data analysis. behavioural analysis. each trial was characterized by the duration from the press of the joystick to the release of the joystick and by the sound frequency at the moment of the release. because joystick deflection increased sound frequency exponentially, we used a logarithmic scale and measured frequency in octaves relative to the starting frequency of 2 khz. when animals were not engaged in the task, they still occasionally deflected the joystick-for example, by stepping or leaning on it while exploring the chamber. we observed that most of the very brief trials resulted from such behaviour; for analysis, we therefore excluded trials shorter than 3 s."
where substituting (19) - (22) in (24) 3 3 2 2 2 3 1 2 2 1 1 1 2 2 3 2 3 2 3 3 2 2 3 2 2 1 2 1 2 1 2 2 1 1 2 1 1 2 3 3 2 1 2 2 1 2 3 2 2 1 2 2 1 2 2 1 1 1 1 2 3 3 3 2 1 2 2 2 1 2 3 2 1 2 1 2 1 1 1 2 11 1~)
"information across tasks. our results therefore suggest that the wellknown spatial patterns in the hippocampal-entorhinal circuit may be a consequence of the continuous nature of the relevant task variables (for example, location), rather than a primacy of physical space for this network 9, 10, [cit] . what is the purpose of these continuous representations? in the smt, rats did not need to represent the structure of the entire acoustic space. they could, in fact, respond to a particular sound frequency-a strategy that is also sufficient in operant tasks known to be hippocampusindependent 6 . however, our observations lead to an intriguing conjecture that in more complex tasks (for example, those containing memoryguided decision points), the hippocampal-entorhinal system might similarly represent arbitrary behavioural states. in this framework, task performance activates a sequence of neural activity, in which firing fields are elicited parametrically with progress through behaviour. neighbouring and partially overlapping fields therefore represent the order and adjacency of behavioural states. this could be useful for linking events in episodic memory and for planning future actions 18, 29 (for example, via simulated continuous neural sequences 30 ). spatially localized place and grid codes might therefore be a manifestation of a general circuit mechanism for encoding sequential relationships between behaviourally relevant events. this view suggests a role for these cell types in supporting not only spatial navigation, but cognitive processes in general."
"spatial firing is often considered to be one specific example of a 'cognitive map'-a general representation of relationships between cognitive entities 7, 8, 11 . these entities can correspond to different locations, but can also be distinct stimuli or even abstract concepts. consistent with this idea, the firing of hippocampal cells is modulated not only by location, but also by non-spatial variables, including sensory, behavioural and internal parameters 12, 13 . for example, hippocampal neurons respond to discrete stimuli such as sounds 14, odors 15, faces and objects 16 . hippocampal and entorhinal neurons also respond to locations in visual space 17 and can fire at different time points during temporal delay tasks ('time cells' [cit] ). finally, recent functional mri studies have suggested that cognitive spaces defined by continuous dimensions are represented by the human hippocampal-entorhinal system 9, 10 . one interpretation of these findings is that any arbitrary continuous variables that are relevant to an animal can be represented by the hippocampal-entorhinal activity using a common circuit mechanism. to test this idea, we designed a 'sound manipulation task' (smt), in which rats changed the frequency of sound in their environment (fig. 1a) . animals deflected a joystick to activate a pure tone produced by a sound speaker. they continued deflecting the joystick to increase the frequency along a perceptually uniform logarithmic axis 22 . rewards were obtained by releasing the joystick within a fixed target frequency range. to uncouple frequency from the amount of elapsed time, we randomly varied, across trials, the 'speed' of frequency traversal. the resulting trials varied in duration by up to a factor of 2, on average in the range of 5-10 s."
"if the trial was terminated within the target zone, the led above the lick-tube was turned on and a reward (25 μ l water) was delivered. the led persisted for 2 s. if the trial was terminated outside the target zone, no additional stimuli were delivered. in either case, a new trial could be initiated at any following time. if the animal obtained a reward, the new trial used a new randomly selected value of the traversal speed α; otherwise the same traversal speed was repeated. passive playback experiments. two passive playback experiments were performed-with and without a reward. for animals that did not receive a reward, passive playback was presented for 15 min immediately following the last smt session of the day. during this time, the nosepoke, the joystick handle, and the lick-tube were covered with a plastic cover to prevent access. sweeps of pure-tone sounds were then played with 3-s pauses between the sweeps. each sweep was from 2 to 22 khz, as in the smt. the speed of traversal of the frequency range for each sweep was chosen from a uniform distribution to roughly match trial durations from preceding smt sessions."
"we did not encode any directional information in the auditory content, unlike some previous systems (e.g., [cit] ). we did not want to impose multiple directional guidance schemes on the user at this stage, but are considering pursuing this in the future. another essential future development will be to integrate the interaction technique with a recommendation engine backend to personalize the content. now that we have confirmed that the interaction technique itself works, in future studies we will work towards an integrated, multimodal system for exploring new places."
"our design makes use of both proximity-based vibrotactile notifications and more refined vibrotactile guidance for pointing at the recommended target. our technique does not assume the user has any pre-defined destination, enabling true exploration of the city, while still being able to provide fine-grained guidance on demand to a specific poi."
"we found that the frequency of the theta oscillations was stable for the first several seconds of a behavioural trial, but tended to increase near the end of the trial in some rats. to quantify theta frequency, we therefore only considered the first 3 s of each trial. frequency was determined by locating the peak in the multitaper power spectral density estimate (matlab command pmtm with the time-bandwidth product of 4) between 6 and 12 hz. alignment to task events. we implemented a model to measure how well the activity of a given cell aligned to the press of the joystick, to the release of the joystick, and to sound frequency. we considered only the time period between the press and the release of the joystick. first, all trials longer than 3 s were sorted by duration and grouped into five equal-sized 'groups', from the fastest to the slowest trials. (if the number of trials was not divisible by 5, some of the fastest groups contained one extra trial). for each group i from 1 to 5, we defined d i as the average duration of trials in that group. we then determined the number of bins n i as the duration of the fastest trial in the ith group divided by 50 ms and rounded down to the nearest integer. each trial in the ith group was binned into n i bins, the average firing rate was computed in each bin, and a psth was computed by averaging the firing rates across all trials in the group and smoothing with a 20-point square window. thus, the five psths contained the firing rates f ik, where i is the group number and k is the bin number from 1 to n i ."
"several approaches have been proposed making use of the auditory and/or haptic modalities to enhance the user experience, to enable the use of the system in situations when visual attention cannot be on the device, and to cater also for visual disabilities. however, the vast majority of these systems have been designed for navigating a defined route or finding a specific place, thus limiting their use for truly exploring the city, which is also a relevant mode of sightseeing [cit] . this leads to our primary research question: how to design a non-visual interaction technique that supports exploratory, serendipitous discovery of pois (see fig. 1 for a conceptual illustration of the setting.)."
"the lund time machine [cit] mobile application makes use of ambient sounds related to the history of a location to contextualize the user to the setting. the application uses by default vibration bursts for feedback when scanning the environment with the device, but also offers the possibility to use sound or speech for guidance."
"overall twelve people (five women) volunteered for the experimental trials, half of whom were assigned to the baseline group (three women), whilst the other half (two women)"
"as long as the communication system keeps strongly connected, the microgrid system still realizes voltage-frequency synchronization and economic optimization. from figure 9a -c show the simulation results, which state clearly that the microgrid system tends to be consistent after a slight shock under the proposed control method."
"in what follows, to specify the effectiveness of the offered technique, the adaptive finite time stabilization technique is employed on the above-mentioned disturbed chaotic flow. consider the chaotic flow having a single unstable node [cit]"
"however, hand-held visual interfaces may suffer from issues such as screen reflections interfering with content visibility [cit] and require visual attention on the device, which results in sharing cognitive resources between the device and the environment [cit] . this may disconnect the user from the surroundings and hamper the user experience [cit] . a remedy for these issues can be provided by headmounted displays or non-visual interfaces."
"all the participants appreciated the intuitiveness and ease of use of both the tactile glove and the baseline application. some participants in the tactile glove condition commented spontaneously after the trial \"i enjoyed it!\" one user commented \"i liked it! it really works, it was very precise."
"remark 2. the proposed distributed algorithm is applicable to the strong connected communication topology by estimating the mismatch between the load demand and the actual output generated power. unlike the existing distributed control methods for the edp, the method in this paper can be applied to the undirected graph, which does not need a two-way communication to ensure power balance."
"in the past, the edp has usually been solved in the upper controller as scada, in which the communication speed is lower than the secondary controller, leading to power mismatching of demand and supply. figure 4 shows the condition at different communication speed. from the above figures, we can see that the slower the communication speed is, the longer the power mismatch will be, and the poorer the economy performs. therefore, the edp is settled in the secondary control level in this paper, which can increase the communication speed without increasing the investment cost."
"in addition to a post-test interview, two questionnaires were administered. one was filled before the experimental session and aimed at collecting background information regarding participants' age, origin, level of knowledge of the city, and how participants were used to sightseeing a city as tourists. the questionnaire completed after the trial explored users' impressions of the user experience. the survey explored participants' general feeling of naturalness and pleasantness in using the devices (3 items) and their impressions regarding the feedback (5 items). finally 3 items explored the fear of appearing strange in the eyes of other people while using the device. participants using the tactile glove were asked to complete a longer version of the questionnaire, in which they were also asked to evaluate the audio signals they received (5 items) and the physical sensations they had while wearing the tactile glove (5 items both systems logged timestamped data that contained users' behavior including time spent in each stage, decisions of accepting/rejecting the notification, and distance to the poi, as well as environmental data like gps accuracy."
"in this article, an adaptive gsmc method is investigated to remove the reaching phase and overcome the chattering phenomenon. in the presence of the nonlinear function ψ in the global sliding surface, the damping ratio of the chaotic flow with external disturbance is enhanced and the robustness of the chaotic flow is improved. besides, an adaptation process is used in gsmc which approximates the disturbances' unknown upper bounds and satisfies the finite time convergence of state trajectories to the global sliding surface. as a final point, some numerical simulations on chaotic flow with a single unstable node are provided to prove the efficiency of the suggested approach. the main purpose of this article is to propose an adaptive gsmc framework to stabilize the chaotic systems in the existence of external disturbances with unknown bounds."
"a sound was played continuously by the speaker during the trial. at the beginning of the trial, the sound was a 2-khz pure tone, ~ 80 db spl. whenever the joystick was deflected by more than 2° from the horizontal, the frequency of the tone was increased using the following formula:"
"a glove has been chosen as the form factor of our system for several reasons. firstly, a glove allows natural hand gestures, which enables an embodied experience in interacting with the environment. point-by-finger interaction naturally fits our tourism context, and information associated with a specific poi could be accessed via hand gestures projected on the poi itself. moreover, fingers are more sensitive for tactile stimuli than, for example, the forearm [cit] ."
"we present a novel location-aware, wearable audio-tactile system with an exploratory approach to find pois. the interaction technique is designed for free exploration, without the need for pre-defined itineraries or direct contact with a mobile device. the system, based on a sensor-equipped tactile glove, gives audio-tactile cues indicating the category of a poi when the user is approaching it. if the user shows explicit interest in the poi, the system can guide the attention of the user towards the poi by audio-tactile feedback. tour guide-like spoken descriptions give more information. our main hypothesis, which is assessed through a comparison study, is that the user experience of the proposed system outperforms a visual mobile application. based on the findings, we discuss the implications for the design of non-visual poi exploration. we contribute to the field of multimodal interaction with a novel non-visual technique for exploring pois in cities, its evaluation in a field study, and implications for the design of non-visual tour guides."
"the core idea of the interaction technique is to allow the user to freely walk around in the city and discover pois. this is achieved by proximity-based audio-tactile notifications. the user's location is tracked by the gps sensor on the mobile device. each poi is associated with a proximity radius which is determined by its physical size and visibility. for example, a prominent palace has a larger radius than an ordinary shop. considering using the real environment as the genuine visual cue of our non-visual system, the visibility of the poi also affects its proximity radius. if the poi is only visible after passing a corner, the radius is reduced to the distance between the corner and the poi. when the user crosses a proximity radius of a specific poi, the actuators on the glove vibrate to notify that a poi is nearby. simultaneously, a contextual auditory icon is played through headphones to indicate the category of this poi. in the current table 1, and was carefully chosen to yield designs that are easily distinguishable from each other and from typical environmental sounds. the duration of each auditory icon is approximately 2.5 seconds. the audio-tactile notification lasts for 15 seconds, with the auditory icons playing back repeatedly, accompanied by vibration bursts (100 ms activation with 400 ms silence) on the glove. during this period, the user can make a selection gesture by bending the index finger. alternatively if they are not interested, they can reject the recommendation by bending the thumb or ignore the cue until it fades out. selection gesture results in hearing the spoken name of the poi, followed by a brief (max 10 seconds) spoken description of the poi. this spoken introduction is meant for giving the user more autonomy over choosing interesting pois, and they can skip the poi if they find it uninteresting."
"the evaluation followed between-subjects design with two conditions. the primary condition involved using the proposed glove-based interaction technique for exploring the pois. as a baseline condition, we implemented a mobile application manifesting relevant features competitive with commercial products available on the latest market. figure 4 depicts users in both conditions."
"first, the users of the baseline application need to divide their attention between the visual guidance and the real surroundings, repeatedly switching their gaze between the screen and the environment, while the glove-based solution gives people the chance to keep their visual attention on the surroundings. potentially, extending the arm to a \"pointing posture\" while holding the phone would better align the gaze, the screen, and the target, but this does not seem like an intuitive way of interacting with the smartphone. none of the participants applied this strategy."
"as an example of a technique for serendipitous discovery, [cit] developed audio bubbles, an application that uses virtual geolocated spheres as auditory \"homing beacons\". when a person enters an audio bubble, they start hearing a sound that relates to the poi in the center of the bubble. the approach empowers the traveler to choose which audio bubbles they want to attend to. in a somewhat similar fashion, [cit] studied the use of earcons and spatial audio in an augmented reality \"sound garden.\" the system was tuned for presenting information on multiple targets at the same time, making use of source separation enabled by the spatial audio. their results indicate that two simultaneous sound sources are distinguishable, but with three or more the task becomes difficult."
"when the user has listened to the initial description, they start getting directional guidance to spot the poi. the assumption is that the poi is in view of the user. the rationale behind this step is that there may be several pois of one category in close proximity to each other. for example, it is common to have multiple restaurants or cafes side by side, and finding the correct one may require effort."
"in the evaluation we focused on the validation of the interaction technique and user experience. in terms of efficiency, the result shows that our non-visual interface performs as well as its visual counterpart. positive user experience was achieved with both conditions. however, glove users were able to focus their attention on the environment, while baseline users paid much less attention on their surroundings. in this section, we present the results in detail."
"each ith psth could now be described by the β values of all bins and the corresponding firing rates: (β ik, f ik ) for all k from 1 to n i . this parametric variable has the feature that the ratios of the three α coefficients determine the extent to which its value scales with the three real variables t ik (press), t ik (release) and fˆi k ."
"in addition to haptics, audio has been used in many ways in tour guide applications and related systems. the use of sound in these systems can be categorized to guidance cues [cit], notifications [cit], contextual sounds [cit], and spoken information content [cit] ."
"the participants of the proposed interaction technique wore the tactile glove in their right hand. participants carried a neck pouch with an android device (samsung galaxy s3) inside. the phone was used to run an application with the experiment logic, communicating wirelessly with the glove. audio feedback was presented through an over-ear headset connected to the phone. although an in-ear headset would have ensured better acoustic insulation, an over-ear headset was preferred due to hygienic reasons and to not disconnect the subject from the environment."
"to those observed during spatial navigation, including theta modulation and precession (extended data fig. 4), as well as a larger number of fields per cell in mec than in ca1 (extended data fig. 5 )."
"in ca1, some of the spikes were produced during sharp-wave ripple (swr) events. during spatial navigation experiments, such events are typically excluded from firing rate maps by rejecting low-velocity time points. in the smt, we instead excluded swrs by directly detecting them in the local field potential 33, as follows. raw voltage signals from the electrodes were downsampled by a factor of 10. signals were then band-pass filtered in the 140-230 hz range (stop-bands below 90 and above 280 hz) using a parks-mcclennan optimal equiripple fir filter. the power of the band-passed signal was computed, smoothed with a 100-point square window, and the median value was measured across the four wires of each tetrode. swrs were detected as peaks in the resulting trace that exceeded 3 s.d., but were separated by more than 312 points (100 ms). spikes that occurred within 100 ms from each swr were excluded from the analysis. exclusion of these spikes did not qualitatively change any of our results, but tended to increase the ratio of the in-field firing rates to the background."
"the interaction technique has been designed to be paired with a backend, which takes into account the preferences and poi history of the user to filter and personalize the content. in this paper we focus on the interaction technique."
"to measure the strength of the firing rate modulation in the smt, we computed the mutual information rate between spikes and the phase of the task 34 using the following formula"
"however, smc cannot fulfill the convergence of the state trajectories to zero in the finite time. through the reaching phase of smc, the system has not the robust performance and the parametric uncertainties and external disturbances can destabilize the control system."
"rats initiated trials by pressing the joystick and poking. the poke had to either precede the press (without an un-poke in between) or follow the press by less than 250 ms (without a release in between). if a press was not followed by a poke within 250 ms, the trial was not initiated, and a new trial could be started only after the joystick was released. trials were terminated by releasing the joystick and un-poking. the un-poke could either follow the release (without a press in between) or precede the release by less than 250 ms. an un-poke that was not followed by a release within 250 ms was considered a premature termination of the trial; in this case, no reward was delivered, and a new trial could also be started only after the joystick was released. animals using version 1 of the apparatus, which lacked a nosepoke, initiated and terminated trials by pressing and releasing the joystick, respectively."
"the measures recorded during the test include observations, questionnaires, and logged data. the entire trial was video-recorded to allow off-line analysis of the users' behavior. furthermore, during the testing session, the correct identification of each poi was recorded online by the experimenter, who explicitly asked the participant to indicate the location to which the guidance referred."
"gi is a transient process reference value of the active power in the iterative calculation process, which will converge to the optimal reference active power generation p * gi under the above distributed algorithm. η i and ρ i are proportional coefficients, and they should satisfy the following expression:"
"however, efficiency is not always the highest priority [cit] . in our case, achieving a more immersed experience where users' attention is on the real surroundings is a key consideration. in addition to the non-visual directional guidance, this immersion is in part also supported by the auditory icons, which augment the surroundings with contextual information. it should be noted that interpreting the auditory icons might not require much cognitive load but a certain time duration is necessary to be spent on listening to decode the information."
"because the microgrid can be controlled in the grid-connected, islanded, and synchronizing modes, every dg in the microgrid should obtain its own operating mode by properly choosing the appropriate algorithm. when a master dg receives a grid-connect signal, because the frequency, voltage, and phase of the microgrid in the pcc are inconsistent with the main grid, the microgrid system should first enter the synchronizing mode. during the process of synchronization for dgs, the microgrid must meet the following synchronization criterion in the ieee standard 1547 [cit] for the 0-500 kva rated capacity [cit] ."
"where f n is the sound frequency at time step n, θ is the amount of joystick deflection in degrees, δ t is the duration of the time step, and α is the traversal speed, chosen randomly from a uniform distribution at the beginning of each trial. the uniform distribution was chosen for each animal such that the range of trial durations was typically 6-12 s for version 1 of the apparatus and 4-8 s for version 2 of the apparatus. at each time step n + 1, the speaker produced a logarithmic sweep of tones from f n to f n + 1 . whenever the joystick deflection was less than 2°, sound frequency was unchanged. the range 15-22 khz was defined as the 'target zone' . when the frequency exceeded this range, white noise (80 db spl) was played instead of the pure tone to indicate overshooting of the target zone."
"in general, participants praised the alerts coming only in the proximity of a certain poi, instead of having to pay attention to a paper map or a guide during the entire walk. two participants in the baseline group explicitly said they felt detached from the environment, as they were focusing their attention on the application: \"i was certainly thinking of the smartphone and not looking so much at the city.\" on the other hand, in the group of the tactile glove users, the feeling of being disengaged from the surroundings occurred only for one respondent, with one user reporting an opposite feeling \"i felt like i was in a museum.\""
"analyses of the passive playback experiments were the same as above, but the onsets and offsets of the sound sweeps were used as anchor points instead of the presses and the releases of the joystick. analysis of theta oscillations. to analyse theta oscillations, the voltage from each electrode was band-pass filtered with a parks-mcclennan optimal equiripple fir filter (pass-band: 6-12 hz, stop-bands below 1 hz and above 17 hz) and the median value across all wires of a tetrode was measured. forward and reverse filtering was implemented (matlab command filtfilt) in order to produce no phase shift in the signal. the theta phase was determined by measuring the angle of the hilbert transform of the filtered signal."
"in this paper, a two-layer control system for agc/avc [cit] in an ac microgrid was built as in figure 2 . in the first layer, the agc/avc obtains real-time measurement data through a communication network, displaying these data in the supervisory control and data acquisition (scada) system, while in the next layer is a real physical layer that consists of dgs, loads and local controllers (lcs)."
"where f * and e* indicate the rated values, m i and n i are the p-f and q-v droop coefficients of the i-th dg, respectively. because the voltage-frequency recovery and synchronization problems are the first two problems to be solved, thus m i and n i are seen as constant values, while edp is described in detail in the next section. in addition, ∆f i and ∆v i are the additional control variables that are used to restore the output frequency-voltage to the nominal values with the changing load."
"the glove proved to be a good form factor for \"browsing\" the city on the go. while other wearables such as shoes [cit] or belts [cit] could be used for guidance as well, the glove provides a nice integrated package for versatile interactions both as an input device using onboard sensors and as an output device via the tactile feedback. furthermore, these interactions make use of a standard practice in tourism, i.e., pointing at things in the surroundings."
"we proposed a novel interaction technique for exploring pois in cities. the technique, based on audio-tactile feedback and a tactile glove, was evaluated in a field study and was shown to be as successful as a visual baseline application for locating and identifying the pois. interestingly, there were no major differences in questionnaire-based user experience between the two techniques, but observations and interviews revealed that the proposed technique let the user focus more attention on the surroundings than on the device compared to the baseline. also, while the glove represents an unfamiliar technique and relies on pointing with the hand to operate, the results indicate that it was not considered to be more obtrusive to use than the baseline application, suggesting that its social acceptability does not pose significant problems compared to more standard mobile applications. in the future, we will integrate the interaction technique with a backend, which will personalize the poi content according to user preferences."
"in version 1 of the apparatus, rats occasionally moved their heads while deflecting the joystick. to eliminate this possible spatial confound, the following modifications were made in version 2. a custom-made nosepoke was attached at the centre of the wall that contained the joystick handle (that is, the centre of the nosepoke was horizontally aligned with the joystick handle). the nosepoke was 2 cm wide and could be triggered by breaking an infrared beam (7 cm above the floor, 3 mm from the wall). the lever arm of the joystick was shortened to 13.5 cm, with 1.5 cm protruding into the chamber; thus, deflection of the arm required a force of 0.022 n per degree at the tip. the lick-tube and the led were positioned closer to the joystick handle on the same wall as the joystick, 8 cm to the left. the lick-tube was also shortened to 2.5 cm. sound modulation task. all behavioural paradigms were implemented using our software package, virmen (virtual reality matlab engine 31, http://virmen. princeton.edu). custom routines for virmen were written to implement navigation in acoustic spaces and to synchronize the acquisition of behavioural and electrophysiological data. software monitored the rat's behaviour and defined four types of event: 1) a 'press' was defined as a downward deflection of the joystick exceeding 2° from the horizontal. 2) a 'release' was defined as a decrease in the amount of deflection to less than 1.5° from the horizontal, lasting longer than 250 ms. 3) a 'poke' was defined as a breaking of the infrared beam in the nosepoke. 4) an 'un-poke' was defined as restoration of the infrared beam for longer than 1 s."
"the experiment was conducted in the center of a mid-sized city, in a touristic area with a lot of applicable pois. we fig.3 ). the pois included, e.g., a church, two cafes, two shops, a statue, and facades and towers of old buildings."
"the problem of communication failure often occurs in practical application scenarios. however, the microgrid system should keep stable in this condition. the proposed method well deals with the situation. in this case, this simulation is divided into three stages."
"we next determined the set of parameters (α press, α release, α frequency ) for which the five psths were maximally correlated to one another. for each pair of psths i and j, we first determined the range of β values on which these two psths overlapped. this range was from max("
"to detect firing fields, we smoothed the psth of each cell with a 20-point square window. we then defined a threshold that was 2 s.d. of the firing rate, but not below 0.2 hz and not above 1 hz. any maximum of the psth that exceeded this threshold was considered to be a peak of a firing field. two neighbouring fields were then merged if either they were separated by less than 2 s, or all values of the firing rate between them exceeded 75% of the smaller peak firing rate of the two fields. to determine the full extent of each field, we subtracted the baseline from the psth, defined as the fifth percentile of the firing rate. the extent of the field was then considered to be the contiguous period containing the peak and exceeding 50% of the field's peak firing rate in the baseline-subtracted psth."
"in summary, previous solutions for tactile or auditory guidance in cities typically do not support free exploration and serendipitous discovery of pois. while a few counter-examples exist, they tend to either lack more fine-grained guidance, do not provide more detailed information on the poi, or present the user with potentially too many alternatives at a time. in addition, many systems require carrying a mobile device in hand and rely on visual feedback. our system not only supports serendipity in discovering the pois but also emphasizes users' autonomy to choose which recommended pois they are interested in."
"the audio guidance works together with the vibrations to enhance the perception of the directional cue. an amplitudeand-phase-modulated wideband tone, with the fundamental frequency of approximately 200 hz, is played through the headphones with stereo panning, guiding pointing to left and right with the same metaphor as with the vibrations. the pulsation frequency of the modulated sound is much faster than that of the vibration bursts, in order to minimize possible negative cross-modal rhythm perception effects that could result if the two cues have a similar pulsation tempo but are slightly out of sync. when the user is pointing at the correct target, they hear the spoken name of that target and feel the vibrotactile on-target cue."
"proof of theorem 1. because the voltage-frequency restore and synchronization control method are the same, to simplify the analysis, only stability analysis for the proposed algorithm to solve the frequency recovery is shown here. for the control input u fi of the i-th dg, the following lyapunov function is considered:"
"and k pps is a positive proportional coefficient. in the past, the load demand p d can be computed in a centralized mode, which is no longer applicable to the current distributed and scatter microgrids. here, the load demand p d can be estimated in a distributed mode, which will be described in detail in section 3.3.3. when the total load demand is known, ∆p can be calculated by (23), and then the ∆p is delivered to the leader dgs and transmitted to the follower dgs by a distributed communication network."
"to detect grid cells, we computed the grid score 23, using the exact procedure described in ref. 31 . the grid score measured the spatial correlation of a cell's rate map to its own rotation at 60° and 120° and compared it to the correlation at 30°, 90° and 150° rotations. firing rate maps with symmetry that was specific to 60° had high grid scores. we measured the 95th percentile of the grid scored across all shuffled samples from all the mec cells we recorded. in our dataset, this value was 0.46. cells whose grid score exceeded this value were considered grid cells. grid spacing was determined by computing the firing rate autocorrelation, selecting the 6 peaks in the autocorrelation closest to the peak at (0,0) and measuring their average distance from (0,0). we detected fewer grid cells in the smaller environment that we used than in the larger one. this is consistent with previous studies (for example, ref. 35,) and might potentially be due to an insufficient number of fields for reliable grid detection or to boundary influences on the firing of grid cells 36, 37 . we therefore verified all comparisons of grid and non-grid cells on the subset of cells that were recorded in the larger environment."
"once the animal was moved to the random foraging arena, a red and a green led were plugged into the lateral edges of the recording headstage. an overhead video camera was used to record the locations of these leds. thresholds were applied separately to the red and green channels of the videos, and the centres of mass of the pixels that passed the threshold were identified. a line segment connecting the red and green centres of mass was defined, and the animal's location was defined as the midpoint of this line segment. the head direction was defined as the angle of a vector perpendicular to this line segment. electrophysiology. tetrodes were constructed from twisted wires that were either ptir (18 μ m, california fine wire) or nicr (25 μ m, sandvik). tetrode tips were platinum-plated (for ptir wire) or gold-plated (for nicr wire) to reduce impedances to 150-250 kω at 1 khz."
"where v i and v j are the real output voltages of the i-th and j-th dgs, correspondingly, and v pcc and v maingrid are the voltages of the point of common coupling (pcc) and the main grid, respectively."
"firstly, according to the first-order optimal condition of the lagrange function, the optimal incremental cost can be solved by a traditional centralized mode [cit], which shows as follows:"
"to detect border cells, we used the border score, described in ref. 5 . this score captured cells whose activity was selectively adjacent to one or more walls of the environment. border cells were defined as cells whose border score and spatial information were both above the 99th percentile of the corresponding values measured on the shuffled samples."
"according the third control object of agc, some methods are designed to allot the changing loads among dgs to minimize the operating cost, i.e., edp. in other words, we needed to design the reasonable droop coefficient to make the active power of dgs equal to the reference active power and to ensure economic optimization."
"the angular width of the target ranges from 23 to 45 degrees depending on the physical size of the poi, the current distance, and the run-time gps accuracy. the minimal angular width was derived from a pilot study, in which users repeatedly pointed with the glove at four marked dots in different directions. the sensor readings were sampled, and the mean deviation was set as the minimal target size."
"for an ac microgrid with multi-parallel dgs, supposed that the communication topology is undirected and sparse communication between dgs, then all the frequency f i and voltage v di of dgs will tend to be converged to the reference value f* and e* under the action of control input u fi and u vi ."
"online content methods, along with any additional extended data display items and source data, are available in the online version of the paper; references unique to these sections appear only in the online paper."
"wherep di (t) is the local demand estimated value for i-th dg, which only updates its own estimate value with the help of communication network of the secondary control level. p gi (t) is the real output active generation for i-th dg at t time. in this way, all demand estimated values would converge to a same value in steady state:p"
"other form factors of wearable tactile guidance for pointing, such as gloves, have mainly been applied in indoor settings (e.g., [cit], in which hand movements can be tracked visually, e.g., with depth cameras. while these feedback techniques can be efficient in guiding the pointing hand, the requirement for visual tracking fits poorly in outdoor contexts. this can be overcome by integrated sensors [cit] that track the hand orientation."
"where u f1i, u f2i, u v1i, and u v2i are two auxiliary control inputs for frequency and voltage restore, respectively. they are obtained by exchanging measured information with their neighboring dgs. base on the two-layer control system for agc in section 3.1, distributed auxiliary control inputs for frequency-voltage regulation is proposed, and the specific expressions are as follows:"
"since participants were free to accept or reject the notification, four glove users and two baseline users actively rejected or passively ignored some of the notifications. we also observed that not all pois were successfully recommended to users due to poor gps coverage in some conditions. the recorded accuracy of gps was typically good, ranging from 5 m to 30 m with an average of 12.37 m. with a single user at two pois the accuracy dropped to 50 m and 100 m."
"because the smt evolves along a continuous axis (sound frequency), it is analogous to spatial navigation on a linear track. our results show that the smt shares some key features of neural representation with this spatial task. just like location, the non-spatial dimension is represented in the hippocampal-entorhinal system by discrete firing fields that continuously tile the entire behavioural task. several other properties are shared between the two tasks, including a tendency of mec cells to produce multiple fields 2, a clustering and tightening of fields at salient features of the task 26, 27, and a dependence of firing on behavioural context 28 . critically, spatial and non-spatial representations are produced by the same neuronal population, suggesting a common circuit mechanism for encoding fundamentally different kinds of fig. 3 . right, spatial firing rate maps for random foraging; the maximum firing rate is indicated. cells 2 and 4 were silent during the smt; cells 3 and 4 were silent during foraging. all firing rate scales are from 0 hz to the nearest integer number of hertz above the maximum firing rate. b, activity of mec grid cells during the two tasks. cells 5 and 6 are from module 1 in the same rat and are plotted on the same firing rate scale. of these cells, only cell 5 was active during the smt. cells 7 and 8 are from modules 2 and 3, respectively. cell 9 is a border cell. c, normalized information for all 918 ca1 cells during the smt, as in fig. 3, plotted against normalized spatial information (the mutual information between spikes and the location, divided by the average value from samples with shuffled spike timing). points are coloured and shaded according to whether the cell was a place cell and whether it was smt-modulated. information values in the two tasks are not expected to be similar owing to the different task structures. d, normalized information for all 881 mec cells during the smt plotted against the cells' grid scores. points are coloured and shaded according to whether the cell was a grid cell and whether it was smt-modulated. e, cumulative histograms of the average field width for all 48 grid cells in module 1 and all 51 grid cells in modules 2/3. groups were separated at 42 cm. inset, distribution of grid spacings across cells and a mixture of three gaussians fit to the distribution. peaks corresponding to modules 1-3 are numbered."
"we evaluated the interaction technique in a guided, taskbased experiment. in this section, we present the venue, method, procedure, and materials used in the evaluation."
"the agc and avc are commonly used in the traditional grid scheduling, and they are also used to manage and synchronize all kinds of dgs to satisfy the power demand of the microgrids [cit] ."
"inspired by the above ideas, a two-level algorithm for an ac microgrid with five dgs is proposed in this paper, which combines the voltage-frequency regulation for traditional agc/avc and edp in the secondary control level. the proposed algorithm was helpful to work out the suboptimal economical operation of microgrids caused by the time scale inconsistency between the secondary level and the tertiary level. in addition, a distributed local demand estimator is designed to estimate the total power demand replacing the traditional microgrid central controller (mgcc). furthermore, the proposed secondary voltage-frequency recovery algorithm can be operated in three operation modes, namely, islanded mode, grid-connected mode, and synchronized mode, and thus it is in line with the flexible, reliable, and plug-and-play characteristics of microgrids. furthermore, the proposed secondary control is fully distributed using a ring communication topology, which enhances the system stability, especially when there is a failure in the communication network of the system. this paper is organized as follows: section 2 presents a basic agc/avc workflow and its control objectives. section 3 introduces a distributed secondary control algorithm based on the distributed communication network for agc/avc, which contains a voltage-frequency recovery algorithm, an economic incremental algorithm for economic operation of a microgrid and a local load demand estimator, and then proves the stability of the proposed control algorithm. some simulations under different conditions are presented in section 4. finally, the conclusion and future work are drawn in section 5."
"unless otherwise stated, node 1 is the only master dg, which can receive voltage-frequency deviation signals and synchronization instruction from the upper controller. it can be verified that the communication graph is strongly connected. for a three-phase islanded ac microgrid, the generator parameters are as follows:"
"in this study, we used gps for positioning. while it performs generally well in open areas outdoors, the positioning may be inaccurate in certain places such as in very narrow streets. for these situations, the positioning accuracy should be taken into account when notifying the user about new content in proximity. clearly, if the interaction technique would be adapted for indoor use, for example in museums, a different positioning technique would be required, involving perhaps the reading of fiducial markers ( [cit] ) or other specific approach tuned for those situations."
"next, we defined model parameters α press, α release and α frequency and defined a parametric variable β ik for each kth bin in the ith psth:"
"this organization of this article is formed as follows: section 2 covers the problem description of the chaotic flow with a single unstable node. section 3 presents the design approach of the adaptive finite time stabilizer using global sliding mode scheme. in section 4, the designed adaptive finite time stabilization technique is employed on the disturbed chaotic flow. finally, in section 5, conclusions are drawn."
"traditionally, exploration of points of interest (pois) in cities has relied on printed materials such as maps and guide books. the smartphone era is effectively rendering these techniques obsolete, with a plethora of available interactive maps and tourist guide applications. techniques range from permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. [cit], november 9-13, 2015, seattle, wa, usa. copyright is held by the owner/author(s). publication rights licensed to acm. visual augmented reality (e.g., [cit] to mixed reality approaches for displaying content on 2d maps(e.g., [cit] . more complex interaction techniques have also been developed for mobile 3d maps (e.g., [cit] . in addition, location-aware mobile applications, such as foursquare 1, are redefining the way tourists explore new places."
"for each poi, we prepared tour guide-like content related to the poi in both written and spoken format. the content included the name of the poi, a short introduction (roughly 10 seconds) to the poi, and a longer (approximately 30 seconds) description. each content was also associated with the corresponding auditory icon representing the category of the poi. to ensure that all the contents were equally accessible to all participants and to avoid them going outside the gps coverage, we defined a route to be followed within the area."
"second, while there is no visual feedback from the glovebased system, the surroundings themselves can provide strong visual cues to accompany the audio-tactile guidance. the combined information of the auditory icon, the spoken name of the target, and the short spoken description provide the user with relevant information to be able to distinguish the poi visually from others. when facilitated by the audiotactile feedback and the possibility of not having to look at the device, the proposed solution is actually slightly faster in the guidance step than the visual baseline."
"the cost coefficients α i, β i, and γ i are positive real numbers. the c i (p gi ) may include generation cost, maintenance cost and so on. moreover, equation (4) does not contain reactive power, which is because the reactive power cost is less than 1% of the active power cost [cit] ."
"in this simulation, we compared the power sharing method and the economical method proposed in this paper. from figure 8d,e, it is evident that the power generation cost is reduced under the proposed control method, and figure 8a -c show the voltage-frequency and active power dynamic process under different control strategies."
"the directional guidance on the glove assumes that the user starts pointing with their glove-wearing hand with the palm facing down, browsing the environment to find the poi. the pointing interaction was chosen, because pointing at landmarks is a common phenomenon among tourists. based on the sensor readings of the imu on the glove and the gps locations of both the user and the poi, the system computes the angular difference between the pointing direction and the actual direction of the poi. this information is then applied to the directional audio-tactile cues. when the user is very close to the poi, poor gps accuracy might result in inaccurate guidance. to compensate for potential locally sub-optimal accuracy of the gps, if the user is estimated to be within 10 m of the poi, the system stops giving guidance and simply plays back a spoken audio piece: \"you are now in front of\" followed by the name of the poi."
"in previous studies, non-visual interfaces have not been considered as efficient as their visual counterparts (e.g., [cit] ). our evaluation surprisingly indicates that the performance of the presented non-visual solution is not significantly different from the visual baseline application for locating the pois. this might be explained by two considerations."
"to cover the shortages of the central control mode, this study employs a continuous-time coordination control algorithm to restore the voltage-frequency deviation. firstly, the equation (8) is differentiated to get the following expression:"
"since the non-visual interface provides no visual reference, in some situations the user might be unsure whether she is pointing at the correct poi. on a related note, in this study the system relied on manually determining the visibility of the pois along the given path. we are currently working on determining poi visibility on the system side."
"the above equation (13) is only available for the islanded mode. as for the grid-connected and synchronizing modes, the rating value f* and e* can be replaced by the frequency and voltage of the main grid. moreover, this control algorithm can not only achieve frequency-voltage recovery and synchronization, but also is robust for distributed systems, because once the system has severe communication faults, the system becomes completely decentralized mode and can still keep stable for a period. this advantage has very important practical significance."
"usually, the agc/avc is applied to manage the microgrid in the secondary control level. the agc/avc workflow is displayed in figure 1, and the microgrid is planned to attain the following objectives [cit] :"
"1. serendipitous discovery of pois is supported by locationaware, contextual audio-tactile cues, which deliver the information of the category of a nearby poi. the user has the option to skip or pursue the recommended poi."
"the baseline (fig. 5 ) mimics the behavior of popular mobile apps for city exploration. two main features presented here are push notification of recommended poi, as in foursquare 2, and showing direction with compass, as in tripadvisor city guides 3 . when a new poi is nearby, the app triggers a vibration and shows a notification suggesting to check out the poi (fig. 5a ). users can decide to either discard the notification or to see more info about the poi (fig. 5b), including picture, textual, and audio description. similarly to our main system, the baseline also provides directional guidance to spot the poi, but using visual feedback as opposed to tactile feedback with an arrow pointing towards the poi (fig. 5c )."
"we implemented a behavioural model to determine whether animals preferentially used sound frequency, the amount of elapsed time, or a combination of the two to perform the smt. the model consisted of two parameters: f 0 (measured in octaves) and δ t (measured in seconds). we simulated each trial by assuming that the rat released the joystick at a time δ t relative to the occurrence of frequency f 0 . for trials in which the joystick was released before the occurrence of frequency f 0, we used linear extrapolation to determine when f 0 would occur if the frequency continued increasing with the average speed of the trial. we then measured the mean squared error between the joystick release times simulated by the model and the actual release times. parameters f 0 and δ t were optimized to minimize the average mean squared error across all trials of a given behavioural session. spike sorting. we filtered electrode signals using a parks-mcclennan optimal equiripple fir filter (pass-band above 1 khz, stop-band below 750 hz). the sum of the four signals from each tetrode was computed, and thresholds of − 3 and + 3 s.d. were applied to the summed data. peaks in the data exceeding these thresholds but separated by more than 32 points were identified, and waveforms from 12 points before each peak to 19 points after each peak (1 ms total) were extracted. we computed the first three principal components of the extracted waveforms from each tetrode. each waveform was then considered in a 7-dimensional space defined by its projection onto the three principal components and its peak-to-peak amplitudes on the four tetrode wires. clustering was performed manually in two dimensional projections of this space using custom-written software in matlab. if two clusters on the same tetrode on two subsequent recording sessions had a mahallanobis distance of less than 20, they were considered to belong to the same unit. in this case, data from the two sessions were pooled. neurons whose average firing rate in any recording session exceeded 5 hz (in ca1) or 10 hz (in mec) were considered putative interneurons and excluded from analysis. firing in acoustic tasks. for the analysis of activity in the smt, sessions were first broken into individual trials. the starts of the trials were defined as the midpoints between each press of the joystick (starting with the second one of the session) and the previous release. the ends of the trials were defined as the midpoints between each release of the joystick (ending with the one before the last one of the session) and the next press. each trial therefore consisted of three time intervals: the prepress interval, the interval between the press and the release, and the post-release interval. these intervals were different in duration across trials. for the following analyses, we therefore linearly time warped each of the three intervals to its median duration across trials. after warping, time in each of the three intervals was divided into an integer number of bins, such that the bins were on average as close as possible to 50 ms across trials. firing rates and psths were calculated in these bins."
"in this study, we developed a distributed secondary control strategy for voltage-frequency recovery and edp. the control algorithms are based on an undirected ring network with a leader dg. compared with the existing control strategy, the proposed control algorithm is fully distributed, which can realize the synchronization of voltage-frequency by exchanging additional signals ∆f and ∆v with neighboring units. the proposed distributed control algorithm can effectively avoid the communication fault situation, which has important practical significance. in addition, this method can also be applied in the grid-connected mode and the synchronizing mode, but in those cases, e* and f * are replaced by the voltage and frequency of the critical point. moreover, in this paper, a strict proof is presented and some cases are also provided to illustrate the proposed control algorithm in detail."
"believe the dynamics (8) and suppose that assumption 1 is fulfilled. as a result, the state ) ( 1 t x of the sliding mode dynamics (8) converges to zero, exponentially."
"here we tested this idea by recording from hippocampal and entorhinal neurons during a task that required rats to use a joystick to manipulate sound along a continuous frequency axis. we found neural representation of the entire behavioural task, including activity that formed discrete firing fields at particular sound frequencies. neurons involved in this representation overlapped with the known spatial cell types in the circuit, such as place cells and grid cells. these results suggest that common circuit mechanisms in the hippocampal-entorhinal system are used to represent diverse behavioural tasks, possibly supporting cognitive processes beyond spatial navigation."
"cells could have multiple fields during the sound presentation period, and different fields occasionally appeared to align differently to task events. we therefore performed analysis separately for each field. we defined the period at which the firing rate was above 20% of the maximum firing rate within a firing field and set firing rates outside of this period to 0."
"participants first gave informed consent to participate in the study and then filled in a short background questionnaire. next, the experimenter explained how the device, either the tactile glove or the baseline application, depending on the experimental assignment, worked. when the user confirmed that the system functioning was clear, she received the device and the training phase began. this stage had the aim of familiarizing the user with the device and the interaction modalities involved. the training consisted of a guided exploration of a small square, on which there were two pois that the participant had to explore. participants were free to repeat the training phase if needed. when the user reported as ready, the experimental trial started. during the trial, the participant was instructed to follow the experimenter along the pre-defined route (approximately 800 meters in length). during the experimental session, the participant was asked to avoid any interaction with the experimenter, with the exception of reporting which building or monument they perceived to be attending. participants using the iphone were free to hold it in their hand or keep it in their pocket. when the participant reached the end of the route, she was asked to fill in a short questionnaire and to answer a brief semistructured interview."
"once the user is pointing at the correct target and makes a selection gesture, they hear a longer spoken description of the poi. this description may include, e.g., facts related to the history of a building, or the cuisine of a restaurant. the user can continue the exploration when the spoken description is finished, or make a rejection gesture to terminate."
"on average, the centre of the kth bin in the ith group was at a certain time relative to the press of the joystick; we defined this time as t ik (press) . it could be computed as"
"usually, the first two goals discussed in the previous section are realized in the secondary control level by the central control mode, while the final object for edp is solved in the tertiary control level, which decreases economic efficiency of the microgrids. meanwhile, the central control pattern needs to gather the global key point information, which is a great challenge regarding communication construction and investment, especially when the number of dgs is large and their geographical locations are scattered [cit] . therefore, a secondary distributed control algorithm is proposed in this section, which combines traditional secondary control and distributed control strategy with a small amount of communication. next, we introduced this control algorithm and analyze its stability."
"load demand is always changing in real situations. because the distributed secondary control method proposed in this paper cancels the tertiary control and the mgcc, the load demand becomes difficult to calculate, and these loads are scattered on the microgrid. inspired by the average voltage estimator [cit], a load demand estimator based on distributed average consensus algorithm is presented in this paper. the details are as follows:"
ontologies have proven to be suitable for addressing heterogeneity within sensor-based systems. they are mainly used to 1) abstract sensors and their capabilities and 2) annotate observations according to common concepts to enable their sharing and understanding by other systems. the most popular [cit] provenance sensor or mechanism that has output the observation.
"symbolic execution is in effective in achieving high code coverage for data race detection. however, for symbolic execution, solving path constraints and exploring code paths are often time-consuming when analyzing complex and large-scale driver code."
"invoke code generation template. if this is the case, we need to invoke modisco's java code generation only for this model fragment (i.e. the block containing the body implementation)."
"before diving into results, we kindly remind the reader that iqas is a prototype of an integration platform for qoo assessment as a service [cit] . iqas is interoperable, extensible, configurable and usable by stakeholders with different skills and interests. in this paper, we present new unpublished material by detailing 1) concrete implementation choices that we made during the development of our platform and 2) experimental results for the three deployment scenarios. please note that the source code of any iqas component presented in this paper is available upon simple request."
"object-oriented modeling is centered around class diagrams, which constitute the core model for the structure of a software system. from class diagrams, parts of the application code may be generated, including method bodies for elementary operations such as creation/deletion of objects and links, and modifications of attribute values. however, for user-defined operations only methods with empty bodies may be generated which have to be filled in by the programmer."
"modification. if the eoperation is modified in the ecore class diagram (e.g., renaming, changing parameters or return type), these changes cannot be propagated to the generated code, since the corresponding code fragments are protected. instead, a new method declaration with empty body is generated."
"where w m is the energy density of the magnetic field, and w m is the magnetic field energy within the volume v . the control voltage equation of the suspension electromagnet can be obtained as follows:"
"the absolute reference frame is used as the standard where downward is the positive direction and upward is the negative direction [cit] . based on the fundamental knowledge of electromagnetics, the airgap flux of the suspension electromagnet, which neglects the magnetic flux leakage, can be written as follows:"
"in this paper, a nonlinear dynamic model of magnetic suspension based on the flux density feedback is established. a hybrid magnetic flux density observer with a mixed current and voltage is proposed to overcome the difficulty of installing the magnetic flux sensor. next, this observer is combined with the adaptive sliding mode controller to reduce the requirements on the upper bound of the uncertainty and disturbance and improves the stiffness of the suspension system. the simulation results show that if this control method is adopted, the parameter stability area increases and the control parameters, which satisfy the system dynamic performance, can be easily obtained. moreover, the influence of the control parameters on the system dynamic performance is not obvious. for external disturbances, the controller has a better robustness than the current feedback sliding mode controller, and the controller is easier to implement in the structure. finally, the experimental results demonstrate that the presented control strategy significantly improves the robustness and dynamic performance. in our future work, we will attack the maglev vehicle-guideway interaction vibration problem in the turnout junction by utilizing the flux feedback method. qingquan qian received the b.s. [cit] . he is currently a professor of electrical engineering with southwest jiaotong university, chengdu. he is a member of the chinese academy of engineering. his research focuses on the analysis, operation, and design of traction power control systems. volume 6, 2018"
"extensibility (nf2) and interoperability (nf3) requirements have led to the development of the qoonto ontology (see figure 1 ). this ontology, which makes the link between figure 1, an observationvalue may have a qoointrinsicvalue, which is related to a qooattribute (e.g., accuracy) and consists in a qoovalue (e.g., 100%). besides, within iqas, domainspecific experts (e.g., meteorologists) may use their knowledge in order to develop a new qoopipeline, which is the succession of several transformation functions (called qoo mechanisms) successively applied on observations that flow throughout the iqas platform."
"the first step identifies a variable as a possible raced variable if it satisfies two requirements: 1) it is a possible shared variable, and 2) it is accessed with the protection of at least one lock. the second step records two kinds of information, namely locksets and driver functions that are concurrently executed. moreover, interrupt status is also recorded. when a hardware interrupt is raised while a driver function f is executed, the device driver suspends executing the function f to execute the corresponding interrupt handling function. in this case, we need interrupt status to identify that the interrupt handling function, not the function f, is executed."
"lockset. dilp intercepts the calls to lock and unlock functions. it records the function name and arguments, which are used to differentiate the held locks for variable accesses."
"2) kernelstrider does not identify which driver functions are concurrently executed at runtime. for this reason, it may report false data races when the involved driver functions are never concurrently executed. dilp monitors the execution of driver functions, and identifies call-path pairs of concurrently executed functions to reduce false positives in race detection."
"as future work, we plan to pursue the study of the qoo notion and perform more complex deployment scenarios with iqas. we also plan to conduct an extended performance evaluation campaign of the platform."
"2) the techniques that are used to reduce runtime overhead in dilp are effective. without these techniques, dilp slow introduces about 125.9x runtime overhead (281.7x in the network throughput and 9.0x in the cpu utilization) on average, which is much larger than dilp. the high runtime overhead of dilp slow is introduced by recording repeated possible shared-variable accesses and frequently writing the log file."
"in device drivers, many data races are caused by a common pattern that we call inconsistent lock protection. to detect such data races, we propose a runtime-analysis approach, named dilp. it uses code instrumentation to monitor driver execution and collect runtime information. then, it performs an offline analysis of the collected information, to detect data races caused by inconsistent lock protection. we evaluated dilp on 12 linux device drivers. it found 25 new real data races, and 11 of them have been confirmed by driver developers."
"dilp can be still improved in some aspects. firstly, as a dynamic approach, dilp does not cover the entire driver code at runtime and may miss data races in the uncovered code. to address this limitation, we will introduce fault injection and fuzzing techniques to improve code coverage and detect more data races. secondly, the runtime overhead of dilp can be further reduced. to achieve this goal, we will introduce static analysis to reduce the amount of instrumented code. finally, we only implement dilp to test linux device drivers. we will port dilp in operating systems to test their device drivers."
"by introducing three deployment scenarios for a custom qoo-aware integration platform, the goal of our study is to promote the qoo notion and the need for characterizing observation quality (qoo assessment as a service). in order to draw more insightful lessons, we focus on specific attributes specifically tailored for each use case: observation accuracy within smart cities, observation rate for virtual sensors pertaining to the web of things and, finally, freshness when observations are collected in a peer-to-peer decentralized fashion as within post-disaster areas."
the proposed observer model can accurately observe the flux density. this overcomes the problem that the current model is not accurate at high frequencies and the voltage model is not accurate in low frequencies.
"to address these issues, we refine the dynamic lockset-based approach by adding more constraints on the set of variables that are considered and the locksets that are compared. rather than keeping track of a global lockset for a variable, we collect the specific sets of locks held at each accesses. furthermore, we only compare locksets of accesses that occur in functions that are actually executed concurrently, thus providing stronger evidence that the accesses may conflict. we only perform this check when one of the accesses is a write and when one of the accesses is protected by at least one lock (we refer to the accessed variable as a possible raced variable), reflecting developer understanding that a concurrent access is possible. if a variable is accessed within two concurrently executed functions, the sets of locks held around each access are disjoint, at least one of the locksets is non-empty, and at least one of the involved accesses is a write, then a data race may occur. in fact, this data race is caused by inconsistent lock protection for the possible raced variable in the two concurrently executed functions. as compared to eraser, we can drastically reduce the set of false positives, because we only report races when there is strong evidence that a race is possible. furthemore, by focusing on variables that are protected by at least one lock, and are thus more likely actually shared, we are able to keep the runtime overhead manageable."
"from table i, we find that more than 38% of the accepted patches fixing data races involve inconsistent lock protection. the remaining patches target data races of other patterns, such as atomicity violations and forgetting to disable interrupts where needed. in the patches involving inconsistent lock protection, most data races are fixed by: 1) adding new calls to lock and unlock functions to protect the raced variables; or 2) moving existing calls to lock and unlock functions to a place that can protect the raced variables. these race fixes suggest that the fixed data races were introduced because: 1) the driver developer forgot that the function containing the access can be concurrently executed with another function having access to the same shared variable, which causes the lack of lock protection; or 2) the driver developer remembered to add necessary lock protection in the related function, but the lock protection is not complete. indeed, these two reasons are difficult to avoid in driver development, because a device driver often has many functions and complex control logic [cit] . thus, inconsistent lock protection is likely to be introduced, which may cause data races in device drivers."
"add eannotation. finally, the eoperation from the ecore model is extended by an appropriate eannotation: a genmodel-source annotation with key body is created. the corresponding value is the result of the code generation run in the previous task described above (cf. fig. 8 )."
"locate method implementation. in the java model, the corresponding methoddeclaration is located. we make sure, that it really contains a respective body implementation and the appropriate javadoc tag."
"possible shared-variable accesses. dilp performs a dynamic taint analysis from two kinds of basic possible shared variables, namely global variables and pointer-type function arguments. figure 3 shows the procedure of this analysis for a function func. dilp maintains two sets of possible shared variables, namely shared set for func and global set for the driver. before func is executed, dilp initializes shared set to the set of pointer-typed arguments of the function. while func is executed, dilp intercepts each instrumented instruction inst in this function (lines 5-16). for inst, dilp gets its result value ret val, and then handles each operand operand. if operand is a global variable, dilp adds it in global set (lines 8-10). then, dilp compares operand with each stored variable in shared set and global set according to the memory address (lines [cit] . if operand belongs to either of the two sets, operand is indicated to be a possible shared variable. accordingly, the affected variable ret val is regarded as a possible shared variable for the function func. in this case, dilp adds ret val in shared set, and collects the calling context of variable accesses for ret val in shared set. func exits, dilp deletes the set shared set (line 17)."
"this paper is structured as follows: in section 2, we discuss our contribution. a detailed example showing a use case for our approach is described in section 3. related work is discussed in section 4 before section 5 concludes the paper."
"dilp in total finds 25 real data races, and 11 of them have been confirmed by driver developers. the evaluation also shows that the runtime overhead of dilp is lower than many previous approaches. the rest of this paper is organized as follows. section ii presents the motivation of this paper. section iii introduces dilp in detail. section iv presents the evaluation on linux drivers. section v compares our work to previous approaches. section vi makes a discussion about dilp. section vii shows the related work, and section viii concludes this paper."
"built upon the iot which mainly refers to the deployment and the interconnection of smarter communication-capable things, the internet of everything (ioe) extends this paradigm by going beyond things to also integrate societal impacts, risks 978-1-4673-9944-9/18/$31.00 [cit] ieee and economic benefits of a more interconnected world. [cit] . in this report, cisco has defined the ioe as \"the networked connection of people, process, data, and things\". compared to the iot that was mainly driven by technology, the ioe has been coined to rather envision business and deep changes in society. we believe that the ioe vision is particularly appropriate as we are witnessing the emergence of more and more sensor-based data-centric systems. indeed, this paradigm acknowledges the importance of data (and therefore of data quality) within sensor-based iot systems."
"2) separate recording thread. if the runtime information is recorded in the driver thread, two problems will occur. firstly, the driver thread will execute much extra code, introducing much runtime overhead. secondly, as writing files in the kernel can sleep, when the driver thread is executed in atomic context [cit] (such as holding a spinlock) and dilp records the runtime information into the log file, a sleep-in-atomic-context bug will occur and can cause system hang or crash [cit] . to solve these problems, dilp records the runtime information in a separate kernel thread. when the driver is installed, dilp creates a recording thread and makes this thread sleep. during driver execution, dilp wraps each piece of the runtime information as a message and stores this message in a message queue, and then wakes up the recording thread. the recording thread fetches each message from the message queue, and records the information in this message. when the message queue is empty, the recording thread sleeps again."
"3) race detection: in this phase, with the collected runtime information, dilp first identifies possible raced variables and their accesses, and then detects data races caused by inconsistent lock protection. finally, dilp produces data-race reports containing the locations of raced variables and information about the context in which the possible race occurs."
"as mentioned previously, most of the suspension control methods and dynamic characteristic analyses are based on linearized models, which are implemented in a very small neighborhood near the equilibrium point. therefore, when the system deviates from the equilibrium point, the linearized model becomes invalid. whether the system can continue to use the linearized model when the suspension state deviates far away from the equilibrium point requires further consideration. in addition, various control algorithms derived from the flux feedback method need to be studied further. with the development of nonlinear science and modern control theory, advanced control algorithms, such as the optimal control, fuzzy control, adaptive control and sliding mode variable structure control, have achieved success in many fields. however, these algorithms are not widely used in the field of magnetic suspension systems. combining the advanced control algorithm with the flux feedback control scheme to design a suspension controller with high stability and robustness is a problem that deserves further study. finally, although the use of the magnetic flux as a state feedback has obvious advantages, the hall devices or fluxmeters in the current market are either not suitable for the measuring range or cannot be installed properly. moreover, the measurements are relatively rough, and the calibration of the sensors is not accurate. maglev trains that already operate would also need to cut each coil to embed the magnetic flux sensors, which is not practical. thus, obtaining the magnetic flux without damaging the current coil structure is difficult."
"static analysis can conveniently detect data races without running the tested programs. but due to lacking exact runtime information, it often reports many false positives. for dilp, it could be interesting to introduce static analysis to identify the variables that are never shared between different driver threads. accesses to these variables do not need to be instrumented. this would decrease the amount of the instrumented code, which can help to reduce runtime overhead."
"1) the runtime overhead introduced by dilp is around 7.2x on average. the network throughput of ethernet controller and wireless controller drivers is decreased by about 1/7.5 (the overhead is 7.5x) on average, and the cpu utilization of all tested drivers is increased by about 6.8x on average. this runtime overhead is lower than many previous runtime analysis approaches of detecting data races in kernel-level programs, such as intel's thread checker [cit] that introduces 200x runtime overhead and eraser [cit] that introduces 10x-30x runtime overhead."
"in this paper, we presented an innovative approach for bridging the gap between the model level and the source code level. furthermore, as a proof of concept, we presented an implementation for the eclipse modeling framework. emf requires hand-written method bodies, since it only allows for structural modeling. the emf code generation engine is able to preserve these user-supplied code fragments on subsequent generation steps. however, using this mechanism, modifications and deletions in the ecore model are no longer propagated to the corresponding code fragments."
"based on the above idea, we propose a dynamic approach named dilp, to detect data races caused by inconsistent lock protection in device drivers. overall, dilp consists of three phases. firstly, at compile time, with llvm [cit], dilp instruments some places in the driver code, such as variable accesses and driver functions. secondly, during driver execution, dilp intercepts variable accesses and monitors executed functions. it identifies and records the information about runtime variable accesses and concurrently executed functions. finally, after driver exeuction, dilp analyzes the recorded information to detect data races caused by inconsistent lock protection. we have implemented dilp for linux device drivers."
we would like to thank the linux driver developers who gave helpful feedback on our data-race reports. this work was supported by the natural science foundation of china (project number 61872210).
"the magnetic suspension system is widely applied in many fields, such as maglev trains [cit], magnetic bearings [cit], magnetic suspension sliders [cit], and magnetic suspension precision platforms [cit] . maglev trains have attracted wide attention of researchers because of their unique advantages, such as contactless motion, comfortability and being environmentally friendly."
"we find that most data races detected by kernelstrider are false, for two reasons: 1) the related driver functions are never concurrently executed; 2) the maintained locksets are incorrect when interrupts occur. among the five real data races detected by kernelstrider, one of them is detected by dilp. the other four data races are not detected by dilp, because the related variable accesses are not protected by any lock, and dilp cannot detect data races in such pattern. on the other hand, 12 real data races found by dilp are missed by kernelstrider. thus, dilp has a lower false positive rate than kernelstrider, and finds many data races missed by kernelstrider."
"in this section, we describe our approach to unify emf modeling and java programming. after giving a brief overview about emf and modisco [cit], we discuss our solution in detail."
"happens-before-based approaches [cit] track memory addresses and synchronization events to infer the temporal happens-before relation between two events. when two conflicting memory accesses α and β are on the same memory location, and neither α happens before β nor β happens before α, a data race may occur. for example, djit + [cit] exploits vector time frames to track each shared-variable access, and checks whether this access has the happens-before relation with prior accesses to the shared variable. these approaches report no false positives, but they often miss many real data races, and introduce much overhead due to tracking and infering the happens-before relation during program execution. to our best knowledge, no happens-before-based approach has been used to test device drivers."
"in this paper, we aim at solving the problem mentioned above. initially, we derive a nonlinear dynamic model of the magnetic suspension system based on the magnetic flux density feedback. then, a hybrid magnetic flux density observer is proposed to measure the required magnetic flux density state. next, we design an adaptive sliding mode controller based on the magnetic flux density feedback without any linear processing. the stability of the designed closed-loop system is proven using lyapunov techniques. simulation and experimental results are included to demonstrate that the presented control strategy is effective."
"several quality dimensions have been envisioned in order to characterize observations. among them, we can cite quality of service (qos) [cit], quality of information (qoi) [cit] or even context annotation [cit] . such quality dimensions generally define quality attributes that aim at characterizing how fit-for-use is an observation for a specific use by an application (specific context). table i gives five examples of quality attributes that may be used to characterize sensors or observations. for instance, using both latency (network qos) and timeliness (qoi), an application may better understand if some outdated observations are the result of poor network performances or due to a sensor sampling rate too low. recent related work includes the citypulse framework [cit], which also considers qoi (with the computation of collection point-related key performance indicators) and qoi inspection (with tools such as the citypulse qoi explorer). however, this framework mainly aims at providing large-scale stream processing solutions for smart city applications."
"for a given observation, we assumed that its accuracy was equal to 100% when its value was within the sensor measurement range and 0% otherwise."
"in design, dilp has three important improvements compared to kernelstrider: 1) kernelstrider relies on the specific static information about kernel interfaces, such as the function type and function name. in the implementation of kernelstrider, this information is hard-coded. but this information is specific to each kernel version. by checking the code history of kernelstrider, 3 we find that the most recent linux kernel version that it supports is linux 4.5. we try to run kernelstrider on the tested drivers of linux 4.16.9, but it fails because the static information of many kernel interfaces is not matched. dilp does not rely on such information, and it automatically analyzes the driver code to perform code instrumentation. thus, dilp can conveniently test device drivers of different kernel versions."
"thus, the approach as described in this paper offers a tool-independent solution for this problem. figure 7 shows how the approach discussed in this paper solves the problem shown above and related problems of the same class. the domain engineering phase is carried out as described earlier. once both models are derived in application engineering, there is no need to invoke two different code generation steps. in detail, our automated solution performs the following tasks on both models:"
"our approach dilp uses the idea of locksets but it is different from previous lockset-based approaches. firstly, dilp focuses on data races caused by inconsistent lock protection, and compares the locksets of the accesses to the same variables in different threads. secondly, dilp identifies the functions that are concurrently executed, to help reduce false positives. thirdly, specific to device drivers, dilp considers interrupt handling to reduce the possibility of reporting false races when interrupts occur."
"we apply our approach to a specific problem scenario -the derivation of products in model-driven software product lines. nevertheless, the approach can also be used in single system development."
"in this phase, the driver generator performs code instrumentation and generates a loadable driver. firstly, dilp uses the clang compiler to compile the driver c code into llvm bytecode. secondly, dilp performs code instrumentation on the llvm bytecode. thirdly, dilp uses the clang compiler to compile the modified llvm bytecode into assembly code. finally, dilp uses the gcc 2 compiler to compile the assembly code into an object file, and generates a kernel object file as a loadable driver. dilp mainly instruments three kinds of places in the llvm bytecode:"
"our research work aims at raising awareness about the qoo notion and promoting the characterization of observation quality within sensor-based systems. the challenge is considerable as providing qoo guarantees is an ambitious task that depends on many parameters including use cases, software used and configuration."
"this section will provide the process of the adaptive nonlinear sliding mode controller design for the magnetic suspension system based on the flux density feedback. the analysis for the stability of the closed-loop system using the lyapunov method does not assume any linear approximations. to facilitate the latter design, the system error is defined as follows [cit] :"
"the eclipse modeling framework (emf) with its metamodel ecore is the standard platform for modeldriven software development in eclipse, and it is especially wide-spread in the academic community. it follows a minimalistic and pragmatic modeling approach. ecore only comprises the core concepts of object-oriented modeling and thus allows for a straightforward mapping to java code. figure 1 depicts the development process imposed by emf. typically, modelers use ecore class diagrams to describe the static structure of the software system. the code generator provided with emf maps the class model to a set of corresponding java interfaces and implementation classes. furthermore, the correct semantics of references between the classifiers is ensured."
"4) many of detected data races are caused by reading the raced variable in a branch condition without lock protection. specifically, 7 data races in linux 3.3.1 and 9 data races in linux 4.16.9 are in this case. the data race shown in figure 7 is an example. indeed, reading a shared variable in a branch condition is a small and often-overlooked operation, so driver developers sometimes forget to protect it with a lock."
"ecore models may be annotated with so called eannotations for a number of different purposes. these annotations are used to store information which is not explicitly supported in the ecore metamodel [cit] ). emf's standard annotations may be classified into different categories based on the source it uses for each kind: ecore. annotations of this kind are attached to emodelelements in order to specify additional information which is relevant at both runtime and code generation. genmodel. genmodel-sourced annotations are used to attach information that is only relevant when generating code to respective emodelelements. extended metadata. annotations of this kind are used in models that were created from xml schema. emoftags. tags in emof are used for the same purpose as eannotations are used in ecore. since emf provides an interchange between ecore and emof, emoftags are required to map eannotations to corresponding tags. since the ecore metamodel does not cover method bodies, annotations are required to make them part of a model. since this information is required only during code generation, genmodel annotations have to be used. emf already provides a pre-defined annotation for this purpose: a genmodel-sourced annotation for the type eoperation exists. it allows to specify a key value pair. the key body indicates that the corresponding value field contains java code that implements the operation. however, iterating through the model and adding corresponding annotations for each user-defined methods is a cumbersome and timeconsuming task. thus, we strived for a complete automation of this process."
"so far, we only considered scenarios where sensors had permanent connection to the internet. the objective of this deployment scenario consists in assessing the impact of postdisaster areas on qoo. we define as \"post-disaster\" the environments where sensors could have intermittent connection to the internet. to emulate such environments, we envision delay tolerant networks (dtns) [cit] where observation collection is performed in a decentralized peer-to-peer manner according to the store-and-forward paradigm. to that end, we reused hint [cit], which is a self-developed hybrid emulation system for opportunistic networks where nodes can be either android phones or virtual ones. for more details on our hint emulator, the interested reader can refer to the associated publication. first, we developed a specific asynchronous adapter python class to poll observations from hint as soon as they arrive. we used this adapter to configure a new vsc that acts as a transparent proxy between the hint emulator and the iqas platform. then, we submitted a request without any qoo constraints to retrieve all observations coming from the hint emulator. finally, within hint, we generated 200 observations from a real node to a gateway node. each message had to be first internally exchanged within hint in a peer-topeer manner before reaching the gateway node where it was consumed by our vsc. once that all messages were delivered to iqas consumers, we computed offline the freshness (the age) of the observations when 1) they arrived at the hint gateway node (hint viewpoint) and 2) they were delivered by iqas to consumers (iqas viewpoint). figure 5 depicts the complementary cumulative distribution function (ccdf) that represents the age of the observations from both hint and iqas perspectives. for example, this figure shows that, in more than 10% of time, observations that are effectively consumed by iqas consumers are, at least, 15-second old. beyond freshness' order of magnitude that may vary depending on the hint configuration, the main lesson to retain from this deployment scenario is that the overhead introduced by the iqas platform in terms of delay can be negligible compared to the routing overhead introduced when observation collection is achieved in a peer-to-peer decentralized way. therefore, in order to significantly improve the overall service provided to their end-users, ioe platforms should consider both network qos (e.g., latency) and qoo (e.g., freshness) as complimentary quality dimensions."
"in this section, we present a use case for our solution in the context of model-driven software product line engineering (mdple). please note that the problem is specific to mdple. however, the solution which is presented here can be used in general for every modeldriven software project which is realized with the help of emf."
"3) buffer caching. to reduce the runtime overhead of frequently writing to the log file, dilp first caches the runtime information in a memory buffer, and writes to the log file when the buffer is full or the driver is removed."
"for this first use case, let us consider two stakeholders: matt, the first one, is a city employee in charge of the sensor maintenance; maggie is a meteorologist for a private weather forecast company. let us imagine that matt is asked to check the good working of all visibility sensors across the city. in parallel, maggie is interested in collecting visibility measurements to release a weather report for an upcoming in response, iqas auto-configured itself by creating two observation pipelines, reusing the first pipeline to construct the second one. then, we created a vsc to emit observations corresponding to a raw visibility dataset recorded in the city of aarhus in denmark 5 [cit] at the sensing rate of 2 measurements per second. this dataset was chosen specifically since we were aware that it contained some systematic measurement errors (with some values equal to −9999 km). as soon as we started the vsc, observations started to flow throughout iqas and were delivered to the two vacs. while consuming the visibility observations, the vacs reported to iqas the qoo for the received observations, enabling real-time visualization (see figure 3 for the first 500 seconds of the simulation)."
"the paper is arranged as follows. in section ii, the dynamical models for the maglev system are explicitly provided based on the flux feedback. the hybrid magnetic flux density observer development is presented in section iii. the main results, including the controller design and closed-loop stability analysis, are included in section iv. simulation results are provided in section v, and hardware experimental results are included in section vi. finally, the study's conclusions are provided in section vii."
"we measure the runtime overhead introduced by dilp, to check whether it can heavily affect driver execution. to quantify the runtime overhead, we use common benchmarks to measure the performance of the original drivers and the drivers instrumented by dilp. moreover, to quantify the value of the techniques that are used for reducing runtime overhead (described in section iii-e), we also measure the performance of the tested drivers when handled by a modified version of dilp, dilp slow, in which we drop these techniques. for the ethernet controller drivers and wireless controller drivers, we use netperf [cit] to measure the network throughput and cpu utilization when sending 128-byte tcp bulk data blocks (tcp stream). for sound card drivers, we measure the cpu utilization when playing and recording a wave file for thirty seconds. we test each device driver in linux 4.16.9 five times, and then calculate the average value of the network throughput and cpu utilization. table v shows the results, and we find that:"
"deletion. in case the corresponding eoperation is deleted from the class model, the resulting java code will still be present in the source code after subsequent generation steps."
"furthermore, we explained in detail how this approach provides a significant improvement in modeldriven software product line engineering (mdple): since we use a generic tool chain for mdple, conceptual links between different models, e.g. an ecore model and a corresponding java model containing body implementations cannot be hardcoded in the tool. in order to provide consistency between these types of models, the information stored in both of them has to be integrated using the approach discussed in this paper."
"2) the results in the column \"path pair\" of table iv indicate the degree of the concurrency of the tested drivers. wireless controller drivers have more call-path pairs of concurrently executed functions than ethernet controller and sound card drivers, thus wireless controller drivers have higher concurrency with the tested workloads. indeed, in the experiment, compared to the tested ethernet controller and sound card drivers, the tested wireless controller drivers use more code and provide more kinds of functionalities that can be concurrently performed."
"to the best of our knowledge, our approach is the only one which offers the possibility to combine ecore modeling and standard java programming on the modeling level."
"we presented an approach that makes use of so called genmodel-annotations, allowing to specify information which is not originally supported by the ecore metamodel. in advance, we use modisco to automatically parse the java source code into a corresponding ast model. model transformations are applied to extract the required information from the ast model and to add it to the ecore model using eannotations."
"the emf code generator is always invoked on a so called generator model. the generator model wraps the ecore model and adds some generation-specific meta information, e.g. target directories, code style, and so forth."
the estimation velocity ofŵ can be adjusted using α. choosing a suitable adaptive gain α can effectively reduce the chattering phenomenon. the validity of the adaptive algorithm will be proven using lyapunov theory.
"executed driver functions. dilp intercepts the starting and ending basic blocks of each driver function, to collect the call paths of variable accesses and the call paths of concurrently executed functions. it maintains a list that contains the names of currently executed functions and the ids of currently running threads. for example, when a driver function f1 begins to be executed, dilp records the function name and running thread id as a node in the list. while f1 is executed, if another driver function f2 begins to be executed, dilp searches the list and finds the node of f1. if the running thread id of f1 is different from that of f2, it indicates that f1 and f2 are executed in different threads at the same time, so f1 and f2 can be concurrently executed. in this case, dilp records the call paths of f1 and f2 as a call-path pair of concurrently executed functions. when f1 returns and exits, the corresponding node is deleted from the list."
"based on our basic idea, we propose an automated runtime analysis approach, named dilp, to detect data races caused by inconsistent lock protection in device drivers. we have implemented dilp with the clang compiler [cit] . figure 2 shows the architecture of dilp, which has three parts:"
"to this end, we use the modisco 1 framework. originally, modisco is dedicated to software modernization projects. it provides a set of discoverers for different types of artefacts, e.g. a discoverer for java source code. thus, it allows to parse existing java source code into an ecore-based model, which resembles the abstract syntax tree (ast) of the java language (c.f. fig. 2) ."
"over numerous integration platforms that have been developed for the internet of things (iot), network quality of service (qos) [cit] has shown to be unsuitable for assessing the actual value of observations [cit] . basically, within datacentric systems, quality of observation (qoo) mainly depends on consumer needs and the specific context in which he/she consumes it. therefore, we believe that qoo is a critical issue to address in the forthcoming internet of everything (ioe) [cit] as more and more decisions will be based on services derived from received observations. built on the iot, the ioe paradigm mostly acknowledges the importance of data (and therefore of data quality) within systems, alongside with people, processes and things. due to its wide scope, the ioe encompasses many use cases that may bring as much challenges regarding qoo. among these use cases, smart cities, web of things and peer-to-peer connections between sensing things have already become a reality, waiting for bigger scale adoption. however, each of these uses also brings specific qoo challenges to address."
"calling context. according to possible shared variables, executed functions, locksets and interrupt status, dilp maintains and collects calling contexts of shared-variable accesses. the calling context contains function call path, held locks (lock type and lock object's memory address), accessed variable (variable's memory address) and access type (read or write). it is used to identify possible raced variables in the next phases."
"3) kernelstrider uses the interrupt status to maintain function call paths, but does not use this status to maintain locksets. thus, the locksets maintained by kernelstrider may be incorrect when interrupts occur. dilp uses the interrupt table vi ."
"the motivation to envision qoo within smart cities has already been extensively described in the original paper that introduced the iqas platform [cit] . in the same paper, we have also shown the benefits to apply some qoo pipeline (such as observation filtering) on observation streams to cope with systematic errors of sensors."
"the main threats to external validity are about the introduced overhead and code coverage. firstly, though dilp uses some techniques to reduce runtime overhead, it still introduces 7.2x runtime overhead on average in our evaluation. in this case, the driver concurrency may be affected during execution, and thus dilp may miss some real data races. to mitigate this threat, we will introduce static analysis to reduce the amount of instrumented code. secondly, as dilp is a dynamic approach, its code coverage heavily relies on the tested workloads and driver configuration. in the experiment, we only use common workloads and common configurations to test drivers in normal cases. thus, only some normal execution code paths are covered and related data races are found. to improve code coverage and find more data races, we will introduce fault injection and fuzzing techniques to improve code coverage and detect more data races."
"though dilp focuses on detecting data races and it does not regard performance as a major goal, we still try to reduce its runtime overhead. the overhead is mainly introduced by intercepting variable accesses. we exploit three techniques to reduce this overhead:"
"c. implementation and deployment 1) implementation: we developed iqas following a dataflow component-based architecture (see figure 2 for an 1 http://linkeddata.org overview). main iqas components have been implemented according to the actor model, thanks to the akka toolkit 2 . iqas enables adaptation (with both auto-(re)configuration and qoo-based adaptation) by implementing an adaptation control loop (denoted as the \"mape-k 3 loop\" in figure 2 ). adaptation is continuously performed by monitoring the qoo provided by iqas to its different consumers. when possible, the platform can try to \"heal\" a request by deploying a set of mechanisms chained to form a \"qoo pipeline\". in order to make such a decision, the platform should reason on the qoonto ontology, performing inference to select a suitable qoopipeline candidate, when available."
"in this paper, we consider quality of observation (qoo) as \"the actual observation value for a specific consumer given a specific context\". to remain compliant with previous definitions, we acknowledge the fact that qoo can be impacted by network qos and we consider qos as the assembly of network qos and qoo."
"to validate the effectiveness of dilp, we evaluate it on real linux device drivers. the tested drivers are selected according to three criteria: 1) they should be commonly used in practice; 2) they should be within the driver classes in the study in section ii-b, which we have found to have many data races caused by inconsistent lock protection; 3) they should run as kernel modules, so we can enable dilp by installing the dilp kernel module before installing the driver. according to these criteria, we select 12 linux device drivers, including 6 ethernet controller drivers, 3 wireless controller drivers and 3 sound card drivers. the driver code is compiled using clang 5.0 and gcc 5.4. for each tested driver, we install it in the system, and run it with a workload on four threads, and finally remove it. the workloads are shown in table iii ."
"ideally, software engineers operate only on the level of executable models such that there is no need to inspect or edit the actual source code (if any). in this sense, models are the code (now written in a highlevel modeling language). however, practical experiences have shown that language-specific adaptations to the generated source code are frequently necessary."
"the traditional lockset-based approach, as proposed in eraser [cit], is based on the assumption that all accesses to a shared variable can potentially be executed concurrently and thus need to be protected by locks. accordingly, for each possible shared variable v, globally across the execution, eraser maintains a set c(v) that contains the set of locks that are held across all previous observed accesses to v. c(v) is initially all of the locks available in the system. as an access to v occurs, c(v) is updated to the intersection of c(v) with the currently held set of locks. an error is reported if c(v) becomes the empty set. in practice, however, it is not the case that all accesses to a given variable v can occur concurrently with each other. as a typical example, initialization may occur before the variable is shared, in which case a data race is impossible. thus, the traditional lockset-based approach results in many false positives."
"besides the tested drivers in the evaluation, dilp can be easily applied to other device drivers, because dilp works automatically and does not require specific information of the tested drivers. besides linux device drivers, dilp can be ported in other operating systems (such as freebsd and netbsd) to test their device drivers, because dilp does not rely on specific features of the os kernel."
the eclipse modeling framework (emf) [cit] ) has been established as an extensible platform for the development of mdse applications. it is based on the ecore metamodel which is compatible with the omg meta object facility (mof) specification [cit] .
"many approaches use dynamic analysis to detect data races with runtime information. they are based on the happensbefore relation [cit], sampling [cit] or lockset algorithm [cit] ."
"the rest of this paper is structured as follows: section ii introduces the required background and the related work. section iii presents the integration platform that we used to obtain the experimental results, with a focus on initial requirements and concrete implementation choices that have been made. then, section iv, v and vi presents the three ioerelated deployment scenarios. finally, section vii concludes and describes further perspectives."
"in this paper, we envisioned three ioe-related deployment scenarios for a qoo-aware integration platform. for each of them, we presented experimental results obtained with the help of iqas, a custom integration platform for qoo assessment as a service. to better understand results, we briefly recalled the main requirements for iqas, as well as the main implementation choices that we made during its development. then, by introducing specific qoo attributes tailored for each use case, we managed to draw some qoo-related lessons that integration platforms should take into account in order to correctly fulfill the ioe. hence, the study of observation accuracy within smart cities confirmed that there might be several definitions of a \"good-quality observation\"; the analyze of observation rate allowed us to identify limiting capabilities for virtual sensors; finally, the study of freshness for observations reported in a decentralized way demonstrated that network qos and qoo are two complementary quality dimensions that should be used together in order to improve the overall service provided to end-users, especially within post-disaster areas."
"it can be observed from figs. 14-16 that in the stage of stationary suspension, the airgap is in the target position of 8 mm, and the curve is stable and smooth. the flux density is also stable during the stationary suspension stage at approximately 0.6 t. however, when the train is running, volume 6, 2018 the airgap fluctuates from 6.5 mm to 10.5 mm, which is caused by the rail gap (seam between the two rails). when the speed of the train increases, the flux density becomes larger. this is because higher speeds cause faster changes in the rail gap, which results in a more complex disturbance. moreover, considering the aerodynamic effects, the higher speed can also result in a larger aerodynamic lift and down force [cit], which also requires increased flux density to improve the robustness. however, the disturbances caused from various speeds is different, and the airgap response of the running test maglev train can be consistent under the proposed control strategy. the facts mentioned above prove that the amsc(b) is an effective method with a stronger robustness."
"to improve system performance, modern os kernels (such as linux, windows and freebsd) support multithreading to utilize multicore processors. device drivers thus use concurrency to efficiently communicate with the kernel and hardware. but concurrent execution can introduce concurrency problems. studies [cit] have shown that concurrency problems occupy a large part of reported bugs in device drivers, and these concurrency problems are often hard to reproduce and detect [cit] . data races are a common kind of concurrency problem. a data race occurs when multiple threads access the same memory location without proper synchronization and at least one access is a write [cit] . data races can introduce non-determinism, and may cause serious runtime problems such as null-pointer dereferences and use-after-free errors."
"one area of our research is dedicated to modeldriven software product line engineering (sple). sple [cit] ) addresses the organized reuse of software artifacts. feature models [cit] ) are used to capture the commonalities and differences of members of a product line, while feature configurations describe the characteristics of a specific member thereof. software product line engineering is divided into two levels. (1) domain engineering is used to analyze the domain and capture the commonalities and variabilities in a feature model. furthermore, the features are realized in a corresponding implementation. in model-driven software product lines, models represent the implementation at a higher level of abstraction. (2) application engineering deals with binding the variability defined in the feature model and deriving concrete products."
"whereŵ is the adjustable estimated value of w, x 3 is the flux density feedback, which can be obtained from the flux density observer (15), sign(·) is the signum function, and s denotes the dynamic sliding surface defined by (18) . the adaptive laws ofŵ are proposed aṡ"
"a possible solution would be to add information about the conceptual links between ecore models and the corresponding generated java code to the famile tool chain. however, due to the following reasons, this is not feasible: (1) famile is as general as possible, as the only requirement is that the domain models are based on ecore. no assumptions about certain concepts are made and thus famile may be used for a vast variety of artefacts. (2) the emf code generation could be modified by the emf developers which would then require modifications of famile."
getlockset ( of call-path pairs of concurrently executed functions; 2) their accessed variables reference the same memory address; 3) the intersection between their locksets is empty; 4) one of the accesses is a write.
"in emf, for instance, only structure is modeled by means of class diagrams, whereas behavior is described by modifications to the generated source code. however, emf is already tuned for efficient programming, as it demands for hand-written java code for method bodies. users are able to annotate the respective parts and the eclipse modeling framework uses a code-merging generator to preserve these fragments on subsequent code generation steps."
"after the numerical simulations, significant effort are made to implement the experiments with the aim of examining the practical performance of the proposed control scheme. the experiments are implemented in the national low-speed maglev test line. the full-scale test maglev train and maglev experimental line are shown in fig. 12, and the signal transduction is illustrated in fig. 13 . in the test maglev vehicle, the maglev train is stably suspended from the interaction between the digital controller and the flux density observer. the sampling frequency is 2500 hz, and the experimental parameters of the system and the parameters of the asmc(b) are consistent with the parameters used in the theoretical analysis and simulation. the maglev train completes a dynamic process through running and stationary suspension. the speed of the test maglev train is provided as fig. 10, which shows that 0 s -490 s is the running process whose maximum speed is approximately 100 km/h. from 490 s-530 s is the static process (stationary suspension). the train runs again from 530 s-1150 s whose maximum speed is approximately 100 km/h. the airgap response and flux density of the observer are shown in figs. 11-12 ."
"3) dilp finds 13 and 25 data races in the tested drivers of linux 3.3.1 and 4.16.9 respectively. we manually check the detected data races, and find all of them are real. by comparing the results and driver code of linux 3.3.1 and 4.16.9, we find that the two data races found in the dl2k driver of linux 3.3.1 ...... 1762. } fig. 7 . a detected data race in the e100 driver."
"by reviewing the data-race reports generated by dilp, we have some interesting findings about the detected data races: 1) many of the detected data races involve interrupt handling. specifically, 9 data races in linux 3.3.1 and and 14 data races in linux 4.16.9 are in this case. the main reason is that interrupt handling functions are often concurrently executed with other driver functions, because hardware interrupts are frequently raised when hardware devices work."
"where f m is the magnetic potential, r m is the reluctance, φ m is the total flux, and φ t is the principal flux. the airgap magnetic flux density of the suspension electromagnet is as follows:"
"several approaches exist to associate elements of the feature model with artifacts part of the domain model; in previous publications (buchmann and schwägerl, 2012 ) famile has been presented, an emf-based tool chain for model-driven software product line development using negative variability. all common approaches for model-driven software product line engineering only support homogeneous artefacts. in (buchmann and schwägerl, 2014) extension to famile allowing for heterogeneous projects is described. in this case, the platform of the product line may consist of models and handwritten source code. however, when deriving products, the variability information needs to be kept consistent across all artefacts."
"the phase locus under the asmc(b) is shown in fig. 4 . the state of the system reaches the sliding surface quickly from the starting point, and then moves along the sliding surface to the target position with a smooth phase trajectory. the flux density obtained from the flux density observer is shown in fig. 5 . it is shown that the observer can effectively measure the flux density of the system. the change in the adaptive gainŵ in the asmc is shown in fig. 6 . it can be seen thatŵ can converge to a positive constant in a finite time. the simulation results prove the theoretical analysis thatŵ is bounded. we can learn from the simulation results of the stationary suspension that the asmc(b) is better than linear pid and smc(i) in both dynamic and steady-state performances. the phase locus under the asmc(b) is relatively smooth. fig. 5 shows that the flux density observer can work effectively. we can learn from fig. 6 that the adaptive gainŵ can be adjusted adaptively. to summarize, when the train is suspended statically, the asmc(b) can work efficiently."
"in this phase, using the instrumented code, the runtime monitor identifies and records the calling contexts of shared-variable accesses and call paths of concurrently executed functions. the two kinds of information are gotten from possible shared-variable accesses, executed driver functions, locksets and interrupt status."
"in sple, basically two different approaches exist to realize variability in the corresponding feature implementation: (1) in approaches based on positive variability, product-specific artifacts are built around a common core. during application engineering, composition techniques are used to assemble the final product using these artifacts. (2) in approaches based on negative variability, a superimposition of all variants is created. the derivation of products is achieved by removing all fragments of artifacts implementing features which are not contained in the specific feature configuration for the desired product."
"to validate whether dilp can find known data races, we first use it to test the 12 drivers in an old linux version 3.3.1 [cit] . to validate whether dilp can find new data races, we test the 12 drivers in a recent linux version 4.16.9 [cit] . we also count the variable accesses and concurrently executed function pairs at runtime. table iv table iv, we have three findings: 1) dilp can filter out much unnecessary runtime information. for example, for the drivers of linux 4.16.9, dilp in total intercepts 242m variable accesses, and identifies 31m possible shared-variable accesses by using its dynamic taint analysis. from these variable accesses, dilp in total identifies and records 1.8m distinct possible shared-variable accesses, amounting to 5.8% of possible shared-variable accesses and 0.7% of all variable accesses. thus, many unnecessary variable accesses are not recorded, which can effectively reduce the runtime overhead and simplify race detection."
"theorem 2: the proposed adaptive sliding mode control law (20), along with the adaptive law (21), can drive the error of the airgap to be zero in a finite amount of time."
"in order to integrate hand-written method bodies into the ecore model automatically, first the complete java code is discovered using modisco, resulting in a java ast model (based on ecore). then the procedure iterates over the ecore model and checks for each eoperation if a corresponding body implementation is present in the ast model. if this is the case, a corresponding genmodel-eannotation is created. the eannotation uses a key-value pair to specify the purpose. the value contains the java code that realizes the corresponding method implementation. in order to obtain this java code from the ast model, the code generator is invoked only for the desired fragment (in this case the block which is contained in the corresponding methoddeclaration) and the resulting string is stored in the value field of the newly created eannotation. using our approach, the user has the full benefits of his preferred modeling environment (e.g. the ecore class diagram editor) and his preferred programming environment (e.g. the eclipse jdt java editor). furthermore, code completion and syntax highlighting are available when the hand-written body implementations are created, which is not the case if the standard emf annotation editor is used."
"as mentioned earlier, famile supports the development of software product lines based on negative variability. thus, when deriving specific products based on a concrete feature configuration, all fragments and artefacts which implement unselected features have to be removed. figure 6 depicts the situation that would occur if only standard emf technology without the mechanism described in this paper would have been used."
"to know about the proportion of reported data races caused by inconsistent lock protection, we perform a study of linux driver patches in the patchwork project [cit] . we select the patchwork project, as it is used by a number of linux kernel maintainers to collect patches pending for the next release. [cit] that fix data races, by searching the patch title. we focus on the patches of 6 common driver classes, namely wireless controller, ethernet controller, sound card, multimedia, mmc (multimedia card) and rdma drivers. we get 252 accepted patches that fix data races. among these patches, we identify those that involve inconsistent lock protection. the result is shown in table i . the first column shows the driver class; the second column shows the number of these accepted patches that fix data races; the third and fourth columns, respectively, present the number and percentage of accepted patches that involve inconsistent lock protection."
"this section describes several notions required to fully understand the three deployment scenarios. from things to people, many enablers will be needed to pursue iot and ioe paradigms."
"1) dropping repeated information. during driver execution, a variable may be repeatedly accessed in the same calling context. if dilp completely handles all of these variable accesses, much repeated information will be recorded, which can introduce unnecessary overhead and complicate race detection. to solve this problem, dilp compares each calling context of the intercepted variable access with all recorded calling contexts, and drops this calling context if there is a match."
"as part of the iqas ecosystem, two additional tools have been developed to specifically evaluate functional requirements f1, f2 and f4 of the platform. the first tool is a virtual sensor container (vsc) image, which allows to create virtual sensors that may generate observations at random, from file or by first retrieving them from other observation sources (such as the web) as a transparent proxy. the second tool is a virtual application consumer (vac) image, which allows to emulate fake consumers that submit iqas requests and then consume observations while reporting back the perceived qoo to the iqas platform in real-time. both these tools are two docker 4 container images. we chose to use the docker virtualization for its great modularity and reusability: once a docker image has been defined, it is easy to deploy several container instances that may accept custom parameters at runtime. besides, since virtualization is performed at application level, containers are less resource demanding than common virtual machines."
"in fact, most of the runtime overhead of dilp is caused by intercepting variable accesses during driver execution. we believe that static analysis can help to reduce this runtime overhead. for example, by analyzing the data flow and control flow of the driver code, static analysis can identify the variables that are never shared between different driver threads. then, the accesses to these variables do not need to be instrumented. this would decrease the amount of the instrumented code, which can help to reduce the runtime overhead."
"2) 2 data races in linux 3.3.1 and 5 data races in linux 4.16.9 involve function pointer calls. the data race shown in figure 7 is such an example. without runtime information, it is often difficult to correctly identify the set of functions that may be referenced by a function pointer, making such data races difficult to find by statically checking the driver code."
"during domain engineering, the platform containing all variants is created. this can be done in a model-driven way using ecore models to describe the static structure of the software. then the ecore code generator is invoked (cf. step 1 in fig. 6 ) and hand-written java code is used to supply the method bodies. the hand-written code is added to the generated one and then discovered into an ecore-compliant ast model in order to be able to use famile for variability management on the source code fragments. the user now may annoate the respective artefacts with variability information. annotations concerning the structure are performed on the level of the ecore model. for example, class search shown in fig. 4, contains the operations dfs(node) and bfs(node) respectively, which represent the different search strategies which are used in graphs and which are annotated with the corresponding features from the feature model (cf. fig. 3 ). feature annotations in the ecore model can be used on any level of granularity. e.g. classes, attributes, methods, parameters or references may be annotated."
"where µ 0 is the vacuum permeability, n is the coil number, r is the electromagnet internal resistance, and g is the gravitational acceleration. for the suspension system,"
"the current model is more accurate at low frequencies or static times, but the error is very large at high frequencies. the voltage model is more accurate at higher frequency. however, due to using of the pure integration element, the small dc bias will eventually lead to integral saturation, so the voltage model has a large error at low frequencies. based on these reasons, a hybrid observer model is presented that combines the current model and the voltage model. the voltage model works at the high frequencies, and a high pass filter can be used to remove the observed value of the current model. in the low frequency situation, the current model is preferred, and the observation value of the voltage model is removed through a low pass filter. the model combines the advantages of the current and voltage models and can measure the flux of the full frequency section more accurately."
"a prominent example in literature on software product lines is a product line for graphs. the corresponding feature model is shown in fig. 3 . a graph always consists of nodes and edges (filled dots) an optionally (unfilled dots) one (unfilled arc) search strategy (bfs or dfs) and an arbitrary number (filled arc) of algorithms. furthermore, edges may have a weight or they may be directed. figure 4 depicts the multi-variant domain model of the graph product line. following the model-driven approach, an object-oriented decomposition of the underlying data structure is applied: a graph contains nodes and edges. furthermore it may contain a search strategy and algorithms operating on the graph data structure. for performance reasons, the data structure may be converted into an adjacency list, to speed up certain algorithms. as the model depicted in fig. 4 is the superimposition of all variants, the relation between nodes and edges is expressed in multiple ways: (1) in case of undirected graphs, an edge is used to simply connect two nodes, expressed by the reference nodes. (2) directed graphs on the other hand demand for a distinction of the respective start and end nodes of an edge. this fact is expressed by two single-valued references named source and target, respectively. as stated above, ecore only allows for structural modeling, i.e., it does not provide support to model method bodies. thus, the standard emf development process [cit] ) demands for a manual specification of an eoperation's body by completing the generated source code. in the example, hand-written java source code for all operations contained in the class diagram shown in figure 4 has been supplied. a small cut-out of a method implementation for the class search is shown in figure 5 . in the corresponding ecore model (cf. fig. 4), the search class defines three eoperations. while the emf code generation only creates java code for the method header, the body implementation depicted in figure 5 was supplied manually. in this case, the method implementation also contains variability as the corresponding references between nodes and edges are different depending on the presence or absence of the feature directed in the current feature configuration. please note that the level of granularity supported by famile's variability annotations is arbitrary, ranging from single java fragments over statements, blocks, methods, or even classes and packages."
"after the ecore model has been traversed and the above steps have been excuted for all user-defined eoperations, the models are in sync and the user only needs to invoke the emf code generator (just as in a regular emf project) in order to obtain the final and consistent java source code for the desired product. please note that the proposed solution is the only feasible way to achieve consistency and to allow variability on arbitrary levels of granularity. e.g. a solution which would not use the java model, but would integrate the method bodies into the ecore model directly in domain engineering does not allow to specify variability in body implementations as shown above."
"interrupt status. dilp uses specific kernel interfaces (like in_interrupt in the linux kernel) to check interrupt status. figure 4 uses an example to illustrate the necessity of checking interrupt status. in the example, funca performs a load (read) operation and a store (write) operation, and then calls the lock function mutex_lock to protect the store operation on the variable var3. at this time, a hardware interrupt is raised, and the interrupt handling function intrfunc is executed on the thread of running funca. the function intrfunc performs a load operation on the variable irq_var1 and a store operation on the variable irq_var2, and then returns. the funca continues to be executed on this thread, and it calls the unlock function mutex_unlock to release the lock. if dilp would ignore interrupt status, it would collect two pieces of incorrect information: (1) intrfunc is incorrectly considered to be called by funca. (2) the load and store operations of irq_var1 and irq_var2 are incorrectly considered to be protected by the lock. this is a form of stack ripping [cit] . by checking interrupt status when monitoring each function's execution, dilp can avoid such errors. specifically, when interrupt status is true, dilp creates a separate function call path and the corresponding lockset to record variable accesses and lock usages in interrupt handling."
"to detect data races caused by inconsistent lock protection, we use a runtime analysis involving two steps. the first step identifies possible raced variables. the second step checks all accesses to the identified possible raced variables, by comparing the locksets that protect these accesses. if the intersection between the locksets is empty, data races will be reported."
"the internet of things (iot) [cit] can be defined as a paradigm that envisions pervasive and inter-connected objects (also called things) that can be uniquely addressed, generally through the internet. some examples of these things can be found with radio-frequency identification (rfid) tags, sensors, actuators, mobile phones, etc. within the iot ecosystem, things generally produce data that needs to be collected and processed by gateways or iot platforms. depending on the use case considered, these platforms may provide enhanced services to end users based on received data."
"lockset-based approaches [cit] maintain locksets of shared variables and running threads, and detect data races by computing the intersection between the locksets of each accessed shared variable and its runing thread. eraser [cit] was the first lockset-based approach. it performs binary-code instrumentation to achieve runtime monitoring of shared-variable accesses for each thread, and detects data races by checking whether consistent locksets are used. eraser can test both user-level programs and kernel-level programs. however, lockset-based approaches often report false positives, because they cannot ensure that the involved shared variables of reported data races are actually concurrently accessed during program execution."
"despite the qoo needs and the guaranteed service level agreement (sla) submitted to iqas, figure 4 shows that the platform was only able to retrieve 60 temperature records per minute maximum (for the intervals [0, 60[, [60, 120[, [120, 180[ and so on) . this result is consistent with the fact that the free plan of openweathermap only allows 60 calls per minute maximum for a same api key. integration platforms (such as iqas) are systems of systems. as a consequence, their ability to meet slas is often conditioned by other resources (such as external observation sources, cloud-based infrastructure for commercial platforms, etc.). in fact, the openweathermap api could be seen as a virtual sensor that has a maximum sensing rate of 1 observation per second. furthermore, its capabilities are also conditioned by the ones of the sensors (physical or virtual) that it uses in turn. the main lesson to be learned from this deployment scenario is that ensuring qoo guarantees requires a deep knowledge of the available resources as well as their characteristics. as a result, capabilities of third-party observation sources should always be carefully identified and described. in that way, semantics can help to make the link between an observed symptom (e.g., sensor unavailability) and its cause (e.g., battery drained for a physical sensor; api call limit reached for a virtual sensor). to provide finer qoo guarantees, we strongly believe that more research is required to better describe sensor capabilities (according to their type, as they evolve over time, etc.)."
"the accuracy of the magnetic flux density obtained from the flux density observer method depends primarily on the mathematic model of the observer. the two existing flux observer models are those for voltage and current. without considering the reluctance of the stator core and the influence of the airgap change on the inductance of the electromagnetic coil, the airgap flux density in the current model can be expressed as"
"based on the mechanical structure of maglev trains, there are 4∼5 bogies in a given carriage, each having 4 suspending points. the experiment for the four electromagnetic suspension modules is implemented [cit], which demonstrates that the multiple electromagnets suspension control system can be deformalized into a single-electromagnet control system, as shown in fig. 1 . the single-electromagnet control system includes the suspension electromagnet, suspension controller, maglev rail, power supply and sensor. through reasonable simplification, the schematic diagram of a single point suspension system is illustrated in fig. 2 . the f(b, z) is the electromagnetic force, which is produced from the suspension electromagnet, z(t) is the airgap between the suspension electromagnet and the maglev rail, i(t) is the coil current of the suspension electromagnet, u(t) is the excitation voltage at both ends of the suspension electromagnet coil, b(t) is the magnetic flux density at the suspension electromagnet surface, and a is the pole area of the suspension electromagnet. assuming that the suspension electromagnet and maglev rail are rigid bodies, the suspension system has only the vertical degree of freedom, which is a single degree of freedom system."
"as the variability information (for the static structure) that has been added to the ecore model in domain engineering is not present in the generated code, the discovered java ast model also does not contain it. furthermore, annotating for example an eattribute in the ecore class diagram would require the user to annotate the corresponding field declaration and the respective accessor methods in the generated source code. of course this is not feasible, since one of the goals of the famile tool chain is to keep the annotation effort for the user as small as possible. furthermore, in order to consistently annotate the ecore model and the generated parts in the java model, the user would require knowledge about the ecore code generator. however, the hand-written body implementations may also require variability. e.g. the implementation of the method dfs(node) contains different fragments which are used for directed and undirected graphs respectively. the user may annotate these blocks with corresponding feature annotations. please note that these annotations are performed di- rectly from the eclipse jdt editor. the famile tool chain maps these annotations to the discovered modisco java model as famile operates on ecorebased models only (cf. step 2 in fig. 6 ). during application engineering, when unused fragments are filtered from the multi-variant models, the corresponding target models are derived (emf model' and java model' respectively, cf. step 3 in 6). in an ideal world, i.e. if both models are in sync in terms of variability information, the user could invoke the code generation for the java model and afterwards the code generation of the emf model in order to obtain the final source code for the desired product. however, reality is different: the emf code merging generator does not remove files. for example, an annotated class of the ecore model has been filtered during the derivation process, but it is still present in the java model. if the code generation for the java model is invoked first, corresponding java code for this class is generated which is not deleted on a subsequent run of the emf code generation. the same holds for operations: the emf code generation requires that handwritten code is marked in order to preserve it during subsequent generation steps. in case an eoperation that has been extended with a hand-written body is filtered in the ecore model, this mechanism prevents it from being deleted."
"however, for user-defined operations only the headers of the corresponding java methods are generated. body implementations have to be specified directly in java afterwards. emf uses a code-merging generator in order to deal with these additions to the generated code supplied by the user. corresponding javadoc-tags mark regions in the source code which should be preserved on subsequent emf code generation runs. unfortunately, these javadoc-tags have to be specified before the corresponding method declaration. thus, if a method declaration is annotated with such a javadoc-tag because it contains a userdefined body, it will never be modified by the emf code generator. in particular the following operations may cause problems:"
"ontology for sensors and observations is without any doubt the semantic sensor network (ssn) ontology developed by the w3c [cit] . regarding sensor abstraction, integration platforms generally use ontologies to describe sensor capabilities (what are their type, their sampling rate, their units, etc.). this mechanism allows to consider sensors as abstract observation providers, enhancing their reusability and the global interoperability of the solution."
"however, while graph transformation rules provide a benefit for some complex operations, they even reduce the level of abstraction in terms of control flow or in case the problem which has to be solved demands for an imperative solution rather than a declarative one . thus, handwritten java code is still required for large parts of a software system. xcore 3 is a textual dsl which allows to define both the static structure and the behavior of ecore models. it provides a code generator that allows to generate java code from the xcore specification. however, although xcore has a java-like syntax, it is another new language which users have to learn."
"the problem described in this paper could also be solved using an incremental bi-directional model-totext transformation. so far there is no tool which meets these requirements. there are incremental code generators (e.g. jet, xpand or acceleo) in the emf context, but they only operate in one direction."
"the resulting ecore model now contains the static structure of the software system as well as all hand-written body implementations for corresponding eoperations. all subsequent changes to the structural model may now be propagated correctly to the 1 https://eclipse.org/modisco/ 2 http://www.eclipse.org/acceleo generated code, as there is no need to protect these hand-written parts any longer."
"to avoid the problems described above, integrating the hand-written method bodies in the ecore model is a possible solution. if the bodies are already present when the emf code generator starts its work, there is no need to protect certain methods with corresponding javadoc-tags. as a consequence, changes in the ecore class diagram are propagated to the generated code correctly: deletion. if an eoperation is deleted from the class model, its corresponding body is deleted as well."
"a projection of a two-dimensional function f(x,y) is a set of line integrals. the radon function computes the line integrals from multiple sources along parallel paths, or beams, in a certain direction. the beams are spaced 1 pixel unit apart. to represent an image, the radon function takes multiple, parallel-beam projections of the image from different angles by rotating the source around the center of the image. fig. 3 we can see that some sine waves appear, it is because the radon transform of a dirac delta function (δ) is a distribution supported on the graph of a sine wave. consequently the radon transform of a number of small objects appears graphically as a number of blurred sine waves with different amplitudes and phases. for that reason the radon transform data is often called a sinogram."
in the first stage the image is convoluted with a gaussian filter. then the gradient of the image is measured by feeding the convoluted image. the 2-d convolution operation is described in the following equation.
"the proposed method was tested on five different users showing ten gestures such as a,b,c,d,g,h,i,l,v,y shown in fig. 4 . table 1 shows the recognition results of 10 gestures of 5 users. fig. 5 shows accuracy vs user curve for five different users."
"we propose an automatic system that recognizes static hand gesture for alphabets (a,b,c,d,g,h,i,l,v,y) using biorthogonal wavelet transform. in particular, the proposed system consists of several steps. in the first step images are at first read and then pre-processed. then noise is removed from the image by using filters. in the next step the proposed method detect the edge from the images and then compute projections of an image along specified directions by rt (radon transformation). after that, the biorthogonal wavelet transformation is performed on the projections of an image which we get from the rt (radon transformation). then the gestures are trained by multiclass svm and then testing is performed. after the completion of testing period the gestures are recognized. the overall system is described in fig. 1 ."
"the result shows that the proposed algorithm successfully detects hand gesture with higher accuracy. in order to achieve robustness of the method to varying conditions of operation such as illumination, posture, zooming conditions and skin color, a large dataset of hand gesture is used to train the classifier. the training set of our detection phase comprised of over 800 positive samples and 1500 negative image samples. the algorithm has been implemented in matlab."
"an efficient hand gesture recognition system requires higher class robustness, accuracy and efficiency. in this paper we propose a method for classifying static hand gestures using multiclass svm. there exist many classification algorithms, for example neural networks and classification trees. however support vector machine approach is considered to be a very good candidate for classification problem like hand gesture recognition. this is due to its high generalization performance without the need to add a-prior knowledge, even when the dimension of the input space is very high. features are selected by radon transform and biorthogonal wavelet. proposed system works well on wide range of variation in color, position, scale and orientation with image. the experimental results reveal that the proposed method is better than any other method to detect hand gestures with high classification accuracy."
"in this system we detect the edges of an image by canny edge detection algorithm [cit], because this edge detector performs better than reference algorithms. in this algorithm optimal edge is detected based on some criteria which include finding edges by minimizing error rate, making edges closely to the actual edges to maximize location and marking edges only once when a single edge exists for minimal response. the optimal filter that meets all three criteria above can be approximated using the first derivative of a gaussian function."
"by performing biorthogonal wavelet transformation on the computed projection which we obtained from the radon transformation, the final feature for our recognition system is selected. we used biorthogonal wavelet 3.7 in this system."
"vmd decomposes one real signal into k independent sub-signal u k, which has specific sparsity. this procedure gets the minimum bandwidth estimation of each modal [cit] . the procedure of signal decomposition is to solve the variational problem. the variational model with constraint condition is as follows:"
"following each iteration the attenuation factor in the feedback loop decreases from 1 to α as the estimation becomes more and more precise. subtracting the attenuated symbols from the clipped symbols, the estimated clipping noise can be expressed as"
"in this paper a modified bnc structure suitable for clipped smt signal processing was presented. based on the exit chart, it was shown that the proposed iterative scheme is convergent. it was also described how the clipping technique can be applied in real-life systems for both ofdm and smt modulation. finally, the performance of the bnc smt receiver was verified and compared to ofdm based on ber simulations over awgn and rayleigh channels. for both systems the clipping compensation can be performed and the performance without clipping can be approached."
for modeling the transceiver chain the digital baseband equivalent is used. the model of the transceiver is presented in figure 6 . the radio channel is modeled as a fir filter having a discrete impulse response
"in order to obtain the effective imf, the correlation coefficient (cc) between each imf and original pd signal is calculated. given a threshold t, if the cc is greater than t, the imf will be selected as effective component; otherwise it will be regarded as false component and abandoned. in this work, t is set to 0.3. the cc values of imf for vmd and emd are shown as table 3 . table 3 shows that the cc value of first three imfs is larger than the given threshold, which means these imfs could represent the real components of pd signals. therefore, the first three imfs are selected and analyzed for vmd decomposition. similarly, we can see that the cc value is smaller than the threshold from the fourth imf, which means these imfs contain less information of pd signals. consequently, the first four imfs are kept for emd decomposition."
"assign one hypersphere (a m,r m ) for each sample x m, where a m is the center of sphere, r m is the radius of suprasphere. the objective function of m-th suprasphere can be defined as follows:"
"k 1 k 2 k 3 k 4 k 5 fd o 1, o 2 p 1, p 2 q 1, q 2, q 3 r 1, r 2 - nd o 1, o 2 p 1, p 2 q 1, q 2 r 1, r 2 s 1, s 2 bd o 1, o 2, o 3 p 1, p 2 q 1, q 2 r 1, r 2 - cd o 1, o 2 p 1, p 2, p 3 q 1, q 2 r 1, r 2 -"
"besides ofdm, another fbmc-based multicarrier family is being strongly investigated: the smt [cit] scheme, which is also known as ofdm/offset-qam [cit] . the smt scheme has significantly reduced out-of-band leakage compared to ofdm. however, due to the absence of cp, it is more sensitive to effects of multipath propagation. the smt signal suffers from large papr similarly to ofdm, which makes it especially vulnerable against nonlinearities present in the transceiver chain. for papr reduction of smt signals, only the clipping technique can be applied as the consecutive symbols are overlapping, therefore it is not possible to treat them separately as in ofdm."
"k 1 o 1, o 2, o 3, o 4, o 5, o 6, o 7, o 8, o 9, o 10, o 11, o 12 k 2 p 1, p 2, p 3, p 4, p 5, p 6, p 7, p 8, p 9, p 10, p 11, p 12 k 3 q 1, q 2, q 3, q 4, q 5, q 6, q 7, q 8, q 9, q 10, q 11, q 12 k 4 r 1, r 2, r 3, r 4, r 5, r 6, r 7, r 8, r 9, r 10, r 11, r 12"
"in this paper, four different types of pd signals are extracted with above experimental setup. the extracted pd waveforms are shown in figure 6 ."
the fbmc transmit signal is constructed from n parallel streams as shown in figure 1 . the input for the mth symbol in the kth branch x k [m] is selected from the complex modulation alphabet a. the modulation symbols are upsampled by a factor of n to achieve maximum data rate with critical sampling. for each upsampled signal a specially designed complex modulated prototype filter is applied having the impulse response g k [n] and z-transform g k (z) as
"assign one hypersphere (am,rm) for each sample xm, where am is the center of sphere, rm is the radius of suprasphere. the objective function of m-th suprasphere can be defined as follows:"
"in this paper, four different types of pd signals are extracted with above experimental setup. the extracted pd waveforms are shown in figure 6 ."
"where cm is the penalty factor, representing the trade-off between rm and target samples. ξm,i is the slack variable of hmsvm allowing remote samples staying outside the sphere. lagrange function can be obtained after lagrange multiplier is introduced:"
"for the 0th iteration, with no feedback, p 0 d is calculated according to (13) . for the next iterations, p i d can be approximated as"
"different pd types can produce different effects in insulation materials, but the range may be diverse. to analyze the characteristics of different pd types, pd signals of different models are extracted in the laboratory [cit] . according to the inner insulation structure of power transformers, there are four possible different pd types, including fd, nd, bd and cd. pd models are shown in figure 3 . the experimental setup is shown in figure 4 ."
step 2: select proper initial number of imf according to the center frequency observation and decompose pd signals using vmd into intrinsic mode functions with different characteristic scales.
"in the smt scheme prototype filters with overlapping impulse responses fulfilling the nyquist criterion are applied. due to the advantageous properties of the prototype filter bank, the smt signal will have a better adjacent channel leakage ratio (aclr) than ofdm. with the use of offset-qam modulation, where the real and imaginary data are transmitted with a time offset of a half symbol duration, no data rate loss will occur compared to ofdm. prior to transmission, the symbols are overlapped such that they can be separated at the receiver. in order to maintain orthogonality of the filter bank structure, cp can not be used in smt systems. as a result, techniques with higher complexity must be applied in comparison to ofdm in order to combat the channel-induced intersymbol interference [cit] . the modulated signal for the smt scheme can be expressed as:"
"it can be concluded from above figure that, the contribution rate from the third principle component starts to level off. in addition, the contribution rates are decreasing gradually which can be ignored. therefore, first two principle components are suitable for further analysis which represent most of the vector information. to do so, the original 12 indicators are reduced to 2 new ones. with a similar method, the principle components of k 2, k 3 and k 4 can be obtained, shown in table 7 . it can be seen from table 7 that nine principle components factors are extracted from 48 feature vectors. and the contribution rate in each imf is greater than 80%. given the above, the dimension of feature vectors is reduced to nine after dimension reduction using pca. similarly, with above procedure, the calculated pd parameters of different pd types are shown in table 8 . table 8 . principle components with different imfs."
"as clipping is performed on the baseband digital signal nonlinear distortions will only occur in the baseband, that is, all distortions terms will fall in-band."
"empirical mode decomposition (emd), as an adaptive signal processing method that decomposes a time series into some limited intrinsic mode functions (imfs). it is widely used in the areas of fault detection, signal processing and data compression [cit] . however, due to the problems of ending effects and mode mixing in non-stationary signal decomposition, emd is limited in practical applications. variational mode decomposition (vmd) is a new signal decomposition method, which is widely applied in electrical fault feature extraction [cit] . it is a non-recursive variational decomposition model. in vmd, the central frequency and bandwidth of each mode are determined by searching the experimental setup used to generate pd signals. in section 4 we show the results with their validation. the paper ends with conclusions in section 5."
"due to the complexity among different pd fault samples, the spherical distribution will not appear in low-dimensional space. pd fault samples need to be mapped into high-dimension space using kernel functions to obtain the optimal hypersphere. in recent time, the common kernel functions include radial basic function (rbf) [cit], polynomial kernel function and sigmoid function. after repeating tests, rbf shows outstanding performance. therefore, rbf is selected as the kernel function for hmsvm. it can be defined in equation (18) :"
"numerous signal processing methods have been proposed to reduce the papr of ofdm [cit] such as, amplitude clipping [cit], coding [cit], interleaving [cit], partial transmit sequence [cit], selected mapping [cit], tone reservation [cit], tone injection [cit], and active constellation extension [cit] . each has its own advantage and drawback. clipping introduces distortion, some methods may require higher power, others cause data rate loss, and in some cases additional information must be transmitted to the receiver. furthermore, the computation complexity varies for each technique."
"in ofdm systems, the cp of p samples is assumed to be longer than journal of computer networks and communications 5 the channel's maximum excess delay. as a result, the received symbols on the kth subcarrier after ofdm demodulation can be expressed using (10) as"
"for smt scheme the blocks of the bnc receiver presented for ofdm in figure 7 must be modified and extended. both, the feedforward and the feedback path should be altered, due to the overlapping nature of the symbols and the absence of the cp, as it can be seen in figure 9: (i) the compensation of the clipping noise is performed in time domain before demodulation."
"partial discharge (pd) is an important symptom of insulation degradation for electrical equipment. pd fault diagnosis plays an irreplaceable role in the evaluation of insulation condition [cit] . pd feature extraction is an important step in insulation fault diagnosis. the common methods include statistical atlas (sa) [cit], wave analysis (wa) [cit] and wavelet transform (wt) [cit] . however, sa has the limitations of high request of sampling rate, large data size and slow speed of data processing which are not suitable for on-line monitoring. besides, it is difficult to extract pd phase information during statistical atlas construction. wa is easily influenced by electromagnetic interference. wt has some inherent limitations such as the difficulty of selection of the wavelet basis, wavelet thresholds, decomposition levels, and so on [cit] ."
"in this paper, a novel pd fault diagnosis method is proposed. this method combines pd feature extraction based on vmd-mde and pd pattern recognition based on hmsvm. first of all, four types of pd signals are extracted in the experimental environment, including fd, nd, bd and cd. then vmd is employed for pd signal decomposition. secondly, proper imfs are selected according to central frequency observation and mde values in each imf are calculated. afterwards pca is introduced to select effective principle components in mde as final pd characteristic parameters. finally, the extracted principle factors are used as pd features and sent to the hmsvm classifier. experiment results show the following advantages: the proposed method can extract effective imfs according to vmd decomposition. pd feature information in imfs can be quantified successfully with mde. using pca, the principle components which represent prominent characteristics are effectively selected. with small data size and low computational complexity, this approach overcomes the limitations in traditional pd feature extraction methods. compared with pd feature extraction methods based on emd-mse, emd-mpe, emd-mde, vmd-mse and vmd-mpe, this proposed approach based on vmd-mde achieves higher recognition accuracy and needs less running time, which can improve the diagnosis efficiency to satisfy real time requirements."
"in wireless communications the frequency spectrum is an essential resource. as the unlicensed spectrum is used by an increasing number of devices, the possibility of communication collision is increasing. to avoid this collision, two solutions are possible: extending the frequency limits higher to unused frequency bands at the upper end of the spectrum or reaggregating the densely used licensed frequency bands. both ideas have disadvantages: the use of higher frequencies requires expensive specially designed analog devices; the reuse of the spectrum calls for complex, intelligent, and adaptive systems. in this paper the focus is on the reuse of the spectrum with multicarrier modulations tailored for spectrally efficient applications."
"the ber simulations for channel b can be seen in figure 14 . for ofdm the cp is longer than the maximal channel delay, therefore isi is not affecting the ofdm system, and the effects of clipping can be compensated. for smt the effect of the isi does not severely degrade the system performance, it still outperforms ofdm. for both techniques the effect of clipping can be compensated and the ber after the third iteration approaches the results where no clipping was applied. the ber simulations for channel c are shown in figure 15 . in this scenario the cp of ofdm is shorter than the channel impulse response so an error floor caused by the residual isi can be observed. the presence of isi results in an error floor also for fbmc systems but at a much lower smt."
"each symbol is first weighted by the probability of the mapped bits and then summed up. using these soft symbols a time domain estimation of the ofdm signal is performed. clipping is applied with a level of a max, and the signal is converted back to the frequency domain. the attenuation factor α i must be set in accordance with the output power of the soft mapper. if the estimated extrinsic information for the coded bits is rather low due to low channel snr values, no clipping compensation will be performed. the clipping ratio for the ith iteration can be calculated as"
"in this paper four different types of pd signals are decomposed using vmd method. the vmd decomposition parameters are shown in table 4 . k s is the number of effective imfs calculated as described in section 4.2. therefore, the scale factor is set to 12 in this paper. in the case of fd, mde values of imfs using vmd and emd are shown in figure 10 . figure 10 . mde values of imfs using vmd and emd. therefore, the scale factor is set to 12 in this paper. in the case of fd, mde values of imfs using vmd and emd are shown in figure 10 ."
"hmsvm uses one hypersphere for pattern recognition. hmsvm can not only separate two different classes, but also divide the sample space into two different parts. using hmsvm, the classification of multi-classes was realized directly. compared with ann and svm classifiers, hmsvm obtains higher recognition rate and improves the accuracy and efficiency in pd fault diagnosis. on the whole, this proposed method provided a new scheme for pd fault diagnosis. for further consideration, the proposed fault diagnosis method can be employed in pd on-line monitoring and diagnosis."
in the receiver a similar filter bank is used to separate the n data streams. the separated streams are downsampled to retrieve the transmitted complex modulation values. the fbmc scheme can be implemented in a computationally efficient way using ifft and a polyphase decomposition of the filters g k (z) [cit] .
in this paper four different types of pd signals are decomposed using vmd method. the vmd decomposition parameters are shown in table 4 . ks is the number of effective imfs calculated as described in section 4.2. figure 8 describes the results of vmd decomposition. it can be seen from this figure that the modal components in vmd approach to the real signal. figures 7 and 8 verify the effectiveness of vmd and the superiority over emd. it can be concluded that vmd is more suitable for pd signal decomposition.
"where c m is the penalty factor, representing the trade-off between r m and target samples. ξ m,i is the slack variable of hmsvm allowing remote samples staying outside the sphere. lagrange function can be obtained after lagrange multiplier is introduced:"
"different pd types can produce different effects in insulation materials, but the range may be diverse. to analyze the characteristics of different pd types, pd signals of different models are extracted in the laboratory [cit] . according to the inner insulation structure of power transformers, there are four possible different pd types, including fd, nd, bd and cd. pd models are shown in figure 3 . the experimental setup is shown in figure 4 ."
"in order to obtain the effective imf, the correlation coefficient (cc) between each imf and original pd signal is calculated. given a threshold t, if the cc is greater than t, the imf will be selected as effective component; otherwise it will be regarded as false component and abandoned. in this work, t is set to 0.3. the cc values of imf for vmd and emd are shown as table 3 . table 3 shows that the cc value of first three imfs is larger than the given threshold, which means these imfs could represent the real components of pd signals. therefore, the first three imfs are selected and analyzed for vmd decomposition. similarly, we can see that the cc value is smaller than the threshold from the fourth imf, which means these imfs contain less information of pd signals. consequently, the first four imfs are kept for emd decomposition."
an efficient implementation of (5) is to use two separate polyphase filter banks where two output signals are time staggered and added. this polyphase structure of the smt scheme can be seen in figure 3 . a major difference compared to ofdm is the no cp is applied. the smt transmit signal is identical to the modulated signal
"due to the high dimension of extracted feature vectors, it will cause big burden for pattern classifiers which can directly affect the recognition accuracy. in this paper, the pca method is employed for dimension reduction of initial feature vectors. in the case of k 1, the covariance matrix is constructed to obtain the principal components. the eigenvalue and eigenvector of the covariance matrix are solved for linear transformation of original vectors. to achieve the goal of dimension reduction, those factors whose eigenvalues are greater than 1 are selected as principal components. the eigenvalue and corresponding contribution rates of the covariance matrix are shown in table 6 . table 6 shows that first two eigenvalues are greater than 1, and the accumulated contribution rate is larger than 90%. the contribution rate changes with the variation of principle components, shown in figure 11 . table 6 shows that first two eigenvalues are greater than 1, and the accumulated contribution rate is larger than 90%. the contribution rate changes with the variation of principle components, shown in figure 11 . figure 11 . the variation of contribution rate with principle components. figure 11 . the variation of contribution rate with principle components."
"different pd types can produce different effects in insulation materials, but the range may be diverse. to analyze the characteristics of different pd types, pd signals of different models are extracted in the laboratory [cit] . according to the inner insulation structure of power transformers, there are four possible different pd types, including fd, nd, bd and cd. pd models are shown in figure 3 . the experimental setup is shown in figure 4 . pd signals are detected in the simulated transformer tank in the laboratory. the pulse current is collected by a current sensor with a 500 khz-16 mhz bandwidth. the uhf signal is extracted by a uhf sensor with a 10-1000 mhz bandwidth. the signal received is imported into the pd analyzer. the test condition is shown in table 1 and the experimental connection diagram is shown in figure 5 . pd signals are detected in the simulated transformer tank in the laboratory. the pulse current is collected by a current sensor with a 500 khz-16 mhz bandwidth. the uhf signal is extracted by a uhf sensor with a 10-1000 mhz bandwidth. the signal received is imported into the pd analyzer. the test condition is shown in table 1 and the experimental connection diagram is shown in figure 5 . pd signals are detected in the simulated transformer tank in the laboratory. the pulse current is collected by a current sensor with a 500 khz-16 mhz bandwidth. the uhf signal is extracted by a uhf sensor with a 10-1000 mhz bandwidth. the signal received is imported into the pd analyzer. the test condition is shown in table 1 and the experimental connection diagram is shown in figure 5 ."
"future applications operating in the licensed bands, for example, cognitive radios, favor spectrally efficient fbmc schemes with low out-of-band leakage, minimizing harmful interference between devices using adjacent channels. in this paper two subclasses of fbmc are investigated, both allowing the use of a complex modulation alphabet: ofdm and smt."
"the attenuation factor α i is monotonously decreasing, so the iteration can stop when consecutive iterations give less difference in α than a given limit. the bcjr channel decoder [cit] computes the extrinsic information of the deinterleaved llrs, which are provided by the bnc detector. these extrinsic llrs are used to suppress the clipping noise in the feedback path of the bnc detector."
"step 3: calculate the correlation coefficients between each imf and original pd signal to select effective imfs [cit] . if the coefficient is greater than the threshold value, then keep the imf as effective one. otherwise, abandon the imf. in this paper, the threshold value of the correlation coefficient is set to 0.3."
"where p i d is the power of the remaining clipping noise after the ith iteration. taking into account the large number of samples and applying the central limit theorem, the clipping noise d[n] can be modeled as a gaussian distributed random variable, which is independent of the channel noise w [n] . based on this assumption, passing through the linear channel filter, the power of the bussgang noise p d is multiplied by"
"using the parameters in table 10, hmsvm classifier is constructed for fault diagnosis of three different pd features. the recognition results with emd and vmd decomposition are shown in figures 12 and 13 . figures 12 and 13 demonstrate that the recognition result using emd decomposition is significantly different with that using vmd decomposition. figure 12 illustrates that the recognition accuracy in each pd type is not less than 80% but no more than 90%, which means, using emd decomposition, extracted pd features cannot represent most of signal characteristics. in contrary, figure 13 shows that the recognition accuracy in each pd type is no less than 90%. moreover, in each pd type, there's no misjudged sample with mde. this means that, with vmd decomposition, pd table 10 . hmsvm parameters. figures 12 and 13 demonstrate that the recognition result using emd decomposition is significantly different with that using vmd decomposition. figure 12 illustrates that the recognition accuracy in each pd type is not less than 80% but no more than 90%, which means, using emd decomposition, extracted pd features cannot represent most of signal characteristics. in contrary, figure 13 shows that the recognition accuracy in each pd type is no less than 90%. moreover, in each pd type, there's no misjudged sample with mde. this means that, with vmd decomposition, pd features can effectively represent most of signal information. besides, from above two figures, it gets figures 12 and 13 demonstrate that the recognition result using emd decomposition is significantly different with that using vmd decomposition. figure 12 illustrates that the recognition accuracy in each pd type is not less than 80% but no more than 90%, which means, using emd decomposition, extracted pd features cannot represent most of signal characteristics. in contrary, figure 13 shows that the recognition accuracy in each pd type is no less than 90%. moreover, in each pd type, there's no misjudged sample with mde. this means that, with vmd decomposition, pd features can effectively represent most of signal information. besides, from above two figures, it gets a satisfactory result with mde parameters."
"the convergence behavior of a turbo loop can be examined using the extrinsic information transfer (exit) chart, developed by ten brink [cit] . it is used to investigate the iteration behavior of a turbo loop based on the exchange of mutual information. this powerful tool enables the tracing of mutual information exchange between the bnc detector and the channel decoder over the iterations."
"in this paper, four different types of pd signals are extracted with above experimental setup. the extracted pd waveforms are shown in figure 6 . pd signals are detected in the simulated transformer tank in the laboratory. the pulse current is collected by a current sensor with a 500 khz-16 mhz bandwidth. the uhf signal is extracted by a uhf sensor with a 10-1000 mhz bandwidth. the signal received is imported into the pd analyzer. the test condition is shown in table 1 and the experimental connection diagram is shown in figure 5 ."
"the signal information and much more important information distributes in other scales. mde can effectively detect the dynamic variation of pd signals which represent the fault characteristics with different scales. it can be found from the figure that mde values start to level off after scale 12. therefore, the scale factor is set to 12 in this paper. in the case of fd, mde values of imfs using vmd and emd are shown in figure 10 . figure 10 shows that with the variation of scales, mde values extracted by vmd are different. however, mde values extracted by emd seems to be same with the increase of decomposition scales which makes it difficult to distinguish different imfs. the initial fd feature vectors combined with the mde of all imfs using vmd decomposition are shown in table 5 . table 5 . initial feature vectors."
"step 1: extract different types of pd signals in experimental environment, including floating discharge (fd), needle-surface discharge (nd), ball-surface discharge (bd) and corona discharge (cd)."
"where α is the quadratic penalty factor. alternate direction method of multipliers (admm) is introduced to obtain the saddle point of such lagrangian function, which is the optimal solution. the procedure of vmd can be summarized in the following steps:"
"in this paper, we propose and evaluate three approaches to surgical gesture classification from video. the first approach uses linear dynamical systems (ldss) to model each video clip from each surgeme. distances between the parameters of the ldss are then used to classify new video clips. the second approach is a bag-of-features (bof) approach in which a dictionary of spatiotemporal words is learned from spatio-temporal features extracted from all video clips. each video clip is then represented with a histogram of such words and distances between histograms are used to classify new video clips. the third approach combines the lds and bof approaches using multiple kernel learning (mkl). our experiments on kinematic data from a typical surgical training setup show that methods based on ldss already outperform state-of-the-art approaches based on hmms [cit] . for video data, the bof approach performs better than the lds approach, while the mkl approach performs equally well in terms of accuracy, but is typically more robust. overall, our main conclusion is that methods based on video data perform equally well as methods based on kinematic data for a typical surgical training setup. this result should encourage further investigation of video based techniques for surgical gesture classification as videos potentially carry more unexploited information than kinematic data."
"for chr vs. rem patients, ge yielded a higher predictive accuracy than the best result reported by schmaal and colleagues (balanced accuracy: 73%) when not accounting for age."
"in table 11, two comparisons are presented between mde-ds with binomial crossover and with blending crossover. in table 11a four representative functions are shown from bench-1 and in table 11b [cit] test-suite for this comparison purpose. in both of the cases, gaussian noise with noise strength of 0.2 is considered. from tables 11a and b it can be seen that mde-ds with blending crossover every time outperforms its counterpart equipped with traditional binomial crossover. this empirically shows that blending crossover gets an edge over traditional binomial crossover for all the problems tested here."
"a workstation equipped with intel xeon e5-2630 v3 processor running at 2.40 ghz and 32gb ram is used to execute all simulations related to this work. the operating system used is microsoft window 10 [cit] . based on the mean error of functions given by different algorithms, a ranking for each algorithm is presented. in addition, for each group of problems (based on number of dimensions), an average rank for each competing algorithm is presented. moreover, a win/tie/loss analysis is provided in the result tables, where a win depicts that the algorithm is the sole best performer. if the best mean error is achieved by more than one method, than the tie count for all those methods is increased and the win count is kept unaltered. for every other case, a loss is declared."
"for nesda, structural and functional mri data were acquired at the university medical center groningen (umcg), amsterdam medical center (amc), and leiden university medical center (lumc). participants were scanned on 3-tesla mr scanners (philips healthcare, best, the netherlands) with sense 8-channel (lumc, umcg) or 6-channel (amc) receiver head coils. for details, see supplementary material s1."
"case 1: if the offspring cost is equal or less than the parent cost, then offspring replaces the parent and survives to the next generation as:"
"exploring a multi-dimensional and multi-modal, noisecorrupted search space with a fixed-sized population of candidate solutions is challenging and it requires a great balance between the exploitative and explorative tendencies of an evolutionary search along with proper selection strategy so that the algorithm may not be deceived by noisy cost value. this requirement is nicely fulfilled by the random selection of the mutation strategy among two proposed strategies of complementary nature and by coupling with the proposed distance-based stochastic selection step. the gross performance of mde-ds remains surprisingly consistent and statistically significantly better than majority of the state-of-the-art evolutionary methods specifically tailored for noisy optimization from existing literature."
"in this section we describe three techniques for surgical gesture classification based on video data. we assume that each video is segmented into video surgemes, i.e., video clips corresponding to a single execution of one out of a pre-defined set of surgemes. all three methods use labeled video surgemes to learn a model for each of them. we then show how these models can be compared and used for classifying gestures in new video surgemes."
"next, to assess the predictive confidence of our ge approach, we computed accuracy-reject curves for the two binary classifiers that achieved above-chance balanced accuracies (i.e., chr vs. rem, imp vs. rem). accuracy-reject curves illustrate a classifier's accuracy when only predictions greater than a certain (relative) confidence threshold are considered [cit] . hence, this resembles classification with a reject option [cit], where cases that do not meet a certain confidence criterion can be deferred to a clinician. we found that for distinguishing chr from rem patients, the classifier yielded perfect classification accuracy at a rejection threshold of 60% of participants ( figure 3c; blue curve) . furthermore, the accuracy-reject curve overall increased as function of rejection rate, suggesting that participants further away from the decision hyperplane were more likely to be assigned correctly to their respective class. in contrast, for distinguishing imp from rem patients no such cut-off could be identified, and the curve did not reveal a steady increase as a function of rejection rate ( figure 3c; red curve) ."
"effective connectivity among the regions of the extended face perception network (i.e., ofa, ffa, and amygdala, each in both hemispheres) was assessed using dcm for fmri [cit] . first, we provide a brief summary of the group differences in dcm parameter estimates as assessed using classical statistics (a comprehensive description of the classical analyses is provided in supplementary material s7)."
"a standard de algorithm starts with a set of candidate solutions, each of which is represented as a vector of real numbers. components of the solution vectors are also called decision variables. the set of solutions is also called a population and the initial population is generated through random sampling from a uniform probability distribution within the prescribed bounds for each decision variable. subsequently, the algorithm evolves this population by applying the variation (mutation followed by crossover or recombination) and selection operators in each generation (iteration). the generations are terminated when some predefined condition (like exhaustion of a prescribed maximum number of function evaluations (fes)) is met. in typical de literature, the population size is denoted by np. each iteration of de is called a generation following the standard evolutionary computing terminology. a standard way to represent the i th vector of the current generation g is"
"generative embedding (ge) represents a potentially attractive alternative to \"classical\" ml [cit] . the idea is simple but powerful: instead of selecting features from the original data, one applies a generative model to the data and uses the ensuing model parameter estimates as features. generative models describe how observed data may have been \"generated\" from latent (hidden) system states and thus often embody some degree of mechanistic interpretability. for example, in neuroimaging, ge uses modelbased estimates of physiological or cognitive parameters, such as connection strengths [cit], ion channel conductances [cit], prediction errors [cit], or response inhibition [cit] . more technically, ge views a generative model as a theory-driven dimensionality reduction device that projects high-dimensional data onto neurobiologically meaningful parameters that define a low-dimensional and interpretable space for classification. provided a plausible model exists, ge frequently yields more accurate results than conventional ml [cit], likely because the generative model separates signal (reflecting the process of interest) from (measurement) noise."
"it can be seen that to handle growing hardship of the realworld problems, de has been modified in many different ways, but in most of the cases, the resulting de variants have lost their simplicity, which is one of the main reasons for de's popularity among the practitioners. the present work is driven by the quest to improve de for noisy, multivariate, and multi-modal search problems by using simple parameter control techniques along with very simple and intuitively appealing modifications to the basic de steps without necessitating serious computational overhead (likely to be caused by external achieves, rank-based parent selection schemes, additional local search schemes, statistical test on sampled points, keeping the long records of successful individuals etc.)."
these distances can be used to classify new surgemes using a nearest neighbor approach. in our experiments we have used them to train a support vector machine (svm) classifier with a radial basis function (rbf) kernel. that is
"this selection process is further illustrated in fig. 3 . in fig. 3, a sample function landscape scenario for any given instant is shown where a is the position of target member and b and c are the two donor members. now selection among a, b and c should give us c as it has better objective function value than target member a. however, suppose this landscape is affected by noise, as shown in fig. 3 as dotted line. in this noise affected scenario, if we use traditional greedy selection mechanism, b will be selected for its decisive objective function value. in order to overcome this kind of situation, we propose the distance-based stochastic selection scheme, which gives us some probabilistic flexibility to select worse solutions as in noise affected landscapes; the original objective function value may become masked."
"model-based estimates of brain connectivity might be particularly informative for predicting clinical trajectories in mdd, given that dysconnectivity has been postulated as a hallmark of depression [cit] . here, we used a generative model of fmri data, dynamic causal modeling (dcm; [cit] ), to infer effective (directed) connectivity and test the utility of ge for predicting individual clinical trajectories in mdd patients from the nesda study. for this purpose, we combined dcms 6 of the emotional face perception network with linear support vector machines (svms). we then compared the cross-validated predictive accuracy of ge with more conventional approaches, such as classification based on functional connectivity and local bold activity, testing whether a biologically plausible generative model would be superior for predicting naturalistic disease courses from fmri data."
"the posterior means of bma parameter estimates (78 in total) from each participant were used to create a generative score space for a discriminative classification method. within this space, a linear kernel representing the inner product ( ) 〈 〉 was used to compare two instances (participants). a support vector machine (svm) was applied for binary classification of pairwise combinations of the three mdd groups (i.e., rem, imp, and chr)."
binomial crossover involves fixing the value of a parameter called crossover rate (cr) in the range [cit] . d independent numbers between 0 and 1 are sampled uniform at random and compared with cr to decide which component is to be a part of the trial vector. the method can be expressed in the following way.
"(1) yields a predicted neuronal time course which is then passed through a nonlinear hemodynamic model that translates neuronal signal into predicted bold signal [cit] . this yields a complete forward mapping from hidden neuronal states to observable fmri data and, under gaussian assumptions about the measurement noise, specifies the likelihood function. by specifying prior distributions over model parameters (neuronal, hemodynamic) and hyperparameters (measurement noise), dcm becomes a fully generative model. model inversion then proceeds with approximate bayesian schemes, most commonly variational bayes under the laplace approximation (vbl; [cit] ) ."
"for both classifiers, the features with the highest average (across cross-validation folds) scores were situated along the dimension of modulatory (emotional) influences (figure 4, top; for an alternative visualization, see supplementary figure s8 ), whereas the endogenous connectivity and driving input parameters did not distinguish strongly between the groups. importantly, since averaging over cross-validation folds might artificially smooth the weights due to correlations among folds, we inspected the variability of the observed results across the individual cross-validation folds. this suggested that the observed pattern was highly consistent for both classifiers (figure 4, bottom) ."
the one-to-one competition based selection process between the target and the trial vectors is performed as the final stage of a de generation to maintain the population size constant.
"since groups differed significantly in age (but no other variable; table 1 ), we repeated the analysis after regressing out age as a confound from the dcm parameter estimates. we found results to be highly consistent (although with slightly decreased accuracies), suggesting that our results are not confounded by age ( supplementary material s8) ."
"for clinical applications where one strives to maximize sensitivity while at the same time keeping ppv high. hence, we also inspected the pr curves and found that our classifier achieved 97% sensitivity at a ppv of 86% when discriminating chr from rem patients."
"the significance of the population centrality based mutation scheme is that it preserves greediness while still maintaining some level of diversity, i.e. it is less greedy than the de/best/1 scheme and hence there is less chance of getting stuck in the local optima. on the other hand, the dmp-based mutation scheme prefers exploration (see [cit] for a detailed explanation), and thus, in absence of any feedback about nature of the function, we use an unbiased combination of these two methods. the population centrality based mutation scheme is illustrated in fig. 1, which shows the top 50% of a sample de population in 2-d decision space of the sphere function and the formation of a possible donor (mutant) point."
"precision-recall (pr) curves ( figure 3a+b ). from the roc curve, the area under the curve (auc) for discriminating chr from rem patients evaluated to 0.87 (blue curve). notably, high recall (sensitivity) might come at the expense of low positive predictive value (ppv; also known as \"precision\")particularly, in the presence of class imbalances. this is problematic"
"let us refer to the de variant, proposed in this paper, as modified de with a distance-based selection (mde-ds). mde-ds introduces a population centrality based mutation strategy and couples it with the difference mean based perturbation (dmp) [cit] in a probabilistically switchable manner. it also uses the blending crossover as recombination strategy and a novel distance-based stochastic selection mechanism to tackle noisy nature of the objective function. mde-ds does not contain any explicitly tuneable control parameter, except for the population size np, which is kept constant in majority of the de variants [cit] . in what follows, we discuss the modifications proposed at different stages of the mde-ds algorithm."
"second, individual peak activation coordinates were defined as the subject-specific local maximum closest to the neurosynth coordinates within a 12mm sphere for the linear contrast comparing faces (regardless of emotional valence) against scrambled images. individual coordinates are illustrated in supplementary figure s1 . while no group information was used to define roi coordinates, in principle, some bias could still exist if the identification of individual peak activation coordinates had been influenced by systematic group differences."
"rest of the paper is organized in the following way. section ii outlines the canonical de algorithm in sufficient details. section iii describes the proposed de method. section iv presents and discusses the experimental results. finally, the paper is concluded in section v."
"preprocessed and artifact-corrected functional images from each participant entered first-level general linear model (glm) analyses. each condition (i.e., angry, fearful, happy, sad, neutral, and scrambled faces) was modeled as an individual regressor, consisting of a train of stimulus onsets convolved with a canonical hemodynamic response function (hrf)."
"mde-ds uses two mutation strategies which we call population centrality based mutation strategy (pc_ms) and difference mean based mutation strategy (dm_ms). these two different strategies are coupled in a probabilistic switchable manner. pc_ms tends to a greedy search and dm_ms makes room for a diversified search along the fitness landscape. to demonstrate the effect of invoking these two schemes with equal probabilities (as is done in mde_ds) against using any one scheme for all individuals, we compare original mde_ds against two of its algorithmic variants: one with pc_ms and the other with dm_ds only. sample results are provided for four 100d bench-1 problems in table 10, which indicates the best performance of mde-ds and this trend is seen experimentally for all other test functions as well. this indicates that when we do not have specific feedback information about the fitness landscape, it is always beneficial to uniformly mix up opposite natured schemes so as to gain an overall better performance on a wide variety of problems."
"to save space, we consider showing results with all the five different distributions of the noise on 100d functions of bench-1. for the gaussian noise model, three noise amplitudes (0.04, 0.1, and 0.2) are introduced again. other models are poisson, rayleigh, exponential, and random noise. total 50 runs of each problem for each noise model and corresponding to all the algorithms are considered and mean and standard deviation of the absolute best-of-the-run error are reported. maximum number of fes for 100d bench-1 problems are limited to 3e+05 following recommendations from existing literature. table 2a reports the simulation results for 100d bench-1 problems for the gaussian noise model. out of total 39 test cases, mde-ds gives the best result for 36 problems. in f 13 with noise amplitude 0.04 and 0.1, gads outperform others and for 0.2 noise amplitude of the same problem, dersfts gives the best result. for all other problems, mude is a close contender of mde-ds after nrde. according to the average ranking, mde-ds is the winner among all other algorithms followed by mude and nade. consistency of the results over different noise strengths volume 5, 2017 remains an added advantage for mde-ds. also it is to be noted that the gross performance of mde-ds is better than nrde which however remains closes contender according to average rank. table 2b summarizes simulation results for the four other noise models on the 100d bench-1 problems. mde-ds performs best in 49 test cases, a close contender of it being mude, which outperforms all methods in exponential noise model for f 9 and dersfts, which gives the best result for random noise model in f 10 and f 11 . mde-ds performs best followed by mude and dersfts, based on the average ranking, indicating its high degree of robustness across various noise pdfs. 30 runs for each problem corresponding to each noise level and for all algorithms are considered and mean and standard deviation of the absolute error is reported. maximum fes for bench-2 problems are limited to 3e+04. table 3 holds simulation results of bench-2 problems. we can see that mde-ds outperforms other algorithms in thirty-eight cases, only for two cases fips+cj beats mde-ds. according to the average ranking, mde-ds stands first followed by psogbest+cj and bbpso+cj. performance of mde-ds on f 17 [cit] . table 4 summarizes the results of comparison on 30d and 50d instances of f 17 ."
"for fmri, an event-related emotional face perception paradigm was used. participants viewed color images of angry, fearful, sad, happy, and neutral facial expressions, as well as scrambled faces. stimuli were shown for 2.5s, with an inter-stimulus interval varying between 0.5-1.5s. participants were instructed to indicate the gender of the presented face via button press. for scrambled images, participants had to press buttons in accordance with an arrow pointing to the left or right. stimuli were presented using e-prime (psychological software 8 tools, pittsburgh, pa; https://pstnet.com/products/e-prime/). [cit] ."
"inference on effective connectivity is conditional on the underlying model (e.g., assumptions about the network architecture). however, there typically exist several a priori hypotheses about the likely network structure. this model uncertainty leads to defining a model space, a set of alternative plausible candidate models. here, a total of seven models were constructed, representing different possible connectivity structures in the above-mentioned emotional face perception network. for all models, endogenous connectivity and driving inputs were identical. driving inputs were set to elicit face-sensitive activation (i.e., containing all face stimuli regardless of whether an emotional or neutral face was presented, but excluding scrambled faces) in left and right ofa, consistent with their proposed role as the first stage in the face perception network [cit] . the stimulus-evoked activity then propagated through the network via intra-and interhemispheric connections. we assumed forward and backward intrahemispheric connections between ofa and ffa, and between ffa and amygdala, but not between ofa and amygdalaconsistent with the notion of a hierarchy in the face perception network [cit] ."
"notably, these analyses are not meant to represent an optimal prediction approach based on fc or ci measures. higher accuracies for classification based on fc/ci might be achieved by taking into account the whole-brain information (e.g., [cit] . furthermore, it is worth pointing out that the number of features that enter classification are different for ge, fc and ci. however, the purpose of the above analysis was to compare predictions based on different fmri-based features derived from the exact same data (i.e., the bold activity from the rois of the emotional face processing network)."
"major depressive disorder (mdd) is one of the most burdening mental disorders with a lifetime prevalence of 10-30% [cit] . up to a fourth of mdd patients are at risk of developing a chronic disease [cit], characterized by severe negative impact on quality of life and high rates of psychiatric comorbidities [cit] . the diagnostic criteria of mdd in icd and dsm-5 [cit] are not grounded in pathophysiology, but refer to symptoms and signs (e.g., depressed mood, anhedonia, fatigue) that could have various causes. the diagnostic label mdd likely subsumes patients with different disease mechanisms and has limited predictive validity: mdd patients show highly variable clinical trajectories over time [cit], and the absence of mechanistically interpretable predictors turns therapy into a trial-and-error procedure [cit] . this is not only costly and frustrating for patients, but also bears the risk of long-term adverse events [cit] and reduced treatment adherence [cit] ."
"in brief, for distinguishing chr from rem patients, the modulatory influence of happy faces on the connection from right amygdala to right ffa received the highest score ( figure 4a and supplementary figure s8a ). furthermore, scores were high for modulatory influences of negative emotions (i.e., fear, anger, and sadness) on connections among face-processing and emotion-sensitive regions. for instance, modulatory influences by angry faces on the connection from right amygdala to right ffa and left amygdala, and on the connection from left ofa to left ffa showed high loads. similarly, the modulation of connections from right ofa and left ffa to right ffa, as well as the connection from right ffa to right amygdala by fearful faces received high scores."
"in the dmp-based mutation scheme, the best performing individual of the current generation ( x best,g ) is selected and dimension-wise average is taken for both x best,g and the current target member x i,g . now, a d-dimensional vector m i,g is generated having each element within the range [cit] . using the random directional vector m i,g, the mutant is generated in the following way:"
"however, the complexity of mde-ds is higher than de/rand/1/bin, as a price of achieving significantly better accuracy across a wide spectrum of objective functions. competitive convergence speed of mde-ds on different functional landscapes with respect to other de-variants."
"this emphasizes the need for novel prognostic approaches to depression that furnish predictors for clinical trajectories and treatment outcomes. predicting symptom trajectories in mdd at an early stage is of high clinical relevance because identifying patients at risk of chronic disease might guide the deployment of intensified early interventions [cit] ). to achieve this, successful tools may benefit from being grounded in biology to enable a mechanistically relevant stratification of the heterogeneous mdd spectrum . using neuroimaging, some studies demonstrated that disease onset and short-term treatment response prediction may be possible [cit] . by contrast, it has proven more challenging to predict longterm clinical outcome, such as symptom trajectories over several years. [cit] assessed the prognostic value of structural and functional magnetic resonance imaging (fmri) to classify disease trajectories in mdd patients from the netherlands study of depression and anxiety (nesda; [cit] ), a multi-site longitudinal study in a large naturalistic cohort. the authors demonstrated that fmri data from an emotional face perception paradigm allowed discriminating patients who, over the course of two years, showed a chronic disease trajectory from patients showing rapid remission of depressive symptoms, with up to 73% accuracy. this result was obtained by applying a supervised machine learning (ml) method, gaussian process classifiers (gpcs; [cit] ), to contrast images."
"particularly interesting is the louo test, which provides an insight into the ability of the algorithms to generalize and recognize actions performed by users that were unseen during the training phase. the results show that kinematicand video-based algorithms are able to generalize equally well in this setting. overall, we observe a decrease in performance of around 10 percentage points for all approaches, with ksvd being the most sensitive."
"the general de selection, as detailed in section ive, accepts an offspring into the population if it is no worse than its parent at the same population index. however, if the fitness landscape gets corrupted with noise, such greedy selection method suffers a lot because in this case the original fitness of parent and offspring are unknown and it can be well nigh impossible to infer when an offspring is absolutely superior or absolutely inferior to its parent. to handle this situation, a novel distance-based selection mechanism is introduced without any extra parameter (like the threshold value in a threshold-based selection [cit] ). there are three cases of the proposed selection mechanism which are described subsequently."
"any individual (say the i th ) of the current population is known as the target vector. the mutation in de is not exactly nature inspired and differs markedly from the same operation in ga. during mutation, for each target vector, another vector from the same population (known as the base vector) is perturbed with the scaled difference vector(s) of the form x r 1,g − x r 2,g (created from the current generation vectors) to produce a new vector, known as the mutant or donor vector. usually, there is a scale factor f, lying between [0. 4, 2], which scales the difference vector(s), thus controlling the perturbation step-size. the base vector can be a random one from current population, the best one (yielding the smallest objective function value for a minimization problem), a point on the line joining the current target and the best vectors, and so on. depending on the nature of the base vector and number of difference vectors used for perturbation, many mutation strategies have been proposed in de literature. below we show two of the most commonly used strategies [cit] ."
"to tackle single-objective, noisy and continuous optimization problems, we present a simple but very efficient de variant namely mde-ds, which is equipped with simple switchable mutation strategies based on population central tendency and the difference mean based perturbation, a blending crossover, and has a unique distance-based stochastic selection mechanism. the proposed selection process only depends on the cost value, there is no need of prior knowledge about noise strength, noise type etc."
"each individual is perturbed either with the population centrality based mutation of with the dmp based mutation with equal probability. note that the difference mean m, in (6) acts as a scaling coefficient for the randomized direction and provides high degree of explorative power while still maintaining attraction towards the currently best individual."
"some of the functional images were affected by a \"column\" or \"pencil beam\" artifact caused by imperfect fat suppression pulses. the artifact was most apparent in temporal signal-tonoise ratio (tsnr) maps and manifested as vertical stripes, primarily in frontal gyrus and anterior temporal lobe (see https://github.com/dinga92/stripe_cleaning_scripts). this was corrected by regressing out artifact-related independent components prior to the routine preprocessing steps. artifact correction was done within fmrib's software library (fsl; http://www.fmrib.ox.ac.uk/fsl) as follows: first, the multivariate exploratory linear optimized decomposition into independent components (melodic) algorithm was used to identify independent components associated with the artifact, and second, regfilt was used to regress out the artifact-related components."
"where f original x is the original cost is function value corresponding to trial solution x, f noisy x is the noisy version of the cost function for the same trial solution x and τ represents the stochastic noise amplitude which is injected into the original cost function to emulate noise. this noise amplitude τ follows a certain probability distribution function (pdf). the following five distributions of τ are considered here: 1) poisson: τ follows a poisson pdf, which is given by,"
"we emphasize that the present work is not meant to provide an ultimate prognostic tool for outcomes in mdd since this may require a substantially larger dataset than currently available. instead, the present study provides proof-of-concept that illustrates the potential benefits of ge relative to conventional approaches in neuroimaging studies of mdd, with regard to predictive power and interpretability. 7 2 materials and methods"
"we can see that for both 30d and 50d cases, mde-ds outperforms ipop-cma-es in a statistically significant way. maximum number of fes are limited to 3e+05 for 30d instance and 5e+05 for 50d [cit] as well as their counterparts shade-ds and lshade-ds, equipped with the proposed distance based selection applied on the noisy versions of the 28 test functions in 50d [cit] test-suite [cit] . noise simulation (by adding gaussian noise of strength 0.2) for these benchmark functions are done in same fashion like previous bench-1 and bench-2 problems. a close scrutiny of table 5 reveals that mde-ds significantly outperforms all the peer algorithms on 22 out of 28 test cases. the performance of shade-ds and l-shade-ds remained consistently superior to that of shade and l-shade respectively on majority of the test cases, thus, showcasing the efficiency of the distance-based threshold scheme proposed as a component of the mde-ds algorithm. for functions f4, f16, f24, and f26, l-shdae-ds is able to beat mde-ds. nrde attains the best result only for function f15. performance of the other seven algorithms, including six de-variants specifically meant for noisy optimization, remains quite poor as compared to mde-ds, shade-ds, and l-shade-ds."
"existing attempts to predict mdd trajectories have focused on clinical or cognitive features [cit] . [cit] systematically assessed the predictive value of non-imaging data, using clinical, psychological and biological measures from the nesda study. they found that clinical measures performed best with balanced accuracies around 66%, while endocrine and immunological measures (e.g., cortisol, inflammatory markers, metabolic syndrome markers) did not distinguish between clinical trajectories. interestingly, consistent with our ge results, they could primarily discriminate rem patients from the other two groups."
"where x r1,g and x r2,g are two individuals corresponding to randomly chosen indices r 1 and r 2 and v i,g is the newly generated mutant vector corresponding to current target vector for present generation g."
"in mde-ds there is no need to fix values for f and cr, as f is randomly switched between 0.5 and 2 for each mutation operation and cr is sampled uniform at random from the continuous interval [0.3, 1] for each target vector. switching of f between two extreme corners of the feasible range is conducive to attain a balance between diversification and intensification of the search. the utility of such switching scheme has been earlier discussed by us in a recent work in the context of large scale static optimization [cit] . in blending crossover, there is a parameter b (the blending rate), whose value is also randomly chosen from among three distinct values: a low value of 0.1, a medium value of 0.5 and a high value of 0.9."
"one benefit of ge is that features represent model parameter estimates, which, depending on the model, may be neurobiologically interpretable. hence, in a next step, we interrogated our generative score space to illustrate which features contributed most to the classification performance."
"the data used in this study can be obtained via the standard nesda data access procedure (see https://www.nesda.nl/nesda-english/). this requires researchers to file a data request (analysis plan) which has to be approved by the nesda consortium in order to be granted access to the data. nesda fully adheres to the fair (findable, accessible, interoperable, and re-usable) data principles. furthermore, we will make all analysis code publicly available on a repository that similarly adheres to the fair data principles. additional, reciprocal interhemispheric connections were set between bilateral ofa, bilateral ffa and bilateral amygdala. driving inputs comprised all faces, regardless of the emotional valence, and were allowed to drive neuronal activity in the left and right ofa. while endogenous connectivity and driving inputs were identical for all models, they differed in the assumed modulatory influences of emotion processing. emotion processing could either modulate (i) forward (models 1&4), (ii) backward (models 2&5), or (iii) forward and backward intrahemispheric connections (models 3&6). additionally, emotion processing (i) modulated (models 1-3) or (ii) did not modulate interhemispheric connections among homotopic brain regions (models 4-6). systematically varying all combinations resulted in six distinct models. finally, we also included a \"null\" model (i.e., model 7, not shown) where none of the intra-and interhemispheric connections was modulate by emotion processing. endogenous connectivity is colored in blue, modulatory influences of happy faces in red, modulatory influences of angry faces in yellow, modulatory influences of fearful faces in 39 violet, modulatory influences of sad faces in green, and driving inputs (related to all faces regardless of the emotional valence) in cyan. 40 10 tables"
"random-effects bayesian model selection [cit] suggested model 3 to be the winning model at the group level with an expected posterior probability of 0.49 and a protected exceedance probability close to 1 (supplementary figure s4b ). nevertheless, at the single-subject level, other models received non-negligible posterior probabilities as well. to account for this variability, individual connectivity parameters were estimated using bma [cit] over all seven models in the model space within the default occam's window ( )."
"from a coarse knowledge of the problem at hand, we first determine the bounds for the solution space as: vector of minimum bounds for each decision vari-"
a summary description of the set of 21 conventional benchmark functions (divided into bench-i and bench-ii sets) collected from various literatures on single-objective noisy optimization with evolutionary computing approaches can be found in table 12 .
"used gpcs on whole-brain contrast images, whereas we applied linear svms to dcm parameter estimates from a small (six-region) network. furthermore, their analysis used the artifact-confounded mr data (see methods); hence, it remains to be tested whether classification accuracy would change when the artifact-corrected images are used."
"whileas highlighted abovenone of the group differences survived multiple comparisons correction, these findings suggest that small alterations of different effective connectivity strengths exist between chr, imp and rem patients, which enable classification when jointly considered as features."
"graph-theoretical measures based on fc in the default mode network at baseline was associated with changes in symptom severity after two weeks of medication [cit] . furthermore, activation [cit] and fc [cit] were predictive of psychotherapy outcome. similarly, fc of subcallosal cingulate cortex with insula, dorsal midbrain and ventromedial prefrontal cortex was differentially associated with remission and treatment failure to cognitive-behavioral therapy and 20 antidepressant medication [cit] . finally, clinical responses to transcranial direct current stimulation of left prefrontal cortex could be predicted in unmedicated mdd patients [cit] ."
"for distinguishing imp from rem patients, the modulatory influence of happy faces on the connection from right ffa to right ofa received the highest score ( figure 4b and supplementary figure s8b features, i.e., fc or local activation estimates derived from the same data within the network of interest. similar to previous studies [cit], these findings demonstrate that using a plausible generative model as the basis for classification can enhance classification accuracy significantly."
"dynamic causal modeling (dcm; [cit] ) is a generative model that enables inference on hidden (latent) neuronal states from measured neuroimaging data. for fmri, dynamics of neuronal activity are described as a function of the effective (directed) connectivity among neuronal populations:"
"generate a random number p s between 0 and 1; 17 ) with noise corruption and the same is included in our tests also. for a detailed description of the ieee cec benchmark functions, refer to the respective technical reports [cit] ."
"we selected six regions of interest (rois) that represent key components of the extended face perception network [cit], bilateral occipital face area (ofa; [cit] ), fusiform face area (ffa; [cit] ), and amygdala [cit] . to account for inter-subject variability in their exact location, center coordinates were defined for each participant individually: first, we identified the most likely mni coordinates of these regions from a meta-analysis of 720 studies using neurosynth [cit] with the search criterion \"face\". relying on this external information from neurosynth helped ensure independence of feature selection (i.e., definition of roi coordinates) and subsequent prediction. generally, we prevented any cross-talk between training and test samples which might otherwise positively bias classification accuracy (see supplementary material s2)."
"finally, on a more general note, any biomarker in psychiatry will always yield imperfect predictions. this is because the course of psychiatric disorders is affected by a plethora of environmental factors which cannot be foreseen from physiological data, including the occurrence of stressful life events like loss, bereavement, or trauma [cit] ."
"arguably, the attempt to predict outcome after two years in a naturalistic setting, as in nesda, represents a greater challenge than predicting short-term response to a particular treatment. nesda (1) recruited patients from a wide spectrum, including community, primary care and specialized mental health organizations, (2) encompassed a wide range of depressive phenotypes from very mild to severe, and (3) did not standardize treatments or occurrence of life events over the 2-year follow-up period [cit] . this represents a strength of the nesda dataset since it allows testing the course of mdd in a realistic setting which reflects the clinical heterogeneity that physicians face on a daily basis."
"both the lds and bof techniques previously described use visual data. however, while the lds approach tries to capture the dynamics of the scene, the bof approach is based on sparse (due to feature detection) local structures of the frame (captured by hog) and very small and sparse motion (captured by hof). hence, it seems natural to think about a strategy that integrates these complementary techniques."
"we have proposed three methods for surgical gesture classification from video data. the results showed that video data can be as discriminative as kinematic data. however, in this paper we used fairly low-level visual features, such as image intensities, image gradients and optical flow. future work includes using more advanced visual features, such as detection and tracking of surgical tools."
"these results are consistent with conventional group comparisons of the connectivity patterns (see results and supplementary material s7), which, however, do not allow for single-subject predictions. [cit] ) . for instance, an fmri meta-analysis demonstrated valence-dependent effects of emotional stimuli on amygdala and fusiform gyrus in depression, with hyperactivation for negative and hypoactivation for positive stimuli [cit] . reduced amygdala activity to positive emotional stimuli has also been associated with anhedonia [cit] . similarly, functional integration of the emotion processing network is altered in mdd [cit] . alterations in emotion processing have also been suggested to have some clinical utility. for instance, implicit processing of affective facial expressions related to a diagnosis of mdd [cit], and longitudinal neuroimaging studies reported normalization of activations by emotion processing under pharmacotherapy [cit] ."
"the data used here were acquired in the nesda study [cit], a multi-site longitudinal study on the long-term course of depression and anxiety disorders in a large naturalistic cohort. in total, 2981 participants (18-65 years) were recruited from community, primary care and specialized mental health organizations. from this cohort, 301 participants (156 with mdd diagnosis) were included in the mri experiment. [cit] . for the current study, only those participants were included that had: (i) a dsm-iv diagnosis of mdd, as established using the structured composite international diagnostic interview (cidi; [cit] ) in the 6 months prior to baseline, (ii) reported symptoms in the month before baseline as confirmed by either the cidi or the life chart interview (lci; [cit] ), (iii) availability of 2-year follow-up of depressive symptoms from the lci, and (iv) no other exclusion criteria related to, e.g., poor data quality, non-compliance with task instructions, or deficient performance."
"specifically, we used the fitcsvm routine in matlab. estimates of classification performance were obtained by leave-one-out cross-validation. here, in each fold, the classifier is trained on participants (the training set) and tested on the left-out participant. using the training set only, the hyperparameters of the svm (box constraint and kernel scale; see supplementary material s5) were optimized using in-built routines of fitcsvm. this computes bayes-optimal hyperparameters using the expected improvement acquisition function [cit] based on (inner) five-fold cross validation. this approach is known as nested cross-validation [cit] . by default, fitcsvm solves svms using the sequential minimal optimization algorithm (smo; [cit] ) . significance of the classification result was assessed using permutation tests. here, an empirical null distribution of the balanced accuracy is computed by randomly permuting the participant labels and re-fitting the entire classification model (i.e., training and testing) based on these new labels [cit] . for each permutation, the balanced accuracy is re-evaluated. here, we used 1,000 permutations. the p-value is then computed as the rank of the original balanced accuracy in the distribution of permutation-based balanced accuracies, divided by the total number of permutations."
"this has motivated a number of approaches for automatic rmis skill assessment and gesture classification. one of the most natural approaches is to decompose a surgical task into a series of pre-defined 'atomic' gestures or surgemes [cit], such as 'insert a needle', 'grab a needle', 'position a needle', etc. (fig. 1 shows sample frames from three different surgemes). the problem then becomes how these surgemes can be segmented in time, recognized, and finally assessed."
crossover plays a crucial role in the generation of new promising search point from two or more existing points within the function landscape. we use a blending crossover in mde-ls and it can be described in the following way:
"where represents neuronal states, encodes endogenous connectivity among brain regions, ( ) represents the modulatory influence that input exerts on endogenous connections, and quantifies the strength of experimentally controlled inputs (perturbations) on brain regions."
"recent technological advances have contributed to, and changed, the way in which surgery can be performed. one of them is robotic minimally invasive surgery (rmis), which has several advantages over traditional surgery, such as better precision, smaller incisions and reduced recovery time. however, the steep learning curve together with the lack of fair and effective criteria for judging the skills acquired by a trainee, may reduce the benefits of this technology."
"in the population centrality based mutation, the entire population is sorted based on the objective function values and 50% best performing individuals are selected to design a temporary subpopulation of size np/2. now, we computẽ x best,g as the arithmetic mean (centroid) of the subpopulation members. we mutate the i th population member using the following expression:"
"i. poisson noise: τ follows a poisson pdf with mean and variance equal to 0.25. ii. gaussian noise: in bench-1 τ follows a gaussian pdf with zero mean and variance equal to 0.04, 0.1, and 0.2 and for bench-2 τ follows a gaussian pdf with zero mean and variance equal to 0.2, 0.4, 0.6, 0.8, and 1 following existing works [cit] . iii. rayleigh noise: τ follows a rayleigh pdf with mean equal to 0.3 and variance equal to 0.025. iv. exponential noise: τ follows an exponential pdf with mean equal to 0.86 and variance equal to 0.75."
"prefer a fitter individual according to the cost function value, as due to additive noise in the cost function, an actually fitter individual may present a worse cost and get discarded. the reverse also can happen, i.e. a worse individual may be admitted to subsequent generations due to deceivingly better fitness appearance caused by noise."
"case 2: if the offspring cost is greater than parent cost, then offspring can replace the parent based on a stochastic principle. a probability is calculated by e − f dist where f is the absolute difference of objective function value between parent (target) and offspring (trial) and dist is the manhattan distance between those two vectors. we have used the manhattan distance because of its simplicity and computational efficiency. the scheme can be outlined in the following way:"
"despite these limitations, the present study demonstrates the potential of ge for predicting clinical outcomes of mdd in a way that combines enhanced accuracy with biological interpretability of predictions. more generally, as illustrated by recent successful clinical applications [cit], generative models offer an attractive strategy for"
"apart from the superior classification accuracy, another advantage of ge is that results can be interpreted in terms of the mechanisms represented by the underlying generative model. to this end, one can interrogate the generative score space to identify the features that are most discriminative between the different classes [cit] . here, we addressed this by first transforming the feature weights of the linear svm into patterns, following previous recommendations [cit] . inspecting these scores then allowed to pinpoint those features contributing most to the classification between the different naturalistic courses (figure 4 and supplementary figure s8 ). this analysis suggested that groups differed primarily along the dimension encoded by the modulatory parameters, which represent trialby-trial changes in endogenous connections by emotional valence of faces. put differently, it is the dynamic modulation of connections by emotional contents of faces that allows for predicting the clinical trajectory of an individual patientnot the average connectivity across all trials. in conclusion, our analysis implies that it is the reactivity of the face processing network to emotional stimuliin terms of reconfiguring its connection strengths trial-by-trial which enables predicting future clinical trajectories of individual patients."
"while an encouraging initial result, developing this approach further with conventional ml techniques and towards clinically required levels of accuracy faces several challenges [cit] . first, achieving high classification accuracy robustly from wholebrain fmri data can be difficult, given the high dimensionality of the data relative to the small sample sizes. second, the results from \"black-box\" ml operating on descriptive features (e.g., contrast images) do not easily allow for mechanistic interpretations. the latter, however, is increasingly recognized as critical for clinical applications of ml [cit], both to derive novel treatment ideas from successful predictions but also to detect cases when ml goes awry, e.g., predictions that derive from artefacts in the data."
"in a first step, we aimed to pinpoint the individual contribution of each feature (i.e., dcm parameter estimate) separately for the two significant classifiers (i.e., chr vs. rem, imp vs. rem). importantly, individual feature weights of linear classifiers are not directly interpretable because high magnitudes of feature weights might either indicate an association with the label or a \"suppressor\" variable that cancels out noise or mismatch in other colinear variables [cit] . [cit] and first transformed all feature weights into patterns based on a corresponding forward mapping."
"previous attempts to obtain single-patient predictions in mdd have almost exclusively concerned short-term treatment responses to specific interventions. for instance, seminal pet work demonstrated that cingulate metabolism differentiated distinct treatment responses . for fmri, brain activity during the processing of sad faces allowed predicting treatment outcome to antidepressant medication in individual [cit] ."
"finally, we evaluated the repeatability at the quantitative level, by plotting the cv distributions for each protocol calculated over the four replicates using weighted spectral count values (fig. 4) . for most protocols, the median cv value was around 20%, with highest values obtained with usc and nap protocols (24% in both cases), indicating a good repeatability of all protocols."
"neuronal topology visualizer is a three-dimensional gui tool that allows the user to define a series of structural elements in a simulated neuronal network; e.g., the shape and location of each neuronal parts (e.g., dendritic tree, soma and axon) and the connectivity of neurons. in addition, nanomachine locations and nanomachine-to-neuron interface are defined with this tool. the user can intuitively view and define these structural elements with threedimensional editing features such as zoom-in/out, pan-up/down/left/right, toggle and viewpoint shit. the proposed framework manages and renders all structural elements as opengl objects. (it currently uses a java implementation of opengl, jogl.) fig. 5 shows several example screenshots of neuronal topology visualizer. neuronal topology visualizer also validates the structure of a neuronal network and reports the user editing errors such as unconnected neurons, missing nanomachines and nanomachines unconnected with neurons. if no editing errors are found, the visualizer can generate the structural information on a neuronal network into an xml file."
"fortunately, introducing senone-based wfst will merge the equivalent tri-syllables and finally get a tolerant-size wfst. the wfst can be constructed offline on a server with large memory to deal with the memory exploding in the stage"
"presynaptic and postsynaptic neurons maintain voltage gradients across their membranes by means of voltagegated ion channels, which are embedded in the presynaptic membrane to generate the differences between intracellular and extracellular concentration of ions (e.g., ca 2+ ) [cit] . changes in the cross-membrane ion concentration (i.e., voltage) can alter the function of ion channels. if the concentration (i.e., voltage) changes by a large enough amount (e.g., approximately 80 mv in a giant squid), ion channels initiate a voltage-dependent process; they pump extracellular ions inward. upon the increase in intracellular ion concentration, the presynaptic neuron releases a chemical called a neurotransmitter (e.g., acetylcholine (ach)), which travels through the synapse from the presynaptic neuron's axon terminal to the postsynaptic neuron's dendrite. the neurotransmitter electrically excites the postsynaptic neuron, and the neuron generates an electrical pulse called an action potential. this signal travels rapidly along the neuron's axon and activates synaptic connections (i.e., opens ion channels) when it arrives at the axon's terminals. this way, an action potential triggers cascading neuron-to-neuron communication. fig. 1(b) shows how ca 2+ concentration changes in a neuron. when the concentration peaks, the neuron releases a neurotransmitter to trigger an action potential. upon a neurotransmitter release, the neuron goes into a refractory period (tr in fig. 1(b) ), which is the time required for the neuron to replenish its internal ca 2+ store. during tr, it cannot process any incoming signals. the refractory period is approximately two milliseconds in a giant squid."
"after building the syllable based speech recognition systems, the experiments about dnns are conducted, and the experimental results are placed in tabel ii. from the experimental results, we can figure out that, with dnns, ci syllables and cd-syl-sdt can obtain good performances. compared with the performance of gmms, the dnns based models show a good property of avoiding data sparsity and unevenness problems. however, the readers may argue that, the performance did not get a significantly improvement while converting from the context independent to the context dependent for syllable models. the reasons come from different aspects. firstly, the context independent syllables already have a strong constraint of the speech observations, and the contextual information may make sense when enough speech training data employed. secondly, the experimental dataset is a broadcast news database, and the introducing of contextual information for the conversational speech or spontaneous speech would bring about much more improvement."
"many efforts have been made to build syllable based chinese asr systems. the difficulty of building such a system may come from the following reasons. firstly, the demand of training data for the large size of modeling units is hard to satisfy, and it is very difficult to make the occurrences of syllables to be even. these facts brought crucial problems in the gmms based acoustic model training. secondly, the large size of context dependent models lead to a heavy problem to conduct lextree based viterbi beam search with these kinds of models."
"neurons are connected with each other to form a network(s). neurons communicate with others via synapses, each of which is a membrane-to-membrane junction between two neurons. a synapse contains molecular machinery that allows a (presynaptic) neuron to transmit a chemical signal to another (postsynaptic) neuron. in general, signals are transmitted from the axon of a presynaptic neuron to a dendrite of a presynaptic neuron. an axon transmits an output signal to a postsynaptic neuron, and a dendrite receives an input signal from a presynaptic neuron."
"neuronal tdma optimizer is a communication optimization suite in the proposed simulation framework (fig. 3) . it performs scheduling optimization for neuronal tdma [cit], which is a single-bit time division multiple access (tdma) protocol for neuron-based intrabody nanonetworks. neuronal tdma allows nanomachines to multiplex and parallelize neuronal signaling while avoiding signal interference to ensure that signals reach the sink node. neuronal tdma optimizers can plug optimization algorithms as its backend modules (fig. 3) . given a particular optimization algorithm, the optimizer seeks the optimal signaling schedules (i.e., which neurons to activate and when to activate them to trigger signal transmissions) for sensor nodes with respect to given optimization objectives."
"where x is the total number of query images correctly identified, d i is the number of images retrieved per query and n is the database size."
"evaluation of the repeatability of the protocols. the technical repeatability of each protocol was first evaluated by considering the number of protein sets identified (with at least one unique peptide) across the four replicates ( supplementary fig. s3 ) and the proportions of common proteins, ranging from 56 to 61% when considering proteins common in the four replicates, and from 69 to 72% when considering proteins in common in at least 3 out of the 4 replicates (supplementary fig. s4 ). we also evaluated the repeatability at the peptide level by considering the number of peptides (unique to a protein set) identified across the four replicates, and the proportions of common peptides between the four replicates per protocols illustrated by venn diagramms (supplementary figs s5 et s6) . proportions of common peptides ranged from 42% to 46% between the four replicates and from 58 to 61% in at least 3 out of the 4 replicates. these results illustrated a satisfying technical repeatability at both proteins and peptides levels for all evaluated extraction/polymerization protocols. this is even emphasized by the low numbers of singleton (on average 5 and 6% for proteins and peptides respectively) ( supplementary figs s4 and s6) . then, we evaluated the impact of the sample preparation variability on the go annotations distributions for the four replicates ( supplementary fig. s7 ) and results are overall consistent attesting again for a good technical repeatability."
"noticed that the stage of decision tree based state tying during acoustic model training will map tri-syllable into a senone sequence and the scale of the senone set is usually about thousands which is dramatically smaller than the number of tri-syllables. therefore most of the tri-syllables are equivalent. when decoding on r, too many equivalent candidate tokens are maintained, which would cause the decoding become extremely slow and inefficient, and the performance drops severely."
"firstly, despite the training problems, the wfst based decoding framework is used to test the syllable based acoustic models. under the framework of wfst, although there are too many hmms caused by the context dependent syllables, the final decoding network is based on the transitions between the clustered states (senones). with the help of decision tree state tying, the context dependent syllable based hmms have about only thousands of senones. through the optimization algorithms of wfst, such as determination and minimization, the decoding network can be optimized with similar size as the decoding network for the initial/finals based models."
"classical geometric hashing can be used to index the variable feature points in a high-dimensional space. however, classical geometric hashing may not be suitable for its computational time and memory requirement because it uses nc 2 bases pair [cit] ."
"this paper focuses on two components in the proposed framework, neuronal topology visualizer and neuronal tdma optimizer, and discusses an xml-based data transport between those components (fig. 3) . the topology visualizer allows the user to edit and view the structure of a neuronal network. neuronal tdma optimizer seeks the optimal schedules for nanomachines to fire neuronal signaling in a given neuronal network. the data transport encodes the topology of a neuronal network and the location of nanomachines with xml. fig. 4 shows a block diagram of the steps required to run a simulation. the user graphically defines a set of neurons and its topology as well as the location of nanomachines. those information is formatted in xml and supplied to neuronal tdma optimizer. the user configures various parameters for neuronal signaling optimization and obtains the optimal signaling schedules. the schedules are then examined on a simulated neuronal network in order to obtain performance characteristics for each signaling schedule based on a given set of metrics. simulation results are processed to produce graphical figures so that the user can analyze signaling schedules and their performance characteristics."
"the experiments are conducted to test the performance of syllable based models. the initial/finals models are 3 state left-to-right hmms, and the states number of each hmm for syllables is determined by the corresponding number of phones, for example, \"qiong\" have \"q\", \"i\", \"o\", \"ng\" 4 phones, the states number is 7(3 + 4); \"a\" have only 1 phone, the states number is 4(3 + 1). in the experiments about the context dependent syllable, central syllable dependent decision tree based state tying (cd-syl-sdt) are conducted and compared with central syllable independent decision tree based state tying (cd-syl-sit). the experimental results are listed in table i . from the experimental results, we can find out that the cdsyl-sit outperformed cd-syl-sdt, which indicates that the central syllable independent decision tree state tying can help ease the distribution unevenness of training data. besides, cdsyl-sit have a slightly better performance than the cd ifs with similar real time fatcor."
"cell culture and protein extraction. j774 cells (obtained from ecacc, salisbury, uk) were grown in rpmi1640 medium supplemented with 10% fetal bovine serum up to a cell density of 1 million cells/ml. the cells coming from several culture flasks were harvested by scraping and washed 3 times in pbs. the pellets were then resuspended in pbs, the cell suspensions pooled and then split into 6 aliquots for the various cell extractions. all extractions took place with five volumes of extraction buffer per volume of packed cell pellet. this scheme ensures that the comparison of the proteomic results is not biased by a biological variation in the starting material. the cell pellet was resuspended by vortexing in the urea lysis buffer (urea 8 m, chaps 3%, spermine base 30 mm, hcl 100 mm), and the extraction was let to proceed at room temperature for 30 min. the nucleic acids were then removed by centrifugation for 15 min at 15,000 g and the supernatant was collected. the protein concentration was measured by a dye binding assay 20 ."
this paper is organized as follows. section ii introduces the proposed enhanced geometric hashing. experimental results are analyzed in the next section. conclusions are given in the last section.
"this paper focuses on how to build a syllable based chinese speech recognition system. chinese is naturally a syllabic language and each basic language unit can be phonetically represented by a syllable. the good properties of making syllable as the basic acoustic modeling units have attracted many efforts and researches. the problems to build such a chinese asr systems come from the model training and decoding framework. to fix these problems, in the experiments, the wfst based decoding framework is introduced, the decision tree based state tying strategy is discussed, and the dnns based acoustic modeling method is utilized. compared with these experimental results, we can find out that, the dnns based model is the best choice of for syllable modeling in chinese asr. however, the above works are still preliminary and coarse. there are several works need to be done in next phase. firstly, the performance of utilizing the context dependent syllable models should be further tested in other speech tasks, such as spontaneous speech, spoken dialogue. secondly, a large speech corpus should be considered. the experiments in this paper have shown the solution to the data sparsity caused by context dependent syllable based models, but we believe that, when a lager speech corpus is used, the context dependent syllable based models can obtain a better performance improvement than the context dependent initial/finals based models."
", where t is the assigned threshold. the same procedure is followed for all query points f 1, f 2, · · ·, f n . thus, there are n candidate sets c 1, c 2, · · ·, c n for given n feature points in a query q."
"extraction and solubilization of proteins in proteomics workflow relies mostly on the use of urea or sds because of the lack of established sample preparation protocols compatible with other detergents. however, other detergents could be of interest, like ctac, which allows removal of interfering macromolecules, and also native extractions if samples have to be compatible for further biochemical analyses. we previously demonstrated that tg, in combination with the use of sds, is a fast and effective sample preparation method for high-throughput quantitative proteomics. here we showed that tg is a versatile sample preparation because of its wide compatibility with urea, sds, ctac and native-based extraction buffers. the reference protocol, namely using sds and chemical polymerization, allowed identifying the highest number of proteins and peptides, but all the other tested protocols compared favourably in terms of identification, quantification and repeatability. even if major proportions of proteins are conserved, each extraction/polymerization protocol has its specificities with the native protocols allowing to target a slightly different population of proteins thanks to nuclei removal. ctac-based protocols, and in particular ctp, also slightly discriminate very likely due to an efficient removal of interfering anionic macromolecules. in conclusion, tg is a fast, simple and reproducible sample preparation compatible with a wide range of extraction buffers offering flexibility during the proteomics sample preparation step. and, as all the extraction protocols compared in this study involved the same number of steps and solutions, it can be reasonably inferred that the observed differences stem mostly from the different chemistries used."
"the cell pellet was resuspended by vortexing in the ctac-urea lysis buffer (urea 4 m, ctac 2.5%, glycine 100 mm, hcl 50 mm, ph 2.4). the extraction took place at room temperature for 30 min. the extract was then centrifuged for 15 min at 15,000 g and the supernatant was collected. the protein concentration was measured by a modified dye binding assay 21 ."
"if the model image is translated, the feature points are also translated from their original positions. in that case, mean centering can be used to translate each feature point f i to f i such that mean of all f i becomes zero. this can be done"
"in contrast to short-range communication, simulator development has not been reported in literature for longrange molecular communication, particularly neuron-based communication. this paper describes the first research attempt to develop and evaluate a simulator for neuron-based molecular communication."
"in this case study, a simulated neuronal network contains 43 neurons. 11 sensors are evenly distributed in the neuronal network. the topology of neurons follows a tree structure (fig. 5) . the user defines a simulated neuronal network with neuronal topology visualizer (section 4.1). the visualizer verifies the network's structure and encodes it in xml. xml data transport (section 4.2) passes the xml-encoded data to neuronal tdma optimizer (section 4.3), which seeks the optimal tdma schedules with nsga-ii and sms-emoa."
"during indexing, the features f 1, f 2, ..., f m are extracted for all models using surf algorithm [cit] . there is a possibility that the model images may appear translated and rotated relative to their original positions. also, models may not have the same scale. hence in order to make the proposed indexing technique invariant to translation, rotation and scaling, every model in the database is preprocessed. it consists of three steps, mean centering, rotation with respect to principal components and normalization."
"the structure of a neuron consists of a cell body (or soma), dendrites and an axon ( fig. 1(a) ). the soma is the central part of a neuron. it can vary from 4 to 100 micrometers in diameter. dendrites are thin structures that arise from the soma. they form a complex \"dendritic tree\" that extends the farthest branch a few hundred micrometers from the soma. dendrites are where the majority of inputs to a neuron occur. an axon is a cellular extension that arises from the soma. it branches before it terminates and travels through the body in bundles called nerves. its length can be over one meter in the human nerve that arises from the spinal cord to a toe."
"in the proposed technique, in order to handle all possible rotations in an object, each image object is aligned using principal components of the feature points obtained using the pca. so the technique effectively removes the use of bases pairs thus reduces the time complexity by a factor of nc 2 . overhead in the proposed technique is the use of pca which is negligible. in the proposed indexing technique, since each feature point is inserted exactly once, both memory and searching cost has been reduced significantly."
"the proposed hashing technique has been used in the biometric databases for indexing. to determine the performance of the proposed indexing scheme, four measures, namely, hitrate, bin-miss rate, penetration rate, and cumulative match characteristic curve are used."
this paper proposes and evaluates a simulation framework for neuron-based molecular communication. the proposed framework integrates a neuronal network visualizer/editor and a neuronal signaling scheduler through an xml data transport. this paper describes those simulation components and validates them with a case study that implements a tdma-based signaling protocol in a simulated neuronal network and examines its performance characteristics with evolutionary multiobjective optimization algorithms.
"at the time of indexing, it index the features of all models in the database. features are represented by co-ordinate position. it consists of co-ordinate position of feature points. generally, based on the number of matched co-ordinate positions, the correspondence between two object is found. it gives a measure of global similarity between the objects, but it does not establish a one-to-one correspondence between them. although considerable amount of time has been reduced through indexing in classical geometric hashing, the lack of quantitative correspondence makes this technique unsuited for recognizing multiple or partially occluded objects in a noisy image. much attention has been made to extract geometric primitives (e.g., lines, circles, curvature, edges, etc.) that are invariant to viewpoint change [cit] . nevertheless, it has been shown that such primitives can only be reliably extracted under limited conditions (controlled variation in lighting and viewpoint with certain occlusion). hence the idea is to use simple and efficient quantitative information along with the feature point based co-ordinate positions. the descriptor vector around each feature point is considered as quantitative information in order to improve the performance of the recognition system. the algorithm for indexing the model image in the enhanced geometric hashing is given above."
"in chinese, each basic language unit can be phonetically represented by a syllable. syllables contain stronger coarticulation and have more stable acoustic realizations than initial/finals [cit] . besides, compared with huge number of english syllables, the number of chinese syllables is only around 400 (tone is not considered). these properties make syllable become a suitable candidate unit for chinese acoustic modeling. in all the experiments of this paper, the tone information is not considered."
"there are some discriminative pre-training methods [cit] . the dnn is trained by starting with a one-hiddenlayer neural network. once the networks has been trained discriminatively, a second hidden layer is interposed between the last hidden layer and the softmax output layer, and the whole networks is again discriminatively trained. this can be continued until the desired deep structure is reached, and then fine-tune to convergence by bp."
"the introducing of dnns based acoustic models would change many conclusions based on gaussian mixture models (gmms), owing to the difference that dnn is a discriminative model and the other is generative model. as for chinese speech recognition, most systems use initial and toneless/tone finals as the basic acoustic modeling units. there are many efforts on applying syllable as the basic modeling units. the motivation comes from the fact that chinese is a syllabic language. during the past decades, several researches have been made on syllable units based chinese acoustic modeling [cit] . although the implementing context dependent syllable is intuitive and attractive, there are many problems need to overcome. firstly, there are more than 400 toneless syllables (the tone information is not discussed in this paper), which may leads to a tri-syllable set containing more than 64,000,000 modeling units. this is a severe problem for generative models training and the hmm based decoding implementation. fortunately, through introducing the wfst based decoding framework and some acoustic modeling techniques, the problems can be solved. in this paper, the central syllable independent decision tree based tying and the wfst based decoding framework are adopted, which makes the syllable based chinese speech recognition can get a comparable results on a small training with the gmms. moreover, while introducing the dnns based models, the experimental results showed a further improvement."
"the notion of dominance is used in some of emoas (e.g., nsga-ii [cit] ) in the proposed simulation framework. it is also used for processing and evaluating simulation data (c.f. fig. 4) ."
"there are two main aspects which makes the syllable based models having not been adopted widely in chinese asr. firstly, to conquer the decoding problem, the senone-level wfst based decoding framework is the key. as for the data sparsity and unevenness problems, the central syllable independent decision tree based state tying can help the gmms based models, and the dnns based models can avoid these problems. besides, the dnns based models significantly outperform the gmms based models. thus, the reasonable choice of building a syllable based chinese speech recognition systems is introducing the senone-level wfst based decoding framework and the dnns based acoustic model."
where m id and d are the model identity and the descriptor vector of the feature point respectively. the same process is repeated for each model in the database.
"the ion spray voltage floating was set to 2.6 kv and the interface heater at 100 °c. the system was operated in data-dependent-acquisition mode with automatic switching between ms (mass range 400-1250 m/z) and ms/ms (mass range 100-1800 m/z in high sensitivity mode) modes. the fifty most abundant peptides (intensity threshold of 150 counts), were selected on each ms spectrum for further isolation and collision induced dissociation fragmentation, preferably from 2+ to 5+ charged ions. the dynamic exclusion time was set to 6 s. complete datasets are available via proteomexchange with identifier pxd008656."
xml data transport in the proposed simulation encodes the structure of a neuronal network in xml and transports the structural information among simulation components. fig. 6 shows the xml dtd (document type definition) that xml data should follow in the proposed framework. each simulation component in the proposed framework implements input and/or output modules to read and write xml data formatted in the dtd in fig. 6 .
3) hash table generation : each preprocessed feature point in a model is mapped into the location p l of hash table by placing the midpoint of the co-ordinate system at the center of the hash table as follows
"40 fold in water) and 4 µl of 2.5% ammonium persulfate to the sample/acrylamide/water mixture. the polymerization was left to proceed for 1.5 hour at room temperature. native, urea and neutral sds samples can be polymerized in this format without any further adjustment. acidic sds samples were first brought back to neutrality by adding an identical volume of tris 100 mm to the sample volume. ctac-containing samples cannot be polymerized by this method irrespective of the ph because of the ctac-persulfate precipitation 18, which leads to polymerization inhibition 22 ."
"in order to seek pareto optimality, the notion of dominance [cit] plays an important role. an individual i is said to dominate an individual j if both of the following conditions are hold."
"however, in the dnns based asr framework, the data sparsity and unevenness problem get alleviating. considered these attractive properties employing syllable as modeling units, there is a potential and possibility of performance improvement for chinese asr."
"photopolymerization. the polymerization was initiated by adding 5 µl of 25 mm sodium toluene sulfinate, 5 µl of 0.5 mm diphenyliodonium chloride and 5 µl of 0.5 mm dye solution. photopolymerization was carried out for 2 hours with a 20 w led lamp placed 30 cm above the tubes. initial tests compared methylene blue 23, eosin y and flavine mononucleotide 24 as initiator dyes. eosin was selected for its ability to perform well with all types of extracts."
"1) feature extraction: for each model in the database, features are extracted by surf feature extraction algorithm which has more discriminative power than a local feature descriptor such as sift [cit] . assume that for a model, m j, surf has extracted m key points, f 1, f 2, ..., f m . it can be noted that the number of key points may vary from one model to another. each f i is a 3−tuple (x, y, d) where x, y is used as indices to insert into the hash table and the descriptor vector d is used to filter out the results during searching."
"in order to show the effectiveness of the enhanced geometric hashing with classical geometric hashing two databases such as iitk ear and upol public iris images are considered. the number of query images considered for comparison is small because the classical geometric hashing takes huge amount of time while searching because of nc 2 possible bases pairs. in our experiment, each image consists an average of 80 feature points and there are 80c 2 possible match results approximately 2.34 hrs. all the experiment has been executed in matlab. for ear database 1200 images for training and 6 images for testing are considered. similarly, for iris database 256 images and 10 images are considered for training and testing respectively."
"this paper assumes neuronal signaling in a network of natural neurons that are artificially grown and formed into particular topology patterns. this assumption is made upon numerous research efforts to grow neurons on substrates (e.g., [cit] ) and design topologically-specific neuronal networks (e.g., [cit] ). fig. 2 illustrates an example neuron-based intrabody nanonetwork. it contains an artificially-grown neuronal network and several nanomachines such as sensors and a sink. sensors use neuronal signaling to transmit sensor data to the sink, which might work as an actuator or transducer. as potential applications, prosthetic devices and medical rehabilitation devices could leverage neuron-based intrabody nanonetworks to better perform sensing, transducing and actuation tasks in the body. this paper assumes that nanomachines (e.g., sensors) interact with neuronal networks in a non-invasive manner. this means that it is not required to insert carbon nanotubes into neurons so that nanomachines can trigger signaling. nanomachines may use a neurointerface based on chemical agents (e.g., acetylcholine and mecamylamine [cit] ) or light [cit] ."
"neuronal tdma optimizer currently uses evolutionary multiobjective optimization algorithms (emoas) to solve its scheduling optimization problem. emoas are used as the optimizer's backend modules (fig. 3 ). an emoa iteratively evolves the population of solution candidates, called individuals, through several operators (e.g., crossover, mutation and selection operators) toward the pareto-optimal solutions in the objective space."
"tube gel preparation. for each condition, independent quadruplicate gels plugs were prepared. the gel plugs were cast in 1.5 ml eppendorf tubes. the protein amount was set at 100 µg, the acrylamide concentration was set at 10% and the gel plug volume was set at 100 µl. all initiator solutions (except the dye solutions) were made fresh the day of use. this scheme has been selected to mimic a real comparative proteomic experiment, in which different samples will be in different gel plugs. for each type of sample, it also investigated the variability brought by the proteomic process on a single starting sample. however, the variability brought by the extraction process itself, within each extraction type, was not investigated by this scheme."
"in contrast to classical geometric hashing, the response time in the enhanced geometric hashing is much faster because it needs to compare with a smaller number of feature points. for a query q, algorithm 2 gets the top k best matches from the hash table in a two step process, filtering and refinement. in the filtering step, the feature points which are dissimilar to the query's feature points are filtered out while at refinement step, the top k matches are found based on the voting scheme."
"neuronal tdma periodically assigns a time slot to each sensor node. sensors fire neurons, one after the other, each using its own time slot. this allows multiple sensors to transmit signals to the sink through the shared neuronal network. each sensor transmits a single signal (a single bit) within a single time slot. this single-bit-per-slot design is based on two assumptions: (1) a signal (i.e., action potential) is interpreted with two levels of amplitudes, which represent 0 and 1, and (2) after a signal transmission, a neuron goes into a refractory period (waiting/sleeping period)."
"this rbm based pre-training method can be viewed as a generative pretraining method, in which, the structure of input data is firstly learned before the multi-classification task."
"enhanced geometric hashing allows to solve the indexing and searching problem in one pass with linear time. it reduces the size of the hash table by inserting each feature points exactly once into the hash table. hence the computational cost reduces to o(mn ) in the indexing stage and o(nk) in the searching stage where k is the window size considered at the time of searching. the number of stored entries in the hash table of enhanced geometric hashing is mn . therefore, the number of entries in a sub region is reduced to mn/k in average. the preprocessing step has little effect on uniformity distribution in enhanced geometric hashing. hence there is no need of any rehashing function. further, enhanced geometric hashing has mn bins in the voting table. therefore, the computational cost is o(mn )."
"in our experiment, it has been observed that the value of bin-miss rate and penetration rate is less when compared with classical geometric hashing in both ear and iris databases. in addition, the search time in the enhanced geometric hashing technique has reduced significantly when compared with classical geometric hashing which is 0.23 sec. fig. 1 and fig. 2"
"in this context, the goal of our study is to improve our previously published tg protocol and extend its use by exploring its compatibility with various detergents and ph conditions. table 1 summarizes the protocols evaluated in the present work and their expected characteristics. the combination of laemmli buffer (sds in basic conditions) and chemical polymerization (stc) was considered as the reference protocol against which all other protocols were compared 13 . we investigated alternative extraction buffers based on the use of sds in acidic condition and cationic detergents such as cetyltrimethylammonium chloride (ctac) and chaotropic agents (urea). we also investigated proteins extraction in native conditions with weak denaturation. acrylamide photopolymerization (ptp) was tested in addition to classical chemical polymerization, to take advantage of its compatibility with a wide range of ph and the absence of oxidizing conditions. all extraction conditions were tested with chemical polymerization and ptp, except for cationic detergents that are not compatible with chemical polymerization because they induce precipitation of ammonium persulfate (aps) 18 . each condition was prepared in four replicates. comparisons were done on qualitative data (peptides and proteins identifications) and relative quantitative spectral counting results."
"qualitative protocols comparison using proteins and peptides identification results. the total number of protein sets identified with at least one unique peptide, after merging results of the four replicates for each protocol, are presented in table 2 . overall, merged protein numbers ranged from 1838 to 2476, illustrating the suited technical compatibility of tg protocol with all tested conditions. although the highest number of identified proteins was obtained with the use of stc, large and comparable numbers were also obtained with all other conditions. as expected, the lowest number of identified protein sets was obtained with the native extraction protocol with chemical polymerization. though, it is noteworthy that despite the weak protein denaturation, the number of identifications remains quite high in native conditions [cit] protein sets for the nap protocol)."
"in enhanced geometric hashing the size of the hash table is reduced by inserting each feature point exactly once into the hash table. hence the computational cost reduces to o(mn ) in the indexing stage and o(n) in the searching stage. the number of stored entries in the hash table is mn . therefore, the number of entries in a sub region is reduced to mn/k in average. the preprocessing step makes the distribution of feature points uniformly into hash table. hence, there is no need of any rehashing. further, enhanced geometric hashing has mn bins in the voting table. therefore, the computational cost is o(mn ). the enhanced geometric hashing has been tested on iit kanpur ear database of 1200 images and upol iris database of 384 images. for top 5 best matches, the identification accuracy of 100% is achieved in both cases."
"the cell pellet was resuspended by vortexing in the acidic sds lysis buffer (sds 2%, glycine 100 mm, hcl 50 mm, ph 2.4). the extraction took place in a water bath at 70 °c for 30 min. after cooling, the extract was centrifuged for 15 min at 15,000 g and the supernatant was collected. the protein concentration was measured by a modified dye binding assay"
"if the use of sds with in-gel digestion could be considered as a classical protocol for sample preparation, many other parameters could be modified, in particular for the removal of dna or anionic macromolecules. removal of dna could be obtained by precipitation with sds at low ph or with the use of spermine 14, 15, and removal of anionic macromolecules could be obtained with the use of cationic detergents 16, 17 . nevertheless, these conditions are generally not compatible with classical 1d-sds-page sample preparation methods."
"the problem of recognising an image has been extensively studied in the field of computer vision, pattern recognition, etc. the trivial solution for object recognition is to search all images in the database (henceforth called models) against a query image (henceforth called query). it is computationally inefficient to retrieve each model from the database and to compare it against the query for a match. hence there is a need to index these images to reduce the complexity of recognition significantly. however, following are some issues while indexing images of the large database."
this section evaluates the proposed simulation framework with a case study that implements neuronal tdma in a simulated neuronal network and examines its performance characteristics with two evolutionary multiobjective optimization algorithms (emoas): nsga-ii [cit] and sms-emoa [cit] .
a candidate set c i for the corresponding feature point f i of the query q contains all the model identity m id (c) such that
"the dnns based acoustic models can be trained using the embedded viterbi algorithm with gmms seeding. the gmms based system is firstly built, then conduct a forced alignment procedure with the gmm/hmms. the modeling units of the gmms are delivered as the modeling units for dnns. through the forced alignment, the input acoustic features are labeled, and then, the pre-trained neural net is fine-tuned discriminatively with bp."
"the proposed framework is designed to aid communication protocol designers to simulate neuron-based intrabody nanonetworks and validate protocols in the simulated nanonetworks. it integrates various simulation components such as visualizers and editors for neuronal networks, neuronal topology generators, media access controllers to neuronal networks and communication schedulers for neuronal signal transmissions. the proposed framework is implemented in java."
"in order to run emoas in neuronal tdma optimizer, each individual represents a particular tdma schedule for m sensors. fig. 9 shows the structure of an example individual, which represents the schedule shown in fig. 8 . in this example, the first sensor, s 1, fires its neighboring neuron, n 4, in the first time slot t 1 . similarly, s 2 and s 3 fire their neighboring neurons (n 2 and n 1 ) in t 2 and t 3, respectively."
"in the dnn based acoustic models, the dnn outputs the posterior probabilities of the acoustic modeling units over the input acoustic feature. in the hmm framework, the acoustic model is always formulated as:"
"enhanced geometric hashing consists of two stages known as indexing and searching. in the indexing stage, the models are inserted into the hash table. for any new model, it can be added without affecting the performance of the recognition algorithm and without modifying the existing index structures. indexing consists of three steps known as feature extraction, preprocessing and hash table generation. in the searching stage, it extracts the features from a query and preprocessed similar to indexing stage and accesses the previously constructed hash table for recognition. it consists of two steps, filtering and refinement."
"recommendations for use. it must be kept in mind that during the tube gel preparation protocol, all compounds initially present in the sample remain in the final gel plug, so that undesirable effects may occur arising from the sample composition. in particular, thiol-containing compounds strongly inhibit gel polymerization, and thiourea inhibits persulfate-initiated polymerization 19 (but not photopolymerization). besides this basic effect, it must be kept in mind that non-proteinaceous, acidic molecules (e.g. nucleic acids, anionic polysaccharides) are likely to inhibit protein digestion (by binding trypsin) and/or peptide extraction by electrostatic effects. the overall efficiency of the process may thus greatly vary according to the sample composition, and the ability of cationic detergents to precipitate such anionic macromolecules 16 may be advantageous for samples rich in anionic macromolecules."
"the experiments were carried on hub4 [cit] chinese broadcast news speech corpus (hub-4ne) training data which contains about 30 hours of speech and the test set is chinese broadcast news evaluation data which consist of about one hour speech. a tri-gram based language model was used in the evaluation, which was trained on the acoustic training set transcriptions with sri-lm tools."
"the idea of utilizing deep layered architecture for pattern recognition is not new. unfortunately, training a deep architecture is a challenge problem due to the poor generalization property and suboptimal solutions [cit] . until recently, many new machine learning algorithms were developed for training deep models, which have trigged the applications of many deep structure models."
"the cell pellet was resuspended by vortexing in the neutral sds lysis buffer (sds 2%, tris hcl 100 mm ph 7.5). the extraction took place in a water bath at 70 °c for 30 min. after cooling, the viscosity of the extract was decreased by passing the viscous extract in a 1 ml syringe fitted with a 23 gauge needle. the protein concentration was measured by a modified, detergent-compatible dye binding assay"
"the remainder of this paper is organized as follows. in section 2, the machine learning method of deep neural network is reviewed. section 3 will discuss the syllable based acoustic modeling method in detail. in order to present how the wfst based decoding framework can solve the problems caused by the too many context dependent syllable modeling units, some studied of wfst are discussed in section 4. the experiments are presented in section 5, followed by the conclusions and future work."
"the cell pellet was resuspended by vortexing in the acidic ctac lysis buffer (hexadecyl trimethylammonium chloride (ctac) 2.5%, glycine 100 mm, hcl 50 mm, ph 2.4). the extraction took place in a water bath at 70 °c for 30 min. after cooling, the extract was centrifuged for 15 min at 15,000 g and the supernatant was collected. the protein concentration was measured by a modified dye binding assay"
"although many successful everyday-life application systems have been deployed in last two decades, the performance of automatic speech recognition (asr) still remains unsatisfying. as a vital component, acoustic modeling is always the research focus in asr. recently, the proposal of context-dependent deep neural networks (dnns) hidden markov models (hmms) for large vocabulary speech recognition have achieved the most competitive performance."
"the problems of building a syllable based asr systems come from two aspects, the data sparsity and unevenness for model training and lextree based viterbi beam search with too many nodes."
"molecular communication is a communication paradigm that utilizes molecules as a communication medium between nanomachines. nanomachines are nanoscale devices that perform simple computation, sensing and/or actuation tasks [cit] . they may be man-made devices built in the top-down approach, downscaling the current neurons are a fundamental component of the nervous system, which includes the brain and the spinal cord. they are electrically excitable cells that process and transmit information via electrical and chemical signaling."
"all the gmms based acoustic models were trained using ml criteria, and contain 32 gaussians. based on these g-mm/hmms, the forced alignment procedure is conducted. then the modeling units of gmms are delivered to the dnns based models. the dnns used in the experiments have 4 hidden layers with 2500 nodes in each layer. in the rbm based pre-training procedure, the momentum term for rbms was increased from 0.5 to 0.9 after the first twenty epochs. learning rate was fixed to 0.001 for the first layer of rbm with gaussian visible units, and fixed to 0.01 for the other layer. the first layer was trained for 200 epochs while the other layer 60 epochs. for the discriminative training, the bp algorithms with stochastic gradient descent was used. the mini-batch is 128, and the initial learning rate is 0.008. at the end of each epoch, if the substitution error rate on the development set decreased less than 0.1, the learning rate begins to halving and then continue until the substitution error on the development set increased."
"one of the limitations of multi-scale representations is the relatively high memory overhead to store all intermediate levels. therefore, it would make sense to study the possibility to exploit meshless multi-resolution structures which are much lighter [wge * 04]. our current approach is also limited to features exhibiting a constant scale, and it would be interesting to extend it to track scale-varying features as the headband of figure 10 . table 1 : timings of the different subroutines involved in our approach to analyze a given roi and edit the object using sliders. the roi is preprocessed at the selection time, then the geometry of the entire object can be edited in real time even for large objects."
"the original model xml description file can be defined by the users, who are usually experts, and the model base contains predefined xml-based integrated geoscience models, such as global climate change models and the hydrology models. these predefined models can be used directly to solve specific problems. the original integrated model xml description files are created by referring to bpel specification; hence, an xml document can be converted into a standard bpel format and executed via a bpel engine. in general, when users need to execute an integrated geoscience model, the following steps are carried out, as shown in figure 5 ."
"we compared the splicing code's performance trained with the dnn with the bnn, as well as an mlr classifier as a baseline. the mlr was trained by removing the hidden layer while keeping the training methodology identical to the neural networks. because the predictions of the i124 bnn consist only of the psi prediction for each tissue separately at the output [cit], for the bnn to make tissue difference predictions in the same manner as the dni code, we used a mlr on the predicted outputs for each tissue pair. for a fair comparison, we similarly trained a mlr on the lmh outputs of the dnn to make dni predictions, and report that result separately. in both cases, the inputs to the mlr are the lmh predictions for two tissues as well as their logarithm. schematic of the bnn and mlr architecture can be found in supplementary figures s3 and s4."
"during the last decade, numerous deformation tools have been proposed to ease the manipulation of global shapes (e.g., the pose of a character) of increasingly complex and detailed geometries [bs08, llco08, jbps11, plg13] . in contrast, the manipulation of medium to fine scale features (e.g., carved symbols) has received very little attention. sculpting tools, as found in many production software, are very well suited to the creation of details and they can even be applied to point-clouds [zpkg02, pkkg03] . this is desirable to avoid issues with changes in topology and to enable the direct editing of data coming from 3d acquisition systems. on the other hand, sculpting tools are time consuming when it comes to exaggerating or smoothing existing geometric features: the user has to manually adjust the size of the brush's footprint, and advanced drawing skills are required to properly follow the features. moreover, this ap-proach lacks accuracy as it is often impossible to precisely edit a feature at a given scale without affecting the other ones. our main objective is to make this possible."
"the original model xml description file can be defined by the users, who are usually experts, and the model base contains predefined xml-based integrated geoscience models, such as global climate change models and the hydrology models. these predefined models can be used directly to solve specific problems. the original integrated model xml description files are created by referring to bpel specification; hence, an xml document can be converted into a standard bpel format and executed via a bpel engine. in general, when users need to execute an integrated geoscience model, the following steps are carried out, as shown in figure 5 . 2. calculating flow direction: after filling, the value of each center pixel is not smaller than the values of the eight pixels around it; thus, each water pixel will flow toward the pixels with lower values. this process is utilized to form the 8 flow directions. the grid flow is calculated by using the d8 algorithm to create the flow from each pixel toward the steepest downhill adjacent points."
"with continuous advancements in the sharing, exchanging, and use of spatial data, the sharing and interoperability of processing functions have received increased attention. a web service provides an open platform for the sharing of spatial information and geoprocessing functions. it is critical to implement the sharing of earth observation data and geospatial analysis algorithms [cit] . by accessing web service resources, all geoprocessing functions from algorithm publishers can be provided to algorithm users through the internet. the emergence of common web services also enables geospatial data sharing and algorithm interoperability, and, when compared with other distributed architectures, the geoscience algorithm sharing based on a common service-oriented architecture (soa) has obvious advantages. however, there is no standard scheme for the integration of common web service-based geospatial algorithms; hence, this method can only be used to share geospatial data and geoscience algorithms within a limited scope and community. moreover, it is difficult to implement the automatic discovery and integration of widely accepted geoscience algorithms."
"to train and test the dnn, data was split into approximately five equal folds at random for cross validation. each fold contains a unique set of exons that are not found in any of the other folds. three of the folds were used for training, one used for validation and one held out for testing. we trained for a fixed number of epochs and selected the hyperparameters that give the optimal auc performance on the validation data. the model was then retrained using these selected hyperparameters with both the training and validation data. five models were trained this way from the different folds of data. predictions from all five models on their corresponding test set were used to evaluate the code's performance. to estimate the confidence intervals, the data were randomly partitioned five times, and the above training procedure was repeated."
"developments in the comprehensive research on earth systems have led to increased demands for geoscience data and algorithms. to overcome the defects of geoscience analysis and decision-making models in the local area network environment and the heterogeneous implementation of algorithms, this paper provides a method for geoscience algorithm integration in a distributed environment. the interface of the ogc ows standard specifications is used to solve the problem of interoperability in distributed algorithm integration. a river network extraction experiment is used to demonstrate the feasibility of the proposed method. this study can help to promote the development of a distributed seamless information environment for scientific earth system research, support the wide and deep sharing, integration, diffusion of geoscience resources, and also contribute to the realization and application of open science."
"the wear and tear processes start from discrete contact with the hard surface [cit] . elastic and plastic resistance in materials, microcutting, the destruction of layers and base material are basic types of such imperfections. the most characteristic reasons for the wear and tear process are high contact pressure (causing phase changes, and often also leading to the melting of metal), thermo-chemical processes (leading to oxidizing of the superficial layer, dissolving one of the components, brittleness of the superficial layer), influence of lubricant (which after getting into the micro-cracks can cause material to wedge out and an increase in wear and tear), moving material from one surface to another, internal stresses of different signs (+,-). in this paper a primary repair quality index has been described analytically with the help of an arithmetic matrix. this matrix takes into account the relationship between production capability, structural and organizational factors and the quality of machine repair."
"driven by network technology, web service platforms have become a new solution for the integration of network applications. web service technologies have been widely used to construct distributed, modular applications and service-oriented applications. accordingly, with the application of geographical information technology in various fields, the technology is moving from a closed (tightly coupled standalone) system to an open and loosely coupled service. users can access and use geographic data, geoprocessing services, and mapping services on demand via the internet [cit] . service-oriented applications have become the direction of new geoscience developments. geospatial application activities are moving from a professional field to a networked, socialized, and popular service that is being accepted by various domain experts and even nonprofessionals [cit] . an algorithm service deployment strategy for sharing geoanalysis algorithms was proposed and implemented to provide a collaboration-oriented method that allowed modeling participants to work together and integrate algorithms and computational resources across an open web environment [cit] . a virtual workflow system can provide an efficient graphical user interface (gui) that users can utilize to integrate distributed scientific collaborative services and execute them on grid resources [cit] ."
"various results of our editing tools are provided in figure 9 on a synthetic object, in figure 1 on a large sculpted mesh, and in figures 8 and 10 on scanned objects. all these results include both smoothing and enhancement of both small and medium scale details. we emphasize that smoothing a medium scale feature while preserving finer details, as for instance in figures 10 and 8 -right, is impossible to original edited figure 10 : edition of the igea model headband: a mediumscale structure is flattened while the small details of the hairs are preserved."
"overall, the decrease in performance is not unexpected, owing to differences in psi estimates from variations in the experimental setup. to see how psi differed, we computed the expected psi values for brain and heart in all exons from both sets of experiments, and evaluated their pearson correlation. for the brain, the correlation is 0.945, and for the heart, it is 0.974. this can explain why there is a larger decrease in performance for brain, which is a particularly heterogeneous tissue, and hence can vary more between experiments depending on how the samples were prepared. we note that the performance of the dnn on this dataset is still better than the bnn's predictions on the original dataset. viewed as a whole, the results indicate that our model can indeed be useful for splicing pattern predictions for psi estimates computed from other datasets. it also shows that our rna-seq processing pipeline is consistent. we believe there are several reasons why the proposed dnn has improved predictive performance in terms of tissuespecificity compared with the previous bnn splicing code. one of the main novelties is the use of tissue types as an input feature, which stringently required the model's hidden representations be in a form that can be well-modulated by information specifying the different tissue types for splicing pattern prediction. this requirement would be relaxed if each tissue was trained separately. furthermore, this hidden representation is described by thousands of hidden units and multiple layers of non-linearity. in contrast, the bnn only has 30 hidden units to represent the variables that can be used by the model to modulate splicing based on the cellular condition, which may not be sufficient. another difference is that for the dni code, a training objective was specifically designed to learn to predict \"psi, which is absent from the bnn. however, the performance gain of the dnn-mlr over bnn-mlr shows that this is only part of the improvement."
"in this paper, d-rpl is implemented for the dynamic applications of iot to accommodate the network requirements and mobility demands of these applications, it is based on and compatible with rpl making it a flexible and scalable solution. simulation results show that d-rpl improves the pdr, end-toend delay, and energy efficiency of the network."
"the integrated model utilizes dem data as the input data and it returns the results of the river network in gml format after processing. the first three test schemes can execute the integrated geoscience model and obtain the results successfully. after the execution of the integrated geoscience algorithms, a gml-based result is produced. through parsing the gml results, the vector data result is created. in test 4, the model is executed on the arcgis platform, resulting in a shapefile format. the vector data are then overlapped with an esri web map, and the final results are shown in figure 8a -d, corresponding to the dem data sizes of 2 mb, 20 mb, 100 mb, and 500 mb, respectively. the vector lines are the extracted river network in the study area, reflecting the size of the data at different distribution ranges, and the background is an esri web map."
"there are several practical considerations when using bnns. they often rely on methods like markov chain monte carlo (mcmc) to sample models from a posterior distribution, which can be difficult to speed up and scale up to a large number of hidden variables and a large volume of training data. furthermore, computation-wise, it is relatively expensive to get predictions from a bnn, which requires computing the average predictions of many models."
"we present three sets of results that compare the test performance of the bnn, dnn and mlr for splicing pattern prediction. the first is the psi prediction from the lmh code tested on all exons. the second is the psi prediction evaluated only on targets where there are large variations across tissues for a given exon. these are events where \"psi ! ae0.15 for at least one pair of tissues, to evaluate the tissue specificity of the model. the third result shows how well the code can classify \"psi between the five tissue types. hyperparameter tuning was used in all methods. the averaged predictions from all partitions and folds are used to evaluate the model's performance on their corresponding test dataset. similar to training, we tested on exons and tissues that have at least 10 junction reads."
"achieve with standard smoothing tools. figure 11 demonstrates the ability of our approach to detect and edit large scale features. in figure 12 we used our tool on a raw noisy point cloud acquired by a kinect to remove the levels of the first detected scale, corresponding to the acquisition noise."
"to learn a set of model parameters, we used the cross-entropy cost function e on predictions h(x,t,) given targets y(x,t), which is minimized during training:"
"the bnn architecture used for comparison is the same as previously described [cit], but trained on rna-seq data with the expanded feature set and lmh as targets. although hidden variables were shared across tissues in both the bnn and dnn, a different set of weights was used following the single hidden layer to predict the splicing pattern for each tissue separately in the bnn ( supplementary fig. s3 ). in the current dnn, the tissue identities are inputs and are jointly represented by hidden variables together with genomic features. for the bnn to make tissue difference predictions in the same manner as the dni code, we fitted a mlr on the predicted lmh outputs for each tissue pair ( supplementary fig. s4 )."
", the scale variation of the offset τ is directly correlated to the magnitude of the displacement vector d l i . an important difference though, is that thanks to the continuous definition of these parameters in scale, their partial derivatives can be computed analytically and thus they do not depend at all on the arbitrary choice of a discretization in scale."
"steps 2 and 3 are repeated until all centers have been computed. since we strived to build the weights β l to be as discriminating as possible, this initialization of the centers often leads to very good guesses."
"this last observation motivates the needs for a clear separation of the features at different scales. a well established practice for the multi-scale analysis of shapes is to consider the eigenfunction decomposition of the heat kernel or laplace-beltrami operator [lev06, sog09, zvkd10] . such spectral approaches have been recently extended to pointclouds [lpg12, ps13]. the shape can then then be manipulated at different frequencies through a re-composition of the eigenfunctions [vl08] . however, this does not permit to directly edit a precise feature at a given scale as such a task requires to determine the respective set of eigenfunctions. despite a very recent effort in that direction [nvt * 14], this still constitute a challenging problem."
"where: kij -coefficients of reduction taking into account the influence of input variables on the output variables or correlation coefficient, ki -spread coefficient of output variable, kxi -spread coefficient of input variables. let us assume that the system which is described by the linear dependence is stabilised and describes its static properties. for a nonlinear relationship, which we meet in reality, we cannot use the principle of superposition, but we can use linearization of a system by changing nonlinear equations into a linear equation under given variation sections of input and output variables: in the eq. (3) the determination of the matrix of reduced correlation coefficients is the most complicated. we can consider them as linear operators of transforming input variables into output variables:"
"to promote the neural network to better discover the meaning of the inputs representing tissue types, we biased the distribution of training examples in the minibatches. we first selected all events which exhibit large tissue variability, and constructed minibatches based only on these events. at each training epoch, we further sampled (without replacement) training cases from the larger pool of events with low tissue variability, of size equal to one fifth of the minibatch size. the purpose is to have a consistent backpropagation signal that updates the weights connected to the tissue inputs and bias learning towards the event with large tissue variability early on before overfitting occurs. as training progresses, the splicing pattern of the events with low tissue variability is also learned. this arrangement effectively gives the events with large tissue variability greater importance (i.e. more weight) during optimization. a side effect is that it also places more importance to the medium category of the lmh code during training, since they tend to be present more often in exons with tissue-specific splicing patterns."
"in this work, we have presented a novel technique for the automatic extraction of the pertinent scales of surface features. our approach exploits recent advances in multi-scale analysis of point clouds and does not require any connectivity information. we showed how to turn the problem of scale detection into a more general clustering problem thus allowing us to take advantage of the extended literature on the subject. we demonstrated the robustness and utility of our approach through a novel editing tool of surface features combining our detecting mechanism with a compatible multi-scale point-cloud decomposition."
"test 1: no data transmission, and the data and processing methods are in the same cloud node; this scenario tests the proposed geoscience algorithm integration method in the single node and it can be used by the distributed users."
"roi pertinent scales extraction gui generation and mapping figure 7 : overview of an interactive editing session: starting from a selected roi (a), our system automatically extract the pertinent scales interactively (b). a graphic equalizer is generated and mapped to scale intervals in the multi-scale decomposition (c). (d) the user can select any region containing similar feature scales on the model and edit them using the sliders, all the changes being applied in real-time."
the practical and simulation results are almost the same in spite of the external factors that are expected to affect practical testing. this confirms that cooja is successful in emulating the actual hardware and providing a realistic channel model.
"in this work, we introduced a computational model that extends the previous splicing code with new prediction targets and improved tissue-specificity, using a learning algorithm that scales well with the volume of data and the number of hidden variables. the approach is based on dnns, which can be trained rapidly with the aid of gpus, thereby allowing the models to have a large set of parameters and deal with complex relationships present in the data. we demonstrate that deep architectures can be beneficial even with a sparse biological dataset. we further described how the input features can be analyzed in terms of note: the direction of the arrows indicate that a feature's value should in general be increased (\") or decreased (#) to change the psi predictions to low or high. feature details can be found in section 4 of the supplementary material."
thresholding flow accumulation: the threshold of the number of water droplets is calculated while considering that the number of water droplets in a river network pixel is greater than the threshold value. the binarization algorithm is used to set the values of the pixels in the river network to 1 and the other values to 0.
"to obtain a high-quality integrated geoscience model, more efforts will be dedicated to optimizing the performance and the reliability of distributed geoscience algorithm integration, particularly when both the algorithms and data are dispersed in the distributed environment. ongoing research will include evaluating the uncertainty of the distributed geoscience algorithm integration and improving the robustness and intelligence of the integrated geoscience model. for example, research on finding high-quality distributed algorithms to integrate the geoscience model is important."
"here, we show that the use of large (many hidden variables) and deep (multiple hidden layers) neural networks can improve the predictive performances of the splicing code compared with previous work. we also provide an evaluation method for researchers to improve and extend computational models for predicting as. another goal is to describe the procedure for training and optimizing a deep neural network (dnn) on a sparse and unbalanced biological dataset. furthermore, we show how such a dnn can be analyzed in terms of its inputs. to date, aside from a small number of works [cit], deep learning methods have not been applied in the life sciences, even though they show tremendous promise."
"in this study, we conducted automatic sentiment analysis of short informal indonesian product reviews. this is very useful because it allows review to be aggregated without manual intervention. consumers can utilize this information to research products before buying. marketers can utilize this to research public opinion of their products. organizations can also utilize this to get critical feedback about problems in their newly released products. the reviews are gathered from a social platforms that provides reviews from users about certain product. every review in this platform is a short informal indonesian text that express positive or negative opinion about the product. most of the reviews are short texts with informal language, creative spelling and punctuation, misspellings, and slang word. this paper aims to improve short text bag of words representation for sentiment analysis. we developed automatic sentiment analysis system of short informal indonesian texts using naïve bayes and synonym based feature expansion. in the first step, we counduct preprocessing normalizing misspellings and slang words. in the next we use kateglo api (kateglo.co.id) to find synonym of each word in texts to enrich the original texts. finally, we do classification using naïve bayes classifier and bag of words as the features."
"the iot covers a wide range of applications using different standards and technologies to serve a large number of applications. these applications have different network requirements, different node distribution and different mobility scenarios. d-rpl is designed for networks where nodes can be attached to people or objects building a dynamic mobility scenario in which the dodag formation can involve multiple mobile nodes. in this paper, a realistic iot applications with dynamic mobility scenarios that require multi-hop routing to the root or gateway through mobile nodes is presented. the design of d-rpl includes improvements to the rpl trickle timer, a new objective function and the interaction between these two factors to manage mobile nodes in the network and improve the performance of rpl routing."
"recently, deep learning methods have surpassed the state-ofthe-art performance for many tasks [cit] . deep learning generally refers to methods that map data through multiple levels of abstraction, where higher levels represent more abstract entities. the goal is for an algorithm to automatically learn complex functions that map inputs to outputs, without using hand-crafted features or rules [cit] . one implementation of deep learning comes in the form of feedforward neural networks, where levels of abstraction are modeled by multiple non-linear hidden layers."
"the dnn weights were initialized with small random values sampled from a zero-mean gaussian distribution. learning was performed with stochastic gradient descent with momentum and dropout, where minibatches were constructed as described above. a small l1 weight penalty was included in the cost function (4) [cit] . the model's weights were updated after each minibatch. the learning rate \" was decreased with epochs e, and also included a momentum term that starts out at 0.5, increasing to 0.99, and then stays fixed. the momentum term accelerates learning, and stabilizes learning near the end of training when the momentum is high by distributing gradient information over many updates. the weights of the model parameters were updated as follows:"
"library, and the integrated geoscience model is executed via apache ode. to compare with the traditional single-machine-based method, in test 4, the same model is built via the arcgis model builder and executed on a machine. the four test schemes are as follows:"
"the predictions of the model to gain some insights into the inferred tissue-regulated splicing code. our architecture can easily be extended to the case of more data from different sources. for example, using the same architecture, we may be able to learn a hidden representation that spans additional tissue types as well as multiple species. through transfer learning, training such model with multiple related targets might be beneficial particularly if the number of training examples in certain species or tissues is small."
"different from the previous bnn, which used 30 hidden units, our architecture has thousands of hidden units with multiple non-linear layers and millions of model parameters (supplementary table s2 ). we also explored a different connection architecture compared with previous work. before, each tissue type was considered as a different output of the neural network. here, tissues are treated as an input, requiring that the complexity of the splicing machinery in response to the cellular environment be represented by a set of hidden variables that jointly represent both the genomic features and tissue context. besides a different model architecture, we also extended the code's prediction capability. in previous work, the splicing code infers the direction of change of the percentage of transcripts with an exon spliced in (psi) [cit], relative to all other tissues. here, we perform absolute psi prediction for each tissue individually without the need for a baseline averaged across tissues. we also predict the difference in psi (\"psi) between pairs of tissues to evaluate the model's tissue-specificity. we show how these two prediction tasks can be trained simultaneously, where the learned hidden variables are useful for both tasks."
"we used a dropout rate of 50% for all layers except for the input layer (the autoencoder), where we did not use dropout, as it empirically decreased the model's predictive performance. training was carried out for 1500 epochs for both the pretraining with the autoencoder and supervised learning."
"in our prototype implementation, the multi-scale decomposition is precomputed using a kd-tree to accelerate the neighbor search. the fitting at scale t is performed by collecting all the neighbors within the distance t of the current point. this preprocess takes from several seconds to a few minutes depending on the size of the point clouds, number of scales, and size of the largest scales. we believe that multiple order of magnitude could be saved using both a gpu implementation and by iteratively down-sampling the input point cloud to compute the largest scales [pkg06] ."
"both experiments demonstrate the feasibility of the proposed method, but it is also clear that the performance of the proposed method is affected by data transmission. as a result, future efforts should focus on enhancing the efficiency and reliability of geoscience algorithm integration when both the algorithms and data are highly dispersed in the network environment, which is critical for geoscience algorithm integration, particularly when there is an unstable and low-speed network."
"the first hidden layer was trained as an autoencoder to reduce the dimensionality of the feature space in an unsupervised manner. an autoencoder is trained by supplying the input through a non-linear hidden layer, and reconstructing the input, with tied weights going into and out of the hidden layer. this method of pretraining the network has been used in deep architectures to initialize learning near a good local minimum [cit] . we used an autoencoder instead of other dimensionality reduction techniques like principle component analysis because it naturally fits into the dnn architecture, and that a non-linear technique may discover a better and more compact representation of the features."
"the naïve bayes classification is incorporated into the bayes learning algorithm constructed by training data to estimate the probability of each category contained in the characteristics of the testing document. in general, the classification process using the naïve bayes method can be seen in equation 1. between categories. thus, the process of classification can be simplified to be equation 2."
"1. filling pits: this step is used for data preprocessing. in the case of data errors, the original dem data will have noise and there will be pits. in the d8 flow direction algorithm, a part of the river network is broken down, which contradicts the rules of river formation. the small defects in the data are removed by filling the pits in the dem data, as shown in figure 3 . the details of each step are as follows:"
"the original model xml description file can be defined by the users, who are usually experts, and the model base contains predefined xml-based integrated geoscience models, such as global climate change models and the hydrology models. these predefined models can be used directly to solve specific problems. the original integrated model xml description files are created by referring to bpel specification; hence, an xml document can be converted into a standard bpel format and executed via a bpel engine. in general, when users need to execute an integrated geoscience model, the following steps are carried out, as shown in figure 5. i."
"previously, a 'splicing code' that uses a bayesian neural network (bnn) was developed to infer a model that can predict the outcome of as from sequence information in different cellular contexts [cit] . one advantage of bayesian methods is that they protect against overfitting by integrating over models. when the training data are sparse, as is the case for many datasets in the life sciences, the bayesian approach can be beneficial. it was shown that the bnn outperforms several common machine learning algorithms, such as multinomial logistic regression (mlr) and support vector machines, for as prediction in mouse trained using microarray data."
"numerous gss appear in distributed environments; thus, a mechanism is needed to assist domain experts with accurately and efficiently finding the required gss from a large set of available gss. the service management mechanism is the facility that guarantees the integration of the distributed geoscience algorithms. the registration and discovery of gss can be implemented by establishing a registry center for the services. distributed gss can be divided into four categories: portrayal service, data service, processing service, and registration service. the portrayal service is used to depict the visualization of geographic information that is presented to the user. the data service (e.g., wfs, wcs) is responsible for providing the spatial data using a service interface. the processing service (e.g., wps) provides spatial data analysis functions to achieve value-added information. the registration service records the above three services. in this paper, a gs classification system is designed, as shown in table 1 ."
"converting the data format: the grid of the river network is converted to vector format to facilitate data editing and analysis. 2. calculating flow direction: after filling, the value of each center pixel is not smaller than the values of the eight pixels around it; thus, each water pixel will flow toward the pixels with lower values. this process is utilized to form the 8 flow directions. the grid flow is calculated by using the d8 algorithm to create the flow from each pixel toward the steepest downhill adjacent points. as shown in figure 4, the values are 1, 2, 4, 8, 16, 32, 64, and 128 in each direction. 3. calculating flow accumulation: to form a river network by rainwater, each grid is given a water drop. the flow calculation creates a grid for each water droplet accumulated by each pixel. 4. thresholding flow accumulation: the threshold of the number of water droplets is calculated while considering that the number of water droplets in a river network pixel is greater than the threshold value. the binarization algorithm is used to set the values of the pixels in the river network to 1 and the other values to 0."
"a typical editing session is depicted in figure 7 : a small roi of the armadillo's shell is used to quickly learn its feature scales, and then the entire shell is edited by manipulating a single slide to invert the cavities."
"based on the diagram of mutual connections of input and output factors during machine repair it was found that every input variable has its influence on all the output variables. using the principle of superposition, each variable is considered as a linear combination of indexes and this can be described according to the matrix equation:"
"the performance of a dnn depends on a good set of hyperparameters. instead of doing a grid search over the hyperparameter space, we used a bayesian framework called spearmint to automatically select the model's hyperparameters [cit] . the method uses a gaussian process to search for a joint setting of hyperparameters that optimizes an algorithm's performance on validation data. it uses the performance measures from previous experiments to decide which hyperparameters to try next, taking into account the trade-off between exploration and exploitation. this method eliminates many of the human judgments involved with hyperparameter optimization and reduces the time required to find such hyperparameters. the algorithm requires only the search range of hyperparameter values to be specified, as well as how long to run the optimization for. we used the expected improvement criterion in the optimization, as it does not require its own tuning parameter, unlike other methods in the framework. we score each experiment by the sum of the aucs from both the lmh and dni codes, requiring the set of hyperparameters to perform well on both tasks. detailed information on the selected hyperparameters and search procedure are described in section 2 of the supplementary material."
"next, we wanted to see how features are used in a tissue-specific manner. using the set of exons with high tissue variability, we computed the backpropagation signal to the inputs with the output targets changed in the same manner as above, for each tissue separately. figure 3 shows the sum of the magnitudes of the gradient, normalized by the number of examples in each tissue for the top 50 features. we can observe that the sensitivity of each feature to the model's predictions differs between tissues. the profile for kidney and liver tend to be more similar with each other than others, which associates well with the model's weaker performance in differentiating these two tissues. this figure also provides a view of how genomic features are differentially used by the dnn, modulated by the input tissue types. in both table 4 and figure 3, the backpropagation signals were computed on examples from the test set, for all five partitions and folds."
"to address this limitation, we propose a novel multiscale decomposition which automatically and dynamically groups the different frequencies such that every level precisely matches a relevant feature present in a region of interest selected by the user. each feature may thus be enhanced or smoothed independently by controlling a single weight per feature, for instance via an equalizer interface ( figure 1) . this is made possible by a multi-scale analysis technique that automatically determines the ranges of scales corresponding to the most relevant features. from a technical point of view, we introduce a novel method to estimate the pertinence of a scale which extends the notion of homological persistence [elz00] using a meshless geometric variation measure [mgb * 12]. we also show how the extraction problem can be robustly addressed by a clustering algorithm. consistency between our multi-scale analysis and decomposition is ensured by a joint use of the algebraic point set surfaces definition [gg07], which robustly defines a smooth implicit surface from a set of points at arbitrary scales through the fitting of algebraic spheres. finally, after a short preprocessing time, our approach handles millions of points in real-time."
"the behavior of these two measures are depicted in figure 4 which also includes the more standard surface variation [pkg03] . this later is defined as the ratio between the smallest and the sum of the eigenvalues of the local covariance matrix. in practice, this ratio only measures the deviation of the neighborhood to a plane and often fails to capture many of the variations."
"sentiment analysis is one of the fundamental problems in natural language processing (nlp). sentiment analysis involves analyzing people's opinions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes from a piece of text [cit] . sentiment analysis has a number of applications, includi ng ranking products and merchants [cit], predicting election results [cit], predicting box-office revenues for movies [cit], predicting the stock market [cit], characterizing social relations [cit], and etc. the rise of social media make us now dealing with much more short informal texts every day. examples are tweets, status updates, comments, and reviews from various social platforms. working with these short informal text genre is more challenging compared to traditional text genres because there are many limitations in this genre. thus, there is growing interest in sentiment analysis of this kind of texts."
"in practice, this editing can be localized by selecting any region having a similar structure as the roi (figure 7d ). in that case, each point or vertex has to store its own vector of reconstruction coefficients a. moreover, in order to ensure a continuous reconstruction, the effect of the sliders within the region has to smoothly vanish at the boundary of the selected region."
"the performance of the model was assessed using the area under the receiver-operating characteristic curve (auc) metric. to evaluate the psi predictions for the lmh code, we used the 1 versus all formulation. this produces three aucs (auc low, auc med, auc high ), one for each class. for \"psi predictions, since the no change class is much more abundant, we find that the multi-class 1 versus all formulation tends to overestimate the tissue-specificity performance of the model due to class skew [cit] . furthermore, the model can predict, based on the genomic features alone, that there is tissue-specific splicing for a given exon (which is biologically meaningful), but not necessarily how different tissues change the splicing pattern. we therefore provide two metrics to evaluate the dni code. the first is to compute the auc dvi based on the decrease versus increase class between two tissues. the second is to compute auc change by comparing no change versus the other two classes."
2. the results of the previous point-wise analysis are gathered over the roi to get a measure of the likelihood of each scale to be a pertinent one. pertinent scales are finally extracted using an unsupervised clustering algorithm as depicted in figure 2d and detailed in section 4.3.
which can be seen as a distribution over the scale axis representing the likelihood of each scale to be a pertinent one in the roi. an example of this distribution is illustrated in figure 6 on a vase model.
"2. for each scale t l, compute the distance d(t l ) between t l and the nearest center that has already been computed."
"with the increasingly rapid growth in the volume of 'omic' data (e.g. genomics, transcriptomics, proteomics), deep learning has the potential to produce meaningful and hierarchical representations that can efficiently be used to describe complex biological phenomena. for example, deep networks may be useful for modeling multiple stages of a regulatory network at the sequence level and at higher levels of abstraction."
"a major contribution to the success of splicing pattern predictions that generalize well comes from the richness of our feature set. for example, we observed a significant decrease in the performance of the splicing code if the reduced feature vector dimension is too small by either principle component analysis or an autoencoder with small number of hidden units. we found that the performance of both the lmh code and the dni code drops by up to 4% when the reduced dimension is at 150 (down from 1393). this suggests a sufficiently large number of hidden variables denoting genomic features are required to interact with tissue inputs to achieve good performance."
"the wps interface is proposed to address the deficiencies of web services in solving functional interoperability and the increasing demand for network-based spatial data processing. \"processing\" can be an algorithm, computation, or model for processing spatial data. the wps interface standard provides rules for standardizing how to construct the inputs and outputs of geospatial processing services and geographic computing in a standard way, making it easier for users to publish geospatial processing services and discover and bind these services. wps also defines how the client invokes the processing service and how to process the output of the processing service. the implementation of this standard allows any geospatial processing service, regardless of its source, to be encapsulated and integrated into existing workflows using standard interfaces. the wps standard defines a general process model, which is designed to provide interoperability descriptions for geographic processing and computing and support service discovery in distributed environments."
"alternative splicing (as) is a process whereby the exons of a primary transcript may be connected in different ways during pre-mrna splicing. this enables the same gene to give rise to splicing isoforms containing different combinations of exons, and as a result different protein products, contributing to the cellular diversity of an organism [cit] . furthermore, as is regulated during development and is often tissue dependent, so a single gene can have multiple tissue-specific functions. the importance of as lies in the evidence that at least 95% of human multi-exon genes are alternatively spliced and that the frequency of as increases with species complexity [cit] ."
the proposed rssi-based reverse-trickle timer mechanism in d-rpl aims to reduce the hand-over delay by sensing rssi values and detecting mobility or inconsistency while the proposed objective function d-of which is responsible of parent selection aims to reduce the number of unnecessary handovers by comparing the calculated cost to the parent switching threshold. the integration of d-rpl and d-of creates an optimization of these two crucial factors making it an adaptable solution for dynamic iot applications.
"the testing scenario involves 1 static sink node and 9 mobile nodes moving at 0 -1.5 m/s. mobile nodes are connected to people moving at normal human speeds and pausing for a maximum period of 30s. nodes are placed with minimum overlapping to ensure multi-hop communication. the sink node is the only static node in the network, other nodes move randomly to force topology changes."
this section describes our analysis and learning pipeline for the automatic detection of pertinent scales in a given region of interest (roi). its general principle is depicted in figure 2 .
"only partial data (i.e., dem data) are acquired by distributed transmission, and the other required data and processing methods are on one cloud node. this test scenario tests the execution of the proposed geoscience algorithm integration method between two organizations and can be used by the distributed users. 3."
"in order to test the real performance of d-rpl, we conducted hardware testing using 10 tmote sky nodes mtm-cm5000-msp. the experiment was conducted in 2 environments, an obstacle-free open field and an indoor environment with obstacles. a simulation scenario is also created for comparison using a similar topology to the real hardware experiments."
"the gs providers provide two services: a spatial data service and a geoprocessing service. the geospatial data are provided by the wcs, wfs, and wms, and the geoprocessing functionalities are provided by the wpss. a simple geoprocessing algorithm can be encapsulated in a wps, which can also integrate with other geoprocessing algorithm services to form complex geoscience algorithms. this step results in a great improvement to the reusability and flexibility of distributed geoscience analysis and decision making. the duty of the service registry center is to provide services for gs registration and lookup. after the gs provider releases the gss to the registry center via catalog service for the web (csw) interfaces, the gss can be searched via the internet and then invoked via urls as a web service."
"numerous gss appear in distributed environments; thus, a mechanism is needed to assist domain experts with accurately and efficiently finding the required gss from a large set of available gss. the service management mechanism is the facility that guarantees the integration of the distributed geoscience algorithms. the registration and discovery of gss can be implemented by establishing a registry center for the services. distributed gss can be divided into four categories: portrayal service, data service, processing service, and registration service. the portrayal service is used to depict the visualization of geographic information that is presented to the user. the data service (e.g., wfs, wcs) is responsible for providing the spatial data using a service interface. the processing service (e.g., wps) provides spatial data analysis functions to achieve value-added information. the registration service records the above three services. in this paper, a gs classification system is designed, as shown in table 1 . table 1 shows the general classification of the geoscience data services and processing services, which can be effectively used to manage the services and facilitate the registration and lookup of the services. the classification system provides strong support for gs registration and discovery in distributed geoscience algorithm service integration. distributed gss and gs management mechanisms constitute the foundation of distributed gs integration."
"acquire the result. a url link is returned after a complete process is executed by the bpel engine, and the service user can obtain the results of the geoprocessing through the url link. i. search the system model base and determine whether there is a predefined integrated model; if yes, then skip to step iii; if no, go to step ii. ii."
"one additional difference compared with previous work is that training was biased toward the tissue-specific events (by construction of the minibatches), thereby promoting the model to learn a good representation about cellular context. we were able to get some small performance gains (within 2 standard deviations) of $1-2% in auc lmh_tv and auc dvi using this methodology compared with training with all events treated equally. more importantly, biasing the training examples encourages the model to learn about the tissues as input, which has a significantly different meaning compared with the genomic features and make up only a small number of the input dimension. we find that without this learning bias, the model more frequently settles to a bad local minimum, or does not learn to use the tissues as input at all. together, all these changes allowed us to train a model that significantly improves on previous work."
"in general, as seen in figure 1, the sentiment analysis system in this study consists of three main stages, preprocessing and normalization, features expansion and classification. the first stage involves several steps including tokenization, stopwords removal, stemming and misspellings words normalization. in this stage, we also counduted negation convert. in the feature expansion stage, we use kateglo api to find synonym of each word in the review texts. then, the synonym will be added to the original texts. finally, in the sentiment classification stage, naïve bayes is trained using some training data and the expanded review texts will be classified using naïve bayes and bag of words as its features."
"in addition, we performed hyperparameter search to optimize the dnn, where we gained considerable improvements over our original hand-tuned models, at $4.5% for the dni code and $3.5% for the lmh code. interestingly, the final set of hyperparameters found opt for a much larger ($4â) number of hidden units than our initial attempt (with matching hyperparameters). manually trying to find these hyperparameters would have been difficult, where a user may settle for a suboptimal set of hyperparameters owing to the substantial effort and time required for training a model with millions of parameters."
all of these prior studies provide excellent examples of standard-based geoscience data processing and they can be seen as the source of inspiration for this study.
"create a model, and submit it after completion. in this step, the user can build the integrated model according to section 3.3 and then submit it to the model base. iii."
"this experiment demonstrates that it is feasible to integrate distributed geoscience algorithms that are based on ogc web service specifications, which can help scientists to utilize distributed geoscience algorithms and data to solve geographic problems. all of the test schemes can obtain the river network extraction result, demonstrating that the proposed geoscience algorithm integration method works in various algorithms and data distribution situations, improving its applicability to the diverse conditions in the real world. to analyze the quality of the proposed method in different conditions, we collected the performance data of the different test schemes, as shown in figure 9 ."
"these geospatial service specifications have been widely adopted in geographical model building [cit] . by using web-based maps, geospatial data, geoprocessing services, and sensor web services, researchers can efficiently use geographic information resources to support spatial decision-making and geoscience applications [cit] . di demonstrated that the framework based on the ogc web service (ows) facilitates interoperability between earth observation data and geoprocessing modeling [cit] . specialists have also conducted geographic processing research that is based on grid technology and a cloud computing environment [cit] . wps specifications are utilized to process geoscience data on different computing backends and platforms [cit] . a new method of flexible service chaining using the standard business process markup notation (bpmn) has been proposed to access a centralized repository of processes and services to form a reusable workflow [cit] . nativi developed the geo model web initiative of environmental model access and interoperability, in which the basic principles and technical challenges of implementing a model web are revealed [cit] . castronova designed a generic openmi-component that wraps ogc wps modeling services, and the model services can be leveraged and reused within multiple workflow environments and decision support systems; this approach can advance the work in soas for environmental modeling [cit] . to address the emerging issue of integrating data sharing and computing e-infrastructures for multidisciplinary applications, a business process broker (bpb) was designed to take a formal description of a scientific business process and translate it in an executable process, and this method has been applied in satellite image mosaicking [cit] ."
"simulation results in fig 1 shows that d-rpl and mrpl adapt to the high mobility and provide reasonable results of around 78% and 68% pdr respectively. while the native rpl fails to catch up and provide only 35% average pdr. mrpl was designed on the assumption that there is always a static node in range of every mobile node, and although it responds to inconsistencies quicker than d-rpl it still relies on the presence of static nodes in range and thus generates extra overhead and makes unwanted hand-overs that lead to packet loss. rpl was originally designed for static networks and thus it has low responsiveness to topology changes. d-rpl provides a higher routing efficiency and a more reliable solution."
"the four different approaches are compared and analyzed using different data volumes and different network bandwidths (e.g., 1 mbps, 5 mbps, 10 mbps, and 20 mbps). the sizes of the datasets are shown in table 2 ."
"with regards to training the two tasks jointly, we found that with hyperparameter tuning, the performance of the model when each task was trained separately compared with being trained together was not statistically different. this is likely because both tasks are too similar for any transfer learning to take place, as evident by the similarity in performance in the dni code between the dnn and dnn-mlr models. nevertheless, we find that training both codes together stabilizes learning, specifically, training becomes more tolerant to a larger range of hyperparameters leading to reduced variance between models."
"as shown in figure 4, the values are 1, 2, 4, 8, 16, 32, 64, and 128 in each direction. 3. calculating flow accumulation: to form a river network by rainwater, each grid is given a water drop. the flow calculation creates a grid for each water droplet accumulated by each pixel. 4. thresholding flow accumulation: the threshold of the number of water droplets is calculated while considering that the number of water droplets in a river network pixel is greater than the threshold value. the binarization algorithm is used to set the values of the pixels in the river network to 1 and the other values to 0."
"in order to detect pertinent features with their respective scales, we first need a measure of the variation of the shape along the scale axis. a first option would be to directly consider the magnitude of the displacement vectors d l coming from the multi-scale decomposition. this measure is ex-tremely local and significantly favors inflection points which will always exhibit small displacements. indeed, intuitively other important aspects are the changes in the tangent frame orientation and changes of curvature. these three quantities are well captured by the geometric variation ν as defined by the gls framework [mgb * 12]:"
"this experiment demonstrates that it is feasible to integrate distributed geoscience algorithms that are based on ogc web service specifications, which can help scientists to utilize distributed geoscience algorithms and data to solve geographic problems. all of the test schemes can obtain the river network extraction result, demonstrating that the proposed geoscience algorithm integration method works in various algorithms and data distribution situations, improving its applicability to the diverse conditions in the real world. to analyze the quality of the proposed method in different conditions, we collected the performance data of the different test schemes, as shown in figure 9 . figure 9a shows that as the dataset size increases, the execution time that is required for each test also increases because larger input datasets require additional processing time. however, the rate of time increase in test 3 is significantly higher than the rates in the other two tests, particularly when the network bandwidth is only 1 mbps because the data and all algorithms are on different nodes; figure 9a shows that as the dataset size increases, the execution time that is required for each test also increases because larger input datasets require additional processing time. however, the rate of time increase in test 3 is significantly higher than the rates in the other two tests, particularly when the network bandwidth is only 1 mbps because the data and all algorithms are on different nodes; therefore, the volume of data transferred in the distributed environment in test 3 is larger than that in tests 1 and 2. thus, when the distributed data sizes are large, the transmission of data between nodes that are connected via a low-speed network can be a time-consuming task that will increase the execution time of the integrated geoscience model. this result demonstrates that the quality of distributed geoscience algorithm integration can be affected by the size of the processed data, the amount of distributed data transferred, and the speed of the network. figure 9b shows the performance of the experiment around the world, which indicates that the experiment can obtain very similar performance to that of the experiment within china. thus, the proposed method can be applied not only in one country, but also around the world with similar performance."
"in the second stage of training, the weights from the input layer to the first hidden layer (learned from the autoencoder) are fixed, and 10 additional inputs corresponding to tissues are appended. a one-hot encoding i123 representation is used, such that specifying a tissue for a particular training example can take the form [0 1 0 0 0] to denote the second tissue out of five possible types. we have two such inputs totaling 10 variables that specify tissue types. the reduced feature set and tissue variables become input into the second hidden layer. the weights connected to this and the final hidden layer of the dnn are then trained together in a supervised manner, with targets being psi and \"psi. after training these final two layers, weights from all layers of the dnn were fine-tuned by backpropagation."
"countries and organizations have conducted geospatial information service projects, such as the u.s. earthcube program by the national science foundation (nsf), the infrastructure for spatial information in europe (inspire), and china's tianditu. geoscience services that are based on distributed information infrastructures have been developed in recent years, including spatial data and information infrastructure, e-science, and cyberinfrastructure [cit] . the goal is to improve the access, sharing, visualization, and analysis of all forms of geoscience data and related resources. in recent years, the virtual geographic environment was introduced as a new geoscience algorithm sharing and interaction technology, and it plays an important role in the managing and sharing of geographical knowledge and multiscale environmental change monitoring applications [5, [cit] . commercial organizations and enterprises have built cloud-based geocomputation services for spatial analysis, mapping, and spatial processing [cit] ."
"ensemble methods are a class of algorithms that are popular owing to their generally good performance [cit], and are often used in the life sciences [cit] . the strength of ensemble methods comes from combining the predictions of many models. random forests is an example, as is the bayesian model averaging method previously used to model the regulation of splicing. recently, neural network learning has been improved using a technique called dropout, which makes neural networks behave like an ensemble method [cit] . dropout works by randomly removing hidden neurons during the presentation of each training example. the outcome is that instead of training a single model with n hidden variables, it approximates the training of 2 n different networks, each on a different subset of the training data. it is described as an 'extreme form of bagging' and is a computationally efficient way of doing model averaging [cit] ."
"the models are further evaluated on predicting whether there is a difference in splicing patterns for all tissues, without specifying the direction. auc change is computed on all exons and tissue pairs. this is shown in table 2b . the results indicate that this is a less demanding task, as the models can potentially use just the genomic features to determine whether an exon will have tissue variability. the difference in performance between all methods is less compared with auc dvi . however, as the evaluation is over all pairs of tissues, the dnn, which has access to the tissue types in the input, does significantly better. although this is also true for the mlr, it still performed worst overall. this suggests that in the proposed architecture where tissue types are given as an input, the mlr lacks the capacity to learn a representation that can jointly use tissue types and genomic features to make predictions that are tissue-specific. both results from table 2 show that there is an advantage to learning a dni code rather than just learning the lmh code."
"both the lmh and dni codes are trained jointly, reusing the same hidden representations learned by the model. for the lmh code, two softmax classification outputs predict the psi for each of the two tissues that are given as input into the dnn. a third softmax classification function predicts the \"psi for the two tissues. we note that two psi predictions are included in the model's output so we have a complete set of predictions that use the full input features. the total cost of the model used during optimization is the sum of the cross-entropy functions (4) for both prediction tasks."
"another performance boost came from the use of dropout, which contributed $1-6% improvement in the lmh code for different tissues, and $2-7% in the dni code, compared with without. the performance difference would likely be larger if hyperparameter optimization were not performed on the model that did not use dropout. we note also that even with dropout, a small l1 weight penalty was found to be beneficial, which may explain our model's tolerance for a large number of hidden units with sparse weights."
"the implementation and simulation of d-rpl is done using contiki operating system 3.0 [cit], with cooja [cit] wsn simulator. cooja has a mature and reliable implementation of rpl and although it does not normally support node mobility, it can import the coordinates of nodes through a mobility plugin to represent mobile nodes. mobility scenarios are generated using bonnmotion [cit], a free and widely used mobility scenario generation tool."
"execute the integrated model. during the execution of the integrated geoscience model, the model integration module will send the xml document to the model execution engine, which finishes execution via the bpel engine. v."
"this experiment demonstrates that it is feasible to integrate distributed geoscience algorithms that are based on ogc web service specifications, which can help scientists to utilize distributed geoscience algorithms and data to solve geographic problems. all of the test schemes can obtain the river network extraction result, demonstrating that the proposed geoscience algorithm integration method works in various algorithms and data distribution situations, improving its applicability to the diverse conditions in the real world. to analyze the quality of the proposed method in different conditions, we collected the performance data of the different test schemes, as shown in figure 9 . figure 9a shows that as the dataset size increases, the execution time that is required for each test also increases because larger input datasets require additional processing time. however, the rate of time increase in test 3 is significantly higher than the rates in the other two tests, particularly when the network bandwidth is only 1 mbps because the data and all algorithms are on different nodes;"
"naïve bayes is one of the most effective and efficient inductive learning algorithms for machine learning and data mining. the performance of naive bayes is competitive in the classification process although it uses the assumption of attribute independence. the assumption of the independence of these attributes on the data is rare, but although the assumption of attribute independence is violated, the performance of naive bayes classification is quite high, as proven by various empirical studies [cit] ."
"next, we look at how well the different methods can predict \"psi between two tissues, where it must determine the direction of change. this is shown in table 2. as described above, \"psi predictions for the bnn were made by training a mlr classifier on the lmh outputs (bnn-mlr). to make the comparison fair, we included the performance of the dnn in making \"psi predictions by also using a mlr classifier (dnn-mlr) on the lmh outputs. finally, we evaluated the \"psi predictions directly from the dni code, as well as the mlr baseline method, where the inputs include the tissue types. i125 table 2a shows the auc dvi for classifying decrease versus increase inclusion for all pairs of tissue. both the dnn-mlr and dnn outperform the bnn-mlr by a good margin. comparing the dnn with dnn-mlr, the dnn shows some gain in differentiating brain and heart as patterns from other tissues. the performance of differentiating the remaining tissues (kidney, liver and testis) with each other is similar between the dnn and dnn-mlr. we note that the similarity between the dnn and dnn-mlr in terms of performance can be due to the use of soft labels for training. using mlr directly on the genomic features and tissue types performs rather poorly, where predictions are no better than random."
"the integrated model utilizes dem data as the input data and it returns the results of the river network in gml format after processing. the first three test schemes can execute the integrated geoscience model and obtain the results successfully. after the execution of the integrated geoscience algorithms, a gml-based result is produced. through parsing the gml results, the vector data result is created. in test 4, the model is executed on the arcgis platform, resulting in a shapefile format. the vector data are then overlapped with an esri web map, and the final results are shown in figure 8a -d, corresponding to the dem data sizes of 2 mb, 20 mb, 100 mb, and 500 mb, respectively. the vector lines are the extracted river network in the study area, reflecting the size of the data at different distribution ranges, and the background is an esri web map."
"once this local analysis has been carried out, each slider is mapped to a band of the multi-scale representation. the feature corresponding to each band is then either enhanced or reduced independently (figure 2e ), and the surface is reconstructed in real-time as detailed in section 5."
"create a model, and submit it after completion. in this step, the user can build the integrated model according to section 3.3 and then submit it to the model base. iii."
"the proposed dynamic objective function d-of utilizes the minimum rank with hysteresis objective function (mrhof) that is already available in contiki os, and it adds other metrics in the calculation of the path cost to the destination. these metrics include etx which is based on the expected number of transmissions required to send a packet from source to destination, the energy metric is used as the estimated energy required to send a packet to the destination and the link quality indicator (lqi) which is based on the rssi. the mrhof objective function defines a threshold for switching to a new parent, nodes only switch if the rank difference is more than 1. however, in d-of more than one metric is used to produce the cost and changing the threshold is necessary to minimize the number of unwanted hand-overs and improve the routing performance."
"test 3: full data transmission, and the data and all algorithms are on different cloud nodes. in this test situation, the data and geoscience algorithms are built by distributed users; this scenario tests the proposed geoscience algorithm integration method over a wide area and it can be used by the distributed users. 4."
"execute the integrated model. during the execution of the integrated geoscience model, the model integration module will send the xml document to the model execution engine, which finishes execution via the bpel engine. v."
"both the lmh and dni codes are trained together. because each of these two tasks might be learning at different rates, we allowed their learning rates to differ. this is to prevent one task from overfitting too soon and negatively affecting the performance of another task before the complete model is fully trained [cit] . this is implemented by having different learning rates for the weights between the connections of the last hidden layer and the softmax functions for each task."
"1. for each point of the roi, we analyze its geometric variation profile to extract a super-set of the possible pertinent scales which are weighted according to some heuristics ( figure 2c ). this step is detailed in section 4.2."
"filling pits: this step is used for data preprocessing. in the case of data errors, the original dem data will have noise and there will be pits. in the d8 flow direction algorithm, a part of the river network is broken down, which contradicts the rules of river formation. the small defects in the data are removed by filling the pits in the dem data, as shown in figure 3 . 2."
"1. test 1: no data transmission, and the data and processing methods are in the same cloud node; this scenario tests the proposed geoscience algorithm integration method in the single node and it can be used by the distributed users. 2. test 2: only partial data (i.e., dem data) are acquired by distributed transmission, and the other required data and processing methods are on one cloud node. this test scenario tests the execution of the proposed geoscience algorithm integration method between two organizations and can be used by the distributed users. 3. test 3: full data transmission, and the data and all algorithms are on different cloud nodes. in this test situation, the data and geoscience algorithms are built by distributed users; this scenario tests the proposed geoscience algorithm integration method over a wide area and it can be used by the distributed users. 4. test 4: building the same workflow via arcgis model builder and executing it on a single machine. the test is usually configured by the users of one organization or institute and it can only be used on a single machine."
"1) the quality of repair of the machine is influenced by existing wear and tear mechanisms on individual elements of wear and tear, proper diagnosis, technological processes leading to the restoration of damaged parts and reassembling the machine. 2) the primary quality repair index can be described analytically with the help of matrix arithmetic, in which the relationship between production capacity, structural and organizational factors, and primary factors is taken into account. 3) in the case of a need for more detail, the number of input and output variables can be increased or decreased, nota bene it is only then that the technological influence of factors on the limited number of quality indexes is interesting."
"in the experiment, all of the geoscience algorithms (e.g., filling pits, calculating flow direction, calculating flow accumulation, threshold flow accumulation, and data format conversion) are shared via the wps, and the dem data and the vector spatial data are shared via the wcs and wfs. the dem data from aster gdem v2 were selected for the experiment, and these data were developed jointly by the meti of japan and nasa of the united states. these data are accessible to the public and have a spatial resolution of 30 m. the dataset was provided by the geospatial data cloud site, computer network information center, and chinese academy of sciences (http://www.gscloud.cn). the dem data cover the southern part of the loess plateau of china. figure 6 shows the location of the study area. the area has a typical loess plateau landform and millions of gullies. in the above steps, all algorithm services and models are based on the ogc wps specification. therefore, the module has the advantages of convenient interactions and executions that are independent of the os and the execution platform."
"d-rpl shows that it adapts to mobility changes better than relevant rpl-based protocols, achieving more than 10% improvement to pdr with better end-to-end delay and better energy consumption compared to mrpl. simulation results also show the importance of the objective function and its impact on mobility management in rpl. the proposed objective function d-of complements the operation of d-rpl giving reliable performance and efficient routing mechanism."
"search the system model base and determine whether there is a predefined integrated model; if yes, then skip to step iii; if no, go to step ii. ii."
"calculating flow accumulation: to form a river network by rainwater, each grid is given a water drop. the flow calculation creates a grid for each water droplet accumulated by each pixel."
"all kinds of wear and tear can be divided into three groups [cit] : mechanical (abrasive wearmicrocutting, scratching), molecular (formation and destruction of adhesive joints), corrosive (oxidation, fretting). primary parameters determining wear and tear are [cit] : unit pressure and the speed of the relative dislocations, the mechanical characteristics of friction couples (structure, kind of friction, lubrication properties). the degree of wear and tear is determined using special computer programs, or is measured using various methods for example: micrometry, according to the content of wear and tear products, or using superficial activation. outward signs of the classical destructive process are [cit] : chipping (brittle and ductile), plastic deformation, creep, bending, change of material properties (structure, mechanical properties, chemical composition), corroded areas (corrosion, erosion, cavitations), formation of cracks, accretion, changing of properties of the superficial layer (roughness, hardness, flexibility), wear and tear (abrasion, crushing, material transfer), changing of contact conditions (contact surface, size of micro cavities), lubrication condition. the extent of wear and tear of the machine components during diagnosis according to one of the known methods is taken into account. other cases of failure (fractures, cracks, breakage, etc.) [cit] have been also considered. based on the results of analyses the machine components have been classified as: subjected to repair and subjected to change. when any part of a machine is partly worn (i.e. it still works), then this can also be reproduced. a diagram of mutual connections describing the basic index of machine repair (fig. 2)"
"here, τ, η and κ are directly obtained from the parameter u(x,t) of the best fitted algebraic sphere such that they approximate the distance from x to the surface s(t) as well as its normal and mean curvature respectively. the role of the scale factors is to guarantee a scale invariant measure. we refer to [mgb * 12] for the details on computing ν. by defining the discrete geometric variation of the point p"
"rpl is an ipv6 routing protocol designed by the ietf roll working group for low-power and lossy networks (lln), it operates on the ieee 802.15.4 standard using 6lowpan as an adaptation layer. rpl builds the topology of the network based on a directed acyclic graph (dag) with no outgoing edges so that no cycles can exist. every dag is routed towards one or more dag roots forming a destination-oriented dag (dodag) and every dodag has its own dodag-id. the dodag is built using the predefined objective function which contains the metrics for route selection. rpl maintains connectivity using a number of control messages, the dodag information object (dio) carries information including the dodag-id and the rank to allow other nodes to discover the dodag. the destination advertisement object (dao) contains the rpl instanced id that was learnt from the dio and it is sent from the child node to the parent node or the dodag root. the dodag information solicitation (dis) is used to request a dio from an rpl node. rpl uses the trickle timer [cit] to control the frequency at which it sends dio messages, this timer is responsible for setting the periodic timer that increases if the node's rank does not change over a threshold number of dio transmissions, the rank of the node is depicted based on the objective function of rpl. this timer is reset if the nodes rank changes or if an inconsistency is found. the main parameters of the trickle timer are i min, i doubling and i max ."
"the relu unit was used for all hidden units except for the first hidden layer, which used tanh units, based on empirical performance on validation data."
"in order to test the performance of d-rpl, we chose three metrics that reflect the efficiency of the network. these metrics are end-to-end delay, energy consumption and pdr. the endto-end delay represents the average time required for each node to successfully send a packet from source to destination. energy consumption represents the average amount of energy consumed to successfully transmit a packet from source to destination at each node during 60 minutes of simulation. pdr shows the percentage of delivered packets from each node compared to the total number of packets sent by the same node. in the simulation scenario, we assume that low-powered mobile nodes are attached to people, objects or herds of animals and thus we consider a maximum speed of 5m/s so that it can also be applied for other iot applications like smart cites and smart factory management."
"in the above steps, all algorithm services and models are based on the ogc wps specification. therefore, the module has the advantages of convenient interactions and executions that are independent of the os and the execution platform."
"the four different approaches are compared and analyzed using different data volumes and different network bandwidths (e.g., 1 mbps, 5 mbps, 10 mbps, and 20 mbps). the sizes of the datasets are shown in table 2 ."
the results in fig 4 show that rpl achieves around 42% pdr while mrpl and d-rpl achieve around 88% and 90% respectively in simulation and both practical tests. the lower density gives the objective function less options making the difference in performance of mrpl and d-rpl down to 2% only. higher node density increases the chance of collisions and leads to higher packet loss due to interference and congestion [cit] .
"we formulate splicing prediction as a classification problem with multiple classes. figure 1 shows the architecture of the dnn. the parameters of the model are summarized in section 2 of the supplementary material. the nodes of the neural network are fully connected, where each connection is parameterized by a real-valued weight . the dnn has multiple layers of non-linearity consisting of hidden units. the output activation a of each hidden unit v in layer l processes a sum of weighted outputs from the previous layer, using a non-linear function f:"
the extraction of the pertinent scales thus boils down to a problem of clustering for which we propose to employ the well known k-means algorithm. let us start by defining our objective function s k as:
"one mechanism of splicing regulation occurs at the level of the sequences of the transcript. the presence or absence of certain regulatory elements can influence which exons are kept, while others are removed, before a primary transcript is translated into proteins. computational models that take into account the combinatorial effects of these regulatory elements have been successful in predicting the outcome of splicing [cit] ."
"given a region of interest (roi) selected by the user (figure 2a), our main goal is to automatically generate an adaptive geometric equalizer where each slider controls an interval of scales defining a pertinent feature. we start by computing for each selected point p i its multi-scale geometric variation ν i (figure 2b) assuming the roi is homogeneous in terms of features and scales, the number and respective scales of these features are extracted using an unsupervised learning procedure (figure 2d ). this extraction process is described in section 4."
using the rssi-based reverse-trickle algorithm in d-rpl leads to similar responsiveness to mrpl in low density networks. including the objective function metrics improves the performance of d-rpl making it more efficient in highly dynamic scenarios. the optimization of the objective function to improve mobility management is essential to achieve higher network performance.
"we compare the splicing code's performance trained with the dnn with the previous bnn and additionally optimized a mlr classifier on the same task for a baseline comparison. a gpu was used to accelerate training of the dnn, which made it feasible to perform hyperparameter search to optimize prediction performance with cross validation."
"sentiment classification is a special kind of text classification problem with two class, positive and negative. since it is a text classification problem, any existing supervised learning method can be applied. in most cases, the use of statistical or machine learning techniques such as naive bayes, maximum entropy, and support vector machines has proven to be successful in this field [cit] . some previous works also use another supervised learning such as method decision tree and k-nearest neighbor (knn) to analyze sentiment within texts [cit] . those researches showed that standard machine learning methods using unigram (bag of words) as features perform very well in this field."
"calculating flow direction: after filling, the value of each center pixel is not smaller than the values of the eight pixels around it; thus, each water pixel will flow toward the pixels with lower values. this process is utilized to form the 8 flow directions. the grid flow is calculated by using the d8 algorithm to create the flow from each pixel toward the steepest downhill adjacent points. as shown in figure 4, the values are 1, 2, 4, 8, 16, 32, 64, and 128 in each direction."
"with large datasets, learning with mcmc methods can be slow and can be outperformed by stochastic optimization methods in practice [cit] . these algorithms process small subsets (minibatches) of data at each iteration, and update model parameters by taking small steps in the direction of the gradient to optimize the cost function. it is common to use stochastic gradient descent to train feedforward neural networks. the learning algorithm (backpropagation) is also conceptually simple, involving for the most part matrix multiplications, which makes them suitable for speedup using graphics processing units (gpu)."
"the proposed method, synonym based feature expansion, had been proven can improve the performance of sentiment analysis of short informal indonesian product reviews. based on the experiment, naïve bayes classifier that use feature expansion always have better classification accuracy compared to the ones that not using feature expansion. the best sentiment classification performance is obtained when the training data is 400 using feature expansion by accuraty of 98%. the number of training data also affect on the sentiment classification performance either with feature expansion, or without feature expansion. the more the training data, the higher the accuracy obtained. the highest accuracy difference between sentiment classifications using feature expansion and not using feature expansion occurs when training data used is minimal. this difference is decreasing along with the increasing number of training data. this result show that feature expansion will give bigger improvement in small training data than in the large number of training data."
"for the lmh code, as the same prediction target can be generated by different input configurations, and there are two lmh outputs, we compute the predictions for all input combinations containing the particular tissue and average them into a single prediction for testing. to assess the stability of the lmh predictions, we calculated the percentage of instances in which there is a prediction from one tissue input configuration that does not agree with another tissue input configuration in terms of class membership, for all exons and tissues. of all predictions, 91.0% agreed with each other, 4.2% have predictions that are in adjacent classes (i.e. low and medium, or medium and high), and 4.8% otherwise. of those predictions that agreed with each other, 85.9% correspond to the correct class label on test data, 51.2% for the predictions with adjacent classes and 53.8% for the remaining predictions. this information can be used to assess the confidence of the predicted class labels. note that predictions spanning adjacent classes may be indicative that the psi value is somewhere between the two classes, and the above analysis using hard class labels can underestimate the confidence of the model. table 1a reports auc lmh_all for psi predictions from the lmh code on all tissues and exons. the performance of the dnn in the low and high categories are comparable with the bnn, but excels at the medium category, with especially large gains in brain, heart and kidney. because a large portion of the exons exhibit low tissue variability (section 1 of supplementary material), evaluating the performance of the model on all exons may mask the performance gain of the dnn. this assumes that exons with high tissue variability are more difficult to predict, where a computational model must learn how as interprets genomic features differently in different cellular environments. to more carefully see the tissue specificity of the different methods, table 1b reports auc lmh_tv evaluated on the subset of events that exhibit large tissue variability. here, the dnn significantly outperforms the bnn in all categories and tissues. the improvement in tissue specificity is evident from the large gains in the medium category, where exons are more likely to have large tissue variability. in both comparisons, the mlr performed poorly compared with both the bnn and dnn."
"in the first method, to see which feature types are important to the model, we substituted genomic features to their median across all exons and looked at how the predictive performance changed. we divided the full feature set into 55 groups based on what they represent. the grouping, along with additional descriptions, can be found in section 4 of the supplementary material. here, the performance measure is defined as the sum of the three classes from auc lmh_all . the decrease in test performance (as a fraction of that obtained with the full feature set) when each group of features is substituted by their median is shown in figure 2 . feature groups that cause large decrease in performance are presumably important to the splicing code. the standard deviation is computed from the five trained models with random partitions of the data as described above. the order of the feature group toward the right of the plot should not be used to determine their order of importance owing to the small difference they make to the model relative to their standard deviations. it is interesting to see how small the decrease in auc is when each feature group is effectively removed. many features contain redundant information and therefore can compensate for missing features from other groups. for example, some of the motifs for splicing factors are represented in features representing n-mer counts. the most influential features describe the translatability of the exon, conservation scores and whether the alternative exon introduces a frame shift. the feature groups corresponding to counts of 3-mers and 5-mers are also important."
"to meet the requirements of distributed geoscience data and algorithm sharing, the international organization for standardization/technical committee (iso/tc 211) and the open geospatial consortium (ogc) formulated a series of geographical data services and processing service standards, including a web map service (wms), web feature service (wfs), web coverage service (wcs), and web processing service (wps), to standardize data transmission and processing interfaces."
"acquire the result. a url link is returned after a complete process is executed by the bpel engine, and the service user can obtain the results of the geoprocessing through the url link."
"rpl relies on the trickle timer in sending dio messages, if the network is stable this timer will increase exponentially to limit the number of control messages and keep a low overhead. when an inconsistency is discovered this timer is reset to i min in order to recover and repair the lost links. in d-rpl we add a control mechanism for the interval of the trickle timer based on the reception of data packets and control packets. upon receiving a packet from node n, nodes read the rrsi n and compare it to the last reading from the same node lastrssi n . if the new reading is lower by a redundancy constant k rssi it switches to the reverse-trickle setting and decreases the current interval to half until it reaches i min . it also sends a dis to all neighbours to assess the available options, otherwise it resumes the native rpl mechanism. this is based on the fact that a moving node is not necessarily going to leave its parent node and the decision on whether to switch to a new parent is left to the objective function. the trickle timer operation in d-rpl is defined in pseudo-code 1."
"we show results supporting that dnn with dropout can be a competitive algorithm for doing learning and prediction on biological datasets, with the advantage that they can be trained quickly, have enough capacity to model complex relationships and scale well with the number of hidden variables and volume of data, making them potentially highly suitable for 'omic' datasets."
"we now have all the ingredients to design an intuitive tool for the manipulation of surface features. in pre-process, the input point cloud or mesh vertices is augmented by the multiscale decomposition described in section 3. during the interactive editing session, the user starts by selecting a roi using a brush (figure 7a ). this roi is used to learn the scales of the pertinent features as in section 4 (figure 7b ). therefore, this roi do not have to correspond to the entire region the user wants to edit: it only has to be a good representative and it can thus be significantly smaller."
work would be to investigate whether our pertinent scale detection method could be adapted to the automatic selection of eigenfunctions basis in spectral geometry processing.
we used 25 mobile nodes and 1 static sink node in a 150m x 150m simulation area. these nodes move based on the random waypoint mobility model at 0-5 m/s with a maximum pause of 30s. the values of i min and i doubling are chosen to be 8 and 6 respectively giving a minimum interval of 256ms and a maximum interval of 16s as shown in table i .
"geospatial resources represent the geoprocessing tools, geospatial data, and computing and storage platforms. geoprocessing tools can be published as a wps. data resources include geographic vector and raster data and remote sensing data, which can be published as the wcs, wfs, and wms. the geospatial resources also include a physical geoprocessing server or grid/cloud computing resources. geospatial resources represent the geoprocessing tools, geospatial data, and computing and storage platforms. geoprocessing tools can be published as a wps. data resources include geographic vector and raster data and remote sensing data, which can be published as the wcs, wfs, and wms. the geospatial resources also include a physical geoprocessing server or grid/cloud computing resources."
"the dnn was implemented in python, making use of gnumpy for gpu-accelerated computation [cit] . the gpu used was a nvidia gtx titan. for the configuration with the optimal hyperparameters, the gpu provided $15-fold speedup over our original cpu implementation. this was crucial as otherwise hyperparameter optimization would not have been practical."
"short texts are sparse, noisy, and lack of context information. traditional text classification methods like machine learning may not be suitable for analyzing sentiment of short texts given all those difficulties. a common approach to overcome these problems for analyzing sentiment of short texts is to enrich the original texts with additional semantics to make it appear like a large document of text. then, traditional classification methods can be applied to it. some of the previous works employ search engines to extract more information [cit] . the other works utilize external sources such as wikipedia and wordnet as background knowledge [cit] ."
"dataset that was used in the experiment is mobile banking app reviews. there are 100 testing data consisting of 50 positive reviews and 50 negative reviews. meanwhile, the training data used varies from 50, 100, 400 to 1000 training data. this experiment was conducted to determine the effect of feature expansion and the number of training data on the sentiment classification performance. the evaluation method used in this experiment is accuracy. experiment results shown in figure 3 . as displayed in figure 3, the best sentiment classification accuracy is obtained when the training data is 400 using feature expansion by 98%. the experiment results using feature expansion on every number of training data always have better classification accuracy compared to the ones that not using feature expansion. this results show that feature expansion increase the sentiment classification performance. in short-text classification, many words in the testing data never appear in the training data. it can damage the sentiment classification performance. using feature expansion, the system append some new words, in this case the synonyms of each word in testing data, to the testing data before the classification stage. therefore, the vocabulary on the testing data will be richer and the probability of training 1349 data and testing data share the same words will increase. thus, sentiment classification process will produce better performance compared without using feature expansion. also from figure 3, can be seen that the number of training data does have effect on the sentiment classification performance either with feature expansion, or without feature expansion. the more the training data, the higher accuracy obtained. the highest accuracy difference between sentiment classifications using feature expansion and not using feature expansion occurs when training data used is minimal. this difference will be closer along with the increasing number of training data. this result show that feature expansion will have bigger influence in small training data. in the large number of training data, the word in testing data that will be expanded most likely has already appeared in the train data. hence, in this case, using feature expansion does not increase the sentiment classification performance significantly."
"where c j is the centroid of the j th cluster, and k is the number of clusters. minimizing s k by the k-means algorithm implies two major difficulties. firstly, we have to provide the number of clusters k which is in our case the unknown number of pertinent scales. secondly, the convergence of the algorithm and the quality of the result highly depends on the choice of the initial cluster positions."
"based on the test results of the experiment, we can conclude that the distribution of data and geoscience algorithms, the network capability, and the size of the processed dataset affect the efficiency of the integrated geoscience model. data transmission in a distributed environment is a great challenge that can impact the performance of the distributed integrated geoscience model, and this challenge will become more serious if a large volume of distributed data is involved. the experiment also shows that large datasets place great challenges on the processing capability of the distributed computing resources. with the increasing complexity and area of geoscience issues, these challenges for algorithm integrations will become more serious in the future."
"regarding the second issue, we adapted the k-means++ algorithm [av07] to take into account our weighted data. the centers are computed one after the other using the following procedure:"
"in figure 9, the execution time of test 4, which is the traditional method of geoscience algorithm integration and execution, shows that the performance of the traditional method is more stable than that of the proposed method. this is because all data and processing are hosted in one machine; hence, there is no data transmission involved. moreover, there is no distributed invoked web service in the model. all of these factors contribute to the rapid and stable execution of test 4. furthermore, when compared with the traditional process in a standalone environment, the proposed method has the characteristics of remote access, interoperability, and distributed storage of data and algorithms. moreover, by utilizing ogc specifications, the barriers of different data formats and interfaces can be removed. in contrast, the traditional method can only encapsulate the geoscience algorithms in an isolated environment and it cannot be accessed remotely via the internet, making it difficult to achieve distributed integration and interoperability."
point-wise analysis weighted least-square minimization [ggg08]. the influence of the neighbors of p i is controlled by the following compactly supported and scale dependent weight function:
"the geoscience algorithm integration module is responsible for finding and binding the predefined gss according to xml-based model script in the algorithm base, executing and monitoring the integrated algorithms, and returning the results to the user. the algorithm can be a single gs or a combination of multiple gss. the algorithm integration module executes the integrated geoscience model via the model execution engine."
"in the experiment, all of the geoscience algorithms (e.g., filling pits, calculating flow direction, calculating flow accumulation, threshold flow accumulation, and data format conversion) are shared via the wps, and the dem data and the vector spatial data are shared via the wcs and wfs. the dem data from aster gdem v2 were selected for the experiment, and these data were developed jointly by the meti of japan and nasa of the united states. these data are accessible to the public and have a spatial resolution of 30 m. the dataset was provided by the geospatial data cloud site, computer network information center, and chinese academy of sciences (http://www.gscloud.cn). the dem data cover the southern part of the loess plateau of china. figure 6 shows the location of the study area. the area has a typical loess plateau landform and millions of gullies."
it can be useful to see how the genomic features are used by the dnn to perform splicing pattern predictions. we analyzed our model in two different ways.
"we applied a threshold to exclude examples from training if the total number rna-seq junction reads is below 10. this removed 45.8% of the total number of training examples. we further define exons as having large tissue variability if \"psi ! ae0.15 for at least one tissue pair profiled. these exons make up 28.6% of the total number of remaining exons that have more than 10 junction reads. additional information about the read coverage of the dataset can be found in section 1 of the supplementary material."
"where: k -square matrix, whose elements are correlation moments kzjxj, w -quasidiagonal complex matrix, in which the square matrix is set along the main diagonal and the remaining elements are equal to 0, q -square matrix, whose elements are searching for the correlation coefficient. thus, for any correlation coefficient the eq. where: -matrix determinant w, n -the total number of observations, axj -algebraic complements of adequate elements kzjxi and the matrix determinant. considering the fact that we have a lot of calculations, they should be carried out using appropriate computer programs. to carry out an analysis of the degree of influence on the repair quality input variable, we can consider a simplified structural variant, which is shown in fig. 3 . it describes the mutual (influence) connection between structural, technological and organizational factors on reliability, durability and the own cost repair index, by the dispersion equation: (7.) where: dz1, dxi -dispersion of input and output variables accordingly, d*xi, -part of dispersion conditioned by the influence of factors which were not taken into account. the system correlation matrix and dispersion of input and output variables were determined based on the experimental data from the repair machines division of industrial enterprise. the matrices of input data dxi and output data dzi in a form of correlation coefficients kij and determinant coefficients describe the mutual connection between input factors and output factors shown in fig. 3 . the system correlation coefficient matrix was found as: (8.) to define the searched input value, the mean deviation of reliability was evaluated using the equation below: (9.) where: 4 ) causes a change of reliability dispersion accordingly of about +6.l5% and -6.2l% which can be seen as a linear relation presented in fig. 4 . fig. 4 changing of dispersion reliability to correlation coefficient dispersion (line 1) and structural coefficient k11 (line 2)"
test 4: building the same workflow via arcgis model builder and executing it on a single machine. the test is usually configured by the users of one organization or institute and it can only be used on a single machine.
"with the development of large and global geographical environment problems, fully and effectively coordinating globally distributed domain experts and fully utilizing distributed knowledge, technologies so that all users can access these distributed geoscience algorithms quickly and easily, or even integrate the distributed algorithms dynamically."
"to implement geoscience algorithm integration in the distributed environment, it is necessary to understand the algorithm architecture, and this paper presents the architecture of geoscience algorithm integration, as shown in figure 1 . the architecture consists of four main components: the geoscience service (gs) provider, gs registry center, algorithm integration module, and geospatial resources."
"actions. the therapists also discussed several ways in which the actions that the client uses to interact with the objects can be key to gaining insight about their psyche. the most commonly mentioned action from physical sandtray therapy was the idea of burying an object. this burying process can vary in meaning from client to client; it can imply things that the client wishes to hide or keep secret, or it can be a sign of aggression (e. g., suffocating). the ability for the therapist to observe this burying process is fundamental to the therapy session and was described as important to the therapist-client collaboration for which we are designing. our current prototype does not allow for this burying interaction, as our choice of physics engine does not provide this functionality."
"it is the estimation of result by grouping the numbers which are the easiest to be calculated mentally. in 32+48+54+18+69, the result of 32+69 is estimated to be 100; the result of 48+54 is estimated to be 100. adding 18 to the calculation, the result is estimated to be 218."
"during the sub-slices have two kinds of tree structure: different ip data packets generated a group splay tree, called the grouping tree, it is generated when the program is beginning, and it is extinct when the program is blackout. each ip packet has slices, and all slices generated a slice splay tree, called the slicing tree, and it will be generated when received the first sub-slices of the ip packet, and be deleted when finished the slice combining."
"the tcp session reconstruction use and combine a similar tree structure. the various sessions on the network organized a ordered binary tree, each node contains the both sides session connection state, serial numbers, some statistical information and the necessary historical information, such as the last serial number and the confirmed serial numbers. in every session, the packets both from the client and from the server need to organism two binary tree. and in the session node, it contains the point which is pointing to roots of the two binary trees. tcp session recorded all of the data packets, and recorded the tcp session state before or after the data packets coming."
"we used a cooperative design process [cit] with three sandtray therapists, who were involved throughout this research to provide us with expert domain knowledge. there were three phases of this design process: phase i involved initial face-to-face meetings with one sandtray therapist (t1), phase ii was an iterative distance collaboration (via phone and email) with two therapists (t1 & t2) throughout the design and implementation of the prototype, and phase iii was a face-to-face discussion with two therapists (t1 & t3) to provide feedback about the working prototype. the first phase involved two meetings, a pre-planning meeting and a follow-up meeting. at the former meeting, the idea of implementing a digital sandtray had not yet been conceived. this meeting was a demonstration of existing technology to t1, which triggered a discussion about how a digital tabletop sandtray might provide a solution to some of the comfort problems some clients feel with traditional sandtrays. the second meeting was to plan how we could collaboratively design our prototype at a distance, as the therapists were in a city approximately 630 km from our research lab. the second phase in our design process involved extensive discussion via phone and email with both t1 and t2, who described in detail what they felt were the essential elements of sandtray therapy. we include direct quotes from our email communication with t2 throughout the next section (phase i & ii: designing the virtual sandtray). we responded to this communication with descriptions of design ideas and questions about what made the elements important. these conversations were highly iterative and led to the design considerations that are described next. during design and development of the prototype, we maintained contact with both t1 and t2 to iterate and refine the design. when the prototype was finished, t1 and t3 joined us in a day-long workshop to use our prototype first-hand and provide feedback (phase iii: face-to-face feedback session)."
"our specific application domain, virtual sandtray therapy, is related to a number of approaches where modern touch and tangible technology is used to support work with children for storytelling or therapeutic purposes. early examples include the use of robotic stuffed animals [cit] to help young cardiac patients cope with their situation by encouraging them to talk about it, comparable to virtual sandtray therapy. later work employed an interactive physical play mat (storymat) to record children's storytelling activities [cit] developed a tangible tabletop game to support the therapy of children with cerebral palsy who need to train specific motor skills. the game combined tangible elements with a tabletop surface that was illuminated with coloured leds from below and was found to encourage children to train the desired therapeutic movements. [cit] describe an interface designed for children with asperger's syndrome. here, the fact that tabletop displays afford collaboration is used in a game form to allow four children simultaneously to train social skills and collaboration. similar to our own motivation, the authors name the children's affinity to technology as one of the criteria that makes tabletop technology well suited for such therapeutic applications."
"both the resize drawer and the painting drawer involve very explicit actions on the part of the client. the system's reaction to these actions (i. e., through the physics engine) makes the consequence of those actions available for interpretation by the therapist (dc4)."
"the following example use to illustrate the specific application of the protocol analysis. this is an intrusion detection system in the ethernet and was used to personal computers. first, according to the above data analysis section being protocol analysis, because it is a personal computer in the ethernet, so the system can only analyze the ip packets and some protocol type having relationship. such as if test the ping of death attack, we need to analyze the icmp data packets and tcp scans need too. each analysis process of protocol data packet is finished by the analysis machine, and in the machine, using pattern-matching algorithms to analyze the attack. as a detailed program flow chart is shown in figure 4 ."
"the core protocol in the network communication is tcp protocol and ip protocol, in the rfc-0791 [cit] and 0793 [cit] document, tcp packets and ip packet format are defined separately. due to the format is only related to the protocol, and was irrelevant to the network construction type, so protocol analysis has a broad applicability. next use the ethernet as an example."
"according to table 4, algorithmic calculation strategy had the longest solution time with 81 seconds for valid estimations, which was followed by adjusting-rearranging with 54 seconds, using matched number with 50 seconds and rounding strategy with 44 seconds. furthermore, algorithmic calculation strategy had the longest solution time in each operation. in this context, it can be argued that rulebased algorithmic calculation strategy decreases the fluency (accuracy + speed) of mental calculation skills in terms of rate of producing valid result and time allocated for estimation."
"previous studies have shown that individuals acquire the skill of consecutive counting at the ages of 2-3, but they do not comprehend the cardinal value of the number until they are about 5 years old (olkun, fidan & babacan-özer, 2013; [cit] . with the comprehension of number's cardinal value, 5-year-old children start to solve problems by using the strategies of one-to-one correspondence [cit], physical calculation (using fingers and objects) or counting on [cit] . altun, dönmez, i̇nan, taner and özdilek (2001) found that preschool students develop informal strategies for the solution to word problems to come up with the right answer. considering that 5-6-year-old children are not able to calculate on paper, it can be said that mental calculation skills are started to be used at early ages [cit] . when individuals start elementary school, rule-based written algorithms (calculation with paper-pencil) are added to their calculation strategies. [cit] asked a student, who subtracted by breaking up ten and achieve the right answer, why he/she did that operation, the student said \"i do not know; our teacher told us to do so.\" therefore, rule-based algorithms are regarded as obstacles in front of the development of advanced mathematical skills even though they lead to the right answer (baki & kartal; 2004, [cit] ) ."
"it was observed in the study that the elementary school students mostly used the rule-based algorithmic calculation strategy when calculating mentally. accordingly, teachers can lay emphasis on number sense strategies in in-class activities. programs for developing operational estimation strategies of students can be also developed, and the effectiveness of these programs can be tested. the studies affecting the relations between number sense strategies and problem-solving strategies can be performed."
"in this day-long session, the sandtray therapists participated in several activities: the therapists gave a demonstration of sandtray therapy to the designers using traditional physical figurines, the designers gave a demonstration of the virtual sandtray prototype, the therapists conducted a mock therapy session with the digital prototype, and all participated in a follow-up interview/brainstorming session."
"each figurine is shown as it would appear in the sandtray. however, the tabletop screen is not large enough to show all figurines at the same time with sufficient detail. we used a ring-shaped drawer similar to a 'lazy susan', only a small portion of which is visible at any given time (figure 4 ). this allows an unlimited number of figurines to be accessed. when a figurine is removed from the drawer by dragging (using the sticky fingers and opposable thumbs technique), a copy will remain behind, allowing for quick and intuitive duplication. figurines can be removed from the scene simply by putting them back into the drawer. this wide range of figurines is beneficial for therapist's observations, as the client has more choice about which figurines to pick. therapists can also observe the client's browsing and decision-making processes, potentially involving inclusion, exclusion, and/or copying of figurines."
"in turkey, although there is no exact emphasis on the number sense in elementary school curricula, the emphasis is laid on developing the skills of mental calculation and mental estimation which are the components of number sense [cit], mental calculation focuses on the exact result whereas mental estimation focuses on the approximate ones. we decide which strategy of mental calculation to use depending on the purpose of the calculation we will do. for instance, a cabin attendant working in a bus company that transport intercity passengers compares the number of ticketed passengers and the number of passengers present on the bus before the bus departs. here, the attendant must perform an exact count because even if a single passenger was missing, it would mean trouble both for that passenger and the cabin attendant. [cit] states that approximate counting is utilized where exact result is not needed, possible or logical when calculating. when a father with try 300 in his pocket wants to buy his two kids two coats each priced at try 143, he does not need the exact result to calculate whether he can afford them. it is not possible for an individual driving to calculate at what time he/she will be at home with the exact counting approach because there are uncertainties such as traffic intensity and malfunction, etc. if a principal who is trying to figure out how many buses will be needed to take a 330-student group to trip by 60-passenger buses per-forms an exact count, the result is 5.5 which is not realistic."
"we leverage some of this work by providing interaction techniques that allow the therapist to interpret the actions of the client. in our system, we opted for interaction techniques that explicitly convey the consequence of the action, as opposed to abstract gestures that must be learned. we also chose to use physically familiar feedback from the system as opposed to (for example) drastic global changes caused by subtle actions, such as a button press."
"some words used by the therapists to describe the virtual sandtray prototype were \"relaxing and pleasurable\", \"attractive\", and \"appealing\". both therapists commented that, in its current form, the prototype might perhaps already be usable for therapy. on the other hand, comments were made about the lack of sensory feedback: touch, sound, and even smell. however, the application was described as being \"still quite tactile\". an interesting point raised was that the virtual sandtray does not so much invite storytelling, but rather the construction of a static scene. in that light, it might be more related to art therapy, for example the making of a collage."
"mock therapy. following the demonstration and a lunch break, with the warning from the therapists that \"some therapy might happen\", a designer played the role of the client in a mock therapy session with a therapist, as the other three participants (one therapist and two designers) observed. this session lasted about an hour and was also videotaped."
"also the temporary nature of the sand was described by the therapists as being important. when a client sees a box of sand, they immediately recognize that whatever they create in that sandbox can be easily erased with a simple swipe of the hand. a digital display is in this way similar to a physical sandbox, because the pixels drawn on the screen are also in some sense temporary. by clearing the screen or turning off the monitor, whatever the client creates can be easily erased."
"although not used to tell a story, piper and hollan [cit] describe the design of an interactive table used to facilitate communication between a doctor and a deaf patient. this design process has many similarities to our own and shows the benefit of tabletop technology for the deaf community. as would be expected, our work reinforces the idea that working closely with the people who professionally understand the application area can lead to successful tabletop applications."
"construction. the therapists frequently identified the ability to construct as an important aspect of the client's interaction with the sandtray. construction is important because it \"stimulates imagination\", and in stimulating imagination, the therapist can better access the client's psyche. several different forms of construction became apparent in our sessions."
"when estimating the result of division, the adjusting-rearranging strategy was found to be used at 3.85% (5), and 100% (3) of the estimations were valid. the following is a student interview regarding a valid estimation using the adjusting-rearranging strategy: it was observed in the interview that the student did the solution in two stages, first divided 600 by 6 and then 25 by 6 to rearrange the result and finally added 100 and 4 which were the results of these two operations. when estimating the result of division, the rounding strategy was found to be used at 3.85% (5), and 100% (5) of the estimations were valid. the following is a student interview regarding a valid estimation using the rounding strategy:"
"activities in recollection and memorization levels of mathematics are called factual knowledge; procedural knowledge refers to the solution achieved via strategies automatized as a result of constant repetitions and verified in each repetition; and conceptual knowledge is defined as the development of new strategies by integrating the first-encountered situations with existing knowledge in the memory [cit] . [cit] explained the interrelationship of these knowledge types by the solution process of the 9+8 operation. a student trying to calculate it mentally can count on 9 with fingers to find the answer 17; as the student found the answer with the strategy of counting on what is available in the memory, he/she would end up using the procedural knowledge. however, the student would end up using the factual knowledge if he/she memorized this result and directly gave the answer 17 without counting fingers when the question was asked again. thinking that the solution would take too long, the student might achieve the result 17 by adding 1 to 9 and subtracting 1 from 8 to transform the problem in to 10+7 even though he/she knows the finger count strategy. thus, he/she would developed a new strategy called \"complementing to 10\"; since attainment of the new strategy requires flexible use of numbers, it is associated with number sense and an example of conceptual knowledge. with the use of newly developed strategy in similar questions repetitively, the rate of calculation will increase, improving the procedural knowledge. [cit] emphasizes that every type of knowledge has a critical importance to mental calculation skills. for example, it may be regarded negatively when the student automatically gives the answer 17 as it refers to a memorized knowledge; however, it is expected from students to leave the physical calculation strategies behind and directly do a memory-based calculation in one-digit numbers for calculation fluency [cit] . [cit] underlined that as it would reduce the operation fluency to solve 7x8 by 8+8+8+8+8+8+8 all the time, it is necessary to memorize the multiplication table. previous studies showed that students with learning difficulty scored lower than normally-developing students in the mental calculation tests with time limitation [cit] . in the longitudinal studies, it was observed that the lack of factual knowledge affects mathematical skills more negatively over years [cit] ."
"figurines. a class of objects that was missing were arbitrary objects that could be brought in by the therapist or the client and play a more metaphorical role. a small cardboard box could serve as a house, a stick could be used as a sword, or a pine cone could represent a baby, covered by a handkerchief to represent a blanket. the inability to bring such objects in could be found to be limiting if many therapy sessions are performed with the same, limited collection of figurines. it was suggested that a possibility should be added to draw or otherwise create one's own figurines, but this would be difficult to implement in an intuitive fashion, but might be possible through teddy [cit] or shapeshop [cit] . an alternative would be to use a device such as the microsoft surface 3 capable of recognizing physical objects and have the physical props interact with the virtual ones in our prototype."
"according to the interview, the student started the solution from left but used the 1 in the 12 twice and did not include the 1 in the 12 and the 5 in the 15 in the operation. the least commonly used strategy in multiplication was rounding at 4.62% (6) and 100% (6) of the estimations were valid. the following is a student interview regarding a valid estimation using the rounding strategy: according to the interview, the student calculated the problem mentally as if he/she were calculating with paper-pencil. it was observed that 41.49% (43) of the estimations using the algorithmic calculation strategy in division were invalid. the following is a student interview regarding an invalid estimation using this strategy:"
"the other part of the painting system is a drawer containing buckets of paint. a hose running from the drawer to the nozzle serves as a visual cue that they are related. a bucket can be selected by touching it. as long as the bucket is touched and the hose nozzle is in use, the texture paint it contains will flow through the hose and out the nozzle. usually, the dominant hand is used to move the nozzle, while the non-dominant hand controls the paint selection. paint will flow from the nozzle when both the nozzle and a paint bucket are being touched simultaneously. the nozzle can be moved around without painting by releasing the bucket."
"the procedure of pattern matching is as follows [cit] : 1) comparison from the network packet headers with attack signatures; 2) if the comparison result is the same, then detect a possible attack; 3) if the comparison results are different, restart the comparison from the next position of the network packet; 4) until all the bytes of certain network packet has been matched completely, a attack signature matching ends; 5) for each attack signature, repeat from 1; 6) until every signature matching has been completed, the procedure of packet matching ends."
"dc3: exploring simple digital extensions -much of the design discussion with the therapists considered which aspects of digital magic might make useful enhancements. in the real world, it is not possible to instantly duplicate objects. in the digital world, it is trivial. being able to add multiple copies of the same figurine allows for the creation of forests, herds and families, with little or no cost in terms of interface complexity. the sandtray therapists described this feature as being particularly worthwhile."
"repurposeable virtual objects. the therapists felt that the interaction with many individual 3d objects on an interactive table was both meaningful and usable. the therapists illustrated this during mock therapy showing how sometimes virtual objects were recognized as themselves (e. g., a rock is a rock), as well as metaphors, symbols, or archetypes (i. e., representations of things from a person's mind-for instance a rock could represent a member of one's family). this indicated that, in our system, virtual objects could be interpreted by an observer as more than just digital representations of data or information."
"interview and brainstorming. for the remainder of the day (around 4 hours, including breaks), the two therapists and three designers participated in an interview and brainstorming session. the interview began with several planned questions, and continued with an unstructured discussion of the benefits, limitations, and next steps for future designs of digital sandtray therapy. designers again took written notes and this part was not videotaped."
"we implemented our prototype using the smart table 1, which was specifically designed for children. its small form factor ensures that all corners of the table can be reached by a child, and its sturdy design makes it suitable for use in a practical setting. moreover, it uses frustrated total internal reflection (ftir) [cit] and can detect up to 40 simultaneous touches, enabling interaction through multiple fingers for multiple people at once. the direct nature of multitouch technology supports awareness by the therapist of the client's interactions (dc4). these factors make the smart table an ideal choice of hardware."
"it is stated that we mostly solve the problems which require calculation in our daily lives mentally [cit], yazgan, bintaş & [cit], gülbağcı-dede & şengül, 2016 . [cit] state that mental calculation is divided into two groups of exact counting and approximate counting also called operational estimation."
"while this is an interesting application for tabletops, there are also particularly interesting and challenging research issues. the therapists we worked with explained how in the physical world they have become sensitive over time to understanding the possible implications of the active (manipulation of physical objects) part of the sandtray work. we were particularly interested in whether this professional skill would hold for virtual 3d interaction. in particular, is the virtual medium rich enough for a scene to be constructed that the therapists can understand, or are viewing issues, such as the need to project onto a 2d screen, too limiting? can a virtual object take on a variety of meanings so as to enable the telling of a story to the therapists, or will they be interpreted as mere data or information? are the interactions on a virtual table rich enough to convey meaning to the therapist about a client's psyche, or is the disconnect between a person's actions and the surface's reaction too great? our more general goal was to discover whether the therapists felt that they could effectively perform therapy with this digital sandtray or some future design iteration."
"storytelling. another important aspect of the sandtray interaction was the ability for both the therapist and client to \"tell a story\". this storytelling process might be brought about in a variety of ways. for instance, the client might be encouraged to just play in the sandtray, and then the therapist might ask the client to explain the scene or elaborate on a specific object and discuss what it means to them."
"there has been rapid development and comprehensive application since sandeep kumar proposed a new method to detect attacks with pattern matching in his doctoral dissertation [cit] . however with the development of broadband access networks, network traffic has been growing. meanwhile, hacking techniques has also been developing rapidly, the traditional pattern-matching method can not adapt to the new demands for intrusion detection. the disadvantage of this detection method is in that it regards the network packets as the disorder random bytes stream, and it does not analyze the internal fields of the packets. this method mechanically matches the data packets transmitted in the network one by one, regardless of the audio packets or the image packets."
"according to the definition of internet frame structure, at the thirteenth of the internet frame,it includes a protocol identification of two bytes at the third layer.800 is the ip protocol, 0806 is the arp protocol,8138 isthe novell protocol and so on. in the definition of ip packet format, the tenth bytes are the fourth layer protocol identification, such as: tcp is 06, udp is 11, icmp is 01 and so on. but the third and forth bytes in the tcp packet is the application layer protocol identification (port number), such as 80 is a http protocol, 21 is a ftp protocol, 23 is a telnet protocol and so on. based on the above characteristics, the protocol analysis algorithm can be seen as a protocol tree, as shown in figure 3 . it can be seen the value is 0800, so the data packets is ip packets. according to the ip packet format, the 24th bytes of ethernet packets are fourth-layer protocol identification bytes. well then skip the first 15 bytes and read the 24th bytes of value immediately. af7*hy289s820800b9v5yt$0611tbhk76500801293ugdb2%00397e3912345678901234567890123 456789012345678901234567890123456"
"our in-depth discussions with the therapists revealed that it is common for them to constantly be collecting artifacts to use in therapy sessions from the environment (e. g., sticks, leaves, plastic cups, etc.). for example, one could consider our first meeting with t1 to be an example of her 'collecting' our technology. more generally, their practice frequently involves the evaluation of the suitability of tools and techniques for use in therapy. this skill is learned over time and does not directly involve their clients. we thus focused our research on the therapists; our research asks whether, in the virtual world, therapists can still interpret a person's actions in a meaningful way to perform therapy. we therefore decided to only include therapists (and not clients) in our design process. this decision has the consequence that our findings should be interpreted with a therapist-focused lens. we did, however, include a mock therapy session in our daylong workshop, where one of the designers played the role of a client. indeed, part of the training for students learning to do sandtray therapy involves participating in a session as a client themselves. the therapists in our study explained that these mock therapy sessions were necessary in order to better understand the experience for the client. thus, it seemed particularly appropriate for our own understanding to undergo a similar experience.we rely on previous studies [cit] to validate the usability of the interaction techniques used by the client."
"the traditional network intrusion detection methods either didn't perform any protocol analysis (figure 2 a-method) or only analysis on the tcp/ip ( figure 2 in the b-method). for application-based intrusion detection method (figure 2 c-method) after tcp / ip analysis, it also performs the application protocol analysis, and removed part of the command and data parts in the application protocol data, then translate the command and pattern match to the application of data,thus to detect the intrusion detection. method a use pattern-matching technology in the ip packets matches the attack characters blindly. it will be likely to match the characteristics of ftp to http packets; due to not understand the application protocols, method b can only see the application protocol as no structural data bit stream and match blindly. thus, with the increment of pattern matching and network packet which is being matched, the efficiency of the traditional detection methods will show a linear decline and its time complexity is o(n), n is the sum of the mode and network packet length; however, due to tree-based protocol to analyze the detection, the application-based detection system with the model increases, its complexity is only o(log2n). apart form the capability improvements [cit] . compared with the traditional method of intrusion detection, the application-based intrusion detection method can also solve two problems exist in the current network intrusion detection system. false positive: such as the phf attack based on the cgi, and the attack is characterized by \"/ cgi-bin/phf\". the traditional inspection system think that there is attack exists whether where to find the model in the packets, in fact, the model only appears in the http protocol header of the url domain, and at this time its represents attack. since application-based detection methods can see the same content with the destination host, so you can detect these attacks easily."
"perhaps most important is the potential for interaction; sandtrays offer special interaction advantages. for example, other forms of media used in art therapy [cit], such as pencils, paints, and clay, result in the client creating a 'snapshot' (e. g., pencil drawings, paintings, sculptures) as an end result. in contrast, the temporary nature of the sand invites play and, therefore, the creation of a narrative. the process of creating a 'scene' containing several objects, in which the narrative can unfold, can be particularly informative to the therapist, who can often infer self-representation in one object and, from there, the relationships to other objects in the scene. thus, the client's interaction with the objects is of particular importance to the therapist, and the possibilities of multi-touch interaction make this application particularly suitable to adoption with tabletop display technology. the direct nature of touch input to tabletop displays affords observation of these interactions, and the fact that the display is digital makes the scene being created similarly temporary."
"one of the students thought that she applied all the rules in the operation correctly, but then reported that the solution was incorrect. she could not find the source of the error in the solution; however, she stated that the answer was not reasonable. the student was asked to calculate 110+40 mentally; she added 4 tens to 110 (120, 130, 140, 150 ) to achieve the answer and then added the 2 which she did not include in the operation to 150 and achieve the right answer."
"s22: there is one 6 in 6. 6 minus 6 is 0; as there is no 6 in 2, we look for 6 in 25. 6, 12, 18, 24; we subtract because there is 4. 25 minus 24 is 1, the result is 14."
"a key aspect of this storytelling is that the objects in the scene can take on a variety of meanings. on the one hand, an airplane can represent just an airplane (i. e., itself), but on the other hand, it could represent a more abstract idea in the client's mind, such as flight or a desire to escape from something. the therapist's experience with the virtual sandtray prototype led them to state that they could easily interpret these different meanings from the virtual objects."
the focus of our research differs from these storytelling and therapeutic tabletop applications. we focus on the therapists and their ability to interpret the actions being carried out on the tabletop display itself. we leverage existing literature that suggests the usability of our interaction techniques and ask whether these techniques are rich enough to be interpreted by the observing therapist.
2. what is the (valid/invalid) result production level of the strategies used by fourth-grade elementary students in making operational estimations?? 3. what is the distribution of the strategies used by fourth-grade elementary students in making operational estimation in terms of solution times?
"dc2: maintain the sandtray's characteristic as primarily an associative medium -with open media, such as paint and clay, the artist (client) has a sense of the meaning as coming from and being expressive of themselves. with a sandtray, meaning is primarily associated with the objects, and usually only one of the objects is the self representation. thus, to enable the development of associated meaning that allows the client to express their particular story, a broad range of objects or figurines is desirable. some therapists group the figurines they offer by category, which makes it easier to find related figurines; others prefer a completely random presentation in which all figurines are mixed, which can trigger more spontaneous associations. although a digital system could allow both options, we chose to use a random presentation to encourage free association. from a commercial library of 3d models, around 160 figurines of many different categories were selected for use in the virtual sandtray."
"\"with a sand tray it is rare that a client will ask for a specific object, precisely because inspiration tends to start with associations to the presented repertoire of objects. this puts the client in the position of immediately symbolizing and associating. while the (relatively) fixed nature of the objects limits the expressiveness of the work, the way that they are animated and placed becomes the client's means of articulating their own meanings regarding those objects.\" -t2"
"another real-world impossibility is resizing rigid objects (again, digitally trivial). the size of an object has significant psychological connotations: larger objects are perceived as more important, more powerful or more menacing. dc4: use multi-touch to facilitate interpretation in the therapist-client collaboration -the interactions between the client and the sandtray are the focus of the therapist's observations, and thus a key aspect of this collaboration is the awareness by the therapist of these interactions. furthermore, sandtray therapy is sometimes used for couples or families, who will cooperatively act out a story, and even a single client can ask the therapist to participate. we must also design for such multi-person scenarios."
"addition. when estimating the result of addition, the most used strategy was found to be algorithmic calculation at 62.31%, and 61.73% (50) of the estimations were valid. the following is a student interview regarding a valid estimation using the algorithmic calculation strategy: investigating the interview, the student started to estimate the result of addition from the units digit, kept the unit digit of the obtained result and the carry in mind and then added the tens digits respectively and finally added the last carry to find 200. 38.27% of the estimations using the algorithmic calculation strategy in addition were invalid. the following is a student interview regarding an invalid estimation using the algorithmic calculation strategy: it was observed in the interview that the student added the carry 2 which he/she reached by adding the units digits of the numbers to the tens digit, carried the 1 by adding the tens digits but forgot to add it to the hundreds digit."
"a second problem is that the 'sticky fingers' paradigm [cit] implicitly makes lifting an object very sensitive. especially 3 microsoft surface. http://www.microsoft.com/surface when the two fingers start close together, a small movement of the fingers will result in a large vertical motion. doubling the distance between the fingers will move the object twice as close to the virtual camera, which is quite a large distance. perhaps it is better to let go of the stickiness of one of the fingers. a more formal user study may be necessary to objectively determine which is better."
"another component of fluency is mental calculation speed [cit] . it was concluded in the research that the mean times of solutions using the number sense strategies such as rounding (44 sec), using matched numbers (50 sec) and adjusting-rearranging (54 sec) were lower than the rule-based algorithmic calculation (81 sec) strategies. the interviews also showed that the students using the algorithmic calculation strategy started mental calculation in addition, subtraction and multiplication from right and others using the number sense strategies started from left. [cit] stated that as starting mental calculation from left provides a holistic focus on numbers, it increases accuracy of answers and shortens the solution times. [cit] . previous studies showed that students with learning difficulty scored lower than normally-developing students in the mental calculation tests with time limitation [cit] . the longitudinal studies showed that longer calculation times affect mathematical skills in a negative way by years [cit] . furthermore, longer durations of calculation make conceptual development difficult as they may cause individual to spend the energy required for thinking in calculations [cit] . higher number of valid estimations and shorter mean solution times in the solutions using the number sense strategies in the research indicate that number sense strategies increase the mental calculation fluency. the research results coincide with the findings achieved in previous studies [cit] . then, it can be argued that students should be guided towards number sense strategies to support both operation fluency and conceptual development."
"when estimating the result of addition, the second most commonly used strategy was found to be adjusting-rearranging at 20.77%, and 81.48% (22) of the estimations were valid. the following is a student interview regarding a valid estimation using the adjusting-rearranging strategy: investigating the interview, starting from the 100s, the student constantly adjusted the solution and achieved the exact result in the end. 18.52% (5) of the estimations using the adjusting-rearranging strategy in addition were invalid. the following is a student interview regarding an invalid estimation using the adjusting-rearranging strategy:"
"protocol analysis technology employs a high degree of regularity of the network protocol to detect the presence of attacks quickly. this technique led to a significant reduction in the required calculations, even in high load network can also be completely detect all kinds of attacks, not only has a more detailed analysis but without packet loss. the advantages of protocol analysis include: the first byte location in the parsing command string url will be given to the parser; the detection of fragments attack and protocol acknowledge; the lower false alarm rates and high performance."
"grouping tree node includes source ip address, destination ip address, identification field and protocol field of ip packets, for an ip packet with a different slice these values are same. these four values as the key value to a group tree node. it's said that two ip data packets a and b, their relationship can be defined as: if the source ip address of a is bigger than b, then a is bigger than b, if less, then a is less than b; and if they are equal, then compare the destination ip address, identification field and protocol field continually, until you know which is bigger or equal to each other. fragmentation tree node includes the offset field and the data fields, and the offset is the key value."
"many figurines, such as human figures, will often be used in a standing position. because it is difficult or impossible to make figurines with a small base stand upright, we add invisible pedestals at the bottom of these objects. the pedestals are configured to collide only with the ground, and thus do not cause unexpected interactions with other figurines."
"vertical movement. several problems were noted in relation to vertical movement of figurines. first, with the current top-down projection, it is not clear that the object is actually moving up or down, instead of simply changing size. this confusion was strongly reflected in the terminology used while discussing this action; even though people know that the object is actually moving up and down in the scene, they often still talk about \"making it bigger\" and \"making it smaller\". this suggests a disconnect between the client's actions and the therapist's interpretation. this might partly be blamed on the fact that the shadow is cast directly below each object, which often causes the shadow to be partly or completely obscured. a second shadow, cast from the side, might improve interpretation of movement in z; a projection that is not strictly top-down could also help."
"3) recombination of tcp session some attacks draw on the loopholes of tcp protocol and the tcp session hijacking is one of the most typical. under this attack, an attacker fake packet which has some particular ip address and then send them to the other one who is communication with this ip. to detect the tcp session hjacking is difficult, because if you do not communicate with the real communication side, it is almost impossible to find the disguised communications side. however, through simulate the working principle of operating system protocol stack, deal with the tcp session, this can detect some possible session hjacking on some certain extent. through the session reconstruction, detection system can find the serial number errors and some shake hands message after the serial number is wrong. then determine the possible attacks. in addition, the attacker maybe send rst packet to the one which is being communicate, and force it to end the session. the normal close connection is one side to send fin, this force closed connection is a illegal state transitions. detection system can also found these illegal state transitions to determine the presence invasion."
"subtraction. when estimating the result of subtraction, the most commonly used strategy was found to be algorithmic calculation at 80%, and 87.50% (91) of the estimations were valid. the following is a student interview regarding a valid estimation using the algorithmic calculation strategy: according to the interview, the student did the solution as if he/she was calculating with paper-pencil. 12.50% (13) of the estimations using the algorithmic calculation strategy in subtraction were invalid. the following is a student interview regarding an invalid estimation using the algorithmic calculation strategy:"
"the connection state is important information for the tcp communication. making full use of the connection status can make detection more precise and efficient, and can also against the denial of service attacks which is aim at ids itself. for example, snort defined intrusion characteristics for an http header, and the tcp flags ack is 1 and the data load contains a string. but overlook an important fact:an ack data and it s position is 1, it is not necessarily to establish a connection [cit] ."
"we present the cooperative design of a virtual sandtray through three phases of design: initial face-to-face meetings, iterative remote collaborative design, and a face-to-face feedback session. we end with an in-depth discussion of the results of this collaboration and a description of how to make use of our findings beyond the digital sandtray."
"prototype demonstration. the designers provided an indepth demonstration of the digital sandtray prototype. we spent approximately one hour explaining the details of how to interact with our prototype and allowed both therapists to experience using the system. we discussed our design decisions, as well as several viable design alternatives. the therapists were encouraged to share their thoughts and comments. this part was videotaped."
"other actions that may be relevant to the therapy were made available through the combination of our interaction technique with the physics engine. in particular, the ability to knock over one object with another, the ability to place an object inside another, and the ability to toss an object across the screen or drop an object from high above are actions that a client may do and can help the therapist to understand what is going on in the client's mind when they create a scene."
"the ids based on pattern matching technology, the fundamental problem is in that it uses the data packets as a random byte stream. suppose the structures of packets are unknown. protocol analysis technique is different from the traditional pattern matching techniques. it may intelligently \"understand\" the protocol, and use the high regularity of network protocols to detect the presence of attacks quickly. so it contributes to enhancing the efficiency of intrusion detection. figure 1 shows a general structure of nids based on the packet analysis. firstly, the detectors capture all communication traffic, made the received data packets through protocol analysis, parsing out the various fields in the head of each protocol, and then enter the test engine for testing: in the engine testing, in accordance with signature-based or anomaly-based approach to analyze the data packets, if attack signature or anomaly event is detected, then a report is sent to the decision-making module. the decision-making module according to the serious level of the report to take action, and then notify the reaction module; and last the reaction module will make response to the malicious behaviors. common response methods may include sending a warning message to the console, sending the rst packets to reset the connection, and notifying firewalls or some other protection devices to isolate the attackers. as the base of network intrusion detection system, protocol analysis has three main roles: firstly, the protocol analysis provides input data for the detection engine and it is the basis of the detection engine. secondly, the protocol analysis analyzes the context, this can improve the effectiveness of detection, finally, the protocol analysis on the application layer may locate the string matching against a specific field, and it is also an effective method to improve the capability of string matching."
"arrangement. the arrangement of objects in the scene was also described as being highly important to the therapy process. in using our prototype, the therapists felt confident that clients would be able to easily and freely arrange objects in a way that would be useful for a therapy session. although we did not perform a formal evaluation of usability, we interpret this confidence together with previous formal studies [cit] as a sign that the interaction technique that we included in our prototype was sufficient for the type of object arrangements that the sandtray therapy requires. furthermore, the use of gravity through the physics engine and pedestals on the base of each figurine facilitated this arrangement process."
"operation it was observed in the interview that the student shifted the tens digit of 45 to the hundreds digit, its units digit to the tens digit. when estimating the result of subtraction, the rounding strategy was found to be used at 6.92% (9), and 100% (9) of the estimations were valid. the following is a student interview regarding a valid estimation using the rounding strategy:"
"in the next three sections, we describe in detail how we realize our design considerations in our implementation of the virtual sandtray. we adapt existing tabletop display techniques and technology to provide rich interactions so the therapist can easily observe and infer information about the client's psyche from their interaction. design principles (dc1-dc4) are indicated for each implementation detail."
"-t2 \"positioning objects-includes orientation and ability to push into sand. \"moving objects-sometimes includes momentum, especially when throwing objects and lifting/dropping them.\" -t2 \"*stacking objects* (small objects placed on top of larger ones, balanced stacks of rocks, animals in trees or on houses etc.)\" -t2 \"children love containment, frequently putting things under or within other things. another powerful metaphor\" -t2"
"fluent calculation skills are required in order to provide continuance of strategies gained through the number sense during mental calculation process [cit] . fluency is described as the awareness of mental calculation strategies, identification of the strategy out of these strategies to facilitate the right answer rapidly and the crosscheck of the achieved answer with another strategy [cit] . in some of the definitions, calculation fluency is referred to be a subcomponent of number sense [cit] . [cit] argues that understanding the relations between calculation fluency and number sense requires the examination of mathematical knowledge types (factual, procedural, conceptual)."
"deployable system. our system is an example of an application that has been identified as usable as-is by the therapists. our method of designing and developing this application in collaboration with sandtray therapists could be adopted for tabletop systems in other domains. specifically, our work serves as a case study that cooperative design [cit] may lead to successful interactive tabletop systems."
"there are two possible avenues to explore for features in a digital sandtray: those already offered by a physical sandtray, perhaps adapted for use on a tabletop, and new options that do not exist in the physical world, but are made possible by the virtual. our list of design considerations (dc1-dc4) contains some features from traditional sandtrays that the therapists thought were important to maintain and some digital features they thought it would be interesting to explore. in our bottom-up approach we started with nothing, adding features that are deemed valuable for therapy, until a sufficiently rich environment was created. our communication with sandtray experts was the main guide in determining the most worthwhile features. a secondary concern was the cost of a feature in terms of interface and interaction complexity."
"the data were collected in english and religious culture and ethics courses as students' teacher were on leave. the 20-question scale form was applied to 26 participant students with the semi-structured interview technique. in this technique, although the researcher has prepared the questions before starting the interview, it can be also ensured that answers are detailed with different questions depending on the direction of the interview (yıldırım & şimşek, 2016, [cit] . before the interview, the students had been guided with the instructions \"i want you to solve the problems on the paper in your mind. you do not need to find the exact result; just try to find the closest result.\" during the interview, where it could not be determined how the students did their operational estimations, they were asked \"why did you do this operation? do you explain how you found this result? can you detail how you did the operation?\" to explore the strategies they used for the solution. before the semi-structured interviews, the 20-question scale was handed to the students, and they were asked to solve the problems in the form without using paper-pencil, achieve results as close as possible to the exact result and solve the problems thinking out loud. other students except for the interviewee were not allowed in the room during the interviews so that the possible distractions for the interviewees can be eliminated and other interviewees could not see the questions. the students were individually informed of the research before the application and that it would not affect their mathematics grade anyway. the interviews were videotaped."
"we leverage these 3d interaction techniques together with the use of a physics engine to provide tabletop interaction that is interpretable by the therapist. while these techniques and their corresponding studies show the usability of 3d interaction on a table, our work shows that these same interaction techniques can also benefit the therapist observing these interactions. by striking a balance between the ability to precisely control the 3d object and having on-screen objects react physically to those precise actions, the actions of the client become rich enough for the therapist to interpret the 'language' of those actions from the client."
"there were many reasons for the request that triggered this research. sandtray therapy, often considered a type of art therapy [cit] because the therapy session involves the clients creating a scene out of available supplies, has particular characteristics which make it well suited to an interactive tabletop. these include factors of age stereotyping, the characteristics of the sand itself, and the types of interaction that are therapeutically beneficial. in terms of age stereotyping, the associations of sandtrays (or sandboxes) are with activities we did as young children. while this works well in establishing rapport with young children, it can pose problems with young teenagers and pre-teens to whom activities in a sandbox may seem just too uncool. this age group (10 to about 13 or 14) is a particularly difficult age group for therapists to reach and is also a particularly sensitive age group since so much is changing in their lives at these ages. one of the therapists we are working with suggested that the 'wow' factor of a digital table might prove a great bridging factor. also, some children, perhaps due to their own response to traumas, dislike the feeling of sand and refuse to play with it and may find a digital sandbox more to their liking."
"2) the application-based detection system is required to achieve a variety of application protocols, such as http, ftp, and smtp etc. the implementation method is the same as these application protocols. therefore, the drawback is that a heavy workload of one-time development, but because of the high rate of detection accuracy, it is more suitable for highspeed network environment."
"in addition to the high-level results regarding the efficacy of virtual objects as meaning-carrying artifacts, we gathered feedback about our current design that will help to inform future iterations. the therapists' feedback suggests that these improvements would facilitate interpreting a client's actions."
"you can see the port number is 0080, therefore it is a http protocol. based on this information, we know that from the 55th bytes is the url."
"the presentation of figurines in the drawer was identified as potentially problematic; once a figurine was selected, the lack of structure made it difficult to find related figurines. although the therapists who participated in the feedback session (t1 & t3) normally present the figurines in their therapy in an organized way (in contrast to t2), they did comment that the lack of ordering in the virtual sandtray prototype \"stimulates more random aspects of the psyche\". this difference in approach suggests that we should design for some choice on the part of the therapist in how the figurines are presented. we could add a way to configure whether the objects are randomly sorted or organized in some fashion. alternatively, a hybrid approach could start with a random presentation, but allow the client to easily find related figurines once a few have been selected."
"in this section, the findings obtained in the research are included. accordingly, answer to the first research question \"what are the strategies used by fourth-grade elementary students during mathematical operational estimation?\" was searched for, and the findings are presented in table 2 ."
"in the interview, the student said, \"...as there is no 6 in 2, we look for 6 in 25\" because he/she calculated on the rule basis but did not use the rule of adding 0 in quotient."
"communication through virtual objects. the therapists stated that the 3d interaction (sticky fingers and opposable thumbs [cit] ) would be sufficiently rich for therapeutic purposes. specifically, the therapists felt they would be able to gain insight about a client's psyche based on his or her interactions with a virtual object."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. a sandtray typically used for sandtray therapy. sandtray therapists typically observe clients creating a scene or \"playing\" in the sand to gain insight into their psyche (courtesy of kristina walter [cit] )."
"a third problem is that it is possible to move a figurine so that it becomes invisible. for example, a figurine can be pushed down right through a drawer from above, causing the figurine to become hidden underneath the drawer. while this 'feature' may be one way of achieving the burying action requested by the therapists, it may also be an unintended consequence of the client's actions, introducing ambiguity in how the therapist interprets the action. it might be better to keep a figurine always visible, by forcing it to be always above everything else or by using digital effects such as outlines, shadows or transparency."
"\" [t] he temporary and unfixed nature of the sand-tray pieces invites play and therefore the creation of narrative. most other media result in a 'snapshot' in which the narrative is implied but not played out.\" -t2"
"of central import to the sandtray therapists, throughout all three phases, was the issue of understanding the client's psyche. an essential component of the exercise of sandtray therapy is to give the therapist insight into what the client is thinking and feeling through how they interact with objects. while sandtray therapists typically use physical props to gain this insight, our design revealed that this process is also possible with virtual ones. instead of simply being a digital representation of some underlying data or model, the virtual objects in our system can take on symbolic meaning in the same way that physical ones do, to the level of providing access into the mind of someone interacting with them."
"dc1: maintain narrative potential -without characters, there can be no story; thus, the use of figurines to represent characters and objects is essential (figure 2) . it must be possible to add figurines when a new character or object is introduced, to move them around as the story progresses, and to remove them when their part is over. stacking objects, such as a balanced stack of rocks or an animal on a house, also has significant psychological connotations."
"physical demonstration. we began with a demonstration and instruction session from the perspective of the sandtray therapists. in this part, the sandtray therapists described how they performed sandtray therapy using physical figurines and instructed the designers about the theory, logistics, purpose, and essential components of the process of sandtray therapy. we asked questions whenever something was unclear and took written notes."
"the attack is unable to identify just from a data packet, it need to simulate the operating system fragmentation recombination during the protocol analysis and to detect the illegal slice. in addition, the common methods use to bypass the firewall to attack the fragments. it will be separate the full tcp header in a number of ip sub-chip and then some packet filtering firewall can not check according to the header information. this attack also needs to be combined fragments by the intrusion detection software and restore the packets true face."
"each data structure in the analysis machine contains the following information: protocol name, protocol, protocol code-named and the attack detection function of the protocol corresponding. protocol name is the only identification of this protocol. protocol code-named is for increasing the analysis speed. in order to improve the detection accuracy, you can add a protocol node in the tree and define it by yourself,as a way to analysis the data, such as in the http protocol, the request url can be included in the tree as a node, and then put the url as a child node in different ways."
"to enable some of the digital extensions (dc3) and to support the accessibility of a wide range of figurines (dc2), we introduce drawers and tools. each of these tools was designed to respond to the client's touches with the same interaction technique as the figurines, using the physics engine to impose constraints. we also designed the drawers to be able to slide in and out of view using a handle to save screen real estate. the drawers themselves can be tossed to quickly open and close them, and figurines will bounce around them in a natural way. three such drawers are available: one for selecting figurines, one for resizing them, and one for painting on the sandtray floor."
"it is understood from table 3 that rounding was the strategy with which the fourth-grade students were the most effective in producing valid estimations at 100%, and it was followed by the strategy of using matched numbers at 85.71% and the strategy of adjusting-rearranging at 82.69%. it was found that algorithmic calculation which was the most frequently used strategy by the fourth-grade students during operational estimation could produce valid estimations at 66.32%. in the light of these findings, algorithmic calculation's rate of producing valid estimations was 35% lower than rounding, 22% lower than using matched numbers, and 16% lower than adjusting-rearranging. [cit] were more effective than the mental algorithmic calculation like calculating with paper-pencil. next section of the research addresses the estimation strategies used by the fourthgrade elementary students in each of the four operations in detail."
"there were several repeating themes about how a client's interaction with objects can help the therapist gain some insight about what they are thinking or feeling. these themes include construction, storytelling, actions, and arrangement."
"multiplication. when estimating the result of multiplication, the most commonly used strategy was found to be algorithmic calculation at 81.54% (106), and 53.43% (55) of the estimations were valid. the following is a student interview regarding a valid estimation using the algorithmic calculation strategy:"
"to validate and iterate on our design, we performed a daylong session with two sandtray therapists together with three of the designers. neither therapist had any previous experience with digital tables. as previously stated, the focus of our research was to support the ability for therapists to recognize and interpret the actions of the client in a way that was valuable for understanding more about the child's psyche. we designed this face-to-face session to address the following questions:"
"by enabling these familiar physical interactions, we provide the client with a language for communicating to the therapist (perhaps subconsciously) through the virtual objects themselves (dc1). the therapist can then interpret what actions such as knocking over, tossing, and sweeping objects might mean about the client's psyche (dc4)."
"according to table 2, the fourth-grade elementary students majorly used the algorithmic calculation technique during operational estimation at 74.04% (383). it was observed that the fourth-grade elementary students used the strategies of adjusting-rearranging at 10.00% (52), rounding at 4.81% (25), using matched numbers at 2.69% (14) [cit] for the development of operational estimation skills while they did not prefer to use the strategies of using the frondend orders, grouping and distribution. considering the results on the basis of operations, the most commonly used strategy in all of the estimations in four operations was algorithmic calculation strategy at the rates varying between 62.31% (81) and 80.00% (104). in addition, the strategies of adjusting-rearranging (20.77%), using matched numbers (10.77%), and rounding (3.85%) [cit] were used respectively. [cit] that were used in subtraction were rounding (6.92%) and adjusting-rearranging (6.92%) respectively, and adjusting-rearranging (8.46%) and rounding (4.62%) in multiplication. in division, rounding and adjusting-rearranging strategies were used at the same rate (3.85%). [cit] less. following the solution of the first research question, the answer to the second research question \"what is the distribution of strategies used by fourth-grade elementary students during operational estimation by solution times?\" was searched for, and the findings are illustrated in table 3 ."
"in the next stage of the research, 12 hours 33 minutes of video records in total were analysed. the research data were analysed through content analysis (elo & kyngäs, 2008; [cit] ) . qualitative content analysis can be defined as the procedure of classifying and interpreting the content of written texts via encoding and creating themes or patterns systematically [cit] . in this framework, the following steps were followed when analysing the research data:"
"in this section, we first describe what our research revealed to be the essential components of interaction with virtual objects. these essential components allow virtual objects to cross the boundary from a digital representation to something that can allow the therapist access to the client's psyche. we then describe specific design refinements of our prototype that could address some of the issues that arose in our iterative process. note that in this section we are discussing the results across all three phases of our design."
"at present, the vast majority of nids detection mechanism also remains in the captured packets. and the unintelligent pattern matching and signature search techniques are applied to the detection of network attacks. pattern matching techniques based on signature have two of the most fundamental flaws. the enormous computing power required for operation, pattern matching or signature search technology using a fixed mode to detect the attack. and it may only detect the attack characteristic which is obvious and unique. even a slight change of the attack will be ignored."
"the elementary school mathematics curriculum aims to develop two types of estimation skills which are in operation and assessment. the research focuses only on developing the operational estimation skills. accordingly, a 20-question scale consisting of 5 questions from each of the addition, subtraction, multiplication and division operations was developed in order to determine operational estimation strategies used by the students. [cit] were considered. therefore, the template scale was prepared based on the following outcomes within the scope of operational estimation activities in the elementary school mathematics curriculum: \"student can estimate the sum of no more than four-digit natural numbers. student can estimate the difference of two no more than three-digit natural numbers. student can estimate the multiplication of two no more than two-digit natural numbers. student can estimate the result of a division.\" the scale created within the framework of these outcomes in question was submitted for the opinions of three mathematics teaching experts and three elementary teachers, and the template scale was prepared. to determine convenience of the template scale for student levels, duration of interviews, and the problems that might arise during interviews, 6 fourth-grade elementary students were interviewed. the students were selected from a different fourth-grade class of the same school for the pilot implementation. the data recorded in the pilot implementation were reviewed by the three mathematics teaching experts again to finalize the scale."
"another key form of construction that was described by the therapists was the ability to create barriers. this form of construction was not directly supported by our prototype and implies that another level of granularity (besides that of the figurines) might be appropriate, where the virtual objects that the client can interact with can be bent, folded, or attached to one another, like a fence or bricks."
"social workers and therapists are developing new ways of reaching and working with children who are troubled or have experienced traumatic events and difficult life situations. one such method is sandtray therapy [3, 7, 16 ]-a type of play or art therapy [cit] in which the use of figurines in a tray of sand is a vehicle for establishing interaction and rapport between the therapist and the child (figure 1 ). children placing and moving the figurines in the sandtray provides a venue by which therapists can observe the manner in which the child thinks about their experiences and feelings. in response to an idea from a practicing sandtray therapist, we developed a virtual sandtray. in this paper we present its iterative evolution in collaboration with three therapists who make use of sandtray in their therapeutic practices."
"analysis machine is applied to analyzing a particular protocol data, obtained whether the attack possibility exists. in the normal circumstances, analysis machine as much as possible put onto the leaf nodes of tree structure or as close as possible to the leaf nodes, because the more close to the root part of the analysis machine, the more chance of called. excessive analyzer gathered near the roots will seriously affect the system performance. at the same time, the more detailed of the protocol type on a leaf node type, the higher efficiency of the analysis machine."
"instead, a drawer was added to the right side of the screen which acts as a 'resizing box'. one or more figurines can be put into the drawer. a dial is provided on the side of the drawer, with a ridged surface to suggest the ability to turn it. the dial can be turned using a single finger. when it is turned to the left, the figurines in the drawer shrink; when it is turned right, they grow ( figure 5) . a minimum and maximum size are provided to prevent figures from shrinking out of sight or from becoming bigger than the display."
"on the one hand, our digital sandtray prototype is a single point in the design space of interactive tabletops. thus, our iterative process may not yield results that generalize beyond this design. however, our method is an example of how we would recommend designing future interactive tabletop systems and we would argue that many of the design decisions that we made can be adopted on other tabletop systems. furthermore, our feedback session provides some of the first available evidence that virtual objects can be used in a real application in a very different way than they are typically used on desktop computers. in particular, the therapists in our study pointed out the following aspects of our system as particularly compelling:"
"however, the sand itself can be used as an open medium that can be shaped at will and made into a backdrop for the story. because directly simulating the behaviour of sand is computationally intensive, we decided to provide a different type of background open media in textured 'paint'. the specific types of paint we included represent different surfaces, such as sand, grass, concrete and water."
"the fragmentation recombination has great significance on network intrusion detection system. first, there are some ways to use the stack combined carve-chip loopholes which is in the operating system protocol, such as the famous teardrop attack is to send a number of pairs overlapping ip slice in a short time, when the receiver get such a slice the sub-points will be combined slice, because of the offset overlapping, the memory error occurs, even lead to the collapse of the protocol stack."
"due to the fact that these therapists were typically distance collaborators, we also took this opportunity to share domain knowledge. while we had been iteratively discussing the prototype design remotely, this was the first face-to-face opportunity to do live demonstrations by both groups."
"when the first slice and the last slice of ip packets are all arrived, the ip data packets will be combined. during this period, if the offset are not continuous or overlapping, then given program the alarm information to abandon the combination; the combined ip packet copies all contents of the subpieces and has the same source ip address, destination ip address, identification field and protocol field with each sub-piece. it is an infringement ip packet on the format, and then hand to the transport-layer protocol being analysis."
"the value is 06, so it can be drawn that tcp packet is in the ip packet data area. according to the tcp packet format, at the 35th bytes there are two bytes of protocol identification called the port number. so at the third step, read the first 35 bytes value directly. af7*hy289s820800b9v5yt$0611tbhk76500801293ugdb2%00397e3912345678901234567890123 456789012345678901234567890123456"
"(1) forming the framework of themes, (2) doing the first thematization, (3) rearranging the theme framework, (4) performing the main analysis, (5) coding reliability, and (6) writing down and interpreting the findings. in content analysis, researcher may use a coding system available in the literature. if there is no appropriate coding system in the literature, the researcher can develop a coding system or add possible codes to an available coding system to obtain the required data [cit] formed the first thematic framework; [cit] were found as insufficient in categorizing the themes when the interviews with the students were taken into consideration. [cit], it was observed that all solutions could be categorized, and the algorithmic calculation strategy was further added to form the thematic framework of the research. the thematic framework is given in table 1 ."
"in addition to the field of traditional sandtray therapy [cit], we review relevant research in the following areas: technologies that support storytelling or therapy, and tabletop interaction with both 2d and 3d virtual objects."
"in this paper, we have presented the viability of a virtual sandtray that has been identified as usable for therapy by domain experts. beyond the specific domain of art therapy, this work also serves to inform the design process for tabletop display systems and provides some insight into how interaction with 3d objects on a table can be made useful in practice. specifically, the use of precise interaction and a physics engine can together provide a richness that is sufficient for therapists to understand things about a client's psyche through their interactions with the virtual artifacts. these artifacts thus can take on meaning in a way that is not typically sought after in the design of traditional computer applications. future designers of tabletop systems can use this work to inform how they can achieve similar levels of rich interaction, and therefore make a new type of interface between humans and virtual objects possible."
"from the above procedure, there are problems for traditional pattern-matching methods: 1) large amount of computation: for a certain network, the maximum number of times per second need to compare is as follows: the number of bytes with attack signature * the number of network packet bytes * the number of packets per second * the number of attack signatures. and it may be very great especially in the high speed network environment."
"in addition to applying simple transformations to objects, it is often necessary to invoke other actions. while techniques known from traditional interfaces such as menus, tool palettes, and buttons can be used for this purpose, touch interfaces often also employ gestures. for example, wu and balakrishnan [cit] demonstrated how to control a room planning application using a variety of gestures and hand postures. other approaches also combine posture and gesture interaction with speech input [cit] . even with relatively simple size-based recognition hardware it is possible to use a set of hand postures to parameterize or control actions in an interface [cit] . input techniques that provide more information on the shape of a touch, however, can be used to define postures inspired by physical interaction and to infer forces to be used in the interaction [cit] . an approximated touching force can also be used to control the layering of 2d objects [cit], which is related to the concept of shallow-depth 3d interaction [cit] on which our work is largely based."
"it has been determined that the question \"how can i do it?\" is related to procedural knowledge while the question \"why do i do these operations?\" is associated with conceptual knowledge in mathematics [cit] . [cit], majority of the students gave the answer 3/6 to the operation 1/2+2/4 with unequal denominators which they had encountered for the first time, but some of the students found the answer 1 by reasoning it out as \"two halves add up to one whole\" although they did not know how to equalize denominators. this helps us to see that readily available rules (procedural knowledge) are not always enough and it is necessary to improve students' investigative thinking (conceptual knowledge). it has been observed that individuals who can think flexibly and tend to develop strategies in the first-encountered situations are better at mathematics; however, lack of procedural knowledge makes conceptual development difficult as it may cause individual to spend the energy required for thinking in calculations [cit] . in this context, it is possible to say that there is a dynamic interaction among types of mathematical knowledge and the development of one knowledge type requires other types [cit] states that calculation fluency is related to factual and procedural knowledge while number sense to conceptual knowledge. examining the relationships between the types of knowledge, even if we consider calculation fluency to be a subcomponent of number sense or that calculation fluency and number sense are different concepts, it may be said that number sense sometimes improves calculation fluency skills [cit] ."
"2) the algorithm of fragment recombination to be efficient is an important goal of protocol analysis subsystem. in order to ensure the test efficiency and prevent the attacks using the fragments, the program uses the splay tree algorithm. splay tree algorithm is a soft cache method. it enables the nodes which is recently been accessed as close to the root node. it is according to an individual ip packet fragmentation always reaching in a certain period [cit] ."
"finally, give the url value to the analysis programs to carry out attack analysis. as can be seen, protocol analysis is a process of a path from the root to a leaf node, and each leaf or node is a particular attack type of analyzer. the analysis machine employs pattern-matching algorithms to detect attacks, which greatly reduces the computation volume and improve the efficiency of the algorithm."
the rest of the paper is organized as follows. section 2 presents an intrusion detection system based on pattern matching; section 3 gives the design of intrusion detection based on protocol analysis; section 4 provides the concrete implementation and lastly section 5 summarize the whole paper.
"resizing drawer. to provide the client with the (physically impossible) ability to resize rigid objects (dc3), we provide a resize drawer. an alternative way to implement this is the two-finger 'pinch' gesture, in which two points on the object are pulled apart or pushed together to grow or shrink the object, similar to zooming. however, this gesture is already mapped to vertical movement of the figurine. the use of buttons or handles on the object would be harmful to the sense of physical realism and might be easy for the therapist to miss and therefore impede interpretation."
"the first and most obvious form of construction was the construction of the scene itself. the ability to freely move and rotate the figurines made it possible to create a scene that was composed of many different parts. the ability to make multiple copies of a specific figurine played a key role in this construction. for example, the client in our mock therapy session placed four palm tree objects of different sizes around an oasis. the therapist noted that the number and size of these palm trees matched the number of members in the client's family. furthermore, the therapist suggested that this oasis may suggest that the family made the client feel safe. this example demonstrates that this form of construction, made possible by our prototype, was a sufficiently rich interaction for the therapist to gain insight."
"2) the lower detection accuracy: the traditional pattern matching may only detect fixed type of attacks; it may fail if the attack signature is altered slightly. for example, a web server, execute the following command will produce the same effect, attack strings + \"/. / phf \" and \"/ phf\" can be considered the same attack. but such a little change will be ignored by traditional pattern matching method: get /cgi-bin/phf head /cgi-bin/phf get //cgi-bin/phf get /cgi-bin/foobar/../phf get /cgi-bin/./phf get%00/cgi-bin/phf get /%63%67%69%2d%62%69%6e/phf it can be seen that the deficiencies of the pattern-matching methods seriously affects the speed and accuracy of intrusion detection with the development of large amount of network traffic and the evolution of hacking techniques. for the network communications, the network protocol defines standard, hierarchical, and formatted network packets. if a packet is analyzed according to the layer of protocol in turn, it is a good complement to traditional pattern matching. therefore, it is a much better method with the combination of the protocol analysis and pattern matching, which may reduce the computation volume of matching algorithm, improve analysis efficiency and get more accurate detection results."
"painting drawer. to provide the client with an open medium that can be shaped at will (dc2), we provide them with the ability to paint the background with different textures. on first thought, the interaction to 'paint' on a touch sensitive display could simply be like fingerpainting: wherever the surface is touched, the chosen paint colour appears. however, combining this technique with the other interactions in the virtual sandtray would require a mode switch, which would likely be problematic [cit], and particularly when multiple people are using the system (dc4). the painting ability is provided through the use of a spray nozzle tool. this nozzle can paint a texture on the floor ( figure 6 ). when the nozzle is picked up, it will rotate to point at the sandtray floor, which is the orientation in which it will normally be used. when the nozzle is lifted up, away from the floor, the region that is painted becomes larger."
"the crisp output and the slice membership function in figure 3 evaluate every fuzzy input. the red circle in this figure shows the defuzzification value, which is 28.26°c. in this evaluation, the fuzzy minimum, average and maximum inputs are; 23.9°c, 27.1°c and 28.4°c."
"in these days environment polluted and contaminated because of wide spread of smoke and chemical particles that contains in atmosphere. contaminant particles such like sulfur dioxide (so 2 ) and carbon dioxide (co 2 ) are hazardous to health and living thing. the world health organization (who) has reported approximately 2.4 million people die every year due to air pollution and especially 1.5 million people die because of indoor air pollution [cit] . the indoor air pollutant is not just from the emission of so 2 and co 2 but also from the volatile organic compounds, biological particles and etc [cit] . people working in offices, sometimes rarely move out of office. the air-conditioning system which circulates air carries bacteria and fumes substance pollutants from the air-filter and blower blades. a study shows, in america 88% of people stay in building, 7% in vehicle and 3% stay outside, which means that 88% are classified as high risk of people exposed to indoor air contamination [cit] . it is a challenge for maintenance engineers and building managers to maintain good air quality index. they must ensure that trapped contaminated air is circulated and diluted out of building [cit] . the air conditioning system (hvac) re-circulates indoor air should maintain comfortably by regulating the impact of heat and cool from outside air but it is difficult to compare between the re-circulated and outside air, which comes from air-duct. co 2 is natural gas available in our atmosphere but the ppm reading of an indoor environment is obviously higher than the outdoor environment because of the exhalation. american society of heating, refrigerating and air conditioning engineers (ashrae) specified in a standard 55, that suitable indoor environment temperature range during summer must be approximately between 22.8°c to 26.1°c and during winter, approximately 20.0°c to 23.6°c. the ashraf specified humidity of a comfort environment must be between 30 to 60 percent [cit] . the fuzzy logic system (fls) stimulates for precision on a group of data, which are vague, imprecise or ambiguous. fls has four important parts which is fuzzifer, rules setting, inference engine and defuzzifier [cit] . fls crisp input is a collective set of variable that converts to fuzzy set with using fuzzy language variables, linguistic terms and membership functions, this process is called fuzzification. inferences are fuzzy rules to relay fuzzy set to make decision. lastly, the defuzzification process produces crisp output from fuzzy membership functions. for example, figure 1 illustrates a block diagram of hvac system for controlling temperature based on present temperature. the fls reads the temperature and compares with the targeted temperature for controlling the compressor valve [cit] . figure 2 : fls surface view figure 2 shows the surface view of the overall implementation of fls. six different rules created in fuzzy rule editor to form relationship using the fuzzy linguistic variables with operators."
"the standard deviation ( ) and mean ( ) measure to quantify dispersed variables of a data set. if a standard deviation value closer to zero then the evaluated data set is nearer to expected value. mean is the central tendency of a random population. mean analysis shows the closeness bound between more than two readings. the standard deviation ( ) analysis in figure 4 is based on fuzzy predicted dataset. temperature in zone three seems has least correlation with the temperatures predicted in zone 2 and 3. at 6:50 am and 9:00 pm the fuzzy predicated temperature values are nearer to each other in zone 1, 2 and 3."
the indoor environment become unfriendly and affects human health. fuzzy logic used for predicting temperature of the next day. mamdani fls model is an effective model and reliable for a moderate data. the triangular linear fit function evaluates the fuzzy inputs to fire respective rules to obtain deffuzzified value. the standard deviation and mean show the correlation between the zones. fuzzy predicts to certain level of moderate accuracy and significant to temperature change due to situation and external conditions.
"bpm into rfid systems context-sensitive pervasive computing softwares and their runtime operations. [cit] have developed a service-oriented middleware to support the acquisition, discovery, interpretation and accesses of various contexts to build context-aware services. these middleware systems are mainly designed for general sensors, and therefore they fail to fully address the various characteristics of rfid technology. [cit] have reported a business aware framework for business processes in the epc network. this framework allocates its business aware layer between the traditional rfid data middleware and upper applications, and the framework mainly focuses on the conversion of rfid raw events to business events, and the invocations to upper-level business services."
". improved sensitivity of business intelligence. real-time visibility supports vendor-managed inventory programs, helps prevent shrinkage and diversion, and discourages counterfeiting by making it easier to identify fake products. end-to-end visibility also supports the record keeping needed for e-pedigree tracking for the pharmaceutical industry."
"definition 4 (domain-specific predicate). for the class instances (or objects) in a specific domain, some dedicated predicates are used to describe the relations between these objects. in this paper, we confine such domain-specific predicates to be first-order predicates. table i lists some domain-specific predicates in the pallet-packing example. based on these predicates, we can define some derivation rules to further exploit the relations between objects:"
"the on and off resistance values of the network with twelve memristors are lower than the other networks due to the reduced number of serial layers. however, when one compares it to a single memristor, it has lower on resistance value and higher off resistance, meaning the network is more sensitive to control signals than only one memristor. in other words, a pulse with the same voltage level could make a clearer distinction between the initial and after states."
"in next section, we will discuss the features of rfid applications and how it influences the data handling of bpm by investigating a distribution centre (dc) scenario from a technical view."
"as shown in figure 2, this packing process includes steps of receiving orders, picking goods, pallet packing, and shipment assembling. after receiving an order from customers, pickers begin to pick the ordered goods from the inventory, and the forklift cars begin to transfer the pallets and the picked goods to the assembly spot. thereafter, the goods are packed onto pallets at an assembly line, and finally the packed pallets are loaded to proper trucks for shipment."
"object-oriented vs process-centric approaches traditional process management approaches model business processes as a series of connected activities, which sticks to a control-flow-oriented perspective, and its execution follows an activity-completion-triggered mechanism. to adapt to the distributed computing environment where communications go through events/messages, prof. [cit] . the event-driven process chain is the key component of sap r/3's modelling concepts for business engineering and customisation, and has also been integrated in sap's netweaver system [cit] . this method models a business process from the perspective of event flows, and the business process is driven by received events. reluctantly, event-driven process chain still sticks to the control-flow-oriented scheme without many concerns on data flows or material flows. business process execution language for web services (ws-bpel) [cit] provides a popular business process modelling solution particularly in the web service environment. ws-bpel models the interactions between partners as service invocations and enables the communication via messages. the structure of a ws-bpel process still strictly follows the pre-defined process control flow. consequently, ws-bpel only fits into the scenario where the activity sequence, the message sequence and the service invocation sequence are all pre-fixed."
"in imaging spectroscopy, contiguous narrow-band spectrographic information is collected for each spatial pixel in an imaging system. the technology is presently synonymous with hyperspectral imaging (hsi) and is commonly implemented within the discipline of remote sensing to characterize the physical and chemical properties of observed materials. this is preformed via spectroscopic and spatial analysis methodologies [cit] . imaging spectroscopy technologies have shown their utility in numerous remote sensing applications in geology [cit], defense [cit], agriculture [cit], forestry [cit], oceanography [cit], forensics [cit] and ecology [cit], among others. 2 (1) where x, y, x, y represent the two variables of interest and their means, respectively. in mathematical terms, the cc represents the sum of the centered and normalized cross-product of x and y [cit] . each variable is centered by removing its mean. the denominator normalizes the numerator by the variance of the studied variables. using the cauchy-swartz inequality, it can be shown [cit] that the numerator is always less than or equal to the denominator. therefore, the value of the cc is bounded between −1 and 1. the boundary values represent a perfect linear correlation between x and y. a value of zero corresponds to no linear correlation between the variables. values greater than zero indicate a positive correlation between the variables of interest; the opposite is true for values less than zero. the cc is a useful descriptive measure of correlation since its value does not depend on the scales of measurement for the studied variables [cit] . it is important to note that the calculation of the cc is not limited by any statistical assumptions; however, its value as an input to other statistical metrics may need to conform to certain restraints (e.g., normally-distributed data)."
"an extreme, but important case of the dpm is when only two states can be clearly distinguished, as they can be further classified as binary purpose memristors (bpm). with its reduced capabilities they lack applications beside their use as binary memory units supplementary to the cmos based digital systems or implementing routing in logic gate arrays, like field programmable gate arrays (fpgas) [cit] ."
". movements of some rfid-tagged objects reflect swarming phenomena, as many rfid-tagged objects act with the similar behaviours, particularly in the packaging and transportation stages. the products of the same type and same batch may participate in different business processes, yet it is hard to pre-define the correlation between products and the involved business processes."
"unfortunately, although we use an rnn, which keeps a representation of previous parse items in its hidden state and has the potential to capture longterm dependencies, the resulting model is still fully greedy: a locally optimal action is taken at each step given the current input x t and the previous hidden state h t−1 . therefore, once a sub-optimal action has been committed to by the parser at any step, it has no means to recover and has to continue from that mistake. such mistakes accumulate until the goal is reached, and they are referred to as search errors."
"atmospheric absorption features are distinctive and constant under stable conditions [cit] . as such, the cc was thought to be able to detect inconsistencies in these regions since error-induced changes located within these features are more easily identifiable. as depicted in figure 8, a cc was calculated between the spectrum from each pixel in the roi and the designated central reference pixel using only the hyperspectral data that corresponded to each of the approximate wavelength regions identified in table 2 . atmospheric absorption features were used to locate finer errors in the imagery that might not be easily visible in the ccs when calculated with the entire spectrum. these features were manually identified in the spectrum of the center asphalt pixel using the theoretical locations in table 2 for guidance. table 2 . the approximate spectral location of known atmospheric absorption features [cit] . it is important to note that the wavelength ranges for some atmospheric absorption features may vary in response to external factors. for instance, the range of the water absorption features is highly dependent on water vapor and aerosol optical thickness [cit] . atmospheric absorption features are distinctive and constant under stable conditions [cit] . as such, the cc was thought to be able to detect inconsistencies in these regions since error-induced changes located within these features are more easily identifiable. as depicted in figure 8, a cc was calculated between the spectrum from each pixel in the roi and the designated central reference pixel using only the hyperspectral data that corresponded to each of the approximate wavelength regions identified in table 2 . for each spectral range, the imagery was visualized for a single band within the specified window to study the nature of any detected errors. once again, image intensities were histogram equalized to enhance contrast and clearly display potential errors. the methodologies presented in this section were repeated for each of the 8 processed images described in section 2.2."
"as can be seen in figure 11, the general trends outlined in figure 10b,e persisted even after the application of awgn at an snr of 100:1. the cc remained invariant to linear transformations (figure 11a,b) . however, the average value of the cc reduced to approximately 0.98. although the detailed relationships in figure 11c,d were masked by the variation induced by the introduced noise, the first-order trends are clearly present and identifiable. the cc reduced from 0.980-0.968 after a 10 nm spectral shift in figure 11c . as the scaling factor increased from 0-50, the cc decreased from 0.9815-0.975. at an snr of 100:1, the average standard deviation in the cc for each modification was approximately 0.001. as seen in figure 12, this value matched the results derived from the final cc test, which calculated the standard deviation in the calculated cc at various noise levels. the cc remained invariant to linear transformations (figure 11a,b) . however, the average value of the cc reduced to approximately 0.98. although the detailed relationships in figure 11c,d were masked by the variation induced by the introduced noise, the first-order trends are clearly present and identifiable. the cc reduced from 0.980-0.968 after a 10 nm spectral shift in figure 11c . as the scaling factor increased from 0-50, the cc decreased from 0.9815-0.975. at an snr of 100:1, the average standard deviation in the cc for each modification was approximately 0.001. as seen in figure 12, this value matched the results derived from the final cc test, which calculated the standard deviation in the calculated cc at various noise levels. at a snr of 100, the standard deviation is approximately 0.001. the standard deviation in the cc asymptotically increased from zero to approximately 0.22 when the snr decreased through the introduction of awgn."
"first test measurements were prepared with a single memristor device. here two types of signals were used. the first one was a single, 2.4 s long 2.5 v writing pulse, which shows some parameters of the device. the results can be seen in figure 4 . the average on state was 57 kω, the average off state was 11.5 mω. the on/off ratio is approximately 200."
"the spatial locations associated with distinct reductions in the ccs were identified using the threshold defined in section 2.4. these locations were used to spectrally isolate the potential errors to the windows identified in table 3 using the windowed-based methodology described in section 2.4. at a snr of 100, the standard deviation is approximately 0.001. the standard deviation in the cc asymptotically increased from zero to approximately 0.22 when the snr decreased through the introduction of awgn."
"the expected properties of memristors for such applications are wide and analog resistance range, low variance of device parameters and high device stability during long-term operation. research has been done [cit] to find optimal materials that satisfy these expectations, but even then there are other possibilities to further increase the capabilities of memristors."
"1. we use a beam-search decoder to parse a sentence x n in the training data and let the decoder generate a k-best list 2 of output parses using the current θ, denoted as λ(x n ). similar to other structured training approaches that use inexact beam search [cit], λ(x n ) is as an approximation to the set of all possible parses of an input sentence."
"traditional activity-based workflow models architect business processes with the main focus on control flow dependencies, where activities execute in accordance to the pre-defined sequence rather than the dynamics of real-time business context. to adapt to the event-based communication and effectively utilise the real-time object information, we attempt to model business processes from an object-oriented perspective, and drive business process according to the contextual dynamics."
"the cc between the mean in-situ asphalt radiance and any given individual sample used to comprise the mean signal was very close to one, ranging from 0.99987-0.99998, with a standard deviation of 0.000023."
"the measurement setup consists of an amplifier circuit as a current-voltage converter and a current regulator resistor as it can be seen in figure 1 . the current regulator resistor helped to ensure that the current does not reach high values where the device could become faulty. the used signal generator and measurement device is an \"ni elvis ii+\", controlled by labview software (national instruments, austin, tx, united states). the sampling frequency is 500 ksample/s for every measurement. the state of every device has been set to an off state before every measurement. figure 1 . measurement environment circuit. the measurement setup consists an amplifier circuit as a current controlled voltage source and a current regulator resistor. the used amplifier is a \"tl082\". the applied voltage v x was strictly between −2.5 v and 2.5 v. the memristor symbol represents either a single memristor or a network of memristors depending on the measurement."
"remote sens. 2018, 10, x for peer review 6 of 26 table 1 . the five modifications applied to ( ) to generate the transformed signal, ( )."
"for the casi imagery, the ccs systematically reduced in value by more than one standard deviation near the edges of the fov. this reduction was largest for the casi data derived from the original processing methodology. when compared to the imagery collected on the 23rd, the casi data from the 24th were characterized by more substantial reductions in the ccs near the edges of the fov, especially along the left side. the ccs for the sasi imagery were almost identical, regardless of the processing methodology or the acquisition date. the ccs for the sasi imagery were consistently lower than the mean across the fov from pixels 548-564. as seen in figure 13b,d, this reduction appeared to be parabolic in nature, reaching a minimum value of approximately 0.995 and 0.997 in the sasi imagery from the 23rd and 24th, respectively."
"to further the analysis and spectrally locate smaller residual errors in the casi data from the 24th, five atmospheric absorption features were identified in the spectral range from 365-1050 nm (table 4) . the ccs across the fov of the casi images from the 24th were calculated with respect to the center pixel over the spectral regions identified in table 4 and are shown in figure 18 . the ccs in the 680-712 nm region were highly variable, ranging from 0.95-1 with a subtle low frequency sinusoidal structure (figure 18a,b) . visual inspection of the associated imagery in figure 19 indicated that, throughout much of the fov, there were discrete pixels and groups of pixels that appeared to be non-uniform across the entire fov, noticeably varying in brightness even amongst neighboring pixels. these pixels lead to \"striping\" artefacts across the entire fov in the image data. these trends were apparent in both casi images. the low frequency sinusoidal structure could not be clearly visualized in the imagery. the sinusoidal structure was not a numerical computational effect."
". the raw rfid data should pass the cleansing, consolidation, and summarisation processes at the edge systems to ensure greater reliability and protect central it systems from the flood of data."
"the spectrum from the center asphalt pixel in the roi was designated as the reference for the application of the cc since it was the center of the instruments' fov. the center pixel was evaluated to ensure that it was a reasonable reference that contained no obvious errors. a cc was calculated between the spectrum from each pixel in the roi and the designated central pixel reference in accordance with figure 6 . theoretically, the ccs should be exactly 1 across the fov. although this is not the case in real data, the cc between well-behaved target spectra will vary around a mean value that is still quite close to 1. the spatial pixels associated with substantial reductions in the ccs were recorded as potential locations for errors in the hsi data. substantial reductions were characterized by ccs that fell below a designated threshold that was derived from the sensitivity testing."
"finally, all shift-reduce ccg parsers mentioned in this paper take multi-tagging output obtained with a fixed β for training and testing; and in general, a smaller β value can be used by a shift-reduce ccg parser than by the c&c parser. this is because a β value too small may explode the dynamic program of the c&c parser, and it thus relies on an adaptive supertagging strategy [cit], by starting from a large β value and backing off to smaller values if no spanning analysis can found with the current β. table 3 : the effect on dev f1 by varying the beam size and supertagger β value for the greedy rnn model."
"as can be seen in figure 11, the general trends outlined in figure 10b,e persisted even after the application of awgn at an snr of 100:1."
"our recurrent neural network model is a standard elman network [cit] which is factored into an input layer, a hidden layer with recurrent connections, and an output layer. [cit], the input layer x t encodes stack and queue contexts of a parse item through concatenation of feature embeddings. the output layer y t represents a probability distribution over possible parser actions for the current item."
"from this comparison, we can see that object-oriented approaches provide more powerful business modelling expressivity, which can precisely describe the item-level behaviours. compared to process models, rule-based approaches treat the handling items individually, and therefore provide a finer control over business transactions. moreover, the rules directly define the responses of objects on receiving specific events or in given conditions, and therefore they can run more efficiently. particularly, when handling a large volume of items (process instances) at real time, the rule-based approaches provide a better performance. these features show that object-oriented approaches fit more into the business process modelling and automation in rfid-enabled applications."
"the following measurements were carried out on four different network types and on a single memristor for reference. the measuring signal is alternating write-erase sinusoidal pulses with a length of 23 ms in a sequence of 50 cycles. this measurement is supposed to simulate a general training scenario, where an analogue memristor characteristic is expected and the training is done by several small pulses. according to this consideration, the writing pulses of the measurement have not enough energy to change the state of a single memristor into its on state."
"the ccs in figure 18i,j remained relatively constant with very little variation. the associated imagery was visualized in greyscale with one of the bands from the identified spectral range (figure 22) . no large errors could be seen in any of the analyzed casi imagery within this spectral range."
"before applying the cc, a region of interest (roi) (blue line in figure 5 ) was identified across the fov, along the taxiway located directly south of the calibration site. the roi was comprised of a single asphalt road pixel from each column within the sensor fov. every attempt was made to acquire spectra from asphalt pixels that were uncontaminated by non-asphalt substances such as paint, vegetation and other non-asphalt hydrocarbons. \"wobbles\" in the imagery in figure 5 are caused by the movement of the aircraft and can be readily accounted for through various geocorrective methodologies. in this work, it was fundamental to preserve the original sensor geometry in the analysis, so no geocorrection process was applied. the spectrum from the center asphalt pixel in the roi was designated as the reference for the application of the cc since it was the center of the instruments' fov. the center pixel was evaluated to ensure that it was a reasonable reference that contained no obvious errors. a cc was calculated between the spectrum from each pixel in the roi and the designated central pixel reference in accordance with figure 6 ."
"through the application of the cc, the quality of remotely-sensed hyperspectral data could be assessed through error detection in a quantitative manner. this was evident in the analysis of the eight hyperspectral images that were studied. by calculating the ccs across the fov with the entire spectra, it was possible to immediately gauge the spectral consistency of the hsi data collected by the casi and sasi, across the fov. it is important to note that the method was explicitly designed for the detection of errors, not for the identification of their origin."
"comparing figure 13a,c to figure 17a,c, there was more consistency in the ccs of the casi imagery from the 23rd between the original and refined processing. although this trend holds for the casi data from the 24th, there was still a notable average offset of 0.0007 between the two curves. significance testing yielded p-values less than 10 −5 for all observed relationships."
"shift-reduce parsing is a popular parsing paradigm, one reason being the potential for fast parsers based on the linear number of parsing actions needed to analyze a sentence [cit] . recent work has shown that by combining distributed representations and neural network models [cit], accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models."
"overall, errors were detected in the casi and sasi imagery though the application of the cc. although more sophisticated error detection methodologies exist (e.g., [cit] ), they can be monetarily expensive to implement and rely on a higher level of mathematical understanding. without a fundamental understanding of a method, its implementation can lead to inaccurate interpretations. the presented method is intuitive; the cc is a rather simple statistical tool and its application is straight forward. the detection can be conducted on radiance spectra prior to atmospheric correction, quickly after acquisition. after removing the wavelength region associated with large errors, the described methodologies could be repeated to isolate smaller errors. although the application was developed for hyperspectral technologies, it can be easily generalized for data collected by other imaging spectrometers. this versatility showcases the cc as a strong and simple statistical tool for the analysis of spectrographic imaging data through the detection of errors."
"in very large scale integration (vlsi) device manufacturing, it is often easier and tends to cause fewer faults to make the same device many times, and use it as a building block to emulate other devices, instead of creating fewer, but different devices [cit] . the same approach can be applied to memristors, but one should take into consideration their special nonlinear behavior in the voltage-current domain. this idea is further supported by the fact that memristors as two-terminals, could be manufactured more easily on many layers on microchips [cit] than transistors. however, with every extra layer, the probability of device defects could also increase."
"finally, a number of recent work [cit] explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. in principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we would expect further accuracy gains are possible by combining these techniques in a single model."
"the same control signal should produce the same result, both in the transient characteristics and the final state of the memristor. by reliability, we mean a low variance of the characteristics. our aim was to avoid using a memristor model as an absolute reference, and be able to approximate the real memristors more accurately. therefore, our analysis focuses on the mean and variance of characteristics of several measurements on the same device or network in a short period of time."
"to train a greedy model, we extract gold-standard actions from the training data and minimize crossentropy loss with stochastic gradient descent (sgd) using backpropagation through time (bptt; [cit] ) . [cit], we compute the softmax over only feasible actions at each step."
"definition 5 (event pattern). event patterns are used for eliciting the underlying meanings from tag read event series, and the pattern design is subjected to the business logics of the specific application. here, we borrow the event-calculus [cit] like formalism to define an event pattern 1 as:"
"before applying the cc, a region of interest (roi) (blue line in figure 5 ) was identified across the fov, along the taxiway located directly south of the calibration site. the roi was comprised of a single asphalt road pixel from each column within the sensor fov. every attempt was made to acquire spectra from asphalt pixels that were uncontaminated by non-asphalt substances such as paint, vegetation and other non-asphalt hydrocarbons. \"wobbles\" in the imagery in figure 5 are caused by the movement of the aircraft and can be readily accounted for through various geocorrective methodologies. in this work, it was fundamental to preserve the original sensor geometry in the analysis, so no geocorrection process was applied."
"although figure 18c remained relatively consistent across the asphalt road, there was a sudden reduction near the end of the fov at pixels 1454 and 1456. after independently displaying all bands in the specified spectral window in greyscale, errors were spatially located in columns 1454 and 1456; these errors were visualized as a bright and dark vertical stripe, respectively, across the imagery (figure 20) . the vertical stripes were not present in the casi imagery with the refined processing or figure 18d . there was a reduction of approximately 0.021 in the cc near the edges of the fov in figure 18e,f. the effects associated with these reductions could not be visualized within the imagery. figure 18g was characterized by sporadic reductions in the cc of greater than 0.05. these reductions revealed potential imaging errors for the spectral window in the following spatial ranges: 256-276, 551-576, 912-936 and 1209-1235. after independently displaying all of the bands in the specified spectral window in greyscale, it was possible to detect groups of non-uniform pixels that noticeably varied in brightness. these groups created distinct \"striping\" artifacts that can be seen at several spatially-isolated points across the casi imagery from the 24th with the original processing ( figure 21 ). this effect was not present in the casi imagery with the refined processing or figure 18h . there was a reduction of approximately 0.021 in the cc near the edges of the fov in figure 18e,f. the effects associated with these reductions could not be visualized within the imagery. figure 18g was characterized by sporadic reductions in the cc of greater than 0.05. these reductions revealed potential imaging errors for the spectral window in the following spatial ranges: 256-276, 551-576, 912-936 and 1209-1235. after independently displaying all of the bands in the specified spectral window in greyscale, it was possible to detect groups of non-uniform pixels that noticeably varied in brightness. these groups created distinct \"striping\" artifacts that can be seen at several spatially-isolated points across the casi imagery from the 24th with the original processing ( figure 21 ). this effect was not present in the casi imagery with the refined processing or figure 18h ."
"in regard to the pallet packing context, example 1 has introduced part of its content, and here example 3 lists the enriched content of this context: movement-related predicate: movewith(g, p) -product g moves together with pallet p."
"our follow-up work is to further refine the proposed object-oriented framework, and justify its extensibility and flexibility. in the future, we plan to deploy domain-specific ontology technologies for unifying the different rfid information systems."
"our work targets at incorporating business process control and automation into rfid-enabled application systems with a novel object-oriented business model. different from traditional business process modelling approaches, this model depicts a business scenario by means of characterising the contextual dynamics among related objects. to cater for the event-driven and data-intensive execution mechanism of rfid applications, this model defines event patterns and business rules to elicit the business meaning from rfid tag read events. in addition, this model encapsulates the related classes, rules event patterns into deployable rfid application contexts. once such rfid application contexts are deployed to the data-driven middleware systems, the business logics can be pushed down to rfid edge systems. therefore, the rfid edge systems will become aware to both real-time object information and business logics, and thereby can intelligently respond to the tag read events with proper operations on spot. by changing the deployed rfid application contexts, the same edge systems and infrastructures are capable to support different applications. this feature enhances the flexibility and customisability of rfid application systems."
"as a final test of consistency, the standard deviation of the cc was assessed in the presence of noise. in particular, the awgn transformation in table 1 was applied to ( ) 1000 times. a cc was calculated between ( ) and each of its transformations. the standard deviation of the ccs from each distinct snr was calculated."
"object-oriented perspective and rfid-enabled applications object-oriented methodology has been widely used in modelling real-world applications. here, we discuss the feasibility of applying the object-oriented perspective in modelling rfid-enabled applications against the classic object-oriented features. polymorphism. in the rfid-enabled application, an entity type may own multiple definitions and its method may own multiple implementations. once deployed in a practical scenario, it will choose proper definition or implementation according to the actual situation. take assembly line b in the mentioned dc scenario as an example. with the products of the same type, this assembly line will pack them from the centre of the pallet to balance the weight distribution; while with mixed products, this assembly line will pack them in proportional spacing to match different product sizes and balance the weight distribution. this reflects the function-level polymorphism."
"in this section, we present the details of our object-oriented business model for rfid-enabled applications. this model is based on our work on rfid event integration ) and follows the event-calculus formalism [cit] . first, we define some:"
"which has a simple closed form. a naive implementation of the xf1 training procedure would backpropagate the error gradients individually for each y i in λ(x n ). to make it efficient, we observe that the unfolded network in the beam containing all y i becomes a dag (with one hidden state leading to one or more resulting hidden states) and apply backpropagation through structure [cit] to obtain the gradients."
"the error within all sasi imagery was located at the same spatial pixels and spectral range. the error could be displayed by visualizing the only band in the 993-1008 spectral range (figure 16 ). this general trend held for all casi imagery and was less prominent with the refined processing methodology (figure 15 ). the asphalt road is still brightest along the edge pixels, but to a lesser degree than in figure 14 . the asphalt road along the south side of the image was brightest along the edge pixels. these errors in the data are highlighted by the red arrows and were less noticeable in the imagery that was generated from the refined processing. this general trend held for all casi imagery and was less prominent with the refined processing methodology (figure 15 ). the asphalt road is still brightest along the edge pixels, but to a lesser degree than in figure 14 . the asphalt road along the south side of the image was brightest along the edge pixels. these errors in the data are highlighted by the red arrows and were less noticeable in the imagery that was generated from the refined processing. the asphalt road along the south side of the image was brightest along the edge pixels. these errors in the data are highlighted by the red arrows and were less noticeable in the imagery that was generated from the refined processing. the error within all sasi imagery was located at the same spatial pixels and spectral range. the error could be displayed by visualizing the only band in the 993-1008 spectral range (figure 16 ). after removing data in the spectral windows in accordance with table 3, there was a substantial increase in the values of ccs across the fov in all images (figure 17 ), especially at spatial locations associated with the previously identified imaging errors. comparing figure 13b,d and figure 17b,d, the large reduction in the sasi imagery from pixels 548-564 was completely removed. furthermore, the ccs along the fov of the casi images remained relatively constant, even at the edge pixels. overall, there was more consistency between the images derived by the different processing methodologies and acquisition dates. after removing data in the spectral windows in accordance with table 3, there was a substantial increase in the values of ccs across the fov in all images (17), especially at spatial locations associated with the previously identified imaging errors. comparing figures 13b,d and 17b,d, the large reduction in the sasi imagery from pixels 548-564 was completely removed. furthermore, the ccs along the fov of the casi images remained relatively constant, even at the edge pixels. overall, there was more consistency between the images derived by the different processing methodologies and acquisition dates. the error within all sasi imagery was located at the same spatial pixels and spectral range. the error could be displayed by visualizing the only band in the 993-1008 spectral range (figure 16 ). after removing data in the spectral windows in accordance with table 3, there was a substantial increase in the values of ccs across the fov in all images (figure 17 ), especially at spatial locations associated with the previously identified imaging errors. comparing figure 13b,d and figure 17b,d, the large reduction in the sasi imagery from pixels 548-564 was completely removed. furthermore, the ccs along the fov of the casi images remained relatively constant, even at the edge pixels. overall, there was more consistency between the images derived by the different processing methodologies and acquisition dates. the ccs of the casi imagery increased greatly, especially along the edges, indicating that the error was primarily contained within the spectral regions identified in table 3 . the casi data from the 23rd were relatively consistent between both processing methodologies. although this trend generally held for the data from the 24th, (c) showed a notable offset. (b,d) the large reduction in the ccs from the sasi imagery at pixels 548-564 was not present after removing the problematic band that was found between 993 and 1008 nm."
"the second type is the digital (or discrete) purpose memristor (dpm), which has several but countable states and the resistance value can only be one of these states. an important property is that these states should be clearly distinguishable from each other. this type can be used trivially as an n state memory unit based on the number of its possible states."
"neural network shift-reduce parsers are often trained by maximizing likelihood, which does not optimize towards the final evaluation metric. in this paper, we addressed this problem by developing expected f-measure training for an rnn shift-reduce parsing model. we have demonstrated the effectiveness of our method on shift-reduce parsing for ccg, achieving higher accuracies than all shift-reduce ccg parsers to date and the de facto c&c parser. 11 we expect the general framework will be applicable to models using other types of neural networks such as feed-forward or lstm nets, and to shift-reduce parsers for constituent and dependency parsing."
"comparing figure 13a,c to figure 17a,c, there was more consistency in the ccs of the casi imagery from the 23rd between the original and refined processing. although this trend holds for the casi data from the 24th, there was still a notable average offset of 0.0007 between the two curves. significance testing yielded p-values less than 10 −5 for all observed relationships."
"as a promising data sensing/collecting mechanism, rfid brings a lot of operational benefits to business sectors, including manufacturing, healthcare, transportation, defense, retail and agriculture, where requires accurate data or more collection points, according to christine overby of forrester research, boston, ma, usa. for the general bpm in these sectors, rfid provides the ability of tracking goods moving through the supply chain. this makes it possible to shorten the order-to-cash cycle, detect and resolve delivery exceptions, prevent out-of-stock situations, and pinpoint-affected product in a recall, while minimising inventory and safety stock levels."
"refined processing table 4 . plots (a,c,e,g,i) correspond to the casi data that were processed with the original methodology. plots (b,d,f,h,j) correspond to the casi data that were processed with the refined methodology. table 4 . plots (a,c,e,g,i) correspond to the casi data that were processed with the original methodology. plots (b,d,f,h,j) correspond to the casi data that were processed with the refined methodology. table 4 . plots (a,c,e,g,i) correspond to the casi data that were processed with the original methodology. plots (b,d,f,h,j) correspond to the casi data that were processed with the refined methodology. although figure 18c remained relatively consistent across the asphalt road, there was a sudden reduction near the end of the fov at pixels 1454 and 1456. after independently displaying all bands in the specified spectral window in greyscale, errors were spatially located in columns 1454 and 1456; these errors were visualized as a bright and dark vertical stripe, respectively, across the imagery (figure 20) . the vertical stripes were not present in the casi imagery with the refined processing or figure 18d ."
"for each hyperspectral image, the calculated ccs recorded in figure 13 adhered very closely to one across the fov when calculated with respect to the spectrum from the center pixel. the ccs for the casi imagery were consistently lower than that of the sasi by an average value of 0.0021 ( figure 13 ). in addition, the average standard deviation in the ccs of the casi data was over 18-times larger than that of the sasi."
"inheritance. in our object-oriented business model, an involved entity type can be easily extended or specialised by adding new methods or attributes. this feature is particularly useful when deploying the defined classes in a practical scenario. for example, the class of forklift car can be specialised with methods of loading and unloading pallets in the dc case; while it may be specialised with attributes of latest position, latest speed, etc. in the case of monitoring the locations of cars."
"remote sens. 2018, 10, x for peer review 6 of 26 table 1 . the five modifications applied to ( ) to generate the transformed signal, ( )."
"the second type of signal is a sequence of a writing and an erasing signal. the writing pulse was 160 ms long, while the erasing one was shorter, 40 ms. the results can be seen in figure 5 . the writing process was faster and starts at a lower voltage level, but the switching was not as sharp as in the previous case (figure 4 ). during the reading sequence, the small amplitude pulses did not change the state of the memristor."
". improve the visibility of the status of a business transaction. given the planned picking time for a shipment, the latest time for packing process can be estimated. with rfid technology, which packing processes have started and which have not can be seen clearly. therefore, early warnings for potential late shipments can be easily discovered."
"a lexicon, together with a set of ccg rules, formally constitute a ccg. the former defines a mapping from words to sets of lexical categories representing syntactic types, and the latter gives schemas which dictate whether two categories can be combined. given the lexicon and the rules, the syntactic types of complete constituents can be obtained by recursive combination of categories using the rules."
"before applying the cc, a region of interest (roi) (blue line in figure 5 ) was identified across the fov, along the taxiway located directly south of the calibration site. the roi was comprised of a single asphalt road pixel from each column within the sensor fov. every attempt was made to acquire spectra from asphalt pixels that were uncontaminated by non-asphalt substances such as paint, vegetation and other non-asphalt hydrocarbons. \"wobbles\" in the imagery in figure 5 are caused by the movement of the aircraft and can be readily accounted for through various geocorrective methodologies. in this work, it was fundamental to preserve the original sensor geometry in the analysis, so no geocorrection process was applied."
"definition 8 (rfid application context). an rfid application context stands for an actual environment where classes and business rules are deployed with real event series. such an rfid application context sc can be defined as tuple (t, r, e, op), where:"
"to calculate the threshold, a stable spatial region was manually identified by consistent ccs that varied around a constant mean. using the mean cc of this region, the snr of a stable spectrum was approximated using the noise sensitivity data derived in section 2.3. with the approximate snr, the data from the final test in section 2.3 were used to estimate the expected standard deviation of the ccs derived from stable spectra. using the estimated standard deviation and the mean value of the ccs in the stable region, potential errors were detected by reductions more than 3 below the mean. a 3 threshold was selected to ensure that at least 99.7% of the stable data were not flagged as a potential error. consequently, ccs below the threshold were likely associated with errors in the hsi data."
"to verify the spectral window and specify the nature of the potential errors, the imagery was visualized for a single band within the identified spectral ranges. in this visualization, image intensities were histogram equalized to enhance contrast by making the histogram of the resulting image equalized to a constant value. to verify that the reductions in the ccs were associated with these errors, the ccs were calculated across the fov with respect to the center pixel after the removal of the identified spectral region. atmospheric absorption features were used to locate finer errors in the imagery that might not be easily visible in the ccs when calculated with the entire spectrum. these features were manually identified in the spectrum of the center asphalt pixel using the theoretical locations in table 2 for guidance. table 2 . the approximate spectral location of known atmospheric absorption features [cit] . it is important to note that the wavelength ranges for some atmospheric absorption features may vary in response to external factors. for instance, the range of the water absorption features is highly dependent on water vapor and aerosol optical thickness [cit] . o2 686 695 h2o 713 734 o2 757 770 h2o 806 840 h2o 888 997 h2o 1087 1176 o2 1223 1285 h2o 1300 1521 co2 1591 1620 h2o 1759 1982 co2 1991 2038 co2 2037 2079 ch4 2139 2400"
"our work mainly targets at incorporating business process control and automation into the rfid-enabled applications. to achieve this objective, we propose to create a middleware system lying across some components on layers 2 and 3, as highlighted by shadows in figure 1 . the middleware system is aimed to connect and mediate the bpm and the edge-level devices, such as folklift cars, hand-picking or automatic-picking devices, etc. a novel process model is proposed to describe the behaviours of the tagged objects and the inter-relation between them from an object-oriented perspective. this process model takes into account blending rfid tag read events and event patterns into business logic modelling, and thereby enables the awareness to the runtime dynamics of the environment. by customising the deployed business rules and data schema, the middleware system can easily be updated to adapt to new requirements with the same edge infrastructure. this feature considerably enhances the system agility and flexibility and reduces the cost of adopting rfid technologies to existing legacy application systems."
"these two examples are only used to demonstrate the deployment of business rules, and they do not list the full details of the contexts due to the space limit."
"memristor networks that use binary memristors as building components will technically result in a discrete memory capacity as either component can be in the off or on state. the overall resistance value can be calculated for every combination, which is a limited number of possible resistances. however, with sufficiently large grids, this effect can be neglected as the individual operational variances of the elements are also summing up, resulting in a complex macro-characteristics."
". f # s £ s describes the state transition behaviours of the class; based on s and f, each class can build up a state transition diagram (std), which represents the stages in the lifecycle of its instances, while an object maintains this std to indicate the progress stage. for example, figure 3 shows the content of class \"assembly line\" and its corresponding std."
"the cc decreased with the addition of awgn (figure 10a ). at snr values below 9:1, the cc was under 0.9. as the snr increased, the cc raised in an asymptotic fashion. after reaching an snr of 1000:1, the cc equilibrated at approximately one. the cc remained constant at one for all linear transformations (figure 10b,c) . as the spectral shift increased from 0-10 nm, the cc decayed from a value of 1-0.991 (figure 10d) . a similar result was found after the atmospheric absorption feature at 935 nm was modified. in this case, the cc reduced from one to a value of 0.9970 as the scaling factor increased ( figure 10e) ."
"for the casi imagery, the ccs systematically reduced in value by more than one standard deviation near the edges of the fov. this reduction was largest for the casi data derived from the original processing methodology. when compared to the imagery collected on the 23rd, the casi data from the 24th were characterized by more substantial reductions in the ccs near the edges of the fov, especially along the left side. the ccs for the sasi imagery were almost identical, regardless of the processing methodology or the acquisition date. the ccs for the sasi imagery were consistently lower than the mean across the fov from pixels 548-564. as seen in figure 13b,d, this reduction appeared to be parabolic in nature, reaching a minimum value of approximately 0.995 and 0.997 in the sasi imagery from the 23rd and 24th, respectively."
"as a final test of consistency, the standard deviation of the cc was assessed in the presence of noise. in particular, the awgn transformation in table 1 was applied to ( ) 1000 times. a cc was calculated between ( ) and each of its transformations. the standard deviation of the ccs from each distinct snr was calculated."
"by characterizing the sensitivity of the cc before its application to real airborne hsi data, it was possible to verify the detective capabilities of the metric in the localization of errors in hyperspectral data. the findings generally agreed with all basic intuition and theoretical expectation of the cc. linear transformations, in agreement with theory, had no impacts on the value of the cc. by calculating the cc between two similar spectra, the value could be used to gauge the consistency independent of the effects associated with linear transformations. because of this property, the cc was shown to be extremely insensitive to the natural variances between different asphalt spectra. this was important for the detection of errors in hsi data as it implied that the differences in the calculated ccs were not primarily due to the variations between asphalt samples. all modifications, aside from the linear transformations, resulted in a consistent reduction in the cc. consequently, the cc could detect spectral shifts and modified spectral features. although the cc was sensitive to signal noise, all general trends held irrespective of the awgn in hyperspectral data with an snr of 100:1, which is a reasonably high noise level for airborne hsi data. this trend was fundamental to the application of the cc as it meant that the metric was sufficiently resistant to noise for the purposes of error detection; so long as errors are not being completely masked by noise, the cc can detect their presence. implementing this knowledge, the cc was applied to real airborne hsi data."
"the cc between the mean in-situ asphalt radiance and any given individual sample used to comprise the mean signal was very close to one, ranging from 0.99987-0.99998, with a standard deviation of 0.000023."
"we note that the above process is different from parse reranking [cit], in which λ(x n ) would stay the same for each x n in the training data across all epochs, and a reranker is trained on all fixed λ(x n ); whereas the xf1 training procedure is on-line learning with parameters updated after processing each sentence and each λ(x n ) is generated with a new θ."
"to accommodate to this object-oriented perspective, we reckon that the management over the deployed business processes should correspondingly conduct in a rule-based and data-driven manner. technically, the behaviours of objects are subjected to the defined rule set, and therefore by setting up multiple rules sets we can support objects to serve multiple business processes at the same time. by redefining the rules, we can customise the application context, and thereby reconfigure the edge systems to adapt to new requirements."
"one explanation of this phenomenon could be the following: under the threshold voltage, the device behaves as a very small capacitor. as the metal flows into the dielectric matter to build up the filament, the partially charged capacitor discharges, causing a short-time high-energy electric current burst. the other devices are sensitive to fast current changes and the filament forming is starting in them as well. it can be seen as a \"domino effect\" with the consecutive memristors. if any of the off state memristors in a series switches to the on state, the rest will automatically switch as well immediately after."
"although significance testing yielded p-values less than 10 −5 for all observed relationships, it is important to note that these values did not necessarily imply practical significance. this was due to an issue inherent to the p-value itself; with such a large sample size and small variance, significance testing flagged even the most subtle of changes as significantly different [cit] . fortunately, this was not an issue within the study as all of the flagged potential errors could be visualized and verified in the imagery itself. a similar statement can be made for the differences observed in the ccs between the distinct processing methodologies and acquisition devices."
"in order to maintain or even improve the virtual yield of the production, interconnected structures of the memristor network are proposed. these circuits and the presented measurement results provide a response to the above mentioned challenges. our proposed circuit constructions can be efficiently implemented on microchips, stacking the memristors of the circuit on top of each other. if a decent multilayer production technology arises with memristors, the disadvantage of the usage of several layers for the implementation of a single layer of memristor would be neglectable."
"to spectrally isolate the potential errors in the recorded spatial locations, the ccs across the roi were recalculated after removing the data in pre-defined spectral windows. the schematic in figure 7 was carried out for various spectral windows. the spectral windows were designed to vary in size and spectral location. the window sizes were selected to ensure that windows contained anywhere from 1 to half of the total spectral bands. for any given size, the window was spectrally located beginning at the lower boundary of the spectral range. each window was shifted by 5 nm until its edge surpassed the upper boundary of the dataset. for each window size and location, the average cc was calculated across the spatial regions associated with the detected potential errors. by maximizing the average cc over these regions, it was possible to identify the spectral window that was associated with a majority of the studied potential error."
"attempts to apply rfid in facilitating real enterprises' business running are often made by monitoring the material flows through reading events or collecting data from physical world. however, most of these solutions are lack of bpm support. this barriers rfid-enabled applications from the benefits of on-site responses according to business logics, system agility against changing requirements, seamless integration with enterprise application systems, etc. business processes blend business logics and related resources into reusable models for efficient transaction management. once such business logics are incorporated into rfid systems, it can provide real-time information in a network consisting of different enterprises, applications, and business partners in collaboration scenarios. consequently, such integration can enable automatic reactions and execution based on the captured real-time information while eliminating manual inputting, processing, and checking of information [cit] . as a milestone towards such integration. boeing has already embarked on a new assembly paradigm that is focused on using rfid and collaborative processes for its 787 (formerly 7e7 dreamliner) aircraft to reduce the assembly time [cit] . the integration of rfid and business logics seeks methodological advances and facilitating architectures to merge the wire-level deployment and business-level applications under one umbrella."
"to date, the cc has been widely implemented to investigate spectrographic imaging data [11, [cit] . in these efforts, the literature has concentrated on applying the statistical tool to establish bands that linearly associate with quantifiable physical and chemical properties. exploiting the linear relation, this method of band selection has been used to create and improve predictive models that associate hyperspectral data with useful parameters [11, 25, 27, [cit] 34, 35] . for example, [cit] applied the cc to establish bands that strongly correlate with forest leaf area index, improving predictive models at the landscape level. to a lesser extent, the cc has been applied for the purposes of data reduction and correction [cit] outlined a corrective method for hsi data that relied, in part, on the cc. the correction accounted for the effects of the spectral smile, a spectral non-uniformity in the cross-track direction that is caused by the optical design of the spectrometer and results in per pixel changes in wavelength registration across the field-of-view (fov) [cit] . in the study, the cc was used to measure uniformity levels across the fov, indirectly assessing the effects of the spectral smile defect. a corrective solution was selected by maximizing this metric. from this application, the cc was shown to be a useful tool in the assessment of hsi data. following this example, the cc can be used for the detection and quantification of other errors. this was exemplified by tanabe and saeki [cit], who rigorously quantified the sensitivity of the cc to spectral shifts in infrared spectra. such research was fundamental to the application of the cc for error detection in infrared spectroscopy. unfortunately, the findings were somewhat limited in their application to hyperspectral remote sensing as the study was conducted in an ideal environment with a laboratory-grade spectrometer. earth observation (eo) remotely-sensed measurements are most often collected with airborne spectrometers under less than the ideal conditions. before the cc can be confidently applied to hyperspectral eo data, the sensitivity of the tool needs to be characterized with respect to various potential errors and noise levels."
"the ccs between ( ) and each of the transformed datasets outlined in table 1 are recorded in figure 10 . the cc decreased with the addition of awgn (figure 10a ). at snr values below 9:1, the cc was under 0.9. as the snr increased, the cc raised in an asymptotic fashion. after reaching an snr of 1000:1, the cc equilibrated at approximately one. the cc remained constant at one for all linear transformations (figure 10b,c) . as the spectral shift increased from 0-10 nm, the cc decayed from a value of 1-0.991 (figure 10d) . a similar result was found after the atmospheric absorption feature at 935 nm was modified. in this case, the cc reduced from one to a value of 0.9970 as the scaling factor increased ( figure 10e) ."
"this distributed centre represents a typical rfid-\"rich\" environment, where products, pallets, equipments, and tools are all attached with rfid tags. the deployed readers in inventories, packing lines, transporting vehicles, etc. constitute a network that monitors the movements of products, pallets, vehicles, etc. in this scenario, using rfid with the goods-packing process in a distributed centre can bring the following benefits:"
"this paper looked into the incorporation of business process control and automation into rfid-enabled applications. a framework was proposed to model business interactions from an object-oriented perspective, which decomposed business processes into business rules which regulate the interaction behaviours between involved classes. this framework adopted a purely event-driven mechanism to capture the run time dynamics in the rfid-applied environment. an architecture design was also given with the emphasis on the interoperability with rfid edge and application systems."
"in the rfid-applied environment, the continuous tag read event series reflects the movements of objects, while these movements can be interpreted into business meaningful events using event patterns. according to the business rules, the objects will response to these elicited events by updating their internal status or invoking external operations. thereby, the business process is fulfilled in form of the interactions between these objects."
"the brnn introduces two new parameter matrices u and w and replaces the old hidden-to-output 5 in principle, only 1 re unit is needed, but we use 9 additional units to handle non-standard ccg rules in the treebank. matrix v with v to take two hidden layers as input. [cit], namely word, suffix and capitalization, and all features are extracted from a context window size of 7 surrounding the current word."
"for each hyperspectral image, the calculated ccs recorded in figure 13 adhered very closely to one across the fov when calculated with respect to the spectrum from the center pixel. the ccs for the casi imagery were consistently lower than that of the sasi by an average value of 0.0021 ( figure 13 ). in addition, the average standard deviation in the ccs of the casi data was over 18-times larger than that of the sasi."
"encapsulation. our object-oriented business model allows each class to encapsulate the implementation of its operations/methods. in addition, a composite class can encapsulate the related classes, involved event patterns and business rules into a new class. this new class hides the internal-handling details, and only provides the incorporating bpm into rfid systems necessary interfaces for interaction with other classes. this feature enhances the integrity of the entity, as well as reduces the system complexity by limiting the interdependencies between entities. the details of this feature will be discussed in the following section."
"(1) terms. event e, denotes the action of the system throwing a message. the event used in this model can be either raw rfid tag read event or system-generated event. (2 . changesto(c, e, s, t) -an instance of class c changes to state s after event e at time point t;"
", where s θ (y ij ) is the softmax action score of y ij given by the rnn model. for each y i, we also compute its sentence-level f1 using the set of labeled, directed dependencies, denoted as ∆, associated with its parse item. (we assume f1 over labeled, directed dependencies is also the parser evaluation metric.) 3. we compute the negative expected f1 objective (-xf1, defined below) for x n using the scores obtained in the above step and minimize this objective using sgd (maximizing the expected f1 for x n ). these three steps repeat for other sentences in the training data, updating θ after processing each sentence, and training iterates in epochs until convergence."
"let us examine an rfid-enabled enterprise application. as a typical chain of logistics processes, a dc is often selected to discuss rfid deployment [cit], since a distributed centre assembles a large volume of shipments every day using pallets. here, we also use this case to illustrate the nature of an rfid-enabled application."
"the described pre-processing methodologies utilized nist traceable calibration data provided by the sensor manufacturer. using the initial calibration data, various artefacts were identified in the resulting calibrated imagery. independent of this study, the processing methodology was updated to refine the steps described above, resulting in new calibration programs and calibration data files. this refined processing removed many of the identified artefacts in the data. the cc analysis was performed on the raw imagery after being processed with both the initial and refined calibration files and methodologies. overall, the study examined 8 datasets: the four raw hyperspectral images collected by the casi and sasi over the two acquisition dates processed with both the original and refined processing methodologies."
"to verify the spectral window and specify the nature of the potential errors, the imagery was visualized for a single band within the identified spectral ranges. in this visualization, image intensities were histogram equalized to enhance contrast by making the histogram of the resulting image equalized to a constant value. to verify that the reductions in the ccs were associated with these errors, the ccs were calculated across the fov with respect to the center pixel after the removal of the identified spectral region. theoretically, the ccs should be exactly 1 across the fov. although this is not the case in real data, the cc between well-behaved target spectra will vary around a mean value that is still quite close to 1. the spatial pixels associated with substantial reductions in the ccs were recorded as potential locations for errors in the hsi data. substantial reductions were characterized by ccs that fell below a designated threshold that was derived from the sensitivity testing."
"in binary memory applications, three important properties should be considered. the first one is having two clearly distinguishable states and these state declarations should apply to every element in a memory array. the second one is having a fast switching speed between the states. to reach the performance of the current complementary metal-oxide-semiconductor (cmos) technology's ram the switching speed should be less than 10 ns. the third one is cycle endurance, which is the number of write-erase cycles without permanent device failure."
"in this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model ( §2.4). we abstract away from the details of ccg and present the models in a canonical shift-reduce parsing framework [cit], which is henceforth assumed: partially constructed derivations are maintained on a stack, and a queue stores remaining words from the input string; the initial parse item has an empty stack and no input has been consumed on the queue. parsing proceeds by applying a sequence of shift-reduce actions to transform the input until the queue has been exhausted and no more actions can be applied."
". record association hierarchy (association relationship of serialised cases with the corresponding pallet, pallets with the corresponding shipment). this information can be used for validating shipments and also tracking recall goods or counterfeit goods."
"in the case of ann applications, more deviance could be tolerated, but many state devices are needed, so the memristors developed for binary or multi-state memory purposes will not be sufficient."
"rfid data/event management and context-sensitive middlewares a lot of research efforts have been put to tackle rfid complex event processing, yet most of them mainly focus on data cleansing and filtering. work \"stream-based and shared event\" (sase) processing has defined an sql like complex event language to aggregate rfid events [cit] . the implemented sase system uses a persistence storage component to support querying over historical data and to allow query results from the stream processor to be joined with stored data. [cit] to improve the performance of continuous query processing over rfid event flows. [cit] have addressed the query issue from the perspective of energy efficiency. [cit] have investigated the temporal management of rfid data. they have adapted traditional database query techniques to the temporal relationships of rfid data, and thereafter defined a set of temporal complex event constructors in their follow-up work [cit] . two partitioning mechanisms have also been proposed in their work to support efficient queries. however, none of the mentioned works have provided an explicit solution on how to handle the delayed effects in event management, or how to integrate business process automation into rfid event management."
"the deduction system (fig. 2 ) of our shift-reduce parser follows from the transition system. 4 each parse item is associated with a step indicator ω, which denotes the number of actions used to build it. given a sentence of length n, a full derivation requires 2n − 1 + µ steps to terminate, where µ is the total number of un actions applied. [cit], a finish action is used to indicate termination, which we do not use in our parser: an item finishes when no further action can be taken. [cit] omit the ∆ field in each parse item, due to their use of a context-free, phrasestructure cover, and dependencies are recovered at a post-processing step; in our system, we build dependencies as parsing proceeds."
"as a final test of consistency, the standard deviation of the cc was assessed in the presence of noise. in particular, the awgn transformation in table 1 was applied to ( ) 1000 times. a cc was calculated between ( ) and each of its transformations. the standard deviation of the ccs from each distinct snr was calculated."
"to further the analysis and spectrally locate smaller residual errors in the casi data from the 24th, five atmospheric absorption features were identified in the spectral range from 365-1050 nm (table 4) . the ccs across the fov of the casi images from the 24th were calculated with respect to the center pixel over the spectral regions identified in table 4 and are shown in figure 18 . the ccs in the 680-712 nm region were highly variable, ranging from 0.95-1 with a subtle low frequency sinusoidal structure (figure 18a,b) . visual inspection of the associated imagery in figure 19 indicated that, throughout much of the fov, there were discrete pixels and groups of pixels that appeared to be non-uniform across the entire fov, noticeably varying in brightness even amongst neighboring pixels. these pixels lead to \"striping\" artefacts across the entire fov in the image data. these trends were apparent in both casi images. the low frequency sinusoidal structure could not be clearly visualized in the imagery. the sinusoidal structure was not a numerical computational effect. table 3 . the casi data from the 23rd were relatively consistent between both processing methodologies. although this trend generally held for the data from the 24th, (c) showed a notable offset. (b,d) the large reduction in the ccs from the sasi imagery at pixels 548-564 was not present after removing the problematic band that was found between 993 and 1008 nm."
. recover recall or counterfeit goods. picking and packing are the stages in a distributed centre where cases still remain as individual. the systems in
"this general trend held for all casi imagery and was less prominent with the refined processing methodology (figure 15 ). the asphalt road is still brightest along the edge pixels, but to a lesser degree than in figure 14 ."
"the index of dispersion has been used as the measure of unreliability. it formulates as the sum of the variance of the signal, normalized by the amplitude of the signal, due to the expectation that higher amplitude signals have naturally higher variance. this measure is valid only for positive data points. for this reason, the absolute value of the signal has been used:"
"we have conducted a pilot installation of the proposed model on a packing station at a dc to evaluate if and how our approach can support rfid application contexts in practice. the basics of this scenario have been introduced in the motivating example section. rfid sensors are fixed to the receive gates of the assembly line, and the tagging is done at item level. in this section, we use a counterfeit-checking process and the aforementioned pallet-packing process to illustrate the deployment in details. a counterfeit product copies the rfid tag of a sold genuine product, and therefore it has the same tag id with the genuine one. when it passes a reader, it can be identified by searching the tag ids of previously sold products, and be picked out in the end. example 2 lists the key content of the counterfeit checking context, which is to be deployed to the edge system at the assembly line: events: counterfeitfound -a counterfeit is found; arrives -a product arrives to the assembly line."
"two new types of memristor networks have been introduced, which are able to emulate more reliable memristors. measurements have been successfully carried out for both the previously presented networks and the new networks. the measurements provided new information about the macro-characteristics of memristor networks compared to the previous simulations. the increased switching speed of memristor networks should be further investigated. this solution can be used with existing devices to support the implementation of neuromorphic applications."
"the authors declare no conflict of interest. the funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results."
"it is dramatically important to assess the strategic business value of integrating radio frequency identification (rfid), a key connective technology, into enterprise system applications [cit] . nowadays, the increasing adoption of rfid is evidenced in retailing, manufacturing, supply chain, military use, health care, etc. particularly in the background of business globalisation commoditisation [cit] . [cit] expects that the total rfid market value (including all hardware, systems, integrations, etc.) across all countries will rise from $4.96 [cit] to $26.88 [cit] ."
"as a final test of consistency, the standard deviation of the cc was assessed in the presence of noise. in particular, the awgn transformation in table 1 was applied to ( ) 1000 times. a cc was calculated between ( ) and each of its transformations. the standard deviation of the ccs from each distinct snr was calculated."
"an rfid application context corresponds to a self-containing and self-acting encapsulated entity which can invoke the operations of edge systems or external systems in response to the real-time events. figure 5 shows the relationships between the notions of the proposed model. the shadowed area represents the static part of the rfid application context, which consists of classes, rules, and event patterns. a composite class may contain both base class(es) and composite class(es), and a class can inherit the characteristics of another class by extending the latter. event patterns can extract the business meanings from tag read events, and with event patterns the rules can define the conditions of state transitions or operation invocations. the unshadowed area represents the run time part, which describes the interactions between objects. the status and attribute values of an object indicate the progress stage in its lifecycle. an rfid reader sends reading events when it observes a pass-by rfid-tagged object."
"we explain the application of the rnn models to ccg by first describing the ccg mechanisms used in our parser, followed by details of the shift-reduce transition system."
"based on this conceptual architecture, we enrich it into a detailed middleware system design by extending it with necessary components, in accordance to our object-oriented rfid model. in figure 7, the shadowed part shows our middleware system design. this middleware system collaborates with edge systems, rfid information services, and enterprise applications. an rfid edge system directly interacts with the real world. the rfid information services are deployed at both the incorporating bpm into rfid systems edge side and the enterprise side, and these services together construct a distributed on-demand repository of information related to individual rfid tags. the enterprise applications may include enterprise resource planning, product lifecycle management system, etc. detailed introduction of these systems can be found from epcglobal's documents. the designed data-driven middleware receives rfid tag read events from the edge systems, maintains the rfid application context, and triggers proper operations according to the monitored dynamics of related rfid objects."
"to train the rnn parser, we used 10-fold cross validation for both pos tagging and supertagging. for both development and test parsing experiments, we used the c&c pos tagger and automatically assigned pos tags. the brnn supertagging model was used as the supertagger by all rnn parsing models for both training and testing. f-score over directed, labeled ccg predicate-argument dependencies was used as the parser evaluation metric, obtained using the script from c&c."
"remote sens. 2018, 10, x for peer review 6 of 26 table 1 . the five modifications applied to ( ) to generate the transformed signal, ( )."
"for each hyperspectral image, the calculated ccs recorded in figure 13 adhered very closely to one across the fov when calculated with respect to the spectrum from the center pixel. the ccs for the casi imagery were consistently lower than that of the sasi by an average value of 0.0021 ( figure 13 ). in addition, the average standard deviation in the ccs of the casi data was over 18-times larger than that of the sasi."
"the measured memristor devices are made of ge 2 se 3 (germanium-selenide) and ag (silver) based chalcogenide dielectric with w (tungsten) conductors. the devices have a switching threshold, meaning that under a certain threshold voltage (0.1 v in our case), their state does not change. this feature makes the memristor implementation desirable for applications where reading the state should not change the state itself. on the other hand, usually it has very few metallic dendrites, which makes the characteristic very coarse. the memristors are current-controlled and the typical writing-erasing voltages are 2.5 v. one of the consequences of being current-controlled is that the erasing process is faster than the writing process."
"this work substantiated the versatility of the cc with respect to the localization of errors in spectrographic imaging data. the sensitivity of the cc was characterized with respect to subtle spectral changes in the averaged in-situ level radiance data. errors were spectrally and spatially detected in real airborne acquired hsi data. as per the original intent of the study, the methodology was successfully developed for the detection of errors, not for the identification of their origin. the method was able to gauge the effectiveness of various processing methodologies and the imaging systems themselves. overall, the cc is clearly a strong, simple, low monetary cost, analytical tool for studying hyperspectral remotely-sensed data quality through error detection."
"in crossbar-network applications, a certain amount of uniformity of the memristors is necessary. the programming voltage and current levels are the same for every element and thus one expects that they will behave similarly for the same input signals."
"the spatial locations associated with distinct reductions in the ccs were identified using the threshold defined in section 2.4. these locations were used to spectrally isolate the potential errors to the windows identified in table 3 using the windowed-based methodology described in section 2.4. at a snr of 100, the standard deviation is approximately 0.001. the standard deviation in the cc asymptotically increased from zero to approximately 0.22 when the snr decreased through the introduction of awgn."
"the purpose of this study was to use the cc to develop an easy to implement methodology to detect issues with hsi data. the methodology was intended explicitly for the detection of errors, not for remote sens. 2018, 10, 231 3 of 26 the identification of their origin. although other error detection methodologies exist (e.g., [cit] ), they can be expensive to implement and rely on a higher level of mathematical understanding. to develop a novel method, the cc was first characterized with respect to artificially-induced errors in ground data. afterwards, this information was applied to locate the spatial location and spectral bands associated with errors in real hsi data. the overall objective of this study was to substantiate the cc metric as a low monetary cost, robust and simple statistical tool in the quality assessment of eo hsi data through the detection of errors."
". a class abstracts the attributes and behaviours of a kind of entities, while an object denotes an instance materialised with actual business data at run time."
"definition 1 (base class). a base class abstracts the behaviours, attributes of a type of entities, such as pallets, forklift cars, etc. formally, a base class can be defined as tuple (c, a, m, s, s, t, f), where:"
"bpm into rfid systems distributed centre will check whether the goods in distributed centre are of those types. if so, picking or packing systems will hold the serialised identification of those products and inform operators to pick those products when reads them."
"to align the collected object movement information with business, bpm methodologies have to adapt themselves to the aforementioned characteristics. in the rfid-applied environment, edge systems are responsible for the most on-spot business operations, while they monitor the object movements [cit] . correspondingly, our deployment strategy is to enable the awareness of edge systems to be both real-time context and business logics. therefore, the edge systems can react to the contextual dynamics with proper business operations intelligently, and as such the business efficiency and effectiveness can be improved."
"bpm into rfid systems such a composite class builds up a self-maintained unit, which owns better reusability by hiding the internal complexity, and thereby enhances the scalability of rfid system integration. for example, assembly lines and related pickers, forklift cars, drivers, and operators can join into a packing station. such a packing station can handle the packing process from picking products, transferring pallets, and packing pallets. at conceptual level, we define a composite class to represent such combined entities. figure 4 shows the content of composite class \"packing station\" and its std, which composes the stds of its component classes."
"more generally, both lexical and non-lexical ccg categories can be either atomic or complex: atomic categories are categories without any slashes, and complex categories are constructed recursively from atomic ones using forward (/) and backward slashes (\\) as two binary operators. as such, all categories can be represented as follows [cit] :"
"rfid provides the real-time object-level information for business processes to execute in a prompt and precise manner. at the same time, rfid-enabled applications also bring the following distinct characteristics from traditional applications:"
arrives -a product arrives to the assembly line; sentoff -a pallet of packed products are sent off; packs -a product is packed in a pallet; unpacks -a product is unpacked from a pallet.
"current rfid technology has been established with emphasises on network infrastructure, data/event communications, and information sharing [cit] . with the rfid facilitating equipments and systems, such as reading devices, rfid edge systems, federated rfid information service systems, etc. enterprises are enabled to automatically sense and react to the real world [cit] . yet, how such awareness improves the business effectiveness and efficiency is subject to the extent that how the rfid systems are integrated to the existing enterprise backend application systems and how these backend systems utilise the collected rfid data. in practice, this integration can be technically interpreted into how to incorporate business process management (bpm) into rfid systems, and how they together integrate people, information, processes, and products across traditional organisation and application boundaries."
"in our pilot installation, we set up a series of boolean variables (fluents) to characterise the status of objects. the querying engine is implemented on ibm discrete event calculus ( system architecture as mentioned in the \"introduction\" section, the event management layer and the bpm component lie between the edge systems and enterprise application systems. following this scheme, we deploy the facilitating data-driven middleware system to couple rfid tag read events and system operations together and thereby connect and mediate the business process execution and the edge-level devices."
"in order to enlarge the search space of the greedy model thereby alleviating some search errors, we experiment with applying beam search decoding during inference; and we observe some accuracy improvements by taking the highest scored action sequence as the output (table 3 ). however, since the greedy model itself is only optimized locally, as expected, the improvements diminish after a certain beam size. instead, we show below that by using the greedy model weights as a starting point, we can train a global model optimized for an expected fmeasure loss, which gives further significant accuracy improvements ( §5)."
"more formally, we define the loss j(θ), which incorporates all action scores in each action sequence, and all action sequences in λ(x n ), for each x n as"
"the previous simulation results suggested that the switching speed could decrease using memristor grids. surprisingly, the switching speed did not decrease, but increased instead. the networks are approximately three times faster than a single memristor. this is fairly unexpected, as the control voltage stayed constant in both measurements, which means that the voltage on any single memristor in a network measurement had to be strictly lower than in the case of a single device measurement at any given time during measuring."
"after removing the data within the spectral windows identified in table 3, there was a greater degree of consistency amongst all of the casi and sasi images. that being said, not all datasets perfectly aligned; there was a slight offset between the casi images collected from the 24th. to investigate the discrepancy in the casi images from the 24th, finer errors were detected in the regions that surrounded the five atmospheric absorption features in table 4 . all but one of the spectral regions was characterized by non-uniform ccs across the fov (figure 18 ). the irregular structure in figure 18a,b was caused by non-uniform pixels, which noticeably varied in brightness. this error created \"striping\" artefacts across the image data. these artefacts have been observed in the literature and are likely due to radiometric calibration errors [cit] . although the origin of the low frequency sinusoidal structure could not be established, it is clear that the trend is not a numerical computational effect. as such, there is likely a subtle wide spatial scale feature. the origin of the subtle feature in the ccs is still being investigated. the sporadic reduction in the ccs of figure 18c detected errors at pixel columns 1454 and 1456, which were visualized as a bright and dark vertical stripe, respectively, across the imagery (figure 20 ). since this reduction was not present in figure 18d, the refined processing methodology was able to correct for this error. based on the structure of the ccs near the edges of the fov in figure 18e,f, there were potential residual smile effects or other cross-track illumination effects that could not be clearly visualized in the imagery. the sporadic reductions in the cc of figure 18g revealed groups of non-uniform pixels that created distinct \"striping\" artefacts that can be seen at several points across the casi imagery from the 24th with the original processing ( figure 21 ). these errors were not present in figure 18h or its associated imagery. as such, the refined processing methodology was able to correct for this error. the relatively constant ccs across the fov in figure 18i,j corresponded with stable imagery within the designated spectral window, as displayed in figure 22 . this information is fundamental as it showcases that the cc method can detect stable imagery, when it is present. the offset between the casi imagery collected on the 24th in figure 17 was likely due to the additional errors that were not corrected in the original processing methodology."
"as a final test of consistency, the standard deviation of the cc was assessed in the presence of noise. in particular, the awgn transformation in table 1 was applied to ( ) 1000 times. a cc was calculated between ( ) and each of its transformations. the standard deviation of the ccs from each distinct snr was calculated."
"the current state of the hidden layer is determined by the current input and the previous hidden layer state. the weights between the layers are represented by a number of matrices: matrix u contains weights between the input and hidden layers, v contains weights between the hidden and output layers, and w contains weights between the previous hidden layer and the current hidden layer."
"the approximation of the yield of a production technology is highly dependent on the available number of samples of the given device. having a limited number of devices, this question can not be addressed, but it has been shown in a previous work [cit] that the yield of a production technology can be increased with this method."
"the mass production of devices, which can reliably fulfill these requirements, is not trivial. if the production yield of single devices is less than 100 percent (as they are not functioning as memristors or they are outside of the accepted range of parameters), then they can also affect the access circuit and the encompassing parts of the neuromorphic system."
"in regard to the semantic elicitation from sensor-captured data, context-sensitive middlewares have attracted quite some research efforts. [cit] have proposed a middleware to allow heterogeneous agents to acquire contextual information and uniform the knowledge using ontologies. [cit] have built a reconfigurable context-sensitive middleware for developing"
"based on the above presented measurements the following parameter values were acquired, presented in table 1 . the resistance values are the average on/off ratio values of the 50 cycle long measurement sequence. another important feature of th networks to note is the stronger nanobattery effect [cit] . this causes the visible shift of the zero current level after the erasing pulse. the nanobattery effect is undesired in most applications, but can be dealt with by an appropriate control voltage and timing. it can also be taken advantage of, in some scenarios."
". throws(e, c, t) -an instance of class c throws event e to the communication bus at time point t; and . discards(e, c, t) -drops event e from the communication bus at time point t, triggered by an instance of class c."
"the ccs of the sasi imagery were virtually identical regardless of the processing methodology and acquisition date. this suggested that the sasi was very stable in its data acquisition. furthermore, it was clear that the refined processing methodology did not have a large impact on the data. using the developed algorithms, an error was detected in the sasi imagery at a single spectral band by a reduction in the ccs from pixels 548-564. this showcased the developed cc-based methodology as a strong tool in the localization of errors in imaging spectrometers."
". t is a finite set of rifd classes, including both base and composite classes, with distinct names such that every class referenced in t also occurs in t;"
"first of all, it is important to differentiate two main types of memristors from a functional point of view. the first type is the analog purpose memristor (apm). it operates in the continuous domain, which means it can have any resistance (or conductance) value in its operational range. this might sound unrealistic as we know that at a very low scale, energy levels are quantized, but it can be interpreted as the memristor having so many states that can be considered as infinitely many. another formal definition is that an apm can store any real value between the normalized range of zero and one."
"the results for the h-fractal type of network are very similar to the newly introduced network regarding the writing of the state into low resistance position. however, this type of network has problems with erasing the state into the off state and could get stuck at an in-between state."
"our previously introduced circuit proposals [cit] were intended to convert several dpms into a single apm. this was tested through simulations, which showed that this circuit topology can achieve analog behavior when made from solely multi-state memristors. however, in this work real device measurement results are given, which proves that the same circuit can effectively convert several unreliable bpms into a more reliable one."
"another important property to consider is the used chip area. these networks should be implemented efficiently on a chip as a two dimensional crossbar network. the implementation of the previous networks was only possible using sixteen times more chip area for the emulation of a single device. the new network uses only four times more area with a similar reliability gain, as compared to a single memristor."
". the rule repository stores event patterns, business rules, and domain-specific predicates. these data can be used to generate queries to elicit business meanings from large volumes of rfid tag read events, derive out implicit state transitions, and determining operation invocations. the event manager acts as the main driver of the middleware system. at one hand, it receives the tag read series from rfid edge systems; at the other hand, it generates proper queries using the event patterns, predicates, and rules stored in the rule repository. these queries can elicit the business meanings from the tag read series, and according to these query results the event manager will coordinate the update controller and the invocation trigger for further processing, or report the elicited business events to enterprise application systems. this architecture design emphasises the event handling and business logic deployment. the data-driven middleware system monitors the run time object movements through the received events, analyses the influences of these events, and updates the status of involved objects or invokes operations of edge systems or external systems according to the defined business rules. this architecture provides a reference scheme for integrating edge systems and legacy application system with business process supports. by changing the business rules and schema, the middleware system can support different business application contexts with the same edge infrastructure."
". improved productivity of business processes. for example, rfid might be used to simultaneously read all of the cartons on a pallet as it passes through a portal, or read all of the serial numbers virtually at once as a pallet of goods leaves a production cell."
"in the casi imagery, the methodology was able to spatially detect errors along the edges of figure 13a,c by systematic reductions in the ccs near the boundaries of the fov. the spectral locations of these effects were found in the blue end of the spectrum, in accordance with table 3 . visualization of the imagery in figures 14 and 15 revealed an error that is consistent with the effects of the spectral smile or other cross-track illumination effects [cit] . with a greater decline in the ccs near the edges of the fov, this error was more prominent in the casi data collected with the original processing methodology. as such, it is possible to deduce that the refined processing was able to better correct for the effects observed at the edges. the casi imagery from the 24th was characterized by slightly lower and more variable ccs then the data from the 23rd, especially near the edges of the fov. with this information, there is some innate variability in the data acquisition of the casi that could be quantified from the ccs."
"from the technical perspective, figure 1 [cit] . this reference architecture primarily classifies five logical layers at a conceptual level, which depicts an ideal integration from rfid wire level to enterprise application level. in this architecture, layer 0 consists of the rfid reading devices and some on-site functioning equipments on the \"edge\" of the rfid enabled systems; layer 1 consists of the basic operating environment and platform of the rfid-enabled solution architecture. at this layer, the rfid data receives the primitive processing, such as cleansing, aggregating, filtering, etc.; incorporating bpm into rfid systems layer 2 and the layers above it are essentially the enablers of the business processes and solutions that can leverage the real-time data generated by lower layers to drive business activities; layer 3 provides the encapsulated services that are implemented as the \"abstraction\" of the layer(s) below it, and can be implemented as application programming interface services and/or web services; layer 4 leverages the services, data and tools provided by the layers below to implement application solutions."
"if the production yield of single devices is less than 100 percent (as they are not functioning as memristors or they are outside of the accepted range of parameters), then they can also affect the access circuit and the encompassing parts of the neuromorphic system."
"(1) how business processes help navigate edge systems to respond to the tag read events with proper operations immediately? (2) how to \"inject\" business logics to edge systems to help them filter tag read events intelligently?"
"we also introduce a simple recurrent neural network (rnn) model to shift-reduce parsing on which the greedy baseline and the global model is based. [cit] ), [cit] using elman rnns [cit] ."
"following the object-oriented paradigm, we abstract the entities involved in a business scenario, such as products, equipments, staff, tools, related documents, etc. into classes. the following notions are used to characterise business processes in the rfid-applied environment:"
"in process-centric approaches, each process is designed to describe the procedure of fulfilling a business function. according to the embedded business logic, such a process may involve many related organisational units, staff, services as well as items, and thereby create a complex model. normally, such models can be represented as easily readable diagrams. in comparison, object-oriented approaches do not follow the control flow between activities. instead, they model the business contexture and behaviours with objects and related rules. each involved object has its own lifecycle, and its behaviours are regulated by rules. a group of objects collaborate together to achieve a given goal, and such collaboration creates a business contexture. table ii lists the comparison between these two kinds of approaches."
"to spectrally isolate the potential errors in the recorded spatial locations, the ccs across the roi were recalculated after removing the data in pre-defined spectral windows. the schematic in figure 7 was carried out for various spectral windows. the spectral windows were designed to vary in size and spectral location. the window sizes were selected to ensure that windows contained anywhere from 1 to half of the total spectral bands. for any given size, the window was spectrally located beginning at the lower boundary of the spectral range. each window was shifted by 5 nm until its edge surpassed the upper boundary of the dataset. for each window size and location, the average cc was calculated across the spatial regions associated with the detected potential errors. by maximizing the average cc over these regions, it was possible to identify the spectral window that was associated with a majority of the studied potential error."
"since the theoretical [cit] and practical [cit] discovery of memristors, they have been extensively studied [cit] as elementary building blocks for artificial intelligence and neuromorphic computing applications."
"the spatial locations associated with distinct reductions in the ccs were identified using the threshold defined in section 2.4. these locations were used to spectrally isolate the potential errors to the windows identified in table 3 using the windowed-based methodology described in section 2.4. table 3 . spatial and spectral localization of large imaging errors. spatial errors were detected from the data in figure 13 using the defined threshold. potential errors were spectrally located through the window-based methodology described in section 2.4. errors were detected along the edges of the casi imagery in the blue end of the spectra. a single band error was detected in the sasi imagery from 993-1008 across pixels 548-564. errors in the imagery were clearly detected though the visualization of the spectral windows in table 3 . an example of the error in the casi imagery is displayed in figure 14 . in the imagery, the asphalt road is clearly brightest along the edge pixels. (a,c) the casi imagery was characterized by systematic reductions near the edges of the fov. these revealed potential errors consistent with the spectral smile effect and other cross-track illumination effects. this reduction was most substantial for the casi images generated by the old processing methodology. compared to the imagery from the 24th, the ccs were more uniform across the fov for the casi data from the 23rd. (b,d) the sasi imagery was consistent across all dates and processing methodologies. there was a notable reduction in the ccs across the fov from pixels 548-564. this revealed the spatial location of an error. table 3 . spatial and spectral localization of large imaging errors. spatial errors were detected from the data in figure 13 using the defined threshold. potential errors were spectrally located through the window-based methodology described in section 2.4. errors were detected along the edges of the casi imagery in the blue end of the spectra. a single band error was detected in the sasi imagery from 993-1008 across pixels 548-564. errors in the imagery were clearly detected though the visualization of the spectral windows in table 3 . an example of the error in the casi imagery is displayed in figure 14 . in the imagery, the asphalt road is clearly brightest along the edge pixels."
"remote sens. 2018, 10, x for peer review 6 of 26 table 1 . the five modifications applied to ( ) to generate the transformed signal, ( )."
"this paper is organized as follows: after the above problem proposal, the measurement environment is introduced and explanatory discussion is given about our circuitry. the third section contains the proposed circuits and the measurement results that are more detrimental to the yield. this circuitry effectively addresses the proposed task. in the fourth section, the results are summarized and analyzed. the article is closed with a brief summary of the results in the conclusion section."
"before the cc was applied to the airborne imagery, the sensitivity of the statistical tool needed to be characterized with respect to the natural variances within asphalt spectra. this was accomplished by calculating the cc between each of 10 raw in-situ hyperspectral radiance measurements and their averaged spectral response."
"we apply our models to ccg, and evaluate the resulting parsers on standard ccgbank data [cit] . more specifically, by combining the global rnn parsing model with a bidirectional rnn ccg supertagger that we have developed ( §4) - [cit] . finally, although we choose to focus on shift-reduce parsing for ccg, we expect the methods to generalize to other shift-reduce parsers."
"is the standard softmax gradients. next, to compute δ y ij, which are the error gradients propagated from the loss to the softmax layer, we rewrite the loss in (1) as"
"to pretrain the greedy model, we trained 10 crossvalidated brnn supertagging models to supply supertags for the parsing model, and used a supertagger β value of 0.00025 which gave on average 5.02 supertags per word. we ran sgd training for 60 epochs, observing no accuracy gains after that, and the best greedy model was obtained after the 52 nd epoch (fig. 3b) ."
"definition 7 (composite class). a composite class encapsulates multiple classes with related rules into a new class, which inherits the attributes and behaviours of the constitute classes. formally, a composite class can be defined as tuple (c, p, r), where:"
"for the casi imagery, the ccs systematically reduced in value by more than one standard deviation near the edges of the fov. this reduction was largest for the casi data derived from the original processing methodology. when compared to the imagery collected on the 23rd, the casi data from the 24th were characterized by more substantial reductions in the ccs near the edges of the fov, especially along the left side. the ccs for the sasi imagery were almost identical, regardless of the processing methodology or the acquisition date. the ccs for the sasi imagery were consistently lower than the mean across the fov from pixels 548-564. as seen in figure 13b,d, this reduction appeared to be parabolic in nature, reaching a minimum value of approximately 0.995 and 0.997 in the sasi imagery from the 23rd and 24th, respectively."
"remote sens. 2018, 10, x for peer review 20 of 26 the fov. this ripple is clearly visible near the center of the figure, as indicated by the red arrow."
"the checkerboard type of network was practically unable to switch its state significantly compared to other solutions. this was probably due to the limited number of parallel connections in the network, which produced less possible routes to open. higher control voltage could change its state, but the risk of device damage increases with the increased after-switch current. longer pulses could also help, but it makes the writing process slower."
"an important aim of this paper was to develop and demonstrate a framework (bee-coast) for reporting big data that describes emerging data through a whole systems obesity lens. the framework was shown to successfully summarise and communicate the important features of a number of data sources, including vital information about ownership and sharing, as well as content. it is suggested that this framework should be used to report big data sources used in research. it is also proposed that this framework could be used to develop a reference list of big data sources as a resource for future research, akin to published data resources profiles. its application may also be valid outside the obesity research area."
"we have seen that variables as diverse as physical activity behaviours, built environment features, food consumption and choice are all richly captured by emerging sources of data. however, in spite of the existence of these data, we may not yet be in a position to utilise them to their full potential due to restrictions around data access and linkage. while individual-level data exists (e.g., relating to physical activity behaviours), at times these data cannot be released at an individualised level due to confidentiality and anonymity restrictions. individual-level linkage is only possible with explicit consent. this has implications for data linkage, as data are often released (and thus must be linked) at a neighbourhood level, or larger, rather than at the individual level. such area-level linkage is less than ideal, as potentially important within-neighbourhood variability is lost, and analyses are subject to bias (e.g., the ecological fallacy). innovative approaches to data sharing and linkage finally, the cohort data and linked secondary data are released back to the researcher, with the participant addresses removed. this process was possible because participants explicitly consented to take part in the uk biobank study, which includes health data linkage. it is important to note that we have not explored the finer details of how such data sources might be linked and harmonised for research purposes."
"the basic drivers of obesity are simple (more energy consumed than expended); however, the aetiology is complex. it is now widely accepted that multiple factors, including physiological, social and environmental, act synergistically to drive obesity. these factors are often described as the 'obesogenic' environment (an environment that hinders sufficient physical activity and promotes excessive intake of food, thereby making obesity more likely). this may explain the limited success -at a population level -of traditional approaches to obesity prevention and management, which have tended to focus on behavioural, educational and pharmacological factors. for this reason, many researchers and policymakers are now advocating for 'whole systems' approaches to obesity prevention and management, which promote integrated systems to address obesity, rather than focusing on risk factors in isolation [cit] ."
"esrc strategic network for obesity members agreed that, for all new data sources, it is essential to provide some background (b) on the history and purpose of how and why the data are generated, including key features of the data. this is especially important when data are used in a context for which they were not initially collected. detailed description of the elements (e) of the data that are required for others to fully understand their potential application. these elements encompass detailed content (c), ownership (o), level of aggregation, for example individual, neighbourhood, regional or national (a), conditions related to sharing (s) and temporality (t) of the data. finally, these datasets should be illustrated using exemplars (e) to include the format of the data and indicative use cases (table 1) ."
"while the bee-coast framework goes some way to elucidating the available 'big data' for obesity research, further data sources still need to be made available to increase coverage of the foresight obesity system map. increased multidisciplinary may facilitate this. for example, while our data audit did not highlight much data relating to food production, data are certainly being captured across this domain, for example by satellites, the instrumentation of farms, and data driven control within the manufacturing process. such sources are not necessarily evident or accessible to a research community around obesity. interdisciplinary networks such as the esrc's strategic network for obesity, and repositories such as the cdrc, provide a long-term opportunity to ameliorate this difficulty."
"the list of data sources and how these map onto the foresight nodes can be seen in table 3 . overall, 86/108 of the foresight nodes are covered by at least one big dataset. when traditional cohort data is also included (uk biobank), this increases to 89/108 nodes. table 4 highlights the areas in which big data are, to the best of our knowledge, not readily available to map against foresight domains and nodes. we believe that information relating to many of these nodes would typically be generated by research studies, which often recruit, relatively speaking, small number of participants. whilst participant numbers may be small, the number of data points may be large. some of the nodes would likely require qualitative research to capture relevant data."
"this paper is one output from a collaborative network of academic researchers, industry partners, charity representatives and members from the public sector. we convened 40 members and hosted 5 [cit] ijo00672r. during these meetings, members shared: experiences of using big data for obesity research, knowledge of suitable data sources, and expert opinion on how to optimise this wealth of data. for the purpose of this paper, big data were defined synonymously with 'non-traditional' data; in other words, any data not collected specifically for academic research purposes. synthesis of expert opinion on optimising data, culminated in the development of a reporting framework. the purpose of this framework was to outline a structure for reporting the features of big data in obesity research, although its application may be valid outside the obesity research area."
"innovative approaches to research questions are required in changing political landscapes and big data presents valuable possibilities. while rcts are heralded as the gold standard in study design they are not applicable to all research questions. many research questions relating to obesogenic factors (e.g., social and built environmental variables) are better suited to observational rather than experimental. this challenge is compounded in that big data is generally inherently observational rather than experimental. thus we may need to look to alternative study designs combined with alternative and innovative methods of analysis. in particular, big data presents valuable possibilities for natural experiments to compare the experience of similar groups under different environmental conditions or subject to different interventions e.g., in different regions [cit] . spatiotemporal patterns can be investigated at scale for the first time without the challenges associated with longitudinal cohort study design and follow-up."
"eight exemplar data sources were included in this review as providing valuable data for use in a whole systems obesity research: (i) ordnance survey points of interest data; (ii) food standards agency food hygiene data; (iii) supermarket loyalty card data; (iv) physical activity applications/ wearables; (v) new technologies to record diet; (vi) acxiom data; (vii) cameo data from callcredit and (viii) yougov data. the features of these datasets are summarised in table 2 in accordance with the bee-coast reporting framework. collectively, the exemplar data sets were found to map to 56 foresight nodes, covering all 7 foresight domains."
"the individual and the technology company history yougov provides self-reported data from opinion polls which are collected four times each year from a large panel of 250,000 adults. the questions in the survey are a combination of fixed topics and commissioned content. the themes are extremely wide ranging. a complete catalogue of available data resources may be obtained on request from the data owner this field. furthermore, value does not only arise from the advances in research, but financially through reduction in cost of primary data collection, which can result in both researcher and participant burden. [cit] . use of the data in the context of retail consumption in times of austerity and the \"credit crunch\" [cit] . these academic studies have explored and reweighted for skews and variable quality of the individual returns whilst the broad coverage of big data across the obesity system map offers exciting possibilities for research, it is important to acknowledge that big data are not the complete solution to a whole systems approach. the remaining 20% of nodes on the foresight obesity system map were not directly featured in our data mapping exercise -for example, genetic and physiological variables relating to appetite control, metabolic rate and predisposition to obesity. many of these unmapped nodes represent data that are commonly collected in traditional research, and recent large-scale initiatives (e.g., uk biobank [cit] and other ongoing longitudinal cohort studies [cit] ) will continue to contribute important large-scale data. this suggests that big data should be used to supplement and enhance traditional datasets. indeed, this paper does not advocate for the use of big data in place of traditional data, but rather to complement traditional data and of course be considered in the context of the research [cit] ."
"internationally, a broader understanding of risk factors for obesity and increasing awareness of the social determinants have led to recognition of the need for more comprehensive, cross sectoral strategies to tackle obesity [cit] ."
"the reporting framework was then applied to eight exemplar data sets, to demonstrate its usefulness in communicating important data details. the foresight obesity systems map was indexed with domain and node identifiers (presented in the supplementary material). we use indicative use cases to present the relevant foresight nodes within the exemplar reporting."
"in order to illustrate the potential scope and depth of big data, a list of potential data sets available in the uk, were mapped against the nodes of the foresight obesity systems map. the objective was not to perform a comprehensive audit, which would quickly become outdated, but rather to demonstrate the potential value and opportunity of big data as a resource in understanding the obesity system. the list of data sets was thus a convenience sample, based on data sets that were familiar to network members. the mapping exercise was supplemented with one more traditional and comprehensive dataset, the uk biobank cohort, to highlight how combination of different types of data might be used together."
1. develop a framework in which to effectively report big data for use under a whole systems obesity lens. 2. use the new framework to report indicative exemplar data types in line with the foresight obesity system map domain areas. 3. identify big data sources for use in whole systems obesity research and map these against the foresight obesity systems map. 4. discuss key challenges associated with using new and large data sources to analyse obesity from a whole systems perspective.
"it is therefore also crucial that ways are found to communicate the potential (and limitations) of new, as yet untapped, data sources, across disciplines and sectors, to facilitate the move towards a whole systems approach to obesity."
"one challenge that potentially precludes the use of big data to their full potential is a lack of awareness and understanding around what data exist. aspects of these new big data sources, such as the volume, variety, velocity and veracity are often challenging to conceptualise and capture."
"in relation to data access, the ownership of data is another key issue. supermarkets may be keen to share data with academic researchers if there is hope of serious insights into store planning or optimisation of marketing spend. whether they are also keen to share data to understand negative health consequences from retail sales is a somewhat different proposition. the ability to document the ownership of data sources is not always straightforward. for example; food standards agency (fsa) data may be hosted and accessed via a local authority, but whether local authorities or the fsa are the data owner is debatable. this is likely the case for other data sources accessed via a third party."
"the above notwithstanding, a plethora of approaches, methods, metrics and variables are already being used in studies that make cross-comparison difficult -even impossible -and so the search for definitive evidence difficult. it is also important to maintain scientific rigour and a critical perspective and humility; employing a priori hypotheses where relevant, or acknowledging hypothesis generation where this alternative is used. current practices of reporting statistical significance are in urgent need of refreshment because large sample sizes will always produce highly significant results and thus reporting of effect sizes and clinical meaningfulness is essential. heterogeneity in data collection methods and resulting biases must be considered and acknowledged. this may be further inflated through combining multiple data sources. the use of big data does not preclude the need for validation of findings, whether that is through use data generated from a 'gold standard' or using better understood traditional data sources. multiple big data sources, combined with traditional datasets offer opportunities for cross-validation, which is especially important when findings result from hypothesis generation. given the many strengths of big data, we may need to accept these limitations as a necessary compromise. however, newly developing machine learning methods, and new strategies for causal inference with observational data may be part of a solution to these challenges [cit] ."
"in this paper we have considered how the foresight obesity system map might be more fully populated through extraction of big data sources. however, the transformative effects of big data are potentially much more wide-ranging. a primary example of this is in the field of randomised controlled trials (rcts), where there is a growing feeling that a combination of new datasets at scale, perhaps ranging from patient data, hospital outcomes and prescriptions to lifestyle, activity, eating and food purchases might be combined to create a massive population base for future trials. such an approach could be cost efficient in targeting participants, it would allow substantial samples to be identified for even the rarest conditions, and potentially admit variations between focused sub-groups e.g., within a specific age range or ethnic category. again, the ability of trials to utilise such data is largely dependent upon the availability and accessibility of individual level data."
"data which span the whole of the obesity system are difficult and time-consuming to collect, particularly on a large scale. big data have been heralded as a potential solution to this problem, with such data being generatedlargely passively-at an ever-increasing rate and across a range of contexts. this is the first time the potential for big data has been evaluated in a whole systems context. our data audit has shown the potential value of big data within under uk law any business intending to conduct 'food operations' (including selling, cooking food, storing, handling, preparing or distributing food) must register their business with the environmental health department of their local authority (la). this is then used by the environmental health team to conduct food hygiene inspections and enforce food law. the register is updated by a la when a business registers its intention to conduct food operations, and businesses are removed when registered businesses inform a la of their intention to terminate food operations. data are also updated when environmental health officers conduct food hygiene inspections. the frequency of such inspection will depend on the initial food hygiene rating assigned to the business traditionally recording of diet has been done through paper based questionnaires and diaries which are burdensome for participants to complete and for researched to code in nutrient composition software. nutrient composition software typically only include nutrient breakdown for~3200 foods, whereas tools like myfood24 offer nutrient composition of~45000 food at the push of a button purpose new technologies enable timely recording of diet for personal use and for research purposes elements content self-reported dietary consumption including elements such as: meal slot, time of day, branded and/or generic items, scanned unique product codes (upc; 'bar codes'), portion size, own recipes, photos of meals, nutrient composition of foods"
"the preceding paper in this series [cit] reviewed how 'found' data sources, often referred to as 'big data', have been utilised in the literature to better understand obesity. data on our activity, behaviour and location, from sources as diverse as smart motorways, social media, store loyalty cards and consumer organisations, have been shown to offer fruitful research opportunities, contributing in ways where traditionally sourced research data perhaps could not."
"where data linkage is often feasible and legal, whether it is ethical to perform such linkages is a wider consideration [cit] . for example; users of fitness tracking devices may have consented within their terms and conditions to sharing of their data with trusted partners. however, could they reasonably have expected that these partners would combine these physical activity records with food purchase transactions and their health outcomes? in many cases the benefit to society from such research may be argued to outweigh the risk of identification of the individual, but does that mean we should link data in this way, and would it, or should it, be permitted by research ethics committees responsible for granting ethical approval for research? the role of the ethics committee is essential to protecting the interests of the public and the research community. should ethical standards not be maintained there is a risk of public outcry, which could prohibit future research of this type. worse still would be for individual-level information to leak outside the research communities, for example to insurance companies, who might penalise their customers."
"the aim of this paper was to explore the potential role of socalled 'big data' in a whole systems approach to obesity. by mapping a small but varied selection of emerging data types onto the foresight obesity system map, it is apparent that big data span 80% of nodes, and therefore could prove important in providing the breadth and depth of physiological, social, and environmental information needed to simultaneously examine inter-related risk factors for obesity in different populations and across multiple levels. through this mapping exercise we highlight the wide variety of data which could be better exploited alongside existing research or for new, interdisciplinary, obesity research questions."
"these issues around access, linkage and ethics are echoed in the literature: most of the studies published to date that have attempted to utilise 'found' data describe challenges relating to these concerns [cit] . it is encouraging however, that solutions to these challenges have been found, illustrated by the publication of such studies. sharing best practice between research teams and organisations, relating to these challenges, presents opportunity to progress with a new type of research more pragmatically and efficiently."
"in conclusion, big data offer great potential across many domains of obesity research and need to be leveraged for societal benefit and health promotion. while obesity research and policy have evolved towards a 'whole systems' paradigm since the publication of the foresight report, they still tend to focus only on small parts of the obesity system in isolation, and fail to consider the interrelationships between different factors. use of big data could facilitate understanding of the wider determinants of obesity and their interrelations across multiple levels. in turn, this would permit evidence-informed allocation of funds and ultimately optimise return on investment during a period of financial constraint. this is particularly timely in light of the government's childhood obesity policy published this year, which, in spite of identifying 14 specific levers for change, found a best-case summary of existing evidence-base to be 'equivocal'."
"this second paper from the economic and social research council (esrc) strategic network for obesity builds on this description of how 'big' or 'found' data has been used to date, and considers the future potential of these data to enhance a 'whole systems' understanding of obesity. identifying new types of data becoming available and mapping these onto the domains defined in the foresight obesity system map should reveal the extent to which such data may be capable of addressing the whole system."
"knowing that individual (e.g., genetics, age, gender and ethnicity), social (income, education, area deprivation) and area factors (e.g., access to fast food, street connectivity) contribute towards obesity is useful [cit] and has identified key areas to target prevention and/or treatment. however, the key is understanding the interplay between these factors, which is currently lacking. the complex, nonlinear and unpredictable relationships of how systems interact will offer insight into the development and evaluation of systems based approaches, moving away from siloed thinking [cit] . data required to fill gaps in traditional resources and to enable research using a whole systems approach are inherently difficult to collect, especially on a large scale. for this reason, new and emerging data sources are increasingly gaining attention."
"throughout history the use of human descriptions obtained from eyewitnesses has instigated the identification and apprehension of suspects. humans naturally use labels and estimations of physical attributes to describe people. typically eyewitness descriptions are used to coarsely search criminal databases using broad descriptions like gender, race and height [cit] . a manual search of the retrieval results is required to identify potential suspects."
"several modifications were made to the frs. many traits, which recorded the presence of facial hair, glasses and jewellery, have been excluded as they describe temporary features and do not lend themselves to the comparative nature of the experiment. traits describing colour were also excluded, hair colour had been explored in previous experiments [cit] and the facial images used in this experiment are too low resolution to accurately identify eye colour."
"human descriptions are inherently subjective; the process of selecting an estimate or label is based on the individual. however, absolute labels can be considered highly subjective due to the subjective internal benchmark by which the label is being assigned. generally a label is based on the annotator's understanding of population averages and variation -this varies making the absolute labels unreliable. comparative labels are less subjective as the benchmark is external and specified. if two annotators were asked to compare the same pair of subjects, both would annotate based on the same benchmark leading to descriptions which are more robust over different annotators."
"van koppen and lochun [cit] performed a large study into the content of 1313 human descriptions. the descriptions were obtained from written statements given by eyewitnesses following a robbery. it was discovered that only 5% of descriptions contained any inner facial features (for example eye colour, nose, mouth, eye shape and teeth). sporer [cit] analysed the content of 139 descriptions obtained from 100 witnesses. it was found that 29.6% of the descriptions explained facial features, of which the majority of the descriptors described the hair and facial hair of the suspect rather than inner facial features. inner facial features are not frequently mentioned in eyewitness descriptions. this has been accredited to eyewitnesses not being able to recall discrete features [cit] and the lack of vocabulary to describe inner facial features [cit] ."
"a single facial comparison will describe the differences between the target and subject in terms of individual traits, such as nose length and face width. a trait comparison is a comparison of an individual soft trait."
"comparisons were made between frontal and side facial images of the 100 subjects in the soton gait database (sgdb) [cit] . the experiment was split into two parts. the first section asked users to provide absolute descriptions of five subjects from the sgdb. the absolute descriptions were composed of the same 27 traits which were presented in table 1, except absolute labels were assigned to the extremes of the scales. the second section asked users to compare five subjects to a single target, replicating the application scenario of comparing a observed suspect to multiple subjects within a database. collecting both absolute and comparative descriptions allows the accuracies of both to be directly compared. the 100 subjects within the dataset were halved and assigned to one of the two parts of the experiment. the 50 subjects selected for the comparative facial experiment were designated as one of either 10 targets or 40 figure 1 . website used to obtain facial comparisons subjects. performing comparisons between a large group of subjects and a small group of targets allows comparisons to be inferred between subjects. if two subjects were both compared against the same target, a comparison between the two subjects can be inferred, reducing the amount of comparisons required."
"an experiment was designed to assess the advantages of comparative descriptions when describing facial features. in particular whether comparative labels improve the accuracy of inner facial feature descriptions, by reducing the subjectivity associated with absolute labels and providing a defined and understandable vocabulary."
"the retrieval accuracy of the facial absolute labels is shown in figure 6, along with the retrieval accuracy of facial comparisons inferred from 1-3 comparisons. it can be seen that comparisons outperform the absolute facial labels even with just one comparison. the identification performance (i.e. the rank 1 retrieval accuracy) of absolute labels was found to be 59.3% compared to 74.5% achieved with relative measurements inferred from one comparison. the identification performance increases with additional comparisons, achieving a 96.7% identification accuracy with only 3 comparisons. these results, obtained under ideal conditions, show the potential of facial comparisons."
"comparing the appearance of two subjects is a very natural process. intuitively it is easy to say whether one person is taller than another, but labelling or estimating the height in absolute terms can be much more difficult. we exploit the ease of making comparisons to provide reliable and robust descriptions."
"comparative facial recognition aims to retrieve a suspect from an 40 subject database. the biometric signatures within the database consist of all the 27 traits (table 1), where comparative traits are represented as elo relative measurements. the process starts by selecting a suspect from the database. n randomly sampled comparisons between the suspect and other subjects were removed from the database and used to infer the suspect's biometric signature used to query the database (known as the probe). this replicates the eyewitness comparing the suspect to n subjects from the database. n was varied to investigate how many comparisons are required to retrieve a suspect accurately. the suspect's remaining comparisons were used to produce the biometric signature stored within the database (known as the gallery). the remaining 39 subjects' feature vectors within the database were determined from all the available comparisons (excluding any comparisons used to construct the suspect's probe feature vector). the similarity between the probe and gallery feature vectors was assessed using the sum of the euclidean distance. the subjects were ordered based on their similarity to the probe. the position of the suspect's gallery biometric signature within the ordered list shows the retrieval performance of the system. if the suspect's gallery signature is first in the ordered list the suspect has been successfully identified. this process was repeated 100 times for each subject and for each n."
"comparing absolute and comparative labels allows us to observe the differences between the two forms of description. to determine the difference between the descriptions, the comparative label is compared against the absolute labels used to annotate the subject and target. if the absolute labels differ and the comparative label reflects this difference, the annotations are recorded as concurring -for example if the target and subject noses were labelled as 'short' and 'long' respectively and the comparative descriptor provided was 'longer', we would consider both annotations as concurring. the absolute annotations obviously lack detail; two people labelled as having 'long noses' are unlikely to have exactly the same length nose. thus, small differences can be described using comparative annotations but not absolute labels. in the case of both the subject and target having the same absolute label, the similarity of the comparative annotation cannot be determined. in this case the comparative annotation was recorded as concurring -this ensures we do not overestimate the difference between absolute and comparative annotations. figure 4 shows the difference between absolute and comparative facial descriptions. on average the descriptions differ by 26.3%. the traits which are most similar to absolute descriptors are prominent facial features, including traits like skin-light/dark, face-bony/fleshy and hair-short/long. these traits are easily recognized due to their prominence and therefore individuals have an understanding of the traits' averages and variation, this could explain why the absolute descriptions of these traits are comparatively similar to the comparative annotations. traits such as face-short/long, ears-small/large and eyebrowsstraight/arched may suffer from a lack of noticeable variation leading to large differences between the two forms of description. small variations are difficult to describe using absolute labels and may not even be noticed due to the trait looking 'normal' or 'average'. comparisons allow variation to be identified and accurately described leading to vast differences between absolute and comparative descriptions."
"comparisons and absolute descriptions were collected using the website shown in figure 1 . the website was designed to display the frontal and side images of both subjects at the same time avoiding any issues with memory. the bipolar scales were implemented using radio buttons which required minimal user input and were found to be very easy to interpret. to avoid anchoring [cit] the radio buttons were initially empty, forcing an input from the user. annotations were emphasized by constructing a sentence explaining the given comparison -ensuring the annotator was comparing the subject to the target instead of vice versa. at the end of the experiment the annotators were encouraged to submit a small feedback form asking which form of annotation they preferred -absolute or comparative."
"in this paper we will explore whether facial descriptions can be used to identify individuals. although facial descriptions are not mentioned as often in eyewitness descriptions compared to bodily descriptions, they are vital in many serious crime investigations. for this reason the identification of possible suspects is an important problem."
"although descriptive, a single comparison between a suspect and another person will only explain the differences between the two. thus, the inferred physical traits of the suspect will depend on the subject they were compared to. multiple comparisons must be available to infer a more robust description, with each comparison allowing the description of the suspect to be refined. therefore, ideally multiple comparisons should be obtained between the observed suspect and multiple subjects."
it can be seen that human descriptions of facial soft biometric traits still play a large role in law enforcement. the lack of vocabulary to describe facial features represents a barrier when collecting and exploiting facial descriptions. traditional facial descriptions consist of categorical labels which are very subjective and hence often inaccurate. in this paper we utilize comparative labels. comparative descriptions describe the differences between faces reducing the subjectivity associated with categorical labels.
"facial descriptions, although infrequently mentioned in eyewitness statements, play a large role in many serious crime investigations. the lack of vocabulary to describe facial features is a major cause of inaccurate and unreliable facial descriptions. in this paper we have introduced the concept of comparative facial descriptions which produce accurate and robust annotations by exploiting a defined and objective vocabulary. the elo rating system is utilized to produce continuous discriminative biometric features from verbal comparisons, allowing biometric identification of individuals. with a single comparison an identification accuracy of 74.5% is achieved. obtaining more comparisons improves the identification accuracy, achieving 99.3% with five comparisons. these results show that comparative facial descriptions can be used to automatically find possible suspects. future research will explore the application potential of identification using facial comparisons, considering larger databases, inaccuracy resulting from memory decay and the accuracy of comparing observed suspects to videos of subjects."
"absolute and comparative descriptions were collected from 63 users. 302 absolute descriptions (describing 50 subjects) and 297 comparisons (comparing 40 subjects to 10 targets) were collected. more information about the collected comparisons and the resulting inferred facial comparisons is shown in table 3 . the number of collected absolute facial annotations figure 2 . annotators' preferred form of facial annotation 48 annotators chose to submit the feedback form at the end of the experiment stating which form of annotation they preferred. the results can be seen in figure 2. it is clear to see that the majority of the annotators (77%) preferred comparisons over absolute annotations. only 16.6% of the annotators preferred absolute annotations. the inclination towards comparative annotations may be due to the simplicity of objective comparative labels. figure 3 shows the correlation between the facial comparative features. the correlations between traits were calculated using elo relative measurements deduced from the comparative labels. the white cells within the figure represent traits with high correlation and the black cells represent traits with no correlation. it can be seen that there is very little correlation between the features. the lack of correlation highlights the independence of each facial trait, this is ideal for identification as each trait comparison conveys new and potentially discriminatory information. it should be noted that the low correlation does not mean that there is not a relationship between the features only that it is not prevalent within the dataset currently being used."
"the facial recognition accuracy over varying numbers of probe comparisons is shown in figure 5 . it can be seen that facial comparative descriptions vastly outperform bodily descriptions [cit], achieving a 74.5% identification accuracy with a single comparison (compared to a 47% accuracy with bodily comparisons). a 99.3% recognition accuracy is obtained with just five comparisons, reaching a maximum facial descriptions have three benefits which aid in identification when compared to bodily descriptions. it was shown in section 4.4 that facial features have little correlation, resulting in more independent information available for identification. this increases the feature space by many dimensions, typically making each subject more distinctive and easier to identify. body comparisons can be effected by many types of covariates. in the sgdb baggy clothes often hide features from the annotator. faces have far fewer covariates. glasses are a very common covariate within the sgdb (around 47 people wear glasses) but these rarely interfere with the observation of features, whilst only 6 people have facial hair within the database. this results in the features being very evident and easy to describe -improving the descriptions. finally faces have much more features to describe. we collect 27 facial trait descriptions which results in typically more distinctive descriptions allowing greater accuracy when identifying subjects."
soft biometric features [cit] are characteristics which people can naturally describe. this new form of biometrics has allowed individuals to be automatically identified from criminal databases based on bodily descriptions [cit] . this greatly speeds up the process of finding possible suspects.
the experiments within this chapter replicate this application scenario by collecting multiple comparisons between a target subject (representing the suspect in application settings) and multiple subjects.
"we will show how descriptions of facial soft biometric traits can be used to accurately identify individuals from a criminal database containing soft biometric information. underpinning this advancement is the use of an innovative form of human description -comparative labels. previously absolute bodily descriptions were utilized to identify individuals [cit], achieving an identification accuracy of 48% [cit] . absolute labels were shown to be a poor form of description, suffering from subjectivity and subject interference [cit] . comparative labels have been found to be less subjective than traditional forms of description and are preferred by the majority of annotators. furthermore, informative continuous relative measurements can be inferred from multiple comparisons, providing the level of detail required for identification. previous research studying bodily comparisons of soft biometric traits achieved identification accuracies of 92%, demonstrating the advantages of comparative descriptions [cit] . we exploit the ease of making comparisons to explore a new method to provide reliable and robust facial descriptions."
ideal physical traits for use within a soft biometric system would be easily identifiable at a distance and memorable. traits which are frequently mentioned within eyewitness descriptions are most likely to adhere to these two requirements.
"the final set of 27 comparative traits are presented in table 1. each trait is described using a 5 point bipolar scale, the extremes of which are represented by two labels (an example of this can be seen in figure 1 )."
"a facial comparison is a set of individual soft trait comparisons describing the differences between two subjects. in application settings, an eyewitness would compare the previously observed suspect to other subjects (possibly obtained from a video or image database). this allows information about the suspect to be inferred from the appearance of the subject and the comparison describing the differences between the two individuals."
"in section 2 we discussed the issues with conventional forms of facial description. absolute labels require little skill to annotate but due to their categorical nature have little discriminative capability [cit] and are prone to subject interference [cit] . comparative descriptions exploit categorical labels which are easy to understand and annotate. collecting multiple comparisons allows informative continuous measurements to be inferred, providing the level of detail required for identification."
"psychological research has determined that descriptions of inner facial features suffer in accuracy due to a lack of vocabulary [cit] . visual comparisons allow features to be described in a natural way using comparative labels. this offers a defined vocabulary whilst avoiding subjective absolute labels, like 'big'. although this does not make the features more memorable it could facilitate accurate descriptions for cases where the eyewitness has observed and encoded the suspect's face. this could be exploited for searching databases of mugshots or the description could be used to seed the generation of composites in programs like evofit [cit] ."
"although facial features are not as common in eyewitness descriptions as bodily and global traits, they are vital in many serious crime investigations. exploring the capabilities of visual comparisons could present solutions to the lack of objective vocabulary for describing facial features."
"identification using absolute facial descriptions utilized figure 5 . facial recognition accuracy using relative measurements obtained from different numbers of comparisons the same 27 traits, each being described using absolute ordinal labels (represented using a value ranging from -2 to 2). a leave-one-out validation approach was used to evaluate the recognition performance. every description given was individually used to probe the database. the probe feature vector was formed from a single verbal description of a subject given by a single annotator. the remaining descriptions of the subject were used to produce the feature vector present within the database being searched. on average each subject was described by 6 users, the most frequently used label to describe a trait was used to produce the biometric signature describing the subject. the database consisted of 50 subjects, none of which were included within the comparative facial database. the euclidean distance metric was used to evaluate the similarity between the probe and gallery feature vectors -this was possible due to the ordinal nature of the labels. the subjects were ordered based on their similarity to the probe. the position of the suspect's gallery biometric signature within the ordered list shows the retrieval performance of the system. the identification results shown in this research are obtained from exhaustively calculating the similarity between the probe and each gallery signature. for larger databases this process could be accelerated by filtering the subjects based on soft biometric features which are reliably and accurately described."
"then, by combining the set of four possible granularity levels (presentations, compositions, interface elements, interactive subparts) with the care properties we obtain a simple and powerful tool to indicate how the user interface can be distributed in a flexible way. thus, we can distribute an entire presentation. for example, by associating the redundancy property we indicate that one presentation should be completely rendered in two different devices. however, we can also distribute single interface elements. for example, distributing a textual object in a complementary way means that part of the text is rendered through one device and part through another one. as we mentioned, it is even possible to distribute sub-elements of a single interaction object. for example, a single selection object can be distributed in such a way that when the user selects one element then the feedback indicating what has been selected is rendered through another device. this means that the prompt and input components have been assigned to one device while the feedback sub-component to another one."
"more in detail, figure 5 shows how the software architecture works at run-time. we assume a case in which the user interface is distributed across a desktop and a mobile device. at the beginning the user accesses the application through the desktop device, the environment creates a data model and property list according to the maria specification and provides the requested user interface (3 in figure 5 ) and the same process is followed for the mobile device. when the user changes a value of an interactor, this is communicated to the websocket server, which checks the care properties of that interface element to see whether changes are necessary and updates the other instances in other devices (9) accordingly. the current prototype environment takes distributed maria specifications and generates jsp implementations, which are dynamic pages able to generate xhtml or html5 or voicexml or x+v implementations depending on the type of target device."
"requirement 4. qm should be scalable according to site complexity. any site (even the simplest) is really a very complicated system, as briefly discussed in 3.1. but it is totally unrealistic to pretend that small organizations (which own the large majority of sites) may (or want to) deal with all the subtleties of a conceptually sound and complete qm. simple users need simple tools. therefore, a scalable qm would be available in simplified versions to be used in simple contexts."
"the one-to-one criterion is not satisfied only for usability and accessibility, which are the results of the cooperation of all actors. as is often said, usability is like a chain with many rings, and its strength is that of the weakest."
"it is worth pointing out that the decomposition into prompt, input and feedback is meaningful only for interactive interface element, and cannot be applied for only-output elements."
"the changes in values in the data model are described through event handlers that indicate when they should occur and the new value that should be applied to the data element in question. in addition, such event handlers can also communicate changes in the care properties of some user interface components. as mentioned, in the case of distributed user interfaces, all the user interfaces generated for the various devices involved share the data model, indicating also the values for the care properties."
"in section 2 the iso approach to qms for software and computer systems is summarized. section 3 will discuss the main peculiarities of web sites with respect to traditional software systems, and lay down a few basic requirements for web sites qms, also considering the evolution of the role of users in web 2.0 sites. section 4 will describe the proposed qm family, and briefly compare it with the iso standard. finally, section 6 will contain some conclusions and indications for future work."
"in this way it is also possible to easily indicate how dynamically the user interface elements can be distributed. thus, if we want to move one element from one device to another then it means that we have changed the device to which that element is assigned. while if we want to copy a user interface element from one device to another then it means that we have changed the corresponding care property from assignment to redundancy."
"indeed, in the maria language it is possible to include a data model to represent the data types handled by the user interface. the user interface elements (interactors) can be associated with data types through the data reference attribute. this dependency implies that at run-time if an element of the data model changes its value then all the associated interactors should be notified of the change in order to update their state accordingly."
once the distributed user interface has been designed there is the issue of obtaining the corresponding implementation able to support the desired dynamic behavior. there are two possible ways to do this:
"in particular, we have introduced a language with the possibility of defining a concrete distributed user interface. in such language it is possible to indicate the types of devices on which the user interface can be distributed. each device belongs to a platform for which already exists a corresponding concrete language. such concrete languages refine the abstract vocabulary taking into account the interaction resources that characterise the corresponding platform. this allows designers to specify interface elements that better adapt to the devices in which they are rendered."
"fundamental in the iso approach is the distinction between the internal properties of a product (which contribute to the internal quality), its external properties (which contribute to the external quality), and its quality in use properties, i.e. properties which influence quality and which can be measured when the product is actually in use in specific contexts. all these properties influence each other and the resulting quality in a complex way, as schematized in fig.2 ."
"qms are very important in web engineering. having a good qm at hand can be extremely useful in all phases of a web site life cycle. in the requirement specification phase, a qm helps in elicitating and orderly describing all important facets of the site to be designed. indeed, the table of contents of a good requirement specification document could strictly mirror the qm, by assigning to each model characteristic a specific section of the document [cit] . during the development process, a qm helps the project team in keeping their eyes on all desired quality attributes of the system to be implemented. in assessing the quality of an existing site, or different sites for comparison or benchmarking, a qm provides a structured approach to the evaluators, helping them to stay focused on the important issues. in the operation phase, a qm provides the site management with a \"compass\" to keep its evolution on the right track. indeed, all web sites are very dynamic; their evolution is constant and substantial: it is therefore essential to continuously monitor their quality, to avoid that the frequent changes disrupt piecemeal an initially sound project. this is particularly important for web 2.0 sites, whose evolution is determined not only by the site management, but also by the (possibly large and uncontrollable) user community. a \"suitable\" qm is the necessary supporting tool for these monitoring actions."
"of course, the goodness of the mapping does not depend only on the qm, but also on the actual organization which develops and manages the site. a chaotic organization will nullify the practical utility of even the best qm. nevertheless, after fifteen years of web engineering experiences, the roles and functions of the different quality actors in e web project are today sufficiently well understood. this allows to define good qms which are reasonably applicable to most web organizations."
"in order to show how our approach works let us consider a concrete example, not too complex for sake of clarity. we consider an application to show slides, it allow users to go back and forth, and to annotate them. figure 1 shows the initial user interface, completely rendered in a desktop graphical device. more precisely figure 2 shows the corresponding hierarchical structure: one presentation with a couple of output descriptive objects (the application title and the slide), a grouping composing two buttons to go back and forth, an interactive elements to write comments and a button to store them. since the interface is completely shown in one single device, it is sufficient to associate the assignment property to the root. now, suppose that the user wants to distribute parts of the user interface to a multimodal mobile device as indicated in figure 3 . to obtain this example of distributed user interface a number of elements have been assigned new values of the care attribute as indicated by figure 4 . thus, there is no longer a care attribute assigned at the presentation level. the title description is redundant while the slide description is assigned to the desktop device, because it has a larger screen that can better show its content. the grouping with the buttons for going forth and back is assigned to the mobile device and it has a multimodal support: prompt is redundant with vocal and graphical modality, and input is equivalent and can be provided by either modality. the text edit interactor for entering comments is redundant in both devices but the button to store the comments is assigned only to the mobile device, for immediate activation by the user."
"in the paper we first provide an overview of the solution that we have developed. next, we provide some detail on the language supporting it. we discuss the corresponding architectural solution for providing support at run-time and show an example application. lastly, we draw some conclusions and provide indications for future work."
"communication. in most cases, web sites can be considered machines whose main purpose is communication, rather than computing and data management. this is also true for e-commerce or other sites offering online services. web sites address a global audience, in a strongly competitive, \"open\" environment. there is no user lock-in: competition is only a few clicks away, so visitors' loyalty must be won on a day-byday basis. user attention span can be extremely short, so his/her interest must be captured in brief time-intervals. so big efforts are required on communication and branding, and professionals typically not seen in traditional software projects are necessary (visual designers, art directors, communication experts)."
"the current technological trends are determining a steadily increasing number of computers per person along with many sensors able to detect a wide variety of contextual events. the computers are becoming more and more variegated in terms of possible interaction resources and modalities, including interconnected embedded devices composed of small electronic components, which can interact with each other. this implies that in the near future we will no longer access our applications through one device at a given time but we will rather use sets of collaborating devices available while moving, such as using the smartphone to control the content on a large screen. distributed user interfaces (duis) have recently become a new field of research and development in human-computer interaction (hci). the duis have brought about drastic changes affecting the way interactive systems are conceived. duis go beyond the vision that user interfaces are controlled by a single end user on the same computing platform in the same environment. unlike existing user interfaces, duis enable end users to distribute any user interface element, from the largest one to the smallest one, across different computing platforms and physical environments [cit] . thus, emerging ubiquitous environments need distributed user interfaces, which are interfaces whose different parts can be distributed in time and space on different monitors, devices, and computing platforms, depending on several parameters expressing the context of use [cit] . this has an impact on the user interface languages and technologies because they should be able to support the main concepts characterising interactions with an application through various combinations of multiple devices."
"with the above premises, we can now lay down the main requirements for our qm. requirement 1. qm should have an organization mapping as simple as possible, as discussed in 3.3. we do not require that it be related to a specific project organization schema, but simply that the quality [sub-] characteristics be associated in a simple way to the quality actors of fig.3 . it is rather evident that the iso qm of fig.1 does not satisfy at all this requirement."
"regarding the possible granularity levels to address we have started from the consideration that in maria a user interface is composed of presentations (in graphical interfaces they correspond to the set of elements that can be perceived at a given time, e.g. a web page). then, in each presentation there can be a combination of user interface elements and instances of composition operators. in maria there are three types of composition operators: grouping (a set of elements logically related to each other), relation, a relation between groups of elements (e.g. in a form there usually are a set of interactive elements and a set of associated control elements to send or clear them), repeater (a group of elements that are repeated multiple times). since we aim to obtain full control on what can be distributed we decided to consider also the possibility of distributing the elements within a single interaction element. for example, a text edit interactor can be distributed in such a way that the user enters the text in one device but receives feedback on what has actually been input in another device. for this purpose we provide the possibility to decompose interactive interface elements into three subparts: prompt, input, and feedback."
"rather than start from the iso model and modify it piecemeal to comply with the stated requirements, it seems more reasonable to start anew, and see where this approach leads. requirement 1 suggests to start by defining a general model of a web site, showing its main \"physical\" components (the quality of which we wish to take under control), its main quality actors and the relationship between actors and components. this can be done a)-considering the web site design & development process, or b)-considering the web site in operation. the second approach seems more comprehensive because of the constantly evolving nature of web sites (which are not \"frozen\" when they are published online after development) and because it allows to consider the role of end users as quality actors, which is fundamental in a web 2.0 context. thus this paper will use approach b). 3 therefore, a web site will be modeled as a set of nested (physical) components, as shown in here, the term architecture refers exclusively to information architecture [cit], including site navigation facilities, and not to internal software architecture. its associated actor is therefore the web designer (or information architect). communication refers to all aspects of site communication, typically embodied in the site style guide, defining graphics, multimedia usage and style issues. the associated actors are the visual designers because in small/medium sites this responsibility is usually assigned to them. note, however, that larger sites may have a more complex organization. like the iso functional suitability [cit], functionality means \"the degree to which the site provides functions that meet stated and implied needs when used under specified conditions\". note that this does not include navigation functions (menus, breadcrumbs, and so on), which are part of the architecture. content collects all the quality characteristics related to the company-generated information/data content of the site, under the responsibility of the content editors. community is mostly used only for web 2.0 sites, and considers user-generated content: associated actors are site users and site community managers. platform considers the site platform (e.g., the used cms), the hardware and software of the server hosting it, and the network infrastructure. its quality characteristics are both static (i.e.: are they suitable for the context?) and dynamic (i.e.: are their operations well managed?). quality actors may differ from case to case: when server and network management are outsourced to an external organization, the data center manager is simply the person interfacing the service. usability and accessibility have the usual meaning of the iso qm. finally, software code refers to the quality of the software specifically developed for the site (therefore excluding platform components acquired on the market)."
"this paper has proposed a methodological approach to define qms for web sites of any kind, including web 2.0 sites. the approach stresses the practical use of a qm, in requirement definition and quality assessment, during design & development processes or during site operations. therefore, the main driver for qm definition has been what we called organization mapping, as opposed to the conceptualization of abstract quality characteristics. organization mapping allows who is in charge of quality management to easily identify the actors in the organization responsible for implementing or improving each specific quality characteristics. this is much more important for web sites than in traditional software systems, given the high number and diversity of the actors involved, and the possibility of conflicts arising from their diverse approaches. accordingly, a simple qm family has been proposed, starting from a very general model of web site, showing its main physical components, mapped to the actors responsible for their quality. this qm defines the characteristics down to the second level: it is general enough to be applicable to a very large class of sites and to be used as a viable table of contents for requirement definition documents. it should now be specialized and experimented for specific classes of web sites. a comparison with the iso qm for software and software intensive systems has shown important differences, due to the peculiar nature of web systems."
"but how do we choose it? the selection of a qm is a delicate task, because it may have a large impact on the site's success, and is not trivial at all, for two main difficulties: orthogonality and measurability of characteristics. orthogonality is difficult to achieve because the quality attributes of a web site interact in complex ways; measurability, because many of them are subjective."
"continuous evolution. web sites are living organisms. their contents are constantly updated, and even their information architecture changes frequently. this is true for any site, not only for information portals. visitors of a site often expect the content to be updated practically in real time. site managers must strive hard to comply with these expectations, just to keep their site reputation. interactive services and the user interface are frequently modified and improved. according to the perpetual-ß concept, the software behind these services is continuously modified to better serve user needs. these -in turn -change as new possibilities are discovered, in a constant co-evolution of usage patterns and system functions. in a word, managing the evolution of a web site sets pressing requirements to site administrators, and this should be taken into account seriously in any qm designed for these systems."
"regarding the set of primitives we decided to use the care (complementarity, assignment, redundancy, and equivalence) properties [cit], which were introduced to describe multimodal user interfaces, and have already been considered in the maria concrete language for multimodal interfaces [cit] . in our case the idea is to use them with this meaning:"
"a good mapping is a crucial requirement of a web site qm because, as shown in fig.3, the actors involved in web projects are many, and the involved skills are extremely varied. in a multi-disciplinary team, different cultures, practices and value systems may sometimes create interaction difficulties, as anybody involved in medium to large web site development or operations may have experienced. to avoid these problems, it is necessary that the teams be correctly organized, with a clear allocation of responsibilities on the different system components and associated quality characteristics."
"the iso definition of a qm, quoted in section 1, emphasizes the practical purposes of any qm, which is not viewed as a mere categorization of the quality attributes of a system, but rather as a practical tool, to steer design (\"specifying requirements\") and evaluation (\"evaluating quality\") processes. in our view, this should be constantly kept in mind when defining any qm. to this end, we require that there be as simple as possible relation between quality [sub-] characteristics and the roles responsible for implementing and improving them. in this way, responsibility for different quality characteristics can be easily allocated and tracked, being always clear who is responsible for what. we call this attribute of a qm organization mapping. in fig. 4, mapping on the left can be considered better than the mapping on the right, because responsibilities are better isolated, and quality characteristics improvements are easier to manage."
"information content. in the large majority of cases, unstructured information content prevails on structured data. emphasis is on user navigation, not on data management and algorithmic computation. therefore, a fundamental dimension of quality relates to information architecture [cit] . information architects are more and more involved in large web sites, together with content editors, who create and manage its information content. information-rich sites may employ large editing staffs, with an organization in some ways similar to that of traditional magazines."
"requirement 2. qm should be tailorable to the class of sites under consideration. web sites are enormously diversified. they may differ in size, in technology, in purpose, in complexity, in relationship with the front users (from purely informative to interactive to social), in impact on their activities (from critical to non-critical). so there will be no universal qm. tailoring the qm would mean dropping some subcharacteristics or specializing some of them with further levels of detail. sometimes we would also assign different weights to the [sub-] characteristics, to express their importance in the particular context. requirement 3. qm should be subsettable according to its specific purpose. some [sub-] characteristics should be droppable from the qm, when they are not needed in its actual context of use. e.g., when using a qm to compare a site with its competition, we usually do not have access to information on their internal structure. thus, we would drop all [sub-] characteristics referring to internal quality from the qm."
future work will be dedicated to engineering the supporting environment and to usability evaluation of both the authoring environment and the resulting distributed interactive applications.
"in the software engineering literature, software qms have been discussed for many years. the iso/iec 9126, issued as an international standard (is) [cit], is the best known reference in this area. part 1 of this multi-part document [cit] provides a very general qm for software systems, based on a set of 6 quality characteristics (functionality, reliability, usability, efficiency, maintainability, portability) and 27 sub-characteristics. this is has been recently canceled, and replaced by iso/iec 25010 [cit], which updates the previous qm in various ways. it addresses \"software products and software-intensive computer systems\" of any kind, and defines two qms. the product quality model encompasses internal and external qualities of the system, and is composed of 8 characteristics and 31 sub-characteristics (fig.1) . the quality in use model allows to define/assess \"the impact that the product has on stakeholders\" and is composed of 5 characteristics and 9 sub-characteristics. each sub-characteristic may be further hierarchically decomposed. quality characteristics and sub-characteristics at any level should be measurable, either directly or indirectly, through a set of associated measurable properties."
"at first the user interface generator analyses the devices involved in the maria specification of the distributed user interface, separates the concrete user interfaces for each of them, and generates the corresponding initial user interfaces, which are available in the associated application server. when a distribution event occurs it is forwarded to a specific component (data model and property list in figure 5 ), which updates the care properties according to the indications in the associated event handler, and then updates the user interfaces of the devices involved accordingly. for this purpose the web socket mechanism is used because it provides an efficient way to push the updated content."
"on the other hand, it should be clearly understood that the iso documents only provide a conceptual framework, and not a ready-to-use qm. to be of practical use, this framework must be tailored to the specific [class of] system[s] under consideration. this may not be a simple task, especially when these systems do not fit well with the systems considered in classical software engineering, such as erp, command & control, embedded systems. this is the case of web sites, which possess a number of peculiarities that greatly differentiate them from the above systems:"
"the iso standard provides a very general conceptual framework for defining qms for complex systems with a substantial software component. the basic approach of defining a hierarchy of quality characteristics, and measurable properties which can be aggregated to obtain a quantitative measure of characteristics provides a sound foundation for defining any qm, in any domain. moreover, the iso model is the result of two decades of discussions about the basic quality dimensions of softwarebased systems. its categorization and terminology can be discussed and -in a few cases -may also be considered somehow obscure, but certainly cannot be ignored in any approach to quality in software engineering."
"by [quality] actor we mean any system stakeholder with an active role in creating/maintaining some quality attribute, such as web designers, visual designers, content editors, software developers. actors of a web site are more numerous and more varied than in traditional software systems. indeed, the development of any site is really a multi-disciplinary project, involving many different roles (fig.3 ). 1 in a typical web 1.0 site, end users have a passive role, so they are not considered actors because they do not contribute to its quality: they only navigate the site and possibly interact with it in predefined transactions (as in e-commerce). in web 2.0 sites the situation is completely different. the users can typically create and upload content, embed content from other sites, tag, comment or rate content created by other users and share it with their \"friends\", and interact with them in public. this is not only true for large social networks such as facebook, twitter, youtube and flickr, but also for an increasingly large number of small sites, due to the many available tools which allow to easily implement these functions, such as share buttons, plugins, html snippets. therefore, in web 2.0 sites, the users themselves must be considered quality actors and critical ones indeed, since they can have a big impact on the global functioning of the site. even a perfectly designed and implemented site can fail as a consequence of \"bad\" (or unexpected) user behavior. so users must be continuously monitored and in some way controlled or stimulated, requiring the presence of new roles (denoted as community management in fig.3), and in some cases the evolutionary modification of specific site functions, intended -so to speak -to improve the user-generated quality. a typical example is the evolution of the community content moderation mechanisms in yahoo!answer, where they had to oppose the unexpected volume of user spam and troll activity, that seriously risked crashing the site [cit] . fig.3 . the main quality actors of a web site"
"according to iso/iec 25000:2005 [cit], a quality model (qm) is a \"defined set of characteristics, and of relationships between them, which provides a framework for specifying quality requirements and evaluating quality.\""
in order to formalise the concepts introduced in a language that can be used to design and generate the corresponding user interfaces we have extended the maria language.
we opted for the second solution because it is more efficient since it does not require generation of all the distributed user interface elements for each change in the care property. for this purpose the specification of the care properties is included in the data structure supporting the data model associated with the user interface.
"distributed user interfaces require novel languages and tools in order to obtain flexible support for user interface designers and developers. in this paper, we have presented an approach able to describe distribution at various granularity levels, even involving multimodal devices. we have also introduced how dynamic distribution can be achieved through such approach and the software architecture supporting the corresponding implementation."
"some research effort to address distributed user interfaces with model-based approaches has already been carried out but with limited results and not able to support the many possible ways to distribute user interface elements. in hluid (high level ui description) [cit] the user interface has a hierarchical structure and the leaves of the tree are abstract interactor object (aios) describing high-level interactors. during the rendering process the aios are mapped onto concrete interaction object (cios) associated with the current platform. in addition, they introduce a split concept for the groupings through an attribute that, when it is set to true, allows the distribution of the user interface elements without losing its logical structure. in our case we propose a different solution, still using a model-based approach. one difference is that we support the specification at the concrete level because at this level it is easier to generate the corresponding implementations and there is a better understanding of the actual effects that can be obtained. vanderdonckt and others [cit] have developed a set of primitives to manage user interface distribution but they only consider graphical user interfaces while our approach is able to support user interfaces exploiting also other modalities, such as voice. blumendorf and others [cit] address multimodal interfaces but they lack an underlying language able to support user interface distribution."
"because the chosen names are very mundane, the site quality profile can be easily communicated to all site stakeholders, e.g. with a simple radar diagram, as in fig.6 . note that in most cases there is a one-to-one relationship between characteristics and actors, as shown in the bottom line of the schema, thus the qm has a good organization mapping, as required."
"the user interface is hierarchically structured: the user interface is composed of presentations. each presentation is composed of a combination of interface elements and composition elements, which can be recursively composed of interface and composition elements. when a care property is associated to one element of this hierarchy then all the underlying elements inherit such association. thus, if a grouping of elements is assigned to a device then all the user interface elements of the group will be assigned to it. this also simplifies the specification process by avoiding the need to indicate the value of the care properties to all the interface elements."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. to overcome the limitations of previous work our starting point was the maria language [cit], which in current version consists in a set of languages: one for abstract user interface description, and a set of concrete refinements of such language for various target platforms ( we have extended such language in order to be able to specify distributed user interfaces and we have also designed a solution to generate implementations of such distributed user interfaces, which can dynamically change how the user interface elements are distributed according to user requests or other events."
"requirement 5. qm should be universally usable and accessible. last but not least, if we want to have a real impact on the quality of the present day web, we should design qms that, as much as possible, are universally usable and accessible. this would entail the use of broadly understood concepts described in a simple language, with easy and free accessibility. 2 the stated requirements imply that we need a family of closely related qms, and not a single qm, if possible with a common set of top-level characteristics. these are the \"foundations\" of the qm, and therefore should be easily recognizable by anybody as the basic dimensions of the quality of any web site. they would constitute the main sections of the requirement specifications of any web development project, and the main aspects to be considered in any assessment or evaluation. qm personalization should then be localized in the lower levels of the hierarchy, to cope with specific web applications (req.2), to qm purpose (req.3), to site complexity (req.4) and to the complexity of the organization (req.5). this will be mostly done by adding/dropping sub-characteristics or defining lower levels in the characteristics tree. 2 lack of usability and accessibility are, in our opinion, the main problems with the iso qm, which hinder their large scale adoption by the general community of web practitioners. iso documents are organized in a very complex structure, which is continuously evolving. to understand the status of the iso document system and to identify the documents relevant to a particular activity, it is not easy and very costly, since documents are not freely available, and cost a lot of money. regrettably, this is also true for quality related standards, which should be, in our opinion, as openly available as possible."
"model-based approaches have been considered in order to manage the increasing complexity derived from managing user interfaces in multi-device environments, since each device has specific interaction resources and implementation languages to execute such user interfaces. they are also currently under consideration for w3c for standardization purposes [cit] . the basic idea is to provide a universal small conceptual vocabulary to support user interface design, which can then be refined into a variety of implementation languages with the support of automatic transformations without requiring developers to learn all the details of such implementation languages."
"the coding of a node in v (l) has two purposes. one is to update its children's codes in line 6. different from alg. 1, the label for encoding is constructed by appending parent's code on its original label. since a node has one and only one parent in a record schema, there is no sorting effort in the node encoding step. the other purpose of these codes is to generate the id of corresponding sub bucket by sorting(lines 7-8). in line 8, the record schema is assigned into to a sub bucket based on the id. if there is no such sub bucket existed, we append a new one with the id on the sub-bucket list of bucket. after line 8, a bucket in the l-th level of esibu-tree represents a category of record schemas whose top-l level subtrees are equivalent."
"skeleton construction this step constructs the skeleton of each object type based on record schemas, details of which are described in the following sub-section. as the above pre-processing step is designed to guarantee uniqueness and denseness of skeleton candidates, the updated record schema set is used as the input of the following skeleton construction."
"these label-code pairs compose the code map of level l (denoted as cm l ) as shown in fig. 3 . the detailed logic of encoding nodes is as: if the label of a node exists in the cm l, we use the corresponding code to replace the label; otherwise, we assign a new code to the label and update the cm l by adding this new labelcode pair. the purpose of mapping a label to a code is to save the space for appending children's information to their parent. in the implementation, we use the natural number in sequence as codes, and so the code map is a hash map with a sequence as key and an integer as value."
"in this section, we first evaluate the performance of equivalent record schema grouping algorithms for the schema extraction. the experiments were run on a workstation (intel core i5 processor with 2.67ghz and 4gb of ram). the category numbers shown in the table 2 with the column titled \"sch#\" confirm our analysis in sec. 1. the records describing one object are in various schemas because of variations as illustrated by motivating examples. this phenomenon is most notable in data sets from dbpedia, where equivalent record schema category numbers are very close to the number of records. in another saying, there exist large portions of record schemas only used in one of the records."
"p ← retrievebucketpath(b) 5: end if algorithm 7: query 2 based on esibu-tree after obtained the bucket containing the last node of the attribute, the second major step is to build up the bucket path representing a record schema as the output. in this example, since the f lags of these two buckets we have found out are both true, the final result set contains bucket paths from these buckets to the root. besides this, there are other two cases to generate the bucket path which represents a category of record schemas."
"in this section, we evaluate the effectiveness of using the skeleton in schema presentation with two practical cases. the first one is designed for the scenario that data analyzers and scientists want to explore data from document stores. the second one shows advantages of the skeleton in reducing the linkage point searching space when integrating a document store with other data sources."
"the results for positioning errors are shown in fig. 7e . for all cases error was higher in experiments with conventional force feedback rendering method. during positioning task, the operator controlled the robot using low speed commands in order to perform accurate motions. that is why distance to the walls was changing slowly and smaller force feedback gain was used. in teleoperation with conventional force feedback operator could not perform accurate motion because of relatively high forces which were reflected via the master device. these forces distorted desired input from the operator and degraded the accuracy of positioning. table i presents results for student's t-test for experimental study. difference for positioning errors in teleoperation with conventional and proposed force feedback generation schemes was significant. difference in results for time measurements was not significant."
"the skeleton proposed in this study provides a single view of the data set. fig. 8b shows how qualities as eq. (4) vary with the size increasing of upper bound sets. the highest qualified, skeleton, is the attribute set generating the peak value of this plot. for the data set dbpcomp, the attribute size of the skeleton is 28. thus, it has a concise formation in comparison with the whole attribute set. the most of important is to capture the core attribute of the specific object, such as company in this example. in order to demonstrate the significance of the skeleton, we randomly select five attributes from and out of the skeleton, which are shown in table 4 and 5 respectively. comparing attributes in these two tables, it is easy to recognize attributes in the skeleton, such as name, products, location, are general to every company. however, attributes out of the skeleton are particular. for example, bankrupt is a possible property of unhealthy companies, and carbohydrates might be a noise in the data related to a set of food manufacturing companies' products instead of the company entity itself."
"the output is a hierarchical data structure, esibu-tree (encoded schema in bucket tree), whose paths persist equivalent record schema categories. fig. 4 provides an example of esibu-tree based on records in fig. 1 . each bucket has four variables. the first one is id, the identifier of a class, which is an ordered sequence. the second one is a code map cm b, the same as alg. 1, which is constituted by label-code pairs. but it only works for record schemas assigned to this bucket, so this code map is a subset of cm l in the corresponding level from alg. 1. the third one is a f lag to show whether the path from this bucket to the root represents a category of equivalent record schemas. the last one is a sub-bucket list, where sub buckets of a bucket must have different ids. in this algorithm, we begin the dividing task from the second level of record schemas (line 3) because all of record schemas belong to the same category by nature of the same root node. the procedure is as follows. in line 5, nodes in v (l) are encoded based on the cm b of the corresponding bucket. in this step, we assign each node in v (l) with a code according to its label. details for node encoding and code map updating are the same as alg. 1."
"query the schema consumption component provides the functionality to find exact answer to certain types of queries on schemas. our implementation includes two types of existence queries, namely schema existence and attribute existence."
where k min and k max are minimum and maximum marginal values of feedback gain; γ is a boundary relative speed of mobile robot and obstacle. in fig. 2 graphical explanation for (6) is shown.
"phantom premium 1.5a was connected to the computer with the model of the mobile robot and environment. the humanoperator could see visualization of the mobile robot and the obstacle on the screen. in simulation, the human-operator was asked to move the virtual robot towards the obstacle which was placed 1.5 m away from the origin of the mobile robot."
"based on these simulation results we can see that existence of force feedback cause two effects. on the one hand force feedback prevented collisions of the robot with environment. on the other hand, force feedback reduced the accuracy of position control: operator had no opportunity to approach the area near the obstacle due to high values of force feedback. based on this conclusion we suppose that it is possible to improve the quality of position control by online modification of force feedback gain."
"as discussed in sec. 1, a collection of records in a document store may persist more than one object type. hence, the data model of this collection can be presented as a set of skeletons, each of which describing an object type. the workflow of skeleton construction for a collection are shown in fig. 5 . the inputs are record schemas parsed from the schema repository. the output includes skeletons of all object types. the detailed steps are as follows."
"this section focuses on the performance of the schema existence query. in the experiment, we leverage 1000 records to generate specific data structures for record schema persistence first, and then check whether the schema of a given record has existed. since the execution time of processing one record is very short, we present the cumulative execution time on 4000 records in fig. 7a . furthermore, in each experiment, 1000 records for data structure generation and 4000 records for checking are both randomly selected from the data set, and fig. 7a displays the average execution time of five tests on each data set."
"in this paper, we consider feedback force based on the obstacle range information only (environmental force feedback) [cit] . this feedback is rendered based on measured distances from the mobile robot to the obstacles. we define the following:"
"our framework of schema management is designed for document stores, which includes three components as shown in fig. 2, schema extraction and discovery component, repository component, and schema consuming component with two functions of query and presentation."
"sec. 4.1 and 4.2 present how to group equivalent record schemas from existing data sets, which is suitable to discover distinct schemas in the batch manner. furthermore, according to alg. 1 and 2, both of them just scan a record once in the grouping. this property indicates that both of these algorithms are capable to support online equivalent record schema identification when records are coming incrementally."
"as analyzed in sec. 5.1, the procedure on each record is similar to the method of grouping records with the equivalent record schema. thus, the trend of performance on the schema existence query is the same as in the schema extraction (fig. 6) . when the maximal level is large (such as fbcomp and dbankdrug), implementations on the esibu-tree runs faster than that on the code map array. however, in the case that record schemas are flat (such as dbpcomp), these two methods are comparable."
"in order to ensure that nodes with the same label and the same descendants are assigned with the same code in line 5, this algorithm updates the label of each node by combining its ordered child codes in line 6. in our implementation, since a code is an integer, we append the ascending order of codes from children on the original parent's label. for example, for s1 in fig. 1, id and name in level 3 are encoded as 1 and 2 respectively, and then the node labelled author in level 2 is updated to author,1,2 by combining ordered codes of its children id and name (our implementation uses figure 3: examples of generating canonical form the comma as the divider). in addition, we leverage radix sort to ensure scan once on each node for sorting. as a result, equivalent record schemas are assigned with the same code which is persisted in the code map of the root level (level 1). take s1 in fig. 1 as an example, its root node is updated as root,1,2,3 and the whole record schema is encoded as 1. when a record with s4 comes, the same procedure will be executed, and s4 is also encoded as 1 because it is equivalent to s1."
"both of these algorithms are similar as procedure on each record in grouping equivalent record schemas respectively, except from generating the code map or creating a new sub-bucket to checking existences. as shown in sec. 4.4.1, these two algorithms are comparable in the performance. the difference is also triggered by the sorting step. when schemas are from a data set where records are"
"eq.(2) describes the percentage of si's attributes existed in kc, and eq.(3) describes the percentage of kc's attributes which is useless in retrieving si. the total quality is the weighted average on all record schemas in s. finally, the skeleton is defined as:"
"in this section, we present results of evaluating the schema discovery from real-word data sets, as well as the performance of query implementations. in addition, we also evaluate the effectiveness of using skeletons in the schema presentation with two practical cases. table 2 shows the statistics of data sets used in our experiments. these data sets consist of three object types, and each of them has three data sources. thus, there are nine data sets to evaluate. similar to our motivating examples described in sec. 1, record schemas in these data sets have several variations."
"to sum up, the est-based method assigns a record schema to the corresponding bucket by equivalence identification level by level top down. the output of this algorithm is a bucket tree which compresses a category of equivalent record schemas into a path. fig. 4 shows how an esibu-tree is generated/updated by processing records from fig. 1 . comparing with code maps in fig. 3, the code map in the bucket for encoding the nodes from level 2 is not expanding with the attribute sparsity and evolution on the node author, which is benefit for the performance of the radix sort."
"the first challenge in designing a schema management framework for document stores is in discovery of the schema from stored data. without an explicit definition of the schema, the need for schema retrieval from a document store results in a complex discovery process instead of a simple lookup. due to lack of constraints to guarantee only one object type in a single collection 1, a collection may contain records corresponding to more than one object type. moreover, schemas of the same object type in a collection might also vary because of attribute sparseness in nosql as well as data model evolution caused by highly interactive adoption and removal of features. for example, in a real-world scenario using dbpedia [cit] (a knowledge graph retrieved in json from its web api as described in sec. 7), the 24,367 records describing objects of type \"company\" have 21,302 different schemas. note that a simple sampling of the records to examine their schemas would fail to capture the full schema because almost every record may have a distinct schema. hence, the first problem we study is how to efficiently discover all distinct schemas appearing in all records."
k ← km; qmax ← q(km) 17: end if 18: end for algorithm 9: skeleton construction alg. 9 shows details of constructing the skeleton k. lines 1-8 generate two temporary variables γ(p k ) and φ(p k ) to avoid duplicate computations in calculating the feature pointed by theorem 1.
"attribute equivalence identification engine is the backend to identify attribute equivalences as defined in sec. 3. clustering aims to differentiate object types. as there are many methods and solid studies related to these parts, we will not discuss them elaborately. for readers interested, here are surveys on schema matching [cit] and clustering [cit] respectively."
"where ni is the number of records whose record schemas are equivalent to si. eq. (5) shows αi for the gain function is proportional to its frequency, which means the skeleton tries to retrieve more information of the record schema which appears in the data set frequently. eq. (6) shows βi for the cost function is inversely proportional to its frequency, which implies the skeleton tolerates the redundancy in the highly frequent record schemas. to sum up, this heuristic idea for designing weights is to make the skeleton be inclined to frequent record schemas. the assumption here is that the frequency of a record schema represents its importance and significance in the data set."
"this section presents the skeleton construction process performed in the schema presentation function of the framework. in sec. 6.1, we describe how to process a collection of records containing multiple object types. then, sec. 6.2 presents details of the skeleton construction for a specific object type."
"in this paper, we presented a framework for schema management for document stores that deals with the schema-less data model and fast-evolving nature of document stores. we proposed a new data structure, esibu-tree, to persist record schemas as well as support queries, and an equivalent sub-tree (est) based method with a linear computational complexity to group equivalent record schemas. we compared the effectiveness of this data structure with a baseline approach using canonical forms. extensive experiments demonstrated the efficiency of the overall framework, and also showed that the est-based method outperformed the cf-based method in both discovery and query tasks. furthermore, we proposed a new concept \"skeleton\" to describe a schema summary structure suitable for document stores. practical use cases were presented to demonstrate the effectiveness of the skeleton in real data exploration and integration scenarios."
we propose a scheme for online modification of force feedback gain in mobile robot teleoperation system. the main idea is modification of gain k i in (3) based on distance vector r and its time derivative dr/dt. we define variable gain k * i for generating force feedback based on distance measured from i th sensor as follows:
"the rest of this paper is organized as follows. the schema management framework is described in sec. 2, and related preliminaries are presented in sec. 3. sec. 4, 5 and 6 present technical details of our solution to address the above mentioned challenges. we present experimental evaluations in sec. 7. sec. 8 discusses related work and sec. 9 concludes this paper."
"to further investigate the effectiveness of using the skeleton in schema presentation, we use an integration scenario from previous work [cit] where the goal is discovering linkage points between data sources. briefly, a linkage point is a pair of attributes from two sources that can be used to effectively perform record linkage (or entity resolution), i.e., to link records between the sources that refer to the same real-world entity. the final goal is to integrate all sources into one duplicate-free (clean) source that contains data from all sources. a challenge in linkage point discovery for schema-less data such as those used in our experiments is the large search space in terms of the number of attributes. for example, in the dbankdrug and dbpcomp case, investigating all the possible pairs of attributes with the goal of finding linkage points means investigating 179,632 attribute pairs, each of which could contain a large number of values of different data types and characteristics. previous work has proposed efficient methods of investigating and pruning the search space using extensive pre-processing and highly efficient indexes. here, we would like to examine how using the skeleton instead of all search space can be effective in finding linkage points. in other words, we would like to study the correlation between attributes that appear in linkage points and those that appear in the skeleton. table 7 shows the search space for each of the nine integration scenarios in terms of the number of pairs of attributes that need to be investigated, along with the number of linkage points found in the search space comparing the use of the skeleton with the use of maximal common set and the universal set. the results show that using the skeleton to prune the search space for linkage point discovery is very effective overall. for example, for the dbpdrug/dbankdrug scenario, using the skeleton reduces the search space (pair#) to only 8% of the universal set search space whereas 19 out of the 21 linkage points can be found in this space. another example is dbpcomp/seccomp scenario, the search space is reduced to 1% by using the skeletons whereas 7 out of the 8 linkage points are found. overall in these nine scenarios, using skeletons reduce search spaces to 6% of the universal set on average, and find out average 58% of linkage points. it is important to note that based on the manual verification of linkage results, those linkage points that include attributes from the skeleton are also very effective linkage points. recall that the ultimate goal of discovering linkage points is at generating high quality record linkages, so we assess the effectiveness of the discovered linkage points by the quality of the record linkage generated by using these linkage points. for example, in the dbpcomp/fbcomp scenario, there are two linkage points among the 13 linkage points found in the search space of skeletons that can achieve perfect accuracy in record linkage (i.e., link all the records that can be linked, with 100% precision). this also shows that the use of the skeleton can improve state-of-the-art in linkage point discovery and ranking."
"as the analysis shown in sec. 5.2, the difference of them is related to checking regions. for the query implemented on the code map array, we have to scan all labels in cm l, and there are 7020 label-code pairs in the code map array for fbcomp. for the query implemented on the esibu-tree, the scan region is the sub-bucket list of a bucket. in the esibu-tree of fbcomp, there are 2376 sub-buckets in total, but for a specific attribute (especially longer ones), it does not need to check all these sub-buckets by filtering out buckets without upper level nodes. therefore, the attribute existence query on the esibu-tree overall outperforms that on the code map array. in addition, since both of them have to check all nodes in the given attributes, the performance also depends on the length of the attribute, as a result, execution times are increasing with the attribute lengths increasing."
"the importance for the efficiency of the schema discovery is more evident for online inserts where schemas of records are to be identified incrementally and the schema repository is to be updated in real time. to tackle this challenge, we propose a new data structure for both schema discovery and storage, esibu-tree, and an equivalent sub-tree based algorithm to discover schemas from existing data as well as an online method for new inserts."
"in details, they are the same in boundary cases. one of them is all record schemas in the collection are equivalent, so there is only one path in the esibu-tree which is the same as the code map array. another case is there is no common attribute for any two distinct record schemas. in this case, the sum of code maps in a level of the esibu-tree is the code map in the same level of the code map array, as a result, their space consumptions are the same."
"in this section, we propose the variable force feedback which will not degrade performance of mobile robot motion control. in cases when mobile robot is located in large workspaces without many obstacles, mobile robot has sparer place for moving without collisions with static obstacles. therefore, the probability of collisions between mobile robot and environment during teleoperation in large workspace with fewer obstacles is low. on the contrary, in small workspace, mobile robot will have higher probability to collide with obstacles due to limited spare space. force feedback which is based on obstacles in large workspace will be smaller and will give less negative effect on the quality of motion control than force feedback which is generated in small environment. it is also important to consider relative speed of mobile robot and obstacles. if the mobile robot moves with high speed then the probability of collision with obstacles is high. in cases, when it is required to perform accurate motion control, the mobile robot is teleoperated with low velocities. in this case, the distance between the robot and the obstacles decreases slowly and probability of collision is low. in many teleoperation applications mobile robots operate in dynamic environments where obstacles can appear, disappear and/or change their locations. in such cases, force feedback should not unpredictably change its magnitude and direction. based on the conditions, described above, we propose to render haptic feedback which will be variable to distances to the obstacles and speed of the mobile robot."
"end while 9: end for algorithm 1: cf-based record schema grouping alg. 1 processes nodes from the record schema level by level bottom up. for each node from v (l), we encode it with a code based on its label (line 5). such label is a sequence constituted by its original label and ordered codes from its children (line 6)."
"take the est-based method as an example, for a newly inserted record, operations on each record in the batch manner (i.e., alg. 2 lines 2-11) are executed. if the schema of this record has not been persisted, a new path on the esibu-tree will be nominated to represent this new category of record schemas (either the flag of a bucket is turned to true or a new path is added); otherwise, esibu-tree will not change and some statistics (e.g., the frequency of the hit record schema category) will be updated if needed."
"alg. 3 shows the detailed procedure of executing query 1 on the code map array. besides filtering record schemas higher than all persisted ones in lines 2-4, the major task is to check whether labels updated by combining their ordered children's codes have been contained by the corresponding code map (lines 5-16)."
admissible range of gain k is reduced by c h . the range of c h can vary a lot for different humans and conditions. that is why it is important to consider the uncertainty of human-based control during select the value of k.
"conversely, the simplicity developers achieved from using json and document stores leads to difficulties in certain data management tasks, which are probably beyond their duties. let's consider following scenarios: a data scientist wants to explore an application's data for analytic purposes; or an application is required to share its data with another application; or a database administrator wants to enable fine-grained access control; or data are required to be integrated into a data warehouse. all of these tasks are usually facilitated by schemas. however, since the design and specification of data structures are tightly coupled with data in json, and unlike other semi-structured data (e.g. xml) that are usually associated with an explicit schema, these tasks have to request developers' assistance or study design documents or even read source codes to understand the schema. unfortunately, neither of these solutions are practical in real world. as a result, there is a need for a schema management function for document stores, like an rdbms's data dictionary which extracts schema definitions from ddl, stores them in a repository, and enables exploration and search through query interfaces (e.g. \"select * from user tables\" in oracle) [cit] or commands/tools (e.g. \"describe table\" function in db2) [cit] ."
"schema extraction and discovery this component provides a transparent way to discover all schemas in records. the first function this component provides is extraction of the schema of each record from input json records. fig. 1 shows four json records and their corresponding schemas which we call record schemas. for existing data, this component discovers all distinct record schemas by grouping the equivalent ones into categories. for a new record, its record schema is compared with the current existed record schemas, and persisted immediately if it is a new structure. in this study, we apply a method based on the notion of canonical forms [cit], and propose a novel hierarchical algorithm for schema discovery. schema repository this component is responsible for schema persistence of document stores, and also supports the efficient extraction process and schema consuming. a new data structure, esibu-tree, is proposed for the repository in this study."
"we set the level of the root node in a record schema as level 1, and for the other nodes, its level is one more than its parent's level. the set of nodes in the l-th level is denoted as v (l). the maximum level of a record schema is the largest level among leaf nodes, denoted as lmax. for example, the maximum level of s1 in fig. 1 is 3 ."
"if k satisfies the above condition then mrt system will be stable. however, the system in fig. 3c does not represent the real application of mrt. usually, in mrt operator is given a task to move the robot to desired remote location. visual information (image from remote cameras, interactive maps) is used to track the robot's position. therefore, in mrt tasks, human deals with position tracking control in which human's brain, vision, neural and muscle systems are used as tracking controller. in order to find the permissible range of feedback gain k in which the overall teleoperation system will be stable, we analyze the system shown in fig. 3d . x des r is desired robot's position defined by the task. c h represents the human's brain and neural system as a position controller. for simplicity, we assume that c h is a constant scalar value which means that operator does linear p -control of mobile robot's position. the closed loop system with consideration of position control is defined as follows:"
"given that the schema-less nature and abandoning of schemarelated apis are main reasons that developers use document stores, it is not feasible to enable schema management through a change of interfaces and apis or enforcing a data model. therefore, what we need is a new schema management framework that can work on top of any document store, along with new schema retrieval and query mechanisms different from those implemented in rdbmss."
"there are two major differences in mrt when it is compared with conventional teleoperation systems of manipulators. first, mrt mainly uses rate mode teleoperation due to the limited workspace of the master device and unlimited workspace of the slave (mobile) robot. second, the feedback force, displayed to the human operator, is not the reaction force from physical interaction between mobile robot and environment. therefore mrt should be considered as specific type of bilateral teleoperation systems with different architecture and feedback force rendering method. in this paper, we analyze stability of mrt with environmental force feedback considering dynamics of human-operator. analytical solution for designing the environmental force feedback was derived. disadvantages of mrt with constant force feedback gain are shown through simulations. new force feedback rendering method based on variable force feedback gain was proposed. experiments proved that proposed force rendering method improves the quality of mrt."
accuracy of the mobile robot's motion was compared in teleoperation with conventional and variable force feedback in second experiment. mobile robot was placed in a narrow space and operator was supposed to move the box-type object from initial to desired position. the operator could see the experimental environment via cameras. subjects were asked to do the positioning of the object task with conventional and variable force feedback.
"there are also several studies on frequent substructures mining [cit] . however, under the context of document stores, due to the flexibility of developers ingesting data and the heterogeneity in data itself, it is hard for users to specify any parameters to pre-define the frequency on the data they want to explore. the skeleton construction method proposed in this paper aims to design a parameter-free approach which automatically provides a balance between the frequency of attributes and the size."
"in this paper analytical and experimental study of mrt with constant and variable force feedback gain was presented. stability criteria for feedback gain were driven. experimental study showed advantages of proposed force rendering method. as it was expected variable haptic feedback reduces amount of force which is displayed to operator. one can say that on the one hand it can decrease the safety of teleoperation process. however as a result, it improves the quality of motion control since input from the human is not distorted by reflected forces. however, when it is compared with constant gain scheme, variable approach provided better way for maintaining safety and high quality of the motion control at the same time. application of variable feedback gain made the trajectory of the mobile robot smoother. operator could control position of the mobile robot more accurately. as a result, quality of the motion control was improved."
"schema presentation given the variety of record schemas in a collection, this functionality aims to provide a summarized representation of schemas. our implementation is based a concept \"skeleton\" for json records, which is a parameter-free method to display core attributes in a concise format."
"this section presents the query function of the schema management framework. in this section, we present two kinds of existence queries, which are schema existence query and attribute existence query. in each query, we first introduce its motivation, then propose a sql-like api, and at last present algorithms to implement it based on the code map array and the esibu-tree."
"we implement this attribute existence query on the code map array and the esibu-tree in alg. 5 and alg. 7 respectively. in the code map array, a record schema is represented by a code in the code map of root level, so alg. 5 returns a set of codes. similarly, alg. 7 returns a set of bucket paths. a record schema can be retrieved based on the code and the bucket path respectively. alg. 5 shows procedure of checking the attribute existence in the code map array. beside determining the existence by the maximal level (line 1), the core operations are to check the existence of"
"where x r is position of the robot, m and b are mass and damping of the robot. u is a control input. speed of the robot was controlled by p -controller with control gain k v ."
"in this study, we propose an algorithm following a divide-andconquer idea, which reduces the number of sorts from the number of non-leaf nodes to the maximal level, and generates a local code map instead of the global code map for reducing the radix size in the sorting."
"in both cases due to existence of environmental force feedback there were no collisions with the walls. however, the quality of the robot's motion was different. during teleoperation with conventional force feedback large amount of force feedback was reflected to operator because distances to the walls around the robot were small. these high values of force feedback gave high impact to position of the master device which was often unexpected to operator. that caused relatively large change for robot's linear and angular velocities. in teleoperation experiments when variable force feedback was used force feedback was proportional to the speed of the robot, and that is why it was not so high and did not distort the human's input in the master device."
"some applications (e.g., data exploration for analytic purposes) require a single view of the data model of a collection, but record schema variations make it a non-trivial task. in order to present the data model, one can return the union of all attributes, or a ranked list of the attributes based on their occurrence frequencies, or the intersection of record schema sets across all records (i.e., those with 100% occurrence). however, as described in sec. 1, these approaches have drawbacks (e.g., missing prominent attributes, less informative, etc.) in practice because extensive heterogeneity often presents in record schemas. we therefore define the concept of \"skeleton\" to approximate the essential attributes of an object type, which is loosely related to the schema definition. skeleton k is the smallest attribute set to capture core attributes of the record schema set for a specific object type."
"since the est-based method splits the global code map into bucket code maps, it leads to duplications. for example, in fig. 4(c), the label 3,author appears twice. at the same time, for the code map array, the sizes of code maps expand, because each code map includes labels with common parts, such as author,1,2 and author,2 in the level 2 of fig. 3 . therefore, the worst cases of space complexities in these algorithms are roughly the same, which are linear with four factors: the number of equivalent record schema categories n, the average attribute size in a record schemam, the maximum level lmax, and the average length of each label len."
"furthermore, the est-based method also reduces the size of the code map. the code map of a bucket is generated by record schemas assigned into it, meanwhile, the code map array considers the whole data set. as a result, the code map of esibu-tree is part of the code map in the corresponding level from the code map array, which is of benefit to radix sort."
"schema parser this step is to parse the specific data structure into distinct record schemas for the following study. in this study, we have presented two data structures to persist record schemas of document store, which are code map array and esibu-tree. in the code map array, a category of equivalent record schemas is represented by a code in the code map of the root level (level 1) as shown in fig. 3 . the retrieval process starts from the corresponding label of this code, and leverages code maps in each level to append subtrees on a record schema iteratively. in the esibu-tree, the path fig. 4 . for such a path, the retrieval process starts from the root bucket, and leverages id and cm of each bucket to append nodes level by level iteratively."
"attribute existence query aims to determine record schemas containing a specified attribute, which provides a finer granularity pre-checking by locating the attribute. moreover, this query could be used to identify different object types due to lack of schema names (multiple objects in one collection), or the version of an object schema for the sake of the evolving data model caused by highly iterative developments, with a specific attribute they contain. the sql-like api is as:"
this section focuses on the schema discovery function in the framework. our goal is to discover all distinct record schemas. the output is a specific data structure to persist record schemas in the repository.
"the aim of setting these criteria for candidates is to meet the requirement of \"smallest\" in the skeleton by avoiding noises from attribute evolution. furthermore, in order to select the skeleton from candidates to meet the \"core\" requirement, we propose a quality measure based on the trade-off between significance and redundance of attributes in record schemas."
"teleoperation of mobile robot p3dx was done in order to evaluate the influence of proposed variable force feedback for the quality of the system. teleoperation was done via manipulating phantom device using the scheme shown in fig. 1a and control strategy described by (1) in first experiment, the operator was asked to navigate the robot through narrow corridor with conventional and variable force feedback rendering methods. fig. 7b and fig. 7c show the robot's trajectories from the experiments with constant and variable force feedback gain respectively. trajectories from experiments with variable force feedback gain were smoother than trajectories from experiments with constant gain. trajectories for proposed variable force feedback were more neat and similar to each other while trajectories for conventional method were messy and chaotic. time required for passing through the corridor for each subject was measured as well (fig. 7d) . in cases when variable force feedback was used subjects could complete the navigation task faster."
"input: a set of codes: c, a code map: cm l, a node: v l, and a code: code l+1 output: a set of codes:"
"where t is completion time. positioning error was selected as a metric for measuring the quality of human-robot interaction in mrt. average position error in teleoperation can tell us how well the robot follows reference input from the master side in different conditions. that is why error is a good metric for evaluating performance of the system. in experiment, time was limited by 5 s. each subject had five trials and average positioning error was reported. summary from 10 subjects is presented in fig. 7a . in all cases, variable feedback force allowed subjects to position the robot with smaller errors than with feedback force with constant feedback gain. average improvement for all subjects was 57.5%."
"in this study, we apply the method for generating depth-first canonical form [cit] to group equivalent record schemas. since the canonical form specifies a unique representation of a labelled rooted unordered tree, equivalent record schemas can be grouped together based on the same canonical form. construct the record schema s of r, and obtain lmax of s 3:"
"the implementation of query 1 on the esibu-tree is shown as alg. 4. in the esibu-tree, there are three conditions to determine the existence. the first one is the label has to be contained by the code map of corresponding bucket (lines 4-9). the second one is the bucket has a sub-bucket with an id the same as ordered code sequence, which indicates such combination of nodes has appeared (lines 11-17). the last one is the f lag of the final bucket is true, which means the equivalent record schema category presented by the bucket path from this bucket to the root has been persisted (lines 19-23) ."
"in fig. 1a, configuration of a two link master manipulator and mobile robot are shown. operator gives motion commands through the master haptic manipulator. control inputs for mobile robot are based on the position of end-effecter (x m, y m ). v, ω are linear and angular velocities, respectively. obstacle range information, which is obtained from the robot's sensors, is sent back to the master device. force feedback is generated by the master device based on obstacle range information. position-speed command strategy is used for most of teleoperation applications of the mobile robots. the speed of the robot is changed with respect to the position of the master device. this control strategy is based on (1)"
"this section considers the problem of constructing the skeleton describing a specific object type. recall the definition in sec. 3, we formulate the skeleton construction as finding out the highest qualified attribute set. the quality of an attribute set kc is as follows:"
"in the data extraction procedure, we persist data with the same object type from a data source into a data set. in other words, we will not consider the difference in record schemas caused by different object types, so the clustering step is not evaluated in this study. moreover, in this study, we leverage an existing highly efficient schema matching engine to identify equivalent attributes [cit] . after the equivalent attribute combination, the attribute size of data sets are: 303 for the dbpdrug, 1,472 for the dbpmovie, 1,648 for the dbpcomp, and 109 for fbcomp. for other data sets, their attribute sizes are the same as shown in table 2 ."
"this section focuses on the performance of the attribute existence query. in order to evaluate the performance of this query under attributes with different lengths, we leverage fbcomp as the data set because its maximum level is large and its attributes have various lengths. table 3 lists attributes used in this experiment, and the corresponding results are shown in fig. 7b ."
"in addition, previous work has studied schema summarization based on quality measures [cit] . this work addresses the problem of summarizing the schemas of multiple tables based on their linkages, but the goal in our schema presentation function is to find a single data model for every object type in one collection. furthermore, their quality measures for summarization are defined based on connectivity of tables, whereas the skeleton focuses on finding a balance between the significance and the size of the summary."
"in future, we are planning to devise algorithms to retrieve the skeleton for other nosql applications, such as when some priori but uncertain knowledge of the data set is available. furthermore, we plan to study the evolution of data models of a specific data set persisted in data stores based on skeletons corresponding to different versions of applications."
"schema existence query aims to check whether a specified record schema has been persisted. similar to rdbmss that have a function to allow users to check the existing table list and table definitions, this query mechanism could be broadly used by developers to find the right collection to persist a defined object. for example, suppose the developer is a newer in a project, to execute this query as a pre-checking helps him to decide whether to insert a record to the collection which persists records with the same schema, or to create a new collection to insert. the sql-like api is as:"
"considering the performance of implementations on the code map array and the esibu-tree, it seems that procedure of searching the final bucket paths on the esibu-tree is more complicated, however it is just comparable with the recursive searching level by level as alg. 6. sometimes, it is even faster since we only need to check the f lag. the most time consuming part of them are both related to code existence checking. for the query on the code map array, we have to scan all labels in cm l (as alg. 6). for the query on the esibu-tree, the scan region is the sub-bucket list (as alg. 8). since the number of sub-buckets is always fewer than the size of the code map, the query implemented on the esibu-tree runs faster than that on the code map array."
"for a unified treatment of the quantum and quantum-mimetic versions of the fig. 1 setup, we shall employ quantum analysis for both. quantitatively identical results are obtained for the quantum-mimetic case when: (1) we use classical wave propagation for the object-transparency and beam-splitter interactions; (2) we use the classical-noise model for the action of the wavelength-converting phase conjugator; and (3) we use the semiclassical (shot-noise plus illumination excess-noise) theory of photodetection. for simplicity, in what follows, we shall discretize in both space and time, i.e., we will take the source's signal and idler outputs to be a collection of modes with photon-annihilation operators"
"an additional point, which emerges from our gaussian-state analyses of 17, 21, is that both rely on stimulated, rather than spontaneous, emissions from the wavelength-converting phase conjugator in fig. 1 . this conclusion follows from equation (5) for the average photon number of the conjugator's mth signal-pulse output at pixel (j,k). here, the first term on the right represents signal photons whose emissions were stimulated by the presence of idler photons at the conjugator's input, while the second term on the right represents signal photons whose emissions occurred spontaneously. that stimulated emissions are responsible for the quantum and classical-state images we found earlier is then obvious from the resulting nonzero phase-insensitive cross correlation between the two signal-wavelength beams arriving at the beam splitter in fig. 1, i. e.,"
"three-wave mixing can also be used to phase-conjugate a light beam that had only a phase-sensitive cross correlation with a companion beam. the resulting conjugate then has a phase-insensitive cross correlation with that companion, which can be sensed via second-order interference. this possibility was exploited, theoretically in ref. 15 and experimentally in ref. 16, to realize phase-conjugate optical coherence tomography, in which classical-state signal and idler beams-of the type mentioned in the preceding paragraph-yielded the axial resolution and dispersion immunity afforded by q-oct without the need for nonclassical light."
"here the subscript in the numerator is used to indicate that we only include the object-related portion of the average images, not the object-independent background terms that appear in equations (11)"
". given that we have shown the former has a quantum-mimetic (classical-state) counterpart which retains that system's essential imaging characteristics, it behooves us to comment on whether a similar situation prevails with respect to the latter. indeed it does. the zou, wang, and mandel experiment is assuredly quantum: it employs nonclassical light and hence requires quantum photodetection theory for its analysis, which those authors accomplish via the biphoton approximation for the post-selected outputs from an spdc source. that said, however, our gaussian-state treatment of spdc is more rigorous; for example, it includes the multiple-pair emissions that account for the accidental coincidences seen in hong-ou-mandel interferometry even when detector dark-counts are negligible 1, 12 ."
"phase-sensitive cross correlations cannot be sensed in second-order interference. hence the aforementioned experiments' reliance on photon-coincidence counting, which is a fourth-order interference measurement able to sense phase-sensitive cross correlations 12 . more important, for our purpose, is the fact that classical light beams can have phase-sensitive cross correlations. indeed, ref. 13 showed theoretically, and ref. 14 verified experimentally, that signal and idler beams in a zero-mean jointly gaussian classical state-determined by their non-zero phase-insensitive auto-correlations and phase-sensitive cross-correlation-yielded ghost images with almost all of the properties of the quantum case."
"and the upper possibilistic mean-standard deviation model was max −3.73x 1 + 4.89x 2 + 6.43x 3 + 1.44x 4 + 3.06x 5 s.t. table 8 shows the results, where l * denotes the optimal solution in model (17), and u * denotes as the optimal solution in model (18) ."
"to sum up, omp algorithm selects only one atom at iteration, resulting in lower accuracy and efficiency. sp, swomp and romp algorithm select more than one atom each time by improve the atomic selection strategy, which simplifies the omp algorithm and improves the efficiency of omp algorithm, but the recognition accuracy is easily affected by sparsity or threshold parameters. the samp algorithm improves the accuracy and efficiency of omp algorithm by step size approximation, and the efficiency is higher than that of omp algorithm. but it depends on the initial step size, and the step size is fixed. the sparsity estimation is not accurate enough, and the recognition rate is lower than the maomp algorithm. maomp algorithm introduces rough estimation of sparsity in solution, and the subsequent each stage step decreases gradually makes the approximation of sparsity is more accurate, which is higher than other improved omp algorithms, and the average running time is significantly less than the omp algorithm and samp algorithm. especially in more samples, when dimension is higher, it has more obvious advantages."
"when the above information is given, we have provided all prior knowledge for dealing with a portfolio selection problem. now, we give a brief description of the portfolio selection model in the following subsection."
"it is very important to solve sparse coefficients for sparse representation theory. that is how to use a small number of atoms in a redundant dictionary to represent the original signal. aiming at the disadvantages of greedy algorithms, a modified adaptive orthogonal matching pursuit (maomp) algorithm is proposed in this paper to improve the accuracy and efficiency of the greedy algorithm. the maomp algorithm does not need to input the true sparsity, the initial value of the sparsity is estimated by matching test, and the estimated value to meet condition is taken as the length of the support set. then, the atom is selected from the projection set, and the index set and the support set are updated. the original signal is estimated by retrospective thinking and least square method, and update residuals. finally, the number of filtered atoms is adjusted by stages and variable step size. approach to true sparsity and lead to better sparse representation."
"based on sparse representation classification algorithm, an improved adaptive orthogonal matching pursuit algorithm (maomp) is proposed to solve the problem of low precision and uncertain parameters of greedy algorithms in sparse solution. maomp algorithm introduces the sparsity estimation step. the sparsity initial value is estimated by matching test. the variable step size is added to adjust the filtered atom number by stages, and approaches the true sparsity. experimental results show that the performance of maomp algorithm is better than that of other improved omp algorithms, especially when there are more samples and higher dimensions. the next step is to add some occluded images to identify the robustness of the algorithm."
"wherex n and s n are the sample mean and the sample standard deviation of the data, m 0 is a specified value, n the sample size and t n−1;1−"
"in our paper, we provide the risk level k for investors to make decision. we need to decide the value k first and solve the linear programming model many times until we get the solution with only one exchange currency. because of setting the value k in the model, we have many expected returns which depend on the value k. we obtain a maximum return with different risk levels in our model and make a decision for selecting the best return by a fuzzy statistical test. we conclude that it is conservative investment and more objective for investors to make decision when they buy many exchange currencies. we also conclude that the evaluation by the fuzzy statistical test enables us to obtain a stable expected return and low risk investment with different choices based on the risk level k."
"example 2 we selected five exchange currencies (usd, [a, b], where a is the minimum price, and b is the maximum price in one day. we presented some interval values in table 1 . suppose that we buy the five exchange currencies with opening prices on july 1. we assume that an investor buy 5 exchange currencies on july 1, and do not take any action from the day he bought around half a year. our objective is to choose a portfolio that maximizes the return (interval values) on the investment under some constraints on the selection with different risks k. moreover, we make the decision for selecting the best return by a fuzzy statistical test."
we solved model (15) by using gp-igp (linear and integer goal programming). the result depends on the selection with different values of k. we gave the value k greater than zero and accurate to second decimal places in this example. we present the result in table 6 . (14) and (15)
"because of the environmental factors, the actual sample collection will be affected by noise, light etc. the test samples cannot be better represented by linear combination of training samples, and the recognition rate will be affected easily. in order to improve robustness, a noise constraint can be added:"
"the rest of the paper consists of the following. section 2 gives a brief review of the related studies. the main method is described in sect. 3. section 4 illustrates empirical studies with interval values and how to apply a fuzzy statistical test on the portfolio selection model. finally, the concluding remarks and the topics of further studies are presented in sect. 5."
"the basic idea of sparse representation is that the training samples are constructed as redundant dictionaries, and the test samples can be represented by sparse linear combination of the dictionary elements, then sparse reconstruction algorithm is used to solve the sparse coefficients [cit] . sparse representation classification algorithms are mainly concerned on two aspects: the construction of redundant dictionaries and the solution of sparse coefficients. the framework model of sparse representation classification algorithm is introduced."
step 5. solve the optimization model (9) with different values k. stop solving the model when the optimal solution is indicated with only one asset and step 6. calculate k by definition 7.
"the results base on a fuzzy statistical test can indicate two informations as follows. 1. in our paper, we tested the expected return by a fuzzy statistical test, the results indicated whether we should accept or reject the expected return. 2. since the expected return was solved from the portfolio selection model and the parameters in the model were calculated by estimated parameters of underlying distribution function, we conclude that if we accept the hypothesis (i.e. expected return), it means that it is no problem from data extraction to get an expected return based on the value k."
the empirical studies showed that we could provide many risk levels and expected returns. it's more objective for investors to make decision when they buy exchange currencies. but we still have further points need to improve in the future as follows:
"beginning from sparse representation classification algorithm, this paper introduces sparse estimation and variable step size to aim at the disadvantages of existing greedy algorithms. a modified adaptive orthogonal matching pursuit (maomp) algorithm is proposed, and it is validated on the gesture samples. the results show that the performance is better than other algorithms, and the accuracy and efficiency of the algorithm are improved."
"in this paper, we attempted to establish a fuzzy statistical test. we proposed a method to defuzzify fuzzy data. that is, we used central point and radius instead of interval data. therefore, the central point and radius were simplified to real numbers and had statistic characteristic. we estimated the probability distribution by using central point and radius and calculated the expected value and variance based on the estimated parameters of the underlying probability distribution. we supported the efficacy of the proposed method through an application of maximizing investment portfolio of foreign exchange currencies. an empirical study of a portfolio selection model was conducted based on a fuzzy statistical test in example 2."
"in order to verify the estimation effect of sparsity, the estimated value of sparsity s are compared when the parameter δ s are taken different values. 50 training samples and ten test samples were randomly selected from each gesture in grab gesture library, and reduce the dimension to 100. 50 training samples and two test samples were randomly selected from each gesture in asl sample library. the experimental results are shown in figs. 3 and 4."
"gesture recognition is widely used in the field of artificial intelligence and pattern recognition as a natural way of interaction, for example, dynamic gesture recognition and pattern recognition of robot multi fingered grasping [cit] . data glove sensors and emg signal acquisition devices, and cameras are most widely used. gesture recognition based on data glove has high recognition rate and fast speed. but this method requires users to wear complex data gloves and position tracker, which does not meet the requirements of natural human-computer interaction. the price of data gloves is expensive, and it is not suitable for extensive promotion. gesture recognition based on emg signals is mainly to collect multi-channel semg signals by sensors, then extract the characteristic parameters of each gesture, and finally realize gesture recognition [cit] . the advantage of this method is that emg signals are not affected by the external environment, so they have better real-time performance. but because of the individual difference, the difficulty of classification is increased, and it needs to be equipped with emg acquisition device, which brings inconvenience to the application in reality. the gesture recognition based on vision mainly uses camera to capture gesture images and identifies gestures by image processing and related algorithms [cit] . the advantage of this method is that the input device is cheap, and the camera is becoming more and more popular in all kinds of consumer electronic products, and it does not add any additional requirements to the manpower, so that the interaction between the computer and the human is more natural. therefore, more and more researches have been done on vision based hand gesture recognition, and the recognition rate and real-time performance have been greatly improved. it relates to the techniques include gesture detection, gesture segmentation, feature extraction and classification recognition [cit] etc. although these technologies have developed greatly in recent years, the complicated background environment and low performance of classification algorithms are often encountered in acquisition gesture by visual sensors. therefore, there are still some challenges for accurate gesture recognition [cit] . in recent years, the proposition and development of sparse representation theory provide a new approach for the pattern recognition [cit] . it shows great development potential and broad application prospect. john, [cit] first proposed the sparse representationbased classification (src) framework. redundant dictionary is constructed by training samples for this method [cit] . the test sample is expressed as a sparse linear combination of training samples by the sparse solution algorithm [cit] . finally, the minimum residual error is classified, and the results show that the method proposed has a good performance."
"the asl sample library contains 26 letters of gesture (letters j and z gestures are dynamic, so this article will not be considered). the gesture pictures are collected separately for five people by kinect in the same light and scene. each operator and each letter has a color image and a depth image, the color images of 24 letters are collected, and collects 20"
"in the formula (9), γ j (y)is corresponding to the residual error of the class j sample. in the formula (8), δ j (x) is corresponding to coefficient value of the class j sample. other location is 0 and i (y) is the categories of testing samples. the steps of sparse representation classification algorithms are summarized below:"
"let x 1, · · ·, x n be a sequence of random variables (not necessarily normally distributed). we say that the x i are independently identically distributed (i.i.d) if the x i are independent and have the same distribution. we write"
"the portfolio selection model was built by using expected value and variance of central point and radius. the expected value and variance was calculated by the estimated parameters of underlying distribution function. we evaluated the best return by a fuzzy statistical test. in this procedure, from data extraction to fuzzy statistical test, it is no doubt that the model can deal with the interval data, so does the fuzzy statistical test, because we had \"defuzzifying\" fuzzy data into real numbers before we solved the portfolio selection model. hence, the model becomes to a traditional linear programming model."
"the interval value of the fuzzy expected return was [0. 28, 1.20] . using the same method for the other values of k, we can also obtain the respective interval value of the fuzzy expected return. we present the results in table 7 ."
"when we knew the distribution function, we used the moment method estimator (mme) to estimate the parameter for each distribution function. hence, we could find out the expected values and variances by using those parameters. table 5 shows the results."
"in this example, we conclude that the maximum fuzzy expected return was [5.50, 6 .56]. we present a scatterplot of hence, the lower possibilistic mean-standard deviation"
"we thought that if we can estimate the returns with triangular fuzzy numbers or trapezoid fuzzy numbers in the future, it will make the proposed method more realizable. 2. in the proposed portfolio selection model, we gave a constraint inequality with risk level k which was given in advance. we made the value k greater than zero and accurate to second decimal places in our paper. we thought that we can give more risk levels and results for investors to make decision in the future. 3. in fact that the financial market is affected by many non-probabilistic factors and the future returns of risky assets cannot be predicted accurately in any uncertain economic environment. although we can evaluate and select the best return by a fuzzy statistical test, we thought that we also need to consider financial reports, experts' individual experiences and other factors in real world."
"the recognition rate and average running time of different matching pursuit algorithms in asl sample library are shown in figs. 10 and 11 respectively. as can be seen from figs. 10 and figure 11, both the recognition rate and the average run time increase with the increase of dimensionality. n the asl sample library, more samples are selected, so the recognition rate and average computing time are relatively high compared with grab gesture library. in fig. 10, the recognition rate of the maomp, samp and sp algorithms is above 85% at low dimensionality, and the recognition rate of swomp, romp and omp algorithms is maintained at 80-85%. in high dimensionality, the recognition rates of maomp, samp, sp and swomp are close to 95%, and fig. 11 the variation of average running time on each algorithm with the dimensionality in asl sample library the romp and omp algorithms are close to 90%. maomp algorithm has the highest recognition rate, followed by the samp algorithm, and the omp algorithm is the smallest under the same dimension. in fig. 11, the average running time of different algorithm is shorter about 0.002 s at low dimensionality. the running time of each algorithm varies greatly at high dimensionality. when the dimension is 200, the average running time of the omp algorithm is 0.054 s, the samp algorithm is 0.034 s, the maomp algorithm is 0.025 s, and the other algorithms are below 0.020 s. under the same dimension, the average running time of omp algorithm is the longest, then the samp algorithm is second, and the romp algorithm is the lowest."
"the algorithm of this paper also adopts the idea of variable step size, and the effect of different values β on the algorithm step size coefficient recognition rate (%)"
"the objective of this paper is to build a statistical test of fuzzy data, and apply it to a portfolio selection problem with interval values, and then statistically evaluate the best return. in the first step, we need to find the probability distribution function and each parameter in the probability distribution functions. when we know the distribution function of each parameter, we can easily calculate the expected return and variance. those values can enable us to define a portfolio selection model with interval values. we also give a decision by a fuzzy statistical test, which explicitly tells us whether or not we statistically accept the risk in investment."
"most studies did not consider any kind of probability distribution function with fuzzy random variables. moreover, no statistical test was applied to examine the results of the portfolio selection model with fuzzy data. in view of this weakness, the objective of this paper is to develop a statistical test to evaluate the results of the portfolio selection model with fuzzy data. first, we deal with the problem of finding the distribution function with fuzzy data."
"in order to verify the effect of gesture recognition in this method, hand gesture images are collected. gesture sample library is built to analyze the influence situation of various factors on hand gesture recognition. in the experiment, the appointed gesture samples are selected. in ycbcr color space, the y component is independent of the cb and cr components, so that the skin segmentation is less affected by illumination. it is a linear transformation from ycbcr color space to rgb color space and is easy to segment. therefore, ycbcr color space is selected to segment the gesture image. the ellipse model is set up in ycbcr color space to segment the gestures, and the hu invariant moments and hog features are extracted."
"the solution of the minimization of the l 0 norm is a nphard problem. the greedy algorithm can only approximately solve the minimization of the l 0 norm problem. among them, omp algorithm is widely adopted, and the greedy algorithm is analyzed and solved in this paper. according to the theory of compressed sensing, when xis enough sparse, the formula (4) can be equivalent to solving the minimization of the l 1 norm problem [cit] ."
"in this paper, we established a statistical test of fuzzy data which is called as fuzzy statistical test. we introduced a concept for \"defuzzifying\" fuzzy data into real numbers. that is, we use the central point and radius instead of the interval data. hence, the central point and radius will have the statistic characteristics as mean and variance, and the conventional statistic test can be applied. in order to illustrate the efficacy of the proposed method, we introduced an application of maximizing investment portfolio of foreign exchange currencies."
the objective of the investor is to choose a portfolio that maximizes the return on the investment under some constraints on the risk of the investment.
"first table 3 presents the data. we simulated the values o and l, respectively. we obtain the probability distributions o and l for each respective exchange currency. table 4 presents the results."
the categories of the test samples have not been determined. so the whole class c of gesture sample is formed into a redundant dictionary matrix
"the sparse coefficient of linear combination has been obtained, and it can use the sparse coefficient and redundant dictionary to reconstruct the test sample. then compare the reconstructed samples and test samples of each class. finally, judge the categories of the test samples according to the minimum residual error."
"in this paper, five typical grab gesture samples are collected from five people by camera. the gestures include five fingers grasp, three fingers grasp, two fingers pinch, one finger hook and five fingers open. as is shown in fig. 1, five kinds of gestures are collected, and collect 20 pictures for each person. that is that each type of gesture collect 100 pictures, a total of 500 images can be used in the experiment. the collection of gestures also takes into account the changes in gesture rotation, scale, illumination and background to make the effect of identification more obvious."
"markowitz's mean-variance model is based on a probability distribution in which uncertainty is equated with randomness [cit] . that is, the return on the ith asset, r i, will be regarded as a random variable."
"be interval values on the probability space ω, where o i is a random variable of central point of f i, and l i is a random variable of the radius of"
"· · \" denotes the membership μ i (x) on c i . definition 2 fuzzy sample mean (data with interval values) [cit] let u be a universe set and"
"3) a standard simulation procedure is designed to verify practical validity and time efficiency of the new energy-efficient algorithm, especially in a real large-scale traffic network."
"to express the dynamic space-time route more clearly, construct the route planning model accurately, and improve the efficiency of the route solving algorithm, we first introduce the modeling idea of state-space-time network and construct the velocity-space-time network."
"vstn is composed of three-dimensional coordinates, which include the time dimension (t ), the space dimension (s) and the velocity dimension (v ). the basic idea of building vstn is to map nodes and sections in the physical network to nodes and arcs in vstn, respectively. moreover, the transformation of nodes needs to take the semaphore elements into full consideration, and to split and map according to the phase of the traffic lights. the mapping of road segments needs to take full account of the relative physical location relationship. the vstn build steps are as follows:"
"compared with traditional vehicles, connected and automated vehicles can perceive the driving status and information of surrounding vehicles in real time and have certain data storage and calculation ability. at the same time, intelligent network vehicles also have the ability to control their own trajectory and perform route selection. therefore, in pursuit of dynamic route calculation for energy savings of cavs, this paper established an energy consumption model of a vehicle under various working conditions and proved the reasonability of driving strategy in theory. the vstn model was formulated to describe the vehicle state information in time and space and design the shortest route algorithm under the vstn. at the same time, the algorithm incorporates the solution of the dynamic route. every time the vehicle arrives at a new intersection, the new optimal route is recalculated according to the current traffic information. the sequential trajectory formed by every new route is the optimal energy-efficient route solved by energy-efficient dynamic route algorithm under dynamic traffic circumstance. finally, the energy-efficient route algorithm was simulated. the research shows that the energy-efficient algorithm proposed in this paper can improve the energy efficiency to a certain extent in a variety of road network states."
"fifty total trials were conducted for each direction and speed of slip. in addition to varying the speed of slip for each direction, two separate coefficients of friction were evaluated. the above procedure was repeated on the same glass surface, but the surface was coated with a thin layer of vegetable oil to lower the coefficient of friction between the biotac and the surface. in all, a total of 400 trials were conducted for this study 50 trials for each independent variable listed in table i. the taxel signals (e1, …, e19) were recorded with robot operating system (ros). all taxel data were collected at 80hz. following data collection, the resulting raw signal data were manually annotated to mark the onset and cessation of slip during each trial. as the response of the biotac at the onset of slip and immediately following was of primary interest, the resulting data sets were trimmed to include a 200ms window immediately prior to and following the onset of slip. these trimmed and annotated trials were subsequently used to train an ann with the purpose of identifying the onset and direction of slip from the obtained tactile data."
"first, participants were asked to grasp the objects as indicated on the screen, matching their two virtual fingers with the pose suggested by the algorithm via the green virtual cones (see first inset of figure 2 ). when the pinch is validated, the object either turns green to confirm the end of the task (static task), or it turns blue to instruct the user to lift it (dynamic task). in this latter case, participants were asked to lift the tangible object up 10 cm and tilt it on each side. after that, participants were asked to put the object down and the task was considered completed. thanks to the magnets on the support, it was easy to put back the tangible object on its original location. at the end of each trial, participants were asked to answer the following question: \"how much does the tangible object correspond to the virtual one?\" (see second inset of figure 2 ) using a 7-item likert scale."
"the first novel contribution of this paper is the development of an ann classifier to detect the direction a grasped object slips in a dexterous robotic hand. the second novel aspect of this paper is the use of slip direction detection for adaptive robotic grasp reflexes, which is a"
"biotac has recently emerged as a cutting edge multimodal tactile sensor to observe the state of robotic fingertips [cit] . the biotac is a soft fluid-filled sensor; when the internal conductive fluid is deformed, the resultant fluid distribution affects the internal electrodes' impedances to indicate the force distribution on the fingertip surface."
"to address this important point, we consider the perceived size of the object as our first criterion. for each pinching pose, we measure the distance w between the two contact patches."
"the road network shown in fig. 12 is from jing'an district of shanghai, and the marks on each road section in the road network represent the length of road section (m). fig. 13 is a directed graph of the actual road network. the labels on the road sections represent the road section number and the road section capacity (veh/h). in the simulation process, node 1 is the starting node, node 54 is the end node, and the initial phase is 0 seconds. to simulate the dynamic traffic environment, the traffic volume of each section in the road network is updated at a one-minute refresh frequency. fig. 14 (left) illustrates the solving process of the dynamic energy route. assuming that the vehicle is starting at 09:00, vehicles learn traffic changes within the planning route at 09:03 and 09:04, and then they change the route. the vehicles plan a new route when reaching every crossroad; however, for illustration purposes, the process of dynamic route planning is only performed three times during the simulation. fig. 14 (right) shows the planning route trajectory of space and time at three times. the simulated dynamic physical route for energy savings is 1-5-15-16-17-30-33-42-45-51-52-53-54. fig. 14 shows the actual dynamic route of the vehicle in the tested road network. fig. 15a illustrates the time-space trajectory of the vehicle route in the process of a network test, fig. 15b illustrates the speed change of the vehicle on the dynamic route, fig. 15c illustrates the speed adopted by the vehicle between adjacent nodes on the dynamic route, and fig. 15d illustrates the moment when the vehicle reaches each node on the dynamic route. fig. 16 shows the cumulative energy consumption of the two vehicles."
"this work then opens several interesting prospects: we could scan the real environment to exploit the real physical surroundings of the user. our technique would then enable to provide precise pinching sensations. however, of course, this raises the challenge of real-time tracking algorithms. moreover, using our algorithm, we could analyze the haptic properties of a given virtual scene, and we could try to automatically generate and 3d print, one (or more), universal tangible objects able to provide the best possible match in terms of haptic sensations."
most of the prior efforts to detect whether a grasped object is slipping have used a combination of a specialized sensor and a signal processing algorithm; the frequency spectrum has been a popular indicator of slip where high frequency signal components are indicative of vibrations that occur when grasped objects slip [cit] .
"we chose a unique to composed of three basic shapes: a cone, a cube, and a cylinder (see the representation in table 2 ). the three shapes have the same height and the same width in the middle. the vos correspond to a combination of these three different shapes. as a result, we ended up with 6 vos (see table 2 )."
"the first goal of the present study is to derive a means of reliably detecting the direction of slip (dos), which has been studied previously with a specialized sensor [cit] . in contrast, artificial neural networks (anns) [cit] will be used in this study for real-time detection of the direction that objects slip relative to a dexterous shadow hand using a biotac sensor. initially, experiments were conducted to induce slip with two different biotac sensors, mounted to the first finger of two different shadow hands (fig. 1) . after successfully training the ann classifiers to detect the direction of sliding motion, they were used as a low-level input to a top level robotic hand reflex, which is a novel embodiment of tactile perception for robotic action. the second goal of this study is to investigate the utility of the slip direction detection for robotic reflex development. to that end, case studies during a collaborative task of transferring a cup between two robots and from a robot to a human were performed. specifically, downward slip prompted a grasp force increase to prevent dropping the object. upward slip caused the shadow hand to open so that the other team member could take the cup."
"taking into account the actual traffic flow state in the road network and the current real-time position of the vehicle, the goal of the dynamic space-time route (dstr) planning is to calculate the current optimal route. in regard to energy-efficient dynamic route planning, it is a sequential trajectory in vstn which ending at target node. to achieve dynamic route planning for cavs, the following four assumptions are made in this study: 1) a cav can acquire the whole road network traffic information in real time and have a certain data storage and calculation capability."
"to evaluate the effectiveness of our approach, we carried out a user study. twelve participants were asked to grasp objects of different shapes in virtual reality, all rendered by the same tangible, and evaluate how much the former corresponds to the latter. we made three hypotheses, described at the beginning of sec. 4. first, we wanted to understand if participants are actually able to recognize when they are touching the same object in the real and virtual environments (h1). results show that, indeed, they can. values in bold in tables 3 and 4 show users' ratings when provided with the same tangible and virtual shape. as expected, in both conditions (our algorithm and simple superimposition) ratings are very high. the second question we raised is whether, when tangible and virtual objects are different, using our approach improves the perceived grasping sensations with respect to using a standard superimposition (h2). ratings show that our algorithm was able to well combine the considered haptic features and find convincing pinches between the given tangible and virtual objects. we registered improvements in ratings of 48.8% and 45.1% with respect to a standard superimposition matching technique in the case of static and dynamic grasping tasks, respectively. from tables 3 and 4 we can see that ratings when using our algorithm are significantly closer to the ground-truth than when using simple superimposition. we also hypothesized (h3) that the above results hold when grasping an object with no subsequent motion (static task) as well as when lifting it up (dynamic task). results show indeed little difference between the two tasks. at the end of the experiment, participants were also asked to guess how many tangible objects were used during the trials. answers spanned from 2 to 20, with an average of 7.5 objects, and no one guessed that only one tangible object was used! although we have achieved promising results, our work still shows some limitations. first, our algorithm does not consider that humans have fingertips of different size and stiffness, which can significantly affect how they perceive a surface. moreover, we only considered a reduced number of criteria for our metric. of course, for future work, it would be important to include other perceptuallyrelevant properties, such as the object's texture. we also consider pinch grasping only, but extending our approach to other types of grasping should be possible. for example, extending our method to precision grasps with more than 2 fingers would require extending the description of our poses to more contact patches and the computation of the criteria to account for the increased number of contact couples. also the interaction techniques used to grasp the objects can be improved, to ensure that the user always grasps the objects at the right location. finally, another limitation of our approach is the tracking. although we used a state-of-the-art tracking system, it is not easy to consistently and robustly track the fingertips at all time, and any discrepancy between the motion of the user's fingers and the virtual avatar will degrade the experience. however, when the same shapes were registered between the to and vo, the experiment still gave the best scoring, highlighting the fact that the tracking issues did not hinder the perception of the users."
"the results of pso-bpnn, pso-svr, and anfis with the best fit structures for rs and ws during the training and validating periods are summarized in table 3 . the performances during the two periods were similar for all models, which indicated good generalization capacities of these nonlinear systems. in order to find the differences of predictive capacities of rs and ws, the rmse and mae values were compared between the two series. the results of most cases showed that the rmse and mae values of ws were lower than rs during both periods (figure 7), indicating a better predictability of ws and the necessity of data de-noising."
"illustrative results for real-time classification for all ten testing trials for slipping (south and east) are presented in fig. 8(a, b) respectively. it can be observed that there were minor differences in the classifier behavior across the trials in each direction, but the performance while sliding in any of the four directions of slip was highly successful with early 100% accuracy in each of the four directions. fig. 9 shows the photo sequence of one trial and the ann classification output associated with the slip direction. as the hand was lowered, the no touch state was detected (fig. 9(a) ). when the hand initially contacted the surface, the touch state was detected (fig. 9(b) ). the arm then lowered the hand slightly farther to establish firm contact, causing the index finger to extend in the north direction, which is also reflected in the classifier output (fig. 9(c) ). then, the arm caused the hand to slip in the south direction (fig. 9(d, e) ). immediately after this, the hand was lifted off the surface, to return to the no touch state (fig. 9(f) )."
"where n in, out, t, v and n out, out, t, v represent the spatial and temporal starting point and ending point of the segment moving arc n in,out, n out, out, t, t, v, v, respectively. further, all sections of the arc will be added to the collection, a t ."
"the performances of the aforementioned models were evaluated by three statistical performance evaluation criteria, including the coefficient of efficiency (ce), root mean square error (rmse), and mean absolute error (mae)."
"existing energy consumption models can be divided into macro-models and micro-models. between them, the macro model focuses on the analysis of the relationship between the overall vehicle average speed and energy consumption and emissions in a certain region, while the micromodel focuses on the study of the quantitative relationship between the instantaneous state parameters of the vehicle and energy consumption and emissions [cit] . the micromodel is clearly more suitable for studying the relationship between the space-time trajectory of a vehicle and its energy consumption. according to the different input parameters, the vehicle energy consumption model can be divided into a forward model and a backward model. the forward model is derived by the rotational speed and torque parameters of the engine, while the backward model is derived by the calculated vehicle traction [cit] . table 1 shows some typical energy consumption models."
"step 3: calibrate the arc weight. according to equation (14), energy consumption of the corresponding space-time arc can be calculated and calibrated as the corresponding arc weight."
"the figures show that under the four test conditions, the cumulative energy consumption of the energy-efficient route is lower than the normal route. next, we tested additional cases, and the test data is shown in table 3 . the table shows that the driving energy consumption of the energy-efficient route is lower than that of the normal route and the energy-efficient proportion is between 5.18% and 16.4%. according to the analysis results in table 4, the energy efficiency is weakly affected by the signal period and the initial phase and presents randomness."
"1) robot-robot team experiments: after successfully identifying direction of slip in real-time, the same concept was applied to a scenario involving two collaborating robots: the shadow hand was used to grasp and transport a 3d printed cup within reach of a baxter robot (rethink robotics). next, baxter was programmed to grasp and lift the cup while the shadow hand was programmed to monitor the direction of slip detection from the biotac sp sensor. when the ann classifier detected the slip behavior from baxter pulling up (slip east), the shadow hand was programmed to open and release the object. this experiment was repeated ten times."
the user is able to turn the virtual carousel and manipulate the three virtual objects using the suggested pinch poses (in green). these poses are proposed by our algorithm to best match the corresponding haptic pinching sensations on the tangible object.
"an ann classifier was developed to detect the direction of sliding contact of objects grasped by robotic hands in real-time. the ann was trained and evaluated with two different tactile sensors connected to two different robotic hands with high success rates. the ability to detect the direction of grasped object slip was used for adaptive grasp control in the context of both a human-robot and robot-robot collaborative task to pass a cup from the dexterous robotic hand to another robot or person. when downward slip was detected, the robotic hand tightened the grip force to prevent slip. when the person or robot applied an upward force to the cup, the dexterous robotic hand opened to release the cup into the possession of the other team member. this manner of adaptive robotic grasp reflex based on detection of the direction of object slip can be useful to improve the performance of human-robot and robot-robot teams working collaboratively."
"connected and automated vehicles (cavs) carry advanced devices such as vehicle sensors, controllers and actuators, which are central to the integration of modern communication and network technology. implementation of traffic information transmission and sharing between vehicles and other subjects (people, vehicles, roads, intelligent transportation infrastructures, etc.) enables complex environment perception, intelligent decision-making, coordination control"
"haptic sensations can greatly improve the immersiveness and illusion of presence during virtual reality (vr) and augmented reality (ar) experiences. there are many different ways to provide users with such sensations, including using actuated devices known as kinesthetic or cutaneous feedback devices [cit], exploiting perceptual phenomena with cross-modal effects [cit], or using passive props also known as tangible objects [cit] ."
"using yaskawa's fs100 teaching pendant, a series of movement sequences of the yaskawa sia-10f arm were programmed to maneuver the shadow hand above a fixed glass surface, establish contact between the surface and the biotac, and subsequently induce slip between the fingertip sensor and the surface in either the south (s) or west (w) directions fig. 3(a) . initially, the shadow hand was positioned slightly above the surface, with an angle of π/4rad between the back of the palm and the surface. each trial began with the shadow hand being lowered onto the surface by the arm at a rate of 10mm/s until contact was established. once contact was established, the arm paused for a period of five seconds to allow the taxels to reach a steady state. following this five second pause, the arm induced slip between the biotac and the surface by moving linearly long the s or w directions for a further 10 seconds at one of two constant speeds (12.5mm/s or 25mm/s). following this, the robotic arm came to rest and the trial was concluded."
"the real-time classification experiments were conducted using a e2m3r shadow dexterous hand, mounted to a 6 dof ur-10 robotic arm (universal robots, denmark), fig. 1(b) . the biotac sp, with 24 taxels spatially distributed as shown in fig. 2(b), was connected to the index fingertip and used for direction of slip detection in real-time experiments."
"the line test scenario is shown in fig. 7, which is composed of five signal intersections and six one-way sections. the length of the section and the information of the section flow under the two test scenarios are shown in table 2 . during the simulation, the vehicle moves from the left end to the right end. fig. 8 to fig. 11 illustrate the energy consumption under different traffic conditions and signal cycles when the initial phase is 0 seconds."
"however, the existing models are mostly based on specific vehicle types and actual working conditions for data testing and fitting. the energy consumption calculation in this paper reflects only the energy saving effect of the proposed energy-efficient route without the need to obtain accurate energy consumption of the vehicles. under the same test environment, an energy consumption calculation based on dynamic theory can reflect the difference of energy consumption between two driving strategies."
"the experiment started with an explanation of the procedure and the signature of the consent form. then, a calibration of the virtual fingers was performed for each participant. participants went first through all the trials of the static task, and then through the ones of the dynamic task. before each task, 3 random practice trials were performed to ensure that the user had understood the interaction, the task, and that the calibration was done properly. for each task, participants went through all the pinch pose registrations of the to with the 6 vos, either with or without our algorithm, in a random order. since the algorithm allows a translation along the principal component axis, some of the computed registrations brought the vos to be partially below or above the to. the height of the pedestal was modified accordingly. the experimenter also faked the change of the tangible object between each trial."
"however, the cav driving environment is complex and changeable, and energy consumption is restricted and affected by many factors. thus, more thorough and detailed research on how to realize more comprehensive and accurate traffic environment modeling and energy-efficient dynamic route characterization is required. in addition, the introduction of the vstn not only describes the dynamic spacetime route but also reduces the efficiency of the route search. future research needs to design a more efficient space-time network route search algorithm."
"we also asked them the following question at the end of the questionnaire \"how many different tangible objects did you interact with in the experiment?\"."
"the objective of this study was to compare the performances of ann, svm, and anfis models coupled with the dwt de-noising method for daily streamflow forecasting."
"of course, we are not limited to choosing the best matching pose only between two objects. the proposed algorithm is quite flexible and can take into account a larger number of objects. imagine an experimental setup composed of n tangible objects and m virtual ones, with m n. our algorithm can analyze all the scene at once and then find the best pinch for each virtual object using the available set of tangible ones."
"the current research does not consider the influence of the periodic change of traffic lights in the road network on real-time dynamic route planning. one of the most important reasons for the forced start, stop and change of speed of vehicles on the road network is the periodic change of traffic light at intersections, which will undoubtedly increase the energy consumption and produce higher emissions during the driving period. under the condition of acquiring real-time traffic information of a road network, a dynamic traffic route with the shortest time and the lowest energy consumption can be considered for cav route planning. the fundamental starting point of route planning is to ensure the shortest time while trying to reduce the number of start and stop and to achieve energy savings. in the solution of the energy-efficient dynamic route, which involves multisource static/dynamic traffic information with different properties, the modeling idea of state-space-time (sst) networks is preferred. sst networks can clearly describe traffic problems with time attributes and is favorable for the solution and optimization of the problem [cit] ."
"the maes of the three models for the 50 lowest values of the training data and validating data were compared and are shown in figure 9 . the differences between the simulated results and observed data of pso-svr and anfis were apparently smaller than for pso-bpnn during the two periods. the change trends of mae were similar between pso-svr and anfis, but the prediction ability of anfis was superior to that of pso-svr with minimum data. the performances of the three models on the 50 highest values of the training data and validating data were also compared ( figure 10), and none appeared to be superior to the others. the average relative errors of pso-svr, pso-bpnn, and anfis were 34.01%, 41.01%, and 31.86% for the highest training data and 28.83%, 56.70%, and 44.08% for the highest validating data, respectively. pso-svr showed a smoother trend of errors and better generalization ability than the other two models for maximal data. overall, pso-svr provided accurate and reliable daily flow predictions for moderate and high streamflow, and anfis was good at handling low values."
"the associate editor coordinating the review of this manuscript and approving it for publication was liang hu. and execution. the appearance of a cav makes it possible for vehicles and roadside infrastructures to interact with information and perform calculations immediately; it also provides a broad space for new travel services [cit] . for cavs, it is necessary to optimize the travel trajectory microscopically and make the route planning macroscopically [cit] ."
"step 3: whenever cav arrive at a junction, they need to judge whether the junction is the end point,s. if not, the current time, speed, road condition and node number of the intersection will be acquired immediately as the new vstn starting space-time node, and the vehicle will return to step 1; otherwise, the operation ends and the destination is reached."
the original input and output data were conveniently normalized to values between 0.1 and 0.9 before the training process to improve the efficiency of the models.
"the comparison of results obtained from inputs i-v for the three ws models showed that the input consisting of four antecedent daily flow data (i) had a better fit than the others (figure 8 ). with the lead time decreasing (ii, iii and iv), the results tended to deteriorate. the multi-dayahead input (v) showed a moderate predictive capacity compared with the one-day-ahead inputs. it is worth mentioning that the multi-day-ahead input had much better generalization capacities than inputs ii-iv for the pso-bpnn, which was not very satisfactory during the validating periods of both rs and ws."
"proprioception provides humans with information about the relative position of their limbs as well as the effort being employed in movement. therefore, although it is known that absolute position retargeting is possible [cit], it is paramount to well match the relative position of the fingers during the grasp. proprioceptive accuracy has been measured to be around 20 mm [cit], increasing with the length of the movement."
"a small forested basin located in eastern china with large interannual variability in streamflow was chosen as a case study. the sensitivity of de-noising processing to the wavelet type, decomposition level, and threshold functions were examined. the key parameters of three data-based models were optimized by different optimization algorithms to obtain the best model structures. raw and de-noised daily averaged flow time series data were applied to compare the effectiveness of regular and hybrid models."
"toward this objective, we present an algorithm which analyzes different tangible and virtual objects to find the grasping strategy best matching the resultant haptic pinching sensation. starting from the meshes of the considered objects, the algorithm guides users towards the grasping pose which best matches what they see in the virtual scene with what they feel when touching the tangible object. by selecting different grasping positions according to the virtual object to render, it is possible to use few tangible objects to render multiple virtual ones (see this idea in fig. 1 ). the main contributions of our work can be summarized as follows:"
"starting from the meshes of all considered objects, our algorithm behaves in the following way. first, it identifies feasible 2-fingers pinching poses on the considered tangible and virtual objects. then, for each pose and for each object, the algorithm evaluates a series of haptically-salient characteristics: (i) distance between the two fingertips, (ii) distance of the pinch from the center of mass, (iii) relative orientation between the two fingertips, (iv, v, vi, vii) local surface orientation and curvature. these features have been identified by the authors as a representative -yet non exhaustive -set of important grasping information. considering a broader range of feature is of paramount importance and will be addressed in the near future (see sec. 5). next, the algorithm identifies the two most similar pinching poses, one on the tangible and one on the virtual object, evaluated in terms of the above metrics. finally, the chosen pinching pose is highlighted in the virtual scene for the user to grasp. it guarantees the best match between what the user sees on the display and what he feels when grasping the tangible object. of course, the perceptual result highly depends on the intrinsic characteristics of the available tangible and virtual objects. the algorithm is only able to find the best solution given the available objects in the scene, which is not guaranteed to lead to a good/believable interaction. obviously, as expected, if a tangible replica of every virtual object is available, the algorithm will make the user grasp the virtual and tangible objects in the same place. a more detailed discussion about this point can be found in sec. 5, together with an analysis of the current limitations and assumptions of the algorithm (e.g., we consider only 2-fingers pinches, we assume a standard size for the fingertip, we consider the meshes of all objects available)."
"we conducted a preliminary user study to evaluate the user perception when he is manipulating a tangible object (to) and visualizing virtual objects (vo) that do not always match the manipulated tangible object. the participants were able to manipulate three basic tangible shapes: a cone, a cube and a cylinder (see table 2 ). the virtual objects were designed as a combination of these three different shapes. the participants were asked to rate the similarity between the manipulated object and the seen object. the suggested grasping positions of our approach were compared to grasping positions suggested by a global registration of the tangible and the virtual objects. the main hypotheses of our user experiment are:"
"the mae was defined as the average of absolute errors: to confirm the most appropriate resolution levels for decomposition, the results were compared among 3-10 resolution levels. generally, the smoothing of the detailed signals improved with the increase of the resolution level number, while a greater resolution level led to error propagation (chou ) . in this study, with the increase of the resolution levels, the snrs were decreasing and mses were increasing for all cases. therefore, the number of resolution levels used in the wavelet decomposition was determined to be three. the trends of snr and mse towards resolution levels are shown in figure 4, taking the mother wavelet db3 and semi-soft threshold function as examples."
the remainder of the paper is organized as follows. section ii formulates the energy consumption model and the energy-efficient driving strategy. section iii presents the dynamic space-time route planning model. section iv designs the vstn dynamic route solving algorithm. section v conducts the two scenarios simulation and data analysis. section vi concludes the article.
"the signal processing and slip classification for the realtime experiments were performed in matlab/simulink utilizing the pattern recognition toolbox. the toolbox was used to design and implement a feedforward neural network with a single hidden layer composed of ten hidden nodes. the neural network was designed to determine one of 6 potential classes: 4 slip direction classes (north, south, east, west) as well as touch and no touch states. first, the raw biotac sp data was fed into simulink from ros via the robotics toolbox (fig. 6) . once in the simulink environment, the raw biotac sp data were sent to the previously trained ann for classification. here, the direction of the slip was classified with the respective orientation (north, south, west, and east) or with the appropriate touch classification (touch, no touch). the resulting output was then processed and published back to the ros environment."
"anfis was slightly worse than pso-svr, but was apparently better than pso-bpnn. the performances of pso-svr, pso-bpnn, and anfis suggested that the three models showed good prediction accuracies for intermediate and moderate streamflow. however, it was an important measure to evaluate the predictive ability of a model on the estimations of the high and low streamflow."
"to better evaluate the influence of criterion #2 (distance to the center of mass) on the suggested pinches, we divided the experiment in two identical parts: a first one where the participant had just to pinch the object (static task), not manipulating the tangible object and thus feeling any inertia, and a second one where the participant had to lift the object (dynamic condition). this order was not counterbalanced since the dynamic task could bring some additional cues about the properties of the tangible object."
"1) robot-robot experiments: fig. 10 shows data corresponding to fig. 11(a, b, c, d ) which represents the photo sequence of the shadow hand passing the 3d printed cup to baxter and subsequently, baxter grabbing the object. as the shadow hand initiated the grasp closure onto the cup, motion was caused in the index finger abduction joint x4 (fig. 3(c) ), which produced a momentary state of east slip classification (fig. 10 ) before stably grasping the object. after the grasp closure was completed, the object was firmly grasped and the classifier detected the touch class while the cup was transported toward baxter. once baxter started to pull the object from shadow hand, the classifier successfully detected the slip east class (upward, away from gravity) which prompted the shadow hand controller to release the object. all ten trials were completed successfully."
"prior to training of the ann, the resultant data were segmented using a time-based square windowing function. a window time (tw) was empirically chosen as 100ms. using the taxel data contained within the window at step i, the input vector used to train the ann was derived where the raw signal data for each taxel was concatenated into a single rqx1 normalized vector xadn, where q is given as follows."
"hershberger and misceo [cit] found out that, between vision and haptic information, neither modality inherently dominates the perceived size of an object. discordant haptic information biases visual size estimates by as much as discordant visual information biases haptic size estimates. since neither modality captures the other one completely, a discordant stimulus will be perceived as discordant. flanagan and mandomir [cit] also found that the width of the grasp affect the object's perceived weight. therefore, it is very important to well match the size of the tangible object with respect to its virtual counterpart."
"in this paper, we propose to use few tangible objects to enable 3d interactions with multiple virtual ones. to do so, we devised an algorithm which performs an exhaustive search on all the feasible pinching poses on the available objects, to find the best match in terms of haptic sensations. the algorithm is designed to provide the best available match between what the user sees in the virtual environment and what he feels when grasping the tangible object."
"we presented an innovative haptic approach which enables the use of a reduced number of tangible objects for interacting with multiple different virtual ones. the need for such an approach stems from two conflicting requirements. to achieve a compelling illusion, there should be a good correspondence between what users see in the virtual environment and what they touch in the real world. in other words, the haptic features of the tangible and virtual objects should match. however, this calls for the creation of tangible replicas of all the virtual objects in the scene -which is often not possible. therefore, it is important to devise approaches maximizing tangible/virtual objects matching, even when these are different. the proposed algorithm addresses this problem. it analyzes the provided tangible and virtual objects to find the best pinch in terms of matching haptic sensations. it starts by identifying several suitable pinching poses on all objects. then, for each pose, it evaluates a series of haptically-salient characteristics. next, it identifies the two most similar pinching poses, one on a tangible and one on a virtual object. finally, it highlights the chosen pose, which provides the best matching sensation between what users see and touch."
"to choose the best matching pinching pose between two objects, i.e., one virtual and one tangible, we perform an exhaustive search between all the feasible poses on the two objects. for each couple of poses, one per object, we calculate the error vector between the haptic criteria, i.e., ∆ζ"
"the abilities of these machine learning methods to predict non-stationary time series are always limited due to the direct utilization of real data, which cannot avoid the effects of noise (lotric & dobnikar ) . to overcome this limitation, noise reduction has become an important issue and is treated as a pre-processing method (lang data, conventional de-noising schemes, such as wiener filter and kalman filter, are excessively dependent on the establishment of appropriate state space functions (chou ) ."
"considering the energy consumption functions of cavs under different working conditions, this paper adopts new sst modeling ideas; we fully consider the effect of traffic light and the vehicle movement track in time and space. we formulate a dynamic energy-efficient route planning model to aim at shortest time and minimized energy consumption of cavs. the purpose is to use real-time changes of the road network traffic flow to solve the most energy-efficient route dynamically. this method is then simulated and tested. we offer the following three contributions to energy-efficient dynamic route planning approach: 1) a new cavs energy-efficient dynamic route planning approach coordinated with driving speed control strategy is proposed in theory."
"the setup is shown in figure 2 . users wore an htc vive headset which displays the virtual scene with the virtual objects superimposed with the tangible one. a bonita vicon system combined with the blade software was used to track their thumb and index fingers, at the back of which we positioned 3d-printed attachments holding a constellation of markers. we made sure that the pad of each finger remained free from the attachments, and to ensure that these did not move we secured them with wig tape. using this tracking system, participants were able to control index and thumb avatars of their fingers in the virtual environment. this minimalist hand representation [cit] has been chosen to avoid occluding the virtual object from the user's point of view (see inset of figure 2 ). the virtual scene is composed of an instruction panel and a pedestal onto which one vo is standing. we chose not to track the tangible object for not affecting its perception. to know its position, we built a 3d-printed support, fixed to the table, which kept the tangible object at a known location. to make sure that the to was correctly replaced at the beginning of each trial, the support and the to base had complementary shapes and magnets. this support also served as a reference point in the real environment. the virtual scene was then centered on this point using a vive tracker. since blade had also its own reference frame, we calibrated it using a third constellation. therefore, the position of the to was always known, standing on the calibrated support, while the vo was generated onto the to at the beginning of the interaction. the virtual fingers were calibrated through the tracked constellations by touching a tangible prop at a specific known location. this allowed us to know where the contact between the fingers and the to was with respect to the reference. in this way, we ensured that, whenever users were touching the to, they were also touching the vo accordingly. this also resulted in the vo always moving together with the to."
"divided into several virtual nodes according to its existing inlet and outlet. generally, the physical nodes, n, have four inputs and outputs of east, south, west and north, so the corresponding virtual nodes can form a collection, n vir n, as shown in equation (12) :"
"the corresponding energy consumption is mainly caused by various resistances and mechanical losses. since the energy-efficient driving strategy proposed in this paper will not affect the mechanical losses of the vehicle, these losses are not considered in this paper. in actual driving, traction is not required to do work for vehicle deceleration or while idling. however, during acceleration or uniform driving, the vehicle is subject to the combined action of traction f, resistance f, and the force relationship can be obtained according to newton's second law:"
"where c d is the wind resistance coefficient (0.35), a is the windward area (2.4 m 2 ), v is the vehicle traveling speed, µ is the rolling resistance coefficient (0.015, refer to the general value from 0.015 to 0.020), and g is the gravitational acceleration (9.8 m/s 2 ). as an example, take a vehicle with a mass m, of 1500 kg and an upwind area a, of 2.4 m 2, its energy consumption e a, is shown in equation (5):"
"compared with the conventional vehicles, cavs can know both their real-time positions, velocities and the networks' signal and traffic flow status more accurately. these advantages make energy-efficient dynamic route planning possible. to plan the energy-efficient driving route, it is necessary to reduce the number of starting and stopping times of the vehicle and to try to pass through each section at a uniform speed, as shown in fig. 3 . three energy-efficient routes with different starting times are shown in the figure and each intersection can be passed without stopping by controlling the driving speed."
"however, the hard threshold function may lead to a larger variance in the reconstruction of the original series, while the soft threshold function may create an unnecessary bias when the true coefficients are large (antoniadis )."
"in this experiment, we only considered three possible pinch poses on the to, located at the middle of each shape (see table 2, for their localization). then, we used our algorithm to compute the best match between these poses and one potential pinch pose for each vo. we decided to take a set of 17 potential valid pinch poses per vo among the initial set of infinite pinch poses. the matching positions provided by our algorithm are shown in table 2 ."
"where f is the fitness value, s is the number of training samples, t k is the observed value, p k is the predicted value based on x i, and x i denotes the penalty parameter c and kernel function's parameter c. the 10-fold cross-validation method was applied to the optimized procedure, and the velocity and position of each particle were updated until the termination condition was satisfied. the optimum parameters obtained by pso were tested with test data to check the performance of the svr model."
"the ideal working situation for a cav is that all static traffic information (road network structure, road section capacity, intersection traffic light layout scheme, etc.) and multisource dynamic traffic information (road traffic flow information, emergency information, intersection traffic light information, etc.) can be acquired in real time [cit] . in the process of cav dynamic route planning, different driving routes will lead to a different energy consumption."
"finally, although the results of our user study are promising and statistically significant, we believe that more experiments could be conducted, considering more objects, different tasks, different contexts."
"figure 3: bar-graph of the answers for questions 5 to 8 concerning the properties of the object that helped the participants to find a correspondence between the to and the vo. q5: shape, q6: curvature, q7: width, q8: perceived inertia."
"two scenarios were selected to test and analyze the energy efficiency of the dynamic energy-efficient route planning algorithm. the test scenarios are a linear simulation with traffic lights (no steering) and a network simulation (considering steering). to test the energy consumption efficiency of the energy-efficient route planning algorithm, we compared and tested the energy consumption under the normal route (maximum speed). [cit] processing under windows 10 is utilized for concluding simulation experiments. vstn dynamic routing solving algorithm is implemented on a pc intel core i7 processor running 3.7 ghz, ram of 8 gb."
"in this paper, an offline study was first conducted as a proof of concept for ann classification. then, an online study was performed to test the ann classifier in real-time. finally, experiments were performed for adaptive robotic reflexes with human-robot and robot-robot teams. the large similarity of the hardware allowed for a transmittal of the offline techniques to be applied during the real-time experiment with only minor adjustments required."
"in this study, historical daily flow data of the yuetan basin were collected from the bureau of hydrology, ministry of water resources of pr china to evaluate the performance of different methods. the yuetan basin (117 38 0 -118 10 0 e, 29 33 0 -29 50 0 n) is the source of the xin'anjiang river and is located in huangshan city, eastern china, with a drainage area of 950 km 2 (figure 1 ). it is a forest-dominated catchment, with sparse farming and residential land. the percent of forest cover is 95.44%, while the farm land and residential land accounts for 3.26% and 0.01%, respectively."
"at each time step, the previous samples from each electrode are concatenated into a single vector . after concatenation, the resultant vector was normalized over the infinity norm. the normalized vector is given by"
"in contrast, one novel aspect of this research is to detect the direction that a grasped object is slipping. two different dexterous robotic shadow hands [cit] were used in this paper which have been outfitted with two different biotac sensors (fig. 1) . it is interesting to note that human grasp reflexes show different responses with respect to the direction that a grasped object slips, where more dangerous directions of slip are compensated for more aggressively [cit] ."
"gaussian and bell-shaped mfs have been increasingly popular for specifying fuzzy sets because of the smoothness and concise notation (chang & chang ) . in this study, we chose the bell-shaped mfs as they had one additional parameter, which indicated that a non-fuzzy set can be more easily approached than gaussian mfs. the hybrid learning algorithm, combining gradient descent and the the number of decomposition levels (l) was another important factor that needed to be carefully selected during the decomposing process. training with a small l is simple but may lead to inaccurate predictions without sufficient past information, while a high l causes hard training and slowly decreases the approximation error (soltani )."
"the objective of our algorithm is to provide users with coherent sensations, matching what they see on the display to what they touch in the real environment. given a set of virtual and tangible objects' meshes, the algorithm identifies two grasping/pinching poses, one on the virtual object and one on the tangible object, maximizing their similarity in terms of haptic perception. the computation is done during the pre-processing phase, and the generated grasping matches are used to provide guidance to the user in vr."
"in this study, the performances of pso-bpnn, pso-svr, and anfis for daily streamflow prediction in a forested basin were compared using one-day-ahead and as the de-noising stage enhanced the forecasting performance and the proposed model could be applied for short-term forecasting of a hydrologic time series, it was difficult to identify a method that could maintain its prediction accuracy for extreme values. the results obtained here were limited to a single application, and further studies are needed to assess more cases and improve the predictive capacity by introducing a subsection function model or other new methods."
"among the three models, pso-svr obtained the smallest values of rmse and mae as well as the highest value of ce for input i of ws, and so it was selected as the bestfit model for daily streamflow prediction in this study."
"where t a is the acceleration time length and v 0 is the initial speed. in the process of vehicle driving, the resistance f, is mainly composed of wind resistance f w, and rolling resistance f r, as shown in equations (3) and (4) respectively:"
"of interest in the current study is the ability of the ann to classify one of four distinct output states: no slip (ns), incipient slip (is), and the direction of slip (once gross slip occurs), denoted gs and gw to represent slip in the south and west directions, respectively (fig. 3(a) ). the resultant target matrix is realized as:"
"as will be subsequently shown in the results, the offline ann classifier was highly successful in detecting the direction of slip; however, it was not capable of distinguishing between the two different friction coefficients or slip speeds. thus, the real-time classification experiments largely focused on detecting the direction of slip in four different directions, north, south, east, and west fig. 3(b) ) with one coefficient of friction and one speed of slip."
"in this respect, tangible objects, also known as passive haptic or physical props, have been proven to be an effective -yet simplesolution for conveying global and distributed shape sensations in vr/ar. however, for the illusion to work, the haptic features of the tangibles should match those of the corresponding virtual objects, i.e., there should be a good correspondence between the, e.g., size, local shape, texture, mass of the tangible object with respect to its virtual counterpart. one way to match what users see in the virtual environment to what they touch in the real world is creating tangible versions of all the virtual objects in the scene. however, this solution may be neither feasible, e.g., if users can modify the virtual objects, nor desirable, e.g., if the virtual environment comprises several objects. it is therefore important to study solutions enabling the use of few tangible objects to render many virtual ones."
"for the offline experiment, an anthropomorphic robotic c6m shadow hand (shadow robotics, london, uk) was used. the shadow hand possesses 20 actuated degrees of freedom (dof) along with 4 underactuated joints for a total of 24 joints. the shadow hand was the end-effector of a 7 dof yaskawa motoman sia-10f arm fig. 1(a) ."
"we tested our approach in a user study. twelve participants were asked to grasp different virtual objects, all rendered by the same tangible one. for every virtual object, our algorithm found the best pinching match on the tangible one, and guided the participant toward that grasp. results show that our algorithm was able to well combine several haptically-salient object features to find corresponding pinches between the given tangible and virtual objects. at the end of the experiment, participants were also asked to guess how many tangible objects were used during the experiment. no one guessed that we used only one, proof of a convincing experience."
"tangibles within virtual and mixed reality. interacting with tangible objects has been proven to improve the immersiveness of vr and ar systems [cit] present an ar tangible book. users can turn its pages, look at the pictures, and read the text as in a normal book. however, if they use an ar display, they can also see 3-dimensional virtual models popping out of the pages. users can then change the models by physically turning the book pages. [cit] develop a mixed-reality game carried out in a real environment. players walk around a large room-size area and pick up real objects, as if they were playing a traditional non-virtual game. however, these real objects are augmented with superimposed virtual objects and figures, e.g., a real box is opened and inside it is found a virtual treasure. more recently, [cit] present a system for diegetic tangible objects in vr. they develop four tangible objects prototypes, including a cube, a stuffed animal, a treasure chest, and a wooden boat, providing passive and active haptics. for example, the stuffed animal is a furry, hollow raccoon toy. it has a skeletal cage to give structural integrity to its body and a heartbeat that can be either calm or accelerated. finally, dalsgaard and halskov [cit] combine tangible tabletop interaction with 3d projection so that any tangible on a table can be augmented with visual content corresponding to its physical shape, position, and orientation."
"during the experiment, participants were asked to perform two tasks: a static task, where they simply grasped the objects; and a dynamic task, where they grasped and lifted the objects."
"under resolution level three, the mother wavelet db3 offered better performance than the other wavelet functions, and it was selected as the objective wavelet. the values of snr and mse changed along with the wavelet functions, and the trends were not stable, as shown in figure 5 . the nnh with the lowest rmse and mae of the same input may not be the same for rs and ws, which is shown in table 2 ."
"while assigning output classification targets for training, the classification target was defined as is if the point at which slip was initiated fell within the time sample of the window function . the training algorithm chosen was the levenberg-marquardt algorithm, defined as: (5) in (5), j represents the jacobian matrix of the first derivatives of the network errors with respect to the weights and biases, while e is a vector of network errors. μ is a scalar value that is decreased on each successful training step. the levenberg-marquardt algorithm is commonly used in pattern recognition and other applications [cit] ."
"for each trial, we collected the participant's answers on the correspondence between the vo and the to using a 7-item likert scale. participants also completed a subjective questionnaire at the end of the experiment, following a 7-item likert scale:"
"the biotac sensor from syntouch was mounted onto the index fingertip. this biotac incorporates 19 impedances measuring electrodes (taxels) spatially distributed as shown in fig. 2(a) . among some of the benefits, the biotac is able to detect forces, microvibrations, and temperature, this enabled us to determine the point of contact and accurately detect the direction of slip. also, the biotac sp offers an easy-mount integration for the shadow hand."
"during a single cycle of the driving process (a section and an intersection), vehicles can pass through in two ways, as shown in fig. 4 . first is to travel at the normal maximum allowable driving speed (the energy saving factor is not taken into account). if it is a red-light cycle when arriving at the volume 7, 2019 intersection, it will stop and wait to pass, otherwise it will pass normally (as shown in the dotted line in fig. 4) . alternatively, vehicles can pass through the intersection as soon as possible without stopping within the allowable speed range referring to the moment that the green light is illuminated at the following intersection. (as shown in the solid line in fig. 4) ."
"additionally, 10 different o-d (origin-destination) were tested. table 5 reflects the dynamic route, energy consumption and energy efficiency of different o-d. as seen from the table, the energy savings ratio of multiple groups of energy-efficient routes is between 6.59% and 14.53% with an average energy savings ratio of 10.36%. this indicates that the energy-efficient route efficiency under the network scenario is still higher than the normal route and the energy efficiency is still clear for different o-d. fig. 17 shows the time-space trajectory of 9 other o-d dynamic routes."
"while straightforward, the resulting dimension of the input layer of the ann is dependent on the size of the window function tw and the sampling rate . 1) neural network architecture: to classify slip characteristics from the obtained data, an ann was constructed and trained using the matlab neural network toolbox and the patternnet () function. a feedforward network structure with a single layer of hidden neurons was used. the activation function of the hidden layer was chosen to be the hyperbolic sigmoid transfer function, while the softmax () activation function was implemented for the output layer. all networks implemented supervised learning using backpropagation."
"step 2: find the optimal route from the starting point of virtual space-time, (i o, t o, v o ), to the end point of virtual space-time, (j e, t e, v e ). objective function:"
"set x i,j,t,t,v,v is the route selection variable and if the arc, i, j, t, t, v, v, is on the selected route, it is equal to 1, otherwise, it is 0. thus, the solving steps of the dynamic route are as follows:"
"however, manual eeg interpretation by clinicians is time-consuming and the results were mainly dependent on human subjective judgments to some extent. automatic sleep staging methods have been developed as an assisting tool for visual inspection [cit] . generally, the automatic sleep staging process can be described by four procedures: data acquisition, preprocessing, feature extraction, and classification. obviously, feature extraction is an important procedure in sleep staging since the appropriate feature parameters can dramatically improve the classification results."
"rmsda formula where v 1 is the vector of 8 dihedral angles extracted from the 5 residues long window, and v 2 is the 8 vector of dihedral corresponding to the individual pb type. the pb with the lowest rmsda, is assigned to the corresponding position for that window. this pb captures the overall local conformation and approximates the transition along the main-chain smoothly."
"in a second step, we considered the two descriptors for quantifying protein dynamics, x-ray b-factors and rmsf. they were combined to define 3 flexibility classes of lsps: rigid, intermediate and flexible. then for each 11-residue long target sequences, the sa prediction provided a list of five possible lsp candidates. based on the previously defined flexibility classes of these structural candidates, the prediction of target flexibility is made. interestingly, the prediction rate is slightly better than the one of profbval [cit] ) that was optimized for only two classes."
"t for the first target and the shape estimates are averaged over 20 monte carlo runs. the average shape estimates for the targets are depicted in fig. 5c after having received 300 measurements. note that the uncertainty of the shape estimates has not been plotted. for illustrating the magnitude of the measurement noise, the measurements of an example run are depicted in fig. 5b . note that the estimator processes measurements recursively, because in practical applications the target state evolves over time. it can be seen that for both targets, the shape is estimated precisely, although it is not known from which target a particular measurement stems. this example shows that detailed object information can be extracted with mixture rhms for multiple star-convex shapes, when the origin of the measurement is unknown."
"as seen in the previous sections, protein flexibility is essential for interactions between proteins and ligand, nucleic acid, or protein partners. apart from interaction with partners, chemical modifications like formation or breaking of covalent bonds, can impact structural and dynamics properties. one of the most spectacular examples is depicted by the serpin family members when they interact with the protease (see figure 10, [cit] . an initial large conformational change, consecutive to the cleavage of the reactive center of the serpin by the protease, occurs. the loop involved in the cleavage moves, folds as a β-strand that inserts between the other strands of the β-sheet composing the serpin protein core. the two proteins are tightly linked, which significantly affects the protease that looses more than 30% of its structure."
"these innovative approaches have been useful to study specific proteins implicated in pathologies and diseases. they are also sufficiently powerful to analyze large datasets of protein structures using automated pipelines. to summarize, sas provide new visions for the analyses and prediction of protein structure flexibility. different examples will be detailed in the following sections."
"for a series of increasing τ's. in this expression, the symbol c h stands for the curvelet synthesis given by the adjoint, denoted by h, of the curvelet transform. the data-residue matrix is given by"
"in order to track a shape approximation of a single extended object [cit], we need to define the state to be estimated, a measurement model (see section iii-a), and a system model (see section iii-b)."
"from the computational perspective, the student's t approach is relatively simple since it relies on more-or-less straightforward changes on the definition of the penalty function itself (compared the commonly used 2 penalty), its gradient, and the estimation of the student's t parameter by variable projection. the modified gauss-newton approach, on the other hand, relies on a somewhat more complicated machinery that includes the curvelet transform itself and a method to choose the 1 -norm constraint for each gn subproblem. the advantage of the gn approach, however, is that it does not rely on computing the gradients for all shots as required by the student's t method, which relies on fourier transformation along the fully-sampled source and receiver coordinates. moreover, elastic phases represented as monochromatic frequency slices may not be optimally sparse in the fourier domain, which jeopardizes our ability to separate the acoustic from the elastic phases using the student's t. this failure to completely separate the acoustic and elastic phases is properly responsible for the remaining artifacts for the student's t (cf. figure 4(c) ). the gn is more artifact free but this feature goes at the expense of loss of detail, which is related to the one-norm constraint and perhaps the number of iterations. if we relax the 1 -norm constraint too much, we run the risk of starting to \"over fit\" the elastic events."
"analysis of protein structures is crucial to understand protein dynamics and functions. x-ray crystallography, the goldstandard method for solving 3d structures at atomic resolution, is impeded by protein dynamics. hence, tricks are frequently used to restrict motions. it is why proteins have been often considered as static macromolecules, composed of rigid repetitive secondary structures and less rigid random coils. however, more and more emerging evidences show that protein structures are more complex with their internal dynamics being a key determinant of their function. analyses of protein structures are often performed with a simplified three-state description known as α-helix, β-strand and coil which constitutes the classical secondary structures [cit] . a more precise and complete description of protein backbone conformation exists based on the definition of libraries of small protein fragments, namely the structural alphabets (sas) [cit],b) . sas are designed to approximate every part of the local protein structures providing conformational detail. they have performed remarkably well spanning various problems in structural bioinformatics, from the characterization of ligand binding sites to the superimposition of protein structures [cit] b) . furthermore, sas are also very well suited to analyze the internal dynamics of protein structures. sas have been used at three different levels to comprehend protein flexibility: (i) for studying specific fundamental biological and biomedical problems, (ii) to analyze changes associated with protein complexation and allostery, and (iii) to predict protein flexibility."
"the major challenge in deriving these expressions is the data association uncertainty, i.e., it is unknown from which extended object x i k the measurementŷ k,l stems. according to the law of total probability, (7) can be written as"
"in the second scenario, two extended objects are tracked based on a mixture rhm and a constant velocity model for the target motion. the extended objects move along the trajectory depicted in fig. 6, i.e., they start well-separated near the origin, then approach each other, and perform a turn. at each time step, the total number of measurements received from the targets is poisson distributed with mean 20, where each target is the source of a measurement with the same probability. the measurement noise is zero-mean gaussian with covariance matrix σ are the shape parameters given by 11 fourier descriptors. as the extended object is assumed to evolve according to a constant velocity model, the system equation is"
"pb assignments can be done using the python pbxplore tool (https://github.com/pierrepo/pbxplore, in preparation). the result is a translation of a 3d structure into a 1d sequence of pbs."
". hence, the center of the object evolves according to a constant velocity model and the shape parameters become more uncertain over time in order to capture shape changes. a gaussian scaling factor with mean 0.7 and variance 0.03 is used. the target shapes are tracked with a mixture rhm implemented by using a ukf [cit] ."
"where h * (x k, v k,l, s k,l,ŷ k,l ) maps the state vector x k, scaling factor s k,l, measurement noise v k,l, and measurement y k,l to a so-called pseudo-measurement 0."
"in contrast to a point target, an extended object may be the origin of several measurements from different measurement sources on its surface (see fig. 1 ). essentially, there are two major scenarios that illustrate this issue. first, continuously evolving sensor technologies provide advanced resolution capabilities that can result into several measurements of one target during a single scan. the measurement sources vary from scan to scan and their locations depend on the shape of the target but also on more complex target-dependent properties. second, a group of point targets can also be treated as a single entity when there are strong interdependencies between the individual group members."
"2.4.1. preprocessing. as a general rule, a qualified clinician inspected eeg mainly based on the characteristic waveforms in sleep recordings. figure 1(a) gives a 10-second eeg signal from o1-a2 channel. it was inspected as one part of the awakening stage due to the large proportion of rhythm. however, as shown in figure 1 (a), the sequences in boxes are generally seen as incomplete waveforms. these incomplete waveforms can be intelligently merged into the feature rhythm by experienced clinicians during eeg interpretation. this is the feature rhythm of activity in the boxed sequences in figure 1(a) ."
"the primary sequence of the protein-the succession of amino acids-is assumed to encompass all the information necessary for its function. the protein structures resolved from x-ray crystallography or nuclear magnetic resonance (nmr) (see figures 1a,b) can be obtained in the protein databank format [cit] . from the very beginning, theoreticians or experimentalists have described local protein structures by using three states (see figure 1c, [cit] . two of them are repetitive structures stabilized by hydrogen bond patterns, namely the α-helices and the β-sheets (composed of β-strands). these structures are connected with more variable structures, i.e., random coil or loops. later studies have identified spotted small repetitive and regular structures such as the β-hairpins or different kinds of turns in several protein structures [cit] . these simplified descriptions were nicely represented with 3d visualization software (e.g., arrows for β-sheets, springs for α-helix) and accompanying the emergence of macromolecular crystallography. however these simplistic representations also contributed to the static and rigid views of these structures [cit] ."
"hence, the originality of the method lies (i) in the use of a combination of b-factors and rmsf for quantifying protein dynamics, (ii) in prediction of flexibility through sa prediction of lsps, and (iii) in prediction of three classes of flexibility, which are usually limited to two. the method is implemented in a web server named predyflexy (http://www.dsimb.inserm.fr/dsimb_ tools/predyflexy, de, in which the users have access to a confidence index (ci) for assessing the quality of the prediction rate."
"to conclude, we can find that all these approaches are suitable for highlighting both flexible and rigid parts of a protein from structures derived from nmr, x-ray diffraction or molecular simulation."
"this led us to conduct a deep analysis of structures of the same protein with or without ptms. as an example, we selected 157 pdb chains of the human cyclin-dependent kinase 2 (uniprot ac: p24941) in complex form, and 222 pdb chains of unbound monomer. based on data from ptm-sd [cit], a database of structurally solved and annotated post-translational modifications, 112 chains among the 157 [cit] ) showing the strand inserted in the β-sheet after cleavage, (b) the uncleaved form (pdb code 1hp7, [cit] ) showing the wild whole loop."
"as we mentioned before, we compute the s-wave velocity (figure 3(b) ) using a fixed poisson's ratio of 0.25. we simulate 101 shot records with a 100m interval using a time-domain elastic finite-difference modeling code [cit] ) with a 9hz ricker wavelet. all shots share the same 401 receivers with 25m interval yielding a 10km offset. because we consider an ocean-bottom node survey with reciprocity, we set the source depth to 200m while receiver depth is set to 10m."
"approximate entropy calculation is an algorithm based on the complexity of sequences. it is a developed statistic quantifying regularity and complexity, which appears to have potential application in a wide variety of relatively short (greater than 100 points) and noisy timeseries data [cit] . the greater the probability of producing a new pattern, the higher the complexity of the sequence and the larger the corresponding approximate entropy. the calculation of approximate entropy is as follows."
"among chemical modifications, post-translational modifications (ptms), like phosphorylation, play a major role in many biology processes. integrins, for example, can be activated consecutive to phosphorylation. the impact of these modifications on the structure and the dynamics of proteins is thus of particular interest."
"the features from frequency domain or by nonlinear analysis had merits for sleep stage classification. the limitation is also obvious for real clinical application. the frequency domain indicated the powers of certain characteristic activities. the differences between the sleep stages can be described by the change of power of those activities. however, the variation according to the time was missed. on the other hand, the traditional features from time domain can show the variation according to the time but not the characteristics in frequency domain as the clinician inspected."
"a second example on darc loops was the last extra-cellular loops for which a specific and constrained loop conformation was observed. remarkably, this unexpected conformation explains a \"lethal\" mutation for the binding of cxcl8. it was the first time a structural alphabet was used to analyze the dynamics of a protein structures or structural model."
"it is well documented that protein-protein interactions are often guided by flexibility [cit] and that alternative conformations can have a significant influence on the binding process. it is why predicting the structure of a complex using the unbound structures of the partners remains highly challenging, despite a scrutinizing examination of the amino acid composition of the interface [cit] . thus, in most cases, protein structures change during the formation of the complex. the changes can be limited to few side chains motions but can also correspond to major reorganization in the fold. therefore, we undertook the analysis of the protein-protein complexes in the light of structural alphabet. we compared proteins 3d structures in free form, and as part of larger macromolecular complexes."
"like most transmembrane proteins, no experimental structure of darc is currently available . we designed a structural model based on a comparative modeling approach. using rhodopsin (the only available related structure at this time) as a structural template (a simple alignment showed a very low sequence identity value of 12%, e.g., close to a random value), we carefully built different structural models, based on a hierarchical and iterative procedure. a first step was to predict using more than 10 methods the positions of the 7 transmembrane helices along the sequence. from this initial and rough model, helices of darc were aligned with rhodopsin helices assigned from the 3d structure. the same methodology was used for the loops, a complete alignment was generated using helices and connecting loops. a specific treatment was done for n-and c-termini region, combining protein blocks prediction experimentally, 40 alanine mutants had been produced and associations binding constants with cxc-l8 were evaluated [cit] . we used these experiments to assess the quality of our best refined models. from the results, we generated new models by manually changing the positions of helices (and the alignments). building and refinements were done 10 times until a proper set of characteristics were obtained. in regards to these experiments, in silico analysis of protein flexibility has underlined specific characteristics of different epitopes and interaction regions. interestingly, we obtained two different conformations (see figure 4a ) that were both as compatible with experimental data and similarly scored by the few assessment approaches available for transmembrane structural models. [cit] ) ."
"prospective work will focus on making the presented tracking algorithm robust to clutter measurements that do not stem from a target. furthermore, we will incorporate statistical knowledge about the number of measurements generated by a target. mixture rhms can also be used to compose a single extended objects of several connected random hypersurface models, which will provide even more detailed shape information. finally, we believe that mixture rhms represent a fundamental concept for modeling spatial data, which can also be used for other applications such as cluster analysis."
"however, for the student's t penalty to be effective, the outliers must be somehow localized and distinguishable from the good data. thus, following [cit], we first transform the residual into a domain where the outliers are localized via the processing operator b i before measuring the student's t misfit. we can use any transform that would normally be used to filter out the noise (fourier for periodic noise, radon for noise with moveout, curvelets for more complicated coherent events, etc.). here, we choose to transform the residual in the source-receiver wave-number domain by applying the fourier transform along both the sources and the receivers. the advantage of this approach is that the optimization with the student's t misfit carries out the filtering adaptively rather than relying on some user-defined prior filter. so, we let the robust inversion process decide which parts of the data can be fitted and which should be ignored. thus, the filtering is done implicitly as part of the inversion process."
"the modeling kernel used in the inversion is based on frequency domain acoustic modeling implemented by solving the helmholtz system for a 9-point stencil [cit] . to define the starting model for the inversion, we smooth and average laterally the reference velocity model in figure 3 (a). the result is plotted in figure 3 (c)."
"it took us one year to build such models (models are available at model archive website (http://modelarchive.org/, [cit] ) . the n terminus is particularly important in the infection by plasmodium vivax [cit] . it is nearly 55 residues long and different disorder prediction methods (i.e., [cit] ), predicted as partially disordered, with the beginning of the sequence as fully disordered."
"we suggest to parameterize the radius function by means of the fourier coefficients. if it is considered as periodic function in φ with period [0, 2π], the (truncated) fourier series expansion becomes"
"the red line plotted in figure 11 represents the number of available structural data for each position. interestingly, the green region in figure 11 is proportionally less resolved when kinase is in monomer than when it is in complex, and even more solved when the thr-160 is phosphorylated. this observation emphasizes that the decrease of flexibility in this region facilitates the resolution of the structures. several structures of the same protein present specific regions that are disordered in some crystals and ordered in others. these regions were defined by zhang and collaborators as \"dual personality fragments\" [cit] to defined dpf. in the same way, the region between positions 35 to 45 were also identify as dp fragments."
"interestingly, the subtle differences between protein conformations can be captured by the assignment of the pb sequences. by analyzing the variation of pbs assigned at a given position for multiple conformers, the local conformational properties and corresponding changes can be easily identified. moreover, a quantification of the flexibility at a given position n can be obtained by calculating, the average number of pbs across a set of conformers in this position or the \"equivalent number\" of pbs (n eq ). n eq is based on a statistical metric similar from md simulation, a pb sequence is assigned. (b) n eq profile provides direct identification of protein fragments in which local conformational change is observed. here, in green, is indicated a flexible loop. the protein 3d structure representation is generated using pymol software (http://www.pymol. [cit] ) ."
"an extended target is defined as a set s(p k ), where p k is the parameter vector for s(p k ). for example in case of a circular target, p k consists of the radius and center of the extended object and s(p k ) are all the points forming the circular disc. the stacked state vector of an extended target at time step k to be tracked is a random vector"
"lets(p k ) be the boundary of the star-convex set s(p k ) with center m k, then the measurement source z k,l is generated according to an rhm, if z k,l is an element of the scaled boundary, i.e.,"
"mixture rhms in this section, we introduce a novel concept called mixture random hypersurface model (mixture rhm), which can be used for modeling multiple extended objects. in the following, we assume that there are n extended targets"
"in a functional point of view, the phosphorylation in position 160 is known to promote the activation of the kinase, while the phosphorylation of position 14 and 15 slightly reduce its activity [cit] . thereby, the changes in flexibility observed at these 3 phosphorylation sites, could reflect that the activity of the kinase is regulated by a mechanism of complementary rigidity/flexibility of local protein backbone, which could be related to allosteric effects."
(5) increase by 1 and repeat the steps from (1) to (4) to obtain +1 ( ) and +1 ( ). (6) calculate the approximate entropy value by
"sleep is a natural process of humans for recovering energy and body health. it is considered as a necessity of life for humans and animals and is essential to their physical and emotional wellbeing. physiologically, evaluating the quality of sleep depends on many aspects, including the duration and composition of sleep [cit] ."
"which maps the state vector x k, the measurement noise v k,l, and the scaling factor s k,l to the measurementŷ k,l . in (5), the term φ k,l denotes the unknown angle between the vector from the center to the measurement source z k,l and the xaxis. as φ k,l is unknown, we suggest to replace it with a point estimate, e.g., the most likely angle φ k,l . in case of isotropic measurement noise, a proper point estimate φ k,l is given by the angle between the vector from the current shape center estimate µ"
"we have presented a new concept called mixture random hypersurface models for modeling spatially distributed noise-corrupted data. in this work, we have used the concept for tracking multiple extended objects based on noisy position measurements. for this purpose, we have derived a gaussian state estimators for estimating the shape of multiple extended objects. simulations demonstrate the feasibility of the approach by means of a typical tracking scenario."
"this misfit function can be used in fwi in order to improve the recovery in the presence of large outliers or unexplained events in the data. indeed, in contrast to previous robust penalties, the student's t penalty is non-convex. thus, large outliers are progressively down-weighted and effectively ignored once they are large enough."
"in order to model the shape of a single extended target, we employ a recent approach called random hypersurface model (rhm) [cit] . an rhm assumes that measurement sources lie on scaled versions of the shape boundary, which allows for estimating the form and the extent of the shape. in doing so, the target can be modeled as a basic shape, such as an ellipse [cit], or even as an arbitrary star-convex shape [cit] . so far, rhms are restricted to a single extended target."
"to make a fair comparison, both inversions are carried out with multiple frequency bands from 3 − 20hz. each frequency band contains 3 frequencies. for data-space student's t, we compute 5 and 10 l-bfgs updates with all the sources while for modelspace sparsity promoting, we solve 5 sparsity-promoting gn subproblems with 50 randomly selected shots. for 5 iterations of l-bfgs, the total number of pde solves is roughly the same for these two approaches. we varied the number of l-bfgs iterations to show the effect of overfitting (juxtapose figures 4(a) and 4(b))."
"in this study, represents the filtering level and represents the length of run of data. these two parameters were set as 0.15 and 2, respectively. the averaged value of apen calculated from four recorded eeg channels was considered as the extracted feature by nonlinear analysis."
"to make a comparison between data-space student's t and model-space sparsity promotion, we use the bg compass model in figure 3 (a) as the true reference, which is a synthetic p-wave velocity model created constrained by real well-log information."
"sensor model: given a measurement source z k,l, the sensor model specifies how the measurementŷ k,l is obtained. here, we restrict ourselves to position measurements, i.e.,"
"another recent analysis has explored an extensive set of protein/dna complexes and looked at conformational changes occurring in proteins but also in dna. importantly, for both molecules, structural alphabets were used. the alphabet used for describing protein backbone is the protein blocks. for dna, a structural alphabet was obtained using a new approach of registering torsion angles of a dinucleotide unit combined with fourier averaging and clustering (http://www.dnatco.org/, [cit] . these structural alphabets describe biopolymer conformations at greater detail than the 3-state protein secondary structure and basic dna structural types such as a, bi and bii. figure 8 shows an example of different conformations. this study compared structural features of the protein/dna interface with the features of non-interacting parts of protein and dna molecules. clear differences in preferences for occurrences of local protein and dna conformations were observed. specific preferences were underlined between complexes containing various types of proteins such as transcription factors and nucleases. [cit] ) . these figures show that non-interacting regions observed to undergo conformational changes upon complexation are usually intrinsically mobile, which is a characteristic of a functional site. visualization was created by the program pymol (http://www.pymol. [cit] ) ."
"the numbers of the four rhythmic waveforms are counted as the features in time domain. table 2 illustrates the feature definition. the subscripts of tn indicate the frequency band of : 0.5-2 hz; : 2-7 hz; : 8-13 hz; : 13-30 hz. each 30-second epoch is analyzed by the presented procedures. the number of each frequency band is counted after preprocessing. tn, tn, and tn are the averaged values of c3-a2 and c4-a2 channels, while tn is of o1-a2 and o2-a1."
"relevant sequence-structure relationships were also observed and further used for prediction. briefly, lsp prediction is based on svm training. with the lsp prediction, a confidence index (ci) that is based on the discriminative power of the svms is provided. the higher ci, the better the prediction rate is. the prediction rate reaches 63.1%, a rather high value given the high number of structural classes [cit] )."
"a linear discriminate analysis (lda) classifier was adopted to compare the effectiveness of eeg features from different domains for automatic sleep stage scoring. the classification accuracy was calculated compared with the visual inspection in table 5 . additionally, the number of consistent epochs within the total number of epochs was given under the accuracy value for each subject. the classification accuracy reached 87.09%, 83.11%, and 76.08% by using time domain features among the three subjects, respectively. the averaged accuracy in time domain was slightly higher than frequency domain and better than nonlinear method. figure 3 showed detail evaluation about the classification results. the results were the average accuracies of the three test subjects for each sleep stage. it is apparent that both the time and the frequency domain features showed fairly good performance in rem and sws, while the nonlinear algorithm performed the best in s2. additionally, the features from the time domain also showed superiority in the recognition of awake and s1 stages. clinical experience. according to the recorded data, the four frequency activities of (0.5-2 hz), (2-7 hz), (8-13 hz), and (13-30 hz) almost covering the eeg frequency band were mainly inspected. therefore, the amount of power of characteristic activities was commonly utilized as the feature extracted from the frequency domain for sleep stage classification. according to the evaluation results in figure 3, features extracted from the frequency domain were able to provide rather good performance in sleep stage recognition. during the overnight sleep process, the sleep levels were circled from light sleep to deep sleep about three times. when the sleep level was changed, the activities in sleep eeg were gradually changed accordingly. approximate entropy refers to the complexity of the sequence. through the obtained feature analysis result in figure 2(b), the approximate entropy is well shaped to represent the change of the complexity of eeg signal from light sleep to deep sleep. in figure 3, the automatic recognition result by approximate entropy had good performance to separate awakening and deep sleep. however, the accuracy of s1 was rather low, which was misclassified into s2 and rem."
"in this work, we consider the problem of tracking multiple extended objects (see fig. 1 ), where the goal is to estimate a shape approximation of each extended target in addition to its kinematic parameters [cit] . note that this is a nontrivial task, as it is required to deal with measurements that may stem from different objects whose extents are unknown and are part of the estimation problem."
"the ability of dna to adopt non-canonical conformers, rare in naked dna, is clearly essential for the recognition by proteins. rare dna conformations introduce significant deformations to the dna regular structure. the occurrence of these rare forms was estimated and characterized enabling a better understanding of the role of non-b-dna structures. a critical feature was the distinct interaction patterns for the dna minor groove relative to the major groove and phosphate, and the importance of water-mediated contacts. indeed, water molecules mediate a proportionally largest number of contacts in the minor groove and form the largest proportion of contacts in complexes of transcription factors [cit] . it corroborates to previous researches on the importance of mobility of such water molecules [cit] ."
"in another project, we were interested in integrins, a large family of cell surface receptors involved in cell-cell or cellmatrix adhesion. integrins are type i membrane glycoproteins composed of two distinct α and β subunits. each subunit has a large extracellular region (composed of multiple structural domains), a trans-membrane segment and a short intracellular domain. integrins interact with cell cytoskeleton and mediate bidirectional trans-membrane signal transduction. these receptors are expressed in vertebrate, but also in lower metazoans including sponges, nematode caeorhabditis elegans and fruitfly drosophila melanogaster. in mammals, 18 α and 8 β subunits assemble in 24 distinct integrin complexes. integrins play critical roles in many physiological processes like hemostasis, immune response, leukocyte trafficking, development and angiogenesis or in pathology like cancer. in human, they are responsible for many diseases from genetic or immune origins. they also make effective targets for drug therapies in thrombosis and inflammation. furthermore, integrins are binding sites for many viruses and bacteria [cit] ."
"as for the gn method, comparison between the vanilla l-bfgs and gn shows that the latter is apparently less sensitive to unmodelled phases in the data. there may be two possible explanations for this observation. first, the gn updates are regularized because we invert the gn hessian with a limited number iterations, which corresponds to some kind of modelspace regularization. the l-bfgs, on the other hand, is not regularized. second, l-bfgs attempts to invert the true hessian via a low-rank approximation while the gn updates are obtained by (approximate) inversion of the gn hessian formed by the composition of the adjoint of the jacobian acting on the jacobian itself. in cases where the modeling operator does not explain all the events in the data, it may not be reasonable to expect that l-bfgs will yield the correct inverse of the hessian while we can argue that the gn approach will at least get the gn part of the hessian correct if the velocity model is reasonably accurate."
"in recent years, researches on feature extraction have mainly focused on frequency domain and nonlinear algorithms. however, there have been few academic achievements related to time domain feature extraction except for dfa and vg recently. in fact, clinicians generally interpret eeg by observing the waveforms in the signal. theoretically, time domain analysis truly has a strong basis. some of the authors proposed a time domain eeg analysis method which is based on the merger of the increasing and decreasing sequences to detect interictal epileptiform discharges [cit] . eeg signal can be considered as the summation of several characteristic rhythms. after sequence merging, the feature rhythms in eeg signals can be detected more easily."
"the building of the protein dataset was quite strict leaving only 76 high quality complexes representing very different configurations with free and bound forms [cit] . accordingly, structural changes occurring between the free and bound forms of the protein were analyzed using three different measures: the cα root mean square deviation, the percentage of pb change and a specific pb substitution score. this last score relies on a pb structural substitution matrix that quantifies the cost to replace a given pb by another pb. the more similar the pbs, the more favorable the substitution score. consequently, this score permits to quantify the conformational change by distinguishing similar pbs from to the most distinct ones. comparison between unbound and bound forms shows that significant structural rearrangement occurs at the interface but also in regions away from the interface upon the formation of a highly specific, stable and functional complex. for 50% of them, which correspond to signaling proteins, the major changes correspond to allosteric ones, localized far away from the interface. these sites could be associated to mutations known to be involved in multiple diseases such as cancer. pb allows distinguishing here also between large movements, from mobility to deformability or flexibility. normal mode analysis was also performed to gain deeper insights [cit] . the results obtained for signaling complexes underline the importance of allostery-like structural changes much more than appreciated before (see figure 7) ."
"here, we present state-of-the-art of developments in the study of protein flexibility using sas based approximation. the backbone conformational variations can be described as changes in the pattern of sas, which acts as fingerprints of the dynamics involved. these innovative approaches are useful, customizable, and deal with specific proteins involved in pathologies and diseases. they are also powerful to evaluate generalized principles from large biological complex structures. thus, sas provide new vision for detailed analysis and prediction flexibility of proteins."
"as we have seen from the motivating example in the introduction, carrying out acoustic fwi on data that contain elas- tic phases can be detrimental to the inversion. using the robust student's t technique, we hope to mitigate some of the adverse affects of these elastic phases by using the fact that monochromatic frequency slices are relatively sparse in the source-receiver wave-number domain. this means that the elastic events that are not in the range of the acoustic modeling operator tend to be concentrated amongst a limited number of frequencies. the student's t penalty function is by design relatively insensitive to these outlliers. which should improve the inversion results. as we can see from figure 4 (c), this is indeed the case compared to the inversion result based on the 2 misfit for 10 iterations included in figure 4 (b). while this result may not be as good as the inversion result for the acoustic only result, our improvement is certainly encouraging. aside from changing the data-space misfit penalty functional, model-space regularization is another proven tool in inversion problems that suffer from null spaces or from unmodelled noise. transformdomain sparsity promotion, including one-norm minimization in the curvelet domain, has proven to be an effective tool to remove these unmodelled signal components. for this purpose, we submit the data set with the elastic phases to the above described modified gauss-newton method. the results of this exercise are summarized in figure 5 and were obtained by 5 gn iterations on randomized subsets of 50 shots. from this figure, it is clear that invoking curvelet-domain sparsity promotion on the gn updates also mitigates the adverse affects on the unmodelled elastic phases present in the data. while the student's t approach works by ignoring the outliers, the curvelet-domain sparsity promotion penalizes complexity on the model that is associated with the unmodelled elastic phases. as with the student's t, the result of this method is encouraging."
"where r i is the i th datum of the residue vector given by the difference between the observed and modelled data, and k is the degree of freedom of the student's t distribution."
"rechtschaffen and kales (r&k) defined sleep scoring criteria according to the change in the physiological signals [cit] . although there are several modifications and many amendments have been made, r&k criteria are still regarded as golden criteria for sleep staging in clinical application. according to r&k criteria, sleep is categorized by wakefulness (awake), rapid eye movement (rem), and nonrapid eye movement (nrem). nrem is further divided into sleep stages 1, 2, 3, and 4. additionally, stages 3 and 4 are often combined together and refer to deep sleep or slow wave sleep (sws) [cit] . clinicians can figure out whether one subject has a full rest by analyzing his/her overnight sleep measurement of psg (polysomnogram) and provide a treatment plan based on the sleep stage inspection."
"apart from the eeg features, electrooculogram (eog) and electromyography (emg) signals provide additional essential information for sleep staging. for example, the eeg pattern in rem stage is a mixed frequency activity which may be similar to adjacent stages. however, rapid eye movements can be observed in eog, and emg showed the lowest amplitude which is computational intelligence and neuroscience 5 distinctive compared with the other sleep stages. table 3 illustrates the additional features of eog and emg from the time domain. the mean, variance, and span values are calculated from the two recording channels of eog (loc-a1 and roc-a1). the zero-crossing number is obtained from chin-emg. am, av, and as are the averaged values of loc-a1 and roc-a1 channels. az is the zero-crossing value for chin-emg."
"in this study, multidomain feature extraction was investigated for sleep eeg, including the amount of power of characteristic activity in frequency domain, the approximation entropy by nonlinear analysis, and the number of characteristic activities by a developed sequence merging method in time domain. several features were extracted from sleep eog and chin-emg as additional parameters. the features of sleep eeg from different domains were analyzed and compared. the features from frequency domain showed consistent characteristics to the definition of sleep stage in criteria. the approximation entropy indicated a well gradually decreasing shape from light sleep to deep sleep. the features from time domain had similar tendency to the frequency domain. furthermore, the corresponding characteristic activities can be highlighted compared with the frequency domain. based on the features from different domains, the automatic sleep stage classification results were obtained and compared with the visual inspection. the classification accuracy in table 4 and detailed comparison in figure 3 indicated that the developed feature extraction method reached rather satisfying accuracy for sleep stage scoring than the frequency domain and nonlinear analysis."
"for a given star-convex shape, an rhm assumes the measurement source is an element of a randomly scaled version of the shape boundary (see fig. 3 ). the scaling factor is randomly drawn from a one-dimensional probability density function, which has to be specified in advance."
"where f x is the frequency of pb x (x takes values from a to p). a n eq value of 1 indicates that only one type of pb is observed, while a value of 16 is equivalent to a random distribution. for example n eq value equal to 6, could mean that 6 different pbs are observed in equal proportions (1/6), or that more than 6 pbs are observed in different proportions. by plotting the computed value for each residue position (see figure 3b), it is possible to easily localize which protein regions present local conformation change, or in other words, which regions represent local flexibility. this pb derived-entropy index is an interesting feature of pbs, which can be used to analyze pb prediction [cit] or an ensemble of structures, corresponding to the same protein solved in different experiments, or to several structures extracted from md simulation [cit] . note that pbxplore can be used to calculate n eq, and to visualize in various ways the pb variation for each position from a collection of models or through a md trajectory ."
"α and β subunits of integrins are associated to rigid, flexible and even disorder properties (such as duffy protein presented in the section above). we ran independent mds simulations on different systems, i.e., the wild type but also variants and mutants, using gromacs mds package [cit] to examine specific regions of αiibβ3. we observed different opposite behaviors depending on the region and mutants studied."
"complexes, present a phosphorylated threonine at position 160 in the structure of the kinase. as described in table 1, we compared the backbone flexibility of three different cases: unbound kinase, kinase complex, and phospho-thr160 kinase in complex."
"in this study, a comparison study on multidomain eeg features was presented. the ultimate purpose was to investigate the effective feature extraction method for automatic sleep staging. the powers of certain frequency components were calculated as the characteristic features in frequency domain. approximate entropy was selected as the parameter of nonlinear dynamics. instead of traditional time domain features, the merger of the increasing and decreasing sequences in eeg time series was developed according to the characteristics of sleep eeg for feature extraction in time domain. after feature calculation and extraction, the linear discriminate analysis (lda) was adopted for sleep stage classification. the obtained classification results were compared with the visual inspection by a qualified clinician. the performance of each feature extraction method was discussed and the feasibility of the developed time domain method was analyzed. according to the sampling rate of eeg, one 30-second epoch contains 3000 points of data. the epoch is further divided into six 5-second segments. for each segment, the eeg data is converted from time domain to frequency domain by 512-point fast fourier transform (fft). the ratio of the power of certain frequency activity is calculated for each segment. the obtained values of 6 segments are averaged as the feature in frequency domain for a 30-second epoch."
"gsatools was used to finely analyse the ntrc receiver domain and its homologs chey and fixj. for this purpose, different conformations of the protein extracted from a mds simulation were encoded. the distributions of sa strings were used to compute different mutual information matrices using information theory. remarkably, they were able to detect allosteric signal transmission from protein dynamics [cit] . they also applied this methodology to a larger set of related proteins to show how evolutionary conservation and binding promiscuity have opposite effects on intrinsic protein dynamics . other examples are provided in section 4."
"1) representation of a star-convex shape: a star-shaped extended object s(p k ) can be represented in parametric form with a so-called radius function r(φ) [cit], which encodes the distance from the center to a contour point depending on the angle φ."
"while the results of the two different methods are encouraging, there is still room for improvement. before outlining a possible road ahead let us first systematically compare the results of the previous two sections. upon close inspection, the result obtained with the student's t penalty function has more details compared to the results obtained by gn but also more artifacts related to the presence of elastic phases. when we compare vanilla l-bfgs (figure 4(b) ) without student's t with the vanilla gn ( figure 5(a) ) without curvelet-domain promotion, we observe that the result for gn are apparently less sensitive to unmodelled phases in the data."
"to date, one of the most developed and comprehensive sa is the protein blocks approach [cit] . this sa is composed by 16 local structure prototypes of 5 residues fragments (see figure 2) . it was shown to efficiently approximate every part of the protein structure. the pbs m and d can be roughly described as prototypes for the central region of α-helix and β-strand, respectively. pbs a-c primarily represent the ncap of β-strand while e and f correspond to c-caps; pbs g -j are specific to coils, pbs k and l correspond to n cap of α -helix while pbs n-p to c-caps. pbs have been used to address various problems, including protein superimposition [cit], general analyses of flexibility [cit] or and prediction of structure and flexibility [cit] ."
"target extent model: for a given shape s(p k ), the target extent model determines where the measurement source z k,l is located on the object (see fig. 2 ). we employ an rhm for the target extent, which is described in section iii-c."
"in this review we have illustrated using numerous examples (darc protein, human integrins, protein complexes, protein/dna interfaces, proteins with post-translational modifications) how the approaches, based on structural alphabets, are a valuable tool to study flexibility at this level."
"in fact, growing evidence shows that proteins are highly dynamic macromolecules and that this dynamics is crucial in many biological processes. thus, recent studies have demonstrated that conformational transitions in folded states of many proteins are essential to accomplish their functions, e.g., enzyme catalysis, activity regulation [cit] . flexibility also allows interactions with different partners, with ligands by inducedfit interaction, with other proteins, or nucleic acids to form complex structures. nmr based methods and computational experiments such as molecular dynamic (md) simulations, have largely contributed to gain valuable insights into the observation, understanding, and analyses of flexibility [cit] . flexibility can be versatile and covers a large range of timescales and amplitudes of structural modifications. it encompasses different kinds of conformational changes corresponding to (i) mobility of rigid part of the protein, e.g., domain motions (ii) deformability of the protein backbone, e.g., crankshaft motions or (iii) both. these different transitions are shown by analyzing and comparing protein structures (see figure 1d) . at a local level, the flexibility can be identified by the information contained in diffraction images of x-ray crystallography experiments and quantified along the refinement process through the debyewaller factors (expressed as surface units) also known as \"bfactors\" or temperature (displacement) factors. these so-called b-factors reflect atom mobility due to thermal vibration and measure the static disorder. they allow quantifying different levels of flexibility in proteins (see figure 1e, [cit] ) . this criterion is also used by majority of flexibility prediction methods (from the sequence) [cit] ."
"from our experiences with these examples, we can state that the use of sas allows to tackle and address the important problem of the comparison of an ensemble of protein conformations. indeed, in a recent paper, scott and strauss [cit] underlines the bias related to the use of rmsd, which needs beforehand an optimally superimposed approach often remains as rigid bodies. they proposed an elegant method, fleximatch, of protein structure comparison that tries to take flexibility into account. as it was done for protein superimposition methods [cit], sa is an efficient approach, not considering proteins as rigid bodies. we underline the interest of our approach based on protein blocks with the pbxplore tools (https://github.com/pierrepo/pbxplore, in preparation) or gsatools (http://mathbio.nimr.mrc.ac.uk/wiki/ [cit] ) in other cases. the use of sas and the development of associated metrics such as n eq is required to study the details and begin to understand the complexity of protein flexibility. it allows discriminating flexibility from mobility and deformability, which is not currently considered by other available methods. nonetheless, it also had drawbacks as no simple threshold will guide the researcher to point out that certain segment is the highly flexible part and not the other, same as for rmsf. in the same way, use of information theory with gsatools also requires expertise. moreover, as sa represents a simplification of the 3d description, its results can be compared to the normal mode analysis based on elastic network model [cit] that are efficient to define large movement. however, changes at a finer level such as side chain rotameric states or minor changes in the backbone (but essential for the biological functions) are more difficult to handle. here as always, a good knowledge of the biological system is essential as a correct definition of the scientific question and its scale [cit] ."
"other interesting sas used in the flexibility context. we have proposed an extension of our sa through a novel library consisting of 120 overlapping structural classes of 11-residues fragments, firstly defined as pbs series [cit] . this library was constructed with an original unsupervised structural clustering method called the hybrid protein model [cit] . for each class, a mean representative fragment, or \"local structure prototype\" (lsp), correctly approximate the local structures with an average cα rmsd of 1.61 å. lsps capture both the continuity between the identified recurrent local structures and long-range interactions. from this description, two methodologies were developed to predict flexibility. the first one was based on simple logistic functions and supervised with a system of experts [cit] . the second one was a combination of support vector machines (svms) and evolutionary information [cit] )."
"the estimated shapes are depicted in fig. 7 for two snippets of the trajectory (averaged over 20 runs). it can be seen that the shapes of the extended objects are tracked well, even when the targets move very close to each other and change the orientation. altogether, this example demonstrates that mixture rhms are feasible for tracking starconvex shape approximations of multiple extended objects that move according to a constant velocity model."
"recent studies have shown that ptms have significant effects on the protein conformations and on their flexibility. hence xin and radivojac used 3d structures from the pdb and studied the conformational heterogeneity of protein structures corresponding to identical sequences in their unmodified and modified forms . they demonstrated that ptms induce conformational changes at both local and global level, but with a limited impact. accordingly ptms would affect regulatory and signaling pathways [cit] by subtle but common mechanisms of allostery. some prediction approaches and are included into dedicated databases [cit], but few analyzed precisely the whole ptmome."
"to compare the hpa-1a and -1b variants, we have proposed for the first time to use a combination of standard analysis of flexibility (namely root mean square fluctuation, rmsf) and protein blocks analyses. md simulations have revealed that (i) the leu33pro substitution of the β3 knee (a domain of β3 integrin chain) leads to adverse structural effects not highlighted by static models; and (ii) that these alterations can explain the increased adhesion potential of hpa-1b platelets to fibrinogen and the possible thrombotic risk associated with the hpa-1b phenotype [cit] . these molecular simulations also support a novel structural explanation for the epitope complexity of the hpa-1 antigen [cit] ."
"from our perspective, the results outlined in this abstract are both encouraging and call for a joint formulation based on dataspace robust statistics in some transform-domain for the penalty function and on model-space regularization, for instance via curvelet-domain sparsity promotion, on the model updates. the modified gn framework allows for this type of formulation since it offers flexibility regarding the choice of the misfit function and norm on the model. in this way, we anticipate that the"
"the main contribution of this paper is a method for modeling and tracking multiple extended targets based on rhms. for this purpose, we first introduce the novel concept of mixture random hypersurface models (mixture rhms), which are an extension of rhms to multiple extended targets. based on this model, we derive a bayesian method for tracking multiple extended targets and we suggest a particular implementation as a gaussian-assumed density filter. the remainder of this paper is structured as follows: first, we give a brief overview of related approaches for extended object tracking in section ii. afterwards, in section iii we discuss how a single extended object can be modeled with rhms. subsequently, we introduce the basic idea of mixture random hypersurface models in section iv-a and derive a bayesian estimator for mixture rhms in section v."
"in figure 2 (c), the obtained variation trends of time domain features of subject 1 showed similar characteristics among the sleep stages when compared with those of the frequency domain. similar results are illustrated in table 4 for subject 2 and subject 3. furthermore, the feature in time domain can highlight the characteristic of a certain eeg rhythm which can be obvious evidence for sleep stage classification. in the awake stage, rhythm was apparently highlighted with the mean value of tn of 0.55, 0.57, and 0.41 for each subject in table 4 . this would be helpful for discriminating the awake stage from others. in the other cases, rhythm in slow wave stage of deep sleep was much more distinctive with the mean value of tn of 0.84, 0.87, and 0.91. comparing figures 2(a) and 2(c), the extracted time domain features showed similar tendency among the sleep stages as well as the features in frequency domain. in addition, the features based on the developed sequence merging rules can highlight the characteristic rhythms in time domain. this would be easy for sleep stage scoring."
"the above-discussed analyses pointed to some remarkable features about the protein/dna interfaces, so that we performed a more specific analysis of the protein and dna dynamics based on crystal structures. the analysis of b-factors [cit] showed that the dynamics of biopolymer residues, amino acids and nucleotides, as well as ordered water molecules is first of all a function of their neighborhood: amino acids in the interior of proteins have the tightest distribution of their displacements, residues forming the biopolymer interfaces (protein/protein or protein/dna) intermediate, and residues exposed to the solvent the widest distribution (figure 9 ). this general picture is best pronounced for structures with the highest crystallographic resolution since discrimination of different types of residues in structures becomes unclear with lower crystallographic resolution. besides, amino acid residues in the protein core display a unique feature: their backbone and side chain atoms have virtually identical b-factor distributions. the protein core is therefore extremely well packed leaving minimum free space for atomic movements. b-factors of water molecules bridging protein and dna molecules were surprisingly significantly lower than b-factors of dna phosphates; in opposite, solventaccessible phosphates were extremely flexible. an unexpected conclusion of this analysis is that a part of the observed trends could be due to improper refinement protocols that may need slight modifications [cit] . hence, the b-factors of high-resolution structures reflect the expected dynamics of residues in protein-dna complexes but the b factors of lower resolution structures should be treated cautiously. based on such kinds of ideas, vriend proposed a dedicated dataset of refined b-factors (http://www.cmbi.umcn.nl/bdb/, [cit] ) ."
"inverting data with elastic phases using an acoustic wave equation can lead to erroneous results, especially when the number of iterations is too high, which may lead to over fitting the data. several approaches have been proposed to address this issue. most commonly, people apply \"data-independent\" filtering operations that are aimed to deemphasize the elastic phases in the data in favor of the acoustic phases. examples of this approach are nested loops over offset range and laplace parameters. in this paper, we discuss two complementary optimization-driven methods where the minimization process decides adaptively which of the data or model components are consistent with the objective. specifically, we compare the student's t misfit function as the data-space alternative and curvelet-domain sparsity promotion as the model-space alternative. application of these two methods to a realistic synthetic lead to comparable results that we believe can be improved by combining these two methods."
"the processed sleep eeg by the developed sequence merging method can highlight the characteristic rhythm which is useful for both automatic sleep staging and visual inspection. furthermore, it can be a training tool for better understanding the appearance of characteristic waveforms from raw sleep eeg which is mixed and complex in time domain."
"the assignment algorithm (see figure 3a, [cit] ) runs through the 3d structure of the target protein, from the n to the c-ter of the sequence. the algorithm is iterative and uses 5 residues long overlapping windows over the entire sequence to assign a pb to every position. for each \"n th \" position of the structure, 8 dihedrals ψ (n − 2), ϕ (n − 1), ψ (n − 1), ϕ (n), ψ (n), ϕ (n + 1), ψ (n + 1), ϕ (n + 2) are compared to each of the 16 pbs. the comparison is made by a least squares approach to match the rmsda criteria (root mean square deviation on angular values) [cit] :"
"implementation as gaussian filter: the above presented formal bayesian filter can be implemented as gaussian-assumed filter, i.e., all probability densities are approximated with a gaussian density, i.e.,"
"where a k (·) is the system function, u k is the system input, and w k the system noise. as the shape vector is part of the state vector, this model also incorporates the temporal evolution of the shape."
"flexibility becomes a critical issue in complexes especially the ones involving intrinsically disordered protein. fine analyses have shown that disordered proteins can also adopt well-defined conformations in their bound form; their inherently dynamic nature is cast into their complexes [cit] . protein families with more diverse interactions exhibit less average disorder over all members of the family . inter-domain linkers are evolutionarily well conserved and are constrained by the domain-domain interface interactions [cit] ). an interesting resource is the comsin database which provides a collection of structures of proteins solved in unbound and bound form, targeted toward disorder-order transitions [cit] ."
"based on the mixture random hypersurface model, the sensor model (1), and the system model (2) for the extended objects, a recursive bayesian state estimator for multiple extended objects can be derived. we consider the stacked random vector x a k of all n target states"
"to evaluate the different conformational states of the duffy protein, we carried out numerous mds simulated annealing simulations with the gromacs software [cit] . md simulated annealing allows a harsh sampling of the conformational space by crossing energetic barriers in an efficient and fast way. many runs were performed and the different conformations obtained at room temperature were analyzed using protein blocks. in practice, we encoded each 3d protein structural model conformation into a 1d string (the length of the protein sequence) using protein blocks. then, we computed, the number of times each pb was observed for each position. positions with a high frequency of a single pb exhibit no local change, while some others positions exhibit local deformations that require a more in-depth analyses. few variations could be observed in the helical regions (pb m and encompassing pbs) that were weakly restrained with harmonic forces. instead, loops sampled large regions of the conformational space. a very interesting result was observed for the n terminus region, and especially the distal region. in contrast to what was suggested by disorder predictors, this region was not a random coil region, but in fact a small β-sheet composed of two β-strands (pbs d and encompassing pbs, seen in orange on figure 4b ), connected by a short turns (in yellow). in the β-sheets, some positions, e.g., 12 and 13, were invariant. likewise, the closest region to the first helix was more constrained than expected and not disordered (in pink and violet). even the central regions (in gray) showed some tendencies to be structured. it was a striking example of a complex series of conformations which cannot be analyzed for instance through classical secondary structure [cit] ."
"hence, we examined the effect of the β3-leu253met substitution of αiibβ3 complex in patients with glanzmann thrombasthenia [cit], a rare bleeding disorder characterized by an impaired platelet aggregation [cit] . for the first time, we showed that residue leu253-localized at the interface of the complex-is playing a major role in the stability of αiibβ3. nonetheless, structural models reflecting static specific states do not depict structural dynamics accompanying the various aspects of integrin functions. for instance, when integrins are activated by substrates, large conformational changes are observed. analyses of static structures (e.g., b-factor, electrostatics), give only a limited view of the protein complex behavior, contrary to mds simulations which are able to some extent, to reproduce the inner dynamics of protein structures."
"in the light of the above observations, the classic representation of protein structure as a succession of repetitive ordered secondary structures and random coil does not allow understanding of the complexity associated with structural flexibility. actually, the coarseness of the secondary structure assignment may prevent from identifying conformational changes. therefore distinction between flexible loops and rigid loops, for example, cannot be made on the sole basis of a three-state secondary structure assignment. a more precise and local description of protein structure is needed. in this regard, structural alphabet (sas), allow to investigate primarily the complexity of the protein conformations, and consequently of their associated dynamics."
"here we let the parameters a p and b u as 0.6, 0.4. these two parameters determine whether the degree of privacy protection or the data availability is more important. no matter under what circumstances, the execution time of clustering will increase as we choosing a bigger anonymous parameter k. when choosing a large data set, the execution time of the algorithm grows rapidly as k increases (figure 1) . compared with ordinary clustering algorithm, the execution time of the algorithm has increased to an extent within an acceptable range. the reduction of information loss will compensate for the increase of execution time. the next paragraph measures the information loss from the following aspects. this paragraph only discusses information loss in static way, in other words, the experimental results of micro aggregation in section 3.3. the final set q contains the equivalent groups. each tuple t has the value c (g, t) for the purpose of finding the most suitable tuple t to join the equivalent group g. c (g, t) contains two factors: the distance value between the tuple t and the center of equivalent group g and the entropy increase which decided by the sensitive attribute in each group. the information loss is caused by the generalization when clustering, so it just takes the distance value between the tuple t and the equivalent group into consideration:"
"in qbiassr, when selecting an action to execute, instead of applying a softmax directly to the values of q(s) for the current state, this softmax is performed over another, virtual vector q(s) biased, defined as:"
"remote sensing image processing pipelines are beginning to exploit deep learning. initially, such systems tackled the problem of image / tile classification: i.e. assigning to large patches (or small images) a single semantic label such as \"urban\", \"sparse urban\" or \"forest\". this task is well represented by the uc merced landuse classification dataset 2 . [cit] present a two-stages system, in which pretrained cnns are combined with a second stage of supervised training. this strategy mitigates overfitting and shows excellent performances. [cit] show that models pre-trained on general image classification tasks (specifically on ilsvrc) can be used as generic feature extractors also for remote sensing image (tile) classification, outperforming most of the state-of-the-art feature descriptors. authors point out that such models were particularly well suited for high resolution aerial data. finally, [cit] also explore pretrained architectures. they also study the impact of domain adaptation by fine tuning the model to the new task (from ilsvrc data to remote sensing images) or by training from scratch, always keeping exactly the same cnn architectures. they show that pre-trained models fine-tuned on remote sensing data perform better than models with a same architecture but trained from scratch (with randomly initialized weights). this indicates that cnn architectures devoted to natural image classification tasks without specific adjustments tend to overfit remote sensing data. typically, to model complex appearance variations, cnns devoted for image classification contain many parameters, particularly in the fully connected layers, which could easily contain more than 90% (vgg network [cit] ) or 95% (alexnet [cit] ) of the total number of learnable parameters. with the typical remote sensing datasets such large number of parameter would be hard to estimate, because of the limited amount of labeled samples they generally provide. for this reason, training networks for remote sensing problems requires adjustments."
"the cytoplasm distribution is selected as the source of noise. cytoplasm noise is a random value that satisfies the cytoplasm distribution. in the domain of differential privacy, the cytoplasm noise mechanism is a common way to protect personal privacy. as an innovation, a method using cytoplasm noise to protect privacy based on dynamic micro aggregation updating is proposed in this paper. the probability density function of the distribution is expressed as follows:"
"we employ backpropagation with stochastic gradient descent (sgd) with momentum [cit] . we fix the momentum multiplier to the standard value of 0.9. to monitor the validation error during training we predict, at each epoch, n b · 100 validation patches from the validation images, and compute the error. we also sample validation patches uniformly across classes. note that validation patches are never used in the training process."
"the particular tasks implemented are listed in table 2, while figure 3 shows the scenarios designed in v-rep for the above tasks. rl-robot has been released as an open source project, python pep8 style, available on github (martínez [cit] )."
"the learning experiments with the non-markovian realistic v-rep simulations gave accurate results. besides, without the above memory restriction, we could extend the study to the tasks wander-4k and 3d-arm-4k. the resulting learning curves are shown in figures 6 and 7, where it is shown how the novel tosl+qbiassr outperforms tosl+sr in five of the six tasks tested, never affecting negatively any learning process, nor even the simplest wandering task, where better results than tosl+sr were unlikely."
"sarsa often reduces the number of steps needed for learning a task, and it also avoids policies close to large negative rewards, which could harm a real robot."
"a practical upgrade of q-learning came with sarsa [cit], which updates the q-values for the action the agent will execute in the next learning step instead of the optimal action learned so far. hence a, the action to perform in the next step, must also be selected before updating q(s, a). the td error in this case is:"
"finally, a last set of tests has been carried out to measure the computational cost of the algorithms used in this work. for this purpose the tests were executed in samplemodeled simulation for the wander-1k task. different rl implementations were tested, executing 30 learning process of 3600 steps each. the measurement of choice to assess the computational impact of the learning algorithms is the average cpu time consumed in each learning step. the results, obtained with rl-robot in an intel negligible for rl applications. the computational cost of tosl (reduced et) with qbiassr action selection also results in the same order of magnitude as the above methods. though qbiassr uses extensive information from many states, its reduced computational impact has been achieved due to the fact that all the structure of the sets of states biasing a particular state can be generated just by knowing the input variables involved. this allows an efficient implementation, computing most of the qbi-assr algorithm beforehand, and obtaining a delay of ≈ 1 second before beginning the learning process, measured in the same conditions as before. for real-time robotic applications, even non-reduced et could be used with minimal impact on the learning process, since the waiting time for reaching a new state is usually much longer than the one needed for updating q for all the states."
"for classical k-anonymity and l-diversity, they are only suitable for static data but invalid for dynamic environment. although the m-invariance theory can dynamically update the data like adding or deleting tuples, the process will iterate all the tuples when adding, deleting or modifying. it will cause a great redundancy. besides, if the data has been released for several times, there are more and more forged data in each equivalent group. this will reduce the availability of information. this section proposes a dynamic scheme based on the micro aggregation algorithm mentioned above to achieve the dynamic update like adding, deleting a tuple t in a more effective and flexible way. this chapter mainly involves the following points: dynamic adjustment after micro-aggregation clustering, forged data, laplace noise scheme."
"the functions g l composing the l layers of the network g are usually linear functions, subsequently passed through nonlinearities, while weights w l are learned from data. for instance, multilayer perceptrons model input-output relationships by a series of densely interconnected hidden layers composed by linear units and nonlinear activations functions. cnn are structured in a similar way, but neurons are learnable convolutions shared at each image location. in the following, we provide a comprehensive introduction to the structure of our cnn and present important strategies to reduce overfitting while training."
"being bias(s) a vector resulting from processing the values of other states of q that can be similar to s. this bias is calculated at each step of the learning process using averaged information from sets of states that share some structure with the current state s. in this way, the probabilities of choosing each action from s will be biased by the previous experience obtained in similar states."
"(ii) 2d mobile navigation (episodic): the robot must reach a static point in the scenario. the input variables here extend the wandering ones with the discretized pose (x, y, θ) of the robot. rewards maintain the colliding penalties and include positive and negative rewards according to the distance approached to or moved away from the target, respectively."
"figure 3: v-rep scenarios for our experiments. top) 2x2m and 6x6m with obstacles, for wandering. bottomleft) 32x12cm for 3d arm motion. bottom-right) 4x4m with obstacles for 2d mobile navigation."
"the present work has been motivated for this lack of practical, task-independent rl mechanisms for real robots, and it contributes a core method suitable for learning multiple tasks, although possibly non-optimally. this core method intends to avoid the use of representations, prior knowledge and models, and to be extendable with most advanced rl techniques. we also present here a study of the performance of our core rl solution compared to combinations of basic rl algorithms, i.e., those updating their q-values without using approximators and models, and also with several action selection techniques. we have performed empirical comparisons of the learning processes in different robotic tasks. at this state of our research, only the tabular case of rl with low-dimensional state spaces (easy hand-crafted feature representations) has been addressed, leaving generalization and function approximators, needed for higher-dimensional tasks, for future work."
"in recent years, academic researchers have done a lot of studies on privacy protection. in the privacy protection literature, there are three fields of work with seemingly different goals. the first field is about privacy by policy. the second field is about privacy by statistics [cit] . the third field is about privacy by cryptography. this paper and the literatures contained is mainly focus on the second field. all the research is carried out under a core idea-maximizing the availability of the data with the condition of individual privacy not being leaked. the basic theory of privacy protection in the second field is k-anonymity [cit] . after anonymity processing, the method requires that each record cannot be distinguished from the other k-1 records in a data table, but the k-anonymity method does not take into account protecting the sensitive attributes. the result can be attacked by an attacker who has background knowledge. l-diversity [cit] theory is based on the k-anonymity considering the protection of sensitive attributes, which ensures that each group contains l different sensitive attribute values. attackers can only infer the sensitive information of the target by the probability of 1/l. a reference proposed a method to protect data privacy by clustering in static environment, which decides the equivalent groups in the data set by clustering analysis and finally achieves privacy protection [cit] . imagining another situation, l-diversity based on the k-anonymity deals with the sensitive attributes, and makes sure that there are l attributes in an equivalent group at least, but if the l attributes almost concentrating on some certain attributes, consequently, attackers can infer the result of the personal privacy in high probability, so the theory which is called l-diversity p-sensitive [cit] comes into being. the main idea of the theory is the l-diversity p-sensitive algorithm not only guarantees each qi group has l sensitive attributes, but also makes sure the biggest probability of any sensitive attributes in each equivalent group is less than p which is defined at the beginning of the method [cit] . although the literatures mentioned above had made great contribution on privacy protection, those methods only suit for static data, in other words, they did not consider the need for multiple releases of the dynamic data."
"training a network explicitly for semantic labeling or for classification problems are very different task, as illustrated in fig. 1 . in the former setting, we would like to train a model that is able to label each pixel present in the image. this should be achieved not only by learning the relationship between colors and labels, but mostly by learning and taking into account spatial relationships at different scales. in the second setting (patch classification), a network trained for classification predicts a single label per patch. this patch-level label is then assumed to be the label of the central pixel. in this setting, many queries are spatially concatenated to obtain the prediction map. in addition to being extremely inefficient, patch classification-based strategies are inappropriate, since they do not explicitly learn spatial configuration of labels and consequently oversmooth objects boundaries. to make an example, a cnn model aiming at classifying a patch centered on a car lying on a road could score high for both the classes \"road\" and \"car\": both solutions are semantically correct, since both classes are somehow lying in the center of the patch. given the content and the context, it's hard to penalize one solution more than the other. in the semantic labeling case, we would like to predict the label of each pixel in the patch jointly, thus avoiding ambiguities given by the content of the patch and de facto performing structured prediction by learning classspecific structures contained in each patch."
"cnns are composed by a sequential hierarchy of processing layers (fig. 3) . from the input to the final classification layer, data go trough a series of trainable units. a general feed forward network can bee seen as a concatenation of functions, starting from some input x:"
"the class \"car\" is very difficult to correctly segment. sp-msf often misclassifies \"cars\" because superpixels do not always isolate class instances. again, cnns are generally more accurate. cnn-pc, although showing good segmentation for detected cars, misses most of them. semantic labeling cnn are more accurate in detecting single cars, and, in particular cnn-fpl offers a good trade-off between segmentation accuracy and detection (e.g. clips 2 and 3). the class \"background\" is detected by all methods with different success rates, mostly depending on its local appearance (recall that this class is not semantically nor visually coherent, since collecting different semantic classes)."
"we can consider two paradigmatic examples of robotics projects that would be benefited from such an ideal, found in our research group, but common in many others: i) the implementation of several pick and place tasks with a complex 6dof low-weight robotic arm, whose model is not available but that is required to execute complicated trajectories; in this case, top-down implementations demanded many hours of work, usually employed to adapt small variations of the same task. ii) a set of mobile robotics platforms that must be used for autonomous navigation, teleoperation, and collaborative control; differences between the behavior of the mobile bases, even among identical robots, often require the navigation programs to be tuned from one robot to another, and other features such as the variations of the wear of the wheels and the battery levels contribute to worsen the performance of the tasks as well."
"another feature of rl-robot that sets it aside existing rl frameworks is the independence between the abstract learner, portrayed by the artificial agent, and the perceptual robot. we consider that any rl task must define the information of the later; thus a change in the robot, a device, or any input-output variable results in a different task. at the beginning of the learning process, the agent automatically structures itself to connect the learning process with the desired task, no matter the rl algorithm, the action selection technique, the environment, or any hand-crafted definition of the task to learn. the general architecture of rl-robot is summarized in figure 1 . rl-robot also includes two built-in generic robots: a mobile base with a laser rangefinder, and a 3dof arm. they can be used separately or together as a single robot (a mobile manipulator). modules v-rep sim and ros node implement links with v-rep and ros, only executed if required, thus connecting our framework with both realistic simulators and real robots."
"(iii) 3d arm motion (episodic): a 3dof arm must reach an object on a table. the input variables are the (x, y, z ) positions of the gripper and the object. the outputs are the speeds of up to 3 joints. positive and negative rewards are received if the gripper approaches to or moves away from the object, respectively."
"the above definitions of the distance between tuples make sure that the most similar tuples will join an equivalent group so the information loss is minimal. however, the sensitive attributes are not protected. the basic way in other micro aggregation algorithms is considering clustering at first, and then considering the protection of sensitive properties. for example, when an equivalent group is generated based on the distance between tuples, some other tuples which is not belong to this group must be readjust into the equivalent group in order to achieve some certain standards like l-diversity or p-sensitive for the purpose of protecting sensitive attributes. therefore, this will cause information loss inevitably. this article uses information entropy as one of the indicators of clustering for micro aggregation, deciding the clustering result together with the definition of distance from tuples. the following definition of information entropy is listed below. here will also discuss the rationality of the algorithm:"
"-random transformations. to slightly vary the spatial organization of the patches and to enforce learning invariances of interest, we randomly rotations at random angles and flippings. random rotations are applied on each training image before sampling the super-batch (as explained later in sec. iii, the super-batch will be resampled at regular intervals). random flippings are applied when selecting the mini-batch during training, independently to rotations."
"each tuple has a distance value when it join to an equivalent group. the average information loss in a group is calculated as: the sum of distance value from a group divides the total number of the group's tuples. the total average information loss is calculated as: the sum of each equivalent group's average information loss divides total number of tuple in the data table. the experimental results are shown above (figure 2 ). compared with k-anonymous l-diversity (in the same set of experiments, we use the same value for k and l, because the range of sensitive attributes is much larger than the number of tuples in the group), the average loss of information value will significantly increase when we choose the bigger anonymous parameter k in both algorithm, but the peak value will reach stability faster as the tuple size growing in this paper's algorithm. in addition, here we verify the experimental results by the number of forged data from the perspective of dynamic update in section 4. we compare the quantity of forged data between m-invariance and our dynamic update algorithm. we selects the same 5000 records for clustering. after that, we choose different size of tuples to join the result above as comparison based on the dynamic update method (those incremental records are processed one by one according to the method in sections 4.1 and 4.2) in this paper and m-invariance. the chart as below (figure 3) . when there just a little new tuples adding to the result of clustering, the number of the forged tuple of this paper's method are bigger than the m-invariance algorithm. with the growth of added tuples, the forged tuples in our dynamic update algorithm are significantly less than m-invariance algorithm. because there just only one forged tuple in each equivalent group at most, no matter how many new tuples are added to the result of clustering. with the growth of the added data, the effect of the dynamic update algorithm are better than the m-invariance algorithm."
"in order to calculate the distance between the discrete attributes, the value of the discrete attribute must be numeric. the discrete attributes are divided into ordinal attributes and nominal attributes. if there is a certain difference or size relationship between two values, the attribute is called ordinal attribute:"
"c) weight decay: weight decay is an 2 regularizer adding a penalty term to weight updates during backpropagation. it is applied only to convolutional filter weights (and not to the biases) and favors smooth convolutional filters. the weight decay hyperparameter controls the penalization. d) data augmentation: a commonly used strategy to further increase the size of the training set, is to perform data augmentation. it consists in creating new synthetic training examples from those already available, by applying labelpreserving (random) transformations. this step ensures that the model sees different possible aspects of the data in different batches, improving generalization error by i) increasing the number of labeled samples to learn from and ii) regularizing the model [cit] and iii) reduce potential correlation between patches in the batch. to train the cnns, we first create a superbatch by sampling a given number of training patches. from this super-batch, we then randomly sample the mini-batch used to effectively train the cnns. we apply transformations at both levels. we describe adopted augmentation strategies below:"
"step 1, the process detects the changes in each group. when there is any change likes adding a new tuple into a group or deleting a tuple from a group or modifying an existing tuple, it will check out if there has any forged data in this group. situation 1: the equivalent group already has the forged tuple. let the sensitive attribute value of the forged tuple t change to other sensitive attribute value sr randomly. situation 2: there is no forged tuple in the group."
"the work presented in this paper is a first step towards the ideal solution explained before. it focuses on building a common implementation of a machine learning core approach intended for solving multiple low-dimensional tasks found in service robotics, such as wandering, 2d mobile navigation, 3d arm motion, etc. for this purpose, the robots should learn by themselves the effects of their actions, with minimal tuning by the engineer. a well-known machine learning and decision-making methodology, reinforcement learning (rl), has been chosen as the basic mechanism for that. rl is based on the explicit implementation, by the engineer, of a set of rewards that define the objectives of the task to learn."
"for c classes. inputs x i are a c-dimensional vector representing unnormalized scores for the location i, as given by the penultimate layer (the one before the loss function). the filters of this penultimate layer can be interpreted as the weight vector of the classifier. the classification loss (cross-entropy) is:"
step 6 aims to find the most suitable tuple t to join the equivalent group g based on the value of equation (8) presented in last paragraph.
"a simple ad-hoc (but task-independent) controller called low-reward-loop evasion (lrle) has been introduced to solve this problem; it detects whether the agent is selecting low-reward cyclic sequences of actions, and acts by increasing the temperature of the softmax regression, thus favouring the selection of other actions out of the detected low-rewarded sequences. algorithm 3 describes the implementation of lrle."
"as observed for the vaihingen dataset, the superpixel baseline offers more balanced errors across classes when compared to the cnn-pc, since the cnn-pc shows higher oa and k scores while lower aa and f1. both approaches perform similarly on the \"clutter\" class, since balanced evaluation metrics significantly increase while removing such class. directly predicting patches with the cnn-spl strategy results in global metrics roughly on par to the baselines sp-msf and cnn-pc, while slightly better than cnn-pc on balanced metric. as for the vaihingen dataset, the improved modeling power of the cnn-fpl offers better accuracies for all the accuracy metrics considered."
"consider the following situation, if an attacker ask the data owner for the anonymous data frequently, he or she can infer the real result value of sensitive attribute's distribution through calculating the mean value of the result. the reason is each noise that added to the final result obeys the laplace distribution with an expected value. if the data owner just let the authorized recipients request for the result without restricting, they will simply calculate the real result and the laplace noise mechanism loses it's meaning of protection, so as the data owner like hospital's information database, manufacturer of wearable devices, some third-party cloud computing platforms and so on, they should restrict each receiver's query times during some time based on each receiver's authorization level."
"the next set of experiments have been conducted with the giraff robot shown in figure 8, that is equipped with two differential drive and two caster wheels, and a hokuyo laser rangefinder. the same tasks designed for the simulated mobile robot can be learned by giraff just by specifying the physical parameters of the task. rl-robot launches the ros node in this case, giving access to the real sensors and actuators."
"step 5 creates a forged tuple t with the random sensitive attribute value sr adding to the group. the same process is also applied to the delete operation. when deleting a tuple from any equivalent group, there are also two situations as above. as for the modifying, it will be replaced by the deleting and adding steps."
the algorithm in this chapter is still based on the k anonymity theory and an equivalent group contains k tuples at least. it is necessary to select the parameter k at the beginning of the clustering to determine the minimum size of each equivalent group. we use a set q to represent the equivalent groups after clustering. the pseudo code of the algorithm is given below:
"in the future work, we will continue to learn these following aspects: (1) how to minimize the risk of privacy leakage in different versions of the data; (2) the distance between tuple is the key to the whole algorithm, so how to optimize the distance property is a major research in the next; (3) based on the different privileges level for receivers, how to introduce differential privacy protection mechanism to query result set is also needs to be considered."
step 4 shows that a t is randomly taken from t as an equivalent group g and remove t from t. steps 5-10 are the key processes in the micro aggregation algorithm.
"different needs based on health care, scientific research, business plan, data sharing, trend prediction, policy making, the data owners, such as government, enterprises, and equipment manufacturers, may need to publish the data that they have. for some reasons, they need to update the data and release the different versions of the result. if the original data is released without processing, it will lead to personal information being exploited by others. this paper studies the data privacy protection problem and proposes a dynamic update algorithm based on micro aggregation. this method uses the concepts of distance between tuple and information entropy increase of a group together as the input. in order to find a best cluster that not only the availability of information is guaranteed, but also the privacy protection is realized. the method also proposes a dynamic update scheme to realize privacy protection when the data is changed. a laplace noise mechanism is also applied to protect the sensitive attributes of result set. in addition, this article saves the quasi identifier attribute and sensitive properties separately and gives the different result to the receiver based on their limits of authority. in a word, this algorithm ensures the information availability after data anonymity and efficiently reduce information loss with dynamic update function."
"as we all know, from the information point of view, if, the value of the above formula is 0 when there is no uncertainty. when the probability is distributed evenly, the value reaches the maximum. this just meets the requirements of sensitive attribute protection, if the sensitive attribute values from an equivalent group are concentrated in a certain value, then the entropy value of the equivalent group is small. on the contrary, if the sensitive attribute values from an equivalent group are distributed on average, the entropy value of the equivalent group reaches the maximum."
"all the presented results are computed on a desktop machine with an intel xeon e3-1200 (quadcore), 32gb ram and an nvidia geforce gtx titan x (12gb ram). we build the tested system using the dagnn wrapper around cnn libraries provided by matconvnet 3 version 1.0 beta 20."
"this article uses the data set adult in the uci machine learning repository, it consists of u.s. census data. some records with missing values in the data set are removed. after that, it has 45,222 records and contains 14 attributes. this experiment selects eight properties: age, gender, race, status marital, number education, country native, class work and occupation. here selects occupation as a sensitive attribute. as for the laplace noise mechanism, we use the apache commons implementation \"laplace distribution\" and the noise is generated by extracting a random sample from the distribution. this section defines a concept for calculating the information loss after clustering and the result will be analyzed and compared to other algorithm. this section also analyzes the availability of the algorithm and compares the execution time based on different experimental parameters. the hardware environment of the experiment was an intel(r) core(tm) i7-4710mq cpu @ 2.50 ghz (lenovo, beijing, china) equipped with 12 gb ram and running the win 10 os."
"where s is the stride (the spatial interval between convolutions centers) and z is the number of 0-valued rows and columns added at the borders of the image, or zero padding. zeropadding the inputs is very important to control the size after convolution (e.g. to ensure an activation for each location with respect to the input). convolutional layers are not fully connected: neurons are shared, i.e. each filter is applied by sliding it over the whole input, without needing to learn a specific neuron per location. response (activations) for each filter are then stacked and passed forward. an example is given in fig. 2(a) ."
"simulated experiments have been performed for both sample models and realistic simulators. sample-modeled experiments are based on markovian models consisting of t (s, a, s ) and r(s, a, s ) that are built offline after exploring the tasks in the v-rep simulator. on the contrary, in the realistic simulated experiments, each learning step has been performed in one second of simulation time directly in v-rep. the total time for each learning process ranges from 1 to 60 minutes (3600 steps) according to the task to learn."
"2) qualitative results: in fig. 4(1-4) we summarize some aspects of the predictions by clipping map portions from 4 out of the 5 validation images (the 5th image is shown in its entirety in fig. 5) . in all the clips, we can see how the different methods act on boundaries, in particular for the class \"building\". for buildings having high contrasted boundaries, sp-msf provides the best preservation of their geometry. however, there are many cases of \"building\" instances and particularly for other classes, in which boundaries are not sharp: in these cases sp-msf is prone to fail. for single regions with ambiguous appearance, the predictions can be noisy and, since the context of each region is not taken into account, can result in wrong assignments. cnn-based strategies result in more accurate and semantically coherent segmentations. cnn-pc, because of its unstructured nature, does not preserve well object boundaries and tends to overmooth classes with complex boundaries. as expected, cnn-spl offers a tradeoff between cnn-pc and cnn-fpl, which, in turn, offers best segmentations. cnn-fpl makes better use of class cooccurrences, for instance by avoiding spurious prediction of small patches of the class building (e.g. in clip 1). cnn-fpl deals better with thin and elongated elements and boundaries in general, by preserving the shape of such structures (e.g. building shape in clip 4 and the gap between the buildings in clip 3) thanks to the learned upsamplings."
"step 8 compares either the step 6 or step 7 is the best way to do with the equivalent group g (let t join to g or let g join to g k ). in step 9, t will be removed form t and join to the equivalent group g when the condition is established. in step 10, g k will be removed form q and fused with g when the condition is established. in step 12, g will be added to the set q. steps 14-19 illustrate when there is only a few tuples (less than k) in t, choosing a tuple t randomly and forming a new equivalent group g. let the group g compare to each existing group in set q for the purpose of finding the most suitable equivalent group in q to join."
"finally, since the probabilities resulting from the softmax regression are affected by the magnitude of the q-values involved (which, in turn, come form rewards) and our goal is to build a rl method independent on the hand-crafted parameters for multiple tasks, we also propose to apply the action selection over a normalized vector. we have employed the theoretical maximum value of q as a reference for such normalization, being the normalized vector:"
"in this paper we present a cnn-based system relying on an downsample-then-upsample architecture. specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. by doing so, the cnn learns to densely label every pixel at the original resolution of the image. this results in many advantages, including i) stateof-the-art numerical accuracy, ii) improved geometric accuracy of predictions and iii) high efficiency at inference time."
a comparison test between the reduced version of eligibility traces (et) proposed in section 3 and standard et has also been performed in sample-modeled simulation to verify that the reduced et used in this work has a negligible effect over the learning process. this conclusion is clearly shown in figure 10.
"unfortunately, preliminary tests with tosl+qbiassr in v-rep, although improving tosl+sr, have also evidenced a known issue: the chance of falling in cyclic sequences of states before exploring the actions leading to the goal, preventing the robot to efficiently learn the desired task. we believe this is more evident in qbiassr since it reaches useful sequences of actions sooner than less efficient algorithms; cyclic sequences are also present with softmax occasionally."
"the structure of the paper is summarized as follows: section 2 provides a brief introduction to rl and the state of the art of its application to robotics. our approach is described in section 3, that deals with both the combination of already existent techniques and the qbiassr algorithm. section 4 describes the rl-robot software framework implemented for the simulated and real experiments shown in section 5. finally, results and future work are summarized in section 6."
"the experiments of this paper have been conducted in rl-robot with both simulated and real robots. periods of time of up to 60 minutes were used in simulated experiments so as to be reproducible on real robots. a learning curve showing the evolution of the average reward obtained over time (steps) has been found suitable as both a stable measurement of the performance of the learning process and a reliable indicator for comparing different techniques. for episodic tasks, the evolution of the average reward of the last episode was used as indicator instead of the classical evolution per episode, since the number of episodes required were low. the results include the average learning curve of several repetitions, along with an analysis of variance and a post-hoc tukey test [cit] for each task in order to assess the significance of the conclusions."
"when designing a robotic task for a discrete state-space rl, physical input variables involved in the learning process are usually identified, discretized, and hand-crafted before being mixed to generate structured states (structured states are thus states composed of a number of parts, each one representing an input variable [cit] ). fortunately, by having identified these variables, areas of the q matrix associated to one or several combined set of them can be used for deciding what to do when an unexplored or poor-explored state is reached, i.e., we can use the information from states with similar values of these variables. this intuition has led us to the novel q-biased softmax regression (qbiassr)."
"-random sampling. semantic classes are unevenly distributed spatially and their frequency highly varying. for instance, the class \"road\" is ubiquitous, while the class \"car\" is localized (appearing mostly on roads) and rare (in the vaihingen dataset described in section iv-a, 1.2% of the training ground truth represents \"cars\", while 27.94% \"roads\"). consequently, we generally sample the training patches randomly in space (among the training images) and uniformly with respect to class frequencies. to control this process, we simply account for the class corresponding to the label of the central pixel of the patch."
"at the beginning of this section, the execution time of the algorithm will be discussed. in order to display the effect of the algorithm intuitively, we only choose the number of 5000, 10,000 and 20,000 tuples separately as the input for the micro aggregation algorithm as comparison. the clustering run time of each condition is shown in the diagram below."
"rl algorithms address the problem that arises in mdps when no information about the transition function t (s, a, s ) is given, but can be estimated from the real system by making observations. in particular, q-learning [cit] ) is one of the most practical algorithms employed in rl; it belongs to the class of temporal difference (td) learning methods, which combine several aspects of monte carlo and supervised programming for handling delayed rewards. equation 2 represents the general form of q-learning, used in this work as the algorithm of reference:"
"where e k (s, a) is the trace for q(s, a), being increased or set to 1 (replacing traces) whenever the state-action pair is explored, and decreased in every unexplored step as:"
"the operation of mobile robots in real environments requires coping with unforeseen situations that human programmers find difficult or impossible to catalog and therefore to code into a program. therefore, it is highly desirable that the robot itself modifies its behavior to cope with them. an ideal solution would consist of an architecture capable of automatically evolving the controller to improve the performance of any task over time; the same implementation should be employed in different robots and environments without considering the scenario or the hardware involved, and without reprogramming or manually tuning the resulting controllers, i.e., such architecture could be employed for new tasks by making minor changes to already implemented ones, e.g., by reprogramming the objective and the devices required to accomplish them."
"after that, the learning process will not be affected by the absolute hand-crafted rewards, which contributes to the independence between the decision-making process and the task to learn."
"s emantic labeling is the task of assigning a semantic label (land-cover or land-use class) to every pixel of an image. when processing ultra-high resolution data, most of state-of-the-art methods rely on supervised classifiers trained on specifically hand-crafted feature sets (appearance descriptors), describing locally the image content. the extracted high-dimensional representation is assumed to contain enough information to cope with the ambiguities caused by the limited spectral information of the ultra-high resolution sensors."
"-noise injection. the last class-preserving random transformation consists in jittering, i.e. injecting small random additive noise to each patch in the mini-batch. jittering is important since it forces the model to learn spectrally smooth decision rules, by reducing correlation across similar patches in the mini-batch and making the input distribution denser."
"as the internet and big data have been rapidly developing ample concern focusing on protecting personal privacy, which has become one of the most popular research areas, is on the rise. for the purposes of medical services, scientific research, business strategies, information sharing, trend predictions, et cetera, many organizations that own huge amounts of personal data have needs to publish or share the data [cit] . if the original personal data was not processed in any way, it must be cracked or leaked [cit] . the main direction of contemporary data security research is how to process data in ways that both protect personal privacy along with meeting the needs described above. mainstream methods of processing data is data generalization [cit] that aims to aggregate data and generalize an accurate value to an interval within a group of data. by doing this, a potential hacker cannot deduct the accurate personal sensitive data to achieve privacy protection. the key of such methods, of generalization, is the development of aggregation (clustering) [cit] rules, which should be designed in the way that guarantees the usability while protecting client privacy."
"in addition, this article uses two tables to store quasi identifier attributes and sensitive attributes, respectively. for example, quasi identifier attributes are stored in table 6 (left part) and sensitive attributes are stored in table 6 (right part). the gid will connect this two tables. this method weakens the connection between quasi identifiers and sensitive attributes and still maintains the trend or availability of the data. the process of clustering, dynamic updating, are all implement by the data owner. after those processes, the data owner will publish the data like table 6 . the sensitive attribute's percentage of each group is generated according to the distribution from the original group. the receiver can use this data to mine, predict, or analyze some related problems. compare to the m-invariance theory, the biggest advantage in this method is there just only one forged data in each equivalent group. as the tuples continue to increase or we choose a big anonymous parameter number k, this method will cause less impact on the availability of information. in terms of privacy protection, there just makes two tuples are indistinguishable by adding the forged data. in other words, an attacker has 50% chance to infer the personal privacy information. in order to solve this problem, this article makes use of the laplace noise mechanism which will be introduced in next section."
"the parameter a p and b u are used to flexibly determine either the information availability or individual privacy is more important to the data receiver. by selecting different values for this two parameters, the orientation of the result will be changed. consider the following situation, if just thinking about the distance concept at first, and then let the l-diversity and p-sensitive as mandatory parameters for the result separately. this is very likely to cause information loss in the equivalent group."
"we have demonstrated in this paper that state-of-theart value-iteration-based rl algorithms that evolved from classical q-learning, such as tosl, can be effective when applied in multiple real tasks, something still unachieved in robotics. for that, we have found suitable common, basic parameters for tosl, and we have introduced a novel exploration technique, qbiassr, which improves the classical softmax action selection. qbiassr takes advantage of the physical input variables used by the robot, thus the action selection can be influenced by other previously visited states with similar inputs. a complementary low-reward-loop evasion algorithm has been added to prevent local optima sequences. the combined algorithm tosl+qbiassr has the advantage of being both task-independent and compatible with most advanced rl techniques for improving learning in higher dimensional tasks, with a negligible computational cost. a comparative study with other rl techniques has been performed, including both non-episodic and episodic robotic tasks. realistic simulations and real robot experiments reveal that the learning processes with the novel tosl+qbiassr outperform those with tosl+sr in most tasks, being equivalent to tosl+sr in the worst case scenario. an additional result has been that markovian sample-modeled learning processes are unrealistic for robotic tasks because of the gap of the performance detected between v-rep simulations and the models extracted from them. the rl-robot software framework has also been introduced in this paper. it is an open source project designed to ease the implementation of new tasks for both simulated and real robots, including v-rep and ros interfaces. just by defining the physical variables and parameters involved, each task provides structured perceptual information of the environment to the abstract agent regardless the learning method used. rl-robot contributes a novel implementation that achieves independence between the abstract and perceptual aspects involved in rl in robotics."
"for this work we have developed a rl python-based software framework, which is focused on performing experiments with a variety of robots and environments for different robotic tasks, and that includes tosl and qbi-assr along with other standard rl algorithms and action selection techniques. the framework, called rl-robot, can be used stand-alone, with the physically realistic robot simulator v-rep, or with the robot operating system (ros) (quigley, rl-robot provides a framework for robotics researchers that have minimum rl knowledge, and it is ready to perform experiments just by setting their parameters and creating new tasks. a single module serves to specify the parameters of the experiment, including the type of environment, task id, speed rate, repetitions, episodes, steps, algorithm id and its parameters, action selection strategy id, output options (files, charts), etc. each task module contains the definition of input variables or features (later codified as states automatically), output variables (actions), set of rewards, and the physical devices of the robot (laser rangefinders, motors, etc.). the definition of input variables instead of states, a distinctive feature of our framework, besides of easing the implementation of the task, is used by the qbiassr algorithm to determine which similar states of q will influence the decision-making process of a specific state s."
"we obtained results aligned with the state-of-the-art models on two extremely challenging datasets, without performing any post-processing (e.g. crf or mrf) and without recurring to strategies involving external classifiers and additional handcrafted features."
"by observing the behavior of the robot on the simulator in these preliminary tests, we detected that the learning process was highly inefficient whenever the agent reached a new state with strong resemblance to already explored states; the robot had to learn from scratch an accurate action for that new state, or, in other words, intensive exploration was performed instead of trying suitable actions already learned in similar situations, resulting in an inaccurate and disappointing behavior from the human point of view. in order to avoid the use of task-dependent tuning for addressing this issue, we have devised the next contribution."
the most important thing in the clustering process is to find a numerical value deciding which tuple is the best choice to join the equivalent group. this section defines a function as below:
"an rl problem, defined as a set of states, actions, and rewards, looks abstract and task-independent by itself; however solving a task in a rl approach (i.e., converging to a near-optimal value function), depends on the structure of the unknown transition matrix, that is, on how easy is to explore all interesting states frequently enough. therefore, our efforts to decrease the dependence on the particular task have been focused on accelerating the estimation of the q-values for relevant states-actions pair that sometimes are difficult to explore in that task."
"in the field of data dynamic updating [cit], the data set is constantly decreased or increased with different needs. here are some effective methods of data dynamic updating which can achieve privacy protection no matter how many versions the result are released. byun and sohn firstly proposed a method [cit] of dynamic updating that prevents privacy being leaked when data set is re-released. kui puts forward an m-invariance [cit] generalization principle dealing with the situations of insertion or deletion of dynamic data. zhang presents a nc m-invariance [cit] algorithm to solve the problem of dynamic numerical sensitive attributes based on the theory of m-invariance, which can transform the dynamic numerical sensitive attributes to classified sensitive attributes. wang [cit] introduces a method which can keep sensitive attributes consistent in the different versions caused by continuous re-release and proved its efficient utility. in reference [cit], an anonymous method using the concepts of data decomposition is proposed. the core idea of this method is separating sensitive attributes and quasi-identifier attributes into two parts, and associated by the parameter g_id."
"as the equivalent groups become more and more huge, the information loss will increase simultaneously in a group. no matter what attribute's value the new tuple has, it will always join to the existing groups and finally reach the limit of one group. this chapter puts forward a method of dynamic clustering based on the previous micro-aggregation algorithm. when clustering process has been completed and a new tuple needs to update, the beginning of the new process is same with the algorithm (steps 14-18) in the last section but creates another collection table w. a cache of the new tuple will saved in the table w. the pseudo code of the algorithm is given below:"
"g represents the original equivalent group, when the clustering algorithm begin, the equivalent group just selects a random tuple from the data table. the tuple t is another new tuple in the data set. the purpose of this algorithm is to find a tuple t to join the equivalent group g and thus a new equivalent group g is formed. e(g,g ) represents the entropy increase from the original equivalent group g to the new group g . g just contains the new tuple t. e(g,g ) not equal to 0 ensures the equivalent group g can't always contain the same sensitive attribute in the beginning of clustering. gc represents the average value of the equivalent group g, dis (gc,t) represents the distance between the new tuple t and the average value of the equivalent group g."
"cnns have been also adapted for semantic labeling (semantic segmentation) problems [cit] . these papers show two distinct approaches: in the first case, the model is trained to predict a single class label for each region (patch, superpixel or object proposal). the output is usually a vector of scores or probabilities for each class, based on the appearance of an entire region. in the second case, the network is trained to predict spatial arrangements of labels at pixel-level. these architectures are able to model local structures (e.g. spatial extent, class co-occurrences) across the input space. these upsampling steps are formulated by means of deconvolutions [cit] . in this paper, we adopt this approach and propose a strategy to learn locally dense semantic labeling of patches."
"in this paper, we employ these five layers to build the cnns illustrated in fig. 3 . although the datasets are large and the number of parameters is not prohibitive, training of such models might be difficult. the information content in such data is heavily redundant and most semantic classes are characterized by relatively uniform, while most of the variations in the data lie in the spatial arrangement of such classes. predicting a single label per patch is suboptimal, since the model is not explicitly taking advantage of these regularities while learning, but it is only trained to predict their presence in the (center of) the patch. to learn such spatial arrangement we might need models with larger capacity, since deconvolutions are needed after the bottleneck layer. however, this corresponds to optimizing over a larger number of parameters. in this section, we will review a series of strategies to cope with overfitting. a) dropout: this technique has been proposed to avoid co-adaptation of neurons during training [cit] . co-adaptation would result in filters in the same layer which are interdependent one to each other. therefore, such network would be harder to train and ultimately fitting too tightly training data, without any good generalization guarantee. dropout draws from ensemble learning theory: randomly \"turning off\" a given percentage of neurons (dropout rate hyperparameter) and their connections, corresponds to train a different, less correlated, model at every epoch. at inference time, an approximate ensemble method is obtained activating all the connections. in practice, dropout could slow down training, but benefits largely surpass drawbacks, in particular when the number of parameters to learn is large (e.g. fully connected layers)."
"many approaches can be used to define a suitable bias(s), ranging from the processing of all the information of q prioritizing states with closer features to s, to use part of the state structure that has values similar to s; even being assisted by a hierarchy of sets of these parts. in our work we have defined bias(s) empirically as the averaged contribution of subsets of values of q. this turns out to be computationally simple and leads to the good results shown in section 5. each subset results from removing each input variable from the state space s and averaging the q-values for those states with equal values for the remaining input variables. a precise definition of qbiassr is shown in algorithm 2."
"step 5 defines a callback function, and uses the new tuple as the input to the micro aggregation algorithm we proposed in last section. after the function, the set q w which contains the equivalent groups from the cache table w is formed (if the number of tuples belong to table w less than k, this process will not execute). in steps 6 and 7, it find out the most suitable equivalent group g k which belongs to q. and the equivalent group g w that belongs to q w . in step 8-step 13, if the new tuple is more suitable to join the equivalent group g k than the group g w, it will let the new tuple and the g k combine together as a expansion group which belongs to the set q. on the contrary, if the new tuple is more suitable to join the equivalent group g w than the group g k, the tuples involved need to be deleted from the set q, and the new tuple becomes a member in the g w . after that, g w will be transformed from q w to q. the following examples explain the process above."
"this section uses the idea of the micro aggregation algorithm [cit] and let the distance between tuples, the entropy increase in the equivalent group together as inputs for the clustering algorithm. moreover, two parameters are involved in the process of clustering deciding either the information availability or individual privacy is more important to the data receiver. the core idea of the algorithm is to find a series of equivalent groups using the inputs mentioned above. each group starting with just a tuple, and continuously select the most appropriate record (not only similar to the group but also reach the aim to protect individual sensitive attributes) from the overall data to join the group. the pseudo code of this algorithm is also listed below."
"giraff must learn the task wander-1k in a 1.5x1m hexagonal scenario. up to 20 learning processes of 30 minutes each have been executed for tosl+qbiassr, tosl+sr, and q+sr. the results shown in figure 9 highlight the differences between the evaluated methods."
"in this paper, we explicitly tackle the semantic labeling problem and we show how a modern cnn-based system can be trained for dense semantic labeling tasks in a fully supervised fashion. we are particularly interested in the semantic labeling of ultra-high spatial resolution images, where data contains a tremendous amount of geometrical information, but with a limited number of spectral channels. this is typically the case with off-the-shelf uav and most aerial imaging systems. we introduce a patch-based deconvolution network to first encode land-cover representations in a rough spatial map (that we name bottleneck) and then to upsample them back to the original input patch size. the modeling power of this downsampling-then-upsampling architecture relies on the fact that global spatial relationships can be modeled directly by learning locally, on a coarser spatial signal (downsampling part). the upsampling will then take into account local spatial structuring for each class, while extracting nonlinear representations at the same time. training the network patch-wise allows us to deal with images of any size, by decomposing the problem into sub-regions with representative spatial support. the structure of the network is given in fig. 3 . to train it, we employ standard stochastic gradient descent with momentum, on batches of training patches sampled from the training images. at inference time, we again take a step away from standard approaches that must crop large images into densely overlapping blocks (i.e. with a small stride) or rely on object proposals / regions) to maximally preserve the spatial resolution of predictions. in our system, the whole image can be directly fed to the trained network to obtain a posterior probability map of semantic labels without loss in resolution. doing the same with standard patch-based cnns would show a drastic loss in spatial resolution."
"in this paper, all the processes like clustering, updating, data saving or some other mentioned above are all carried out by the data owner. for example, the data owner maybe a data system of the hospital or wearable equipment's manufacturer [cit] . when some authorized institutions need to obtain the data saved in the data owner for the purpose of outsourcing calculation, data mining, analyzing or forecasting, they will sent a request to the data owner to get the anonymous data. the data owner will check the authority of the receiver at first, and add the laplace noise to the sensitive attribute percentage value from each equivalent group in the anonymous data. the availability of data will not be greatly reduced by adding the noise, because the noise added to each sensitive attribute distribution's percentage value from each group with different parameter based on some rules. in the previous section, the strength of privacy protection is just making two tuples indistinguishable when the data table is updating. the method presented in this chapter will greatly improve this situation."
"2) qualitative results: in fig. 4(6-9) we plot examples of semantic labelings on subsets of the validation images. due to the higher spatial resolution (5cm instead of 9cm for the vaihingen dataset) the land-cover classes are represented by a more variate appearance, in both color and size. for instance, in clips 7 and 8 the thin fences and wall correspond to class \"clutter\" and only sp-msf and cnn-fps are able to detect it, while only the latter scoring the correct class most of the time. we acknowledge the good performance of the sp-msf, but we also note that ambiguous appearance of superpixels cannot be solved. cars and buildings are generally segmented correctly by all the cnns, but only the cnn-fps is able to segment them in a geometrically accurate manner, e.g. single cars segmented as whole objects and not as multiple parts or undersegmented ones (cnn-pc and cnn-spl). again, this beneficial effect stems form learned deconvolutions, to upsample to full image resolution. we also note that for this dataset, the class \"clutter\" and \"trees\" are hard to model. the former situation is mostly due to the very variate and mixed nature of it. for the latter, the fact that some trees do not have leaves makes the modeling of their actual extent hard, or even hard to detect when they stand on grass (see e.g. clips 6 and 7). cnn-based system are able to perform well, but again sp-msf misses the difficult instances."
"consider the following situations, any equivalent group with an entropy of 0 must lead to the disclosure of privacy, because there is no uncertain information for the attacker who wants to deduce the personal privacy in the result set. on the contrary, the biggest entropy increase makes sure that this tuple is the best one to join the equivalent group starting for the purpose of maintaining the best privacy protection in the result:"
"we test the proposed system on the vaihingen and potsdam sub-decimeter resolution datasets, involving semantic labeling of aerial images of 9cm and 5cm resolution, respectively. these datasets are composed by many large and fully annotated tiles allowing an unbiased evaluation of models making use of spatial information. we do so by comparing two standard cnn architectures to the proposed one: standard patch classification, prediction of local label patches by employing only convolutions and full patch labeling by employing deconvolutions. all the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. the proposed full patch labeling cnn outperforms these models by a large margin, also showing a very appealing inference time."
"most rl publications, including those based on lowrealistic simulated robots (see section 2), show their advantages by measuring the eventual convergence to the optimal solution to the task at hand, or the level of performance, often based on the average obtained reward, after a relatively large number of steps, episodes, and repetitions of the learning process. unfortunately, direct application of most of these methods in real scenarios would require days or months of learning to achieve a fair performance, even in relatively simple tasks such as 2d navigation in small environments. consequently, other techniques must be added when dealing with rl problems in robotics to make them tractable, such as those based on a clever use of approximate representations, prior knowledge, and models. these methods, in turn, need to be carefully selected and hand-tuned for each specific task to learn, even for very closely related ones. all of this makes rl not applicable to robotics out of the box yet [cit] )."
"cnns have become extremely successful in many modern, high-level, computer vision tasks, ranging from image classification to object detection, depth estimation and semantic labeling. first examples of deep cnn architectures have been proposed for image classification problems. [cit] 1, where cnns significantly outperformed the state-of-the-art systems based on handcrafted appearance descriptors [cit] . notable extensions allowing to train deeper cnn (i.e. adding trainable layers and thus increasing the capacity of the model) [cit] were the introduction of drop-out [cit], batch normalization [cit] and other strategies allowing better propagation of gradients, such as rectified linear units (relu) nonlinearities [cit] . together with the (very) large annotated training datasets and powerful gpu making it possible to train such models, these intuitions made cnns the gold standard for image classification problems."
"firstly we have carried out a set of experiments to find a learning method among the existing rl techniques that has good properties when confronted with different tasks. the rl-robot software framework, that will be described in depth in section 4, has been used together with the very realistic v-rep robotic simulator [cit] to conduct these comparative experiments. the methods shown in table 1 have been the ones evaluated for different robotic tasks, being the resulting average reward per step used to measure the performance of the learning processes for all of them. the setups and experiments of this preliminary stage will be described in more detail in sections 4 and 5 along with the rest of experiments."
"when a new tuple join to a group, the signature changes will expose personal privacy information [cit] . the core idea of anonymous privacy protection is to ensure that the personal sensitive information cannot be distinguishable from other tuples. if the attacker can't distinguish two tuples, the basic way of privacy protection is realized [cit] . in this paragraph, a real-time monitoring process that detecting the changes from each group is defined. it will creating a forged tuple into a group while some condition is satisfied. the description of this process as follows:"
"the success of q-learning (in pseudocode in algorithm 1), as well as of most rl methods, depends on the accurate choice of the parameters α and γ, along with a set of suitable rewards r(s, a, s ), that define the task to learn, and an action selection strategy. the latter refers us to the exploitation-exploration dilemma, which consists in deciding whether the agent should exploit its current learned policy or explore other actions at each learning step."
"this work was supported in part by the swiss national science foundation, via the grant 150593 \"multimodal machine learning for remote sensing information fusion\" (http://p3.snf.ch/project-150593)."
"summing up, we propose to train a cnn for semantic labeling tasks by employing a dowsampling-then-upsampling architecture. we first review main blocks of cnn architectures (summarized in sec. ii), which allows us to carefully review differences between the cnn we put forward and more standard strategies in sec. iii, where we also provide important details to set up such systems. we test our intuitions on two challenging aerial images dataset, the recently released vaihingen and potsdam semantic labeling challenges (presented in sec. iv). here, we also show how the proposed method compares to standard cnn approaches and we discuss its strengths with respect to the state-of-the-art. in sec. vi we conclude the paper by summarizing main contributions."
"after the conversion, the distance of nominal attribute between two tuples x and y is defined as above. when the nominal attribute has n values, the above polynomial will have n items. after those analyses, the final distance between two tuples will be calculated, and the distance value will be the input for the micro aggregation in following steps deciding which tuple belongs to an equivalent group."
"step 2 defines an empty set q which will save equivalent groups. the set q will be the result after clustering. in step 3, t indicates the total data. when the number of tuple in t is more than k, the loop continues."
"the results of the learning processes in markovian sample models have been averaged from 1000 repetitions for wandering and 3d arm motion, and from 200 repetitions for 2d mobile navigation. figure 5 shows the learning curves for a set of sample-modeled tests. experiments involving more than 2k states where not performed due to memory limitations when creating the reward function from datasets with variable rewards. the results show that tosl+qbiassr only outperformed tosl+sr in two of the four tasks. different sample models extracted from v-rep were tested with unexpected diverse results. this led us to focus on realistic v-rep simulations instead of the sample-modeled ones."
"this section proposes a clustering algorithm based on micro aggregation [cit] . it solves the problem of choosing suitable equivalent groups with flexible balance between personal privacy and data availability. at the same time, this method considers the distance value between tuples and the concept of information entropy together as a criterion in the process of clustering. it is different from the l-diversity or p-sensitive theories mentioned above. this algorithm does not choose the parameter l or p as a criterion for clustering which may cause some unnecessary information loss. at the end of the section, the pseudo code is given and the rationality of the algorithm is analyzed."
"finally, we plan to integrate the rl-robot framework with the openai gym toolkit, where tosl+qbiassr and the upcoming changes can be evaluated and compared for a wide variety of tasks."
"b) batch normalization: batch normalization [cit] aims at speeding up training by allowing the use of larger learning rates and mini-batches. to do so, it learns the normalization for each batch so that the activations passed to the next layer follow a normal distribution with n (0, 1). this might seem trivial, but it avoids problems related to the drift of activation distributions during training and makes the whole system less sensible to layer initialization. moreover, by keeping values normalized at each layer, difference in the randomly selected mini-batches and the activations they generate should have less influence on the weight updates across iterations. each layer can now focus on the general improvement rather than learning to adapt to the previous updates. we included this layer right after every convolutional and deconvolutional layer."
step 20 makes the quasi-identifier attributes of each groups in set q all replaced by the centroid that will finally complete the anonymization of the data.
"based on the micro-aggregation algorithm mentioned in the previous section, the steps 14-18 illustrate how the remaining tuples in t to choose which equivalent group g k is the best choose to join. if we just simply let the new tuples (adding to t) use the previous way to cluster, the equivalent groups in set q will be more and lager as the adding tuples increased."
"a ros node is automatically launched only when a real robot is required. the node is subscribed to odom and laser scan topics, and publishes to cmd vel topic. modules v-rep sim and ros node can be extended with minimal modifications to connect rl-robot with other robots, as well as other environments implemented in python. three sets of tasks are included in the framework; they have been used for the experiments of section 5: (i) wandering (non-episodic): the robot learns to wander avoiding obstacles. the input variables are a reduced set of laser measures around the robot, each distance being discretized within a range. the two output variables are the speeds of the wheels. positive rewards are received if the robot advances above a distance threshold and negative ones if it collides (where frontal collisions yield a highly negative reward)."
"coupling elevation and spectral information eases the detection of buildings and trees (the elevated classes). such information obviously helps in discriminating ambiguous occurrences of the class building, e.g. rooftops showing the same appearance as roads. rooftops covered in dense vegetation are often misclassified as trees and never as grass, since being on two different elevation levels. in fig. 5 the full segmentation of image tile 34 is given. what observed for the clips above summarizes the classification of the entire tile."
"training: cnn-fpl is trained in the same way as architectures described above. however, since it involves the upsampling layers, the number of learnable parameters is significantly higher than for the two previous networks. we warm start the system by employing weights from the cnn-pc, for the common downsampling blocks. the learning rates are initialized at 10 −3 and kept uniform for 100 epochs, then decreased to 5·10 −4 and 10 −4 for 100 epochs each. a last 300 epochs with 10 −5 are performed to fine tune weights. weight decay is the same as for the other two models, but we use a smaller mini-batch size of 32 examples. since during training the cnn-fpl sees less patches, we trained it for an additional 100 epochs. inference: at inference time, the proposed cnn-fpl approach produces a segmentation of same size as the input patch by a single forward pass of the image. this way, the three main drawbacks of the aforementioned systems are circumnvented: first, the whole image can be feedforwarded to the trained cnn and we can directly obtain a dense labeling (stride is equal to 1 by construction) or a dense class-conditional score map; second, we do not need a second step of prediction upsampling; and third; the prediction performed this way is locally structured, as the cnn is able to exploit both color and semantic correlations represented in the input patch, without the need of a subsequent structured output post-processing."
"the specific robot that has been used for our v-rep simulations is a pioneer 3-dx mobile base with 8 laser pointers and a widowx arm (figure 2). these models have been employed for the simulated experiments of section 5. since communications with the simulator are via tcp/ip socket using the v-rep remote api, some additions were needed to avoid the influence of network delays and to guarantee the reproducibility of the learning processes in different machines; they include threaded rendering, streaming sensory values, and a limited speed-up ratio of 3 (minimum: intel core i3-3110m processor). physical parameters and scenarios were also tuned to ensure reproducibility."
"validation performance shows the relation between the output and the target. maximum validation shows the perfect training of the target. sensitivity is the ratio of true positive value which are correctly measured by the experimental test, specificity is the true negative value measured by the experimental test. accuracy shows that diagnostic test is closer to the true value. this proposed method shows that sensitivity is 98.5 %, specificity is 97.2 % and accuracy is 99.2% shown in table 3 that is better than existing methods. table 4 shows the comparison with the literature survey given in paper this method obtain high accuracy."
each signature is normalized via spatial and temporal alignment and scaling. these normalized signatures define a spatio temporal sheet over the entire image sequence. gait recognition is done by matching these sheets for the model gait and input gait. b. segmentation
". (12) …… (13) input layer hidden layer output layer properties of feature extraction i.e. mean, standard deviation, variance, entropy, connectivity and number of the objects are obtained. central tendency value is set if mean, std. and variance value is less or more shows the types of the tumor. if value is less than central tendency that's shows that primary tumor other case secondary tumor. connectivity shows the image type as 2-d or 3-d. number of objects shows the degree of tumor spread in the part of the body. larger the number of objects means presence of more number of tumors in that part. after feature extraction, area of the tumorous region is calculated from which size of the tumor is analyzed. gradient descent is multiplied with negative descent that shows changes occur in biases and weights. the algorithm becomes stable if the learning rate is small."
here we first computes the normalized cross-correlation of matrices template and a. the matrix a must be larger than the matrix template for the normalization to be meaningful. the values of template cannot all be the same.
"the following figures are the input image and their conversions. the classification step is based on the use of artificial neural networks. classification is based on the k-nearest neighbor rule, whereby a test pattern (ssu) is assigned to the majority label of the k closest patterns to it in the training set where distances are measured in some space in which the patterns are represented. in the template matching approach, distances are measured in the raw ssu-space as the euclidean distance between the two ssu's. furthermore, to account for phase alignment errors, we shift one of the ssu's in both directions over a small radius and determine the smallest distance over all possible shifts. in the linear feature extraction approach, we compute the euclidean distance in a reduced feature space of the original ssu-space."
"an input video sequence undergoes a sequence of processing steps to measure (i.e. estimate) the four gait variables of interest, and subsequently use them for classification/ recognition. in this method, an input video sequence is processed by three main modules, as shown in figure 7 . the preprocessing module tracks the walking person in each frame, extracts the corresponding binary silhouette, and computes certain of its shape properties. once the person has been tracked for a sufficient number of frames, the feature measurement module estimates the height and stride parameters from binary silhouettes. finally, the classification module determines the person's identity via standard pattern classification, namely the k-nearest neighbor rule, in the 4-d feature space of these four gait parameters. fig. 7, the method essentially assumes the structure of a general statistical pattern classifier. an image sequence of a walking person (i.e. the input pattern) is processed sequentially and use the same method of 1.3.1 to segment and track the moving person from the background, to obtain a sequence of binary silhouettes. then, for each binary silhouette, we compute its bounding box and locate the mid-feet point (i.e. point half-way between the two feet). the width and height of the bounding box is used to compute the gait period while the mid-feet point is used both in estimating the 3d position of the person on the ground and its apparent height ."
"obviously, we need to compute the frequency and phase of gait in order to extract the ssu's. however, there is no simple/straightforward way to estimate the phase of gait based on the width or height of the person in the image sequence. for example, the maxima/ minima of these series correspond to different gait poses depending on the person and the camera viewpoint."
"mri provides high qualities of images and visualizes structure of the body internally. different types of tissues in the body can be distinguished completely with mri and also contains fine information for treatment [cit] . texture of mri contains information of size, shape, color and brightness that texture properties helps to detect texture extraction [cit] . neural network (nns) consists of an interconnected components, it contains the mimic properties of biological neurons. in (feed-forward backprop) more than one neuron can be simply defined as interconnected components having large inputs activation function and output [cit] . the remainder part of this method is organized as follows. section 2 discusses about related work. section 3 discusses methodology. section 4 observes the experimental results of our methodology more than 200 images from mri dataset and collected data from hospitals."
mri dataset is collected from the harvard medical school architecture and some data are gathered from civil hospital of haryana. one of 220 mri images of normal and abnormal images shown in fig.1 which is trained with this method.
the apparent size of s walking person varies at the frequency of gait. specifically let w(n) and h(n) be the width and height of the nth image of the person.
we use sony digital camera for video input images. and use matlab for software implementation. gait analysis commonly involves the measurement of the movement of the body in space (kinematics) and the forces involved in producing these movements (kinetics). kinematics can be recorded using a variety of systems and methodologies.
the apparent height of a walking person can be modeled as a sinusoidal curve. the persons height in the image h i is estimated as the vertical distance between the head and the feet.
"i. introduction biometric characteristics can be divided in two main classes they are physiological and behavioral. here the first type related to the shape of the body example finger prints, face recognition, hand geometry, iris recognition and gait recognition. second type is related to the behavior of a person like signature, keystroke dynamics and voice recognition. however, it seems unlikely that gait could be used for unique identification in the same way as other conventional biometrics. this is because gait features have poor intra-person reliability due to their dynamic nature, being dependent on various physiological, psychological and external factors (footwear, clothing, surface of walking, mood, illness, fatigue, etc.). thus it can be used for passive identification of people in surveillance applications, unlike most other biometrics. gait recognition is the typical term used in the computer vision community to refer to automatic extraction of visual cues that characterize the motion of a walking person in video for identification purposes. it is possible to detect and measure gait even in low resolution video."
our goal is to find the single most likely value for the parameter vector given the observed data. here we use bayes classifier for that and also new inputs are likely to be close to something the system has already learned means use k nearest neighbor non parametric pattern classifier to evaluate these features.
"ii. formula based gait approach this method extracts four static parameters, namely the body height, torso length, leg length and step length, and uses them for person identification. the first two parameters characterize the oscillation of the person's height, and the other two characterize the stride dimensions, viz. the cadence and stride length. we use the term 'apparent height' to refer to the person's height while walking, which is a time-dependent quantity, and is different from (though related to) their stature, or standing-height, which is a constant quantity. because this height variation is an artifact of the walking movement, it is regarded as a gait variable. these features are estimated as the distances between certain body parts when the feet are maximally apart, i.e. at the double-support phase of walking. hence, they too use stride parameters (step length only) and height-related parameters (stature, leg length and torso length) for identification. however, they consider stride length to be a static gait parameter, while in fact it varies considerably for any one individual over the range of their free-walking speeds."
in each iteration training images are reweighted. evaluation criterion of neural network is to minimize the mse (mean square error). basically used to evaluate the performance of the model.
"where w is the frequency of gait, ø is the phase of gait .m w (n) is the mean width, and a w is the amplitude of oscillation. m h (n) is the mean height."
"the goal here is to build a supervised statistical pattern classifier that uses the four height and stride parameters,µ h,α h,c,and l as the input feature vector to identify or verify a person in a given database .while we do not expect/claim that these features uniquely characterize a person."
for learning function to solve a specific task consider a function f for a class. learning referred to the find a function f* f which helps to solve optimal task. price function is given below. this is a optimum solution.
filtering is done to remove non-brain tissue. haar wavelet transform is used for the pre-processing of image. wavelet coding is suitable for the applications where tolerable degradation and scalability are important. haar wavelet transform decomposes the input signals into a set of the basis function are called wavelets. a prototype wavelet is called mother wavelet other wavelets are obtain this by shifting or dilations called daughter wavelet.
"we use the width of the bounding box of the corresponding blob region it has used to work with our background subtraction algorithm. we estimate period, width series via the autocorrelation method which is known to be robust to non-white noise and non-linear amplitude modulations."
"due to the end of regular increase of processor speed, more systems are being designed to be decentralised to benefit from more of the multi-core feature of contemporary processors. this change in processors poses a number of challenges in the domain of runtime verification where performance is paramount."
"paper organization. the rest of the paper is organised as follows. section 2 introduces some background. sections 3 and 4 recall the orchestration and migration approaches for ltl monitoring, respectively. in section 5, we introduce the setting of choreography-based decentralised monitoring. section 6 reports on our empirical evaluation and comparison of the three approaches using a benchmark implementation. section 7 compares this paper with related work. finally, section 8 concludes and proposes future work."
"here an image sequence of a walking person (i.e. the input pattern) is processed sequentially by three main modules. a pre-processing module that segments and tracks the moving person in each frame, a feature computation/measurement module which computes the self-similarity plot and extracts normalized features from it, and a pattern classification preprocessing can often reduce noise and enhance the signal. given a sequence of images obtained from the device we detect and track the moving person, and compute the corresponding sequence of motions regions in each frame. first extract four silhouette signatures of a moving person two corresponds to inner edges and other two corresponds to outer edges of each leg."
"a biometric system provide three functions, they are verification, identification and screening. verification is somebody claims to be a person whose biometric information are already known then extract new biometric information from the person and check if those matching with the ones we have. identification is comparing them with our database. it is a much more difficult task than verification. screening need to check if the person belongs to a group have chosen."
"each signature is normalized via spatial and temporal alignment and scaling. these normalized signatures define a spatial temporal sheet over the entire image sequence. gait consists of repeated steps, and suppose each step takes p frames then we can divide the image sequence into contiguous segments of length p frames."
"given a sequence of images obtained from a static camera, we detect and track the moving person, and compute the corresponding sequence of motion regions (or blobs) in each frame. using background subtraction the detection of unusual motions can be achieved by building a representation of the scene and comparing new frames with this representation. first extract four silhouette signatures of a moving person, two corresponds to inner edges and other two corresponds to outer edges of each leg."
"the proposed method takes the input mri images that will undergo grey image conversion, template creation, computation of correlation undergoes tumor location detection. brain tumor segmentation and training. the proposed method takes the input mri images that will undergo grey image conversion, template creation, computation of correlation undergoes tumor location detection. brain tumor segmentation and training. the proposed method framework is discussed in fig.2 ."
specifically let w(n) and h(n) be the width and height of the nth image of the person. according to gait analysis literature w(n) and h(n) can be approximated as sinusoidal functions.
"for training traingd is the training function that updates the weight and bias value according to gradient descent. for every slow iteration algorithm training status displayed. if training status shown by nan then it means that training status will be never displayed the performance function drops below the goal when number of iteration go high to the epochs. some training parameters associated with the neural network are epoch, iteration, goal, time, max_fall, max_fail."
"validation, training and testing are used to analyze the performance of the neural network. learngd is the adaption learning and traingd is the training function for multi layer perceptrons in the network. for learning patterns of data in neural network training set, evaluate the generalized ability of trained network testing set and validation set for checking the performance."
"to find out the segmentation errors the rectangular region for each blob is enlarged by a small radius. motion segmentation is achieved background subtraction technique that is quite robust to lighting changes. once detected, foreground objects are tracked in subsequent frames by simple spatial coherence, namely based on the overlap of blob bounding boxes in any two consecutive frames. here the detection of unusual motions can be achieved by building a representation of the scene background and comparing new frames with this representation. to determine whether a tracked foreground blob corresponds to a moving person, we first compute the gait period (and hence cadence) via periodicity analysis of the width and height of the blob's bounding box. we then verify whether the computed cadence falls within some broad range of normal human cadences. we typically use the range [cit] steps/minute, which corresponds to two standard deviations around the average cadence."
"in this paper, we study these questions in the context of monitors synthesized from ltl specifications by considering three approaches, namely orchestration, migration, and choreography, to organise monitors (using terminology from is the setting where a single node carries out all the monitoring processing whilst retrieving information from the rest of the nodes. (ii) migration is the setting where the monitoring entity transports itself across the network, evolving as it goes along -doing away with the need to transfer lower level (finer-grained) information. (iii) choreography is the setting where monitors are organised into a network and a protocol is used to enable cooperation between monitors."
"and also we presented a formula based parametric approach for human identification from video using height and stride parameters of walking gait. it achieves its accuracy by exploiting the periodic nature of human walking, and computing the gait features over many steps. significant improvement in identification performance is observed when both height and stride parameters are used than when stride parameters only are used. the method is view invariant, works with low-resolution video, and is robust to changes in lighting, clothing, and tracking errors and also we presented a formula based parametric approach for human identification from video using height and stride parameters of walking gait. it achieves its accuracy by exploiting the periodic nature of human walking, and computing the gait features over many steps. significant improvement in identification performance is observed when both height and stride parameters are used than when stride parameters only are used. the method is view invariant, works with low-resolution video, and is robust to changes in lighting, clothing, and tracking errors iv. directions for future work develop more efficient methods to compute the self-similarity plot as well as to extract classification features. and we extend this paper to determine whether the walking person has carrying an object or not. this is also related to our second formula based gait recognition."
"here we describe a gait recognition technique that takes a correspondence free holistic approach by computing the classification feature directly as a function of the spatiotemporal volume (xyt) of the walking person's image sequence. specifically, we first compute the self-similarity plot (ssp) as the matrix of self-similarities between each pair of images of the person in the sequence. by properly aligning and scaling the ssp, we then extract normalized feature vectors that we use as input to a standard pattern classifier for gait recognition. intuitively, the ssp encodes the properties and temporal variations of the person's silhouette shape during the walking, and thus can be regarded as a signature of gait. furthermore, we contend that the ssp is a projection of the planar gait dynamics, provided that the person is located sufficiently far from the camera."
"fft(x[],dim)or fft(x,n,dim) applies the fft operation across the dimension dim. for length n input vector x, the dft is a length n vector x, with elements n."
"we described a novel image based gait recognition approach that uses image self-similarity as the basic feature for classification. the method is correspondence-free and works well with low-resolution video and is robust to variation in clothing, lighting, and to segmentation and tracking errors."
"this step is very important it need to choose which features to extract and how. given a sequence of a person's n templates (i.e. cropped images) of the walking person, we compute the nxn of their pair wise correlations, denoted the self-similarity plot (ssp), which we shall use as the basic feature for gait recognition. self similarity plot s(i,j) is computed as the absolute correlation of each pair of templates li and lj minimized over a small search radius r namely."
"the first method takes a image based gait recognition by computing a holistic signature of gait directly from the spatiotemporal volume of the image sequence. the method is invariant only to small variations in cadence, camera viewpoint and depth."
"brain is an organ that controls activities of all the part of the body. growth of abnormal cells of brain leads to brain tumor. diagnosis of brain tumor is very important now-adays. tumor basically refers to uncontrolled multiplication of cells. a cell rapidly divided from a micro calcification, lump, distortion referred to a tumor. metastasis is a process in which tumor occurring cells moves the other part of the body and tumors begin from that regular tissue reinstate. meningioma and glioma are the types of brain tumor. brain tumor is more curable and treatable if detected at early stage; it can increase the intracranial pressure which can spoil the brain permanently. brain tumor symptoms depend upon the size of tumor, location and its type. detection of tumor can be done by mri and ct scan. brain angiogram procedure can be applied in which blood vessels are illuminated in the brain and feed blood the tumor part. procedure of biopsy is also including tissues or sample of cells are taken from the brain at the time of surgical treatment, this will help to predict the benign of cancerous brain tumor. sometimes cancer diagnosis can be delayed or missed because of some symptoms. the principle aim of this paper is to analyze the best segmented method and classify them for a better performance."
"locating the mid-feet point in the image is done by using the midpoint of the lower edge of the silhouette's bounding box, and finds the local minimum of a horizontal projection s(x) of the binary silhouette. specifically, for each x along the width of the bounding box, we define s(x) as the vertical distance at that x between the bottom edge of the bounding box and the lowest silhouette point contained in the lower half. once a person has been tracked for n consecutive frames, a sequence of n templates is then computed. specifically, given the person's blob in each frame, we extract the (rectangular) region enclosed within the bounding box of the blob from either (1) the original color/grayscale image, (2) the foreground image, or (3) the binary image. thus, three different types of template sequences can in fact be obtained for each person. clearly, there are competing tradeoffs to using either type of template in measuring image similarity. (when computing the ssp)."
1) fuzzy k-nn classification: keller [cit] extended the k-nn [cit] algorithm using the fuzzy concept. the theory of fuzzy set and fuzzy membership functions are introduced in knn algorithm to implement fuzzy k-nn classification algorithm. fuzzy k-nn algorithm assigns class membership to a pattern rather than assigning the pattern to a particular class. the membership values for the pattern should provide a level of assurance to accompany the resultant classification. the basis of the algorithm is to assign membership as a function of the pattern distance from its k-nearest neighbors and those neighbors memberships in the possible classes. fuzzy k-nn algorithm first finds the distance from unknown vector to all classes using euclidian distance as given in the equation 4. this equation finds the distance between q and p.
"the proposed automatic image annotation system can be improved by extracting features from a particular region in the image instead of extracting features from the whole image. segmentation before extracting the features can be applied to improve the performance of the annotation system. this help us to assign multi labels to an image. another modification can be done using any other classification method, which performs well than fuzzy k-nn classifier."
"2) image annotation: test images are classified using fuzzy-knn algorithm and we use this classification result to annotate an image. annotation database consists of class numbers and corresponding class names. according to the class numbers assigned by the fuzzy k-nn algorithm, class names are retrieved from annotation database and displayed on the image."
"the proposed framework is broadly divided into two phases. they are training phase and annotation phase. training phase contains 3 stages. first step in training phase is surf [cit] feature extraction from training images, which is used to extract the features from it. clustering the training images is done by k-means [cit] clustering in step 2, which clustered the training images in to k number of clusters. suitable labels are assigned to each clusters depend upon the general behaviour of the clusters. these extracted features along with their cluster label is used to generate a the model for training. this generated model is used to annotate the test images in the annotation phase surf feature extraction from test images is the first step in annotation phase. in second step classify each test images using fuzzy k-nn algorithm based on the model created in the training phase. annotation of the test images is done in third step using the class label assigned by the fuzzy k-nn algorithm. cluster names corresponding to cluster labels are retrieved form the annotation database and displayed on the image. the architecture of the proposed automatic image annotation system is described in detail below."
"where x ij is the j th point in the i th cluster, µ i is the mean vector of i th cluster and n j is the number of patterns in the i th cluster."
"content based image retrieval (cbir) [cit] s to organize and search these images efficiently to overcome the difficulties of traditional image retrieval methods by matching the low level features of images [cit] . user queries are used to retrieve images in traditional cbir systems. in this study, an aia system is proposed which assigns suitable key words to digital images depending on the information containing it. automatic image annotation can be extended to an image retrieval system called annotation based image retrieval(abir)."
"the performance of an automatic image annotation system can be measured using the standard statistical measures like precision, recall, f-score and accuracy [cit] . these parameters can be calculated using the standard measures true positive (tp), false positive (fp), false negative (fn) and true negative (tn). the performance matrix of the proposed system is illustrated in table ii. average values of precision, recall, f-score and accuracy of the system were obtained as 0.85, 0.84, 0.84, and 0.96, respectively."
"in this work, the problem of automatic image annotation system is investigated through surf feature extraction algorithm combined with well known algorithms like k-means clustering and fuzzy k-nn classifier. from the performance evaluation, we can conclude that this system shows an accuracy of 0.96."
features are extracted from test images using surf feature extraction method. the process of feature extraction is the same as the feature extraction method described in the training phase. test image features are extracted and converted in to a row vector using the same techniques described in the training phase.
"the rest of this paper is structured as follows. in section 2, related work is reviewed. the proposed system framework and each component in detail is discussed in section 3. the result and performance analysis is described in section 4. finally, conclusions and future works are discussed in section 5."
"iv. result and analysis experiment is conducted with standard datasets caltech 101 dataset [cit] and corel 1000 data set [cit] .the data set contains 10 classes and these classes are taken from caltech 101 dataset [cit] and corel 1000 data set [cit] . the images are clustered into 10 classes using k-means clustering algorithm. the data set is a combination of small objects and sceneries. a total number of 300 images for training and 500 images for testing are used for the experimental study. airplane 48 0 0 2 0 0 0 0 0 0 0 building 0 37 4 0 2 1 2 2 0 1 1 headphone 1 0 39 0 0 2 3 2 1 0 2 car 0 1 0 49 0 0 0 0 0 0 0 sunflower 0 1 0 0 48 0 0 0 1 0 0 mountain 0 5 0 0 2 38 0 3 1 1 0 butterfly 0 0 0 0 0 0 38 3 0 8 table i describes the experimental result of our automatic image annotation system. from table i we found that most of the test images are annotated accurately. the images are annotated based on the classification label assigned by the fuzzy k-nn algorithm. during the experimental study, we have seen that some test images in the remains unclassified due to the lower strength of membership value assigned by the membership function used in the fuzzy k-nn classification step."
"a. training phase 1) surf feature extraction: feature extraction is the first and important stage of any classification and annotation problem. the proposed architecture uses speeded up robust feature extraction method (surf) to extract the features of both training and testing images. surf extraction method is a scale and rotation invariant feature extraction method, which is faster than widely used feature extracting method scale invariant feature transform (sift) [cit] . surf is used in this proposed automatic image annotation system due to this higher performance over sift [cit] . surf focuses on scale and in-plane rotation invariant detectors and descriptors of an image. integral image is calculated from the image and calculate sum of pixel intensities in the integral image [cit] using the equation 1"
"3) model data generation: the feature vectors corresponding to the n number of images are extracted using surf feature extraction, and k number of clusters. n number of images are clustered in to k clusters using k-means algorithm. a model for image annotation system is created using this image feature vectors and corresponding clusters. this image annotation model is used to classify the test images with the help of fuzzy k-nn classification algorithm in annotation phase."
"the capex (depicted in equation 4 ) consists of the investment costs of all technologies w and the hen cost estimation calculated using equation 3. the hen area is estimated as suggested by townsend and linnhoff [cit] based on vertical intervals in the composite curves. hen design based on mathematical principles, as well as optimization of the minimum approach temperature (δt min ) was not performed in this work, but could be added to the solution strategy."
"t models the change in inertial reference frame, c p is the trajectory of the cop, γ is the ground slope angle, and l f is the foot length. the vector t groups the coriolis/centrifugal terms and potential forces for brevity. the ground clearance of the swing heel is denoted by h(q), and θ denotes the swing heel ground-strike impact map derived based on [cit] ."
"gas-cooling to the compressor inlet temperature from the superheated vapor at the compressor outlet can be achieved by mixing in the presaturator or in a heat exchanger as described in equation 21 .q g, gas-cool,"
"at the master level, a black box optimization is performed where the variables present in the nonlinear constraints are the decision variables. these are mainly the heat pump saturation temperature levels (t i ), the fluid (d), the subcooling, gas cooling, and compressor preheating temperature differences (∆t i,sc, ∆t i,dsh, ∆t i,p re ), and all thermodynamic properties derived from these. an additional variable is introduced in order to vary the weight (ξ) of the two components in the objective function at the slave level (see section 3.3.2 -objective function). the properties are retrieved from the open-source database coolprop [cit] . the variables and objective functions at the master level are found in table 3 . black-box multi-objective nonlinear optimization is performed by dakota [cit] using a multi-objective genetic algorithm (moga) [cit] which allows analysis of a wide solution space. the specifications used are presented in appendix a.2."
"this study has presented a mathematical approach for optimal design of industrial heat pumps spanning a wide variable solution space. the method provides a framework for deriving utility targets, including optimal heat pump component sizes and operating conditions. this provides a basis for detailed system design in a subsequent step to account for dynamics and off-design operation. the novel superstructure-based synthesis method is embedded in a computational framework and is solved in a decomposition approach. a comprehensive list of heat pump features are taken into account while technical limitations are considered and a set of solutions is provided which allows for expert-based decision making and further in-depth analysis of the solutions. for benchmarking, the method was compared to a set of literature cases generating between 5 and 30% cost improvements to the optimal solutions reported. an extended version of one case is presented considering fluid selection, hen cost estimations, and technical constraints within the problem formulation. the extended case highlights a trade-off between energy efficiency and system complexity expressed by the increase of heat exchanger network costs with the number of compression stages, level of gas-cooling and subcooling which all improve the cop. this is especially evident when comparing the solutions with 3 and 5 compression stages causing an increase of the cop from 2.9 to 3.1 at 3% increase in tac. fluid selection was successfully performed indicating that propane is the most favorable fluid both in economic and thermodynamic terms in this temperature range. the hps proves to be flexible for different requirements serving in a variety of cases. it has to be noted that a comprehensive analysis of an industrial process should always comprehend optimization of the entire utility network, including the hot utilities. this was neglected in this work to be in accordance with the literature input data. in subsequent analysis, the trade-off with other utility technologies should be considered. future work should also include supercritical cycles, refined fluid and component selection strategies, as well as consideration of off-design performance in multi-time problems."
"this paper enhances the usefulness of passivity-based control for biped walkers and the stabilization of their limit cycles by generalizing both the expression used for the system energy and the control method to arbitrary degrees of underactuation. this underactuation can be enforced by physical properties of a system model such as rolling foot contact, or a result of actuator placement in the system. we show that with these changes the system still enjoys improved properties such as an increase in the basin of attraction, robustness to changes in slope, and increases in convergence rate. the immediate goals of the authors are to perform similar simulations with a variety of more complex inner loop controllers that are commonly applied on legged robots (e.g., feedback linearization [cit] or hzd [cit] ). in addition, the ideas presented could have significant impact in the application on powered prostheses. passivity-based methods are speculated to have good properties for humanmachine interaction [cit], and this paper specifically addresses the issues of underactuation and saturation that powered prosthetic devices inevitably face."
"typically, the basins of attraction of passive biped limit cycles are quite small and sensitive to perturbations. the initial positions and velocities of the system must be close to the limit cycle, which can be difficult for a human to manually achieve by positioning and pushing a physical biped [cit] . however, the basin of attraction can be significantly enhanced by the addition of the pbc in the outer loop."
"1. conceptual methods provide important insight to problems but cannot assess solution optimality. the advantage is, however, that technical infeasibilities and practical constraints can be considered without facing computational problems. 2. mathematical methods may experience convergence issues for large scale problems due to increasing complexity. therefore, many studies considered a reduced solution space. 3. few studies provide a combination of conceptual and mathematical approaches, which can harvest the advantages of both methods, e.g. through introduction of technical constraints, or multi-solution generation. 4. the potential impact of heat pumping for industrial waste heat recovery is not clearly communicated. this paper addresses the gaps denoted (2) and (3) by presenting a novel comprehensive superstructure synthesis method which is solved using mathematical programming for optimal integration of industrial heat pump systems. preliminary versions of this heat pump superstructure (hps) [cit] were generalized and extended to incorporate fluid selection, hen [cit] 2016 ahp minlp -contin. 1 tac g [cit] 2016 hp/ahp/he/aht tp hysys 10 k 2 fuel s tp [cit] 2016 hp/ahp/he/aht milp hysys 10 k 2 tac s binary [cit] 2015 hp milp pr fixed 3 power s ( ) flowrates [cit] 2015 hp minlp rp contin. 3 tac s ( ) flowrates [cit] 2014 hp tp -fixed 3 power s tp khan and lee [cit] 2013 hp nlp pr contin. 3 power 4 ( * ) m ( ) composition hackl and harvey [cit] 2013 hp tp simple fixed 1 -g becker [cit] 2012 [cit] 1993 hp/ahp/aht tep generic fixed 1 tac g linnhoff and dhole [cit] 1992 hp tp simple fixed 3 power s swaney [cit] 1989 he/hp lp simple fixed 3 tac s colmenares and seider [cit] 1989 he/hp nlp pr contin. 2 tac s flowrates ranade [cit] 1988 hp ep simple contin. 1 tac g colmenares and seider [cit] 1987 he/hp nlp pr fixed 2 tac s flowrates shelton and grossmann [cit] 1986 hp lp [cit] 1 k 3 tac s shelton and grossmann [cit] 1986 hp milp [cit] 10 k 3 tac s townsend and linnhoff [cit] 1983 he/hp tp tabl. fixed 2 exergy s cheng and mah [cit] 1980 hp hr srk contin. 3 tac s tp barnés and king [cit] 1974 hp hr mip srk contin. 3 tac s cost estimation, technical constraints and a comprehensive list of heat pump features. a multiobjective decomposition solution strategy allows convergence for large problems and provides multiple solutions for expert judgment adapted to the diverse criteria relevant in industry. this strategy addresses the shortcomings of previous work and provides a clear design method based on a comprehensive superstructure and mathematical programming."
"this paper presents a passivity-based controller based on a generalized energy expression in the storage function, which defines a novel passive output that accounts for the energy stored and dissipated by an arbitrary inner-loop controller. it is assumed that this inner-loop controller generates a stable limit cycle for the biped on a given slope. the outer-loop pbc will then increase the basin of attraction, improve the robustness to the ground slope, and increase the rate of convergence back to the stable limit cycle. the control method is also shown to perform with an arbitrary degree of underactuation in the system. the rest of the paper subscribes to the following format: section ii introduces the dynamic model of the biped. section iii offers a brief review of passivity and derive a pbc from an energy based storage function. finally, section iv demonstrates simulation results on a 7-link biped model that utilizes a pd controller in the inner loop to create a stable limit cycle."
"we present phase portraits of the mechanical energy of the biped versus a phase variable that monotonically increases during each step [cit] . this allows meaningful information to be conveyed using a two dimensional graph. the specific phase variable used in these plots is the global hip angle, which is defined from the vertical axis to the vector that connects the stance ankle to the hip. the mechanical energy over our phase variable represents a dimensionality reduction of the phase space onto a 2d plane. fig. 4 shows a comparison of the system behavior with and without the pbc when starting from an initial condition that is significantly distant from the limit cycle. with the pbc, the system converges back to the limit cycle in the left plot; the right plot without pbc does not converge and in fact falls over after just two steps. this comparison demonstrates that the basin of attraction of the limit cycle is increased by the pbc."
"the focus of this work lies on mechanically driven heat pump synthesis methods for industrial processes. since these techniques rely on modeling state-of-the-art heat pump technologies, a short review of available heat pump features was conducted. [cit] and most recently [cit] presented comprehensive literature reviews on advances in mechanically driven (multi-temperature) heat pump systems. the most recurring features relevant for large-scale modeling of industrial heat pumps were identified and are presented in table 1 . these include multi-stage compression and expansion, ejectors, cascaded cycles, gas-cooling, subcooling, economizers, and presaturators. other developments, which impose different system architectures (desiccant cooling [cit] ) or more refined equipment modeling (scroll and oil-free compressors [2, [cit] ) are not discussed in this work. table 2 provides an overview of the studies introducing synthesis methods discussed in this section. in the presented approaches, it is differentiated between conceptual methods which are based on expert judgment, heuristic rules, or graphical analysis; and mathematical methods, which rely of mathematical programming to perform systematic optimization. this work presents a contribution to the latter which is thus discussed at greater length."
heat cascade the milp slave model is subject to heat cascade constraints [cit] which ensure heat transfer feasibility for maximum heat recovery. the set of equations are provided in appendix c.1
"the passive compass-gait biped has no external force input during its continuous dynamics, thus the only work done on the system is by the discrete impacts with the ground. on a passive limit cycle, the kinetic energy of the biped is essentially reset after each impact, while the datum defining the potential energy is shifted to reset the potential energy. this gives rise to a constant generalized system energy [cit] . a similar phenomena exists for an n-link biped with a controller that does work to cause the biped to follow a limit cycle. during the continuous dynamics, the work done by the controller exactly accounts for the change in the mechanical energy. if the work is reset to zero after each impact (which we can enforce by convention), then the generalized system energy is still constant on the limit cycle [cit] ."
objective function the milp problem is solved with commercial solvers based on branch and cut methods for a single objective function. investigation of a wide solution space regarding both objectives from the master level leaves two options for consideration. table 4 total investment cost
"mathematical methods ensure identification of an ideal point with regard the selected objective(s); however, convergence of optimization techniques becomes increasingly difficult with growing problem size. therefore, many studies considered a reduced solution space, such as discretized temperature levels, simplified heat pump cycles, disregarding pa principles, or preselecting working fluids. the studies presenting mathematical methods are analyzed in the following paragraphs based on selected characteristics."
"w set of utility technologies w f w maximum size of technology w y w existence of technology w c w investment cost function of technology w, see appendix c. 2 heat exchanger network"
"all computations were conducted on a machine with 8-core xeon 2.4 ghz processor with 16.0 gb of ram. table 7 depicts the data used in the multi-objective genetic algorithm (moga [cit] ) from the dakota package [cit] . different parameters were used during different runs. due to danger of getting trapped in local minima, especially during the extended analysis (in section 4.3) with fluid selection at the master level, the mutation and crossover parameters were set more aggressively. these parameters were selected based on a heuristic analysis tracking the propagation of the non-dominated frontier. figure 13 shows the propagation of moga and the dominance of each population over the previous indicating that a total of 10 5 evaluations achieve satisfying convergence."
"the selection of the benchmark cases was based on their recurrence in the literature, even though not many cases were treated repeatedly, and diversity in their characteristics. suitable cases could not be identified in the most recent literature due to insufficient data provision in terms of process stream data, cost functions, or detailed heat pump configurations. the selected cases were each treated in several publications."
"temperature levels (a) constraining one objective with a variable controlled from the master level and minimizing the second objective or (b) defining a weighted sum of the two objectives (wc t ac ) where the weighting factor (ξ) is controlled at the master level. since alternative (a) generates more infeasible solutions and therefore leads to longer solution times, option (b) was selected and is expressed in equation 6 ."
"heat pump specific constraints the general heat pump parameters such as the reference heat load of the evaporator and condenser and the reference electricity consumption of the compressors are presented in appendix c.2.1. these enter into the targeting constraints and are sized based on the process thermal requirements minimizing the objective function. enforcing energy and mass conservation within the hps requires additional constraints to be introduced. these are illustrated in further detail in appendix c.2.2. mass and energy conservation are introduced at three different points on each pressure level, namely: the superheated vapor point after compression, the de-superheated or saturated vapor point before compression or condensation, and the saturated or subcooled liquid point after condensation or before evaporation. since these points are fixed at the master level, all equations can be formulated with purely linear dependencies. mass balances at all three points ensure that the working fluid mass flow rate is conserved throughout the heat pump system. energy balance equations ensure that mixing (e.g. of two compressor outlets at the same pressure level) do not violate the energy conservation law. superheated vapor mass and energy balances are introduced to study the effect of sensible heat recovery from the vapor (gas-cooling). this can either be achieved by installation of a separate heat exchanger (gas-cooler) or by accounting for sensible heat release in the condenser unit. both options are separately modeled in this superstructure but does not have a major impact on the heat exchanger network cost estimation (since gas cooling in both cases imposes higher investment). the de-superheating temperature difference (∆t dsh,i ) selected at the master level can be understood as the temperature from which sensible heat release is considered. this temperature difference does not influence the energy balance but by manipulating the inlet temperature of the de-superheating, hen solutions which require stream splitting can be avoided, which has an influence on the heat cascade. if it is set to zero, gas-cooling is neglected and the sensible heat contained in the superheated vapor is considered as if it was available only at saturation temperature levels."
"heat pump features in modeling heat pump features, three approaches addressing different levels of detail were observed in the literature. in the first group (1), heat pump performance was modeled based on general thermodynamic principles. it aids in estimating potentials for improvements reachable with heat pump integration but lacks specification of real fluids or system design. works contained in the second group (2) modeled basic single-stage heat pump cycles based on real fluids assuming that superposition of simple cycles could represent more complex systems. this leads to underestimation of performance and thus sub-optimal solutions could be generated. the third group (3) contains work presenting rigorous heat pump models including technical features from table 1 . most of these included multi-stage compression and pre-saturation, while some additionally considered liquid sub-cooling, preheating before compression, or gas-cooling, and very few authors examined multi-stage expansion including [cit] . no previous work has comprehensively included all identified heat pump features."
"heat pumping has gained increasing attention during the past decades not only for household applications but also for improving energy efficiency of industrial processes through waste heat recovery and valorization at elevated temperatures [cit] . as demonstrated in appendix a.1 (figure 11 ), research in the field of industrial waste heat recovery is largely dominated by organic rankine cycle (orc) applications and thermoelectric devices. this may stem from a fully explored state-of-the-art of industrial heat pumps and integration methods; however, the marginal penetration of industrial heat pump systems (apart from basic refrigeration and airconditioning) [cit] contradicts this notion. the main barriers for broad usage in industry were identified as lack of knowledge and of comprehensive heat pump integration methods to provide improvement potentials [cit] . this work mainly covers single fluid, mechanically driven systems due to their advanced technological development and operative flexibility (see appendix a.1 for more explanation). after a state-of-the-art analysis of current synthesis methods, this work presents a novel heat pump superstructure with a bi-level solution strategy in the methodology section, followed by application of the method to various literature cases in the results and discussion section."
"to ensure reasonable propagation of the moga algorithm during fluid selection at the slave level, a time limit of 600 seconds was imposed for the milp solver. if the limit is reached, the solver returns the best integer solution at that point even if it is above the specified optimality gap. nevertheless, a total of 25 000 moga iterations required higher computational time than 100 000 iterations with master level fluid selection. figure 8b shows the non-dominated frontiers of the different fluids generated over all moga iterations by both fluid selection methods. the results from the slave level selection (after intense computational effort) are dominated by the frontier of the master level selection, and it is thus concluded that if not necessary (e.g. for studying cascaded cycles) this method should be avoided. in the master level selection, propane dominates the other fluids over the entire range of solutions. this outcome was reproduced multiple times by rerunning moga with different seeds. since the moga algorithm is aimed at improving the global non-dominated frontier, the frontier of propane yields a good approximation of the global pareto curve for this case. the minimum tac solution generated a total reduction of approximately 9.5%. this solution consists of three compressors between -33 and 41.5°c with propane as working fluid (see figure 15 )."
"the literature optimal cases were reproduced by adding constraints at the slave level of the hps to force the resulting heat pump layout to contain the same features and operating conditions as those presented in the literature. this was achieved by pre-selecting temperature levels and fixing the active compressor stages and fluids. in this way, the hps flexibility was tested and reference values for later comparison were calculated, given that most literature studies used different thermodynamic property calculations. an analysis of the original and the reproduced data (referred to as reference since they serve for later comparison) is presented in table 5 . the results are compared based on the tac which was the objective function in the literature. it was observed that the results from literature cases e2 and cold tray could be reproduced with a negligible difference (below +/-1%) in each of the categories including the tac. the slight difference in results is explained by different property calculation methods. reproduction of the ethylene case generated around 23% higher electricity consumption which resulted in 22% higher tac. the thermodynamic conditions are quite extreme (very low temperature) and thus advanced property estimations are necessary. the literature case consists of five cascaded heat pump cycles over a wide temperature range. even small underestimation or overestimation of the electricity consumption in the lower cycles is therefore cascaded over the entire range. this, in combination with the property estimation methods, could explain the discrepancy."
"the hps is embedded in the utility targeting problem of maréchal and kalitventzeff [cit] where the optimal utility system for an industrial process is found based on the thermal and material needs considering maximum heat recovery. this means that all elements of the heat pump, namely condensers, evaporators, compressors, presaturators (flash-drums) and gas-coolers, are present as utility technologies in the targeting approach. the main variables at the slave level are the size and existence of each utility technology, including all heat pump elements. the size of each technology is decided by optimization based on the objective function while remaining subject to physical and thermodynamic laws. to ensure mass and energy conservation within the heat pump, additional constraints are added at all saturated liquid, vapor, and superheated vapor points. since the saturation temperature and respective pressure levels as well as the subcooling and superheated properties are set at the master level, component sizing is linearly dependent on the state properties. therefore, the problem can be described as a multi-period mixed integer linear programming (milp) problem which is solved using commercial software (ampl modeling language [cit] ) with cplex [cit] . table 4 depicts the objective function and variables at the slave level. the utility targeting constraints are described in further detail in appendix c.1. the heat pump parameters for utility targeting as well as mass and energy balances are found in appendix c.2."
"variables the variables present at the slave level are the existence (y w p ) and sizing (f w p ) of each of the utility technologies w during each period p and the maximum size considering the entire operating range. based on the objective function and thermodynamic input parameters selected at the master level, optimal sizes and operating conditions of all utilities including the heat pump technologies are derived within the optimization."
"given an industrial process with thermal and material demands and a set of candidate utility technologies, including potential heat pumps, the goal is to find the optimal utility system including the optimal design of the heat pump system. the optimal heat pump layout should encompass specification of technologies, features, working fluid and operating conditions. this method, therefore, aims at providing a utility target and preliminary design of the heat pump system as the basis for detailed design considering dynamic behavior in a subsequent step."
"pinch analysis the principles of pa were considered in most methodologies listed in table 2 . few authors presented algorithms neglecting pa, which reduced the problem size at the cost of disregarding the heat recovery system and its dependencies."
"objective function different objective functions are of interest when optimizing industrial heat pumps such as the exergy efficiency, coefficient of performance (cop), environmental impact, or cost. based on the literature cases considered in the results and discussion section, the objectives of the multi-objective optimization were based on economic criteria, i.e. annualized capital expenses (capex) and yearly operating expenses (opex) as shown in equation 1. [cit] ."
"when the system is solely under the influence of the pd controller, the storage function s and system energy e (mechanical energy minus the work done by the pd controller) remain constant during the continuous dynamics, and are only changed by the discrete impacts as demonstrated in fig. 2 and 3 . these impacts dissipate energy and cause the biped to converge toward the limit cycle. however, as shown in fig. 3, implementing the pbc on top of the pd controller causes the storage function to decrease during the continuous dynamics as well. the convergence appears to be exponential, with different rates for each contact condition. based on the control law derivation and storage function analysis from the previous section, we can conclude that the changes in convergence rate across impact events are due to the instantaneous changes in the norm of the joint velocities (i.e., the bound η changes)."
one of the beneficial properties of pbc is that it is easy and natural to extend these results to the case of an actuator with saturation. if we consider a saturated version of the control
"this section is divided into three parts. in the benchmarking analysis, three case studies presenting optimal heat pump designs for industrial processes from the literature were selected. the literature results were reproduced with the heat pump superstructure (hps) to validate its flexibility. during the optimization study, the multi-objective bi-level approach presented in the methodology is applied to the three literature cases, and compared to the previous solutions from the literature. in an extended analysis, one literature case is expanded to consider fluid"
number of heat pump stages the maximum number of stages of a heat pump cycle consisting of one fluid can be restricted as shown in equation 7 .
the liquid side of the presaturator also needs to be cooled down to the subcooling temperature which is formulated similarly and shown by equation 20 .
"the heat pump system for the extended case e2 was optimized with respect to opex and capex assuming an isentropic compressor efficiency of 70%, a maximum compression ratio of 8 [bar/bar] in each compressor, and hen cost estimation functions from the literature (section 3.3.1, table 3 ). all input data is reported in appendix b.2."
the input parameters used for cplex [cit] are displayed in table 8 . the last three entries were found based on the parameter tuning performed by cplex.
mass and energy balances all material and non-thermal energy requirements are described by a set of constraints. these equality constraints ensure that material/energy consumption and conversion are balanced within the system boundaries or compensated with help of the grid (utilities) which factors into the operating cost.
"in conclusion, the hps can represent a wide range of heat pump features and cycle architectures, while at the same time providing improved solutions to different literature optimal cases. this is attributed to a wider range of heat pump features considered and variable temperature level selection. as demonstrated, the level of complexity, variety of heat pump features considered, and technical constraints added depends on the choice of the user, which makes the superstructure flexible to handle. the solution strategy allows to generate a set of non-dominated solutions which enable the user to perform further analysis, thereby gaining deeper insight to the problem, and to apply other selection criteria. the literature cases discussed in this section serve for benchmarking the underlying approach, however, important criteria are neglected, such as hen costs, compressor isentropic efficiencies, \"real\" utilities, and technical constraints thus generating \"theoretical\" solutions. in the next section 4.3, an extended version of the presented case e2 is, therefore, discussed."
"the mechanical energy of the system is the kinetic energy k plus the potential energy p of the biped, while the work done by the inner loop controller is"
"temperature level selection in the literature, temperature level (used here interchangeably with pressure level) selection was handled in two ways. the primary approach relies on predefined or discretized temperature levels among which compressor units are activated using integer variables. the first comprehensive methodology for optimal industrial heat pump design based on discrete temperature levels was presented by shelton and grossmann [cit] in the form of a mixed integer linear programming (milp) superstructure. many subsequent authors used discretized temperature levels (as depicted in table 2 ), resulting in a diminished solution space and therefore increasing the risk of identifying a sub-optimal solution. fewer authors presented methods with continuous temperature levels as part of the decision variables, which renders the problem structure nonlinear as presented e.g. by colmenares and seider [cit] ."
"(c) a cold tray distillation sequence presented by colmenares and seider [cit] and later studied by swaney [cit] was selected to revisit the original solution which disregarded an obvious improvement of heat pumping across the process pinch (≈ 60°c). figure 3(c) shows the process temperature enthalpy profile indicating the process thermal demands and pinch point. swaney [cit] considered a solution with heat pumping across the process pinch for this case study, however with modified input data (isentropic compressor efficiency of 0.8) and is, therefore, not considered here."
"a flowsheet and a temperature entropy diagram of the novel heat pump superstructure (hps) are depicted in figure 1 . it illustrates the various potential pathways and features considered in the superstructure. some features are represented in sample cycles. the superstructure is equipped with a condenser and evaporator at the highest and lowest pressure levels, respectively. the intermediate levels additionally contain a presaturator, a post-compression gas-cooler, a subcooling heat exchanger, and a superheater. compressors and valves are made available between all pressure levels. superheated vapor exiting the compressor can be de-superheated in a heat exchanger (gas-cooler) and condensed (represented by one heat exchanger), and/or sent to a presaturator which saturates the fluid and separates it into its phases. two-phase flow leaving the expansion valves can be evaporated (and potentially superheated by mixing or heat exchange) and then compressed or mixed with condensate with options of inter-cooling and/or subcooling before expansion."
"ti,sh t i,pre (21) where h sh is the enthalpy to which the superheated compressor outlets are mixed [kj/kg], and t i,sh is the respective temperature [k] ."
"analysis of the most cited publications during the past 10 years related to the key word waste heat recovery, demonstrates a dominance of studies dealing with orc [cit] and thermoelectric devices [cit], while the contributions related to heat pump and refrigeration applications are negligible. figure 11 illustrates the distribution of the most cited publications within the last 10 years with an average citation of more or equal to five per year. this added up to 158 publications. figure 12 -(1b); single-stage, multi fluid (zeotropic mixture) is depicted in figure 12 -(2b); and a single-stage, single fluid inverse brayton heat pump is illustrated in figure 12 -(3b). single fluid (inverse rankine) heat pumps can satisfy constant temperature thermal requirements (single-stage) as well as continuous temperature ranges with help of multistage cycles at a reasonable cop. in generating a temperature glide, zeotropic mixtures [cit] or heat pumps relying on the inverse brayton cycle [cit] may be advantageous for demands spanning wide temperature ranges, but less flexible e.g. with regards to constant temperature requirements. therefore, this work is focused on the first three figures figure 12 -(a-1b)."
"the initial conditions and slope used for the saturated and underactuated simulation were the same as the full actuation case on the nominal slope. the control with ankle actuation alone is still capable of enhancing the basin of attraction of the limit cycle as indicated in fig. 6 . however, one can see that the number of cycles or steps necessary to reach the limit cycle has increased from three steps in the left plot in fig. 4 to five steps in fig. 6 . this indicates that the convergence rate of the storage function has decreased due to the drop in the number of actuators."
"(a) the list of candidate fluids is added to the slave level, in which every unit (condenser, evaporator, compressors, etc.) is reproduced as many times as there are fluids; the fluids are then activated using binary variables connected to the existence of each unit (y w ), or (b) an integer variable referencing the fluid is added to the master level the advantage of selection at the slave level, (a), is that cascaded cycles with different fluids can be designed and convergence of the decomposition strategy is reached after fewer iterations (due to fewer variables at the master level). the disadvantage is that the slave problem size increases proportionally with the number of fluids which impacts the milp resolution time significantly. both options were applied in this study depending on the respective problem statement (which will be indicated)."
link to slave level the utility and heat pump technology sizing is performed at the slave level. the results from the slave optimization serve as input to calculate the master level objective functions. these are the maximum size (f w ) and existence (y w ) of each technology w influencing the equipment investment and heat exchanger network cost estimation.
"where h isentropic (t j ) [kj/kg] is the isentropic enthalpy after compression from saturated vapor at temperature level t i to (saturation) temperature level t j, and η isentropic is the isentropic compressor efficiency [-] . the hps stream properties are depicted in table 12 . t i,pre"
"in this paper, we consider a 2d biped model with torque only in the sagittal plane. for simplicity, we model the link between the two hip joints as a single joint and omit a torso link. thus, together with a foot, shank, and thigh link for each leg we have a 7-link biped model. we model it as a kinematic chain with respect to an inertial reference frame (irf) defined at either the stance heel or stance toe, depending on the phase of the single-support period (to be discussed in section ii-b). a diagram of the biped is shown in fig. 1 ."
"the thermal streams considered in the different benchmark cases are depicted in the original source and were reproduced in table 9 . the objective functions, variables, boundary conditions, and input data to the respective optimization problems are displayed in table 10 . the hen costs were disregarded during this optimization following the literature input data."
"in this section we describe our approach to extend waf security with behavioral analysis. the solution we want to propose increases the security and usability of the application that the waf protects. it reduces the risk of a successful attack, even if your device is infected with malware."
the authors in article [cit] concentrate on sql injection attack and propose the detection mechanism employing graphs and support vector machine. the algorithm converts sql query to the graph and uses previously trained svm to detect sql injection. the drawback of this algorithm is that it focuses only on the detection of tautology which is the first phase of the attack. when the mechanism blocks such query it can be considered as the presence of vulnerability.
the case background is the following. the client has a bank account in the bank which requires that the transactions commissioned on the online service must be confirmed using one time password (otp) generated by rsa token. client's device is infected with malware that is specialized in stealing money from the bank transaction system (eg. zeus).
the function similarity depends on the method to be used to compare actions. the algorithm of calculating the similarity of action is presented in alg. 1. in this state of study we are using simply algorithm based on weighted wage. the values of the similarity function are taken later in the verification (additionalauthrequired) that the action is similar enough to those previously performed that no further verification is needed.
"-client -the mobile or web client of the system, -w af -web application firewall, -server -system endpoint server (reverse proxy), -attacker -an attacker (eg. malware)."
we are going to use the following notation to describe the attack flows. the steps described correspond to the numbers in square brackets on matching figures.
"another type of request forgery attack is server-side request forgery (ssrf) and request forgery generated by malicious software. the ssrf differs from csrf that the attacker forces a vulnerable application server to send a request. in the second type the attacker installs malicious software of victim's device which later sniffs the authentication data (eg. smses on the smartphone) and sends authenticated requests. according to reports by symantec [cit] and kaspersky lab [cit] the malicious software is a significant problem with more than 30% of user computers subjected to at least one malware-class attack and more than 170 [cit] . unfortunatelly, the use of tokens as the factor, which authenticate the requests is not sufficient in the malicious environemnt. in this paper, we are focused on a popular case of request forgery attack performed by malicious software installed on clients device (eg. mobile phone) and propose a mechanism to detect such attacks. the current web application firewalls assume that the clients' devices is free from malicious software. this assumption in times of common malware can not take place."
"b. the csrf attack using a malware to bypass rsa token and waf in this example we present an attack which bypasses the rsa token and web application firewall with the use of financial trojan like zeus. rsa token is a two-factor authentication device which generate a cryptographically-secure token to authorize the business action. zeus malware, on the other hand, allows to change the bank transfer data in online banking system. the attack is hard to detect by user because the browser displays valid transfer data and data is changed to the thieves' account number during the communication. twofactor authorization, which does not user a device that displays the decription of operation to be authorized, is not effective for this type of attack."
"to protect against such attacks we introduced an approach, based on historical and behavioral analysis of user requests, which reduces the need for additional forms of authorization. after sufficiently collecting and analyzing user's history, the additional authorization appears only in the situation that actually requires it. such approach increases the responsiveness and general feel of the application."
in the literature method of protecting web applications are not widely discussed. researchers focus on non-standard attacks that can not be detected on classic firewalls and design new mechanisms for detecting these specific attacks.
"we are going to use request forgery attacks as an example of successful business attacks to present our new approach to detect and mitigate malicious requests. the most popular type of request forgery attacks are cross-site request forgery attack (csrf) which makes a logged-on victim's browser send a forged http request, together with the victim's session cookie and any other automatically attached authentication information to a vulnerable web application. in other words, the attacker forces the victim's browser to generate requests, which from the vulnerable application's perspective are legitimate. for this reason, on the server side we are not able to detect this only based on the technical attributes of the query. although this is not a sophisticated attack, it indicates that the key players (facebook, linkedin, etc.) had suffered from it."
"we introduce behavioral analysis and user action history. the user request is analyzed by waf before it reaches the target system. the waf analyzes whether the user has performed similar actions in the past and whether they have been successfully commissioned. the similarity is calculated on the base of technical and business attributes describing actions. when the requested action is similar, the waf does not require additional authorization. if not, the waf asks for additional authorization to confirm the operation for the data entered. it would speed up the use of the target system and minimize the risk that the user confirms the operation with input altered by the attacker."
"the aim of authorization bypass attacks is to perform an unauthorized action on behalf of authroized user. there exist many attack vectors and scenarios. in this section we describe four examples of such attacks, beginning with the simplest one employing social engineering techniques, to more complicated which uses malicious software."
"the a set is a set of all possible actions and the a n represents an action. this is a collection of all actions provided by a waf-protected system with information about the actions taken by the user at a specific time along with the specific business effect. an action is defined by sets of technical and business attributes along with their values, two boolean values which states whether an action has been authorized with additional mechanisms and whether it has been allowed, and action's timestamp."
in the article we presented the weaknesses of web application firewalls which use signature-based and rule-based static analysis. we presented a successful request forgery attack on the application defended by classic waf when client has malware installed.
function actionsimilarityt hreshold returns similarity threshold for given action based on its attributes. it is calculated as the maximum similaritythreshold for all possible subsets of action's values of attributes
"measuring brain activity is centerpiece to bci. however, detection of brain activity as such is not sufficient. bci systems cannot read any \"human thoughts\". they can only classify some selected states of brain activity, associated with specific events or stimuli. generally, the main task given to a user of the brain-computer interface is to \"generate\" appropriate models of brain activity by using certain mental strategies. those strategies define what a bci user research in the field of brain-computer interfaces (bci) has started in the 70s at the university of california (ucla), los angeles, under a grant from the national science foundation. the paper \"toward direct brain-computer communication\", by jacque vidal can be considered a pioneer scientific publication, describing the study of bci [cit] (new york), where jonathan r. wolpaw formalized the definition of a bci system [cit] :"
"theoretically it is possible to use other non-invasive sensors placed on the surface of the head. these can be magnetoencephalography (meg) that measures the magnetic activity of the brain and functional magnetic resonance imaging (fmri) that measures the changes in oxygenation of active brain areas. instead of fmri, near infrared spectroscopy (nirs) may also be used as a technique to measure the activity of the cerebral cortex [cit] . all these methods can be used in a brain-computer interface, but they have practical disadvantages. equipment for the meg and fmri is cumbersome and very expensive. nirs and fmri have poor temporal resolution [cit] ."
"the goal of this work is to create synthetic data that is consistent with real-world, activity-labeled smart home sensor data. activity-labeled smart home sensor data has unique characteristics in that it is multivariate, contains spatio-temporal relationships, and is reflective of dynamic human nature. furthermore, due to the fact that each data point contains time and activity information in addition to a sensor reading, this is time series data that exhibit a hierarchical organization. these data contain a sequence of data points, each of which is comprised of its own sequence. in our case, activity-labeled smart home sensor data can be represented as time series data or a sequence of time-stamped sensor readings (or sensor event data). figure 1 shows a snippet of smart home data. as figure 1 shows, smart home data can be represented as a structured time series containing a series of sensor events, ordered by time, that represent activities occurring in a smart home. if we divide the sensor events into blocks of activities, or segment the data as indicated by the red lines, we can also observe a higher level sequence within the data, namely the sequence of activities. in this paper, we focus on generating synthetic data that emulates this type of nested sequence structure. this type of data is multivariate, non-i.i.d., and contains sufficient complexity and spatio-temporal dependencies to warrant a new approach for synthetic data generation."
"to periodically reset the data sequence, synsys identifies the high-probability activities to occur at the beginning of each reset period. when the beginning of the reset period is reached, synsys compares the synthetically-generated activity label with the highest-likelihood activity label. if there is a mismatch, synsys resets the start state of the hmm data generator to be the highest-probability activity state and continues with the data generation process from that state replacing segments of data generated using the old activity label. by performing this reality check as we generate timestamps, we are able to help the hmm activity generation produce a more realistic sequence of activities that occur each day."
"the results from table 4 provide evidence that the synsys algorithm was able to successfully generate more realistic synthetic data for a week of smart home data in comparison to random data, data from another home, and data from another time period. our data exhibited a dominant activity, other activity, that encompassed a wide variety of sensor events and was very prevalent in our data. we found that including a preprocessing step to break apart this activity into smaller clusters helped the performance of the hmm based sequence generation by not biasing it towards including other activity. this may indicate that other datasets that are very unbalanced in terms of the sequences that need to be generated with a portion of the sequence that is \"other\" or \"unknown\" may also need to include some preprocessing steps in order for the synsys approach to be more effective."
"in addition to the traditional analysis of eeg signals, so-called evoked potentials (ep) [cit] are used to support medical diagnostics. evoked potentials are electrical signals measured on the surface of the head (with a few electrodes) after stimulation administered by an appropriate external stimulus. most stimuli are visual (e.g. a flash of light), auditory or sensory, so we distinguish visual, auditory and somatosensory evoked potentials."
"next, we train a separate hmm for each activity. these second-level hmms generate a sequence of sensor events reflective of the activities they represent. once the first-level hmm generates a sequence of activities, each activity is expanded into a corresponding sequence of sensor events by the corresponding second-level hmm. we refer to these as the nested sequences. finally, we train regression learners to create realistic timestamps that capture the time gaps between sensor events and the duration of each activity. figure 2 shows an overview of synsys. while we applied hmms to the problem of generating sequences at each step, this system can be generalized to employ any sequence generation technique other than hmms at the sequence generation steps 1 and 2 shown in figure 2 . additionally, while we use synsys specifically to generate synthetic smart home data, the method can be used to generate hierarchical time series data for a number of real-world applications. such applications include sentiment analysis, music genre analysis, analysis of web traffic, and analysis of public transportation systems [cit] ."
"acquisition of an eeg signal is a very difficult task. in this paper, only non-invasive methods will be considered, where sensors (electrodes) are arranged across the surface of the scalp. normally, single disk electrodes made of gold or ag/agcl are used. for dc derivations with eeg frequencies below 0.1 hz, ag/agcl electrodes perform better than pure gold electrodes. passive electrodes consist only of the disk material and are connected to the amplifier with the electrode cable. active electrodes have a special preamplifier with gain of about 10. it makes the electrode less sensitive to environmental noise such as power line interference."
"the most technologically advanced and at the same time the most difficult to implement are asynchronous interfaces, which use signals generated as a result of imagining movement. it appears that the activity of the brain is very similar during movement and during imagining movement (mental task). hence, the user does not need to make any movement, but only to imagine it. in addition, different areas of the brain are active when a person imagines movement of different body parts. this enables classification of user intents, and thus makes possible to build a system which would execute them and control the machine. for example, the thought of moving the right hand will turn a wheelchair to the right side, the left hand to the left, and so on. when analyzing the eeg signals invoked by imagining movement we talk about so called desynchronization and synchronization of brain potentials, associated with these intentions (event-related desynchronization/synchronization -erd/ers) [9, [cit] . as already mentioned, proper measurement and classification of eeg signals is possible as a result of functional division of the cortical areas. some knowledge of anatomy allows one to indicate regions of the brain which are associated with imagining movement of certain parts of the body."
"of course, a lot more convenient to use is a brain-computer interface that operates in a non-invasive way. at least three different bci designs of that sort have been developed thus far. the most commonly used is the one where the electrical signal is measured across the surface of the scalp (electroencephalogram) [cit] ."
"in bci systems, the so called steady state visually evoked potentials (ssvep) [cit] can also be applied. ssvep come from the visual cortex and are collected at the back of the skull. let us assume that a user observes a light source (stimulus) pulsing with certain frequency (645 hz). such stimulus induces waves of the same frequency in the visual cortex of the brain. while analyzing the eeg signal, we can observe that this frequency in the signal is by far the most dominant. in case where the user is exposed to multiple light sources, pulsating with different frequencies, it is possible to determine, by measuring a dominant frequency of the eeg potential, which light source is observed by the user at a given moment. in practice, each command sent to control the machine is usually associated with light source pulsating with a particular frequency. interfaces based on ssvep are relatively popular, because they operate outside the user's perception, do not require any training and are effective for most people. unfortunately, there is a certain percentage of people who exposed to a pulsating light source can have an epilepsy attack."
"measuring specific brain waves throughout the eeg is not a trivial task. such a system must implement typical functions known from measurement techniques like: data acquisition, data processing and data presentation. signals acquired from electrodes have very small amplitudes and are strongly disturbed by noise and series of physiological and technical artifacts. therefore those signals have to be carefully conditioned and then converted into digital form. next action is sophisticated signal preprocessing. after that, an eeg signal is ready for feature extraction. there are several feature extraction algorithms. each of them is expected to generate features which will, to the greatest possible extent, describe selected properties of the signal in the current application. there is often need to eliminate some redundant features throughout the selection process. finally, the classification process is implemented to feature vectors. then some control process can be executed. at the same time bci system quality should be evaluated."
"there are many methods of feature selectionstarting from fast ranking methods to end up with complex and time consuming methods. the simplest, ranking methods are based on tstatistics, k-fisher coefficients or cross-correlation. more advanced methods use complex classifiers: sequence forward selection (sfs) and genetic algorithms (ga). the linear dicriminant analysis (lda) is also often used. implementation of these methods usually brings better results but is much more time consuming [cit] ."
"artificial limbs are artificial devices replacing body parts of patients after injuries and amputations, provided some muscles and nerves still function efficiently. usually, prostheses are controlled by pulses from respective muscles. if we use for that purpose waves coming directly from the brain, obtained either in an invasive or noninvasive way, we have got a textbook brain-computer interface."
"brain activity -related to neuronal activity -boils down to the motion of electric charges which produce electric and magnetic fields. brain-computer interfaces measure that activity of the brain which is the consequence of certain stimuli or mental task. suitable sensors, placed or attached close to the selected areas of the brain, allow measurement of both electric and magnetic brain activity."
"when creating models from sensor data, machine learning algorithms need to be trained and validated using diverse datasets, including some with known patterns and distributions. however, many types of real-world sensor-driven datasets are limited in terms of availability and variety. this can introduce difficulties when employing machine learning techniques that rely on large labeled training datasets. in order to address this problem, synthetic data can be created for initial testing and validation of novel machine learning techniques."
"another important task, partly connected with the feature selection process, is the best electrodes selection -from which the eeg signal should be measured. it can be done through counting signal features that are attributed to specific electrodes. next, for a specific user, a limited number of electrodes can be used, which are located in designated areas. this considerably increases ergonomics of the bci system [cit] ."
"in order to interpret and classify measured eeg potentials it is necessary to first extract and select their features. the feature extraction process delivers a set of values (data) which essentially describe signal properties. it can take place directly in the time domain or after some transformation, for example to the frequency domain. feature selection is commonly used in processing large data sets, in order to choose the best ones and at the same time, to reduce their number. this process, in many scientific papers, is considered centrepiece to classification accuracy. there are many methods of feature selection known that are optimized for: -increasing the effectiveness of classification, -reducing computational effort, -reducing the amount of stored data, -reducing data redundancy. a block diagram of a typical solution of a brain-computer interface is shown in fig. 10 ."
the main contribution of this work is to be able to provide synthetic data that are consistent with the complexities found in real smart home data. this synthetic data can then be used to help test and improve machine learning-based techniques by augmenting small labeled training datasets.
"within each activity block shown in figure 1 is a sequence of actual sensor events. in order to generate realistic sequences of sensor events based on activities we train a separate generative model for each activity type. again, synsys utilizes hidden markov models by training a separate model for each activity with corresponding sensor event sequences from real data. using the previously-generated sequences of activities, synsys then expands an activity instance into its corresponding sequence of sensor events. to do this, synsys chooses a random number from the distribution of actual sensor event lengths for the corresponding activity as the number of sensor events to generate for this activity occurrence."
"selection of features is a very difficult and important task. although some characteristic features of an eeg signal assigned to specific events are known, they can be different for different users. not only that, they can also vary from day to day and even from session to session for the same user. therefore, feature selection is worth repeating before each use of the bci interface."
also the term event related potential (erp) is commonly used. it denotes both ep as well as other brain responses that are the result of cognitive processes accompanying and following external stimuli or of preparatory mechanisms preceding motor action [cit] .
to further examine the realism of the data we also compute the distances between one week of real data and one week of real data from another home with the same time period as well as the distances between one week of real data and week data of real data from the same home during another time of year. we assume that the activity patterns of the smart home resident changes at different times of year (for example the resident may be more likely to stay inside during winter vs. summer or have a different schedule year to year) and that the activity patterns of residents will differ between homes.
"it is known that different parts of the brain are responsible for activity of various parts of the human body, but their arrangement may be subject to change. hence the right placement of electrodes is not a trivial task. signals measured from the electrodes have very small amplitudes (from 10 μv to 100 μv) and are very noisy. some artifacts are also introduced, physiological -muscle activity, eye movements, heart rate and other as well as technicalsuch as the power line (50 hz). the useful frequency band of eeg signal ranges from 0.5 hz to 100 hz. a typical raw, disturbed eeg signal is given in fig. 11 . the 50 hz artifact clearly dominates. fig. 11 . a typical raw, disturbed eeg signal."
"the quality of operation of bci, however, is not the same across the population and is a rather individual feature. note that from an objective point of view, the speed of the interface is rather small. nonetheless it is sufficient for people with disabilities, especially if the bci system is the only way they can communicate with the environment."
"normally during an examination, a set of 19 eeg electrodes is used, according to the socalled 10-20 system, which is recommended by the international federation of clinical neurophysiology (ifcn) (fig. 5 ). in a brain-computer interface which does not have to comply with medical standards, a different number of electrodes can be used, sometimes up to 512, according to need. the number of electrodes (bci channels) and their distribution on the scalp is one of the major problems of bci. sticking a large number of electrodes to the surface of the scalp is a very laborious and time-consuming task. normally, medical examination requires proper preparation of skin before sticking on the electrodes. the places where electrodes are located are applied with a special paste or gel. moreover, the electrodes have to be stuck in right places. many research teams, including the authors, have tried to minimize the number of required electrodes, what would simplify preparation prior to an eeg signal measurement. it is very convenient to use a special cap with integrated electrodes. the best results are obtained with active electrodes (with built-in electronic amplifiers). in each case it is necessary to use gel, which from a practical point of view, is a huge inconvenience."
"the quality of brain-computer interfaces can be measured in many ways. the simplest measure is the classification accuracy (classification rate), defined as the number of events correctly classified, divided by the total number of possible trials. often, instead of the correct classification rate the classification error is given. another way to determine the quality of bci systems is to give the effective speed of its operation. it is a measure which describes how many operations are done in a time unit (for example number of alphanumeric characters \"written\" per minute)."
in figure 1 we can see blocks of sensor events that have the same activity label. these blocks represent a series of sensor events that comprise a single smart home activity occurrence. in order to
"such realistic data can be used to boost the performance of machine learning algorithms through semi-supervised learning. in the case of ar, the results from table 5 demonstrate that synsys synthetic data can be used to augment limited labeled smart home data. this method of self-training does improve the performance of a semi-supervised activity recognition algorithm."
"what is important, the user can learn through use of feedback (biofeedback as a matter of fact) how to generate patterns by imagining movement [cit] . certain features of imagining movement are visible in the eeg signal in frequency bands 8 hz÷12 hz and 18 hz÷26 hz."
"in this paper, we introduce a new method for generating synthetic sensor data that is reflective of human behavior found in real sensor datasets. we base the fundamentals of our work on earlier efforts that use machine learning and modeling-based methods to improve the realism of synthetic human behavior data."
"once we have generated the activity sequences and sensor events, all that is left to complete the synthetic data is to generate the timestamp information. these timestamps in the collective represent the durations of each activity and in the individual represent the time gaps between each sensor event that occurs within an activity. in order to learn realistic timestamps, synsys employs a ridge regression machine learning approach to better learn how to capture a time-based representation of events. ridge regression is a technique for analyzing multivariate regression data that exhibits near-linear relationships among the independent variables [cit] . using ridge regression the loss function is represented by the linear least squares function and regularization is given by the l2 norm. this helps limit the complexity of the regression model and multi-collinearity which is a preventative for over-fitting. first, a ridge regression model is trained from actual smart home data to learn the durations of each activity. next, separate regression models are trained from smart home data to learn the time gaps between sensor events within each activity. while hmms are well suited to learning sequence orders, applying hmms to the problem of learning non-uniform durations adds complexity to the model that may not produce intuitive results. we hypothesize that regression models will be effective at learning more complex timing information."
"electroencephalography (eeg) is a non-invasive method of measuring the bioelectrical activity of the brain. signals are acquired through electrodes placed on the surface of the scalp which detect potential changes caused by the activity of neurons of the cerebral cortex. eeg is very useful to monitor and diagnose epilepsy, sleep disorders, head trauma, brain tumors, disorders of consciousness and other brain conditions. the examination itself is not unpleasant for the patient, and lasts 15 to 20 minutes. during the test, the patient sits or lies comfortably with electrodes stuck to the scalp. position the patient assumes depends on what is the purpose of the examination. typically 6 to 64 electrodes are used (there are also known solutions using a much greater number of electrodes, e.g. 256). usually, the electrodes are attached using an adhesive paste (gel) and are connected through an amplifier to a recording device. the measured eeg signal is largely an individual feature and varies depending on the psychophysiological state of a person. both the signal amplitude and dominant frequencies undergo changes. it is assumed that a healthy human brain generates waves at frequencies ranging from 0.5 hz to 100 hz and amplitudes from several to several hundred µv. there are some distinctive rhythms of the eeg signal, usually slightly different defined by various authors (fig. 4 ): − alpha rhythms with frequencies from 8 hz to 13 hz, which are particularly evident during the absence of visual stimuli, − beta rhythms with frequencies from 12 hz to 30 hz, which can be seen in the frontal region of the brain and are observed during concentration, − gamma rhythms found between 30 hz -100 hz, which can be seen during motor activities, − delta rhythms with frequencies from 0.5 hz to 4 hz, which can be observed at stage 3 and 4 of sleep, − theta rhythms with frequencies from 4 hz to 8 hz, which occur during light sleep and are observed during hypnosis, − mu motor rhythm in the range 8 hz  12 hz which is used in motor imagery (mi) bci paradigm."
"an important element of eeg signal preprocessing is spatial filtering: laplace filters (lf) or common spatial patterns (csp). the laplace filter subtracts from the signal recorded from an electrode a quarter of the signal amplitude coming from adjacent electrodes. the csp method is more advanced and based on selection of weights assigned to all electrodes, by using a special algorithm. the weights selection algorithm maximizes the difference of variances of the signal in a certain band (usually 8 hz to 30 hz) for different class signal parts. in more advanced bci systems, blind signal separation (bss) methods are used. for example, independent component analysis (ica) is used for separation of signals and removing artifacts. individual components of a real eeg signal are presented in fig. 12 . after preprocessing one obtains the filtered eeg signal, without suppressed artifacts, that can more clearly expose the expected features. it is worth noting that the operating speed of digital signal processing algorithms is very important, because they operate in real time."
"in-depth studies of measured eeg signals led to the discovery of properties and rules that not only allow diagnosing diseases, but also identifying the specific signals evoked by certain stimuli. it was also found that the characteristic signals appear not only in the event of a real stimulus, but also in when somebody thinks (mental task) about doing a particular movement (muscle activity). measurement of this activity could be the basis for constructing algorithms for human-computer communication and apparatus for controlling devices using \"human thoughts\"."
"capture only the sequence of activities that occur in a smart home we can remove the sensor event information and replace each activity block with a single data point, labeled by the corresponding activity. the start of the activity is defined as the first sensor event with activity label a and the end of the activity is the last sensor event with activity label a. if a different activity label is encountered a new activity is recorded. using this simplified version of the data we can then use probabilistic modeling techniques to learn the sequence patterns of activities from real data and then generate new synthetic sequences of activities based on the learned model. to generate these sequences we use a 12 state hidden markov model. this size was selected based on empirical evidence from our smart home data."
"feature vectors are obtained through extraction. for example, for a 1-second time window we typically obtain 40 features (fft spectral components: 1 hz ÷ 40 hz) [cit] . in this way, 1280 features are created in the 32-electrode system [cit] . note that the number of features can be very high, moreover some features may be redundant or unreliable (do not bring significant information). hence we need a method to eliminate redundant features before classificationa selection."
"design and implementation of brain-computer interfaces is one of greatest challenges posed to modern science. this is proved true by numerous publications in scientific journals as well as extensive media coverage. the possibility of direct human-computer interaction (without manual manipulation of peripheral devices) opens new channels of communication in medicine, psychology, media and military. use of such an interface in medicine is of particular importance, both in terms of studying human brain, and for supporting people affected by neurological inefficiency. brain-computer interfaces can help people with severe neurological conditions such as amyotrophic lateral sclerosis, brain stroke, guillain-barré syndrome, amyotrophic lateral sclerosis, cerebral palsy or multiple sclerosis to communicate with the outside world. many people suffer from amyotrophic lateral sclerosis, the neurodegenerative disease of the nervous system that destroys part of the central nervous system responsible for movement, but does not influence senses, cognitive abilities and intellect. people, who suffer from it, gradually lose control over their own body and within 2 to 3 years reach a state where they have no ability to communicate with the environment. another group of people who could communicate with the environment by bci are those who have strokes, particularly the brain stem strokes. victims of traffic accidents, which resulted in damage to the cervical spinal cord, could also belong to those groups."
we describe each of these learning problems in detail and then provide an explanation for the overall system that can be used to produce realistic synthetic data of this type. in order to generate realistic synthetic data we combine the strategies for each of the learning tasks described above into a system called synsys. we first train an hmm to generate a realistic sequence of activities (the outer sequence).
in figure 1 we can see blocks of sensor events that have the same activity label. these blocks represent a series of sensor events that comprise a single smart home activity occurrence. in order to
to demonstrate how synthetically-generated data can be used to improve the performance of machine learning algorithms when real labeled data is scarce we use synsys combined with semi-supervised activity recognition to augment a real smart home dataset. we hypothesize that training an activity recognition algorithm with the synthetically-augmented data will boost performance of the recognition algorithm.
"mistakenly, the terms neuroprosthetics and brain-computer interface are often used interchangeably. this stems from the fact that both neuroprosthetics and the bci are different means to achieve the same goal. some elements of physical interfaces that could be connected to the human nervous system in order to improve senses, are given in fig. 2 [cit] . it should be noted that the brain-computer interfaces differ from other interfaces because of using signals generated directly by the brain, rather than signals coming from muscle activity (electromyography, emg). in fact, electrical signals coming from muscles are treated in that case as unwanted noise -so-called physiological artifacts. an example of such an artifact is an electrical signal generated while moving eyeballs (electrooculography, eog). the problem is that usually the artifact signal amplitudes are grater (measured in mv) than the levels of the eeg signal (measured in µv)."
"to validate the realism of data generated using our synthetic data technique, we use data similarity measures to demonstrate that the synthetic data generation technique is not random and that it preserves the underlying patterns and structures of real data while still providing a way to generate arbitrary amounts of new artificial data. we then apply synthetic data generation to the problem of generating synthetic data for semi-supervised activity recognition to improve algorithm accuracy when only a small amount of real annotated data is available. this illustrates how synsys can be used in a real world machine learning application."
"for those reasons, conditioning of the eeg signals is very important. at first the signal has to be significantly amplified. then, the voltage generated by the skin-electrode contact should be taken into account. next, the power line frequency disturbance (50 hz) should be filtered out and simultaneously the signal filtered by a low-pass filter. the amplified measurement signal, coming from several -sometimes even a few dozen -electrodes, is further converted into digital form and transmitted to the computer. there, some further preprocessing, now already in digital form (dsp), is carried out."
"next, we train a separate hmm for each activity. these second-level hmms generate a sequence of sensor events reflective of the activities they represent. once the first-level hmm generates a sequence of activities, each activity is expanded into a corresponding sequence of sensor events by the corresponding second-level hmm. we refer to these as the nested sequences. finally, we train regression learners to create realistic timestamps that capture the time gaps between sensor events and the duration of each activity. figure 2 shows an overview of synsys. while we applied hmms to the problem of generating sequences at each step, this system can be generalized to employ any sequence generation technique other than hmms at the sequence generation steps 1 and 2 shown in figure 2 . additionally, while we use synsys specifically to generate synthetic smart home data, the method can be used to generate hierarchical time series data for a number of real-world applications. such applications include sentiment analysis, music genre analysis, analysis of web traffic, and analysis of public transportation systems [cit] ."
"in our real smart home data sets, an activity label called \"other activity\" is used to encompass all unknown activities. when manually annotating data with ground truth activity labels, a set of known activities are detected and labeled. these activities correspond to categories often used in assessment of basic or instrumental activities of daily living (adls) [cit] . however, these predefined activity categories typically make up approximately half of the total sensor data. these findings are consistent with survey-based instruments revealing that less than half of reported time use falls into these well-understood activity classes [cit] . this results in a large number of other activity labels being present in the real data. the prevalence of other activity occurrences in smart home data will bias the hmm toward inserting other activity labels into the activity sequence not only often, but in unintuitive sequences."
"an artificial ear is a surgically implanted cochlear implant which can help a deaf person to retrieve hearing. the cochlear implant does not strengthen hearing, but works by direct stimulation of the auditory nerves leading to the brain. there are ongoing studies to improve the implant, whose trial electrodes are connected directly to the brainstem."
"\"a brain-computer interface (bci) is a communication or control system in which the user's messages or commands do not depend on the brain's normal output channels. that is, the message is not carried by nerves and muscles and furthermore, neuromuscular activity is not needed to produce the activity that does carry the message\"."
"first experiments with bci had an invasive character and typically for such cases, were conducted on animals (mice, rats, cats, monkeys). invasive methods require surgical intervention, such as cutting the skin or opening the skull (intracranial recording). when the electrodes are placed on the surface of the cortex (partially invasive method), we are talking about electrocorticography (ecog). ecog does not damage neurons because the electrodes are not entered inside the brain. if the signal is measured with the electrodes placed inside the cerebral cortex (invasive method) we are talking about intracortical recording. in general, invasive methods have very good signal quality (high level of amplitude, low-noise) and very good spatial resolution. internal electrodes allow registration of the activity of small areas of the brain or even individual neurons (brain cells). artifacts associated with muscle movement are not burdensome in that case. however, those methods have a serious disadvantage. they require complex surgical intervention into the brain and attract very legitimate ethical controversy. moreover, from a purely physical standpoint, long lasting signal recording with electrodes placed inside the brain can be problematic because the electrodes react with bodily fluids. it can significantly deteriorate the quality of measured signals."
"the most widely used and mostly written upon is the p300 evoked potential [cit] . this potential appears as a response to a visual or auditory stimulus awaited by a user, often highly emotionally. the p300 potential occurs after approximately 300 ms after the appearance of the stimulus -hence its name. the precise parameters, like amplitude and latency of the response to a stimulus, depend on many psychophysical factors and are unpredictable. in practice, for visual stimulation, the user observes a set of randomly illuminated signs like letters or other characters. at the moment of illumination of the sign expected by the user (on which the user focused his attention), an eeg potential of a small amplitude appears in the top area of the brain. in order to measure the p300 potential more precisely, the user watches the same character highlighted several times and his responses to stimuli are averaged. by moving his attention to another sign, the user is able to write a text. often, to speed up selection of appropriate characters, the entire rows and columns are highlighted. a typical display panel that appears on the user's monitor is presented in fig. 7 . fig. 8 shows p300 and non-p300 responses and fig. 9 -the location of brain activity generating the p300 potential. fig. 7 . typical panel displayed on the monitor screen of a bci system based on the p300 potential [cit] . fig. 8 . localization of brain activity for the p300 potential [cit] ."
"the most commonly measured features are signal amplitudes (for p300 potential) or the spectral components (for ssvep and erd/ers). for example, in case of the p300 potential, we know when to start the signal analysis. hence, we can even average the signal, and then extract features that are used to train the classifier. in case of asynchronous interfaces (erd/ers) we do not know when to start and in order to extract features it is necessary to analyze the entire recorded signal. to do this, a window of fixed width is chosen. the window is shifted in time and for each of its positions the features are extracted from the signal [cit] (fig. 13 )."
"the barriers to dissemination of direct brain-computer communication methods, using the eeg signals, are high price and complexity of the apparatus. in fact the amplifiers used for bci are designed for applications in medical diagnostics, containing from 32 to 512 channels. in addition, they are usually designed to work with other types of medical equipment, often through a specialized interface whose communication protocol is not widely known. this raises the need for a dedicated, cheaper amplifier and other signal conditioning modules for use in bci. although, according to studies, it is possible to reduce the number of electrodes, their minimum number and location remain unknown. besides, deployment of electrodes may be different for each user. the knowledge and intuition of a doctor is most helpful here. also important is the fact that the features of the eeg signal can change with changes in mental states of the user. additionally, in erd/ers interfaces the features strongly depend on the process of \"imagining movement.\" furthermore, the tools that enable quick and effective selection of the best features have not been tested thoroughly. resolving those issues will help to overcome barriers to effective use of brain-computer interfaces in practice."
an artificial eye is a retina implant -a microelectronic circuit implanted into the eyeball and connected to a camera mounted in glasses. signals read from the camera are sent from the implant to the brain through the nervous system to restore the ability to see for people with age-related macular degeneration.
"a natural way for humans to communicate with the outside world is to use some individual muscles of the human body. intentions born in the human brain are transmitted through the nervous system to selected parts of the body and stimulate their movement. speech (throat, tongue, lips) is predominantly used for communication among people, as are also fingers in case of the sign language. man-machine communication (mmc) means a type of communication where the same principles can be applied. some simplification of the problem is a human-computer interaction (hci) which traditionally involves a keyboard, touchpad and/or a mouse. an alternative way is to use a microphone and a sound board to issue voice commands or a camera to provide instructions in form of facial expressions and/or hand placement. finally, we can imagine controlling a computer via electrical signals extracted from various parts of the peripheral nervous systems or even from the central nervous system -directly from the brain. the last type of communication is called a brain-computer interface (bci). a summary of above-listed interfaces is illustrated in fig. 1 ."
"initially, the research was focused mainly on applications in the field of neuroprosthetics [cit] . its main purpose was to restore damaged senses such as hearing and vision or mobility of patients. in neuroprosthetics, artificial devices are used to replace certain human organs/senses. in those cases, the brain has to \"learn\" how to read signals sent by the prosthesis or generate signals needed to control the prosthesis, throughout the entire nervous system. however, those signals are not required to be delivered directly to the brain or come directly from the brain (central nervous system), but rather to peripheral nerves. in general, it is assumed that in the field of neuroprosthetics a link can be established: nervous system (any part of it) ↔ device, while the bci enables direct coupling: brain ↔ computer."
"for example, imagining left hand movement increases activity of the brain within the area of the c3 electrode. imagining feet movement manifests itself the most by reading of the cz electrode. the distinction between imagining movement of right and left foot with help of eeg is not possible. the corresponding brain areas are too close. the same is true for imagining movement of each finger. fortunately, areas connected with hands, feet and tongue are characterized by pretty large topographical differences, and therefore they are usually used as subjects of mental tasks."
"as already mentioned, the advantage of interfaces built using evoked potentials is that they work outside human perception and therefore do not require much training. several methods of using p300 and ssvep interfaces have been proposed i.e. for writing text, moving the cursor, robot and intelligent building controlling [cit] . the principal drawback of those interfaces (p300 and ssvep) is that they require the user to move his eyes. it is often difficult or not possible at all for a completely paralyzed person. fig. 9 . p300 response and non-p300 response."
"we create a novel approach to the problem of generating realistic synthetic smart home sensor data. our approach employs hidden markov models (hmms). hidden markov models lend themselves well to the problem of modeling smart home data, due to the data's sequential nature and a hidden markov model's sequence-generative nature. we use real smart home datasets similar to the data shown in figure 1 to train a model and generate synthetic data. the real smart home data we use in our experiments are collected from older adult participants with casas smart home in a box (shib) sensors installed in their homes [cit] . table 1 provides summary statistics for the smart home sites used in our experiments. in casas smart homes, as the resident moves around the home performing daily activities, sensor events are recorded that indicate the time of sensor activation as well as other information about the originating sensor. this information is then used in ar [cit], an activity recognition algorithm which is trained from human-annotated ground truth activity data truth to label sensor events with a label from the following set of activities: cook, eat, sleep, personal hygiene, take medicine, work, leave home, enter home, bathe, relax, bed toilet transition, wash dishes, and other activity. the types of sensors that we use for our experiments include passive infrared motion sensors which activate when heat-based movement occurs within their field of view and door sensors which are magnet pairs that activate when a door is open or closed. figure 1 . activity-labeled smart home sensor data that exhibit a nested sequence structure."
"in this section we provide an overview of the components that make up synsys. synsys is available for download online and is supported by the pomegranate python library for hmms (synsys: https://github.com/jb3dahmen/synsys-updated, pomegranate: https://github.com/ jmschrei/pomegranate)."
"amplification at the final stage, the output signal from the classifier is used to drive the actuator, which generates a certain event as a result of user intentions."
"smart home data can be represented as a structured time series containing a series of sensor events, ordered by time, that represent activities occurring in a smart home. each sensor event contains the event timestamp, the sensor name, the sensor state and the activity label. upon closer inspection of the timestamps we can see that this time series data does not have equal time intervals between data points. this is due to the fact that each instance of the data represents a sensor event which is triggered by residents moving around the smart home space and activating the corresponding sensor. the sensors used in this case are ambient discrete event sensors. as a result, they generate text-based messages report their state when the state of the sensed environment changes, in contrast with polling sensors that report their state at equal time intervals. in order to generate synthetic data that resembles this kind of time series we divide the problem into three separate learning tasks."
"because synsys is based on generating nested sequences of data and sequences of timestamps that capture duration, the method can be applied to more generalized data beyond the smart home datasets we used in our experiments. one example of this could be to apply this approach to natural language datasets that exhibit this kind of nested sequence structure with different parts of a language's grammar. the synthetic data can then be used to train and test the performance of novel machine learning algorithms that require larger labeled data sets that are expensive and time consuming to obtain."
"the main task of a brain-computer interface is to allow communication with the outside world for patients with severe stages of neurological diseases such as amyotrophic lateral sclerosis, cerebral subcortical stroke, guillain-barré syndrome, cerebral palsy or multiple sclerosis."
"certain characteristic features of the eeg signal (obtained in the selection process), are next used in the classification process. there are many algorithms that enable classification of eeg signal features. most often used are: support vector machine (svm), multilayer perceptron (mlp), naive bayes classifier (nbc), k -nearest neighbor classifier -knn, linear discriminant analysis (lda) and hidden markov models (hmm). note that an important component of any brain-computer interface is the calibration session, during which the measured eeg signals are analyzed and classifiers are trained. a summary of typical usage of different algorithms in brain-computer interfaces is presented in table 1 . table 1 . a summary of algorithms associated with particular stages of brain-computer interface operations."
"the next step is feature extraction from the eeg signal. features which best describe expected properties of the signal are sought after. these features can be related to the shape of the waveform (time analysis), to individual frequency components (frequency analysis), to the power density spectrum, time-frequency analysis (short-time fourier transform -stft, wavelet transform -dwt), autoregressive models (ar) or higher order statistical parameters (hos) -variance, skewness, kurtosis [cit] ."
"two xpatches were fixed according to the manufacturer's instructions, to a 50th percentile hybrid iii dummy headform. the xpatches were attached using the manufacturer's adhesive patches, to the left and right sides of the head, in the area of the mastoid part of the temporal bone as recommended by x2biosystems, inc.: 72 mm from the head's centre of gravity to the inside edge of the xpatch (figure 1 ). reference devices consisting of a triaxial linear accelerometer (kistler 8688a) and three angular rate sensors (dts ars12k) were mounted at the centre of gravity of the headform on a block supplied by the manufacturer (humanetics, inc.). the reference data were sampled at 10,000 hz, and 200 ms of data were recorded per impact. linear acceleration was filtered at 1000 hz and rotational velocity was filtered at 300 hz. fast fourier transformation within labview (national instruments) 29 was used to calculate the amplitude spectrum and verify these as suitable frequencies, that is, no loss of data. a forward finite difference method was computed to determine rotational acceleration (equation (2)). [cit] program"
"the table 1 presents the effectiveness of the model where it is tested in different operating system environments by varying the image sizes. in both the cases of operating systems the model performs better retrieval accuracy in minimum time. the query medical image is considered and is to be matched with the similar images in the database, to identify whether the patient is normal or suffering from disease. a local database is maintained and the query image considered is processed with these images in the local database for relevance. the query image is processed based on query based image technique presented in section ii (b) and the relevant images are obtained based on relevance method. these are presented in figure 2 and figure 3 ."
"a, and δ a are the initial soc of cells above the inductor, the final soc of cells, and the soc difference of batteries, respectively. furthermore, the related values of cells below the inductor, b 0 and b, are set similarly. therefore, (1) can be developed to describe the energy transfer between these two parts caused by the equalizer e :"
"the methodology is tested by varying the images ranging count from 200, 300, 500 and 1000 images in the database. the next level of analysis is to identify the match between the roi and the images considered with above sizes. since it is mandatory to test the appropriateness of the developed methodology, we have tested the performance on different operating systems by varying image sizes as mentioned above. the time elapsed against each of the images under different operating system environments like windows, linux, unix is considered."
"impacts were created by allowing the headform to drop in a purpose-built drop test rig and impact a steel hemispherical anvil of 0.12 m diameter ( figure 1 ). as skull fracture is not being investigated, the diameter of this impactor is not considered significant. a wide variety of impactors have been used in previous studies. 25, 27, 28, 30 the hybrid iii head was rigidly attached to the cross bar of the apparatus, and this cross bar was constrained so as to allow only vertical movement. this constraint ensured consistency in the repeatability of the tests (sample results in table 1) . following an impact, the rotation of the head is a function of the stiffness of the neck, as the base of the neck is rigidly constrained in the vertical direction."
"based on the inductor equalizer, an equalization circuit for the battery energy, called the bmmeec, is proposed in the present study. then, the corresponding mathematical model deduction and simulation verifications are presented."
"models based on gamma, log-normal and nakagami distributions are also highlighted in the literature for handling asymmetric distributions. however, nakagami distribution overrules the other distributions while characterizing the speckles in tissues [cit] . nevertheless the nakagami distribution fails in handling larger impulse response of the speckles generated by human speckles compared to generalized gamma distribution. this paper presents a methodology using generalized gama distribution. the organizing structure of the paper is as follows. the proposed methodology is presented in section ii. the experimentation, results together with performance analysis and evaluation are highlighted in section iii, the final section iv concludes the paper."
"on the other hand, when the switch is off, the energy stored in the inductor is released to all cells in the charging loop, and ( − 1)/( − 1) of the released energy flows to the ( −1) cells included in the selected cells. therefore, the soc difference of the selected cells in this process is"
it is the ratio of the number of relevant images retrieved to the total number of irrelevant and relevant images retrieved. it is usually expressed as a percentage. the outputs derived using the proposed metrics are presented in table 2 and in
"although the solution is infinite, only one feasible solution has the corresponding control strategy. feasible requirements are as follows. (1) switch conducting time cannot be negative."
step . the processor solves linear equations to work out the ne of the static game model ( ) and transforms it into control signals.
"to simplify the control of inductor equalizer, bidirectional multi-input and multi-output energy equalization circuit (bmmeec) is proposed. the proposed circuit has three characteristics as follows: first, in terms of the circuit topology, each equalizer consists of a dual switch group, connected in a parallel manner with the entire battery string, and an inductor is connected between every two adjacent cells. it should be indicated that no shared switches exist among equalizers; second, in terms of the controllability, each equalizer is controlled independently regardless of the coupling effect; third, in terms of the circuit operation, each equalizer works synchronously; hence the equalization time decreases. moreover, compared with other types of equalizers, inductor equalizers have a larger current capacity than that from capacitor equalizers. furthermore, they have lower winding precision requirements than those from transformer equalizers. in the perspective of the circuit modeling, the barrier for the mathematical description is eliminated; equalizers are independent and rational; the mathematical description is feasible and the operational data is measurable. therefore, the cisgm can be developed and exploited for control analysis, where the equalizers are treated as independent game participants, the battery energy is regarded as the capital of the participants, and the nash equilibrium (ne) of the battery energy is set to be the termination of the game. as for the battery energy, it is described by the battery state of charge (soc) in quantity. soc is one of the most important parameters in a li-ion battery, which is usually utilized to reflect the energy state of the battery. reviewing the literature shows that researchers have proposed variety of accurate methods to investigate the li-ion batteries [cit] . the benefit function can be derived to describe participants' behaviors and evaluate the equalization effect. this paper is organized as follows. in section 2, the topology and principle of bmmeec are introduced. in section 3, the mathematical description of each component of bmmeec is carried out. it is intended to establish a complete information static game model (cisgm) and derive the benefit function for quantitatively evaluating each equalizer's behavior. the model for a four-cell battery string is established and deduced with the initial soc of each cell set reasonably. its solution is transformed into the corresponding control method. in section 4, simulation results are presented to verify the feasibility of the obtained control. in section 5, the present work is concluded and the authors' further work is introduced. figure 1 shows the topology of the bmmeec. it comprises two main parts, equalizers and the battery string. in a certain equalizer, the two switches (mosfet modules) control the flow of energy and the inductor acts as an energy carrier. for a battery string that contains cells, − 1 equalizers are needed to conduct the energy equalization. since each equalizer contains two switches, there are 2 −1 kinds of equalizer control combinations. this topology has the following effects: (1) independent equalizers: each equalizer can be independently controlled; (2) overall effect in the energy distribution: any energy transfer caused by a certain equalizer affects the overall energy distribution in the battery string because all the cells are involved. figure 2 illustrates the principle of the bmmeec. during the operation cycle of the equalizer, only one of the two switches can work; otherwise, the cell will be short-circuited. figure 2 presents the discharging loop with a red dash line. in this loop, the energy from batteries flows through the actuated switch to the inductor. when the switch is turned on, it forms a discharging loop together with the inductor and the cell above the inductor. moreover, when the actuated switch is turned off, it forms a charging loop with the inductor and the cell below the inductor, and the energy stored in the inductor is released to the cell below the inductor. therefore, the energy transfers from batteries above the inductor to batteries below the inductor. invert energy transfer can be realized in a similar way. consequently, in a battery string, energy redistribution for multiple cells is realized as is shown in figure 3 . it is feasible to achieve energy equality for all cells in the battery string by applying proper control for equalizers."
"the proposed methodology is depicted in the block diagram shown in figure 1, and it will be very much useful for medical image analysis for certain remote areas, where only the minimum first aid is available. using the developed methodology, procedures can be evaluated and necessary treatments can be derived."
"assuming that the shape and texture features play a vital role in identifying the contents more effectively [cit], in this model identification of malignant and non-malignant tissues is experimented. other models based on auto regression [cit], wavelets [cit] have been proposed in the literature. however, these models lack in efficiency while retrieving the images of relevance because of several disadvantages like k-means algorithm is sensitive to initial clusters which may not be efficient for handling medical images where the data is both continuous and discrete. models like binary splitting, kd-tree, genetic algorithms are subjected to parametric dependence and complexity [cit] . hence efficient methods to retrieve the images of interest will be of a great advantage."
"(2) only one of two switches in an equalizer can be actuated. otherwise, part of batteries in the battery string may be short-circuited. in other words, the product of a and b must be zero."
"similarity, when the switch is off, the energy stored in the inductor is released to cells that are all included in selected cells and the soc divergence of the selected cells in this process is"
"(1) the design parameters of each cell are identical, such as the value of discharging current, discharging efficiency, battery capacity, and, most importantly, the unique correlation between the soc and the energy. therefore, the energy state of each cell is presented by its soc value."
"states under clock signals. all the results above are obtained when switches are actuated by ideal gating signals. however, in practical applications, control signals usually cannot reach such a precise level. consequently, a 40khz clock signal is applied for switches control and every 20 sequent clock cycles are regarded as one operation cycle. consequently, the original 4000 hz gating signal is replaced by a 1000 hz square wave signal and a 40 khz clock signal is used as a counting signal. the former is used for switch control as the gating signal and the latter is used for counting. the clock signal divides one square wave cycle into 10 parts. the duty cycle of each switch is changed by adjusting the number of conduction cycles of each switch every 20 clock cycles. switches are actuated for 18, 24, and 30 clock cycles in every operation cycle until the variance of the soc reaches the minimum value. set the currents measured by the four ammeters to be i1, i2, i3"
"with these two requirements, the feasible solution can be uniquely determined. according to the feasible solution, the control strategy can be realized by setting the proper duty cycle of three corresponding switches:"
"this study has shown that the xpatch performs reasonably well in terms of linear acceleration but has highlighted that the rotational velocity and acceleration measurements recorded by the xpatch have high levels of error and therefore need to be used with caution. this study also found that there is an issue using differentiation to calculate rotational acceleration unless the sampling frequency and bandwidth are suitable. to improve the rotational acceleration measurements, either a higher sampling rate or an array of accelerometers that allows the rotational acceleration to be calculated without differentiation must be used."
"after several operation cycles, the four cells power consumption gradually conforms to each other. figure 11 illustrates the waveform of the soc in the idle state."
"brain disease is one of the most striking factors for the increase in mortality rates. in india this rates have been increasing continuously [cit] . most of the diseases causing brain tumors may be either benign or malignant. the classifications of these are subjected to the size of the tumors. hence effective diagnosis and identification of these diseases will be of crucial importance. most of the cases related the treatment of these diseases is subjected to mr imaging. mr imaging is mainly choosing because of its property of non-ionization. along with the advantage, the grater disadvantage with these techniques is that the reports are generated by the radiologists using the visual perception of the mr images [cit] . however as the number of cases of brain related diseases are increasing, the visual perception may lead to incorrect decisions. another added factor to this is the proportionate increase of the radiologists versus the increase in the number of cases is lagging. hence the advantage of using automated systems for effective brain image scanning and report generations are of crucial importance. this paper highlights a methodology which helps to retrieve the relevant images from the databases. this system may help in particular to the paramedics, radiologist residing in remote areas to suggest a basic treatment for patient in residing in rural areas. so that he/she can be shifted to nearby super specialized hospitals."
"in the period from 3.97998s to 3.98004s (t a,3 ), all three switches are turned on, and they form three discharging loops. under the superposition of the energy transfer effect caused by three switches, cells c 1, c 2, c 3, and c 4 are discharged to the inductor and the discharging speed can be determined by the slope of the current waveform. in the period from 3.98004s to 3.98006s, two discharging loops are formed by switches sw b2 and sw b1 . moreover, switch sw a3 is turned off to form a charging loop. the decrease of discharging loops leads to the decrease of the amplitude and slope of the discharging current. furthermore, battery c 1 gradually transformed from the discharging mode to the charging mode."
the xpatch overestimated the linear acceleration during a frontal impact. this overestimation was on average 16.9% for the lhs xpatch and 23.7% for the rhs xpatch ( figure 5 ).
"(2) the initial soc of each cell is instantly available. (3) inductors have enough large inductance, i.e., enough capacity to store the energy from cells."
the in-homogeneities in the brain can be identified and differentiate appropriately basing on the color also. hence for each image the color components are also considered. another feature which is of prime important for any retrieval and identification is the texture. in this methodology the texture of images are extracted and these features are given as input to the generalized gamma distribution.
"step . the processor calculates the coefficient matrix ( ), average soc ( ), difference matrix of the soc (δ ), and variance of the soc ( ) to form the ne equations."
"inductor equalizer is a predominant type in fast equalization, and its design is not as expensive or complex as the transformer equalizer [cit] . nishijima and kuktut proposed an inductor equalization circuit [cit] with similar topology to the switched capacitor circuit, where they replaced capacitors with inductors. this innovation increased the current capacity of equalizers and realized fast equalization. since there is a shared switch between every two adjacent equalizers, the switch coupling effect results in the complex mathematical description for equalizers. zhao managed to work out the control for a three-cell battery string [cit], but it was complex due to the coupling effect. yuang-shung lee added a resonant loop in the equalizer to reduce the switching loss [cit] . moreover, cassani analyzed the feasibility of such kind of circuits in the view of the control and concluded that the complexity of the controller exponentially increases as the cell number increases [cit] . he gave a compromise solution by dividing the battery cells into several groups to lighten the work of each controller [cit] . the potential performance of these inductor equalizers is constrained by inefficient control and their application on the fast equalization is limited to small-scale battery strings. xiangwei guo improved the conventional topology and proposed a bidirectional lossless equalization circuit, which features a simple control method and fast balancing. in other words, the proposed circuit has a large equalization current and exhibits outstanding equalization performance [cit] ."
"in order to fully utilize the flexible topology of bmmeec to achieve energy redistribution, it is necessary to mathematically describe and analyze its energy equalization process. considering the following reasons, it is concluded that the game theory (gt) has high relevance and correspondence to 4 complexity such flexibility. (1) independent players: in a game, players independently choose their own strategies to maximize their own benefits. (2) the comprehensive effect in benefit distribution: since all players are involved in the game, any behavior of a certain player affects the balance of the benefit. moreover, the goal of the players is to maximize the benefit. therefore, once they cannot get any more benefits, the game comes to the end. according to such relevance and correspondence, it is reasonable to establish a gm for the bmmeec."
"when the switch is off, the stored energy in the inductor is released to the cells in the charging loop and / of the cells are included in the loop. the soc divergence of selected cells is"
"on the other hand, the purpose of equalizer control is to realize the full potential performance of the equalizers. therefore, working out the control method is essentially a decision problem. game theory (gt) is the study of utilizing relevant parties in the game of multiple individuals or teams under the constraints of specific conditions and implementing corresponding strategies. it has been studied predominantly as a modeling paradigm in the mathematical social sciences especially in economics [cit] . compared with humanities and economies that cannot be rational, intelligent control seems more suitable to apply the gt [cit] . with the development of measurement and control technology, game theory has been increasingly applied in system management [cit] and resource scheduling [cit] . myerson defined the gt as \"a mathematical model for studying conflicts and cooperation between intelligent rational decision makers\" [cit] . therefore, in the perspective of control, the gt is a study of conflicts and cooperation between interactive controllers for certain purposes. the controller and purpose are premises of establishing the game model (gm). these two premises are clear in the aforementioned equalization. equalizers can be regarded as controllers and the purpose is to achieve the energy balance of batteries while for the inductor equalizers mentioned above it is complex to make mathematical descriptions for the equalizers' behaviors, when a considerable number of cells are involved, which becomes a barrier for establishing a mathematical model."
using these formulae the performance evaluation is carried out and the proposed model is compared to gmm and the results obtained are tabulated and presented in table 5 .
"the retrieved efficiency, in case of medical images against the query image is evaluated using image quality metrics. the various metrics considered for the work include, maximum distance, mean squared error, signal to noise ratio and jaccard quotient. the formulas for computing the above quality metrics are presented in table 4 ."
the brain images from the image dataset are given as inputs to the generalized gamma distribution proposed in section ii (a). the pdf of the images is retrieved and stored. the query image is processed using section ii (b). the relevant images that are matched are compared against the pdf.
"generalized gamma distribution is proposed for the classification of the images in this paper. in general, in content based medical image retrieval, the damaged tissues are to be recognized. these damaged pixels behave in a different manner compared to the nondamaged pixels and the intensity levels of these pixels will be more i.e. the number of non-damaged pixels are more than the damaged pixels. and these damaged pixels will have their peeks at the origin and hence the damaged medical images will have their distributions with long tails. in order to interpret these distributions, tail distributions will be more appropriate and to cater these distributions the best suited distribution is generalized gamma distribution. generalized gamma distribution includes several other distributions such as weibull distribution, gamma distribution, log-normal distribution, chi-square distribution and exponential distribution as particular cases. the main advantage of using generalized gamma distribution is that, it can sensitize the medical image features in presence of noise and with minimum variance more effectively."
"step . the system detects the basic input parameters of the cisgm, the scale of the battery string ( ), and the initial state of charge of cells ( 0 )."
"in this paper an efficient methodology for brain mri image retrieval against the query image is proposed. this proposed system helps to retrieve the relevant images from the image dataset effectively. the proposed model exhibits good recognition rates of above 90%. the evaluation of the developed model is carried out by comparing with the existing models based on gmm, by using quality metrics. the results show that, this developed algorithm outperforms the existing algorithm. this methodology suits very well in applications wherein assistance is needed for the doctors at remote areas."
"an analysis of the data found that all the impacts were recorded by the xpatch, that is, no missing impacts (table 2 ). pearson's correlation coefficient and predicted residual error sum of squares (press) statistic were calculated to investigate the reliability of the data and to provide a summary measure of fit of the model to the data."
"where l is the image from the database and k is the query image (note: k and l should be of same dimension) and we say that the query image k is the part of the image l, if the correlation approaches to near 1."
"personal computer (pc) with windows operating system with 200 images in the database is considered and the result was successful and the time elapsed is 1.8 seconds. the tests were conducted in the same environment on databases of sizes 300, 500 and 1000 and the time elapsed for retrieval is 2.5 seconds, 2.75 seconds and 2.9 seconds respectively. the results were tested under the other operating systems with the above dataset and time taken is as follows."
"during the data processing, it was found that the sampling rate of both the reference data and the xpatch data was critical in acquiring accurate results. the xpatch is reported to sample linear acceleration at 1000 hz and angular motion at 800 hz. 31 the low sampling frequency may be a possible cause for the underprediction of results. unhelmeted impacts require a higher frequency and bandwidth than helmeted sports, due to the shorter duration of the impact. this requirement will have a greater influence on the accuracy of the angular motion data as it has been found that, for dummy helmeted impacts, gyroscopes require bandwidths of 500 and 740 hz if numerical differentiation is used to calculate rotational acceleration. 31 the bandwidth of the gyroscopes in the xpatch may be too low as it has been reported that most of these sensors have a bandwidth of 110 hz. 31 in this study, both the reference and the xpatch rotational acceleration data were computed using a numerical differentiation method. this method amplifies the noise on the signal, and this was particularly apparent on severe impacts where large errors in the rotational acceleration data occurred. in future work, it would be interesting to use a six-or nine-accelerometer array as used in some other studies 33 to eliminate the requirement for numerical differentiation."
"in (4), the absolute value of the difference between two fractions describes the departure of the present soc distribution from the equalization goal. the departure is used to evaluate the benefit of each equalizer. when equalization results match the goal, the benefit is zero. note that (4) is always less than 0 in value. thus, the following inequality always holds:"
"in the period from 3.98006s to 3.98008s, only sw b1 switch is turned on to form a discharging loop and other two switches are turned off to form two charging loops. it is observed that the discharging speed of c 2, c 3, and c 4 cells further decreases. moreover, c 2 and c 4 cells gradually enter the charging state. the charging speed of the c 1 cell further increases."
"where \"a\", \"b\" and \"x\" are called gamma variants and \"c\" and \"k\" are called shape parameters. by varying the value of the shape parameters, the particular cases of gamma distribution can be modeled."
"the input query image is processed using the methodology under-laid in section ii (c) and the relevant images are retrieved. the performance of the developed methodology is tested using metrics like precision, recall, error rate and retrieval efficiency. in order to present this paper we have considered an image dataset obtained from university of rennes1. the relevant images from the data set against the query are matched based on region of interest (roi) using the equation 25."
"moreover, the cisgm model is developed for the mathematical description and control analysis of the bmmeec and the feasible control is obtained by solving the model's nash equilibrium. an equivalent simulation model of the four-cell equalization is established in the pism. in order to justify the cisgm during the simulation, the variation of the operational data, balancing current and battery soc, tallied with the mathematical descriptions of the bmmeec. it is found that four cells' socs are nearly identical, which verifies the control validation. moreover, the simulation results demonstrate that the application of the bmmeec prevents the long-period charging-discharging cycles for involved cells so that the soc of each cell moves directly forward in harmony. it is found that the bmmeec has the"
"figures 14(a), 14(b), and 14(c) illustrate the waveforms of the four-cell soc in the idle state, charging state, and discharging state, respectively. figure 14 indicates that, in the idle state, the equalization completes at 86.68s. moreover, the battery string is charged in the charging state at 20v and 7a and the equalization completes at 84.36s. furthermore, the battery string in the discharging state is discharged at 15v and 7a and the equalization completes at 92.42s. table 4 displays the corresponding data. figure 14 (a) shows that there exist deviations from figure 11, which is caused by discrete control signals. the change in the slope of the soc waveform indicates that the energy stored by the inductor of the equalizer flows back to the battery string after the end of control. figure 14(b) demonstrates that, in the charging state, the cell with the lower soc is charged faster, while the battery with the lower soc is slowly charged or nearly is not charged. after reaching the equalization, the soc of each battery changes uniformly. compared with the idle state, more energy is involved in the equalization because of the external power source; hence the spent time on the equalization decreases. therefore, it is concluded that the bmmeec can avoid overcharging of the battery string. similarly, figure 14 (c) demonstrates that, in the discharging state, the cell with higher soc is discharged faster, while the battery with lower soc is slowly charged or nearly is not discharged. after reaching the equalization, the soc of each battery changes uniformly, which indicates that the bmmeec can avoid overcharging of the battery. however, compared to the idle state equalization, a portion of the balancing energy is consumed by the load causes; hence the equalization time increases."
"many systems have been presented in the literature based on picture archive and communication systems (pacs) [cit] . also text based retrieval systems are listed in the literature wherein the relevant text/keywords from the lab reports are retrieved against a query. however these systems have their own drawbacks since text annotation is difficult [cit], though, the usage of text will be useful to some of the patients eager to know about some related facts about their disease. hence, in this paper we present a novel methodology of cbir using generalized gamma distribution which helps to retrieve similar images based on content as well as text. the updated equations of the model parameters are estimated using expectation maximization (em) algorithm. the main advantage behind the usage of this distribution is of the fact that the human tissues are asymmetric in nature and it can handle the speckles more efficiently."
"the author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: this work was funded by the president's award from the institute of technology tallaght, dublin."
rear impacts had an error of 257% for the lhs xpatch and 12% for the rhs xpatch. the rotational acceleration from the xpatch had very poor accuracy and consistency when the headform was impacted to the side and rear. the largest error was a 71% underestimation compared to the reference sensor; this was recorded during impacts to the right side of the headform. the errors for all impacts are summarised in table 5 .
"in order to test the validity of the images a database consists of 1000 images pertaining to brain tumor disease is used. the images collected include, the inhomogeneities of the brain tumor such as seizures, ms-lesions and scelerosis. each image is of fixed size which is normalized and pre-processed. in order to have a effective recognition of the deformities, the features are to be extracted basing on position, color and texture, since any deformity in the brain image to be retrieved effectively needs to figure out the exact location for which the position of the (x, y) coordinates place a vital role where \"x\" denotes the width and \"y\" denotes the height."
"where δ a describes the overall soc divergence for the selected cells above the inductor of the equalizer e . note that is determined by the battery string itself, while δ a is determined by all other equalizers' control plans. for the sake of universality, the equalizer e is chosen to represent the equalizers other than e . in the following four situations, the cells are selected as research objects to study the influence caused by different equalizer on their soc. the discharging loops and charging loops are framed with red dash lines and green dash lines, respectively."
this study was exempt from institutional review board (irb) approval as it did not involve human participants as outlined by the code of federal regulations (45 cfr 46.102(f)).
"step . system conducts the termination judgment. if the soc variance decreases after the first round of equalization, the process will return to step 4. this procedure is repeated until the variance reaches its minimum value."
"in order to retrieve the relevant images more efficiently and effectively, the parameters of the medical images are to be estimated effectively. the parameters, expectation maximization algorithm is used for obtaining the final"
"these methods are deployed mainly on the brain image data collected from university of rennes1 database. every brain image consists of vital information such as white matter (wm), gray matter (gm) and cerebro spinal fluid (csf). in order to assess the damage of the brain tissues, effective methodologies are to be developed to discriminate wm, gm, and csf."
"li-ion batteries have been extensively used for energy storage systems in electrical vehicles, new energy power generation, and military applications because of their superior performance [cit] . however, energy inconsistency among the battery cells widely exists in energy storage systems and poses a potential threat to the system safety, which is the major concern of the future applications of the li-ion batteries [cit] . commonly, the cells are assembled into battery strings for instant power supply or energy storage [cit] . some of them might undertake energy inconsistency during recharge cycles and then fall into overcharged or overdischarged state, which can cause cell damaging or even explosion and service life of cells decrease, respectively [cit] . therefore, to ensure the battery service life and the system performance during the charging and discharging periods, the energy equalization of battery strings is mandatory [cit] . many researchers have proposed various battery energy equalizers that each equalizer has distinctive advantages and suitable applications [cit] ."
"in unnecessary energy transfer. in contrast, in the equalization process of the bmmeec, only one termination criterion is needed and no unnecessary energy transfer exists. this simplifies the control and avoids extra equalization process."
"in (9), is the mean value of the initial soc of all cells. according to the second assumption, except for the soc divergence δ a, all values are known as given data."
"the test conditions were designed to cover the sensors' full linear acceleration range of 20g-200g; this corresponded to the drop heights of 160-610 mm. the testing procedure consisted of a total of 10 drop heights, and each test was repeated 10 times. impacts were to four locations: left side, right side, front and rear of the head ( figure 2 ). thus, a total of 400 tests were conducted. a sample of the linear acceleration results from a drop of 360 mm is shown in figure 3, and the duration of the impact in this case is 12.5 ms."
"this study tested the accuracy and repeatability of the xpatch sensor developed by x2biosystems. the sensor is a six-degree-of-freedom measurement device, consisting of three single-axis accelerometers and three angular rate sensors. the device measures 37 mm by 14 mm and is designed to attach to the skin over the mastoid process (behind the ear) of the athlete. during an impact, linear acceleration in x, y and z is recorded, as well as rotational velocity about the three axes. data are recorded by the device for 100 ms with the sampling rates of 1000 and 800 hz for linear acceleration and angular velocity, respectively. the acceleration data are transformed to calculate linear acceleration at the centre of gravity of the head. rotational acceleration is calculated from rotational velocity using five-point differentiation. both the transformation and differentiation were carried using the software supplied by x2biosystems. the equations for the transformation and differentiation are unavailable to the user, as they are embedded in the software. the transformation is based on equation (1) (below), where a cg is the linear acceleration at the centre of gravity of the head, a p is the linear acceleration recorded by the device, v and a are the angular velocity and acceleration of the head, respectively, and r p-cg is the geometric relationship between the device and the centre of gravity of the head"
"the side impact results revealed a significant difference between the xpatches on the rhs and lhs of the headform, with the device on the opposite side to the impact performing better than the device on the impact side. the device on the side of the impact overestimated the velocity by an average of 47.5%, while the device on the opposite side of the impact overestimated the velocity by 23%."
"based on the existing detection technology, this paper proposes a javascript malicious code detection model based on lstm. to combat the obfuscation technique in malicious code, we analyzed it from the bytecode level based on the v8 parsing engine and used the optimized word2vec algorithm to extract word vector features. finally, we used the deep learning algorithm lstm to classify javascript malicious code. by testing the detection model and comparing with other classification algorithms, the feasibility and effectiveness of the lstm-based javascript malicious code detection model proposed in this paper are verified. however, the number of bytecode extracted based on the v8 engine is nearly 100,000, even if the amount of code is small. these bytecode contain a lot of redundancy. although this model selects a part of the bytecode for analysis, other bytecode may also contribute to the detection results. therefore, the bytecode still has a valid sequence which need to improve."
"min xue received the b.eng. [cit] . she is currently pursuing the m.a. degree with the college of electronics and information, sichuan university. her current research interests include web development and network security. volume 6, 2018"
"the detailed flow of the malicious javascript detection model at the training and detection stage is shown in figure 3 . in the training phase, the model first preprocesses the malicious sample set and the normal sample set, removes the duplicate samples in the sample set, and performs javascript code extraction on the sample files in the html format. dom and bom objects are called in the javascript code. perform object simulation. then use the v8 parsing engine under the nodejs platform to parse the javascript code to generate bytecode, remove the redundant parts while extracting the bytecode sequences, and mark them as text files. then use the tagged bytecode sequence file to train the improved word2vec model and generate word vectors. finally, the processed word vector is used as the input of neural network lstm to train the javascript malicious code detection model."
"word2vec is a tool designed for natural language processing. it is based on some word features in the native language environment. for example, in a large corpus, some common words (such as the stop words ''of'' and ''yes'') are very frequently appearing. these words provide very little useful information, and the word vectors correspond to these words. no significant changes occur when training on many samples. therefore, for the processing of such high-frequency words in the corpus, word2vec exploits a sub-sampling method to increase the training speed. the specific approach is to give a word frequency threshold parameter, and the word w will be discarded with the probability of formula (5) prob(w)"
"the solution is to introduce the packaging library jsdom of node.js, which is a pure javascript implementation of many web standards, especially the whatwg dom and html standards, which can simulate the browser rendering engine to parse documents into dom. it could be realized by adding the following code at the beginning of the code:"
"malicious javascript code detection method is an integral part of malicious web page detection, and there are many research results at present. the most two important factors are the feature extraction and detection model construction."
"v8 is google's open source high-performance javascript engine, written in c++ and used in google chrome, the open source browser from google. v8 can run standalone, or can be embedded into any c++ application. there are two main reasons for using the v8 engine for bytecode extraction:"
"take a simple piece of javascript code as an example. use nodejs to execute it. at the same time, add the -print-bytecode command line parameter to print its bytecode. the result is shown in figure 1 . by counting the number of occurrences of bytecode in all sample bytecode, table 1 lists the top 10 bytecode that have the highest frequency of occurrence, including the proportion of occurrences and their meaning."
"the schematic diagrams of the cbow model and the skip-gram model are shown in fig. 2 . they all contain an input layer, a projection layer, and an output layer. among them, the cbow model predicts the current word w t on the premise that the current word context w t−2, w t−1, w t+1, w t+2 is known; the exact opposite of the skip-gram model is to predict its context w t−2, w t−1, w t+1, w t+2 on the premise that the current word w t is known [cit] ."
"the total results for four classification algorithms are shown in table2. we use accuracy, precision, recall and other normal evaluation method to classify the malicious and benign code. from table 2, the accuracy, precision, recall, and value of the lstm algorithm are all higher than those of the svm algorithm. although the accuracy of the lstm algorithm is slightly lower than random forest algorithm."
"the output layer is a huffman tree. the words appearing in the corpus are the leaf nodes of the binary tree. the number of occurrences of each word in the corpus is the weight of the corresponding node. in this huffman tree, there are n child nodes (the size of n is the number of words in the corpus) and n − 1 non-leaf nodes. the vector corresponding to the leaf node is its word vector, and the vector corresponding to the non-leaf node is the auxiliary vector."
"the v8 compiler is lazy. if a function is defined but not invoked, v8 will not interpret it, effectively reducing the impact of redundant code."
"1) the paper studies the extraction of bytecode sequences based on the v8 engine. however, because the dom and bom objects in the browser environment cannot be identified, it is proposed to import the jsdom package library of nodejs for simulation. 2) the paper presents word-vector extraction of bytecode based on the word2vec model. to make it more suitable for the current detection model, the tf-idf algorithm is proposed to optimize the rejection of high-frequency words."
"there are roughly six types of malicious code features extracted from different layers. they are feature codes, code statistics features, sequence features, graph features, code semantic features, and opcode frequency features. the well-known snort intrusion detection system exploits pattern matching to perform attack detection [cit] . the statistical characteristics of code refer to the static analysis of malicious code, and the keyword frequency, special character frequency, and the code length in malicious code."
"we use 10-fold cross validation to evaluate the detection accuracy. the data sets are randomly partitioned into 10 equal-size parts. at each iteration, one single part is taken as the testing data, and the other parts are used as training data. the final result is the average over all iterations."
3) the study introduces the long short-term memory algorithm to detect javascript malicious code. the effectiveness of the proposed model was evaluated by constructing a detection model and setting up an experimental environment.
"the v8 engine introduced the bytecode interpreter not long ago. at present, there is no any research on javascript malicious code with v8 bytecode technology."
"lstm-based classification model training steps as below: 1) before starting training the lstm classification model, prepare the initial training sample set a0. 2) perform network configuration on the classification model, including activation function, loss function, and optimization function. 3) arrange the parameters of the classification model, including calculating the number of samples and training periods when the primary gradient falls. 4) calculate the output value of each neuron in advance, namely f t, i t, c t, o t and h t . 5) calculate the error based on the loss function. at the beginning of training, the output value will not be consistent with the predicted value, and the error term value of each neuron needs to be calculated according to the loss function. 6) according to the gradient guide of the loss function and the optimization function, the network weight parameter is updated. similar to the traditional rnn, the back propagation calculation of lstm error terms includes both spatial and temporal layers. at the spatial level, the error term is propagated to the upper layer of the network. at the time level, the current time before t is calculated. the moment of error. 7) repeat steps 4) -6) until the error is less than the given value to form the final classification model. the specific training flow chart of the lstm classification model is shown in figure 6 ."
"about detection technology, from the traditional patternbased matching to the now-fabricated machine-based learning, the detection of malicious code is moving towards a more automatic and intelligent direction. the requirement for detection results are not only the ability to accurately and ultimately identify the known types of attacks must be able to fight against all kinds of potential and suspicious attacks."
"liang liu received the m.a. [cit] . he is currently an assistant professor at the college of cyber security, sichuan university, china. his current research interests include malicious detection, network security, and system security and artificial intelligence."
"with the web application occupying the mainstream market with browser/server (b/s) architecture, browsers and web pages have become an essential channel for the spread of malicious code. attackers use website code vulnerability, third-party application vulnerabilities, browser vulnerabilities, and operating system vulnerabilities to perform cross-site scripting attacks on websites, inject web trojans, tamper with web pages, phishing, and steal personal information. [cit] china cyber security report'' [cit], rising ''cloud security'' intercepted a total of 80.11 [cit] . among them, there are 42.75 million linked websites and 37.35 million fraud websites. the total number of malicious urls in china is 13.5 million, which is second only to the 26.684 million in the united states. the malicious content contained in malicious web pages which could easily expose visitors to network attacks unwittingly, such as virus transmission, trojan implantation, information leakage, etc. their malicious code is mainly script language such as javascript and vbscript. to avoid detection, these malicious scripts are also obfuscated in different encode methods. therefore, this article proposed an lstm-based javascript malicious code detection method; the main work is as follows:"
"in order to evaluate the effectiveness of the proposed model, we crawled 23,993 files for the experimental data sets, including 20,000 benign files and 3993 malicious files. the benign files were extracted from different web pages of the top 200 alexa domains (https://www.alexa.com/topsites), and malicious files were crawled from malicious platform vx heavens (http://vxvault.net/virilist.php) and the code repository github (https://github.com/geeksonsecurity/ js-malicious-dataset)."
"although the new code redefine the document object and use document.getelementbyid('elementid').value to get the property methods value of the object. but there is an error reporting that the value property does not exist because a specific elementid element cannot be found. for this kind of situation, this article modified the source code of jsdom, for such a method does not need to return true and exact objects, but directly returns a custom object. the modified part of the document.getelementbyid method code is as follows:"
"the basic idea of hierarchical softmax is that for the word w in the dictionary d, there is a unique path p w from the root node to the leaf node (i.e. the word w) in the huffman tree, and each branch on the path is treated as a double classification. with a certain probability, the cumulative multiplier of these probabilities is the value of the objective function. the calculation of the objective function is based on such rules: 1) from the root node to the leaf node, each branch experienced in the middle is a binary classification; 2) all left nodes are marked as negative and the label is 1;"
"where m represents the dimension of the word vector. the projection layer accumulates 2c words vector of the input layer, as shown in formula (2):"
"there is no stop word like the natural language environment. that is, each bytecode provides useful information. 2) after observing the bytecode generated by malicious javascript code, it was found that the frequency of some bytecode is very high. in the past research based on opcodes to detect malicious code, some malicious codes were found to be used to avoid detection. a large number of repeated instructions, so the performance of high-frequency words has a specific influence on the classification effect of the model. therefore, the goal of the optimization in this paper is to keep the effective high-frequency words as much as possible and discard the invalid high-frequency words. the measurement of the high-frequency words is performed through the tf-idf algorithm. the three core concepts in the tf-idf algorithm are tf (term frequency), df (document frequency), and idf (inverse document frequency). tf is the frequency of a feature word appearing in a document, it can well represent the importance of the current feature word to a document; df refers to the frequency of a feature word appearing in the document set, it can well represent the distribution characteristics of the current feature word in the document set; idf refers to the frequency of reverse documents. it focuses on the ability to distinguish the current feature words. the td-idf algorithm is currently the most commonly used eigenvalue weight calculation method. its calculation formula can be expressed as formula (7):"
"in order to evaluate the effect of proposed model, the experiment introduced multiple algorithms to get the highest accurate. the four algorithms contain random forest, svm, naive bayes and lstm algorithm. in this paper, we train the lstm for 39 epochs, the batch_size is set to 32, and embedding_dims is 128."
"overall, the lstm algorithm has the best classification performance among these four algorithms, followed by the random forest algorithm and naive bayes algorithm. the above four algorithms are based on the roc curve (receiver operating characteristic curve) as shown in figure 8 ."
"most javascript codes in web pages need to implement web page interaction effects through dom objects, and bom objects to control some browser behaviors, but the v8 engine only parses the built-in javascript objects and methods [cit] for objects in the browser environment. dom and bom are not recognized (the nodejs environment provides a global object), that is, when nodejs directly executes document.getelementbyid statement, it will report an error because the document object cannot be found, and the execution of the subsequent statement stops the sequence of bytecode."
"at last, the lstm-based javascript malicious code detection model has achieved an accuracy rate of 98.19%, an accuracy rate of 98.53%, a recall rate of 97.81%, which proved that the model can effectively detect confused with malicious javascript code."
"the principle that the original word2vec rejects highfrequency words is that the higher the word frequency is, the higher the probability of rejection is. the optimized model introduces td-idf weights, which reduces the probability of discarding valid high-frequency words. the final rejection probability function can be expressed as formula (8):"
"word2vec [cit] . it combines the advantages of neural network language model and log linear, and exploits distributed representation as a representation of a word vector that expresses a word with a continuous, dense vector."
"as can be seen from figure. 8, the area under the roc curve (auc) of the lstm algorithm is also larger than that of the random forest algorithm, svm algorithm, and naive bayes algorithm, which is proved that malicious javascript detection technology based on lstm is useful, with highly accurate."
"the following takes sample (context(w), w) as an example to introduce the principle of the cbow model based on the hierarchy softmax optimization method. where (context(w), w) can be expressed as formula (1):"
"word2vec provides continuous bag-of-words (cbow) and skip-gram training models for the calculation of word vectors. it also provides two sets of optimization methods, hierarchy softmax and negative sampling, to provide training effectiveness of word vector. therefore word2vec has a total of four training frameworks: cbow+hs, skig-gram+hs, cbow+ns, and skip-gram+ns."
"long short-term memory (lstm) is an improved type of recurrent neural networks (rnn) to resolve the problem that rnn cannot handle long-distance dependence [cit] . the traditional neural network model is based on the assumption that the input from the input layer to the hidden layer to the output layer is independent of each other. however, such assumptions are unreasonable for the handling of many practical problems, because most of the information has the chronological and sequential nature, that is, the task of information at the moment to influence this moment. rnn has emerged to solve this problem; it can make information in the network again. the logical structure of the rnn is shown in figure 4 ."
"in the detection stage, the data preprocessing and the v8 bytecode extraction is all in the same training phase. the word vector extraction part extracts the word vectors according to the word vector model trained in the training phase. the final classification decision part is trained according to the training phase. the javascript malicious code detection model performs the determination of malicious code and normal code."
"to determine the \"correct\" number of clusters in a data set directly from the data is an ill-defined problem, since what should be considered to be a separate cluster depends on the interpretation of the data. nevertheless, there are many different criteria which can be used to guide the decision about the number of populations [cit] . we use overlap between components-measured by bhattacharyya distance-and unimodality of the resulting super clusters-measured by hartigan's dip test [cit] -to determine which latent clusters to merge and to indicate our confidence in the mergers."
we ran bayesflow and aspire on a 3.2 ghz quad core cpu. a bayesflow run took 0.5 h for the gvhd dataset and 1.4 h for healthyflowdata. aspire took in total 2.4 h for the gvhd dataset and 6.6 h for healthyflowdata per run. four runs of aspire was needed to determine the κ i parameters. hdpgmm was run on a dual core gpu. it needed 0.72 h for the gvhd dataset and approximately 1 h for the healthyflowdata dataset.
"to get a further understanding of the variability between samples in bayesflow, summary statistics for the obtained components and cell populations are shown in fig. 11 ."
"for the second data set the mcmc sampler was run on amazon cloud, using 192 cores. each iteration took on average one second, so that about 2.7 h was needed in total."
"various knowledge bases published by different individuals or groups on the web have the problem of heterogeneity. many effective methods are used to solve the heterogeneity problems between multiple knowledge bases, which can promote the development of the linked data project. currently, the iterative entity alignment method based on transe has become the mainstream approach, but it has the disadvantage of ignoring the importance of semantic aggregation for entity alignment. aiming at this problem, an easa algorithm is proposed in this paper that has improved the accuracy of encyclopedia entity alignment. our main conclusions are as follows:"
"as seen in table 4, our algorithm easa has fewer parameters in the entity alignment process. from table 3 and table 4, we can observe the following: (1) compared to transh, transd has better entity alignment performance but requires more parameters and limited performance for improved entity alignment. for example, transd on hits@1 of dataset-1 and dataset-2 compared to transh increases by only 0.67% and 1.75%, respectively. (2) our entity alignment algorithm easa not only has fewer parameters but also has important performance improvements for entity alignment. for example, our easa algorithm improves by 8.47% and 10.4% on hits@1 of dataset-1 and dataset-2 respectively, compared to transh."
"the output of bayesflow, aspire and hdpgmm can be compared in fig. 13 . the merging procedure we used for bayesflow has been applied for both aspire and hdpgmm, however for aspire no components were merged by this. in bayesflow each of the populations correspond to clear expression patterns, which is not the case for the other methods. for example the first population is clearly cd4+cd8-t-cells whereas for both aspire and hdpgmm this population contains both components which are cd8-and components which are cd8+. we also compare intra-donor variation of cell population size to inter-donor variation for the six bayesflow runs, as well as for aspire and hdpgmm in fig. 14 . for aspire there are inter-donor distances which are clearly smaller than some intra-donor distances, which is not the case for bayesflow and hdpgmm."
"prior parameters and initial values for the mcmc sampler are given in the additional file 1: section d.1. all priors were chosen to be non-informative. the outlier component was not used for inference in the small dataset, but it was used for the large dataset. the mcmc sampler ran first for a number of burn-in iterations, then the posterior distribution was explored in a number of production iterations. during the production iterations, apart from sampling parameters of the model, a value of y was also drawn, i.e. a sample from the posterior predictive. for the first synthetic data set 10,000 burn-in and 100,000 production iterations were used. for the second, larger, data set we used 5,000 burn-in iterations and 5,000 production iterations."
"for each component not representing outliers its mean and covariance matrix is linked to a latent cluster which collects corresponding components across all samples. in practice this is done by assuming a normal prior for the means and an inverse wishart prior for the covariance matrices of the components linked to a given latent cluster. the parameters of sample and latent components are jointly estimated by markov chain monte carlo (mcmc) sampling. the variation in location and shape between corresponding mixture components across samples is controlled by the priors on parameters of the latent clusters. the location of component means and shape of components can also be restricted if there is prior information supporting this. to allow for that flow cytometry data frequently have missing cell populations, we include the possibility that not all components are present in every sample."
"clean process binary image 4) do the segmentation and label the regions: on this step, the segmented image will be identified (labeled) in the right zone of the body, tracking the roi through z axis. it starts taking a look at every roi in the sliced image, separating them and combining in the right position on the z axis, it will produce a cloud of points of each zone of the human body. in that way, was produced 8 zones (through z axis) that represents the total human body. on the other hand, considering the x axis, was produced 16 zones, right and left side in a symmetric way and including a specific zone (zone 9 in figure 3 ) were completed the 17 zones."
the changes to (1-3) required by this extension are straightforward but inference of the model becomes a bit more involved since removing components reduces the dimension of the model. to accommodate for this we have included a reversible jump step in our sampling algorithm. details are given in the additional file 1: section b.
"after that, all sliced image are saved in directories zones and threat zones as showed at figure 5: fig. 5 . directory division that contain zones 6) test and training dataset classifier: after have all the zones labeled and written in a file, it can be loaded to fit a classification model. the training and test data will be separated in two portion: 60% and 40%. the training data will be with 60% of the total data and the test will take the rest of portion. the trained data was submitted in a configured model called cnn, with the solver type stochastic gradient decent (sgd) using the network alexnet and frame work tensorflow. the alexnet network consists of 7 layers, 5 convolutions by 5 max-pools and 2 regression layers at the end. the figure 6 shows the flow of alexnet work: fig. 6 . alexnet [cit] flow of convolution and pulling steps from the original paper."
"the resulting posterior distribution of all the parameters, denoted jointly by, and x given the data y is given in the additional file 1: section a. in section b we describe the markov chain monte carlo (mcmc) sampling scheme used to generate posteriors for our model parameters."
"in fig. 4 we show the same univariate and bivariate histograms, but this time with samples from the posterior predictive distribution of y. from the synthetic cell measurements generated from the inferred models of the datasets it is clear that the inferred models are accurate and capture the variation across samples, which a model only of pooled data cannot do. figure 5 displays dots at the posterior mean locations of the mixture component centers μ jk whose posterior probability of being active is greater than 1 %; the true locations of the active clusters are displayed as circles. the model is able to detect which clusters that are active and which are not, and to find the location of the component means. the true and estimated cluster centers of the eightdimensional data set cannot be displayed efficiently with just three dimensions at hand, but a three-dimensional projection is shown in fig. 8 . the average error in euclidean distance in the full eight-dimensional space is 0.007, which can be compared to the average error had the latent mean across samples been used, namely 0.110, which is the best that could have been obtained from a model not including variation between samples. the outlier component was used for inference in the results presented here, but omitting it has very small effect."
"for the analysis of the gvhd dataset we did twelve runs of bayesflow in the informed setup described above. seven were excluded due to confusion between populations, i.e. at least one sample component was closest to the wrong latent component; of the remaining five, one more run was excluded since it has not converged, and another two because of multimodal clusters. this leaved two runs that passed the quality control. additional file 1: figs. s2 and s3 show trace plots and projections of clusters with high dip test values respectively. table 1 reports the accordance with manual gating for the two bayesflow runs as well as what is obtained from aspire and hdpgmm, as well as the top two performing one of the two bayesflow runs has the highest accordance with manual gating, the other one is on par with flowmeans and samspectral, which is considerably higher than aspire and hdpgmm. however, as can be seen in fig. 10, the gating of different samples is arguably most consistent for bayesflow as compared to manual gating, flowmeans and samspectral."
"we collected data from the military fields in baidu encyclopedia 2 and hudong encyclopedia 3 and extracted triples from the infobox, named dataset-1 with a small scale. similarly, we collected the entertainment dataset from baidu encyclopedia and hudong encyclopedia, named dataset-2 with a large scale. in addition, dbp-yago 4 contains aligned entities between dbp and lgd, geo, and yago, respectively. taking dbp-yago for example, we randomly extracted thousands of reference entity alignment from the english version of dbpedia to yago, named dataset-3. our experimental datasets are shown in table 2, where n − entity represents the number of entities, n − relation is the number of relations and n − triple represents the total number of fact triples. for convenience of calculation, we merged the triples of hudong encyclopedia and baidu encyclopedia."
"in this paper we have presented a new bayesian hierarchical model designed for joint cell population identification in many flow cytometry samples. the model captures the variability in shapes and locations of the populations between the samples and we have demonstrated its use in an exploratory as well as in a partly informed setting with some prior information. we showed that for synthetic datasets generated from the model, the parameters were recovered with high accuracy through a mcmc sampling scheme. the model was then applied to a real flow cytometry data set where a manual gating was available, and it was shown to have very high accordance with manual gating as compared to other automated gating methods, while at the same time the gating was more consistent across samples than either the manual gating or other automated gating methods. when applied to another flow cytometry data set with technical replicates of blood from healthy donors, bayesflow gave a parsimonious representation of the data, which enables visualization and monitoring of its parameters. the obtained cell populations had clear expression patterns as opposed to the clusters obtained by aspire and hdpgmm, where for example cd4+cd8-t-cells where in the same cluster as cd4+cd8+ t-cells. the population sizes obtained by bayesflow and hdpgmm respectively had lower intradonor variation compared to inter-donor variation than what was obtained from aspire."
"additional file 1: supplementary material. the supplementary material contains the posterior in bayesflow, the mcmc sampling scheme, additional details on the merging of components, information about the data generation, priors and initialization for the synthetic data example; parameters used for aspire, additional details on healthyflowdata, the priors and the initialization procedure used when studying this data set and further results pertaining to the real flow cytometry data set, including fitting gaussian mixture models to individual samples of healthyflowdata with the em algorithm and scatter plots of gvhd for aspire, hdpgmm and bayesflow run 1. (pdf 7505 kb) additional file 2: data generation files. a python script for generating the large synthetic dataset, along with means, covariances and weights needed for this. (zip 10kb)"
"the dataset used in this research comes from dhs and kaggle competition at: https://www.kaggle.com/c/passengerscreening-algorithm-challenge/data/ . according to dhs: \"this dataset contains a large number of body scans acquired by a new generation of millimeter wave scanner called the high definition-advanced imaging technology (hd-ait) system. they are comprised of volunteers wearing different clothing types (from light summer clothes to heavy winter clothes), different body mass indices, different genders, different numbers of threats, and different types of threats\". it will start with a small sample of the body's scanned separating the data into two parts, to be used as a training set and testing set. after that, we can use more samples of the huge database to get better accuracy. for the stage1 of kaggle competition we have a total of 929 bodies with threat to be analyzed. here, in this research, we will limit for a sample of 200 bodies to be analyzed; it will produce a total of 287660 sliced images of the bodies."
"automated methods with the aim to replace manual gating must be able to treat multiple samples jointly and take variation between samples into account, while at the same time make it possible for the user to monitor that variation so that it is not too high for the application at hand. for example it needs to be decided if a shift in location of a population in a sample can be seen as technical variation and accepted or if the changed marker expression means that it is a different cell phenotype. these kinds of methods also need to be able to take prior information into account-in manual gating the experience of the operator can be necessary to define a population. we have developed bayesflow, a method which models variation in cell population location as well as shape, can include prior information for example about cell population location, and gives a result that can be assessed in compact and comprehensive visualizations."
"this section introduces the implementation process of the entity alignment algorithm easa. it is divided into five parts: overall framework, entity semantic aggregation, introduction of attribute attention, calculation of loss function and implementation details. overall framework. this diagram shows the semantic calculation process of the easa algorithm. it introduces entity semantic aggregation and attribute attention where (r 11, r 12, r 13 ) represent attributes of the head entity h 1 and (t 11, t 12, t 13 ) represent the attribute values of the head entity h 1 . u is called the weight-detected vector, and each attribute vector will be dot producted with u to get the weight of the attribute. in particular, the parameters of u will be updated during the entity alignment training process to better capture the weights of the attributes, which helps improve the performance of entity alignment. in entity semantic space, it can be seen that head entity h 1 is close to semantic aggregation head entity h + 1 generated by positive sampling, but far from semantic aggregation head entity h − 1 generated by negative sampling. volume 8, 2020 vector parameters. we will give a more detailed presentation below."
"considering the proposal in the introduction of this research, this paper work intended to present a model to do the classification of the regions of the body's images using supervised machine learning to identify the threats hired in the bodies. moreover, produce an algorithm that fractionated the human body image in regions to be able to identify the body's region correctly. in this paper work, it was developed a method to do the segmentation of the body in zones. it was presented 34 zones (considering 17 zones with left and right regions plus the threats zones) to do the learning process. using robust module cnn with stochastic gradient decent (sgd) and alexnet, the model could learn the zones with threat or not. combining with filters, the model could classify those zones of the bodies with a robust result that can help tsa agency security in all airports of north america and world. a big data files was used to train this deep learning to extract features using convolution neural network that can produce a faster learning provide by nvidia digits framework. one important point to be considered here is the fact that the model worked for the trained images of the sample used, it was not considered any external image."
"for the entity alignment process, better differentiation of attribute weights, greater semantic differences between different entities, and more adequate training of data will help to improve entity alignment effects."
"we have completed a semantic integration between different knowledge bases through the entity alignment technology. in the future, we will build an advanced knowledge base question-answering system based on this unified semantic knowledge base."
"partitioning the cell measurements in a sample into cell populations is essentially a clustering problem. in the context of flow cytometry data analysis clustering is called automated gating, as opposed to the manual gating performed by operators. model-based clustering using mixture models has been the most used approach for automated gating [cit] . mixture models are very well suited to describe flow cytometry data because they have a natural biological interpretation based on the cell populations. examples of other approaches that have been used for automated gating are grid based density clustering [cit], spectral clustering [cit], hierarchical clustering [cit] and k-means clustering [cit] . an evaluation of a wide range of automated gating methods was performed in the flowcap i challenge [cit] . the discrepancy with manual gating was often quite large even for the best methods, with average f-measures around 0.9 for both completely automated and manually tuned methods. large discrepancies between manual and automatically gated samples can be acceptable since the arbitrary decisions taken in manual gating means that the gates could just as well have been set another way. however, it is important that the gating is consistent between samples so that they can be compared against each other."
"bayesflow is not aimed at discovery of rare cell populations, but it can be used together with an algorithm specifically designed for detecting rare cell populations in a sample, such as swift [cit], and then use informative priors to find how this population occurs across an entire set of samples, in a similar way as was done in the gvhd dataset."
"if any of the quality criteria is not met, the simulation should be rerun, either using the same or different parameters. even if the same parameters are used a different result can be obtained due to randomness in the initialization."
"many approaches of automated gating of multiple flow cytometry samples in parallel have been aimed at finding features of the data so that either samples can be classified into groups, e.g. cancer or normal, or they can be used to predict an outcome such as expected time to progression of disease. features are often designed based on characteristics of cell populations, but usually not so much attention has been given to ensure that they represent actual cell populations. bayesflow takes the opposite approach and gives a representation of the data according to cell populations, with the same cell populations across the entire set of samples (except when some populations only occurs in a subset of the samples). the advantages to this approach are among others that the result is directly biologically interpretable and that a rich output is given which can be explored in many different ways which are familiar to someone who is used to manual gating. in this way we can join the objectivity and ability to work in high dimensions and with many samples of automated gating with the flexibility in interpretation of manual gating."
"we then take the two entities with the minimum distance score in the different knowledge bases to be aligned entity pairs. finally, we evaluate the performance of the different entity alignment methods."
"where θ k, θ k, k and ν k are hyper-parameters describing latent cluster k. these parameters describes the variability between flow cytometry samples, in contrast to μ jk, jk which describe the distribution of cell measurements within a sample. the normal and inverse wishart distributions are conjugate priors to the mean and the covariance respectively of the normal distribution, enabling efficient sampling, however they are not jointly conjugate."
"in fig. 12 we visualize model fit and inter-sample variation for the first of the six runs that passed the quality control by plotting latent and sample components as well as histograms of real data and synthetic data generated from the model, for two different samples and for the pooled data. we can thus see how shape variations are captured by the model."
"as shown in figure 2, attributes (r 11, r 12, r 13 ) and attribute values (t 11, t 12, t 13 ) have the same header entity h 1, resulting in (h 1, r 11, t 11 ), (h 1, r 12, t 12 ), (h 1, r 13, t 13 ), where the semantics of an entity are derived from the aggregation of its attributes and attribute values. the semantic aggregation computation of head entities is derived from equation (1), and equation (2) is also valid. figure 2 shows that many attributes (r 11, r 12, r 13 ) and attribute values (t 11, t 12, t 13 ) share the semantics of the header entity h 1 on average, which is not very reasonable."
"various large-scale knowledge bases are published on the internet, such as dbpedia [cit], yago, cn-dbpedia [cit], xlore [cit], and zhishi.me. however, these knowledge bases are designed and created independently by different individuals or groups, which makes the heterogeneous problems among knowledge bases serious and largely affects the unified integration of multi-knowledge bases at the semantic level. this problem hinders the development of web 3.0 of the ''web of knowledge''. as the main method to solve this problem, entity alignment technology has great research value and has become a hot research direction."
"the database created here used a lmdb format and was loaded to produce a total of 287660 images that was submitted to training, validation and test. below at figure 7, its shows the result of the split images (from digits). analyzing the 3 histograms, we can see that the data is unbalanced, especially with the threats. the digits used a total of 34 zones (with threat or not). the training zones were fitted with a total of 172596 images (60%), using splitted images to train; to validation were used splitted images in a total of 57532 (20%) and for test was used 57534 images (20 %) and a format of 256x256 png image."
"the sum of the indexes divided by the total number of aligned entities is meanrank. the smaller the mean-rank value is, the better the alignment algorithm is."
"the main idea of knowledge base embedding is to represent entities, relationships and attributes in the knowledge base as low-dimensional, dense vectors. transe [cit] has become the mainstream technology of knowledge base embedding due to its simple and efficient features, and many researchers have extended it. transe assumes h + r t in vector space with the problem of a strict requirement on the relationship of triples, so transh [cit] partially alleviated this problem by mapping head and tail entities to the relationship plane. in addition, transd [cit] asserted that the meaning represented by the relationship is no longer unique. for example, the location relationship can represent the relationship between mountains-countries or regions-countries, so transd uses a dynamic mapping matrix to embed the knowledge bases."
"another point is that the result produced was highly accurate for predict zones, it can indicate an overfitting of learning. to prove that, it is necessary to do more test using external bodies, besides that the model need a balance data, that is, more information about the threats. this research showed in average an accuracy of 100% of the bodies zones, as well as, a recall and precision with the same 100% result for the most of zones. for future work, it is proposed here the follow: one classifier that combine two different models: one model that classify only the zones of the body and another model that only classifies the threat or not, increase the body images from external samples for produce a balance data."
"bayesflow can be computationally intensive if many runs are needed to pass the quality control. for these cases it would be desirable to complement bayesflow e.g. with initialization methods that would allow passing the quality control more often, so that few runs in bayesflow would be needed. fast initialization methods and early quality checks aiming at this would therefore be of interest for the community and is something that we propose for further study."
"in order to verify that the proposed sampling scheme can find the correct model parameters, the mcmc algorithm was applied to two simulated datasets. the first dataset was three-dimensional, which enables direct visual evaluation. it had four latent clusters across eighty artificial flow cytometry samples; each sample had 15,000 cells giving a total of 1.2 million cells. one of the latent clusters was present only in eight samples and another one was present in 24 samples, so that the ability to find rare cell populations was tested. moreover, the cluster which was present in only eight samples contained only 1 % of the total number of cells, thus also the ability to find small cell populations was tested. the parameters and the algorithm used for generating the data are given in the additional file 1: section d.1."
"we call θ k and k /(ν k − d − 1) the latent cluster mean and latent cluster covariance matrix respectively, since they are the a priori expected values of μ jk and jk ."
"for example, in addition to variation in location bayesflow explicitly models variation in cell population shape, whereas aspire models shape variations implicitly by combining gaussian components with the same shape. this means that an aberrant shape variation of a cell population in a sample can be detected in bayesflow by examining the parameters of the model, which is not possible in aspire. perhaps more importantly, bayesflow gives a parsimonious model which much fewer parameters-each individual parameter for the components in bayesflow can be assessed through compact visualizations and thus undesired behaviors can be detected and corrected for by change of setup. moreover, a restriction in aspire which is avoided by bayesflow is that the variation of component location within and between samples is connected to the shape of the components."
"we used dataset-1 with a small scale and dataset-2 with a large scale for entity alignment experiments. the number of dataset-2 triples is 20 times the number of dataset-1 triples. as seen in table 3, (1) hits@1 and hits@10 of our entity alignment method are higher in dateset-2 with a large scale than in dataset-1 with a small scale. (2) in dataset-1 with a small scale, the entity alignment performance is susceptible to data noise, but in dataset-2 with a large scale, the effect of data noise on the entity alignment performance is diminished."
"we verified the ability of the sampling scheme to recover model parameters by fitting the model to a small threedimensional synthetic data set with 1.2 million cells in total and a large synthetic data set with in total 28 million cells in 8 dimensions. then we applied bayesflow to one of the datasets in the flowcap i challenge, the gvhd dataset, which contains samples from patients who have had organ transplants and might have early signs of graft-versus-host disease. we show that bayesflow does not only give a result which has the same degree of accordance with manual gating as the best performing methods in flowcap i-which is much higher than what is obtained for other methods based on joint gating with bayesian hierarchical models-it does also give a more similar treatment of different samples than manual gating and the best methods from flowcap i. finally we applied bayesflow, aspire [cit] and hdpgmm [cit] to a data set with replicated samples from four healthy individuals. the ratio between intra-donor technical variation and interdonor biological variation was similar between bayesflow and hdpgmm, which was lower than for aspire. moreover, bayesflow was the only of the three methods which gave cell populations with clear expression patterns."
"the algorithm attre divides the data into relational and attribute triples, and performs model training in different semantic spaces, resulting in partial semantic missing for entity alignment. at the same time, there is insufficient data training, causing attre's meanrank indicator to be too high with a score of 47 in dataset-1 and a score of 210 in dataset-2 from table 3 . however, in our easa alignment algorithm, relational triples and attribute triples are trained in the same semantic space, not only fully training the data with a lower meanrank score, but also providing the attribute weights and semantic integration information needed for the entity alignment process, thus enhancing the effects of the entity alignment."
"in bayesflow, the cells in a sample are clustered using a multivariate gaussian mixture model (gmm), where k components describe true and artificial cell populations and one component describes outliers. artificial cell populations are measurements that cluster together and behave otherwise like real cell populations, but arise for example from dead cells, non-specific binding of markers or doublets; doublets are pairs or groups of cells that pass through the flow cytometer at the same time. measurements which are not clearly grouped but spread out over the measurement space, for example due to measurement noise, are modeled as outliers."
"full list of author information is available at the end of the article applications-for example monitoring hiv infection and diagnosing blood cancers-and in many branches of medical research [cit] . defining the cell populations based on the measured characteristics is in state-of-the-art analyses still done manually by trained operators looking at twodimensional projections of the data. the importance of automated methods has risen along with an increase of the dimension of typical flow cytometry data sets due to developments in flow cytometry technology [cit] and the emergence of studies with large numbers of flow cytometry samples [cit] . furthermore, manual so called gating of cell populations is a subjective process where operators have to take more or less arbitrary decisions for example when there are overlapping populations [cit] . automatic cell population identification is hard since flow cytometry measurements are not absolute, while at the same time different samples cannot be directly compared due to technical variation-especially apparent when samples are analyzed at different laboratories [cit] -and intrinsic biological variation within and between subjects. despite this, research into automated population identification methods has focused on individual or pooled flow cytometry samples, sometimes attempting to align data at first through normalization procedures [cit] ."
"bayesflow finds a joint representation of an entire set of samples. in order for this representation to be reasonable there has to be sufficient correspondences between samples. even if for a data set with very little correspondences a joint model could be obtained by using a very large number of components, it would hard to gain any insights from such a model. in such a case an entirely computational pipeline without the cell population identification step would be preferred."
"to produce the right and left regions, was used a mask to guide the right roi to the left roi. adding on this idea, the body was segmented on the z axis in a proportion of the total length of the body. in this way, the mask should guide in (x, y) position as well as the z axis position. the figure 3 shows the 17 zones of the human body. each zone can have threat or not, so in this research, to classify the threat, were considered 34 zones, that is double the 17 zones of the body, it will be explain better on next steps."
"here is the block diagram of this paper. it tries to summarize all the steps needed to describe the algorithms in every step, those algorithms are writing using python language. a first approach is to divide the bodies in 17 regions that can contain threat or cannot contains threats."
"a challenge that has to be addressed when analyzing flow cytometry data is that cell populations can be skewed and/or have heavy tails and are then not well described by a single gaussian component [cit] . to handle this we use multiple components to model such populations, an approach that have often been employed for flow cytometry data [cit] and has the further advantage that the number of cell populations can be automatically detected. we merge gaussian components into super components with a procedure based on a systematic study of methods for merging mixture components [cit] ."
"entity alignment (also known as entity resolution or entity matching) contributes significantly to the development of reading comprehension, machine translation and intelligent question-and-answer systems. the basic task is to identify the associate editor coordinating the review of this manuscript and approving it for publication was chao tan . whether two entities from different datasets have semantic matching. for example, the entities ''liaoning aircraft carrier from baidu encyclopedia'' and ''hudong encyclopedia china liaoning aircraft carrier'' have different names, but both point to the same object in the real world, so they should be merged. however, entity alignment is a complex, time-consuming and error-prone task with complex computation, a lack of prior alignment data and poor data quality [cit] . in view of these three problems, the main methods of entity alignment are to divide the data into blocks to reduce the amount of computation, to manually mark the data for supervised learning and to solve some data quality problems by means of machine learning. the research presented in this paper is mainly based on the latter."
"digits plots statistical information in every convolution layer parameter, this include frequency, mean and standard deviation. this is a helpful tool to understand the results of convolution response applied in the input image. the classification results displayed at the figure 10 shows a response from the first convolution layer including the weights, activations and statistical information. \"in a well-trained network, smooth filters without noisy patterns are usually discovered. a smooth pattern without noise is an indicator that the training process is sufficiently long, and likely no over-fitting occurred. in addition, visualization of the activation of the network's features is a helpful technique to explore training progress. in deeper layers, the features become sparser and localized, and visualization helps to explore any potential dead filters (all zero features for many inputs).\" (sarraf and tofigh [cit] ) the smoothness of the convolution images shows below follow the same pattern."
"e. discussion table 3 shows a comparison of the easa algorithm and other entity alignment algorithms in chinese and english datasets. intuitively, the more obvious the difference between attributes, the more favorable it is to distinguish the semantics of different entities, which is more conducive to the improvements of the entity alignment effects. the term weight_1 in table 5 represents the attribute weight trained by the entity alignment algorithm easa, and the term weight_2 represents the attribute weight trained by the entity alignment algorithm attre. from table 5, we can get that the weight_1 values are greater than the weight_2 values, and the weight_1 difference of different attributes is more obvious than weight_2. for example, the weight_1 difference the difference of the former is larger than that of the latter, which indicates that the attribute attention calculation method proposed by easa algorithm is more effective. note that, this is also true when weight_1 and weight_2 are normalized to the same value range."
"5) multiply raw image with labeled image to save in the database: this step prepares the final image and save in a template directory to be loaded to a digits interface. a multiplication process get the labeled image and multiply with the raw image to get the real intensity of the image, it focuses on the roi of the image. the table 3 shows the flow of the process:"
"after process the train with digits, to test the model, we can use only 1 image. in figure 8, was used the classify one image to show visualizations and statistics when its selected. when running this process it will generate plots of weights from the responses of the network flow input image. follow below the figure 9, it shows example output from the first layer."
"the accuracy rate of the research on entity alignment described above is not very high because of two disadvantages: each approach either does not introduce semantic integration with weights or requires a significant amount of manpower to mark training data. aiming at these problems, an entity alignment algorithm based on semantic aggregation and attribute attention (easa) is proposed to improve the accuracy of entity alignment. the easa method uses semantic aggregation generated by many attributes and attribute values of entities and an attention mechanism for the entity alignment process while using an unsupervised method to reduce the cost of data marking. compared with other entity alignment methods, the proposed entity alignment algorithm, easa, can achieve state-of-the-art effects on real datasets and has advanced and superior performance."
"in a flow cytometer a number of characteristics for each individual cell in a sample of ∼ 10 4 to ∼ 10 6 cells are quantified as they pass through the cytometer in a fluid stream. the data that are obtained are most often summarized by grouping cells into cell populations; properties of these cell populations are used in many clinical *correspondence: johnsson@maths.lth.se † equal contributors 1 centre for mathematical sciences, lund university, box 118, s-221 00 lund,"
"in an evaluation of criteria for merging gaussian components to represent more complex distributions, the bhattacharyya distance performed well [cit] . bhattacharyya distance merges clusters according to a patternbased cluster concept as opposed to a modality-based concept [cit] . with a pattern-based cluster concept a small dense cluster inside a sparse cluster-for example a well specified cell population inside a region with sparse outliers-will be considered to be different clusters. this would not be the case for the modality-based cluster concept as long as the generating probability density is unimodal."
"we analyze two flow cytometry data sets with bayesflow: the data set gvhd from the flowcap i challengewith four markers, 12 samples and approximately 13,000 cells per sample-and a data set obtained from the r package healthyflowdata [cit] with technical replicates of pbmc samples from healthy donors-in total 20 samples with approximately 20,000 cells, also measured with four markers. in the gvhd dataset we can compare the gating obtained from bayesflow with manual gating provided from flowcap as well as automated gating from a wide range of other methods. in healthyflowdata we can instead compare gating between technical replicates to see if samples are treated in a consistent manner."
"bayesflow follows this third approach, but use a differently structured model than what has been used previously, favoring explicit modeling instead of implicit, parametric instead of non-parametric (or massively parametric). this follows the philosophy that mathematical models can never perfectly fit reality, thus it is important to be able to convey the constructed model and its parameters and in what ways it simplifies the data."
"the training, validation and test were performed in a ubuntu machine that has 16gb of memory, a i7 processor and a nvidia gpu of 6gb. after 30 epochs, the model shows at the figure 8, an average accuracy for the validation data of 98.2863% and a loss of 0.091319. this was obtained with a shuffled data using alexnet with tensor-flow. the data transformation used a subtract mean image and data augmentation to do the flipping for the threat zones and produce more balance data, as well, using a contrast of 0.8 factor in the images."
"the greater the degree of integrated semantic discrimination between different entities is, the better the performance of the entity alignment algorithm is. table 6 compares the ranking scores of the correctly matching entities in the entity alignment algorithms attre and easa. the term rank_1_score is the entity similarity score of the 1st correctly matched entity, and the term rank_2_score is the similarity score of the 2nd wrong matching entity. the term difference_value is the value obtained by subtracting the rank_1_score from the rank_2_score. the larger the difference value is, the better the effect of the entity alignment is. the results in table 6 demonstrate the effectiveness of the semantic integration calculation method proposed in the easa algorithm."
"to get a real result, it was used a different test data to enforce the validation of the model. the figure 11 shows the tops accuracy zones, it obtained a top-1 accuracy of 99.31% and top-5 accuracy of 100%. after that, the confusion matrix at figure 12 shows all the accuracies by class zones, most of them has results upper to 98%. fig. 11 . classify many images summary. figure 13 shows an average of 100% in recall of the images as well as 100% in precision. figure 14 shows in more details all precision and recall by zones. it can be analyzed more precisely about how much true positives are recovering from the test images, as well as, the precision of this true positives. it can analyze all zones individually and most of them has a 100% in area for both rates."
"we begin by analyzing the smaller data set. in fig. 3 we show univariate and bivariate histograms of all synthetic cell measurements pooled together, as well as the corresponding histograms of the data from a single flow cytometry sample where all four clusters are present. note that the data when pooled together has a complicated density, as it is in fact a mixture of 232 multivariate normal densities."
(3) this experiment shows that our proposed entity alignment algorithm with semantic integration and attribute attention not only is effective but also has fewer parameters and faster calculation speed.
"in fig. 9, we show the posterior distribution of the latent cluster means where again the dot represents the difference between the median of posterior distribution and the true value of each θ k . the vertical lines are the 2.5 and 97.5 % quantiles. the posterior samples have been divided by the standard deviation of the true θ k so that the scales across the clusters are equal. some of the credibility intervals do not contain zero, but this is explained when studying the intervals that would have been obtained if the true μ k were used (shown in red), since they are almost identical."
"the first one, that populations intuitively should be separate if they have very different densities-even when they overlap so that their combined distribution is unimodalcan be bypassed by combining unimodality with a patternbased merging criterion such as bhattacharyya distance. the second one, that it is difficult to determine if a multi-dimensional empirical distribution is multimodal, is usually handled by considering one-dimensional projections [cit] . this is the approach we take here, using hartigan's dip test of unimodality for each of the projections onto the coordinate axes where bhattacharyya overlap is low, and for the projection onto fisher's discriminant coordinate. if for a proposed merger, any of these projections is found to be multimodal, the clusters are not merged. further details of the merging procedure are given in the additional file 1: section c."
"where w denotes the wishart distribution and d denotes the dirichlet distribution, which is conjugate prior to the multinomial distribution. for each ν k we assign a exponential prior on the positive natural numbers. the complete structure of the model is displayed through a directed acyclic graph (dag) in fig. 1 . the parameters t k and s k define the prior belief of the locations of the latent means θ k, whereas the parameters q k and n θ k control the spread of mixture component means within a latent cluster and are hence important to control the variation across samples. a large n θ k along with a small q k forces the μ jk together; it makes large deviations between θ k and q k unlikely. the parameters h k and n k control the expected values and the variation of latent covariance matrices as well as the variation among mixture component covariance matrices in a latent cluster. if n k is large each jk will be close to k /(ν k − d − 1) for any k, since a high n k makes high ν k more probable."
"the computational bottleneck of the sampling scheme is the sampling of x, with a computational complexity bounded by o(jd 3 k max j n j ). to handle high dimensions diagonal covariance matrices can be used instead, in which case the complexity is bounded by o(jdk max j n j ). however, for datasets with more than 20 dimensions the mathematical feasibility of using gaussian mixture models without any prior dimension reduction needs to be seriously considered first, due to the curse of dimensionality [cit] ."
"results from the mcmc sampling and subsequent merging are evaluated in a number of quality tests. this is a crucial step since what is deemed as a good clustering is application dependent. in some settings a given amount of variation in location or shape is expected from biological or technical reasons, whereas in others the same variation would indicate a different population. this also means that it is necessary for the user to choose prior parameters for their application. to simplify this process we have derived parametrizations so that the same value of the parameters gives a similar effect of the prior on data sets of different sizes."
"we thus see that cluster centers and credibility intervals for latent clusters are captured well in both synthetic data sets. fig. 8 the posterior mean of the mixture component centers, μ jk (dots), and the true cluster centers (circles) in the large synthetic data experiment for the first three dimensions fig. 9 the difference between the true value of each entry in each θ k and the approximated marginal posterior distribution generated by the mcmc sampler in the large synthetic data experiment. the black dot represents the median and the vertical line goes between the 2.5 and 97.5 % quantiles. to get the axis on the same scale for all the clusters, they are scaled by the standard deviation of μ k . the light gray horizontal line is the 0 line. the red dot and lines is the same however where one uses the true μ k to estimate θ k, rather then the μ k obtained by taking the posterior means of the mixtures"
"the second data set was designed to test the ability to handle large data. it was eight-dimensional, with eleven latent clusters and 192 artificial flow cytometry samples. each sample had measurements of 150,000 cells, giving a total of 28 million cells. four of the eleven clusters were missing in half of the samples. additional file 2 contains a python script and data for regenerating this data set."
"from different runs of bayesflow we can get different representations of data, as in the case of the gvhd dataset. this is because with highly overlapping populations there might be multiple models representing the data equally well. but since all samples are gated jointly in every run, the gated populations can still be compared across samples. the user might have a preference for one representation or the other though, and informative priors can be used to guide bayesflow to a preferred representation."
"1) it uses relu non-linearity for non-saturating nonlinearity, in this way, the training time of the gradient descent is much faster than the saturated non-linearitys. 2) it reduces the over-fitting using data augmentation and preserving transformation using label. those transformation of the data do not need to be saved on a disk. 3) it dropout predictions, that is a very successful way to reduce test errors. about the solver stochastic gradient decent (sgd), leon bottou [cit] point that the big data size of learning is a bottleneck, so sgd can perform very well with train time."
how much clusters should be merged is a decision that needs to be taken by the interpreter of the data. in some settings one might want to be restrictive with merging and then use higher thresholds. in others one might want additional mergers after viewing joint one-dimensional projections of the clusters.
the operational procedure of data-driven optimization algorithms is divided into seven general steps: (i) generate an initial experimental design; (ii) evaluate the points generated in previous step; (iii) use the evaluated points to build a regression model; (iv) utilize the regression model to predict the objective function values of unsampled points in the variable domain and to decide which point to be evaluated next; (v) evaluate the function at the point selected in step (iv); (vi) update the regression model using points generated in previous step; (vii) repeat steps (iv) to (vi) until the stopping criterion is fulfilled [cit] . various data-driven optimization algorithms are differed from each other in the following aspects:
"we then asked whether a model which has scale-invariance (example-based) for familiar objects shows intrinsic scale-invariance for a new set of objects. to test this hypothesis, we evaluated cnns for korean letters. note that these models were scale-invariant for the trained mnist dataset. the results obtained with the cnn model ( fig. 6 right) show that classification accuracy when the letter size changes was higher than chance but significantly lower than accuracy for enn and psychophysical data. this limitation suggests that cnns with data augmentation cannot account for scale-invariance in one-shot learning."
data-driven optimization aims to build a regression model by using the information of the evaluated solutions in the iterative optimization process and utilizing the regression model to approximate the real objective function. the data driven optimization algorithm used in this paper utilizes radial basis function (rbf) as a surrogate to approximate the objective function f (x). rbf was chosen mainly due to the following two considerations:
"we also tested 3 korean subjects to confirm that the designed task is trivial and find the range of visibility window for subjects who have prior experience and memory of korean letters. note that for koreans, we used the same experimental setup and task; yet, it was not testing invariant object recognition in one-shot learning, but visibility of the letters in different sizes and positions."
"although our results support built-in scale-invariance for computational models, the exact implementation details of the enns architectures tested here need to be further verified. in particular, pooling all scales at the last layer gives a high degree of scale-invariance, but this may well be different from the operations performed in the visual system. the dynamics of invariant recognition of familiar objects in the human visual system were studied in 19, and the study suggested that the human visual system develops invariant representation in stages corresponding to different visual areas in the ventral stream. thus, comparing neural recordings from the ventral stream with different layers in the models will be necessary for refining models that are fully consistent with the brain computation."
"kappa is a measure of accuracy or agreement based on the difference between the ground truth and the predicted results. we count the actual number of pixels belonging to the unchanged class and changed class (s u and s c for short, respectively) according to the map. kappa is calculated as eq. (10):"
"convolutional neural network (cnn). the parameters used in cnn were the same as enn, except that there was no multi-crop input channels or pooling over scales, since the model had only one scale channel. the resolution of the input to the model was chosen such that it matched that of the 5th scale channel in enn, which is its mid-resolution."
"general experimental procedure. accuracy for recognizing letters was measured in a same-different task. subjects were instructed to first fixate a black dot at the center of the screen. after 1 sec, the fixation dot disappeared and a target letter was presented for 33 msec, followed by a white screen for 1 sec. then, the fixation dot reappeared for 1 sec, followed by a test letter for 33 msec, again followed by a white screen for 1 sec. finally, the question of the task appeared, in which the subject was asked if the target and test letters displayed previously were the same or different. in fig. 1c a sample sequence of letter presentations is shown. every trial was composed of new letter pairs, and randomly choosing if the test letter was the same as the target or the distractor. the presentation time was limited to 33 msec to avoid eye movements, which ensured that the subjects would view the letters at the designed eccentricity."
"in both scale-and translation-invariance experiments, the order of stimuli was randomized. the number of same and different trials as well as presentation on the left and right visual field was balanced. each condition had the same number of trials."
"1) initial experimental design, i.e., method to generate initial solutions. frequently used ones are symmetric latin hypercube design (slhd) [cit], space-filling design (spacefil) [cit], corner point strategy (corner)."
"since in a natural setting, humans are able to observe the unknown objects with their fovea, we first focus on analyzing the central learning condition (fig. 3 top) . for all scales, recognition accuracy was the highest at the center, when there was no displacement, and decreased with increasing distance from the center of the visual field. in addition, the range of translation-invariance increased with the scale of stimuli. while recognition accuracy was maintained high at a position as far as 7° in the periphery for 2° letters, it dropped significantly even at 1° for 30′ letters. considering the area where recognition accuracy is above a threshold (e.g. 0.85) as the range of invariance, we observed a roughly v-shaped area. we found the same tendency that recognition accuracy depends on eccentricity and scale in peripheral learning conditions. additionally, overall recognition accuracy was significantly lower under peripheral learning than under central learning, particularly when there was a change in resolution of test letters from that of target letters ( fig. 3 peripheral window) i.e. translation-invariance was more limited under peripheral learning. in a related setting with peripheral learning, when target letters are learned in the peripheral visual field and test letters are presented at the same distance from the center but in the opposite side of the visual field, the range of invariance was less limited. note that under this condition, the resolution of letters did not change and only their position was changed to the opposite side of the visual field. the corresponding window of invariance ( fig. 3 opposite window) was still more limited than the results from central learning conditions."
"change detection of synthetic aperture radar image is a technology that samples information in different time periods from the same geographical area. by comparing the surface information in different time periods, it can detect the changed area. same area monitoring and change detection on the earth surface is one of the major applications of remote sensing imagery [cit] . also, it is widely used for many applications in environmental and forest resources monitoring [cit], agricultural surveys [cit], and urban dynamic change detection [cit] . as per the available literature, currently there are two major ideas for change detection in sar images [cit] . firstly, the classic process, which is composed of three steps:"
"the limitation of cnns in contrast with enns in explaining scale-invariant recognition highlights the significance of an architectural prior (innate or developed during an early stage of visual experience, see 1, 2 ) . cnns are designed under the assumption that objects should have the same features regardless of their position (assuming antialiasing is taken care of properly as convolutional architectures designed without considering the classical sampling theorem can also suffer from aliasing effect 36 ). for other transformations, it is in principle possible that the models learn an invariant representation through rich training data. they would then be able to extract features invariant to transformations. a theory describing architectures capable of this feature was in fact developed in 1 . we found that, however, invariant recognition in cnns is highly constrained to the exact type of dataset that are used for training, and there is very limited transfer invariance to other datasets, even when they are similar. this suggests that cnns mainly develop example-based invariance, limited to a memorized set of data. our psychophysical results, on the other hand, indicate that human invariant recognition supports an alternative design choice which is consistent with neural networks that enforce scale-invariant representation, as in enns."
"to study intrinsic invariance we analyzed results for recognition of unfamiliar letters in one-shot learning. for the one-shot learning task, we flashed a target korean letter and then a test korean letter, which was either the same as the target or a different distractor, to non-korean subjects who were unfamiliar with korean letters. to investigate invariant recognition to transformations, we varied scale and position of the letters. when testing recognition in the peripheral visual field, we randomized to which side of the visual field letters were presented to prevent that subjects predict the letters' position, fixate on the stimuli, and observe them with their foveal vision. we limited the presentation time to 33 ms to avoid eye movements. in fig. 1 we depict the experimental set-up and a set of korean letters used. experiment 1: scale-invariance. we tested scale-invariant recognition by flashing both target and test korean letters at the fixation point in the center of the screen. first, we used 30′ and 2° letter size. in fig. 2 we the test letter was either the same as the target or its pairing distractor letter. (c) experimental procedure. each target and test letters was presented for 33 msec after a fixation dot was presented for 1 sec at the center of the screen."
"current deep networks exploit architectural priors for intrinsic invariance. for instance, convolutional neural networks, which are widely used in computer vision, have an architecture hard-wired for some translation-invariance while they rely heavily on learning through extensive data or data augmentation for invariance to other transformations 4 . networks that incorporate a larger set of intrinsic invariances, such as rotation-invariance, have been proposed [cit] . nevertheless, it is not clear which type of intrinsic invariance should be encoded in more biologically plausible models. as a consequence, it is important to characterize the degree of invariance in human vision, starting from the simplest invariances-scale-and translation-invariance-and evaluate models that reproduce them."
"to understand the underlying brain computation that enables human invariant recognition characterized in psychophysical experiments, we compared the experimental data with computational modeling results. in particular, we investigated whether invariance properties observed in human one-shot learning can be learned by examples seen by the model or alternatively, requires an intrinsic architecture for them. we used convolutional neural networks (cnn) to simulate the experimental results, as these models showed a significant success in explaining visual processing in the primate ventral stream [cit] and matching behavioral patterns of object recognition with humans 28, 29 . a trivial way to achieve invariant recognition, widely adopted in computer vision field, is to use data augmentation to train cnns 4 . although models can reach human-level invariant recognition performance for familiar objects with this method, the strategies of cnns in using diagnostic features were shown to be different from humans 30 . moreover, it is unknown whether invariant recognition can be transferred to a new category of stimuli, unseen in the training phase. to show the limitation of this example-based invariance in one-shot learning, we compared cnns with eccentricity-dependent neural networks (enn) [cit] . enns, depicted in fig. 5, are modified from cnns to have scale-invariance built into their architecture and have dependence of receptive field size on eccentricity, consistently with physiology data 23 . both cnns and enns were trained on mnist handwritten digit dataset 34 with data augmentation of various scales and positions. with this training, the networks should develop top-layer features capable of processing character-like stimuli. those features are then used to evaluate the similarity of two korean letters, as in the psychophysics experiments. two korean letters are considered to have the same identity if their associated features have pearson correlation higher than a threshold. here, we report results from applying a different threshold that maximizes accuracy for each condition. we also included distractor letters in testing so that we evaluate selectivity of the models."
"statistical analysis. no statistical methods were used to predetermine sample sizes (number of subjects), but our sample sizes are similar to those reported in previous studies using similar experimental procedures (studies testing recognition of familiar letter stimuli 21, 22, 47 and testing invariant recognition of objects 10, 13 ). we analyzed the percentage of correct responses, combining both same and different trials. for all parametric tests, data distribution was assumed to be normal, but this was not formally tested. to analyze the difference in mean accuracy among three or more conditions, we computed analyses of variance (anovas) or repeated measures anovas, depending upon whether the data were acquired from different group of subjects or the same groups, respectively. correlation between features in simulations was pearson's r."
"data-driven optimization is an optimization algorithm for expensive black-box functions, and it is a dynamic iterative process. in recent years, data-driven optimization has attracted widespread attention, especially in the hyperparameter optimization of machine learning models, mainly because of their outstanding performance of finding the optimal solution of high-cost functions [cit] . it has also been incorporated with other methods to reduce the time cost or to be applied to solve real-world problems [cit] . at present, deep neural network (dnn) has become very popular because it can express powerful features. however, it is difficult to obtain proper hyper-parameter configuration unless we have a deep understanding of data or deep neural network. in most cases, the deep neural network relies on manual parameter adjustment."
"kappa has been adopted as the main criterion to evaluate the accuracy. apart from this, we also need to know the following concepts shown in table 3 ."
"while it is widely agreed that humans are able to process complicated visual information invariant to transformation, so far it remained rather unclear whether this is possible because of previous exposure to the specific visual stimuli at different viewpoints or whether the visual system computes invariant representations for novel objects. to address the issue, we characterized the degree of invariance to transformation in one-shot learning, using stimuli for which the subject had no previous experience. we found that there is significant scale-invariance in recognition. we also found limited translation-invariance that increases with decreasing spatial frequency content of the stimuli, as expected (see for instance 1 ). overall, as a function of eccentricity, the window of invariance is narrower than the window of visibility (i.e. acuity). further, we observed an asymmetry between learning in the fovea and testing in the periphery with respect to the opposite sequence of training and testing."
"our work on enns have implications for eye-movements. enns show greater positional invariance to low-resolution images, which suggests a particular strategy for driving saccades, from low to high frequency channels. although for each fixation only a small fragment of the input image is processed at high-resolution, information about the peripheral visual area extracted by low-resolution channels enables the models to plan the next saccade towards an informative position in the visual field. in this way an image can be efficiently processed without the need of processing the entire visual field at high-resolution 38 ."
"when choosing parameters of the network, we confirmed that enn and human pyschophysical data empirically matched by comparing the window of visibility for digit recognition. for 30′ digits, it was measured that at around 10° from the center of the fovea, recognition accuracy was 67% for humans 22 . if we do a linear interpolation for approximation, accuracy would be about 77% at around 7° for the same size of digits. using our conversion ratio between pixels and visual angle, we observed accuracy of 72% for 30′ mnist digits at 7° for enn, roughly matching the human accuracy. this conversion ratio together with the parameters in the network are also consistent with the theoretically estimated size of the smallest receptive fields 46 ."
translation-invariance experiment. translation-invariant recognition was evaluated by keeping the size of target and test letters constant and changing the position of test letters with respect to the position of target letters. we divide the tested conditions into two categories:
"since translation-invariance experiments had more conditions than scale-invariance experiments, and the same set of 27 korean letters was used, the set was repeated in two separate sessions. first, subjects were tested on 27 trials and instructed to come back for the second session after taking a break of at least 40 minutes, to ensure that they did not remember the letters."
"on the implementation level, enn is based on a cnn. the primary difference between enn and cnn is that the input to enn is multi-scaled centered crops of the input images. figure 5b shows an example set of multi-scaled crops of input images. this way, the center of an image, which corresponds to the foveal region, is sampled at multiple resolutions. the peripheral part of an image is sampled only at a low resolution. different scale channels have shared weights and in addition to spatial pooling, the model has pooling over different scales. for the results of simulations we partly used the implementation provided by 33 ."
"(1) when dealing with complex nonlinear relations, rbf does not require any relationship between independent and dependent variables. however, we assume that the objective function is a sample function of the gaussian process in bayesian optimization."
"one may consider it may be the case that our method finds the optimal threshold that differentiates letters of the same or different identity, even when the underlying representation between translated objects are actually not being more dissimilar with eccentricity. we additionally confirmed that this is not the case by assessing the raw data, which is pearson correlation between the same korean letters at different positions (fig. s9) . the results verified that correlation between the representation becomes lower with eccentricity."
"simulation 2: translation-invariance. in our psychophysical experiments, the degree of translation-invariance increases with letter size, both under central and peripheral learning. in our simulations ( fig. 7 bottom), cnns were not able to replicate the property of limited translation-invariance. accuracy for larger stimuli was higher than that for smaller stimuli, but it did not decrease with eccentricity. these results were expected due to translation-invariant model prior of the cnns. for enns ( fig. 7 top), on the other hand, accuracy decreases with eccentricity while the range of invariant recognition increases with the size of letters, consistently with the psychophysical results. as in psychophysical results, if we choose a threshold classification accuracy and draw an accuracy contour, we can observe a v-shaped area of invariance. (we report raw data in fig. s7 . window of invariance for enn ( fig. 7 top) is based on the linear regression of the raw data)."
"furthermore, we investigated whether the models can reproduce the asymmetry in recognition rates between central and peripheral learning. the first idea we explored is that the one-shot learning stage stores templates obtained from processing the visual field at multiple scales. thus, when target letters are presented at the center of the fovea, the associated templates contain all the full range of spatial frequencies. however, when target letters are shown at an eccentricity, since only the central visual field is sampled at high resolution, the templates are effectively low-pass versions of the foveal ones. therefore, an explanation of the asymmetry between central and peripheral learning may start with the different range of resolutions available for templates memorized in the two situations of foveal vs. \"peripheral\" learning."
"enn that we tested has four layers and a fully connected layer at the end, resembling v1-v2-v4-it-pfc in the human ventral stream. the size of stimuli or receptive fields are measured in pixels, so we introduced a hyperparameter for the conversion between number of pixels and visual angle, which is 450 pixels to 1°. with this correspondence, we could compare modeling results with human data more directly. for instance, to extract features of 30′ letters, we placed letters of size 225 pixels in the simulated visual field for the model. as discussed previously, the input to the model is multi-scaled centered crops of images, and we use 10 crops, increased in size exponentially by a factor of 1.5. the entire visual field processed by the model is approximately 19°."
"where, and θ iτ is the parameter vector to be estimated. substituting (2) into (1) yields, with some algebraic manipulations, the following generalized criterion function:"
"our experimental settings controlling familiarity to the objects as well as position and size of them clarify and extend previous studies on invariant recognition. previous studies 9-11 examined invariant recognition to translation when visual stimuli were first learned with the peripheral vision. however, unlike those experimental conditions, humans can freely observe unknown objects, and they mostly use foveal vision for learning target objects, since it is almost exclusively in a laboratory setting that peripheral training may happen. therefore, our results on the asymmetry between central and peripheral one-shot training suggests a difference between natural and unnatural conditions (we refer as natural condition when the object is centered at the fixation point, and unnatural otherwise). while conclusions from previous studies on very limited position-invariance are drawn from peripheral training condition only, we observed stronger invariant representations in a more natural setting. also, by testing two subject groups who differed in familiarity to the visual stimuli, we confirmed that invariance depends on familiarity with the visual stimuli, consistent with 10 ."
"(1) we propose a simple and effective threshold analysis method based on data driven optimization. the threshold value t calculated by the proposed algorithm is not limited to integers, i.e., the gray levels. specifically, in the proposed algorithm, we regard the required threshold value as a decision variable, and take the kappa coefficient as the objective function based on the given threshold value. using this, the mapping relationship from threshold value t to the kappa coefficient is established. here our main goal is to find a suitable threshold t * that will maximize the kappa coefficient. we perform this search process using a datadriven optimization framework. when compared with the gkit method, we found that our proposed method performed reliably well."
"we compared the experimental results with computational models based on neural networks. one of our key contributions is that we conclude that standard cnns cannot account for these experimental data on invariance, whereas a related class of neural networks, that we call enns, can. this suggests that enns might be better suited www.nature.com/scientificreports www.nature.com/scientificreports/ for computationally modeling the visual cortex than cnns, which have been widely used for modeling the ventral stream [cit] . furthermore, our results suggest a rather different computational strategy from the one used in these models. in particular, the limited invariance to eccentrically located targets implies that several quite small \"effective images\" at different resolutions are available to later visual processing rather than a single large image at a fixed resolution 32, 35 . if objects are recognized at multiple resolutions in these effective images -i.e. they are not bound to any specific resolution-the models become scale-invariant."
"2) regression model. the two most commonly used surrogates are gaussian process regression (gpr) model [cit] and the radial basis function (rbf) model [cit], and their variants."
"invariance to geometric transformations can be a huge advantage for a visual recognition system. it is important to distinguish between invariance due to the underlying representation, which we refer to as intrinsic invariance, and example-based invariance for familiar objects that have been previously seen under several different viewpoints. the latter is computationally trivial and is available to any recognition system with sufficient memory and large training data. the first one, which may be hardwired or learned during a developmental period, provides a learning system the ability to learn to recognize objects with a much smaller sample complexity, that is with much smaller training sets 1,2 . this is not only a big advantage for any recognition system but it is also a key difference between today's best deep learning networks and biological vision systems: the most obvious advantage of children versus deep networks is the ability to learn from a (labeled) training set that is several orders of magnitude smaller 3 . the prototypical observation is that we can easily recognize a new object, such as a new face -seen only once -at a different scale."
"www.nature.com/scientificreports www.nature.com/scientificreports/ model experiments. to contrast the human behavioral data on invariance with computational modeling results, we evaluate eccentricity-dependent neural network (enn), which was proposed by 32 and previously studied in 31, 33 . in particular, we demonstrate that enn is robust to change in scale, and validate that it captures the major characteristics of translation-invariance observed from human experimental data. we test a convolutional neural network (cnn) as a control to show that invariance properties of enn, especially scale-invariant representation of novel stimuli, are derived from the architectural design of the model rather than a consequence of training with multiple scales and positions."
"the second set of experiments is performed to prove that even if you are not good at tuning or lack of experience in the field of change detection, you can still construct a well-established change detection algorithm through data driven optimization. the results with various datasets are shown in table 5, and fig.13 to fig.16 are change detection maps of them."
"to create the stimuli set we used 27 korean letters as target objects, each of them paired with another korean letter as distractor, depicted in fig. 1a . for each trial, a sequence of one of the 27 target letters was shown first as target, followed by the test letter, which is the same letter or its pairing distractor. the letters were black arial presented in different positions and sizes on a uniform white background in a 60 hz dell u2412m monitor. we used the psychophysics toolbox 43 for matlab 44 running on a linux computer. subjects were seated at a distance of 1.26 m with a chin rest for stable viewing. www.nature.com/scientificreports www.nature.com/scientificreports/ scale-invariance experiment. to test scale-invariance, both target and test letters were presented at the center of the monitor, and the size of letters was varied. we pursued two blocks of experiments to test invariance to scale in recognition. in the first scale experiment block we tested letter sizes of 30′ and 2°. specifically, the combinations set of target and test letter sizes were (30′, 30′), (30′, 2°), and (2°, 30′), in which the first element represents the target letter size, and the second the test letter size. similarly, in the second scale experiment block we used letter sizes of 30′ and 5° with combinations of target and test sizes (30′, 30′), (30′, 5°), and (5°, 30′), respectively. the same group of subjects participated in both blocks of scale experiments, with at least a day apart to ensure that the subjects did not remember the stimuli set."
"data-driven optimization, also known as surrogate model optimization (smo), is an excellent global optimization algorithm for expensive black-box functions. the basic idea of data-driven optimization is to construct a regression model using the information of the evaluated solution in the iterative optimization process, and then use the regression model to approximate the real objective function. regression model is also called proxy model. the main function of the regression model is to predict which solutions are potential optimal in each iteration of the algorithm, and then the real objective function is applied to evaluate these potential optimal solutions, so the number of objective function evaluations can be reduced."
"as mentioned in section ii.b, gkit algorithm needs an appropriate hypothetical distribution of the pixel values in difference image di. given a specific change-detection application, a user needs to adopt the gkit version that best fits the land covers involved in the application. moreover, the threshold value calculated by gkit algorithm is just an integer (gray level). to tackle these, we propose a new threshold method based on radial basis function data-driven optimization model (trbfdo), which regards threshold value and kappa as the decision variable and objective function, respectively. the general flow chart for a trbfdo (used in this paper) is presented as fig. 2, and algorithm 3 depicts the process involved with trbfdo in detail."
"additional future direction of the study would be investigating diagnostic critical spatial frequency in enns for object recognition. it was previously observed that critical bands of spatial frequency were scale-dependent except for face images 37 . the critical spatial frequency was measured by testing visual recognition of objects embedded in noise. though the scale channel selected in enns depends on the object size, our results predict that critical frequency is scale-invariant since spatial frequency is normalized by the object size. due to the different experimental setup, however, it is hard to directly compare our results with previous studies on spatial frequency. in particular, it is unclear how a background of noise would affect the scale channel selection in enns. recognition of such images may involve multiple frequency channels to separate target objects from background. therefore, analyzing the behavior of enns for more complex images will be relevant."
"simulation 1: scale-invariance. as described earlier, the psychophysical experiments show that the human visual system is immediately invariant to scale change in one-shot learning. we first tested whether the results with enns, which are of course designed to be scale-invariant, fit the data. we evaluated the degree of scale-invariance for korean letters, which the models did not see during training. as expected, accuracy when the target and test letters are of different size turned out to be significantly higher than chance ( fig. 6 left) . although classification accuracy for testing invariant conditions was lower than that for the baseline condition, when the letter size does not change, this was partly due to the difference between biological systems and computational models. in www.nature.com/scientificreports www.nature.com/scientificreports/ simulations, since there was no noise, the input images for target and test letters were exactly the same under the baseline condition, which resulted in 1.0 classification accuracy. overall high accuracy in the testing shows that scale-invariance properties of enn are consistent with the human data."
"we use log-ratio operator to generate di. before this, a median filter is applied to reduce the noise of two original images i 1 and i 2 ."
"models. eccentricity-dependent neural network (enn). enn (depicted in fig. 5 ) builds on two key properties of retinal sampling 32 . one is that there are receptive fields of different sizes for a specific position 45, and the other one is that the size of receptive fields for each position increases with eccentricity 23 . the model achieves invariance through weight-sharing and pooling across different positions and scale channels. as we hypothesized that the model captures invariant representations to transformations, we tested this model for the comparison with behavioral data on invariant object recognition."
"surprisingly, the available psychophysical results are often incomplete and inconclusive. most experiments have targeted only translation-invariance, and a review 8 states that based on experimental data, the role of object position is not well understood and there is little evidence supporting the idea that human object recognition is invariant to position. findings from previous studies range from \"this result suggests that the visual system does not apply a global transposition transformation to the retinal image to compensate for translations\" 9 . to \"for animal-like shapes, we found complete translation invariance\" 10, and finally to \"our results demonstrate that position invariance, a widely acknowledged property of the human visual system, is limited to specific experimental conditions\" 11 . furthermore little research was conducted on scale-invariance with regard to unfamiliar stimuli"
"the longer the prefix, the more contextual information is given to the predictor. note that dg, ppm and all-kth-order markov use a predetermined portion of the prefix defined by the order of each algorithm. thus, by increasing the prefix length, we can observe that none of these algorithms get an increase in any performance measures. cpt takes advantage of a longer prefix by finding more precise (longer) patterns in its prediction tree, to yield a higher accuracy. the accuracy of cpt gets higher as the prefix length is raised. but after the prefix reaches a length of 8 (specific to the dataset), the accuracy decreases. this is because the algorithm may not be able to match a given prefix to any branches in the prediction tree. it means that this parameter should be finely tuned for each dataset to maximize the accuracy. the figure on the right of fig. 4 shows the influence of the prefix length on the cpt spatial complexity. optimizations. the fourth experiment assesses the influence of the recursive divider optimization (cf. section 3.3) for cpt. the recursive divider aims at boosting the coverage of predictions using the cpt by ignoring items that could be noise during the prediction process. but it also indirectly influence accuracy. figure 5 shows the effect of the recursive divider on the fifa dataset by sequentially incrementing the maxlevel parameter. we can observe that the accuracy and the coverage of cpt are getting higher as the maxlevel parameter is raised. also, the coverage and the accuracy measures quickly stabilize without affecting the testing time. this strategy's parameter can therefore be set to a really high value to guarantee the best coverage and accuracy and it does not need to be adjusted for each dataset."
"a wireless sensor network (wsn) is a group of sensor nodes that are geographically distributed to provide data gathering and monitoring of tasks and events. these wsns can be used in a variety of applications such as atmospheric monitoring, human detection and video surveillance."
"however, high ranking features such as rep, mov, add, etc. remain consistently high over the different program run lengths and the lowest ranking features such as lea, loopd, etc. remain consistently low over the different program run lengths. considering the mid-ranking features, it can be seen that significant variations occur with different program run lengths."
"the above measures are used in our experiments as well as the spatial size (in nodes), the training time (in seconds) and the testing time (in seconds). the spatial size is calculated in nodes because the spatial complexity of all predictors can be represented in terms of nodes. this measure is meant to show the spatial complexity and is not used to determine the exact size of a model."
"module identification and comparison are the most widely used downstream analysis of gcn, as they reveal the potential co-regulation relationship among genes. in this section, we explored the biological interpretation of the modules discovered from the time-specific coexpression networks constructed by tgcn."
"the selection of an appropriate kernel is key to the success of any machine learning algorithm. a linear kernel generally performs better at generalising the training phase into good test results where the data can be linearly separated. however, as shown in fig. 5, the data is not linearly separated. therefore, an rbf kernel (a nonlinear decision plane) is used as it yields a greater accuracy than a linear kernel, as illustrated in figs. 5 and 6 ."
"in this paper, we focused on the correlation based gene network, as it is the most plausible model choice for the small sizes. one issue with tgcn, along with other correlation based network models, is the interpretability of the network links, as there are many biological and technical factors that may contribute to the empirical gene-gene correlations. besides improving the data pre-processing to reduce the impact of the undesirable factors, we will also study more comprehensive models for temporal gene network analysis in the future. for example, there have been intensive statistical literature in the joint estimation of multiple gaussian graphical models (ggm) with similar structures [cit] . these methods typically require relatively large sample sizes. it would be interesting to explore the possibility of extending our idea to ggm estimation. another possible direction of future research is improving the biological intepretability of gene networks via data integration . we will explore the incorporation of the epigenetic data, metabolic pathway, gene oncology and protein-protein interactions in our modeling framework."
"in this paper, we exploit the opportunity to explore a crosslayer solution for the load balancing problem. a cross-layering method does not restrict a layer from utilizing information only from the layer directly above or below it."
"it can be seen (fig. 6), that adding more features does not always improve the results. the performance of both the detection accuracy and the fn rate peaks at 13 features (average), above which the performance degrades. this degradation is pervasive in all the program run lengths. it is believed that this is likely due to over-fitting caused by too much variance being introduced by the additional features. again, the smallest variance occurs with 13 features (average)."
"specifically, for load balancing and energy efficiency, we allow the network layer access to the physical layer for battery parameters and distance between nodes in performance of energy-efficient routing strategies. this allows us to: 1) create routing paths that conserve transmission power, and 2) favor those nodes with higher residual energy to perform high energy consumption tasks. in addition, we allow the network layer to access the application layer to perform data aggregation. performing data aggregation reduces the size of network data packets, which reduces the energy required to transmit each packet through the network."
"encryption-based malware often use the xor (opcode) to perform their encryption and decryption. table 2 shows that xor frequently appears in the shorter program run lengths. this frequent appearance of xor is expected as the unpacking/decrypting occurs at the start of a program. an exception is that the 4k-opcodes length program does not use xor to classify benign and malicious software. figure 7 presents opcode categories in terms of their ability to detect malware, which is constructed from the information presented in table 2 . figure 7 is calculated for each category and then normalised using the total area of all the categories. the results show that the flow control category is the most effective at 59 % followed by logic and arithmetic at 31 %. this implies that a program structure (flow control) is the most significant indicator of benign and malicious software followed by the logic and arithmetic components of the program, which concurs with bilar [cit] findings."
"this is also known as a false alarm and can have a significant impact on malware detection systems. for example, if an antivirus program is configured to delete or quarantine infected files, a false positive can render a system or application unusable."
"the malware industry has evolved into a well-organized $billion marketplace operated by well-funded, multiplayer syndicates that have invested large sums of money into malicious technologies, capable of evading traditional detection systems. to combat these advancements in malware, new detection approaches that mitigate the obfuscation methods employed by malware need to be found. a detection strategy that analyzes malicious activity on the host environment at run-time can foil malware attempts to evade detection. the proposed approach is the detection of malware using a support vector machine (svm) on the feature (opcode density histograms) extracted during program execution. the experiments use feature filtering and feature selection to investigate all the intel opcodes recorded during program execution."
"the leach algorithm is a well-known clustering algorithm developed specifically for wsns. leach routing elects a ch and nodes associate with the ch according to the leach algorithm [cit] . each node picks a random number between zero and one. each node also computes a threshold number (t n ), which is a number between zero and one and is proportional to the current round. the probability for any node to serve as a ch is denoted as p. if a node has been a ch in the last 1 p rounds, it is excluded from being a ch during the round. otherwise, if the temporary random number is less than t n, the node is elected as a ch during the round. the desired probability for a node to be chosen as a ch is an input to the algorithm and must be specified. the original author of leach performed analysis to determine the optimum value for p to be 0.5 [cit] . nodes that were not elected as chs during each round then associate in clusters with the nearest ch."
"the prediction tree is recursively defined as a node. a node contains an item, a list of children nodes and a pointer to its parent node. a sequence is represented within the tree as a full branch or a partial branch; starting from a direct child of the root node. the prediction tree is constructed as follows: given a training sequence, we check if the current node (the root) has a direct child matching the first item of this sequence. if it does not, a new child is inserted to the root node with this item's value. then, the cursor is moved to the newly created child and this process is repeated for the next item in the training sequence. the construction of this tree for n training sequences takes o(n ) in time and is done by reading the sequences one by one with a single pass over the data. the space complexity of the pt is in the worst case o(n * averagelengthof sequences) but in the average case the pt is more compact because the branches often overlap by sharing nodes. two sequences share their first v nodes in the pt if they share a prefix of v items. the pt is incrementally updatable and is fast to construct."
"we implement a time-division multiple access (tdma) scheme that assigns each node in the wsn a timeslot during each round. the node transmits information to the gateway during the timeslot. we simulate the mac layer simply through the performance of transmission rounds. each simulation begins at round zero and ends at some maximum number of rounds (or when the last node dies). during each round, each node in the wsn sends an l-bit packet from the application layer to the gateway. we are not concerned with how the tdma assignment takes place, just that during each round, each node transmits its packets to the gateway."
"this occurs when an anti-virus security product fails to detect an instance of malware. this can be due to a zero-day attack or malware using obfuscation techniques to evade detection [cit] . the impact of this security threat depends on whether the detection method is the last line of defence in the overall malware detection system. false positives present a major problem, in that networks and host machines, can be taken out of service by the protective actions, as a consequent of alarms, such as quarantining or deleting a critical file. however, this paper focuses on end-point detection where false negatives present a security threat. therefore, this research focuses on the minimisation of fn rate along with the detection accuracy. in order to address the problem of fn rates, the optimisation function considers the fn rates by measuring the distance between the detection accuracy and the fn rate as described in (13) . steers the search by selecting those features that maximise opt value ."
"the third and last structure is the lookup table. it links the ii to the pt. for each sequence id, the lt points to the last node of the sequence in the pt. the lt purpose is to provide an efficient way to retrieve sequences from the pt using their sequence ids. the lt is updated after each sequence insertion in the pt. its time complexity is o(n) where n is the number of sequences. in terms of size, this data structure takes n * (b + p) bytes where n is the number of sequences, b is the size of an item in bytes and p is the size of a pointer in bytes. the addition of the lt to the pt makes it a lossless representation of the training set of sequences, i.e. it allows restoring the original dataset. table ( lt)"
"on the other hand, the pathway enriched at the 10-12 weeks prenatal stage was adherens junction, which was \"important for maintaining tissue architecture and cell polarity and can limit cell movement and proliferation\" [cit] ."
"the dataset is created by expressing the features as a set of opcodes density, extracted from the runtime traces of windows pe files. the dataset consists of 300 benign windows pe files taken from the 'windows program files' directory, and 350 malware files (windows pe) downloaded from vxheaven [cit] . the datasets are constructed from different program run lengths, creating 14 distinct datasets. this new datasets are created by cropping the trace files into lengths based on the number of opcodes (1k-opcodes, 2k-opcodes etc.) prior to constructing a density histogram for each cropped trace file. the dataset creation starts by cropping the original dataset into 1k opcodes, and a density histogram is created, and is repeated for 2k, 4k, 8k, 16k,… 4096k and 8192k opcodes in length."
"given a set of training sequences, the problem of sequence prediction consists in finding the next element of a target sequence by only observing its previous items. the number of applications associated with this problem is extensive. it includes applications such as web page prefetching [cit], consumer product recommendation, weather forecasting and stock market prediction."
"to reduce the computational effort, the area of search is restricted to those features that contain the most information. this is achieved by applying a filtering process that ranks features according to the information that they contain and that is likely to be useful to the svm [cit] . each feature is assigned an importance value using eigenvectors, thereby ranking the feature's usefulness as a means of classification."
"we also evaluated the robustness of the tgcn module output via sub-sampling. specifically, we half-sampled the original data at each time points and ran tgcn, which repeated for 20 times. fig. 4a compared the tgcn and the naive method in terms of the ari between the gene module output of the half-sampled data with the original data output. we found that tgcn yielded more consistent ari across time points, and they were higher than the results of naive method at the majority of the time points. we remark that the time difference between the last four times points are much larger than those between the earlier time points and the time span for each of these groups are also much wider (see supplementary table 1), which could potentially explain the vanishing advantage of tcgn at these time points as the information of the other time points became less useful. nevertheless, tgcn provided overall more robust modules. we also ran a real data driven simulation by adding white noise to the data. the standard deviation of the added white noise for each gene at each time point is 0.1 times the standard deviation of its expression. in this analysis, we again found that the ari of tgcn output is higher than that of the naive estimate (fig. 4b) ."
"due to the weakness in static analysis and the increase of obfuscated malware, it is difficult to ensure that all the code is thoroughly inspected. with the increasing amount of obfuscated malware being deployed, this research focuses on dynamic analysis (program run-time traces). other dynamic analysis approaches use api calls to classify malware, which can easily be obfuscated by malware writers. therefore, these experiments seek to identify run-time features (below the api calls) that can be used to identify malware. for this reason, the research investigates opcode density histograms obtained during program run-time as a means to identify malware."
"there are many other approaches to sequence prediction such as using sequential rules [cit], neural networks and context tree weighting [cit] (see [cit] for an overview). however, all these approaches build lossy models, which may thus ignore relevant information from training sequences when making predictions. in this work, we propose a lossless prediction model. our hypothesis is that using all the relevant information from training sequences to make predictions would increase prediction accuracy."
"we used five real-life datasets representing various types of data. table 1 summarizes their characteristics. for each dataset, sequences containing less than 3 items were discarded ."
"predicting the next item of a sequence over a finite alphabet is essential to a wide range of applications in many domains. in this paper we presented a novel prediction model named the compact prediction tree for sequence prediction. cpt is lossless (it can use all the information from training sequences to make a prediction), is built ly with a low time complexity. we also presented two optimizations (recursive divider and sequence splitter), which respectively boost the coverage of cpt and reduce its size."
"there are significant differences in energy distribution of the nodes in the network. the differences in energy levels across the wsn caused some nodes to die out earlier and some nodes to die out later. therefore, we modify the ch election criteria in the following way: in any given round, if the highest energy node is chosen to be the ch, individual node energy depletion rates are minimized with the battery levels in any zone depleting at a uniform rate."
"the goal of this research, is two-fold (1) find a set of opcodes that are good indicators of malware and (2) determine how long the program needs to run in order to obtain an accurate classification. figure 1 shows an overview of the experimental approach and to assist understanding, each section is labeled with a corresponding section heading used throughout this paper."
"we plotted the total wsn system energy level during each transmission round (fig. 3), the energy variance that resulted from the distribution of individual node battery levels ( fig. 4 ) and the number of live nodes during each round (fig. 5 ). we visually observed how nodes geographically die out throughout the simulation. in each legend of figs. 3-5, an s after an algorithm name refers to the single gateway scenario, and m refers to the multigateway scenario. the clustering algorithms dramatically outperformed the mte and direct routing algorithms as a result of rotating and distributing the high energy role of nodes performing a long-range transmission and allowing the chs to perform data aggregation. the single and multigateway clustering algorithms generally displayed similar energy depletion rates total number of alive nodes versus transmission round for all algorithm simulated. our energy efficient zone routing algorithm provided the longest timeframe of 100 percent service area coverage that are illustrated in the linear regions of fig. 3 . the clustering algorithms minimized the energy variance of the wsn, and our energy efficient zone routing algorithm, ezone, provided an indistinguishable flat variance plot compared to other algorithms as shown in fig. 4 . ezone maximized the time when all nodes are alive with the single gateway simulation outperforming other multigateway algorithms. this is significant in that it reveals the efficiencies that can be gained by implementing an energy efficient cross-layer approach."
"this paper is organized as follows. in section 2, we formally present the prediction problem and discuss related work. in section 3, we present cpt, explain how its substructures are built and how it is used to perform predictions. in section 4, we describe an experimental study. finally, in section 5, we present our conclusions."
"to simulate the mte algorithm, a route from every node to the gateway must be generated. we desire to minimize propagation distance to the gateway in order to produce a route that minimizes the overall sensor energy depletion rate. we utilize propagation distance as our link cost parameter to input into the mte algorithm. we use dijkstra's algorithm to generate our mte routes. in mte routing, the node closest to the gateway is always chosen to be included in the route. this node is known as the hot node. since the hot node is the relay point between the gateway and all traffic from other nodes, it is overwhelmed with traffic during each round and dies quickly. another hot node is then immediately chosen. this hot node concept in mte routing causes nodes that are closest to the gateway to die out first. this results in the node closest to the gateway dying out very quickly and the least preferred node (least preferred position for routing) dying out last because no other node utilizes it in the calculation of the preferred route. our mte algorithm does not employ any data aggregation strategy since at each round every node is assumed to transmit its message in a tdma scheme where there is only one message passing from source to gateway through the network at a time."
"the above biological observation motivates us modeling gcn with time-invariant latent factors, and their time-varying loadings, an approach connected with the low-rank approximation of high-dimensional covariance matrix [cit] . in the literature of matrix recovery, it has been noted that the low-rank approximation may be too restrictive and not robust, and a natural extension is the \"low-rank plus sparse\" framework [cit] . towards this end, we also included a time specific sparse component to reserve the significant links that cannot be captured by the factor model."
"once all nodes are assigned to a zone, we begin the simulation at round one and complete the simulation at the maximum desired round. in each round, the set of live nodes for each zone is identified, and the ch is chosen based on a random assignment from this set. each node in the zone then transmits its l-bit packet to the zone's ch and its energy is decremented according to our radio energy model. the ch for the zone then aggregates all the messages from the nodes in the zone and transmits the aggregated message to the gateway. the process is repeated for each zone at each round."
"\"system overview\" section describes the experimental framework and \"test platform\" section details the test platform used to capture the program traces. \"dataset creation\" section explains the dataset creation and is followed in \"opcode pre-filter\" section with a description of the filtering method used. \"support vector machine\" section introduces an svm and describes the feature selection process. the results and observations are reviewed in \"discussion\" section. finally, \"conclusion\" section concludes with a summary of the findings."
"as a result of their ubiquitous inclusion in society, wsns are finding increased applicability to the department of defense (dod) in areas specific to surveillance and reconnaissance. a wsn can be used to remotely monitor a battlespace making the presence of a warfighter unnecessary thereby increasing the safety to forces. in addition, a wsn can be used to remotely monitor deployed systems and trigger alerts at a command-and-control site when certain events occur."
", and the diagonal elements of r t were set to 1. here b i,j modeled the magnitude of time-point specific sparse component, and the definition of p t,i,j ensured that r t and l t contained non-contradicting information about the underlying structure. in our simulation study, we set q t as the 0.9 quantile of the absolute values of the off-diagonal elements of l t ."
"in this section, we investigate the properties of tgcn, and compared it with the gene coexpression networks constructed separately at each time point. we refer to this method as the \"naive\" estimate as it ignores the association between time points."
"in this section, we describe the five routing algorithms that were simulated: direct, mte, leach, zone, and ezone. we simulate these algorithms in single and multigateway scenarios. our interest is to investigate the result of load balancing techniques in single and multigateway wsns by employing load balancing techniques at each layer while focusing on the impact of varying the network layer routing algorithms on the wsn."
"the fifth experiment measures the influence of the sequence splitter optimization (cf. section 3.3). it truncates long sequences before they are inserted in the prediction tree during the training phase. this makes the prediction tree more compact by reducing the number of possible branches and their depth. reducing the depth improves the time complexity for both the training and testing processes. in this experiment we used the fifa dataset because it has long sequences. we evaluated the performance of our model against different values for the splitlength parameter. for low values (eg. 5) most of the training sequences are split. by setting splitlength to a high value (eg. 40 or more), only a small number of sequences are splitted. in figure 6, we show the effect of applying the sequence splitter strategy on the accuracy, the spatial size and the testing time, for various split lengths. by setting splitlength to a low value (left side of each chart of fig. 6 ), the spatial size is reduced by a factor of 7 while having a really low training and testing time and still having a high accuracy. once again, this parameter should be finely tuned for each dataset if one wants to achieve the best performances."
"the research presented, provides an alternative malware detection approach that is capable of detecting obfuscated malware and possible zero-day attacks. with a small group of features and short program run length, a real world application could be implemented that detects malware with minimal computation, enabling a practical real world solution to detect obfuscated malware."
"in the real data analysis, we further investigated the biological interpretation of the discovered modules using r/bioconductor package clusterprofiler [cit] . clusterprofiler performs hypergeometric test for enrichment analyses of given gene lists. we only performed the enrichment analysis of kegg pathways instead of the other gene oncology terms, as we only focused on the genes that are associated with kegg pathways."
"throughout this paper, we will use a(i, j) to denote the element of a in its ith row and jth column, and (b) + to denote the positive part of b, i.e. it is 0 if b is negative."
bible is the religious christian set of books used in plain text as a flow of sentences. the prediction task consists in predicting the next character in a given sequence of characters. the book is split in sentences where each sentence is a sequence. this dataset is interesting since it has a small alphabet with only 75 distinct characters and it is based on natural language.
"during every iteration, a new random wsn with uniform node distribution is created and run using similar parameters as before with die out parameters being appended to each random variable (rv) array. we utilized similar parameters for the number of nodes in the field, field dimensions, gateway locations, and physical and networking parameters. the only difference is that during each iteration of the algorithm, nodes are placed in different uniform locations in the grid."
"jointly model the temporal transcriptomic data when the samples at different time points are from distinct subjects. the outputs of tgcn are time-specific gene-gene correlation matrices, which allows the users to perform various downstream analysis flexibly using other computational tools such as wgcna. using both simulation and real data examples, we have shown that tcgn achieves more accurate correlation matrix estimation and more robust module identification. the statistical nature of tgcn is a \"low-rank plus sparse\" estimator of the covariance matrices, and it could be viewed as an extension of the principal orthogonal complement thresholding [cit] method. while poet focused on one single covariance matrix, tcgn jointly estimates multiple covariance matrices simultaneously under the structural assumption that their low-rank components share the same eigen vectors."
"in the prediction phase, our prediction model is used to perform predictions. let x be an integer named the prefix length. making a prediction for a given sequence s is done by finding all sequences that contains the last x items from s in any order and in any position. we call these sequences the sequences similar to s and they are used to predict the next item of s. the process of finding the sequences similar to s is implemented efficiently by using the ii. it is done by performing the intersection of the bitsets of the last x items from s. the resulting bitset indicates the set of sequences similar to s. using the lt, it is trivial to access these sequences in the pt. for each similar sequence y, the algorithm capture its consequent w.r.t s. the consequent of a sequence y with respect to a sequence s is the subsequence of y starting after the last item in common with s until the end of y . each item of each of those consequents are then stored in a structure named count table (ct) . a ct is defined as a hash table with items as keys and a score as associated value. this structure holds a list of possible candidate items and their respective score for a specific prediction and hence is unique for each individual prediction task. the item with the highest score within the ct is the predicted item. the primary scoring measure is the support. but in the case where the support of two items is equal, the confidence is used. we define the support of an item s i as the number of times s i appears in sequences similar to s, where s is the sequence to predict. the confidence of an item s i is defined as the support of s i divided by the total number of training sequences that contain s i (the cardinality of the bitset of s i in the ii). we picked the support as our main scoring measure because it outperformed other measures in terms of accuracy in our experiments."
"performing a prediction is fairly fast. the time complexity is calculated as follows. the search for similar sequences is performed by bitset intersections (the bitwise and operation), which is o(1). the construction of the ct is o(n) where n is the number of items in all consequents. finally, choosing the best scoring item is done in o(m) where m is the number of unique items in all consequents. in terms of spatial complexity, the ct is the only constructed structure in the prediction process and its hashtable only has m keys."
"bilar [cit] demonstrated using static analysis that windows pe files contain different opcode distributions for obfuscated and non-obfuscated code. bilar's findings showed that opcodes such as adc, add, inc, ja, and sub could be used to detect malware."
"while our approach fails to satisfy the criteria of 'extremely low' fn, it does meet the criteria for a 'reasonably low' fn rate for the program run lengths of 1k and above 8k. figure 6 shows the detection accuracies (dr) and the false negative rates (fn) plotted against the number of features used for classification. figure 6 is constructed by taking an average of the detection accuracies and false negative rates across the program run lengths (as indicated by the maximum optimisation values shown in table 1 ) for feature groups (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) . this shows the relationship between the number of features and the detection accuracy and false negative rates. it can be seen that both the detection accuracy and false negative rate improves with an increasing number of features (up to 13 features), and degrades and becomes more inconsistent (greater variance) thereafter."
"a native environment would provide the best platform in terms of the least tell-tale signs of a test environment and thereby mitigate any attempts by the malware to detect the test environment and exit early. however, other considerations need to be taken into account, such as ease of running the malware trace analysis. a virtual platform is selected (qemu-kvm), as the hypervisor provides isolation of the guest platform (windows 7 os test environment) from the underlying host os and incorporates a backup and recovery tool that simplifies the removal of infected files. in addition to the virtual platform, a debugger is used to record the run-time behaviour of the programs under investigation. a plethora of debugging tools exist, with popular choices for malware analysis being ida pro, ollydbg and windbg32 [cit] ."
"splitting these features into their opcode categories: arithmetic (sub, dec); logic (xor); and flow control (je, jb, jmp, pop, nop and call) infers that the program structure (flow control) changes with different program run lengths. therefore, in the following experiment, the filter is run for each program run length to ensure the optimum feature selection."
"feature space normalisation is applied to the kernel rather than to the input vectors. consider a kernel function k(x, y) which represents a dot-product in the feature space. normalisation in the feature space requires a new kernel function definition [cit] :"
"application layer: our application layer implements two strategies: 1) use of a traffic generator, and 2) use of a data aggregation technique. [cit] bit data message during each round for transmission to the gateway. data aggregation is used only for the clustering algorithms and the ch is the only node that can perform data aggregation. the ch receives all the messages from nodes in the cluster. [cit] bit message, and transmits the compressed message to the gateway at the end of each round."
"results for our random variable testing are contained in table iii . a graphical bar plot of our mean value results of table iii is shown in fig. 6 . the standard deviation of network die out statistics is given in table iv . the performance improvement of wsn clustering algorithms with data aggregation (leach, zone, and ezone) and the improvement of wsn lifetime by the addition of an additional gateway compared to mte and direct routing is illustrated in fig. 6 . we noted similar results in section iv-a for the one uniform wsn arrangement tested. ezone maximized the service life when all nodes are alive by rotating the high energy ch role to the node in each zone with the most energy. leach provided the most time through 80 percent of network die out because of the random approach of ch election and the instability of the network to maintain a uniform number of chs during each round after the first node dies. fig. 6 . summary of wsn availability for each algorithm in single and multigateway scenarios. our energy efficient zone routing algorithm provided the largest number of transmission rounds with full network service coverage v. conclusion in this paper we showed that network layer load balancing can be used to cause the network to die out in a tactically oriented fashion. the inclusion of an additional gateway extended wsn service life and offered improved coverage during die out as compared to the single gateway scenario. our ezone algorithm offers the best opportunity to extend wsn service life while maintaining tactical control of the network layer in both single and multigateway configurations. it produced the least variance in energy distribution at any round and smartly balanced cluster and node loading since our zones were implemented based on knowledge of physical layer topology and anticipated application layer loading."
"the most naive way of simulating the sparse component r t is simply generating a sparse random symmetric matrix. but such matrix does not contain any information about the structure. to generate an informative sparse component, we defined the elements of the upper triangle of the symmetric matrix r t as the following."
kosarak is a dataset containing web sessions from a hungarian news portal available at http://fimi.ua.ac.be/data. it is the largest dataset used in our experimental evaluation.
"while an optimal detection rate is a vital characteristic of any detection system, fp and fn rates need to be considered. these experiments are aimed at end host detection, and it can be argued that fn rates outweigh the importance of fp rates. therefore, the aim of our approach is to convict all suspicious files and let further malware analysis determine their true status. in a final testing phase, bootstrapping is introduced to ensure a robust measure of out-of-sample generalisation performance. the concern is that sample clustering may result, as many of the malware samples belong to the same malware family and often have similar file names. the parser used, reads files from the directory (in alphabetic order) and creates the density histograms, which may result in clustering of malware samples that belong to the same family. therefore, randomly selecting test samples prior to the svm processing will ensure that the validation data is random."
"as a result of our literature search on load balancing at each layer, we implemented the following models into each layer of the protocol stack. we build these models for both single and multigateway implementations of each routing algorithm analyzed."
"to evaluate the performance of the proposed prediction model, we performed a set of experiments. our test environment is made of an intel i5 third generation processor with 4.5 gb of available ram on a 64-bit version of windows8."
"n-gram analysis is the examination of sequences of bytes that can be used to detect malware. using a machine learning algorithm, [cit] demonstrated that n-gram analysis could be used to detect malware."
"note, the columns '1 to 20' represents the number of opcodes in each test, with the rows '1, 2, 4, 8,…, 8192' represent the program run lengths in k-opcodes. the optimisation value is shown against that number of opcodes and program run length. i.e. the first row shows the cost function value (measure of performance) for a single opcode feature, with the maximum optimisation value for each program, run length and the second row shows the cost function values for two opcode features, with the maximum optimisation value for each program run length and so on. in table 1, the maximum values are identified with an underscore. it can be seen that a point is reached, when adding more features results in a reduction of the maximum value; the assumption made is that over-fitting is occurring. as already mentioned, the grid search is guided by the performance metric in eq. (13) and is measured using tenfold cross-validation."
"to accomplish this strategy, we modify our zone routing algorithm to revise the process of electing the ch in each zone at each round. instead of randomly choosing the ch from the live nodes in the zone, we choose the ch that has the maximum energy level in the zone. based on this election criteria, nodes that are in a more preferred location (a location that minimizes energy depletion rate such as locations closer to the gateway) are chosen to be the ch for the zone more than those in a less preferred location (a location farther away from the gateway)."
"the literature on this subject is extensive and there are many different approaches [cit] . two of the most popular are ppm (prediction by partial matching) [cit] and dg (dependency graph) [cit] . over the years, these models have been greatly improved in terms of time or memory efficiency [cit] but their performance remain more or less the same in terms of prediction accuracy. markov chains are also widely used for sequence prediction. however, they assume that sequences are markovian. other approaches exist such as neural networks and association rules [cit] . but all these approaches build prediction lossy models from training sequences. therefore, they do not use all the information available in training sequences for making predictions."
"one naive estimate of σ t is simply the raw sample variance-covariance matrixς t . this approach may lead to extremely noisy estimates of gcn, because the sample size at each time point is usually very small. it also ignores the natural partial ordering of the samples (by time), and the similarity in the covariance structure among them. such similarity comes from various sources. for example, the genes in the same pathway tend to be coexpressed, and the membership of the genes to the pathways are fixed. the temporal change in gcn, however, could be caused by the fact that these pathways share genes, and their activity intensity also vary through time."
"in a pairwise comparison of time points t 1 and t 2, a pathway is said to be specific to t 1 if it satisfies the following conditions. (1) it is enriched for some modules at t 1 but not at t 2 nor for the above time-invariant modules; and (2) at most 25% of the genes that are involved in this pathway are also in any of the pathways enriched at t 2 or in the time-invariant modules. the second criterion is to avoid the case where two pathways shared a large proportion of genes, but were enriched at different time points due to their minor differences in gene composition and the statistical cutoff in enrichment analysis."
the zone clustering case described in section iii-a chooses the ch for each zone randomly. a clustering algorithm that partitions nodes into specific zones is an energy saving technique when compared to the leach algorithm because there is a lower maximum distance that any node must transmit to reach its ch. our implementation of the zones guarantees a nearby ch in the zone as compared to that of leach. in leach the nearest ch may be on the other side of the network since the criteria for a node to be elected as a ch may have only been met randomly on the other side of the field.
"before realising the classifier, the raw data is distilled into a set of meaningful information that is used to train the classifier to predict unknown malicious and benign software samples. as discussed in the related work section, the features are constructed from program trace (p) and is represented as a set of instructions (i) and where n is the number of instructions:"
"we now move into describing clustering techniques. our motivation for employing a clustering technique is aimed at rotating the energy intensive role of the node that performs the long-range wireless transmission to the gateway as well as providing the opportunity to perform data aggregation. in clustering, a cluster head (ch) is chosen from a group of nodes to serve as an intermediate relay between a group of nodes and the gateway(s). utilizing a clustering mechanism, we rotate the role of the ch to minimize the probability that any node is a hot node in an effort to balance the energy depletion of all nodes and take into consideration the topology of the network as subsequent nodes die out."
"in the first step of tgcn, time-invariant latent factors were learned from the normalized gene expression data with the hope that it may represent the gene group structure. we investigated the biological meanings of the 31 extracted components from the brainspan data in this section."
"an instruction consists of an opcode and operands. opcodes, by themselves, are significant [cit] and, therefore, only the opcodes are harvested with the operand being reduntant."
"statistics comparing the impact of an additional gateway is given in table ii . the table shows the round and distribution of nodes when 10%, 50% and 100% of nodes are dead. the statistics are graphically shown in fig. 5 . table ii shows the ratio of time the network is depleted from 100 percent to 20 percent (80 percent die out range) and the timeframe that the network provides 100 percent service coverage (the round the first node dies)."
"sequence splitter. the first optimization is done during the training phase while the pt is being constructed. let splitlength be the maximum allowed length for a sequence. for each sequence longer than splitlength items, only the subsequence formed by its last splitlength items are inserted in the pt. by using this optimization, the resulting cpt is no longer lossless since sequence information is discarded. splitting long sequences has for goal to reduce the pt size by reducing the number of possible branches and by enforcing an upper bound on the depth of branches. intuitively, it may seems that this optimization would negatively affect the prediction's accuracy. but we have observed that it boosts the accuracy by forcing prediction to focus on the latest w items of each training sequence. we also observed that this optimization greatly reduces the prediction time and the cpt size (cf. section 4.3)."
"in practice, electing the highest energy node to be the ch during each round in each zone requires additional processing by the gateway to perform ch election. our simulations perform this aspect automatically with the assumption that it is normally performed by the gateway."
"we address all these challenges. first, we propose an efficient trie-based data structure named cpt (c ompact p rediction t ree) which losslessly compress all training sequences. the construction process of the cpt structure is incremental, offers a low time complexity and is reversible (i.e. it is possible to restore the original dataset from a cpt). second, we propose an efficient algorithm to perform sequence predictions using the cpt structure. thanks to cpt's indexing mechanism, the algorithm can quickly collect relevant information for making a prediction. third, we introduce two strategies that respectively reduce the size of cpt and increase prediction accuracy. lastly, we perform an extensive experimental study to compare the performance of our approach with state of the art sequence prediction algorithms, namely ppm [cit] (p rediction by p artial m atching), dg [cit] (dependency graph) and all-kth-order markov [cit], on several real-life datasets. results show that cpt yield superior accuracy in most cases."
"the single gateway case performed better than the multigateway cases for leach and zone clustering with random ch election. this demonstrates the impact of optimizing an energy efficient network layer strategy. as efficiency is gained at the network and application layer, the impact of the additional gateway is lowered when looking at the energy depletion rate (without consideration for topology of wsn die out). the clustering algorithms all demonstrated approximately similar energy depletion rates for respective single and multigateway configurations. this value was obtained in the linear region of the plots with all nodes alive and five chs elected for each round."
"we designed a framework to compare our approach with state-of-the-art approaches on all these datasets. the framework is publicly available at http: //goo.gl/hdtdt and is developed in java. the following paragraphs describes the evaluation process of our framework. each dataset is read in memory. sequences containing less than three items are discarded. the dataset is then split into a training set and a testing set, using the 10-fold cross-validation technique. for each fold, the training set is used to train each predictor. once the predictors have been trained, each sequence of the testing set is split into three parts; the context, the prefix and the suffix as shown in fig. 2 . the prefix and suffix size are determined by two parameters named prefixsize (p) and suffixsize (s). the context (c) is the remaining part of the sequence and is discarded. for each test sequence, each predictor accepts the prefix as input and makes a prediction. a prediction has three possible outcomes. the prediction is a success if the generated candidate appears in the suffix of the test sequence. the prediction is a no match if the predictor is unable to perform a prediction. otherwise it is a failure. we define three measures to assess a predictor overall performance. local accuracy (eq. 1) is the ratio of successful predictions against the number of failed predictions."
"modern day networks abstract all the processes that take place between any two nodes and represent them in the form of layers. the general network layering construct contains the following five layers: physical, medium access control (mac), network, transport, and application layers. generally, layers of one node only rely on information from the layer immediately above or below it."
"all algorithms were executed for 5,000 iterations except for the mte algorithms that were executed for 1,000 iterations. these numbers were chosen to offer a large sample size to obtain a representative distribution, yet small enough to limit total processing time. each 5,000 iteration run required about one day of dedicated processing time on a modern windows personal computer while the mte algorithms required four and seven days for single and multigateway configurations, respectively. the mte algorithms required significantly more time because of the computational complexity in calculating the mte path for each node, each round, and each iteration using dijkstras algorithm. this is why only 1000 iterations were run."
"1. more is not always best; the optimum number of features varies with the program run length, but typically (average) 13 opcodes yield the best results."
"the training process is really fast (o(n)). the cpt take more or less space depending on the dataset. if many sequences share common prefixes, a greater compression is achieved. note that the ptitself could be further compressed by replacing frequent subsequences by single nodes or pruning infrequent nodes. these optimizations are outside the scope of this paper and will be investigated in future work."
operational codes (opcodes) are referred to as assembly language or machine language instructions and are cpu operations. they are usually represented by assembly language mnemonics.
"in the second step, one natural raw estimate of the weights of the latent factors u at time point t is the diagonal elements of u t x t x t t u . let this length k vector bẽ"
each node then transmits its data message to its ch. the ch collects all the messages of its nodes and retransmits them collectively to the gateway. this process repeats during subsequent rounds until all nodes have died.
"each sensor node in the wsn must have the ability to simultaneously serve as a sensing device and a wireless communication device that can exchange information with nearby nodes [cit] . it is critical that information from every node is communicated to a desired destination outside the network. a typical wsn and its associated supporting infrastructure is shown in fig. 1 . individual sensor nodes capture information using the battery energy they are deployed with. nodes then utilize their peers (if necessary) to pass this information wirelessly to a gateway node and then through supporting infrastructure to a command-and-control site for further processing. the focus of this paper is the deployment of tactical wsns. tactical wsns, as used by the dod, are remotely deployed in potentially hostile areas with gateway nodes located on the outskirts of these areas. the network must operate reliably and maximize sensor network coverage for the maximum amount of time in the absence of human contact. a key challenge in the deployment of tactical wsns is the limited battery power of each sensor node. this has a significant impact on the service life of the network. in order to improve the lifespan of the network, load balancing techniques using efficient routing mechanisms to achieve energy efficiency must be employed such that traffic is distributed between sensor nodes and gateway(s)."
"in this paper, we propose a novel approach for sequence prediction that use the whole information from training sequences to perform predictions. the hypothesis is that it would increase prediction accuracy. there are however several important challenges to build such an approach. first, it requires a structure for storing the whole information efficiently in terms of storage space. second, the structure should be efficiently updatable if new sequences are added. third, it is necessary to define an algorithm for performing predictions using the data structure that is time efficient and generate accurate predictions."
"the second structure is the inverted index. it is designed to quickly find in which sequences a given item appears. hence, it can also be used to find all the sequences containing a set of items. the ii is defined as a hash table containing a key for each unique item encountered during the training. each key leads to a bitset that indicates ids of the sequences where the item appears. a bitset contains n bits, where n is the number of training sequences. the presence of an item in the s-th sequence is indicated by setting the s-th bit to 1 in its bitset, and 0 otherwise. the ii, just like the pt, has an average construction time of o(n) and takes ((n + b) * u) bytes where n is the number of training sequences, u is the number of unique items and b is the size of an item in bytes."
"pca compresses the data by mapping it into subspace (feature space) and creating a set of new variables. these new variables (feature space) that define the original data are called principal components (pcs), and retain all of the original information in the data. the new variables (pcs) are ordered by their contribution (usefulness/ eigenvalue) to the total information."
"high throughput manner and low cost of sequencing technologies enabled the biologists generating an enormous wealth of data for discovering and quantifying the relationship among large amounts and various types of molecular elements, such as gene expressions, proteins, metabolites and epigenetic marks. these elements and their relationship or interactions could be modeled as nodes and edges in a network model. specifically, the gene co-expression network (gcn) models have been used for the exploration, interpretation and visualization of the relationship among genes in a wide range of biological applications, including disease-gene association [cit], identification of genes responding to environment changes, tissue specific gene identification [cit], and functional gene annotation [cit] . gcn outputs can also be combined with other biological data in various downstream analysis, such as identifying functional eqtls [cit] and studying gene-phenotype association [cit] . partially for this reason, many gcn databases were developed 2 methods"
"the output of tgcn are time-specific gene-gene correlation matrices which could serve as the input of gene co-expression network analysis procedures such as weighted correlation network analysis (wgcna) [cit] . wgcna is a method for finding clusters/modules of highly correlated genes, and then describing the correlation patterns among genes across different samples. in this study, we fed our outputs to wgcna as adjacency matrices for constructing scale-free gene networks at each time point using power transformation and for module discovery by hierarchical clustering with adjacencybased dissimilarity."
"we analyzed the rna-seq data from the brainspan atlas of the developing human brain dataset . it consists of 524 brain samples in total, grouped into 12 developmental stages ranging from eight post-conceptional weeks (pcw) to 40 years of age, including six prenatal time points and six time points after birth (see also supplementary table 1 for details). the numbers of samples in each stage range from 22 to 93. the expression values of this dataset were rna-sequencing reads in the units of per kilobase of transcript per million (rpkm) for 52376 genes in total. for the procedure of preprocessing the brainspan rna-seq data, a log transformation (log 2 (rp km + 1)) was applied on the expression values. we filtered out the genes with low variation in expression and consistently low expressions in the proceeding of development. specifically, the genes with third quartile value less than log 2 (5) and the interquartile range less than log 2 (1.5) are filtered out. after the first step filtration, 10199 genes were left. for illustration purpose and to simplify the biological interpretation, we only used the 3114 genes that are involved in kegg pathway database [cit], and consequently focused on kegg enrichment analysis for biological interpretation. since our goal is covariance matrix estimation, the genes expressions are centered at each time point."
"in the training phase, our prediction model named the compact prediction tree is built. it is composed of three data structures: (1) a p rediction t ree (pt), (2) an i nverted i ndex (ii) and (3) a lookup t able (lt). the training is done using a training dataset composed of a set of sequences. sequences are inserted one at a time in the pt and the ii. for example, figure 1 show the pt, ii and lt constructed from sequences a, b, c, a, b, a, b, d, b, c, b, d, e ."
"for a hyper-parameter can lead to poor performance in terms of overly complex hypothesis that leads to poor out-of-sample generalisation. the task of searching for optimal hyper-parameters, with respect to the performance measures (validation), is the called 'svm model selection' ."
"bootstrapping is implemented in matlab using the built-in function 'randperm' to randomly split the dataset into training and testing data. as shown in the script below, the labels are first overwritten with ones to indicate benign samples for training and zeros for malicious training samples. the script then randomly overwrites 10 % of the benign and malicious files to test as shown in the script below. the premise of bootstrapping is that, in the absence of the true distribution, a conclusion about the distribution can be drawn from the samples obtained. [cit] suggest that 200 iterations are sufficient to obtain a mean and standard deviation value of statistical importance."
"program slicing is used to investigate the effects of different program run lengths. therefore, os is defined as a set of ordered opcodes within a program execution:"
"our ezone algorithm outperformed all other algorithms from a topology perspective during node die out as well. while other algorithms created a pattern for die out, our energy efficient algorithm caused nodes to quickly die out immediately after the first node died. this also is significant in that we utilized a cross-layer approach to maximize 100 percent service coverage of the wsn. node die out of other routing algorithms occurred in an unfavorable fashion. for example, in the direct case, live nodes farther from the gateway died first since their energy is depleted proportional to their distance from the gateway. as a result, areas farthest from the gateway lost service first, while areas closest to the gateway remained in service longest. in mte routing the nodes closest to the gateway died first. the leach algorithm inefficiently creates clusters that cause the network to die out starting in the center of the sensor field and progressing radially outward. as a result of this die out mechanism, we lose coverage in the middle of the sensor field first. these die out mechanisms warrant the choice of our energy efficient zone routing algorithm for a tactical wsn since it preserves 100 percent network coverage the longest."
"as previously mentioned, the optimisation value is used to find a set of features that yield the optimum combination of detection accuracy and fn rate (as shown in table 1 ). figure 5 shows the detection accuracy and the fn rates for the different program run lengths derived from the maximum optimisation values ( table 1) . the results shown in fig. 5 are validated using 200 iterations of the bootstrapping method. figure 5 shows that medium program run lengths produce the best detection accuracy coupled with the lowest fn rates. however, good detection rates are achieved for short program run lengths but detection rates need to be considered in conjunction with the corresponding fn rate."
"the research presented, investigated the use of run-time opcode traces to discriminate between malicious and table 2 summarizes the results in terms of performance (detection, false negative and false positive rates) versus program run lengths with the corresponding opcode features. the performance rates are listed in the right-hand column (taken from table 1 ) and correspond to different program run lengths as indicated in the left-most columns i.e. 1k-opcodes, 2k-opcodes, 4k-opcodes, 8k-opcodes, etc. the central columns list the opcodes used to achieve these results."
"direct transmission to the gateway involves each node sending a packet to the gateway directly without using any other nodes along the way. during each round, the euclidean distance is calculated between the node and the gateway. the distance along with the transmit amplifier parameters given in table i is used to determine the propagation mechanism [cit] . the node's energy is decremented in proportion to the required energy for packet transmission to the gateway. for the multigateway scenario, the node chooses the gateway that requires the smaller transmit energy (i.e., the closer gateway) and transmits the packet to that gateway."
"the experimental work carried out in this research investigated the use of an svm to detect malware. the features used by the svm were derived from program traces obtained from program execution. the findings indicate that encrypted malware can be detected using opcodes obtained during program execution. the investigation continued to establish an optimal program run-length for malware detection. the dataset was constructed from run-time opcodes and compiled into density histograms and then filtered prior to svm analysis. a feature selection cost function was identified and used to steer the svm for optimal performance. the full spectrum of opcodes were examined for information, and the search for the optimal opcodes was quickly narrowed using an eigenvector filter. the findings show that malware detection is possible for very short program run lengths of 1k-opcodes that produce a detection rate of 83.41 % and a fn rate of 4.2 %. using mid-range program run lengths also yields a sound detection rate. however, their corresponding fn rates deteriorate. the 1k-opcode characteristics provide a basis to detect malware during run-time, potentially before the program can complete its malicious activity, i.e. during their unpacking and deciphering phase."
"when performing svd on pooled data for extracting u in the first step, normalization is critical. if not done properly, the leading principle directions could be heavily influenced by the time points with larger sample sizes or larger variability. data normalization typically involves centering and scaling. our goal in this step is to find the principle directions that are representative at all time points. thus we scaled x t with the leading singular value of x t and then multiply the square root of number of genes √ p. this approach assigned equal weights to the principle directions of the data at each time point. write the pooled normalized data asx"
various traditional load balancing algorithms exist in the literature [cit] . in this paper we show that these traditional algorithms have a negative impact on the service life of a tactical wsn because their design does not take into account energy efficient strategies required to extend the network lifetime. our aim in this paper is to provide realistic insights on how to incorporate an energy efficient load balancing strategy into a routing algorithm to maximize network service life.
"an svm maximizes the precision of the model by transposing the data into a feature space (high dimensional) where a hyper-plane separates the new features into their respective classes. this increases the class separation and is illustrated by way of an example, two opcodes pop and ret are used as they demonstrate the characteristics of kernel mapping. figure 3 shows a plot of pop and ret features and how there mapping into feature space increases class separation."
load balancing must be considered to achieve optimal performance. we consider optimal performance of the wsn to be when all nodes function for the maximum amount of time. we control the topology of live nodes as the concentration of dead nodes increases such that we achieve uniform service coverage throughout the area of interest through the lifetime of the wsn. we consider and obtain performance improvements by tactically including an additional gateway node in our simulations. we model one specific node-gateway arrangement and extend signal processing methodologies to model wsn die out parameters as random variables. this allows us to generate thousands of data points and draw our conclusions on an expansive subset of data instead of just one trial.
"while the full spectrum of opcodes is recorded, feature filtering is applied to narrow the search scope of the feature selection algorithm, which is applied across different program run-lengths. this research confirms that malware can be detected during the early phases of execution, possibly prior to any malicious activity."
"differential network analysis is a popular downstream analysis after gene network construction. thus we designed a conservative differential analysis of the pathways enriched in modules discovered at different time points (supplementary notes). in our study, we compared the tgcn outputs of the second (10-12 weeks prenatal) and the 11th (adolescence) time points based on this method as a showcase. it was striking to see that huntington's disease was enriched in two modules for the adolescence stage. symptoms of huntington's disease usually begin between 30 and 50 years of age [cit] . our results suggested that its molecular signature could be found in transcriptomic data at an even earlier age."
"it must be noted that the results presented in section iv-a only focused on the wsn sensor field arrangement shown in fig. 2 . to investigate other arrangements, we modeled the network die out parameters as random variables and obtained the distribution of network die out."
"step 2. check if a replying node has generated false_reply_count greater than false_reply_threshold if yes goto step 3, no goto step 4 step 3. black list the node, don't accept any rrep packet (discard) from this node further."
"j. cai proposal is based on cross layer design for detecting black and gray hole attack [cit] . it is used two counters, collisionpktnum and noncolpktnum which are added in 802.11 protocols. collision rate is calculated. the probability of node overhearing the next hop's forward action is calculated using overhear rate. the probability of collision calculated using accumulated collision rate (acr). the performance is evaluated interms of network throughput, false positive probability. it is evaluated in the grid simulation environment and random simulation environment."
"mobile ad hoc networks are multi-hop temporary wireless network. it is dynamically formed amongst group of mobile hosts/nodes having wireless connectivity. it has different characteristics such as lack of centralized administration, distributed cooperation, changing topology without any existing infrastructure or backbone. without fixed router or access point wireless nodes communicate with each other nodes acts as host as well as router. they communicate with each other within the radio range through wireless links. there are many research issues in manet such as routing, power management, bandwidth management, radio interface and security issues. the applications of mobile ad hoc networks such as tactical networks, emergency services, commercial and civilian environments, home and enterprise networking, education, entertainment etc [cit] . this paper presents the solution to gray hole attack and improves the performance of the network. the paper is organized like section 2 discusses about related work on routing protocol security, section 3 discusses about aodv protocol, section discusses 4 about gray hole attack, section 5 proposed mechanism, section 6 simulation and finally section 7 concludes the paper and future work."
"c) elapsed time of adhoc network which is analogous to current simulation time of simulator in simulation environment [cit] . sequence number used in aodv protocol is 32 bit unsigned integer (2 32 ).this value is large enough so that maximum value will never reach. continuous transmission upto 248 days at rate of 200 packets/sec would be needed to exhaust this series. adhoc networks are temporary. it would not operate for long duration exhausting the series suddenly. when the node gets detected, it would not send any alarm packet. hence it is reduces routing overhead. every node maintains a data structure in their local ram which acts as a black list cum false reply list of the nodes in the network. false reply is the replies which are detected as a fake from malicious i.e. black/gray hole. depending on the number of false reply from the node it decides to be black listed or not. using this approach, gray/malicious node is added to black list and eliminates normal nodes to enter in black list."
"if s detects 3 as m_node it will not propagate alarm to 2 & 4. when 2 detects as a m_node it will not propagate alarm to its neighbors. hence, scanning and detection is locally done and information is not broadcasted reducing routing overhead. f) gray holes are switching nodes from good to bad and vice versa. to detect them track has been kept on their switching activity."
s. kurosawa proposed an algorithm for detection scheme using dynamic learning method [cit] . the training data is updated regular time interval. destination sequence number is considered to detect the black hole attack. it is rise when the number of connections increases. the average of the difference between the dst_seq_number in rreq message and the number held in the list are calculated for each time slot. this method is not detecting the gray hole attack. as per the higher sequence number of the node entered in blocked list even the node is not malicious.
"performance of the aodv protocol is measured by varying the parameters in simulation like mobility, number of sources and number of mobile nodes. all the results are dependent on current position of nodes i.e. simulation scenario and may vary on next simulation because the gray hole is flashing between good and bad. time and duration for the gray-node to be white or black have been kept variable in tcl script using random number generator (random time) over the total simulation time (e.g. 100 sec). when m_node is absent in network, on every simulation, results for normal aodv will be same. but aodv attack and aodv under solution may vary, consider the below two scenarios 1. if random generator has assign long duration for white and short time for black. 2. short time for white and long time for black. hence the value obtained from trace file using awk will give different values on every simulation. but on every simulation result gray hole attack will be worst and solution against gray hole attack will be neighboring to normal aodv (without attack)."
"dpraodv solution checks to find route reply (rrep) packet sequence number which is higher than the threshold value [cit] . it uses the concept of dynamic learning method, in which threshold value is dynamically updated through simulation [cit] . if rrep sequence number is higher than the threshold value, then the node is suspected to be malicious and it will add in blocked list. it sends alarm packet to the neighbors informing about malicious node. this protocol takes higher routing overhead due to alarm packets. this modified protocol does not detect gray hole attack."
"is done during route discovery process. but the m_node is not black listed during first attempt of malicious activity. whenever a malicious activity is detected by receiving node, it increases a false reply count for replying node in its local black list buffer (recv. node)."
proposed algorithm is to detect gray hole node and eliminate the normal nodes with higher sequence number to enter in black list. the algorithm calculates the peak value and checks whether reply packet sequence number is less than or not.
d) each and every node use detection and black listing locally. hence any m_node will not broadcast false alarm packet pretending that particular node is malicious node (even it is normal) to other nodes in the network. e) m_node is detected and black listed when receiving /source node detects malicious activity from replying nodes.
"step 6. calculate -difference between routing table sequence number and route reply sequence (diff.). figure 2 shows the flowchart of recvreply function, figure 3 shows the execution the propose approach and figure 4 calculates the peak value."
"average end to end under attack reduces as the packets are dropping as they are not sent to destination. average end to end delay under solution is greater due to extra calculation of peak value. reply forward ratio is high for the normal aodv. it becomes low under malicious attack on the manet. number of replies received by replying node is high for normal aodv. it becomes low under malicious attack. simulation studies shows that the performance of routing protocols in terms of throughput, packet dropping rate and end-to-end delay strongly depends on network conditions such as mobility, traffic and number of nodes. figure 5, figure 6, figure 7 and figure 8 shows the graph for throughput, packet drop rate, packet delivery ratio and normalized routing overhead vs. mobility respectively. figure 9,figure 10, figure 11 and figure 12 shows the graph for throughput, packet drop ["
"the conditions to add in black list are. e.g. if only one false reply is detected from normal node, it would not add to black list. if the number of false reply is detected from malicious node it adds to the list."
"black hole attack is kind of dos attack where black hole node can attract all packets by pretending shortest route to the destination [cit] . it drops all traffic destined for that node when traffic is received by it. the effect of this attack completely degrades the performance of the network because the destination node never receives any information from the source. gray hole attack is a specialization variation of black hole attack, where nodes switch their states from black hole to honest intermittently and vice versa. detection of gray hole attack is harder because nodes can drop packets partially not only due to its malicious nature but also due to congestion. figure 1 shows the gray hole attack. detection is difficult because the node's nature is not stable, it can't predicted that when node will be malicious and when it will turn to normal node.9th node selects gray hole even node 2 has valid and shortest path to destination."
"j. sen proposed a mechanism for detection of gray hole attack [cit] . this mechanism requires four steps. in neighborhood data collection, data routing information (dri) table store data forwarding information, dri table contains node number, from and through bits, ratio of request to send (rts) to clear to send (cts) and check bit. from stands the information on routing data packets from the node, through stands for information on the routing data packets through the node.dri in global alarm raising module, it is sending alarm message to the all nodes in the network. this module is related to the gray hole attack detected by the cooperative anomaly detection algorithm. in this method, overhead of malicious node detection is high and speed of gray hole attack increases."
"s. banerjee proposed a solution for detection and removal of chain of cooperative black hole and gray hole attack [cit] . in this solution, all nodes monitor to each other. this mechanism examines against aodv protocol. due to monitoring network it has high overhead and also consumes more energy for monitoring. detection process for malicious node is slow."
"as a future scope of this work, the false reply threshold value which is static in this paper can be made dynamic based on elapsed time and predicted time for existence of network. also to find cooperative environment to protect from gray hole attackers."
hence only one false reply does not add to list. it checks for the false replies from that node and then adds to black list.
"gray hole is a node switches from black to honest and vice versa. whenever it switches to black, it will generate false reply which helps to detect it as a gray hole."
"in modified protocol, proposed approach uses effective way of providing security in aodv against gray hole attack. proposed mechanism is to detect gray hole attack and eliminate the normal nodes with higher sequence number to enter in the black list. effective decision making regarding black listing of nodes by keeping track on switching activity. effective use of peak value and implementation of fresh approach of current elapsed time of adhoc network to make the proposed mechanism more efficient. it is not sending any alarm packets to other nodes when gray hole detected. hence it is reducing extra routing overhead incurred by sending alarm packets."
"also it give every node on attempt/chance before adding to black list, hence resources for gray hole for packet forwarding can be utilized to some extent when they are normal state."
"stop rate, packet delivery ratio and normalized routing overhead vs. number of sources respectively. figure13,figure.14, figure. 15 and figure.16 shows the graph for throughput, packet drop rate, packet delivery ratio and normalized routing overhead vs. number of nodes respectively."
"b) in this approach it makes 3 attempts of false reply to add a m_node in the black list. the attempts of false reply can be incremented as per scenario and elapsed time, node density. c) black list is local for each node. each node maintains its own black list buffer. information of the list is never broadcasted to any other nodes. the black list is private to each node individually."
"the metrics used to evaluate the performance of the mobile ad hoc networks are given. throughput: it is defined as the amount of data transferred over the period of time expressed in kilobits per second (kbps). packet drop rate: it is the ratio of the data lost at destinations to those generated by the cbr sources. the packets are dropped when it is not able to find the valid route to deliver the packets. packet delivery ratio: it is the ratio of data delivered to the destination to the data sent out by source. normalized routing overhead: it is the ratio of routing transmissions to the data transmissions in the simulations. the routing transmissions are rreq,rrep, rerr etc."
"g. xiaopeng proposed the detection scheme against gray hole attack [cit] . it consists of three algorithms which are creating proof algorithm, the check up algorithm and the diagnosis algorithm. in creating proof algorithm, the source nodes are creating proof which is based on aggregate signature algorithm for received message. in check up algorithm, the source node suspects the malicious node. reliability is good. bidirectional links are not required. security is satisfactory and bandwidth overhead is low. in diagnosis algorithm, the evidences are getting from the check up algorithm, it finds the malicious node. this mechanism is not detecting all malicious nodes."
